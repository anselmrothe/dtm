UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Speech and Gaze Conflicts in Collaborative Human-Robot Interactions
Permalink
https://escholarship.org/uc/item/44z8484b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Admoni, Henny
Datsikas, Christopher
Scassellati, Brian
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

           Speech and Gaze Conflicts in Collaborative Human-Robot Interactions
                                 Henny Admoni, Christopher Datsikas, and Brian Scassellati
                          (henny@cs.yale.edu, christopher.datsikas@yale.edu, scaz@cs.yale.edu)
                                           Department of Computer Science, Yale University
                                                  New Haven, Connecticut 06520 USA
                              Abstract                                 have investigated the effects of speech-gaze conflicts. In this
                                                                       paper, we investigate how speech-gaze conflicts are handled
   Gaze and speech are both important modes of communication
   for human-robot interactions. However, few studies to date          by human partners in collaborative, embodied human-robot
   have explored the effects of conflict in a robot’s multi-modal      interactions. We focus on object selection tasks in which a
   communication. In this paper, we investigate how such speech-       robot provides instructions to a human, because these scenar-
   gaze conflicts affect performance on a cooperative referential
   task. Participants play a selection game with a robot, in which     ios are central to collaborative action, and because communi-
   the robot instructs them to select one object from among a          cation misinterpretation in such scenarios can be costly.
   group of available objects. We vary whether the robot’s gaze           We compare congruent gaze—in which the robot looks at
   is congruent with its speech, incongruent with its speech, or
   absent, and we measure participants’ response times to the          the object it references in speech—and incongruent gaze—
   robot’s instructions. Results indicate that congruent speech        in which the robot looks at a different object—to a control
   facilitates performance but that incongruent speech does not        condition in which the robot does not exhibit gaze cues. To
   hinder performance. We repeat the study with a human actor
   instead of a robot to investigate whether human gaze has the        quantitatively measure the effect of speech-gaze conflicts, we
   same effect, and find the same results: in this type of activ-      record the time between when the robot begins its instructions
   ity, congruent gaze helps performance while incongruent gaze        and when participants select an object. Response time serves
   does not hurt it. We conclude that robot gaze may be a worth-
   while investment in such situations, even when gaze behaviors       as an approximation of task efficiency; faster responses mean
   may be unreliable.                                                  less overall time taken for the task.
   Keywords: human-robot interaction; eye gaze; non-verbal                As a final manipulation, we also include a human agent
   communication                                                       condition, in which the robot is replaced by a person who
                                                                       performs the robot’s role in the experiment. The human agent
                          Introduction                                 condition attempts to discover whether robot gaze is any more
In typical human interactions, eye gaze supports and aug-              or less influential on human behavior than human gaze.
ments spoken communication (Kleinke, 1986). People gaze                   The results of this study provide evidence of the effective-
almost exclusively at task-relevant information (Hayhoe &              ness of gaze in collaborative human-robot interactions. As
Ballard, 2005), and gaze is used to disambiguate statements            described below, we find that congruent gaze facilitates per-
about objects in the environment (Hanna & Brennan, 2007).              formance in both robot and human conditions. Interestingly,
Similar mechanisms are also at play in human-robot interac-            we also find that incongruent gaze does not hinder perfor-
tions: task-relevant robot eye gaze can be used to improve the         mance in either the robot or the human conditions. In other
efficiency of collaborative action (Boucher et al., 2012).             words, in this task, people are able to recover quickly enough
   For example, imagine a human and robot collaboratively              from speech-gaze conflicts that their performance is statisti-
constructing a birdhouse. The robot can use its eye gaze to            cally no different than not having gaze at all. These results
clarify an ambiguous speech reference, saying “Please pass             suggest that adding referential gaze may be a low-risk way to
the green block” while looking at a particular green building          improve human performance in similar environments, even
block to distinguish it from among other green blocks. This            when the gaze system is unreliable.
multi-modal communication makes the interaction more ef-
ficient by using multiple channels to convey information, re-                                  Related Work
quiring less investment in costly mechanisms like generating           Directional eye gaze seems to be a special stimulus, evoking
sufficiently descriptive speech, and improving the naturalness         reflexive attention shifts that are robust to top-down modula-
of the interaction (Huang & Thomaz, 2011).                             tion (Friesen, Ristic, & Kingstone, 2004). Functional MRI
   But robots are not perfect, and sometimes speech and gaze           studies reveal a significant overlap in the brain areas that pro-
cues will conflict. Sensor errors, hardware malfunctions, and          cess theory of mind and those that process eye gaze (Calder
software bugs can cause mismatches between a robot’s gaze              et al., 2002). In fact, observing someone signaling the pres-
and speech. In such cases, a human partner receives incor-             ence of an object with referential gaze elicits the same neural
rect or contradictory information from the robot. The human            response as observing someone physically reaching to grasp
might misinterpret the robot’s speech or, at best, must hesi-          that object (Pierno et al., 2006), indicating that people use
tate to decide what the robot means, decreasing the collabo-           gaze as a powerful indicator of others’ future behavior.
ration’s efficiency and increasing the human’s cognitive load.            Where we look is closely coupled with what we say in
   While a growing body of evidence shows that people can              human-human interactions. Objects or figures in the envi-
interpret robot gaze and speech, only a few studies to date            ronment are typically fixated one second or less before they
                                                                   104

                                                                      formation. In most of the literature about referential gaze in
                                                                      HRI, however, robot gaze is congruent with speech.
                                                                         Some researchers have investigated the effects of speech
                                                                      and gaze conflicts in HRI. In a video-based study (Staudte
                                                                      & Crocker, 2011), participants evaluated the correctness of a
                                                                      robot’s statements about objects in front of it (for instance,
                                                                      “the cylinder is bigger than the pyramid that is pink”). When
                                                                      the robot’s gaze was congruent with its speech, response
                                                                      times were shorter than a no-gaze control; when gaze was in-
                                                                      congruent, response times were longer than the control. This
                     (a) Robot agent condition                        suggests that people relied on gaze to facilitate sentence pro-
                                                                      cessing, and that incongruent gaze hinders comprehension.
                                                                         Unlike our experiment, however, Staudte and Crocker’s
                                                                      task involved sentence evaluation rather than object selec-
                                                                      tion, which requires a different cognitive skill set. Further-
                                                                      more, their study was conducted with video stimuli instead
                                                                      of embodied robots. While virtual robots increase the ease
                                                                      of use and replicability of stimuli, they may not have as
                                                                      strong an influence on human behavior as physically embod-
                                                                      ied robots (Bainbridge, Hart, Kim, & Scassellati, 2011).
                                                                         In contrast, research using an object selection task and an
                                                                      embodied robot finds no difference in response times between
                                                                      no gaze and incongruent gaze conditions, though results sup-
                    (b) Human agent condition
                                                                      port the benefit of congruent gaze (Huang & Mutlu, 2012).
Figure 1: Participant view of the experiment. MyKeepon or             However, this study used a between-subjects design in which
a human actor provided verbal and gaze cues about which               the robot exhibited only one type of gaze (congruent or in-
shape to select.                                                      congruent) to each participant. Participants could acclimate
                                                                      to the robot’s gaze strategy, which does not address situations
are named in conversation (Griffin & Bock, 2000; Yu, Scher-           where gaze is usually helpful but occasionally incorrect.
merhorn, & Scheutz, 2012). When referencing objects, peo-                The current work is inspired by these studies, and builds
ple use eye gaze as a strong and flexible cue for eliminating         upon them by investigating conditions in which speech and
ambiguity (Hanna & Brennan, 2007). When access to a part-             gaze are incongruent rather than only congruent (Boucher et
ner’s eye gaze is restricted, for instance because the partner is     al., 2012), using a physically embodied robot rather than a
wearing sunglasses, people are slower at responding to their          video (Staudte & Crocker, 2011), and introducing uncertainty
partner’s referential communication (Boucher et al., 2012).           about the robot’s reliability to avoid habituation to one partic-
   As in human-human interactions, eye gaze is an important           ular condition (Huang & Mutlu, 2012).
part of human-robot interactions. Robot eye gaze can influ-
ence whether people join a conversation or feel excluded from
                                                                                              Experiment 1
it (Mutlu, Shiwa, Kanda, Ishiguro, & Hagita, 2009), can in-           This experiment is designed to investigate whether gaze con-
fluence people to favor certain objects over others (Mutlu, Ya-       flicts hinder task performance in collaborative human-robot
maoka, Kanda, Ishiguro, & Hagita, 2009), and can facilitate           interactions. Participants engaged in an object selection task
cooperative behaviors like object handoffs between humans             with a robot. On each trial, the robot provided spoken instruc-
and robots (Strabala et al., 2013). Exhibiting joint attention,       tions of the form “Please pick the [shape] in the [color] zone”
a type of social gaze, increases ratings of a robot’s compe-          where shape and color referred to objects in front of the par-
tency and naturalness (Huang & Mutlu, 2012).                          ticipant (Figure 1). Each of the nine objects was referenced
   More specifically, studies of human-robot interaction have         nine times during the interaction, for a total of 81 trials.
shown that robot gaze can be used to clarify speech. If a                On each trial, the robot also provided a gaze cue, which
robot gazes toward an object while naming it, people select           was either congruent with the speech (i.e., looking at the same
the object more quickly than if the robot names the object            object), incongruent with the speech (i.e., looking at a differ-
without looking at it (Boucher et al., 2012; Huang & Mutlu,           ent object), or absent (no movement). The robot started each
2012). With both robots (Huang & Mutlu, 2012; Mutlu, For-             trial in a neutral position, with gaze directed straight forward
lizzi, & Hodgins, 2006) and virtual agents (Andrist, Pejsa,           and approximately 30 cm below the participant’s eyes. To
Mutlu, & Gleicher, 2012), gazing at task-relevant objects dur-        initiate a gaze cue, the robot first attempted to establish joint
ing teaching—for instance, looking at a map while describ-            attention by looking up at the participant’s face (mutual gaze),
ing political boundaries—increases peoples’ retention of in-          and then engaged in object reference by looking down toward
                                                                  105

            Gaze ahead
       Gaze to person
        Gaze to object
              Utterance                                         Please pick the         cube in the green zone.
                  Time   0ms         700ms               1700ms                  2700ms                                 4300ms
                                                             1860ms                 2850ms        3440ms
                                                          Speech onset          Shape reference  Color reference
                                                                                                 (linguistic disambiguation)
Figure 2: A visual representation of the speech and gaze during a typical trial. This figure shows a congruent trial: the agent
both verbally and physically indicates the green cube. In an incongruent trial, the spoken word “green” is replaced with one of
the other two zone colors. In a no-gaze trial, the agent gazes straight ahead.
the selected object, then returned to look at the participant’s        mands and retrieves information such as encoder positions
face before returning to the neutral position (Figure 2). The          from the MyKeepon hardware. This allowed for easy control
robot did not use sensors to confirm that mutual eye gaze was          of the MyKeepon robot platform.
successful; instead, the experiment was pre-scripted and ran
autonomously. In no gaze trials, the robot did not move at all         Procedure
and continued looking ahead in neutral position.                       Twenty two people participated in this experiment (10 fe-
   In human-human communication, eye gaze moves toward                 males). Their ages ranged from 18 to 34, with a mean age
an object prior to a verbal reference and away from the object         of 22, and most were Yale undergraduate students. Partici-
just as it is named (Yu et al., 2012). We carefully aligned the        pants were compensated $8.
verbal and gaze cues to mimic natural behavior (Figure 2).                Participants were told that they would play an object selec-
   We measured how much time participants took to select a             tion game to help evaluate a new robot platform called My-
shape. By comparing response times in the congruent, in-               Keepon. They were shown the nine shapes and told that in
congruent, and no gaze conditions, we are able to determine            each round of the game, MyKeepon would provide instruc-
whether gaze has any facilitation or hindrance effect on the           tions on which shape to choose. Participants were informed
speed with which people respond to the robot’s instruction.            that they should select the shape as quickly and accurately
                                                                       as possible. They were also told to return their finger to a
Apparatus                                                              marked start position on the table between trials. This instruc-
The experiment apparatus is a black box measuring 120cm                tion was given to eliminate any “hovering” over the buttons
by 40cm by 6cm (Figure 1). The robot was placed on the                 so that response times are consistent across trials.
table across the box from the participant, approximately 80               In each trial, a computer-generated voice provides a verbal
cm away. Three zones are marked by colored paper on top of             cue, which is a sentence that first indicates the type of ob-
the box: red, blue, and green. Each zone contains an identical         ject and the zone the object is in, for example, “Please pick
set of white blocks in simple shapes—a cube, a pyramid, and            the cube in the green zone” (Figure 2). The sentence is con-
a cylinder—arranged side-by-side in a single row on top of             structed so that the specific object referred to by the sentence
the zone. A momentary pushbutton switch in front of each               remains ambiguous until the color of the zone is stated near
object is used to select that object, and the precise timings of       the end of the sentence. Until this point of linguistic disam-
button presses are recorded on a nearby computer.                      biguation, there are three potential matches for the sentence
   We used a relatively inexpensive, easily modifiable, and            (the named shape in the red, blue, and green zones), so partic-
commercially available robot called MyKeepon (Figure 1).               ipants cannot select an object with more than 33% reliability.
This 32cm tall, snowman shaped, interactive robot toy has                 Simultaneous with the verbal cue, the robot also provides a
a rubber yellow skin and four degrees of freedom: rotation             gaze cue by orienting toward one of the shapes. On congruent
around the base, left/right lean, front/back lean, and up/down         trials, the robot turns toward the shape named by the verbal
bob. MyKeepon is a consumer-grade version of a research                cue. On incongruent trials, the robot turns toward a different
robot called Keepon Pro, which was designed to be a socially           shape at least three spots away from the correct shape. This
evocative but simple robotic agent (Kozima, Michalowski, &             restriction ensures that there is no confusion about whether
Nakagawa, 2009). The robot’s minimalist design and salient             the gaze was directed toward the correct shape. On no gaze
eyes make it a useful platform for HRI studies about eye gaze.         trials, the robot remains looking straight ahead.
   We modified a MyKeepon to make it programmable for                     Participants first practiced two congruent gaze trials under
this experiment. The MyKeepon internal microprocessors                 experimenter supervision to familiarize themselves with the
were connected to an Arduino Uno, an open-source electronic            task; these practice trials were not recorded, and participants
prototyping platform. Using the I2C bus on the MyKeepon                were not told that the robot’s gaze would vary in other trials.
microprocessor and open-source software (Michalowski,                  After the practice, each participant experienced two sections
Machulis, & Gasson, 2013), the Arduino sends motor com-                of the experiment with no breaks between them.
                                                                   106

   In the first section, called the blocked section, participants             Agent     Gaze type      RT (ms)    SD (ms)     N
saw each trial type blocked together: first nine no gaze tri-                           Congruent        4572        256
als, then nine congruent trials, then nine incongruent trials,                Robot     Incongruent      4814        235      22
with no demarcation between the blocks. The purpose of the                              None             4834        222
blocked section is to establish a baseline measure of reaction                          Congruent        4427        309
                                                                              Human     Incongruent      4568        289       9
time (in the no gaze block) and to observe how performance                              None             4621        189
changes as participants become familiar with the robot’s gaze.
   The second section of randomized trials followed the               Table 1: Response times (RTs) for all trials in Experiments 1
blocked section immediately. During the randomized section,           and 2. RTs are measured from start of trial, including time to
each participant saw a unique random ordering of all 54 com-          speak sentence. When measured from linguistic disambigua-
binations of shape, color zone, and gaze type. The purpose            tion, RTs are similar to previous work (Boucher et al., 2012).
of this section is to measure the effects of gaze cues when
participants did not know whether the cue would help or not.             The response facilitation from robot gaze supports previ-
   After both sections, participants were given a survey with         ous findings in HRI (such as Boucher et al., 2012; Huang &
demographic questions and one free-response question: “Did            Mutlu, 2012; Staudte & Crocker, 2011). However, the lack
you notice anything unusual about the robot’s behavior?”              of hindrance from incongruent gaze conflicts with previous
                                                                      findings (Staudte & Crocker, 2011).
Results                                                                  To test whether this effect is due to the robot or to the task,
Twenty-two participants each completed 27 blocked trials              we conduct a new experiment with a human in place of the
and 54 randomized trials for a total of 1782 data points.             robot. If the same procedure—now with human gaze—yields
Four trials (0.2%) were discarded because no response was             the same effect, we can conclude that the task, and not the
recorded within 12 seconds, either because the participant            agent, is responsible for the absence of hindrance.
did not press a button or because the button press did not
register. We also discarded the no gaze blocked section for                                  Experiment 2
one participant (nine trials, or 0.5% of all trials) due to self-     We replicated Experiment 1 with a small number of partic-
reported noncompliance. Participants were highly compliant            ipants. The apparatus and procedure are identical to Exper-
with the verbal cue, selecting a shape that was different from        iment 1, except that the robot is replaced by a human actor
the robot’s spoken instruction on only five trials (0.3%). Re-        (Figure 1b). For consistency, the verbal cue is still provided
sults are shown in Table 1 and Figure 3a.                             by the computer-generated voice from Experiment 1. We
   A repeated measures ANOVA of response time by gaze                 took care to make the human gaze as similar as possible to the
type for all trials shows a significant main effect (F(2, 42) =       robot gaze; therefore, the actor practiced looking at the ob-
43.181, p < 0.001). Post-hoc tests with a Bonferroni correc-          ject for the correct duration and shifting her gaze away from
tion reveal that response times to congruent gaze were signif-        the referenced object just before it was named. On the post-
icantly shorter than response times to incongruent gaze (by           task questionnaire, the free-response question was changed
242ms, p < 0.001) and to no gaze (by 262ms, p < 0.001).               to: “Did you notice anything unusual during the experiment?”
There was no statistical difference between incongruent and              Nine participants (2 females) took part in Experiment 2.
no gaze trials.                                                       Their ages ranged from 18 to 20 (mean of 19). They were all
   There are several conclusions to be drawn from these re-           Yale undergraduates and they were compensated $8.
sults. First, people were highly accurate and highly consis-
tent in following the robot’s speech, complying with speech           Results
instructions on 99.7% of trials even though 33% of the trials         Table 1 and Figure 3b show the results of Experiment 2. A
included a conflicting gaze cue. The high rate of compliance          repeated measures ANOVA to test the effect of gaze type on
with speech suggests that cases in which participants failed          response times found a significant main effect (F(2, 16) =
to follow the speech cue involved button press errors, though         7.892, p = 0.004). Post-hoc tests with a Bonferroni correc-
we did not explicitly ask participants to report errors.              tion reveal that response times to congruent gaze are shorter
   When the robot’s gaze indicated the same shape as the ver-         than response times to incongruent gaze (141 ms, p = 0.018)
bal cue, participants used gaze to guide their responses, as          and no gaze (194 ms, p = 0.033). No significant differ-
indicated by the significantly improved response times in the         ence was found between response times in incongruent and
congruent gaze condition. Surprisingly, participants were not         no gaze conditions. Participants made an erroneous selection
hindered by incorrect robot gaze: they responded no slower            (not following the speech cue) on 20 (2.7%) of the 729 trials.
to incongruent trials—when gaze and speech did not match—                To compare robot and human gaze, we conducted an
than they did to no gaze trials, where there was no gaze cue.         ANOVA on response time with gaze type as a within-subjects
In other words, congruent gaze helped people respond to a             factor and agent type as a between-subjects factor. The analy-
robot’s verbal instructions more quickly, but incongruent gaze        sis reveals a significant effect of gaze (F(2, 86) = 6.564, p =
did not make them respond more slowly than no gaze at all.            0.002) but no effect of agent (F(1, 86) = 0.351, p = ns) and
                                                                  107

                                          Mean	  response	  *mes	  to	  robot	                                                             Mean	  response	  *mes	  to	  human	                                Response	  Times	  on	  Consecu.ve	  Blocked	  Trials	  	  
                                                     instruc*ons	                                                                                         instruc*ons	                                                      by	  Block	  for	  All	  Agent	  Condi.ons	  
                               5000	                                                                                                5000	                                                                   5000	  
 Response	  .me	  (ms)	                                                                             Response	  .me	  (ms)	  
                               4800	                                                                                                4800	                                                                   4800	  
                               4600	                                                                                                4600	                                                                   4600	  
                               4400	                                                                                                4400	                                                                   4400	  
                                                                                                                                                                                                                                                        No	  gaze	  
                                                                                                                                     4200	                                                                   4200	                                    Congruent	  
                               4200	  
                                                                                                                                                                                                                                                        Incongruent	  
                               4000	                                                                                                4000	                                                                   4000	     1	  	  	  	  	  	  	  	  	  2	  	  	  	  	  	  	  	  	  3	  	  	  	  	  	  	  	  	  4	  	  	  	  	  	  	  	  	  5	  	  	  	  	  	  	  	  	  6	  	  	  	  	  	  	  	  	  7	  	  	  	  	  	  	  	  	  8	  	  	  	  	  	  	  	  	  9	  
                                          Congruent	  	  	  Incongruent	  	  	  No	  Gaze	                                               Congruent	  	  	  Incongruent	  	  	  No	  Gaze	  
                                                                                                                                                                                                                                                                                                                                        Trial	  number	  
                                   (a) Experiment 1, robot agent.                                                                      (b) Experiment 2, human agent.                                                                  (c) Blocked trials, both agents.
Figure 3: Response times to agent instructions. Figures (a) and (b) show mean response times across all trials for each exper-
iment. Figure (c) shows the blocked trials section separated by trial number. In all figures, congruent gaze facilitates response
times, while incongruent and no gaze conditions show no significant difference. Error bars show 1 standard error.
no interaction (F(2, 86) = 0.291, p = ns). Post-hoc pairwise                                                                                                                         cue at all. Therefore, while they use gaze to plan their mo-
comparisons on the significant result show that congruent                                                                                                                            tions, participants quickly recover from erroneous planning
gaze led to shorter response times than incongruent gaze (191                                                                                                                        when the point of linguistic disambiguation is reached. This
ms, p = 0.017) and no gaze (228 ms, p = 0.003) for all partic-                                                                                                                       facilitation occurs even in the randomized section, when par-
ipants regardless of agent condition. No significant difference                                                                                                                      ticipants could not know ahead of time whether gaze would
was found between incongruent and no gaze conditions.                                                                                                                                be congruent, incongruent, or absent. In short, current results
   The blocked section of the experiment reveals how partic-                                                                                                                         suggest that there are scenarios in which adding eye gaze cues
ipants acclimated to a consistent gaze type. Because there is                                                                                                                        to a robot’s behavior is a worthwhile investment: at best, it in-
no significant difference between agent conditions, we can                                                                                                                           creases comprehension and efficiency, and at worst (when the
collapse the data across these conditions for this analysis.                                                                                                                         gaze cue is in error), there is little damage to performance.
Figure 3c shows mean response times for each trial in the                                                                                                                               Other research has shown that incongruent gaze hinders
blocked section, averaged across participants in both agent                                                                                                                          performance in robot-instruction tasks (Staudte & Crocker,
conditions. Recall that participants saw nine no gaze trials,                                                                                                                        2011). However, our study’s task involves a lighter cogni-
then nine congruent trials, and then nine incongruent trials.                                                                                                                        tive load, which may explain our divergent findings. In both
The no gaze block serves as a baseline for response times                                                                                                                            studies, participants identify the referent of the robot’s gaze
without gaze. As shown in Figure 3c, response times re-                                                                                                                              and speech, but in our task, they simply select that referent,
mained fairly stable during the no gaze block. Response times                                                                                                                        whereas in Staudte and Crocker’s task, they compare features
improved during the congruent block, indicated by the down-                                                                                                                          of that referent to features of other visible objects and then
ward slope of the congruent block line. In contrast, there was                                                                                                                       decide if a given statement is true or false. Thus, our exper-
no improvement of performance over the nine incongruent                                                                                                                              iment’s task requires less cognitive processing, which may
blocked trials. Participants performed slightly better on the                                                                                                                        allow people to quickly overcome the incongruent gaze. This
incongruent block than on the no gaze block that preceded it,                                                                                                                        conjecture is supported by findings from a different study that
though this effect may be due to practice.                                                                                                                                           used a similar task to ours (Huang & Mutlu, 2012). This study
   Although participants were never explicitly told to follow                                                                                                                        also found no difference between no gaze and incongruent
gaze, they rapidly adapted to using congruent gaze to im-                                                                                                                            gaze, while confirming the benefit of congruent gaze.
prove their performance. The rate of improvement does not                                                                                                                               An alternate explanation is that the agent looks at the par-
decrease by the ninth trial, suggesting that more congruent                                                                                                                          ticipant before speaking on incongruent trials, but not on no
gaze trials might have led to continuing improvements.                                                                                                                               gaze trials, which may cue participants for the impending se-
                                                                                                                                                                                     lection and negate any hindering effects of incongruent gaze.
                                                            General Discussion                                                                                                       A revised no gaze condition in which the robot looks at the
For both robot and human agents, participant response times                                                                                                                          participant but not to a block would clarify this possibility.
were faster when the agent’s gaze cue was congruent with its                                                                                                                            Although mean response times did not significantly dif-
verbal cue, compared to incongruent gaze and no gaze con-                                                                                                                            fer between robot and human agents, some differences did
ditions. Because the gaze cue is delivered before the point of                                                                                                                       emerge between these conditions. Participants in the human
linguistic disambiguation, the fact that participants responded                                                                                                                      agent group responded more quickly on average, although the
more quickly on congruent gaze trials indicates that they                                                                                                                            difference was not significant (possibly because of the small
planned their motion according to the gaze cue before hearing                                                                                                                        group size). Perhaps relatedly, the error rate for participants
the disambiguation. When the cue was incongruent, however,                                                                                                                           in the human agent group (2.7%) was higher than the error
participants responded no slower than if there were no gaze                                                                                                                          rate for participants in the robot group (0.3%).
                                                                                                                                                                            108

   In response to the post-interaction survey question asking        Griffin, Z. M., & Bock, K. (2000). What the Eyes Say About
whether they noticed “anything unusual” during the experi-             Speaking. Psychological Science, 11(4), 274–279.
ment, five of the nine participants in the human agent group         Hanna, J. E., & Brennan, S. E. (2007). Speakers’ eye gaze
(56%) made reference to intentional misdirection by the actor,         disambiguates referring expressions early during face-to-
writing things like “She built up my trust and then betrayed           face conversation. Journal of Memory and Language, 57,
me” and “She tried to trick me with her gaze.” In comparison,          596–615.
only six of the 22 people in the robot group (27%) included          Hayhoe, M., & Ballard, D. (2005). Eye movements in natural
such statements about intentional action from the robot. Even          behavior. Trends in Cognitive Sciences, 9(4), 188–194.
with identical behaviors, there was some difference in agency        Huang, C.-M., & Mutlu, B. (2012). Robot Behavior Toolkit:
attributions between robots and humans.                                Generating effective social behaviors for robots. In 7th
   However, human gaze is inherently less precise than robot           ACM/IEEE International Conference on Human-Robot In-
gaze. Future experiments could record the human actor’s face           teraction (HRI ’12).
to verify that human gaze timings were comparable to robot           Huang, C.-M., & Thomaz, A. L. (2011). Effects of respond-
timings. To generalize the results, future work should also            ing to, initiating and ensuring joint attention in human-
test different collaborative scenarios to understand the condi-        robot interaction. In 20th IEEE International Symposium
tions under which facilitation is possible without hindrance.          on Robot and Human Interactive Communication (RO-
Eye tracking would reveal at which point people decide to              MAN 2011). Atlanta, GA USA.
follow or ignore a robot’s gaze. Future work should also ran-        Kleinke, C. L. (1986). Gaze and eye contact: A research
domize the assignment of conditions, rather than recruiting            review. Psychological Bulletin, 100(1), 78–100.
independent groups of participants, to rule out the possibility      Kozima, H., Michalowski, M. P., & Nakagawa, C. (2009).
of group effects causing the observed variations.                      Keepon: A Playful Robot for Research, Therapy, and En-
   MyKeepon has limited articulation and a simplified ap-              tertainment. International Journal of Social Robotics, 1,
pearance. We chose this robot intentionally—MyKeepon’s                 3–18.
large eyes and gross body movements make its eye direction           Michalowski, M., Machulis, K., & Gasson, M. (2013, July).
highly salient—but it is simpler and smaller than many other           BeatBots MyKeepon GitHub Repository. https://github
robots. Our results, therefore, may be most applicable to this         .com/beatbots/MyKeepon.
type of robot. Future studies should investigate robots with         Mutlu, B., Forlizzi, J., & Hodgins, J. (2006). A Storytelling
articulated eyes as well as anthropomorphic robots to find             Robot: Modeling and Evaluation of Human-like Gaze Be-
whether physical appearance and eye motion affect gaze cues.           havior. In 6th IEEE-RAS International Conference on Hu-
                                                                       manoid Robots (Humanoids ’06).
                      Acknowledgments                                Mutlu, B., Shiwa, T., Kanda, T., Ishiguro, H., & Hagita,
This work is supported by NSF grants 1139078 and 1117801.              N. (2009). Footing in Human-Robot Conversations: How
Thank you to Iulia Tamas and Terin Patel-Wilson for their              Robots Might Shape Participant Roles Using Gaze Cues. In
assistance, and to reviewers for their helpful comments.               4th ACM/IEEE International Conference on Human Robot
                                                                       Interactions (HRI ’09). La Jolla, California: ACM.
                           References                                Mutlu, B., Yamaoka, F., Kanda, T., Ishiguro, H., & Hagita,
Andrist, S., Pejsa, T., Mutlu, B., & Gleicher, M. (2012). De-          N. (2009). Nonverbal leakage in robots: Communication
   signing Effective Gaze Mechanisms for Virtual Agents. In            of intentions through seemingly unintentional behavior. In
   ACM Annual Conference on Human Factors in Computing                 4th ACM/IEEE International Conference on Human Robot
   Systems (CHI 12). Austin, Texas: ACM Press.                         Interactions (HRI ’09). La Jolla, California: ACM.
Bainbridge, W. A., Hart, J. W., Kim, E. S., & Scassellati, B.        Pierno, A., Becchio, C., Wall, M., Smith, A., Turella, L., &
   (2011). The benefits of interactions with physically present        Castiello, U. (2006). When gaze turns into grasp. Journal
   robots over video-displayed agents. International Journal           of Cognitive Neuroscience, 18(12), 2130–2137.
   of Social Robotics, 3, 41–52.                                     Staudte, M., & Crocker, M. W. (2011). Investigating joint
Boucher, J.-D., Pattacini, U., Lelong, A., Bailly, G., Elisei,         attention mechanisms through spoken human-robot inter-
   F., Fagel, S., et al. (2012). I Reach Faster When I See You         action. Cognition, 120, 268–291.
   Look: Gaze Effects in Human-Human and Human-Robot                 Strabala, K., Lee, M. K., Dragan, A., Forlizzi, J., Srinivasa,
   Face-to-Face Cooperation. Frontiers in Neurorobotics, 6,            S. S., Cakmak, M., et al. (2013). Toward Seamless Human-
   1–11.                                                               Robot Handovers. Journal of Human-Robot Interaction,
Calder, A. J., Lawrence, A. D., Keane, J., Scott, S. K., Owen,         2(1), 112–132.
   A. M., Christoffels, I., et al. (2002). Reading the mind from     Yu, C., Schermerhorn, P., & Scheutz, M. (2012). Adaptive
   eye gaze. Neuropsychologia, 40(8), 1129–1138.                       eye gaze patterns in interactions with human and artificial
Friesen, C. K., Ristic, J., & Kingstone, A. (2004). Attentional        agents. ACM Transactions on Interactive Intelligent Sys-
   effects of counterpredictive gaze and arrow cues. Journal           tems, 1(2).
   of Experimental Psychology: Human Perception and Per-
   formance, 30(2), 319–329.
                                                                 109

