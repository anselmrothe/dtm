UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Valence Weakly Constrains the Information Density of Messages
Permalink
https://escholarship.org/uc/item/4px4c62w
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Vinson, David
Dale, Rick
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Valence Weakly Constrains the Information Density of Messages
                                                                      !
                                              David W. Vinson (dvinson@ucmerced.edu)
                                                    Rick Dale (rdale@ucmerced.edu)
                                                     Cognitive & Information Sciences
                                                     University of California, Merced
                                                   5200 N. Lake Rd., Merced, CA 95343
                                                                      !
                               Abstract                                may help explain why a user chooses a given word. This
                                                                       simplification is justifiable, of course, because of the
  Some recent analyses of language as a transmission medium            difficulty in defining other contextual factors (e.g., at a
  have fruitfully applied information theory in various ways to
                                                                       semantic level), and the complexity that seems endemic to
  sequences of words. In most cases, the information contained
  in a word is defined as a function of that word’s local context      high-level aspects of language (see Jaeger, 2010 for
  (e.g., its probability conditioned on the preceding word). A         discussion).
  central assumption in much of this work is the important role           More recently, studies have begun to show information
  of context. For example, the hypothesis of uniform                   density is influenced by factors at a variety of linguistic
  information density (Jaeger, 2010) requires some notion of           levels including syntactic variation and phonetic reduction
  context in order to be tested. We sought a structured corpus in      (Aylett & Turk, 2004; Jaeger, 2010; Mahowald, Fedorenko,
  order to extend and explore the potential role of a context in       Piantadosi, & Gibson, 2013). Relatedly, the information
  the observed information density of messages. Specifically,          density of a linguistic message may be subject to more
  how might a language user’s affective state influence their          social or cognitive constraints that help define the content of
  language use? We used a database of over one hundred
                                                                       a message, reaching beyond phonological and syntactic
  thousand consumer reviews that includes an assortment of
  user-related variables. These user-related variables, such as        levels. In other words, in abstract terms, the transfer of
  the overall rating of a review used here as a proxy for a user’s     information may be subject to a variety of ubiquitous
  affect, appear to have an interesting relationship to basic          contextual constraints at a variety of levels.
  information-theoretic measures such as the average amount               One such constraint, and one of interest to the current
  and variability of observed information of a review's words.         paper, is the relative valence of a linguistic message (and
  We discuss these results in terms of the broader context that        potentially, the language user herself). If a message is
  may shape the information structure of messages, and relate          intended to be a highly positive evaluation of some
   !
  these findings to existing theories.
  Keywords: language; information theory; context; corpus
                                                                       situation, does the language user seek different patterns of
                                                                       information density to convey it?
  analysis; word distribution; natural language processing                As we review below, there is reason to suspect that such a
                                                                       pervasive contextual variable — like that of intended
                                                                       message valence; one specific to the user while composing a
                           Introduction                                message — may shape the information content of that
Tools from information theory have allowed researchers to              message. Evidence for this would further support the idea
explore whether language use is, in some sense, optimal                that the information-theoretic properties of language are
(e.g., Levy & Jaeger, 2007). At the production level,                  contextually modulated. We sought a corpus well suited to
speakers may structure their utterances so as to optimize              test this idea. We analyzed over 100,000 consumer reviews
information density (Jaeger, 2010), while over longer                  with associated information about a review’s rating. Even
timescales aspects of language such as word length, may be             after accounting for a variety other linguistic variables,
optimized according to information content (Piantadosi,                findings show the valence of a user’s message influences the
Tily, & Gibson, 2011).                                                 amount of information transmitted.          Crucially, specific
  In most cases, factors beyond the lexical level that                 findings depend on how the linguistic context and as a
influence information density must be abstracted away. For             result, information, is quantified.
example, “context” is often confined to a lexical definition,
namely the immediate preceding word. In this case, the
                                                                       Lexical Constraints on Information
information encoded by a word can be expressed using the               Studies that stem from an information-theoretic standpoint
log of the probability that the word would occur in this               have only recently begun to theorize what contextual effects
lexical context:                                                       influence the transfer of information at a lexical level
  !                I(w  i) =    log2 p(wi |wi   1)
                                                                       (Aylett, 1999; Genzel & Charniak, 2002). One such theory,
                                                                       known as Uniform Information Density (UID), states that
                   !
  !
Though easy to compute, this definition abstracts away a
                                                                       language users will structure their utterances so as to
                                                                       optimize information transfer within a given context (Frank
                                                                       & Jaeger, 2008; Levy & Jaeger, 2007). That is, a speaker
variety of other contextual factors such as a user’s cognitive         will communicate at a rate that is optimal for transferring
affective state, message content and intended audience that            the greatest amount of information within a specific (noisy)
                                                                   1682

channel, without loss of information or miscommunication             massive amount of text data where individuals label their
(Genzel & Charniak, 2002). Recent evidence supporting                experiences as positive or negative on a scale of 1-5 and
this notion shows speakers may be sensitive to linguistic            briefly report on them. We speculate that one’s experiential
probability distributions that help define the information           rating provides a measure whereby the influence of one's
density of a message (Fine & Jaeger, 2011; Fine, Qian,               affective valence on information density can be assessed;
Jaeger & Jacobs, 2010).                                              even if only weakly connected.
   In support of optimal information transfer theories Aylett           We hypothesize that when individuals experience a
(1999) found individuals take longer to communicate more             positive affective state, their use of language may be more
information dense messages. In addition, Levy and Jaeger             informative, more lexically rich and differ in frequency of
(2007) found that speakers' use of an optional “that”                use compared to individuals in a more negative affective
complementizer is dependent on the information density of            state. Provided this hypothesis, one’s affective state, may be
their utterance. Further, Piantadosi, Tilly and Gibson (2011)        predicted by their language use; acting as a constraint on the
show word length in general may be optimized to the amount           density of information transferred over the course of a single
of information transferred, in contrast to the well known            message or, in this case, a consumer business review.
Zipf’s law which posits that word length is optimized for the
frequency of word use (Zipf, 1949). Each study shows                                       Current Study
information density optimization occurs in subtly different,
but related ways.                                                    The current study used a dataset from Yelp, Inc. consisting
   Crucially, a word's lexical context stands as the primary         of over 100,000 consumer reviews of businesses throughout
constraint guiding one’s understanding of the amount of              the city of Phoenix AZ. This dataset consists of written
information present within any given message; even though            reviews associated with the reviewer’s explicit feelings
other, higher-level visual and social constraints are known to       specified by a rating from 1 (negative) to 5 (positive) stars
influence language use and comprehension (see Vinson,                about the business reviewed. Each review was subject to
Dale, Tabataebeian & Duran, in press, for review).                   being labeled useful, funny, or cool by other reviewers. For
Importantly, if individuals are sensitive to specific linguistic     the purpose of this study we assume the cognitive-affective
probability distributions, social or cognitive factors               state of an individual is in some way correlated to the
influencing these distributions may affect the density of            number of stars associated with their review; more stars
information within a message.                                        being correlated with a more positive affective state while
                                                                     fewer stars are correlated with a negative affective state.
Affect and Message Valence                                           Because this corpus of reviews consists of both an explicit
                                                                     rating of the business and a linguistic message about the
The information density of a message is at least partially           consumer’s experience, it is highly suitable for testing how
dependent on contextual constraints such as its local lexical        various contextual factors, in particular cognitive-affective
context.      However, lexical contexts may be further               states, influence the information density of a linguistic
influenced by other, more global, constraints.                       message.
   Several findings, especially in social cognition,
recommend this hypothesis. In particular, past research              Measures and Method
suggests cognitive or affective states with more positive
valence are likely to generate more flexible, open-ended             Prior to testing how a reviewer’s cognitive state might
behaviors (Cacioppo & Gardner, 1999; Diener & Diener,                influence the information density of their message, we must
1996; Fredrickson, 2001; Isen & Means, 1983). Consider               define measures of information that seem relevant. This has
an example study that shows this tendency. When primed to            been done in a variety of ways. Here we define information
experience a positive affective feeling, doctors correctly           in four very simple ways, commensurate with classic
diagnose patients faster than doctors not primed to have this        information-theoretic definitions. Each function defines the
experience (Estrada, Isen, & Young, 1997). Doctors were              linguistic context of an utterance slightly differently.
more likely to accept new information when in a positive             Importantly, such differences might reveal a unique
affective state than when in a neutral state. Similarly, it may      relationship to message valence. Listed here are the four
be that when experiencing a positive affective state, one            functions along with a brief definition of each:
might transmit a more information-dense message or one                  (1) Review-internal entropy (RI-Ent). A review may
that is more variable or open, than when in a less positive          simply be structured in distinct ways depending on how a
state provided transmitting and receiving information is             language user decides to use lexical tokens more or less
affected by similar contextual constraints.                          regularly in a way purely internally to a review itself. In
   This notion finds relevance in information-theoretic terms        other words, the frequency distribution over words may
where positive valence may provide an appropriate context            reflect a diverse selection of types (higher entropy), or it
for transferring more or broader information. We speculate           may be relatively more repetitive (lower entropy). This can
                                                                     be expressed in the following way:
further on this relationship below, but one possibility is that
particular affective states might increase the channel                  !
capacity for both sender and recipient. Though a
provocative hypothesis, the corpus we use here provides a
                                                                 1683

                                                                                        direct measure of how informative a word is, then a review
                                                                                        may vary in its informational content depending on the
             5000
                                                                                        language user’s state.
 Frequency
                                                                                          (3) Average conditional information (ACI). A more
             2000                                                                       common way of expressing the information encoded in a
             0
                                                                                        word is relative to some context (i.e., a second-order
                                                                                        estimate). As noted in the introduction, this is commonly
                                   9               10            11             12      taken to be some immediate lexical context. In our case, we
                                                                                        extract a very simple contextual information measure:
                    Very low AUI: “This is a
                                            AUI (bits / word)
                                                        Very high AUI: “I don't
                                                                                             !                                                   N
                                                                                                                                                 X
                                                                                                                                         1
                    great place for lunch and           know if this qualifies as                                          ACIj =                      log2 p(wi |wi   1)
                                                                                                                                     N       1
                    dinner. The food is great,          an update. However 101                                             !
                    the price is good and the
                    service is friendly and
                                                        Bistro is now closed.
                                                        Eighty sixed. Nada here
                                                                                             !
                                                                                        Here the information in a word is the negative log of the
                                                                                                                                                 i=1
                    quick.” [AUI = 7.4]                 anymora. Adios. Hasta
                                                        la pasta.” [AUI = 13.9]         probability of its occurrence given the previous word. This
                                                                                        differs from RI-Ent and AUI in that it accounts for the most
                                                                                        immediate or local context, namely, the previous word.
 Figure 1: The distribution of AUI. Note: we omit .05% of the                             (4) Conditional information variability (CIV). ACI
 data that is farther out on the details (72 of about 124,000                           reflects the average information, but the work of Jaeger
 cases). Example (short) reviews are shown on the relevant                              (2010) and Levy and Jaeger (2007) suggests that the
 side of the distribution. All measures exhibited unimodal,                             uniformity, or variability, of this information measure may
                                                                                        be interesting to explore.
 near-normal distributions, with some observations on distant
 tails (expected given the very large sample).                                               !                                       CIV j = (CIj )
                                                                                                                                     !
                                        N
                                        X                                                    !
                                                                                        Here, CIj is the set of conditional information scores for
                     RI-Entj =                   p(wi |Rj )log2 p(wi |Rj )
                                                                                        each word of the jth review; we compute the standard
                     !
!                                        i=1
Here, RI-Entj denotes the jth review, containing N words, as
                                                                                        deviation of this set. Greater variability in information
                                                                                        density would reflect an increase in the channel capacity.
                                                                                        This would permit more variability in word choice allowing
the probability of the ith word occurring within that review
(for notational convenience we treat it as a conditional
probability, equivalent to restricting computations to a given
review). This measure can be seen as a kind of lexical
richness score, expressed as the expected number of bits
                                                                                                   Review length (words)
                                                                                                                           250
required to encode a message given its unique internal word
distribution. If information density is high, the text can be
said to be lexically rich. Indeed, it can be easily shown that                                                             230
RI-Ent correlates with common measures of lexical
richness, such as type-token frequency. Put simply, a review
with higher entropy will have more unique tokens, thus                                                                     210
being, in a sense, more “information dense.”
   (2) Average unigram information (AUI). This measure is                                                                        1    2           3          4         5
computed from the lexical distribution over the entire set of                                                                                    Stars
Yelp reviews. As noted in the introduction, the information
encoded in a word can be simply seen as the negative log of                                 Figure 2: The number of words in a review (y-axis) by
the probability of its occurrence (the less probable a word,                                star rating (x-axis). Error bars reflect 95% confidence
the more informative). For any given review j:                                              intervals over the whole filtered Yelp dataset.
      !                                        N
                                            1 X
                               AUIj =             log2 p(wi )                                 1  The full Yelp dataset contains about 229,000 reviews. We
                                            N i=1
                               !
      !                                                                                     filtered this dataset by choosing reviews with 100 words or more
                                                                                            so as to increase the reliability of our information measures.
                                                                                               2 Due to the computation required in estimating models from
This differs from the previous measure in that the
probability of a word’s occurrence is defined by a much                                     so much data, we chose simple and multiple regression with lm
larger distribution of words. If we regard the overall                                      in R; we also confirmed general patterns by centering scores
                                                                                            relative to reviewers, and exploring linear mixed-effects models.
distribution of terms in the Yelp corpus as a simple but
                                                                                     1684

differences in the rate of information transmitted (i.e., by                          124,622 Yelp reviews1 were imported and processed in
diminishing range restriction).                                                    Python using json. We used nltk and numpy/scipy
   Measures 1-3 are derived from previous studies that focus                       libraries to carry out most calculations. To calculate RI-Ent
on the probability of a word’s occurrence. A word’s                                we used nltk’s MLE entropy function.
probability is dependent on subtle differences in how its
lexical context is defined. The fourth measure, CIV, is                            Results
novel. According to UID the variability of information                             At least one obvious measure may correlate with star ratings:
density should remain relatively constant across a message.                        review length (in number of words). We first test this variable
CIV measures the variability of information across                                 and then include it as a covariate when testing our key
messages within a specific context. Differences in CIV                             information-theoretic measures.
dependent on messages’ affective context would indicate                               Review length. It is well known that bin count can impact
fluctuation in the context’s channel capacity. This would                          our key information-theoretic measures. In fact, review
support a notion of optimal information transfer that is                           length indeed differed by star rating (see Fig. 2). We used a
dependent on other contextual factors such as cognitive-                           simple linear model to predict review length by stars.2 There
affective states.                                                                  were significantly more words per review for lower stars (r2
   From these measures, reviews can be defined as more or                          = .01, t(124,621) = -38.7, p < .001). This represents a small
less information dense dependent on their general and local                        but significant effect—detectable thanks to the massive
linguistic context. For example, Fig. 1 shows the Average                          power of the large Yelp corpus. We used review length as a
Unigram Information distribution over more than 100,000                            covariate in our subsequent analyses of information-
reviews along with two example reviews. Using simple                               theoretic measures.
measures we tested if information encoded in a message is                             (1) Review-internal entropy (RI-Ent). When not
related to cognitive context: the intended valence of that                         controlling for review length there was a small, but highly
message. To test this, we use star rating to predict                               significant effect of stars in predicting RI-Ent (r2 = .009,
information in regression models: Does variation in valence                        t(124,621) = -34.01, p < .001). Again, this shows a reliable
(rating) predict the level of information encoded?                                 but weak effect; stars account significantly for about 1% of
                                                                                   the variance in RI-Ent (see Fig. 3A).
                                 6.62                                                       9.85
        RI-Ent (bits / review)
                                                                                            9.80
                                                                  AUI (bits / word)
                                 6.58
                                                                                            9.75
                                 6.54
                                        A                                                   9.70   B
                                 6.50
                                        1   2    3      4   5                                      1   2        3          4          5
                                                Stars                                                          Stars
                                 7.05                                                       3.93
        ACI (bits / word pair)                                    CIV (SD of bits / word)
                                 7.00
                                                                                            3.91
                                 6.95
                                 6.90                                                       3.89
                                 6.85
                                        C                                                          D
                                                                                            3.87
                                        1   2    3      4   5                                      1   2        3          4          5
                                                Stars                                                          Stars
  Figure 3: Initial relationships, without additional covariates, between star rating and (A) review-internal entropy (RI-Ent),
  (B) average unigram information (AUI), (C) average conditional information (ACI), and (D) conditional information
  variability (CIV).
                                                                1685

                                                                       and AUI (as an additional covariate), this relationship
                                                                       shrinks in effect, but remains statistically reliable (r2 = .003,
                                    0.02                               t(124,621) = 20.01, p < .001).
 Residual CIV (SD of bits / word)
                                                                          (4) Conditional information variability (CIV). At first
                                    0.00                               blush, CIV also has a nonlinear relationship with stars (Fig.
                                                                       3D). Again, the quadratic term for stars significantly
                                                                       predicts CIV scores (r2 = .004, t(124,621) = 21.35, p < .001),
                                                                       though the effect is even smaller. However, we controlled
                                    -0.04                              for both review length and ACI, since the height of ACI will
                                                                       generally correlate with CIV's range (due to range
                                                                       restriction with true 0). When we do this, the relationship
                                                                       between CIV and stars completely changes (see Fig. 4).
                                                                       Now, greater informational variability appears to be related
                                    -0.08
                                                                       to increased positive valence. This relationship, between the
                                            1   2    3      4   5
                                                                       residual of CIV and star rating, is statistically reliable but
                                                    Stars              again very small (r2 = .009, t(124,621) = 32.86, p < .001).
 Figure 4: After controlling for review length and ACI,                   Other user-related variables. The Yelp dataset also allows
 there is a small but significant relationship between CIV             us to explore information as it relates to the “listener” in this
 and stars.                                                            context. Users who read reviews have the option to rate
                                                                       them as useful, cool, or funny. Exploration with simple
   We controlled for review length by fully residualizing RI-          logistic regression finds that even using these simple,
Ent in the following way: We predicted RI-Ent by review                surface information-theoretic measures provides a boost in
length, and stored the residuals as a new outcome variable             predicting whether a review will be categorized as funny or
for the linear model with stars as the predictor. Residuals            cool (useful is not predicted by information measures,
would therefore reflect unique variance associated with star           surprisingly). Some details are shown in Table 1, detailing
rating in predicting RI-Ent. When doing this, there is no              fully specified models with centered interaction terms for all
longer a significant effect of rating (r2 ≅ 0, t(124,621) =            information-theory values, review length, and comparison
                                                                       models.
-0.43, p = .67). It appears that the variability present in RI-
Ent does not covary with message valence when review                   !
                                                                       Table 1: Basic results of logistic regression when categorizing
length is controlled.
                                                                       reviews along certain “listener” dimensions.
   (2) Average unigram information (AUI). Interestingly,
and unexpectedly, AUI shows a quadratic relationship with                              Full model      Length only     Intercept only
stars (see also Hu, Pavlou & Zhang, 2006 for similar
findings). This is plainly seen in Fig. 3B. To model this, we              cool?     57.9% (168117)   55.8% (170419)   52.7% (172411)
converted stars into a quadratic term ([1,2,3,4,5] =
[4,1,0,1,4]). When not controlling for review length the raw               useful?   68.5% (152015)   68.5% (152976)   68.5% (155378)
analysis revealed a small but highly significant effect of
stars in predicting AUI (r2 = .010, t(124,621) = 36.24, p < .001),         funny?    63.9% (159643)   62.4% (163005)   61.4% (166277)
such that information density of a review increased as rating
levels became more extreme.                                            Note: Categorization uses a 0.5 threshold in GLM predictions
                                                                       using family=binomial(“logit”). AIC shown in parentheses.
   When taking out review length, and running the                      All full models have lower AIC, though performance difference is
regression with residuals, this effect remained (r2 = .011,            small.
t(124,621) = 36.90, p < .001), suggesting it is highly
independent of review length. Again, though a weak effect,
there is a relationship between the average single-word                                     General Discussion
information of reviews and star ratings. It appears that               Variance in information density is partially, if only weakly,
positive valence is not predictive of overall information;             captured by a review’s star rating. Though we obtain very
rather, the extremity of the valence predicts slightly more
loading of reviews with greater information density (i.e.,             small effects overall, we would argue that these remain
equivalently, lower frequency terms).                                  theoretically intriguing. For example, we find a curious and
                                                                       unpredicted quadratic relationship between average lexical
   (3) Average conditiona l information (ACI). Interestingly,
                                                                       information and review rating. This suggests participants
ACI also showed a nonlinear relationship with stars, shown
                                                                       may be choosing lower frequent terms–greater lexical
in Fig. 3C. With star rating predicting ACI alone, there is a
                                                                       richness–when composing reviews at the extremes of the
significant quadratic relationship (r2 = .013, t(124,621) =
                                                                       scale (in contrast to our hypothesis that positive reviews,
39.89, p < .001). The same pattern appears to hold: Extreme
                                                                       specifically, would be of greater lexical richness).
reviews seem to generate more information in bigrams
                                                                          Overall, the variability in information density is at least
patterns, though this seems to be more pronounced in the
                                                                       partially accounted for by contextual influences beyond the
negative reviews. When residualizing ACI by review length
                                                                       linguistic level. If star rating is an indication of a reviewer’s
                                                                    1686

affective valence, then the cognitive state of a reviewer may       Estrada, C. A., Isen, A. M., & Young, M. J. (1997). Positive
stand as one contextual factor that can account for changes           affect facilitates integration of information and decreases
in the information density expressed in a message. This               anchoring in reasoning among physicians. Organizational
supports previous findings that show contextual factors               Behavior and Human Decision Processes, 72(1), 117–
affect the rate of information transfer (Jaeger, 2010; Genzel         135.
& Charniak, 2002).         In light of current and previous         Ferrer-i-Cancho, R., Dębowski, Ł., & Martín, F. M. D. P.
findings, speakers may be sensitive to a variety of linguistic        (2013). Constant conditional entropy and related
probability distributions well suited to convey messages              hypotheses. arXiv preprint arXiv:1304.7359.
under any variety of constraints. Perhaps it should be no           Fine, A. B., & Jaeger, T. F. (2011). Language
surprise that the content of a message expressing intense joy         comprehension is sensitive to changes in the reliability of
is information-theoretically different, slightly, from one of         lexical cues. In Proceedings of the 33rd Annual Meeting
mediocrity.                                                           of the Cognitive Science Society (pp. 925–930).
   An underlying principle of Uniform Information Density           Fine, A. B., Qian, T., Jaeger, T. F., & Jacobs, R. A. (2010). Is
suggests language users preference a uniform distribution of          there syntactic adaptation in language comprehension? In
information across a message. If the variability of                   Proceedings of the 2010 workshop on cognitive modeling
information across a message increases, information transfer          and computational linguistics (pp. 18–26). Association for
would be less uniform. When controlling for ACI and the               Computational Linguistics.
word length of a message the variability of information             Frank, A., & Jaeger, T. F. (2008). Speaking rationally:
across messages increased as star rating increased. This              Uniform information density as an optimal strategy for
suggests the optimal rate of information transfer across              language production. In Proceedings of the 30th Annual
messages is dependent on context; in this case messages’              Meeting of the Cognitive Science Society.
affective valence. Positive affective states may result in          Fredrickson, B. L. (2001). The role of positive emotions in
more open ended and flexible behaviors (Cacioppo &                    positive psychology: The broaden-and-build theory of
Gardner, 1999; Diener & Diener, 1996; Fredrickson, 2001;              positive emotions. American Psychologist, 56(3), 218.
Isen & Means, 1983) possibly moderating the optimal rate            Genzel, D., & Charniak, E. (2002). Entropy rate constancy
of information transfer within a specific context. This opens         in text. In Proceedings of the 40th annual meeting on
up the possibility that what is considered optimal may be             association for computational linguistics (pp. 199–206).
subject to a variety of higher-level constraints (see also,         Hu, N., Pavlou, P. A., & Zhang, J. (2006) Can online
Ferrer-i-Cancho, Debowski & Moscoso del Prado Martin,                 reviews reveal a product’s true quality? Empirical
2013 and Mahowald et al., 2013 for a more recent debate               findings and analytical modeling of Online word-of-
over the use of constant entropy rate measures in describing          mouth communication. In Proceedings of the 7th ACM
language use). To be sure, the optimal rate of information            Conference on Electronic Commerce, 324–330.
transfer over some context may be more or less uniform; the         Isen, A. M., & Means, B. (1983). The influence of positive
variability of information expanding and contracting                  affect on decision-making strategy. Social Cognition,
depending on one’s current affective state or intended                2(1), 18–31.
message valence.                                                    Jaeger, T. F. (2010). Redundancy and reduction: Speakers
   In summary, speakers may be sensitive to the rate of               manage syntactic information density. Cognitive
information in sequencing their message; adjusting their              Psychology, 61(1), 23–62.
message according to a particular rate of information               Levy, R. & Jaeger, T. F. (2007). Speakers optimize
transfer (Jaeger, 2010; Fine & Jaeger, 2011). Our results are         information density through syntactive reduction. In B.
commensurate, in a way, with this intuition. The information          Schlokopf, J. Platt & T. Hoofman (Eds.), Advances in
density of a message, and the variability of that density are         neural information processing systems (NIPS) 19,
sensitive, at least weakly, to the message’s affective valence.       849-856. Cambridge, MA:MIT Press.
                                                                    Mahowald, K., Fedorenko, E., Piantadosi, S. T., & Gibson,
                          References                                  E. (2013). Info/information theory: Speakers choose
                                                                      shorter words in predictive contexts. Cognition, 126(2)
Aylett, M. P. (1999). Stochastic suprasegmentals:                     313-318
   Relationships between redundancy, prosodic structure and         Piantadosi, S. T., Tily, H., & Gibson, E. (2011). Word
   syllabic duration. Proceedings of ICPhS–99, San                    lengths are optimized for efficient communication.
   Francisco.                                                         Proceedings of the National Academy of Sciences, 108(9),
Aylett, M., & Turk, A. (2004). The smooth signal                      3526–3529.
   redundancy hypothesis: A functional explanation for              Vinson, D. W., Dale, R., Tabatabaeian, M, & Duran, N.D.
   relationships between redundancy, prosodic prominence,             (in press). Seeing and believing: Social influences on
   and duration in spontaneous speech. Language and                   language processing. In Mishra, R. K., Srinivasan, N., &
   Speech, 47(1), 31–56.                                              Huettig, F. (Eds.) Attention and Vision in Language
Cacioppo, J. T., & Gardner, W. L. (1999). Emotion. Annual             Processing. Springer.
   Review of Psychology, 50(1), 191–214.
                                                                    Zipf, G. K. (1949). Human behavior and the principle of
Diener, E., & Diener, C. (1996). Most people are happy.               least effort.
   Psychological Science, 7(3), 181–185.
                                                                1687

