UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Dependencies and Hierarchical Structure in Sentence Processing
Permalink
https://escholarship.org/uc/item/6wp8798q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Author
Baumann, Peter
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                 Dependencies and Hierarchical Structure in Sentence Processing
                                         Peter Baumann (baumann@u.northwestern.edu)
                                                          Department of Linguistics
                                                Northwestern University, Evanston, IL, USA
                               Abstract                                 1997), i.e. the length of the path in the phrase-structure tree
                                                                        to ‘travel’ from one element to the other.
   In memory-based models of human sentence processing it is
   assumed that the completion of a dependency between a syn-              In this paper, we evaluate these three measures of distance
   tactic head and its dependents is a major source of processing       between linguistic dependents. We will assume that the num-
   difficulty in non-ambiguous sentences, and that this integration     ber of words intervening between two dependent elements is
   cost is a function of the distance between the two elements.
   However, it remains open how to measure the distance between         a good approximation of the time of memory decay so that
   two dependent elements. While many current models employ             the three distance measures can be summarizes as follows:
   a linear distance measure, we instead propose to measure the         • L INEAR D ISTANCE: number of words between two depen-
   distance between a head and its dependents as the path in the
   phrase structure tree connecting the two elements. We evalu-            dents
   ate this structural distance as a measure of dependency inte-        • DLT D ISTANCE: number of discourse referents1 between
   gration and show that it is a better predictor of human reading         two dependents
   times than other measures. Moreover, we find that evaluated on
   reading data from naturally occurring texts, dependency inte-        • S TRUCTURAL D ISTANCE: number of syntactic nodes
   gration is not actually a cost, as higher dependency integration        crossed in the syntactic phrase-structure tree between two
   distances led to lower reading times.                                   dependents
   Keywords: sentence processing; syntax; reading; eye-                    Following a recent trend in psycholinguistics (e.g. Pynte,
   tracking; modeling; memory
                                                                        New, & Kennedy, 2008; Demberg & Keller, 2008), we do not
                                                                        focus on specific constructions or sentence types to evaluate
                           Introduction                                 the three distance measures, but instead employ a regression
How do we understand language and what are the cogni-                   analysis on an ‘eye-tracking corpus’, i.e. eye-tracking data
tive mechanisms reflected in measures of human sentence                 of people reading naturally occurring texts, to determine if
comprehension? Many attempts to answer this big ques-                   dependency integration cost, when determined by one of the
tion can be subsumed under two broad categories: memory-                three distance measures is a significant predictor of reading
based models and experience-based models. One particular                times.
aspect, in which these two modeling approaches differ, is                  We will show that only structural distance between two de-
the role they assign to hierarchical syntactic structure: while         pendent elements is a significant predictor of reading times,
experience-based models of syntactic expectations are of-               and that this effect can only be found on verbs, but not on
ten built on phrase-structure grammars modeling hierarchical            nouns. More importantly, our analyses show that a higher dis-
sentence structure, memory-based models tend to be agnos-               tance between two dependent elements leads to lower reading
tic about it, as they are usually formulated in terms of non-           times. This result is the exact opposite of what most memory-
hierarchical dependency structures.                                     based models predict, but it is not unprecedented: in a self-
   Within the latter group of models it is a common assump-             paced reading experiment conducted in German, Konieczny
tion that the completion of a dependency between a syntactic            (2000) observed that verbs were read faster when the number
head and its dependents (such as e.g. a verb and its argument)          of intervening words between them and their arguments was
is a major source of processing difficulty in non-ambiguous             higher. Similar anti-locality effects have since been observed
sentences (Gibson, 1998), and that this integration cost is –           in controlled experiments in Japanese (Nakatani & Gibson,
at least in part – a function of the distance between the two           2008) and Hindi (Vasishth & Lewis, 2006). And in an analy-
elements.                                                               sis very similar to ours, Demberg and Keller (2008) found a
   However, specific models differ with regard to the ques-             similar negative effect of DLT distance, i.e. higher distance
tion of how to measure the distance between two dependent               led to lower reading times.
linguistic units: some models emphasize the role of work-                  Our results can be interpreted in different ways: one may
ing memory decay and thus assume the distance to be mea-                argue that that dependency integration can lead to integra-
sured in time (Wanner & Maratsos, 1978; King & Just, 1991),             tion costs, but that dependencies may also help to predict and
others, like the Dependency Locality Theory (DLT; Gib-                  thus facilitate the upcoming head, and that the latter process
son, 1998, 2000) emphasize the general capacity limitations             is more common than the former in everyday language com-
of working memory and measure the distance in terms of                  prehension. A somewhat weaker conclusion is that current
intervening discourse referents, which are supposed to oc-              memory-based models, such as DLT, may be too narrow in
cupy memory slots. A third possible alternative, which has
not been implemented in memory-based models, is struc-                      1 We follow (Gibson, 1998) and assume that nouns and verbs in-
tural distance within the phrase structure (see e.g. O’Grady,           troduce discourse referents.
                                                                    152

scope, in that they can only account for effects that arise from              in terms of linear distance or structural distance: for linear
a very limited set of dependencies.                                           distance, one can see that in an SRC (1a) the relative pronoun
                                                                              who and the verb are adjacent, while in an ORC (1b) they
   Memory-based Models and Relative Clauses                                   are separated by the intervening subject. The predictions of
In sentence processing research it is well-established that                   structural distance are illustrated in the phrase-structure trees
object-extracted relative clauses (ORC) are harder to process                 in Figure 1: for the SRC (top) the nodes crossed between the
than comparable subject-extracted relative clauses (SRC), not                 verb and the head noun the journalist are VP, S’ and NP, while
only in English (King & Just, 1991), but in may other lan-                    for ORC (bottom) there is an extra S node, which is crossed
guages.                                                                       in addition to VP, S’ and NP. This yields a greater structural
   Accounting for this processing difference has been one of                  distance for the ORC than for the SRC.
the major motivations for memory-based models (King &                                                       NP
Just, 1991; Gibson, 1998, 2000). One particular memory-
based processing model, the Dependency Locality Theory
                                                                                                  NP                  S’
(DLT; Gibson, 1998, 2000) explains SRC/ORC processing
difference in terms of an integration cost occurring at the                                  the journalist
                                                                                                             who             VP
heads of linguistic dependencies: one component of the in-
tegration cost is a distance-based cost, which is monotonely                                                        attacked        NP
increasing in the number of discourse referents intervening
between the head and its dependents, e.g. a verb and its argu-                                                                  the senator
ments.2                                                                                                   NP
   In the case of English relative clauses (1), integration cost
of DLT makes the right predictions for processing differences                                 NP                   S’
on the embedded verb attacked.
                                                                                         the journalist
(1) a. The journalisti [ whoi attacked the senator ] is famous.                                           who                S
    b. The journalisti [ whoi the senator attacked ] is famous.
                                                                                                                    NP               VP
In an SRC (1a), the embedded verb needs to integrate its pre-                                                                  attacked
                                                                                                                the senator
ceding subject, the relative pronoun who, which is co-indexed
with the noun phrase the journalist3 . Since no discourse ref-
erent occurs between the subject and the verb, there is no in-                Figure 1: Phrase-structure tree of English relative clauses:
tegration cost on the verb in SRCs.                                           SRC (top) and ORC (bottom)
   In an ORC (1b), on the other hand, two integrations need
to take place at the embedded verb attacked: the embedded
                                                                              Korean Relative Clauses
subject the senator needs to be integrated with the verb. Since
no discourse referent occurs between subject and verb, there                  While DLT makes the correct predictions for relative clauses
is no cost associated with this integration. Second, the object               in English, this is not the case for all other languages. One
the journalist (or the co-indexed relative pronoun who) needs                 particularly interesting case is Korean, a head-final language
to be integrated with the verb. This integration incurs a cost                with prenominal relative clauses. Despite being significantly
of 1, as there is the discourse referent the senator intervening              different from English, all available empirical evidence from
between the object and the verb. So overall, there is an inte-                self-paced reading and eye-tracking experiments indicates
gration cost of 1 on the embedded verb in an ORC, which is                    that in Korean, like in English, ORCs are more difficult
higher than the integration cost of 0 in an SRC.                              to process than SRCs (Kwon, Polinsky, & Kluender, 2006;
   A qualitatively equivalent prediction is obtained when in-                 Kwon, Lee, Gordon, Kluender, & Polinsky, 2010).
tegration cost is not measured in terms of DLT distance, but                  (2) a. [ uywon-ul        kongkyekha-n ] enlonin-i           ...
    2 In DLT, there is a second component to integration cost for the                [ senator-ACC attack-ADN ] journalist-NOM . . .
introduction of discourse referents, but since this component makes                  ‘The journalist who attacked the senator is famous’
the same contribution in all cases considered here and we are mainly              b. [ uywon-i          kongkyekha-n ] enlonin-i            ...
interested in the contribution of distance-based costs, we will ignore               [ senator-NOM attack-ADN ] journalist-NOM . . .
this component.
    3 In this presentation, we adopt a ‘gap-free’ or direct association              ‘The journalist who attacked the senator is famous’
based processing model (e.g. Pickering & Barry, 1991), in which
extracted elements, such as relative or interrogative pronouns, di-           However, integration costs derived from linear and DLT dis-
rectly associate with their subcategorizer, i.e. the verb, without posit-     tance predict that there should be no processing difference
ing any ‘gaps’. This choice was made purely for the purpose of an             between Korean SRCs and ORCs (2). In both the SRC (2a)
accessible presentation. All claims hold (and could be made even
stronger, as in the case of Korean below) when assuming a more                and the ORC (2b), only one integration occurs when process-
traditional filler-gap processing mechanism.                                  ing the embedded verb kongkyekha-n: in an SRC, it is the
                                                                          153

embedded object uywon-ul that can be integrated and in an            Corpus (Kennedy & Pynte, 2004) to evaluate computational
ORC the embedded subject uywon-i. Crucially, in both cases           models of sentence processing. In particular, they compared
there is no material intervening between the embedded verb           the experience-based model of surprisal (Hale, 2001) and the
and the integrated element and so the integration costs are the      memory-based integration cost of Dependency Locality The-
same for SRCs and ORCs.                                              ory (Gibson, 1998, 2000). For surprisal, the authors found
   Integration cost measured in terms of structural distance,        that it is a good predictor of human reading times in natural-
on the other hand, predicts the observed pattern, as illustrated     istic texts. For integration cost, on the other hand, the situ-
in Figure 2: like in English, the nodes crossed between the          ation was a bit more complicated: when evaluated over the
verb and the head noun the journalist in an SRC (top) are            whole data set, integration cost was a statistically significant
VP, S’ and NP, while for the ORC (bottom) there is an extra          predictor of reading times (Demberg & Keller, 2008, Table 1,
S node, which is crossed in addition to VP, S’ and NP. This          p. 199). However, the direction of the effect of integration
yields the correct prediction that ORCs should be harder to          cost was the opposite of what was predicted by DLT: a higher
process than SRCs.                                                   distance between two dependent elements led to lower read-
                                                                     ing times. While Demberg and Keller (2008) interpret their
                                NP                                   findings as a failure of DLT to serve as a “broad-coverage the-
                                                                     ory of syntactic complexity”, their finding of a negative effect
                                                                     of integration cost on reading times (i.e. higher integration
                 S’                              NP                  costs lead to lower reading times) has since been replicated
                                                                     on the same data set, but with different parsers (Baumann,
                                        the journalist-NOM           2012; Schuler & van Schijndel, 2014).
                        VP
                                                                                                Analyses
                 NP          attacked
                                                                     In the following two statistical analyses we evaluate the rel-
          the senator-ACC                                            ative importance of dependency integration based on three
                              NP                                     measures of distance as predictors of reading times in an on
                                                                     ‘eye-tracking corpus’, i.e. eye-tracking data of people read-
                                                                     ing naturally occurring texts (cf. Pynte et al., 2008; Dem-
              S’                              NP                     berg & Keller, 2008). In Analysis 1, we evaluate the distance
                                                                     measures across syntactic heads of all word classes, while
                                      the journalist-NOM             in Analysis 2, we differentiate between nominal and verbal
                        S
                                                                     heads, as some memory-based models predict dependency in-
                                                                     tegration only for verbal heads (e.g. Gibson, 2000).
              NP                VP
                          attacked
                                                                     Data and Dependent Variable
       the senator-NOM
                                                                     We used the English portion of the Dundee Corpus (Kennedy
                                                                     & Pynte, 2004), an eye-tracking corpus based on texts from
Figure 2: Phrase-structure tree of Korean relative clauses:
                                                                     the British newspaper The Independent. The corpus consists
SRC (top) and ORC (bottom)
                                                                     of 51,502 word tokens from 20 texts, which were read by
                                                                     ten English native speakers, while their eye-movements were
Anti-Locality Effects                                                recorded using a Dr. Boise eye-tracker. Following common
                                                                     practice (e.g. Pynte et al., 2008; Demberg & Keller, 2008)
Another challenge for DLT and memory-based models in                 we preprocessed the data and removed cases (i.e., word-
general, comes from anti-locality effects, which were first re-      participant pairs), in which the word was not fixated, was
ported from German by Konieczny (2000) and Konieczny and             presented as the first or last on a line, was attached to punctu-
Döring (2003): in self-paced reading and eye-tracking exper-        ation, contained more than one capital letter (likely to be an
iments, Konieczny observed that – contrary to the predictions        acronym) or any non-alphabetic symbol.
of DLT – verbs were read faster when the number of inter-               Since we are interested in how to measure distance in de-
vening words between them and their arguments was higher.            termining the cost of dependency integration on syntactic
Similar effects have since been observed in controlled ex-           heads, we restricted our data set to syntactic heads, which
periments in Japanese (Nakatani & Gibson, 2008) and Hindi            are preceded by at least one dependents. In all other cases,
(Vasishth & Lewis, 2006).                                            i.e. on words, which are no syntactic heads, or heads which
                                                                     precede their dependents, memory-based models of sentence
                       Related Work                                  processing do not predict any dependency integration cost.
In recent years, it has become standard to evaluate compu-           While such a restriction of the data set was not performed by
tational models of language processing on ‘eye-tracking cor-         Demberg and Keller (2008), it seems mandatory from a sta-
pora’: Demberg and Keller (2008) used the Dundee Reading             tistical perspective, as a high number of data points with an
                                                                 154

irrelevant value in the predictor of interest can have adverse           In addition to including these control variables as single
effects on the fit of a linear regression model.                      predictors, we also included all binary interaction terms be-
   The sequence of fixations obtained from eye-tracking ex-           tween them, which improved the model fit in a log-likelihood
periments can be analyzed by calculating a range of eye-              test. The full set of predictors used in all models is listed in
tracking measures. We chose first-pass reading times as our           Table 1.
dependent variable, as it is supposed to be indicative of early
syntactic processing. The first-pass reading time on a given          Measures of Dependency Distances
word is the sum of all eye fixations on that word in the first        The three measures of dependency distance were calculated
pass reading, i.e. before leaving the word either to the right        based on dependency relations and syntactic tree structures
or to the left.                                                       obtained from parsing the Dundee Corpus with the Stanford
                                                                      Parser (de Marneffe & Manning, 2006; Klein & Manning,
Procedure                                                             2003). For each dependency relation, the distance between a
The three measures of distance and several other well-                head and its dependent was calculated as follows:
established control variables, which are known to have an             • L INEAR D ISTANCE: number of words between dependent
influence on reading times, were calculated for each word                and head
and annotated to the reading time data of the Dundee corpus.          • DLT D ISTANCE: number of nouns and verbs between de-
On this data set we performed regression analyses using lin-             pendent and head
ear mixed-effects regressions with PARTICIPANT, WORD and              • S TRUCTURAL D ISTANCE: number of non-terminal nodes
TEXT NUMBER as random effects, as a generalization of the                crossed when traversing the syntactic tree structure from
common by-subject and by-item analyses. All models were                  dependent to head
fit in R using the lme4 package (Bates, Mächler, Bolker, &           Since the dependency distances are claimed to cause integra-
Walker, 2013).                                                        tion costs at the heads of a dependency (Gibson, 1998, 2000),
   Since our goal is to determine the relative importance of          we annotated all syntactic heads with their respective depen-
the three different measures of distance, we first fitted a base-     dency distances. If a head had more than one dependency,
line model with all control variables to the first-pass reading       its distance measure is the sum of distances of the individual
times in our data set. We then calculated three new regres-           dependencies.
sion models for our three measures of distance, which in-
cluded all baseline predictors and one of our three measures.                                   Analysis 1
These three models were then compared to the baseline model           In our first analysis, we evaluate the three distance measures
through a log-likelihood test, which follows a χ2 -distribution.      across syntactic heads of all word classes by fitting one model
If one of the models was a significantly better fit to the read-      for each distance measure and comparing it to the baseline
ing time data (as determined by the log-likelihood test), we          model via log-likelihood tests.
conclude that the predictor added to the model is a significant
predictor of reading times (cf. Gelman & Hill, 2007).                 Results
                                                                      The model comparisons via log-likelihood tests showed that
Control Variables                                                     integration cost as measured by S TRUCTURAL D ISTANCE is
All regression models included the following control vari-            a significant predictor of readings times, as adding it to the
ables, which are known to have an influence on reading times          regression model significantly improved the model fit over
(cf. Demberg & Keller, 2008): the number of characters per            the baseline (χ2 = 10.49, p < .01). Integration cost based on
word (WORD LENGTH), the position of word in a sentence                the other two distance measures, however, turned out not to
(WORD POSITION), an indicator variable whether there was              be significant predictors of reading times, as neither of them
no fixation on the previous (PREV NOFIX) the next (NEXT               led to a significantly better model fit (L INEAR D ISTANCE:
NOFIX ) word, the frequency of the word ( WORD FREQ ), the            χ2 = 0.76, n.s.; DLT D ISTANCE: χ2 = 1.70, n.s.).
frequency of the previous word (FREQ PREV WORD), the for-                The coefficients and standard errors of the model with
ward transitional probability (FORW TRANS PROB), i.e. the             structural distance are listed in Table 1. It can be seen that
bigram probability P(wi |wi−1 ), and the backward transitional        S TRUCTURAL D ISTANCE is negatively correlated with read-
probability (BACKW TRANS PROB), i.e. the bigram probabil-             ing times (β = −3.17, t = −3.24), i.e. higher dependency dis-
ity P(wi |wi+1 ).                                                     tances lead to lower reading times.
   All frequencies and transitional probabilities were obtained
by fitting a unigram or bigram model with modified Kneser-            Discussion
Ney smoothing (Chen & Goodman, 1998) to the British Na-               Our results show that dependency integration as measured by
tional Corpus (100 million words) using the SRILM toolkit             structural distance is a significant predictor of reading times,
(Stolcke, 2002). All continuous variables were centered and           while the other two distance measures do not make depen-
scaled to two standard deviations to minimize collinearity. In        dency integration a significant predictor of reading times.
addition, all frequencies and transitional probabilities were         This result is in contrast to the one obtained by Demberg and
log-transformed before scaling.                                       Keller (2008), who found DLT’s dependency integration cost
                                                                  155

                                                                       Data
Table 1: Coefficients and standard errors of the regression
model for structural distance                                          We constructed two subsets of the data set used in Analysis 1:
                                                                       one subset containing only syntactic heads that are verbs and
  Predictor                                   Coef.    Std.Error       one subset containing only heads that are nouns. The decision
  (I NTERCEPT )                             213.36          8.45       of the whether a word is a noun or a verb was based on the
  WORD LENGTH                                 39.74         1.36       part-of-speech tag of each word obtained from the Stanford
  FREQ PREV WORD                            -28.82          1.36       parser.
  PREV NOFIX                                  28.17         1.41
                                                                       Results Nouns
  NEXT NOFIX                                  10.15         1.24
  WORD POSITION                               -6.23         0.92       For the subset of nominal heads, none of the three distance
  FORW TRANS PROB                           -11.33          1.78       measures made dependency integration a significant predictor
  WORD FREQ                                   -9.20         1.66       of reading times (S TRUCTURAL D ISTANCE: χ2 = 2.06, n.s.;
  BACKW TRANS PROB                            -1.97         1.19       L INEAR D ISTANCE: χ2 = 0.0001, n.s.; DLT D ISTANCE:
  WORD LENGTH      : WORD FREQ              -28.89          2.55       χ2 = 0.70, n.s.).
  FREQ PREV WORD      : PREV NOFIX            16.22         1.93       Results Verbs
  NEXT NOFIX    : PREV NOFIX                   9.37         1.72
                                                                       For the subset of verbal heads, the results are very sim-
  WORD FREQ : FREQ PREV WORD                   8.31         1.94
                                                                       ilar to the full data set: the model comparisons via log-
  WORD POSITION : FREQ PREV WORD              -6.47         1.79
                                                                       likelihood tests showed that dependency integration as mea-
  FORW TRANS PROB : NEXT NOFIX                -6.17         1.74
                                                                       sured by S TRUCTURAL D ISTANCE is a significant predictor
  WORD LENGTH : BACKW TRANS PROB               6.60         2.38
                                                                       of readings times on verbs (χ2 = 7.62, p < .01), while depen-
  WORD POSITION : WORD FREQ                   -4.39         2.00
                                                                       dency integration based on the other two distance measures
  WORD POSITION : WORD LENGTH                 -5.02         2.11
                                                                       turned out not to be significant predictors of reading times on
  S TRUCTURAL D ISTANCE                       -3.17         0.98
                                                                       verbs (L INEAR D ISTANCE: χ2 = 0.01, n.s.; DLT D ISTANCE:
                                                                       χ2 = 1.57, n.s.). The effect of S TRUCTURAL D ISTANCE on
                                                                       reading times was again negative (β = −4.03, t = −2.76), i.e.
to be a significant predictor of reading times in the Dundee           higher dependency distances lead to lower reading times.
Corpus. One explanation for the two differing results may be
that we restricted our data set to include only syntactic heads        Discussion
with non-zero distance values, i.e. syntactic heads preceded           The results of Analysis 2 are a refinement of the results of
by at least one of their dependents, and did not include the ex-       Analysis 1, as they show that only for verbs dependency inte-
tra component of DLT’s integration cost for the introduction           gration as measured by structural distance is a significant pre-
of the head in our calculations. In addition, we obtained our          dictor of reading times, while for nouns no such effect could
dependency parses of the Dundee Corpus from the Stanford               be found. While our results are in line with the assumptions
Parser, which outputs both phrase-structure and dependency             of some memory-based models (e.g. Gibson, 2000), which
parses, while Demberg and Keller used a dedicated depen-               predict dependency integration only for verbal heads, the di-
dency parser.                                                          rection is again the exact opposite of what these models pre-
   More importantly, however, like Demberg and Keller                  dict. However, the fact that the results on verbs were similar
(2008) we also obtained a negative effect of dependency inte-          to the ones on the whole data set, is in line with the (psy-
gration on reading times, i.e. i.e. higher dependency distances        cho)linguistic intuition that dependency integration should
lead to lower reading times. This results is the exact opposite        mainly affect verbs.
of what most memory-based models, in particular DLT and
its integration cost, predict, but given that the direction of the
                                                                                               Conclusion
effect is stable across different evaluations based on different       Dependency integration is a central component of many
parsers, we are willing to accept that dependency integration          memory-based models of sentence processing and it is gener-
is negatively correlated with reading times of natural texts.          ally assumed that the length of a dependency, i.e. the distance
                                                                       between a dependent and its head, influences processing of
                           Analysis 2                                  the head. We showed that measuring the length of a depen-
                                                                       dency in terms of the structural distance between a dependent
Our first analysis has shown that dependency integration is            and its head makes dependency integration a significant pre-
a significant predictor of reading times but only when mea-            dictor of reading times.
sured by structural distance. Since some memory-based mod-                More importantly, however, our analyses confirmed that
els predict dependency integration only for verbal heads (e.g.         a higher distance between two dependent elements leads to
Gibson, 2000), we now we differentiate between nominal and             lower reading times (cf. Demberg & Keller, 2008; Baumann,
verbal heads and repeat the previous analysis for nouns and            2012; Schuler & van Schijndel, 2014), which is the exact op-
verbs separately.                                                      posite of what most memory-based models predict.
                                                                   156

   Given the ample experimental evidence in support of               North American Chapter of the Asssociation for Computa-
memory-based models, our results can be interpreted in dif-          tional Linguistics (NAACL). Pittsburgh, PA.
ferent ways: one may simply argue that current memory-             Kennedy, A., & Pynte, J. (2004). Parafoveal-on-foveal effects
based models, such as DLT, are too narrow in scope, in that          in normal reading. Vision Reserach, 45, 153–168.
they can only account for effects that arise from a very lim-      King, J., & Just, M. A. (1991). Individual differences in
ited set of dependencies, such as e.g. dependencies resulting        syntactic processing: the role of working memory. Journal
from extractions (or movement) like in relative clauses.             of Memory and Language, 30(5), 580-602.
   Taking a more positive stance, one may assume that de-          Klein, D., & Manning, C. D. (2003). Accurate unlexicalized
pendency integration can lead to integration costs, but that         parsing. In Proceedings of the 41st Annual Meeting of the
dependencies may also help to predict and thus facilitate the        Association for Computational Linguistics (ACL ’03) (pp.
upcoming head. Under this assumption, our results simply             423–430). Sapporo, Japan.
state that the latter process is more common than the former       Konieczny, L. (2000). Locality and parsing complexity. Jour-
in everyday language comprehension. Since predictions of             nal of Psycholinguistic Research, 29(6), 627–645.
upcoming words is the main mechanism in experience-based           Konieczny, L., & Döring, P. (2003). Anticipation of clause-
models (e.g. Hale, 2001), this assumption may may point a            final heads: Evidence from eye-tracking and SRNs. In Pro-
way towards a model of sentence processing that integrates           ceedings of ICCS/ASCS Joint International Conference on
aspects of memory-based and experience-based models. We              Cognitive Science. Sydney, Australia.
leave this task for future research.                               Kwon, N., Lee, Y., Gordon, P., Kluender, R., & Polinsky,
                                                                     M. (2010). Cognitive and linguistic factors affecting
                     Acknowledgments                                 subject/object asymmetry: An eye-tracking study of pre-
The author thanks Masaya Yoshida and Brady Clark for their           nominal relative clauses in Korean. Language, 89, 546–
support, and the audience of the 25th Annual CUNY Con-               582.
ference on Human Sentence Processing, in particular Lars           Kwon, N., Polinsky, M., & Kluender, R. (2006). Subject pref-
Konieczny, for valuable feedback and critical questions dur-         erence in Korean. In D. Baumer, D. Montero, & M. Scanlon
ing a lively Q&A session.                                            (Eds.), Proceedings of the 25th West Coast Conference on
                                                                     Formal Linguistics (WCCFL 25) (pp. 1–14). Somerville:
                          References                                 Cascadilla Press.
Bates, D., Mächler, M., Bolker, B., & Walker, S. (2013).          Nakatani, K., & Gibson, E. (2008). Distinguishing theories
   lme4: Linear mixed-effects models using Eigen and S4.             of syntactic expectation cost in sentence comprehension:
Baumann, P. (2012). The role of hierarchical structure in syn-       Evidence from Japanese. Linguistics, 46(1), 63–86.
   tactic dependency integration. Paper presented at the 25th      O’Grady, W. (1997). Syntactic Development. Chicago: Uni-
   Annual CUNY Conference on Human Sentence Processing.              versity of Chicago Press.
   CUNY, New York, NY.                                             Pickering, M., & Barry, G. (1991). Sentence processing with-
Chen, S. F., & Goodman, J. (1998). An empirical study of             out empty categories. Language and Cognitive Processes,
   smoothing techniques for language modeling (Tech. Rep.).          6(3), 229–259.
   Center for Research in Computing Technology, Harvard            Pynte, J., New, B., & Kennedy, A. (2008). A multiple re-
   University.                                                       gression analysis of syntactic and semantic influences in
de Marneffe, B. M., Marie-Catherine, & Manning, C. D.                reading normal text. Journal of Eye Movement Research,
   (2006). Generating typed dependency parses from phrase            2(4), 1–11.
   structure parses. In Proceedings of the Fifth International     Schuler, W., & van Schijndel, M. (2014). Effects of integra-
   Conference on Language Resources and Evaluation (LREC             tion in eye tracking. Poster presented at the 27th Annual
   2006). Genova, Italy.                                             CUNY Conference on Human Sentence Processing. Ohio
Demberg, V., & Keller, F. (2008). Data from eye-tracking cor-        State University, Columbus, OH.
   pora as evidence for theories of syntactic processing com-      Stolcke, A. (2002). SRILM - an extensible language mod-
   plexity. Cognition, 109(2), 193–210.                              eling toolkit. In Proceedings of the Seventh International
Gelman, A., & Hill, J. (2007). Data Analysis Using Re-               Conference on Spoken Language Processing (ICSLP ’02).
   gression and Multilevel/Hierarchical Models. Cambridge:           Denver, CO.
   Cambridge University Press.                                     Vasishth, S., & Lewis, R. L. (2006). Argument-head distance
Gibson, E. (1998). Linguistic complexity: Locality of syn-           and processing complexity: Explaining both locality and
   tactic dependencies. Cognition, 68, 1–76.                         antilocality effects. Language, 767–794.
Gibson, E. (2000). The Dependency Locality Theory:                 Wanner, E., & Maratsos, M. (1978). An ATN approach to
   a distance-based theory of linguistic complexity.        In       comprehension. In M. Halle, J. Bresnan, & G. A. Miller
   A. Marantz, Y. Miyashita, & W. O’Neil (Eds.), Image, Lan-         (Eds.), Linguistic Theory and Psychological Reality. The
   guage, Brain. Cambridge: MIT Press.                               MIT Press.
Hale, J. (2001). A probabilistic Earley parser as a psycholin-
   guistic model. In Proceedings of the Second Meeting of the
                                                               157

