UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Harvesting Motion Patterns in Still Images from the Internet
Permalink
https://escholarship.org/uc/item/2kn7k1h0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Wu, Jiajun
Wang, Yining
Li, Zhulin
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                    Harvesting Motion Patterns in Still Images from the Internet
                                                Jiajun Wu (jiajunwu.cs@gmail.com)
                                            Yining Wang (ynwang.yining@gmail.com)
                                             Zhulin Li (li-zl12@mails.tsinghua.edu.cn)
                   ITCS, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084
                                                     Zhuowen Tu (ztu@ucsd.edu)
                       Department of Cognitive Science, University of California, San Diego, La Jolla, 92093
                              Abstract                                  can be divided into four categories: states, activities, achieve-
   Most vision research on motion analysis focuses on learning          ments, and accomplishments, based on their telicity and con-
   human actions from video clips. In this paper, we investigate        tinuity (Rothstein, 2004). After we introduce these linguis-
   the use of still images, rather than videos, for motion recog-       tic categories, we then discuss the relationship between our
   nition. We present evidence from both human cognition and
   computer vision that still images do indeed contain a wealth         notion of M-verbs and the linguistic categorization of verbs
   of information about motion patterns. Our contributions are          before drawing further conclusions on the linkage between
   three-fold. First, we automatically determine classes of mo-         M-verbs and continuity of verbs. As shown in Figure 2, these
   tions that can effectively be characterized by still images. To
   make this determination we introduce the notions of motion           findings guide us to a novel setting bridging verbs in linguis-
   verbs (M-verbs) and motion phrases (M-phrases); these refer          tics and motions in computer vision.
   to linguistic concepts motivated by visual cognition and are not        With a foundation of M-verbs and M-phrases, we then
   restricted only to motions performed by humans. Second, we
   build UCSD-1024, a large dataset distilled from more than two        build the large UCSD-1024 dataset from more than two mil-
   million still images. These images come from 1,024 categories        lion images across 1,024 categories. As our first step to
   of motion; we use crowdsourcing to provide human validation          UCSD-1024, we develop a semi-supervised knowledge ex-
   of the motion categories. Third, we exploit motion patterns
   from UCSD-1024 using a weakly-supervised learning strategy           pansion framework to determine precisely which phrases (se-
   and demonstrate performance competitive with state-of-the-art        lected from a large corpus and a small number of labeled
   computer vision action classification methods.                       seeds) correspond to actions that can be effectively conveyed
   Keywords: motion pattern discovery; image with implied mo-           by still images. Combining this semi-supervised output with
   tion; visual perception
                                                                        corroboration from Amazon Mechanical Turk, we construct
                          Introduction                                  a dictionary of 1,024 M-phrases. We subsequently use this
Action recognition has long been a topic of interest in vision          dictionary, in concert with the Google and Bing image search
research. In addition to traditional computer vision meth-              engines, to build UCSD-1024.
ods that aim to learn effective action models from videos                  Learning mid-level representations is a popular topic in
(Sadanand & Corso, 2012; Soomro, Zamir, & Shah, 2012),                  computer vision (Lim, Zitnick, & Dollár, 2013; Q. Li, Wu,
much recent research has investigated the use of still im-              & Tu, 2013). Here we learn a dictionary for motions using a
ages (Delaitre, Laptev, & Sivic, 2010; Khan et al., 2013).              hierarchical model based on mid-level representations on an
While certainly inspiring, we note that these recent still-             eighty motion subset of UCSD-1024 that we henceforth re-
image-based approaches ignore two basic questions under-                fer to as UCSD-80. We then perform action classification on
lying the use of still images in action recognition: Given that         Stanford 40 (Yao et al., 2011) and obtain encouraging results.
actions are intrinsically dynamic processes, are still images
rich enough for action recognition of any kind? If so, what                                     Related Work
types of actions can be captured by still images?                       Most existing action recognition research in computer vision
   In this paper, we demonstrate that still images are indeed           is based on video clips (Sadanand & Corso, 2012; Soomro et
rich enough for use in motion understanding; in the process,            al., 2012). Recently, researchers have pursued action recog-
we also characterize those motions that are recognizable even           nition in still images (Delaitre et al., 2010; Khan et al., 2013).
in still images. This is an interdisciplinary topic at the in-          Delaitre et al. (2010) performed action recognition in still im-
tersection of cognitive science, computer vision, natural lan-          ages using a combination of bag-of-feature methods and part-
guage processing, and linguistics. We first discuss findings            based representations, and built a dataset of seven categories
in cognitive science that provide theoretical support of our            and 968 Flickr images. Yao and Fei-Fei (2010) proposed
claim of still image richness. We then use the machine learn-           Grouplet, a structural representation for interactions between
ing strategy of multiple instance learning (MIL) to further             humans and objects, and the PPMI dataset of seven types of
demonstrate the expressiveness of still images by learning              activities. However, none of these explicitly address the un-
from still frames in videos. We propose motion verbs (M-                derlying questions for action recognition: how expressive are
verbs) and motion phrases (M-phrases) for the class of verbs            still images and what types of actions can still images effec-
and phrases describing motions that can be effectively con-             tively convey. Finally, the largest existing dataset is Stanford
veyed by still images. In linguistics, it is known that verbs           40 (Yao et al., 2011), comprised of 9,532 images in 40 cate-
                                                                    1790

gories. By comparison, the proposed UCSD-1024 dataset is
distilled from roughly two million images across 1,024 cate-                                           Player          Subject
gories. Moreover, UCSD-1024 is not restricted only to mo-
tions performed by humans.
   In linguistics, verb categorization dates back to Aristo-                                          Kicking         M-verb-ex
tle’s trichotomy (Taylor, 1977); see the representative works
(Tenny, 1987; Rothstein, 2004) for further discussion. More
                                                                                                         Ball          Object
recently, (Taylor, 1977) linked continuity and tense and
(Fleck, 1996) discussed the spatial and temporal properties
of verbs topologically.                                                                              In the grass      Adverbial
   The task of expanding M-phrases is related to the thesaurus
extraction task insofar as both aim to create a list of terms.                Central Component               Optional Component
The use of online texts for thesaurus extraction was first in-
vestigated in (Jannink, 1999). Curran and Moens (2002) pro-                    Figure 1: An illustration of M-phrases
posed an automatic thesaurus extraction algorithm based on                                               Stages    Telic
syntactic structures and word distributions of online texts.                             States             -        -
The C-value/NC-value method proposed in (Frantzi, Anani-                              Activities            +        -
                                                                                    Achievements            -       +
adou, & Mima, 2000) uses both syntactic features and statisti-                    Accomplishments           +       +
cal measures for phrase extraction. Our task differs from the-
                                                                     Table 1: Types of verbs from linguistics (Rothstein, 2004)
saurus extraction in that the definition of M-phrases involves
both semantic and visual understanding of the phrases, mak-        A Demonstration with Multi-Instance Learning
ing the problem much harder.                                       Here we show that still images contain rich information about
                                                                   motion patterns. We apply multiple instance learning (MIL)
                 Motions in Still Images                           (Andrews, Tsochantaridis, & Hofmann, 2002) to video cat-
                                                                   egories to discover the most relevant frame and then learn
In this section, we discuss motion recognition in still images     models for the corresponding action in that frame using video
from a cognitive science perspective. We then use a standard       sequences as bags and frames as instances.
machine learning method, multiple instance learning (MIL),            Specifically, we first randomly select seven categories
to demonstrate the potential effectiveness of still images in      from HMDB-51 (Kuehne, Jhuang, Garrote, Poggio, & Serre,
motion recognition.                                                2011); within each category we then randomly select five
                                                                   video sequences. We next compute GIST (Oliva & Torralba,
A Cognitive Science View                                           2006) for a number of frames. Treating each video sequence
                                                                   as a positive bag and a collection of irrelevant images as a
Still images with implied motion have long been of interest        negative bag, we learn one instance-level classifier for each
in psychology and cognitive science. Freyd (1983) showed           category using mi-SVM (Andrews et al., 2002). We sub-
that visual stimuli in which motion is only implied, such          sequently sample a few more video sequences in each cate-
as frozen motion photographs, could nonetheless prompt the         gory and perform video classification on them using average-
brain to rapidly and automatically extrapolate motion paths.       voting on images with the learned classifiers. The classifica-
Kourtzi and Kanwisher (2000) subsequently demonstrated             tion accuracy is 64.7%, providing strong evidence that still
that viewing static images with implied motion could prompt        images can effectively convey motions.
activity in medial temporal/medial superior temporal cor-
tex (MT/MST). Proverbio, Riva, and Zani (2009) showed                                   Verbs at a Glance
that such observations could also enhance the activity of          In this section, we pursue a new approach: in order to char-
movement-related brain areas. Thus, these demonstrations           acterize motions/verbs that can be effectively captured in still
provide strong neurological evidence that motion can be un-        images, we must first introduce a novel partition of verbs. We
derstood even in still images.                                     provide notions meant to refer to those verbs or phrases that
   Recent work from Boroditsky’s group (Winawer, Huk, &            describe motions that can be effectively conveyed by still im-
Boroditsky, 2010, 2008; Dils & Boroditsky, 2010) provides a        ages. In particular, these still-image-conveyable motion verbs
more thorough theoretical cognitive foundation. Winawer et         and phrases are called M-verbs and M-phrases, respectively.
al. (2010) connects visual imagery of motion with perceptual
                                                                   M-verbs and M-phrases
motion, (Winawer et al., 2008) relates still images of actions
with human cognition, and (Dils & Boroditsky, 2010) links          We define motion verbs (M-verbs) as verbs that can be ef-
visual motion understanding with motion language. These            fectively conveyed by still images. We roughly categorize
studies inspire us to integrate cognition and linguistics into     M-verbs into three classes:
vision applications.                                               • Simple verbs: run, laugh, swim,
                                                               1791

       0.8
                                                                                       Simple Present
       0.6
                                                                                       Present Continuous         4% 3%     Composition of M-verbs
       0.4
       0.2
                                                                                       Simple Past              14%                    Activities
                                                                                       Past Continuous                                 Accomplishments
         0
                                                                                       Present Perfect                 79%             States
           shaving surf smoke   walk   jog  weightlift play  sled shoot   hold
            beard                                      water      arrow  umbrella      Past Perfect                                    Achievements
                                                       polo
(a) Percentage of images labeled as positive ones by all three workers with respect to ten categories (b) The composition of M-verbs with
and six tenses. Note that present continuous tense is almost consistently the highest.                     respect to four verbal categories.
                           Figure 2: M-verbs and M-phrases with respect to tenses and verbal categories.
• Compound verbs: cliff diving, ice skating,                               states, activities, achievements, and accomplishments. To en-
• Special verbal phrases: push up, pull up.                                sure accuracy, we further ask three individuals to indepen-
                                                                           dently label each verb; any verbs for which these labels dis-
   Noting that there are verbs that cannot be associated with              agree are given to an expert linguist for a final label. We see
a visual impression in the absence of sufficient contextual in-            from Figure 2b that most M-verbs are either activities or ac-
formation, e.g. closing v.s. closing eyes, we define extended              complishments. This, together with the linguistic knowledge
motion verbs (M-verbs-ex) as a superset of M-verbs contain-                from Table 1, indicates that continuity (stage) plays a key role
ing all verbs that could potentially convey visual motion in an            in the composition of M-verbs. In this sense, M-verbs con-
appropriate context. Based on M-verbs-ex, we propose mo-                   nect concepts in linguistics and vision.
tion phrases (M-phrases), which extend M-verbs by incorpo-                 Topology, Continuity, and Tense
rating subjects, objects, and adverbials. As shown in Figure 1,
                                                                           As mentioned previously, there is a strong tie between con-
an M-phrase contains an M-verb-ex as its central component
                                                                           tinuity and motion verbs. In linguistics, boundaries of space
with subject, object, and adverbial as optional components.
                                                                           and time of verbs can be modeled from a view in topology
   M-phrases are both flexible and expressive. The use of sub-
                                                                           (Fleck, 1996), which is also closely related to the use of dif-
jects and objects makes them capable of describing possible
                                                                           ferent tenses with respect to the continuity of verbs.
motions in still images. Note that here we do not require each
                                                                                 We are thus motivated to explore how the use of different
image to contain an entire human body. For instance, M-
                                                                           tenses might help our construction of M-verbs and M-phrases
phrases like dog running, horse galloping, clapping hands,
                                                                           in UCSD-1024. We employ Amazon Mechanical Turk for
and scowling describe motions that could be accomplished
                                                                           corroboration. Specifically, we select ten M-verbs and six
with non-human creatures or parts of human bodies.
                                                                           tenses. For each phrase as a combination of an M-verb and
M-verbs from a Linguistic Point of View                                    a tense, we crawl 1,000 images from Google image search
                                                                           using that phrase as query. We then ask workers to classify
Aristotle’s trichotomy classified verbs into three categories:             whether each crawled image is relevant for that query.
state-verbs, energeia-verbs, and kinesis-verbs. Later linguists                  The result is shown in Figure 2a. We see that present con-
(Taylor, 1977; Tenny, 1987; Fleck, 1996) further developed                 tinuous tense yields the highest intra-annotator agreement in
the partition and it is now commonly agreed that a verb or                 nine of the ten categories. These empirical statistics (and the
verbal constituent belongs to one of four categories:                      linguistic analysis considered earlier) lead us to use present
• States: is (hirsute), love (a school),                                   continuous tense in all M-verbs and M-phrases.
• Activities: swim, talk,                                                                             Building UCSD-1024
• Achievements: discover (America), pass (an exam),                        We now introduce UCSD-1024. We aim to exploit the rich
                                                                           knowledge in still images by building the largest still im-
• Accomplishments: build (a house), stab (Caesar).                         age motion database with M-phrases. UCSD-1024 is distilled
These four categories are also associated with two crucial as-             from over two million images across 1,024 categories.
pectual properties: whether the verbs in question can appear               Collecting Seeds
in progressive forms (Stages) and whether they occur with                  We first collect M-phrases to serve as seeds for dictionary ex-
movement towards an endpoint (Telic) (Rothstein, 2004).                    pansion. We collect 101 M-phrases from UCF101 (Soomro et
   M-verbs and M-phrases are sets of verbal constituents de-               al., 2012), 40 M-phrases from the Stanford 40 Action Dataset
scribing motions that can be effectively conveyed by still im-             (Yao et al., 2011), 90 M-phrases for common sports, and
ages. This focus thus brings us to a novel setting for which               11 M-phrases for common facial expressions. After remov-
existing theories in linguistics and vision fail to fit as closely         ing duplicates and merging M-phrases with similar meanings,
as necessary. Here we would like to explore some possible                  these sources contribute 237 M-phrases.
connections between motions in vision and verbs in linguis-                      We obtain additional seeds by exploiting the inherent struc-
tics that will help us better fit our focus.                               ture of M-phrases. We employ 13 common subjects, 78 M-
   We select 100 M-verbs from 287 seeds (introduced in the                 verbs, 15 objects, and 12 adverbials to generate ca. 200, 000
next section). We then manually classify these verbs into                  M-phrase candidates. We then determine the popularity of
                                                                       1792

                                                                         0.4
                                                                         0.3
                                                                         0.2
                                                                         0.1
                                                                           0
                                                                              0   1  2   3   4  5      0   1  2   3  4   5
                                                                                       Baseline   NLP framework
                                                                   Figure 4: Percentage of images with different ratings, anno-
                                                                   tated by two specialists
                                                                   of the verb. Only the head nouns of the arguments will ap-
        Figure 3: Framework for M-phrases expansion                pear in the final M-phrases and some post-processing steps
these candidates using search engines; by manually select-         such as changing all verbs into their -ing forms were carried
ing 50 M-phrases from the 200 most popular, we arrive at           out to make the M-phrases suitable for image collection.
237 + 50 = 287 seed M-phrases.                                     Quantitative evaluation: We evaluate the quality of our M-
                                                                   phrase expansion algorithm by rating the images crawled.
Expanding M-phrases
                                                                   Specifically, we randomly crawled 500 images using M-
From these 287 seeds, we expand our dictionary of M-phrases        phrases we expanded and another 500 images using phrases
via a three-step framework shown in Figure 3. We first use         generated by a baseline approach: randomly combining seed
seed verbs (M-verbs included in our seed dictionary, such          nouns and seed verbs. We then ask annotators to rate the
as throw, ride) to crawl sentences from the Internet. We           1,000 images without revealing to them which method we
then extract syntactic and semantic features based on crawled      used for a particular image. The ratings are shown in Fig-
sentences and apply supervised classification to determine         ure 4, where images score “5” when most relevant to motions
whether a sentence contains M-phrases. Finally, after clas-        and score “0” when least relevant. Our M-phrase expansion
sifying the sentences, we design a rule-based extractor to ex-     results in more consistent images, thereby validating our M-
tract M-phrases from each containing sentence.                     phrase expansion algorithm.
Data source: We crawl sentences from Collocation Explorer,
a system that automatically detects collocations from the          Enriching M-phrases Using Crowdsourcing
British National Corpus. The primary advantage of Collo-           Both the British National Corpus and WordNet are “closed
cation Explorer is its ability to return sentences containing      universes” — they do not include every valid M-phrase. To
user-specified verbs. By using our seed M-verbs, we obtain         open the “closed universe”, we recruited Amazon Mechanical
sentences of higher quality than randomly picking sentences        Turk (AMT) workers to provide additional phrases. We asked
from a corpus.                                                     each user to provide 10 phrases that could be effectively con-
   In addition, Collocation Explorer allows advanced search        veyed by still images. Each answer was required to be five
patterns, further contributing to the precision of our system.     words or fewer and to not make a complete sentence. To en-
We use our seeds to crawl 45,140 sentences.                        sure diversity, we limited each user to at most 50 phrases.
Syntactic and semantic features: We design both syntactic          We manually rewrote 70 phrases most suitable to our task as
and semantic features for our unsupervised learning frame-         M-phrases and added them to our dictionary.
work. Features used include syntactic categories and head-            From the combined output of our expansion framework
noun Part-Of-Speech (POS) tags.                                    and our AMT task, we selected 1,024 M-phrases. We then
   To address polysemy, we use word sense disambiguationto         built the UCSD-1024 image dataset with the help of Inter-
assign each word in a sentence a corresponding “synset” in         net image search engines and crowdsourcing. We first used
WordNet (Fellbaum, 2010) that represents the meaning of the        Google and Bing to crawl 1,000 images for each M-phrase.
word. We then use bag-of-word features on the synset defini-       When submitting queries, we restricted the results to be pho-
tions to separate words with different meanings.                   tos only. We then removed all broken links, any images
Supervised classification and M-phrases extraction: We             smaller than 100×100, and duplicates. For each category,
used support vector machines (SVMs) to classify each sen-          we retained 1,000∼1,900 images for further processing.
tence as either “containing” or “not containing” each M-              Images provided by search engines are diverse, but also
phrase type. We randomly picked 1,024 sentences as train-          noisy. We used AMT to recruit workers to assist in cleaning
ing data and manually labeled them. We determine the SVM           up the data. For this task, we created a large number of hits,
parameters via five-fold cross-validation.                         each of which contained 40 Internet images crawled with one
   Finally, we extract M-phrases from each containing sen-         M-phrase. Workers were asked to decide whether each of
tence. We being by anchoring the key M-verb in each sen-           the images was relevant to the M-phrase query, with each hit
tence, and then using the Enju parser to locate the arguments      assigned to three different workers.
                                                               1793

              M-verb                           S + M-verb-ex                                  M-verb-ex + O / Ad                S + M-verb-ex + O / Ad
     crawling        throwing   whistle blowing             child running         raising hands           applying eye makeup       wind blowing leaf
     marching      applauding    water flowing              man smoking       pushing against wall          applying lipstick     boat drifting on water
     brushing          diving     leaf swirling            fish swimming          delivering ball           blowing dry hair     fish swimming in tank
      pushing         walking      cat running                 kid skiing         brushing hair             blowing bubbles   feather drifting past window
      cycling         smiling     dog barking             woman smoking          cooking dinner             blowing candles          dog licking hand
      jogging         dancing      man sailing             baby crawling            lifting box               brushing teeth        bird clapping wing
     archering        dunking  military marching             band playing         climbing rock                cutting trees         face being angry
      bowling        drinking      car running               baby wailing          closing eyes             raising eyebrows       face being disgusted
       boxing          fishing   child clinging              child writing           fixing car                 fixing bike        face being surprised
     kayaking         bathing      dog baying                 dog eating       playing badminton             playing football     dentist cleaning tooth
     coughing       decanting   fish swimming                girl dancing         playing guitar               playing cello     people crowding street
     dabbling       harvesting    girl walking                girl pouting    ascending mountain             assembling car      parent protecting child
     refueling       spinning     man leaping                 man sitting      bonding with child             brushing wall       pitcher delivering ball
      spitting     telephoning potato sprouting             train derailing       cheering child            conditioning hair   squirrel leaping from tree
    undressing         yelling    tree swaying            water bubbling       cleaning fingernail            cleaning stove      smoke rising from fire
      dancing        crawling    water pouring              woman biting        disciplining child            drinking soda        spider spinning web
                                           Table 2: A subset of M-verbs and M-phrases we employ
                                                       Figure 5: A subset of the UCSD-1024
   We vet submission quality as follows: among the three sub-                    repeat the experiment ten times for each category. The results
mitted labels for an image, we regard one of them as question-                   show that the percentage of correct submissions varies from
able if it differs from the other two. For each hit, if over 50%                 63.0% to 96.3% for different categories. Figure 6 demon-
of its 40 submitted labels are questionable, we consider it a                    strates the twenty M-phrases with highest or lowest agree-
bad hit. For each worker, if his bad hits compose over 30%                       ment and provides examples. The high percentage shows the
of all his submissions, we reject all his submissions, block                     intra-category agreement of images obtained from the Inter-
him from further participation, and reassign other workers to                    net and AMT.
finish his hits. For other workers, we reject only “bad” sub-                         Apart from intra-category consistency, we also consider the
missions.                                                                        intra- and inter-annotator agreement of UCSD-1024.
   The final submissions are highly consistent. Quantitatively,                       Intra-annotator agreement (InAA) measures the consis-
30.1% of the images are labeled as positive by at least two of                   tency of annotation by the same annotator. We invited 20
the three workers, and 46.2% are labeled as positive by all                      annotators to annotate the same set of images twice, with an
three workers. For higher accuracy, we retain only images                        one-month gap in between, and investigate the consistency
labeled as positive by all three labelers. Figure 5 shows a                      of the labels. Inter-annotator agreement (ItAA) can be con-
subset of UCSD-1024 after quality control.                                       veniently measured via AMT, where each image was labeled
                                                                                 by three different workers. Although we discarded all im-
             Human Validation of UCSD-1024                                       ages with inconsistent labels, the percentage of consistently
In this section, we discuss the data consistency of UCSD-                        labeled images still provides useful information on the con-
1024 and verify that UCSD-1024 keeps both intra-category                         sistency of UCSD-1024. Figure 6b shows the intra- and inter-
consistency and annotator agreement.                                             annotator agreement for several motion categories.
   For intra-category consistency, we conduct another AMT
experiment: We randomly select two images from one cate-                                    Learning Hierarchical Action Models
gory, and eight images from the others (so that no two of the                    To demonstrate the richness of UCSD-1024, we further de-
eight are from the same category). Without revealing the la-                     velop a hierarchical model for action classification. For ex-
bels we then ask each user to pick out two images that they                      perimental use, we employ UCSD-80, a subset of UCSD-
think belong to the same category from the ten images. We                        1024, for more efficiency at the cost of some expressiveness.
                                                                            1794

    90
    85                                                                                                 100
    80                                                                                                  90
    75                                                                                                  80
    70                                                                                                  70
    65                                                                                                  60
                                                                                                        50
    60                                                                                                  40
    55                                                                                                  30
    50                                                                                                  20
                                                                                                        10
                    paddling
               pizza tossing                                 rowing boat
                                                         using computer
               mixing batter                                 walking dog
                playing flute
                racewalking
              rope climbing
                                                                     scuba
                                                            playing piano
                                                              ice dancing                                0
             mopping floor
            playing croquet                                       jumping
                                                           wakeboarding
               weight lifting                                cutting trees
                                                               snorkeling
         playing racquetball
              feeding horse                                      wrestling
                     smiling                                    sky diving
                                                             windsurfing
                     sculling
                    phoning                                       sledding
                                                           playing soccer
                    hurdling
          playing basketball                                        sailing
                                                       cutting vegetables
                 waterskiing
                                                                                                             Inter-annotator   Intra-annotator
             doing aerobics                                 playing guitar
                                                               swimming
                  drumming
              rock climbing                                         rafting
                (a) 20 categories with lowest and highest intra-category consistency                 (b) Intra-/inter-annotator agreement
Figure 6: From (a) we can see that for categories with relatively low consistency, the images within each category are still
highly consistent. The low score is largely due to human variance in classifying images between similar classes.
            Method              Accuracy         # Features             Freyd, J. J. (1983). The mental representation of movement when
                                                                          static stimuli are viewed. Perception & Psychophysics, 33(6),
         HOG+LBP                  23.4%         2400×21                   575–581.
       Visual Concepts            27.5%          716×21                 Jannink, J. (1999). Thesaurus entry extraction from an online dic-
        Motion (ours)             29.6%          80×21                    tionary. Proceedings of Fusion.
   Motion+VC+HOG (ours)           33.1%      80×21+716+2400             Khan, F. S., Anwer, R. M., van de Weijer, J., Bagdanov, A. D.,
          Object Bank             32.5%              —                    Lopez, A. M., & Felsberg, M. (2013). Coloring action recognition
                                                                          in still images. International Journal of Computer Vision, 1–17.
                                                                        Kourtzi, Z., & Kanwisher, N. (2000). Activation in human mt/mst
Table 3: Comparison of feature accuracy and length for dif-               by static images with implied motion. Journal of Cognitive Neu-
ferent action classification approaches applied to Stanford 40.           roscience, 12(1), 48–55.
                                                                        Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011).
Following (Q. Li et al., 2013), we learn 716 mid-level clas-              Hmdb: a large video database for human motion recognition.
                                                                          IEEE International Conference on Computer Vision.
sifiers using Visual Concepts (Q. Li et al., 2013). We then             Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags of
train one-vs-all SVMs for the 80 categories on the response               features: Spatial pyramid matching for recognizing natural scene
of the mid-level classifiers, resulting in 80 layered models for          categories. IEEE Conference on Computer Vision and Pattern
                                                                          Recognition, 2, 2169–2178.
motions.                                                                Li, L.-J., Su, H., Fei-Fei, L., & Xing, E. P. (2010). Object bank: A
   We test these models in action classification with Spatial             high-level image representation for scene classification & seman-
Pyramid Matching (Lazebnik, Schmid, & Ponce, 2006) on                     tic feature sparsification. Advances in Neural Information Pro-
                                                                          cessing Systems.
Stanford 40 (Yao et al., 2011); the number of features are              Li, Q., Wu, J., & Tu, Z. (2013). Harvesting mid-level visual concepts
therefore multiplied by 1 + 2 × 2 + 4 × 4 = 21. Table 3 shows             from large-scale internet images. IEEE Conference on Computer
that our action model achieves better results with a much                 Vision and Pattern Recognition.
                                                                        Lim, J. J., Zitnick, C. L., & Dollár, P. (2013). Sketch tokens: A
smaller size of dictionary than Visual Concepts (Q. Li et al.,            learned mid-level representation for contour and object detection.
2013). Combining features from lower layers, our method,                  IEEE Conference on Computer Vision and Pattern Recognition.
with only very light supervision in M-phrases expansion, out-           Oliva, A., & Torralba, A. (2006). Building the gist of a scene: The
                                                                          role of global image features in recognition. Progress in brain
performs Object Bank (L.-J. Li, Su, Fei-Fei, & Xing, 2010)                research, 155, 23–36.
which requires fully supervised bounding boxes in training.             Proverbio, A. M., Riva, F., & Zani, A. (2009). Observation of static
                                                                          pictures of dynamic actions enhances the activity of movement-
                     Acknowledgments                                      related brain areas. PLoS One, 4(5), e5389.
                                                                        Rothstein, S. (2004). Verb classes and aspectual classification.
This work is supported by NSF IIS-1216528 (IIS-1360566)                   Structuring Events: A Study in the Semantics of Lexical Aspect,
and NSF CAREER award IIS-0844566 (IIS-1360568). We                        1–35.
thank Patrick Gallagher for his helpful suggestions.                    Sadanand, S., & Corso, J. J. (2012). Action bank: A high-level
                                                                          representation of activity in video. IEEE Conference on Computer
                                                                          Vision and Pattern Recognition.
                          References                                    Soomro, K., Zamir, A. R., & Shah, M. (2012). Ucf101: A dataset of
Andrews, S., Tsochantaridis, I., & Hofmann, T. (2002). Support            101 human actions classes from videos in the wild. CRCV-Tech
  vector machines for multiple-instance learning. Advances in Neu-        Report-12-01.
  ral Information Processing Systems.                                   Taylor, B. (1977). Tense and continuity. Linguistics and philosophy,
Curran, J., & Moens, M. (2002). Improvements in automatic the-            1(2), 199–220.
  saurus extraction. ACL workshop on Unsupervised lexical acqui-        Tenny, C. L. (1987). Grammaticalizing aspect and affectedness.
  sition.                                                                 Ph.D. Thesis.
Delaitre, V., Laptev, I., & Sivic, J. (2010). Recognizing human         Winawer, J., Huk, A. C., & Boroditsky, L. (2008). A motion af-
  actions in still images: a study of bag-of-features and part-based      tereffect from still photographs depicting motion. Psychological
  representations. British Machine Vision Conference.                     Science, 19(3), 276–283.
Dils, A. T., & Boroditsky, L. (2010). Visual motion aftereffect         Winawer, J., Huk, A. C., & Boroditsky, L. (2010). A motion afteref-
  from understanding motion language. Proceedings of the Na-              fect from visual imagery of motion. Cognition, 114(2), 276–284.
  tional Academy of Sciences, 107(37), 16396–16400.                     Yao, B., & Fei-Fei, L. (2010). Grouplet: A structured image rep-
Fellbaum, C. (2010). Wordnet. Springer Netherlands.                       resentation for recognizing human and object interactions. IEEE
Fleck, M. M. (1996). The topology of boundaries. Artificial Intelli-      Conference on Computer Vision and Pattern Recognition.
  gence, 80(1), 1–26.                                                   Yao, B., Jiang, X., Khosla, A., Lin, A. L., Guibas, L., & Fei-Fei,
Frantzi, K., Ananiadou, S., & Mima, H. (2000). Automatic recogni-         L. (2011). Human action recognition by learning bases of action
  tion of multi-word terms: the c-value/nc-value method. Interna-         attributes and parts. IEEE International Conference on Computer
  tional Journal on Digital Libraries, 3(2), 115–130.                     Vision.
                                                                   1795

