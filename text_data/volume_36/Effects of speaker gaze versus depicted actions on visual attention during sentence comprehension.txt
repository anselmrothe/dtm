UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Effects of speaker gaze versus depicted actions on visual attention during sentence
comprehension
Permalink
https://escholarship.org/uc/item/2dt2c1k6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Kreysa, Helene
Knoeferle, Pia
Nunneman, Eva M.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

                Effects of speaker gaze versus depicted actions on visual attention
                                            during sentence comprehension
                                          Helene Kreysa (helene.kreysa@uni-jena.de)
                        Department of General Psychology, Friedrich Schiller University of Jena, Germany
                                       Pia Knoeferle (knoeferl@cit-ec.uni-bielefeld.de)
                                         Eva M. Nunnemann (eva@nunnemann.com)
                                         Cognitive Interaction Technology Excellence Cluster
                                                         Department of Linguistics
                                    Bielefeld University, Inspiration I, 33615, Bielefeld, Germany
                             Abstract                                   Abashidze, Knoeferle, & Carminati, 2013). In that context,
                                                                        Abashidze and colleagues have argued that recently-seen
  An eye-tracking study compared the effects of actions
  (depicted as tools between on-screen characters) with those of        actions may be prioritized because they can be verified and
  a speaker’s gaze and head shift between the same two                  have actually happened, while the absence of a visible
  characters. In previous research, each of these cues has              action leads to uncertainty as to whether that action is going
  rapidly influenced language comprehension on its own, but             to be performed (even if such a “future” action occurs
  few studies have directly compared these two cues or, more            overall very frequently in the experimental context). On the
  generally, distinct non-linguistic cues in their effects on real-     other hand, our everyday conversations do include
  time sentence comprehension. We investigated how                      communication about absent actions and events; even
  participants used action tools and speaker gaze separately and        concrete verbs do not invariantly reference actions and their
  in combination for visually anticipating the upcoming
                                                                        associated patients in the immediate environment.
  mention of a sentence referent. We discuss implications for
  accounts of visually situated language comprehension.                 Especially when compared to other contextual cues, the
                                                                        precise importance of visible and/or depicted actions for
  Keywords: eye tracking, spoken language comprehension,                situated language comprehension thus remains an open
  speaker gaze, depicted actions                                        issue.
                                                                           Consider for instance, a speaker’s eye gaze as another
                         Introduction                                   contextual cue for comprehenders. We know that when
  Recent years have seen numerous studies on how                        speakers talk about nearby objects, they tend to inspect them
individual contextual cues affect the unfolding interpretation          just before mentioning them (e.g., Bock, Irwin, Davidson, &
of spoken sentences (for recent reviews see Altmann, 2011;              Levelt, 2003; Griffin, 2004; Griffin & Bock, 2000;
Huettig, Rommers, & Meyer, 2011; for theoretical accounts               Kuchinsky, Bock, & Irwin, 2011; Meyer & Lethaus, 2004;
and computational models see Altmann & Kamide, 2009;                    Meyer, Roelofs & Levelt, 2003). This close link between
Crocker, Knoeferle & Mayberry, 2010; Mayberry, Crocker,                 speech-related eye movements and reference has important
& Knoeferle, 2009; Knoeferle & Crocker, 2006, 2007).                    implications for language comprehension. Crucially, a
Among these cues are sentence-based ones such as case                   speaker's gaze can help listeners to visually anticipate the
marking, verb meaning or temporal adverbs, but also                     next-mentioned referent (Hanna & Brennan, 2008;
extralinguistic cues such as contrast between objects, or               Knoeferle & Kreysa, 2012; MacDonald & Tatler, 2013;
information from action depictions. For instance, when a                Staudte & Crocker, 2011).
verb refers to an action, participants rapidly integrate the               In summary, numerous studies have shown that individual
action (and its associated thematic role relations between              cues – such as actions or a speaker’s eye gaze – can permit
two characters) and use it as a cue for anticipating upcoming           comprehenders to rapidly anticipate relevant referents. By
role fillers when the sentence context is otherwise                     contrast, only few studies have asked how the type of
ambiguous regarding the referents’ thematic role relations              contextual cue affects visual anticipation. Neider, Chen,
(Knoeferle, Crocker, Scheepers, & Pickering, 2005).                     Dickinson, Brennan, and Zelinsky (2010), for instance,
  As already mentioned, actions are by no means the only                examined how two partners locate a randomly-appearing
cue that can rapidly affect real-time spoken sentence                   sniper target in a semi-realistic city environment. The two
comprehension. A speaker's emotional expression can for                 partners communicated either by shared voice, by shared
example enhance a listener’s visual attention to valence-               gaze, or they could exploit both of these information sources
matching event photographs as they are identified by a                  in their joint search. Both partners had to locate the sniper
spoken sentence (Carminati & Knoeferle, 2013). However,                 target and make a joint decision. In the shared gaze
seeing an action does appear to be of substantial importance            condition, one partner would see the other’s eye gaze in the
in informing comprehension, and comprehenders may                       form of a gaze cursor which was superimposed on the city
prioritize actions even when they are very infrequent (e.g.,            scene. Partners took less time to find the target in the shared
                                                                    2513

gaze than the shared voice condition, despite the arguably           determining the strength of its influence on incremental
greater information content of the voice. This suggests that         language comprehension and ambiguity resolution (e.g.,
different cues can have differential benefits for successful         McRae, Tanenhaus, & Spivey, 1998). By contrast, they do
communication between two interlocutors.                             not exploit other (informational) preferences that may guide
   Another study pitted depicted but implausible action              (visual) attention and constrain comprehension. At least in
events against stereotypical thematic role knowledge                 part, this omission is due to the fact that little is known
associated with an agent. In the utterance Den Piloten               about how different non-linguistic cues compare in their
bespitzelt gleich… (‘The pilot (obj) spies-on soon’), the            effects on visual attention and language comprehension.
verb could either be grounded in a depicted spying action,              If several cues are available, they may frequently all point
thus guiding attention to its agent, or it could be related to a     to the same referent as the most likely next-mentioned
nearby stereotypical agent (a detective) depicted as                 entity. Our study investigates this type of situation; we ask
performing an unrelated action. Faced with this ambiguity,           how two such cues conspire in enabling visual anticipation
participants preferred to inspect the agent of the depicted          of referents, whether one disambiguating cue is more
action over the stereotypical agent, prioritizing verb-action        effective than another, and whether both cues jointly are
reference over expectations of what a stereotypical agent            more effective than a single cue in guiding visual attention.
might do next (Knoeferle & Crocker, 2006).                           We also ask whether these cues differ in how they are
   Thus, with regard to allocating visual attention, people          inspected visually. To our knowledge, no such comparison
benefitted from relying on either gaze cues or depicted              has been reported to date, despite its relevance to modelling
actions, relative to other contextual cues. But these cue            typical instances of situated language comprehension.
preferences for speaker gaze on the one hand and depicted               In order to address these open questions, we pitted two
actions on the other hand emerged in two different tasks and         contextual cues against each other. Specifically, we
experimental paradigms. A direct comparison of the two               compared speaker gaze with depicted actions in a design in
cues within the same paradigm and experiment is lacking.             which either one of these cues, both cues, or neither were
   One might argue that it is unsurprising that these two cues       available to comprehenders during utterance presentation.
have distinct effects on visual attention. Real-world                This allowed us to investigate whether the influence of the
situations likely contain a myriad of cues to co-occurring           two cues on spoken comprehension is additive or
speech content, among them information from the prior                interactive, as well as the extent to which they affect
discourse, co-present objects, speaker gaze, and actions.            processing in a similar or different manner. In this context,
These cues may appear sequentially, but often they will              it is important to note that the two cues differ in important
arguably all be available at the same time and compete for a         ways: While speaker gaze shifts can be processed
comprehender’s attention. Overt visual attention is by               peripherally (at least to the extent that they are accompanied
definition serial, and since it is a key player in relating          by head movements, as in our stimuli), benefitting from
language to the extralinguistic context, a better                    depicted actions generally requires both object recognition
understanding of how comprehenders allocate visual                   and semantic integration with the spoken sentence.
attention to simultaneously competing cues is critical for              We recorded participants' fixations as they watched
models of visually-situated language comprehension.                  videos of a speaker producing a transitive sentence about
   This is because these models to date accommodate the              two virtual characters. Post-sentence, participants verified
rapid influence of individual contextual cues but – perhaps          whether a schematic depiction of role relations matched (vs.
due to the lack of pertinent data – have neither modeled             mismatched) the thematic role relations of the previously-
their combined effects nor do they say anything about the            heard sentence. Critically, we varied (a) whether the speaker
relative influence of different linguistic and non-linguistic        shifted her gaze between the sentence referents, and (b)
cues. One notable exception is the model by Knoeferle and            whether an object semantically related to the verb appeared
Crocker (2006; 2007), which proposes that reference to a             between them. Differences in the effects of the two cues
depicted action by the verb has priority over associated             could reveal themselves in anticipatory fixation of the next-
world knowledge (which could prompt attention to a                   mentioned character and in post-sentence response times.
stereotypical role filler of an action). While they postulate a
relative priority, other data suggest that distinct language-                       Eye-tracking Experiment
world relations are processed in a striklingly similar manner.
Vissers et al. (2008), for instance, reported identical effects      Methods
in event-related brain potentials in the processing of
                                                                     Participants Thirty-two Bielefeld University students
different kinds of spatial picture-sentence mismatches.
                                                                     participated in the experiment (ages 19-31). All were native
Thus, the extent to which different contextual cues have
                                                                     speakers of German, had normal or corrected-to-normal
distinct effects on a comprehender’s visual attention remains
                                                                     vision and participated for course credit. All gave informed
a point of debate. Moreover, being able to quantify such
                                                                     consent.
informational biases is essential for refining current models.
For instance, constraint-based models of language                    Materials and Design Using the virtual platform
processing rely heavily on the probability of a cue for              SecondLife®, we created 24 experimental and 48 similar
                                                                 2514

filler items. For each item, we recorded a video of a speaker       the speaker’s gaze shift. Figure 1 illustrates the four speaker
looking at three easily recognizable SecondLife® characters         × action conditions.
on a computer monitor. As the speaker inspected these                  The third factor (‘Match’) related to the congruency of the
characters, she produced a sentence describing an event             sentence with a post-trial response template. In 50% of
taking place between two of them.                                   experimental trials an arrow pointed from the position of the
   The experimental sentences were in German and all had a          waiter to that of the millionaire, thus matching the
subject-verb-object structure (passive and dative-initial           directionality of the sentential role relations; in the other
sentences occurred in some filler sentences; both are               50% of cases it pointed from one of the two outer characters
grammatical in German). Experimental trials depicted the            towards the central waiter, leading to a mismatch. The filler
agent in the centre of the scene and mentioned it first in the      trials ensured that all four response templates occurred
sentence. This is illustrated by the sentence Der Kellner           equally often, and that overall there was an equal number of
beglückwünscht den Millionär am Nachmittag (‘The waiter             matches and mismatches.
congratulates the millionaire in the afternoon’) together with
Figure 1 showing the waiter as the central on-screen                Procedure After participants had given informed consent,
character. The two characters situated to the left and right of     the eye-tracker (EyeLink 1000, SR Research) was set up for
the agent were the second-mentioned sentential patient (e.g.,       monocular tracking of the right eye with a 9-point
the millionaire on the right) and an unmentioned                    calibration procedure. Participants received on-screen
“competitor”, respectively. Half the videos showed the              instructions and four practice trials. Each trial began with a
patient on the right side of the screen, half on the left.          drift correction, followed by the video. The video always
   The items were assigned to eight lists in a 2×2×2 design:        showed the speaker smiling into the camera during the first
Speaker gaze was the first factor: In 50% of trials, the            few frames. Then she looked at the middle character, the
speaker was visible, in which case she shifted gaze from the        right, and the left character, and back to the middle
agent to the patient character before mentioning the latter. In     character (i.e. the agent) before beginning to speak. This
the other 50% of trials, a grey bar obscured the speaker.           inspection sequence was identical across all trials.
                                                                       During the sentence, the speaker shifted gaze once more,
                                                                    turning her head to the right or left of the agent in order to
                                                                    look at the second-mentioned patient character. This gaze
                                                                    shift began just after the onset of the verb (M = 711 ms
                                                                    before the onset of the patient noun phrase). At the end of
                                                                    the sentence, the speaker looked back into the camera, the
                                                                    video terminated, and the response template appeared.
                                                                       Participants’ task was to watch the video, listen to the
                                                                    sentence, and then to indicate as quickly as possible by
                                                                    pressing one of two buttons on a Cedrus® response box
                                                                    whether the arrow on the response template correctly
                                                                    pointed from the position of the agent to the patient. For half
                                                                    of the participants the “match” response button was the left
                                                                    button on the button box; for the other half the button
                                                                    assignment was reversed. Participants took a short break
                                                                    after 36 trials, followed by a recalibration. At the end of the
                                                                    experiment, participants filled in a debrief questionnaire
                                                                    permitting us to assess whether they had guessed the
                                                                    purpose of the experiment.
         Figure 1: Examples of all four video conditions,
  clockwise from top left: Speaker-only, Speaker & Action,          Analyses We analyzed log-transformed response times
                 Action-only, no cue baseline.                      (RTs) for accurate responses within 2 SD of each
                                                                    participant’s mean per Match condition. For the analysis we
                                                                    used linear mixed models with crossed random intercepts
   The second factor was object-presence: In 50% of trials,         and slopes for participants and items. Following Knoeferle
an action-related object was presented on the screen exactly        and Kreysa (2012), fixation patterns were analyzed as mean
when the speaker began to shift gaze. This object related           log probability ratios for gazes to the patient character
semantically to the action described by the verb (e.g., a           relative to the unmentioned competitor (ln(P(patient)/
bunch of balloons symbolized the verb “congratulate”). The          P(competitor)). A score of zero indicates equal attention to
object appeared between the agent character (e.g., the              the patient and the competitor; a positive score implies the
waiter) and the patient (the millionaire). Noticing the             patient was fixated more, and a negative score that it was
position of this action-related object was informative about        fixated less than the competitor. These log gaze probability
the upcoming patient – similar to noticing the direction of         ratios were computed for two time windows: The first
                                                                2515

(‘SHIFT’) spanned eight 100 ms time bins from the onset of         participants’ visual attention to the patient could be affected
the speaker’s gaze shift (which was also the time at which         by the two types of cues.
the tool appeared), lasting roughly until the onset of the            Figure 2 shows an early increase of fixations to the patient
determiner of the patient noun phrase. The second time             character in the speaker-only condition (solid red line): By
window (‘NP2’) comprised the first eight 100 ms bins from          about 500 ms after gaze shift, participants had followed the
the onset of the patient noun phrase (about half its total         speaker’s gaze to the patient, which was well before this
duration). We fitted separate linear models for log ratios         character was mentioned.
averaged over participants and items.                                 By contrast, for the conditions in which an action-related
   The initial models included three fixed factors for RTs         tool was displayed, listeners’ attention shifted to the patient
(Match, Speaker, and Action) and three fixed factors for           only later, while it was being named (the purple and blue
log-ratio gaze probabilities (Speaker, Action, and Time bin;       lines). This delay is likely due to an abrupt drop in patient
Time bin had eight 100 ms-levels to capture developments           fixations just after the action tool appeared. At this point,
across time), as well as all two-way interactions, random          participants’ attention was drawn to the onsetting tool.
intercepts for participants and/ or items, and random slopes       Finally, when no cue was available (dotted black line),
with the fixed factors and their interactions. This full model     fixations to the patient increased only once the patient had
was fitted by maximum likelihood; in cases where it did not        been mentioned (as expected for the baseline).
converge (this only ever occurred in RT analyses),
interaction terms were removed from the random parts of
the model in rising order of variance explained. The first
converging model according to this strategy was defined as
the maximal model, against which all simpler models were
compared by log-likelihood ratio tests. We also ascertained
via log-likelihood ratio tests whether interactions in the
fixed-effects structure improved model fit for the maximal
compared to simpler models. Fixed-effect interactions that
did not contribute significantly were removed, as were the
corresponding random slopes, until model fit either did not
improve further, or until a main-effects-only model
remained. In this final model, we again included as many
random slopes corresponding to the fixed effects as
possible, while maintaining convergence. We report the t-
values for all fixed effects and interactions in the final
models. Following standard procedure in the literature, we
considered coefficients as significant only if the absolute
value of the t-statistic exceeded 2. We report the t-values.
Results
Response times We calculated the time from the onset of
the response template until the button press for correct               Figure 2: Proportion of fixations over time to the patient
responses and analyzed the log-transformed RTs. The fixed           character, by condition. The graph begins at the onset of the
part of the final model for RTs contained only the three            speaker’s gaze shift, which was also the onset of the action
main effects of Match, Speaker, and Action; the random part         object, if visible. Mean onsets of the NP2 and of the ending
consisted of the random intercepts for participants and                        phrase are marked by grey vertical bars.
items, and a random slope each for Match, Speaker, and
Action by participants only (R2 = .505, sigma = 0.251). In
this model, only the factor Match affected RTs (t = -5.40).           Inferential analyses of log gaze probability ratios
Participants verified a match between sentence and template        corroborated the descriptive impression: In the earlier
faster by around 150 ms (M = 927 ms) than a mismatch               SHIFT time window, participants generally fixated the
(M = 1078 ms).                                                     patient more than the competitor (ts > 3.7), and this
                                                                   tendency increased over time (ts > 8). Both Speaker gaze
Eye-movement analyses We compared the allocation of                and the Action tool increased the likelihood of fixating the
attention to the patient character over time between the           patient in the by-items analysis, (ts > 2.4), but this was not
conditions (combined cues; speaker-only; action-only; no-          apparent in the by-participants analysis, nor did the two
cue). Figure 2 graphs the fixation patterns to the patient         factors interact. However, both Speaker (by participants)
character, beginning when the speaker shifted her gaze or          and Action interacted with the Time bin factor, reflecting a
the object appeared. Since the videos did not differ before        general increase in patient fixations over time.
that point in time, this was the first opportunity at which
                                                               2516

   In the NP2 time window, once the speaker began to speak           can be sufficient to achieve this level of facilitation;
about the patient, participants were even more likely to             additional cues do not necessarily lead to further benefit.
fixate this character than the competitor (ts > 17). More               One potential limitation of the design used here is that it
interestingly, this tendency increased substantially both            was possible for listeners to use the appearing action object
when they had just seen the speaker’s gaze shift (ts > 6), and       without considering its identity, since it always appeared
when an action-related object had appeared on the screen (ts         between the agent character and the upcoming patient. Thus,
> 4). In this time window, Speaker interacted with Action            independent of the semantic content of the depicted object,
such that the combined availability of both cues led to less         its location on the screen pointed unambiguously to the
patient fixations than the gaze-only condition, but to more          next-to-be-mentioned character. In consequence, fixating
patient fixations than action-only (ts = |5|)). Only when            the patient character did not necessarily require participants
neither cue was present were participants equally likely to          to process the identity of the object and to match it
fixate the patient and the competitor character.                     semantically to the verb they had just heard. We aim to
                                                                     clarify this issue in a future study by having an action tool
                    General Discussion                               appear on either side of the agent (see Kreysa, Nunnemann,
   We pitted two types of contextual information against             & Knoeferle, 2013, for preliminary results). We will present
each other, both of which have individually been shown to            a verb-irrelevant object between the agent and the
facilitate spoken sentence comprehension: In one condition,          competitor character, and a verb-related object between
participants could see how a speaker shifted gaze to look at         agent and patient character, making it essential to check the
a depicted character she was about to mention – thus, gaze           semantics of the depicted object(s) with the semantics of the
provided a cue for listeners to predict how she would                verb. It is possible that such verb-action integration takes
continue her sentence. In the contrasting condition, an              time and that performing such integration would further
object related to the verb appeared on-screen. In this case,         delay the visual anticipation of the target character relative
listeners were able to predict the next-to-be-mentioned              to the gaze condition. Alternatively, it is possible that even
patient by integrating the identity of this object with the          in the present study, where only one object appeared,
meaning of the verb. We also included a condition in which           participants did integrate the verb with the action object. In
both types of cues were available simultaneously, as well as         this case, we should see no substantial difference in visual
a baseline condition with no predictive cues.                        anticipation even when two competing action objects appear
   In all three conditions with predictive cues, listeners were      (one on either side of the agent).
significantly faster to fixate the upcoming patient referent of         In spite of this potential limitation, the present study
the sentence than in the no-cue baseline condition. They             shows clearly that a single contextual cue is all it takes to
were able to use either type of cue to guide visual attention        predict upcoming sentence content; more cues do not result
and arguably to anticipate that this character would be              in greater facilitation. In addition, the easier or more
mentioned next.                                                      superficial a particular cue is to process, the faster its effect
   At the same time, we found interesting disparities in how         is on fixation patterns. An open question, however, is
each of these two contextual cues affected immediate                 whether faster anticipation necessarily means better
fixation patterns during sentence processing. Seeing the             understanding.       While      prediction     may        benefit
speaker's gaze shift in the absence of an action object led          comprehension, in-depth processing and encoding of
listeners to follow this gaze shift to the patient almost            information about the patient arguably requires more than a
immediately and without directly fixating the speaker. This          few rapid fixations to that character. In future research, we
suggests the implication of low-level, potentially peripheral        plan to include a memory gating task to address this issue.
processing of the gaze and head shift. In contrast, although         Post-experiment, participants will be asked to sequentially
appearing action objects ultimately enabled participants to          recall components of the sentence heard during the main
fixate the upcoming referent roughly to the same extent as           experiment (first the verb, then the patient of the sentence,
did the gaze shift, the action-based anticipatory fixations          assisted by images of the action tool and potential patient
occurred approximately 200 ms later, due to prior fixation           characters, respectively). If either or both of our contextual
of the action object itself. Interestingly, the simultaneous         cues affect participants' short-term (post-experiment)
availability of both types of cues was at most as helpful as         memory of different sentence components, then this should
the gaze cue on its own, never better.                               be reflected in the recall rates. For instance, if the action
   We can draw two important conclusions from these                  (but not gaze) is truly integrated with the verb, then we
results: First, not all contextual cues are equal with regard to     should see better recall of sentence content if participants
how they influence ongoing language comprehension. Here,             saw an action-related object in the main experiment than if
further research is necessary to explore the nature of these         they saw only the speaker or neither cue. By contrast, if
(and other) distinct cues, with the aim of extending existing        gaze is particularly useful in cueing visual attention to, and
models of comprehension with an account of both the                  subsequent encoding of upcoming referents in memory, then
relative and joint effects of distinct visual cues. Second,          recognition of the patient in the gating task should be better
more is not always better: It seems that contextual cues have        for the gaze than for the action conditions.
a ceiling in the facilitation they can provide. A single cue
                                                                 2517

   Meanwhile, the present results already reveal that              Knoeferle, P., & Crocker, M. W. (2007). The influence of
contextual aspects of situated language can differ in the time       recent scene events on spoken comprehension: evidence
course with which they affect language-mediated attention.           from eye movements. JML, 57, 519-543.
We believe that this important fact has so far received too        Knoeferle, P., Crocker, M. W., Scheepers, C., & Pickering,
little attention in theories and models of situated sentence         M. J. (2005). The influence of the immediate visual
comprehension.                                                       context on incremental thematic role-assignment:
                                                                     evidence from eye-movements in depicted events.
                    Acknowledgments                                  Cognition, 95, 95-127.
This research was funded by the Cognitive Interaction              Knoeferle, P., & Kreysa, H. (2012). Can speaker gaze
Technology Excellence Center (German Research                        modulate syntactic structuring and thematic role
Foundation, DFG) and by a grant within the SFB 673                   assignment during spoken sentence comprehension?
“Alignment in Communication” awarded to PK (DFG).                    Frontiers in Psychology, 3, 538.
                                                                   Kreysa, H., Nunnemann, E. M., & Knoeferle, P. (2013).
                                                                     Comparing effects of speaker gaze and action information
                         References                                  on anticipatory eye movements during spoken sentence
Abashidze, D., Knoeferle, P., & Carminati, M. N. (2013).             comprehension. In K. Holmqvist, F. Mulvey & R.
   Gaze cue effect during language comprehension. In: R.             Johansson (Eds.), Book of Abstracts of the 17th European
   Fernández & A. Isard (Eds). Proceedings of the 17th               Conference on Eye Movements, Lund, Sweden. Journal
   Workshop on the Semantics and Pragmatics of Dialogue.             of Eye Movement Research, 6(3), 150.
   Amsterdam.                                                      Kuchinsky, S. E., Bock, K., & Irwin, D. E. (2011).
Altmann, G. T. M. (2011). The mediation of eye movements             Reversing the hands of time: Changing the mapping from
   by spoken language. In S. P. Liversedge, I. D. Gilchrist,         seeing to saying. JEP: LMC, 37, 748-756.
   & S. Everling (Eds.), The Oxford Handbook of Eye                MacDonald, R. G., & Tatler, B. W. (2013). Do as eye say:
   Movements (pp. 979-1003). Oxford: Oxford University               Gaze cueing and language in a real-world social
   Press.                                                            interaction. Journal of Vision, 13(4):6, 1-12.
Altmann, G. T. M., & Kamide, Y. (2009). Discourse-                 Mayberry, M., Crocker, M. W., & Knoeferle, P. (2009).
   mediation of the mapping between language and the                 Learning to Attend: A Connectionist Model of the
   visual world. Cognition, 111, 55-71.                              Coordinated Interplay of Utterance, Visual Context, and
Bock, K., Irwin, D. E., Davidson, D. J., & Levelt, W. J. M.          World Knowledge. Cognitive Science 33, 449-496.
   (2003). Minding the clock. JML, 48, 653-685.                    McRae, K., Spivey-Knowlton, M. J., & Tanenhaus, M. K.
Carminati, M. N., & Knoeferle, P. (2013). Effects of                 (1998). Modeling the influence of thematic fit (and other
   speaker emotional facial expression and listener age on           constraints) in on-line sentence comprehension. Journal
   incremental sentence processing. PLoS ONE 8(9):                   of Memory and Language, 38, 283–312.
   e72559. doi:10.1371/ journal.pone.0072559.                      Meyer, A. S., & Lethaus, F. (2004). The use of eye tracking
Crocker, M. W., Knoeferle, P., & Mayberry, M. (2010).                in studies of sentence generation. In J. M. Henderson & F.
   Situated sentence processing: The coordinated interplay           Ferreira (Eds.), The Interface of Language, Vision, and
   account and a neurobehavioural model. Brain and                   Action: Eye Movements and the Visual World (pp. 191-
   Language, 112, 189-201.                                           211). New York, Hove: Psychology Press.
Griffin, Z. M. (2004). Why look? Reasons for eye                   Meyer, A. S., Roelofs, A., & Levelt, W. J. M. (2003). Word
   movements related to language production. In J. M.                length effects in object naming: The role of a response
   Henderson & F. Ferreira (Eds.), The Interface of                  criterion. Journal of Memory and Language, 48, 131-147.
   Language, Vision, and Action: Eye Movements and the             Neider, M. B., Chen, X., Dickinson, C. A., Brennan, S. A.,
   Visual World (pp. 213-247). New York: Psychology                  and Zelinsky G. J. (2010). Coordinating spatial
   Press.                                                            referencing using shared gaze. Psychonomic Bulletin &
Griffin, Z. M., & Bock, K. (2000). What the eyes say about           Review 17 (5), 718-724. doi:10.3758/PBR.17.5.718
   speaking. Psychological Science, 11, 274-279.                   Staudte, M., & Crocker, M. W. (2011). Investigating joint
Hanna, J. E., & Brennan, S. E. (2007). Speakers' eye gaze            attention mechanisms through spoken human-robot
   disambiguates referring expressions early during face-to-         interaction. Cognition, 120, 268-291.
   face conversation. JML, 57, 596-615.                            Vissers, C., Kolk, H., Van de Meerendonk, N., & Chwilla,
Huettig, F., Rommers, J., & Meyer A. (2011). Using the               D. (2008). Monitoring in language perception: Evidence
   visual world paradigm to study language processing. Acta          from ERPs in a picture-sentence matching task.
   Psychologica, 137, 151-171.                                       Neuropsychologia, 46, 967–982.
Knoeferle, P., & Crocker, M. W. (2006). The coordinated
   interplay of scene, utterance, and world knowledge:
   evidence from eye tracking. Cognitive Science, 30, 481-
   529.
                                                               2518

