UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Organizing the space and behavior of semantic models
Permalink
https://escholarship.org/uc/item/9rh653kf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Rubin, Timothy
Kievit-Kylar, Brent
Willits, Jon
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                           Organizing the space and behavior of semantic models
     Timothy N. Rubin (timrubin@indiana.edu)                                       Jon A. Willits (jwillits@indiana.edu)
     Brent Kievit-Kylar (bkievitk@indiana.edu)                                  Michael N. Jones (jonesmn@indiana.edu)
                                  Department of Brain and Psychological Sciences, 1101 E. 10th Street
                                                           Bloomington, IN 47405
                               Abstract                                 generated word-associations (Griffiths, Steyvers, &
   Semantic models play an important role in cognitive science.         Tenenbaum, 2007) or semantic priming (Jones et al., 2006).
   These models use statistical learning to model word meanings         The high variability in both the types of semantic models
   from co-occurrences in text corpora. A wide variety of               and tasks on which they have been evaluated makes it
   semantic models have been proposed, and the literature has
   typically emphasized situations in which one model                   difficult to compare results across publications. In
   outperforms another. However, because these models often             particular, any two semantic models typically vary with
   vary with respect to multiple sub-processes (e.g., their             respect to several sub-processes (such as the type of
   normalization or dimensionality-reduction methods), it can be        structure in which they encode data, or the type of
   difficult to delineate which of these processes are responsible      dimensionality-reduction method they employ). This makes
   for observed performance differences. Furthermore, the fact          it difficult to identify which modeling choices are
   that any two models may vary along multiple dimensions
                                                                        responsible for the observed differences in model behavior.
   makes it difficult to understand where these models fall
   within the space of possible psychological theories. In this         Furthermore, comparisons are often made on tasks capturing
   paper, we propose a general framework for organizing the             only a subset of the possible types of word relationships,
   space of semantic models. We then illustrate how this                making it difficult to know in what aspects one model
   framework can be used to understand model comparisons in             outperforms another.
   terms of individual manipulations along sub-processes. Using            The goal of the current paper is two-fold. First, we present
   several artificial datasets we show how both representational        a framework to organize the space of computational models
   structure and dimensionality-reduction influence a model’s
   ability to pick up on different types of word relationships.
                                                                        of semantics. This framework is useful for understanding
                                                                        the various dimensions along which semantic models differ.
   Keywords: Semantic Modeling. Language                   Models.      Furthermore, by identifying existing models—such as LSA
   Computational Models. Model Comparison.                              (Landauer & Dumais, 1997) or HAL (Lund & Burgess,
                                                                        1996)—within this framework, it provides a clearer picture
                          1. Introduction                               of nature of the relationships between these models. Second,
Consider the words robin, sparrow and wings. It is clear to
                                                                        we illustrate the usefulness of such a framework for
any reader that there exists a semantic relationship among
                                                                        understanding how different modeling choices influence a
all three of these words. However, the types of relationships
                                                                        models’ ability to pick up on different aspects of word
between the pairs are different; a robin is a similar animal to
                                                                        similarity. To this end, we present experimental results on a
a sparrow, whereas wings are a feature of both a sparrow
                                                                        number of artificial datasets, using a set of models that vary
and a robin. In many instances, the usage of the word robin
                                                                        along two different dimensions within our framework.
is indistinguishable from the usage of sparrow; that is, the
                                                                        These results illustrate how both a model’s representational
two words could be exchanged and no one would be the
                                                                        structure and use of dimensionality-reduction interacts with
wiser. However, replacing either word with wings would
                                                                        the ability to pick up on different aspects of word-similarity.
typically produce an incoherent sentence. One might be
able to replace the word wings with arms while retaining the            Components of Semantic Models (Semantic Modeling
basic meaning of a sentence, but this would feel like an                pipeline): Most semantic models largely consist of the same
incorrect usage of the word. This example illustrates the               basic components/sub-processes, where several choices
range of ways in which words can be semantically related:               exist for each of these steps. To understand the relationship
two words might be largely substitutable for one another                between different semantic models, it is important to first
(e.g., sparrow and robin), two words might be associated                explicitly define what each of these components is. The
with one another (e.g., sparrow and wings), and two words               relationships between any two models can then be well
might belong to the same class of words while not being                 described by the individual choices they employ for each
highly substitutable (e.g., wings and arms). A central aim of           modeling component. To give an overview of our
computational models of semantics is to learn about these               framework, we first summarize the basic steps in
types of word relationships using linguistic data as input              constructing a semantic model from a tokenized corpus.
(Jones, Kintsch, & Mewhort, 2006).
                                                                        Model Components and Sub-processes:
   A variety of semantic models have been proposed in the
                                                                        1. Encoding Region: The “window” over which text is
psychological literature (see McRae & Jones, 2013 for a
                                                                        encoded within a representational structure.
review). The relative ability of different models to capture
                                                                        2. Representational Structure: The form of the matrix in
human behavior is evaluated using tasks such as synonym
                                                                        which words within encoding regions are stored.
tests (Landauer & Dumais, 1997), predicting human-
                                                                   1329

                                                                                        Corpus:     A B C D E     .   A A B C .     C D D .
3. Representational Transformation: Matrix
normalization and dimensionality-reduction method                                     Window 1      A B C D E     .   A A B C .     C D D .
                                                                                      Window 2      A B C D E     .   A A B C .     C D D .
4. Similarity Metric / Decision Process: Process by which
                                                                             Sliding Window 3       A B C D E     .   A A B C .     C D D .
information is retrieved from the semantic structure                                  Window 4      A B C D E     .   A A B C .     C D D .
                                                                                      ...          ...
      2. Organizing the space of semantic models
In this section, we focus in detail on the steps highlighted                          Window 1      A B C D E     .   A A B C .     C D D .
above. After describing each step we discuss the space of                    Fixed    Window 2      A B C D E     .   A A B C .     C D D .
                                                                                      Window 3      A B C D E     .   A A B C .     C D D .
modeling options that are available. We then locate a
number of previously described models within this overall                                               WW                                    WD
framework.                                                                 Region 1           Region 2        Region 3       Full Model     Full Model
                                                                         A B C D    E       A B C D    E    A B C D    E    A B C D E          1 2 3
2.1. Encoding Region                                                 A 0 1 1 1      1    A 1 2 2 0     0  A 0 0 0 0    0  A 1 3 3 1 1       A 1 2 0
                                                                     B     0 1 1    1    B    0 1 0    0  B   0 0 0    0  B   0 2 1 1       B 1 1 0
The encoding region defines the span of the individual               C        0 1   1    C       0 0   0  C      0 2   0  C       0 3 1     C 1 1 1
observations of text that are encoded by a model. In the             D           0  1    D          0  0  D         1  0  D         1 1     D 1 0 2
                                                                      E             0     E            0  E            0  E             0   E 1 0 0
context of corpus-based semantic models, the encoding
region corresponds to the “window” over which a sample of            Figure 1 Top: Comparison of encoding regions for a simple corpus
                                                                     using a “sliding” vs. a “fixed” window. Bottom: Using the “fixed”
text is encoded into the matrix structure. For example, many
                                                                     encoding regions defined above, a comparison of the WW vs. WD
semantic models employ a sliding window of 10 words,                 representational structures.
wherein the co-occurrence of words within a 10-word
window is encoded for all unique 10-word windows in a               data in a corpus. The two choices for this representational
corpus.                                                             structure we will refer to as Word-by-Word (WW) and
   Defining a model’s encoding region consists of two               Word-by-Document (WD) representations. Within both
distinct options, which correspond to different theoretical         structures, each unique word is represented via a row in a
stances regarding the process by which semantic knowledge           matrix. In a WW representation, each column also
is encoded. The first option determines whether regions             corresponds to a unique word in the corpus. In a WD
employ a “fixed” or “sliding” window. A fixed window                representation, each column corresponds to a unique
method utilizes a set of rules that govern region boundaries        encoding region. To be consistent with the psychological
within a text. Typically, these regions are defined such that       and NLP literature, we refer to this as a WD structure,
they capture semantically coherent regions of text, such as         despite the fact that the columns could correspond to any
sentences, paragraphs, or documents. In contrast, a sliding         type of encoding region (e.g. sentences, paragraphs, or all
window method utilizes all possible N-length sequences of           regions defined by a sliding window method).
word-tokens as encoding regions. A model employing a                    In a WW structure, a word’s presence within an encoding
fixed window therefore posits that co-occurrences are               region is encoded entirely via its co-occurrence with other
tracked primarily within linguistic boundaries (such as             words. For all pairs of words within a region, w1 and w2, a
sentences), rather than over arbitrary distances within a text.     count is added to the WW matrix at element (w1,w2) and
   The second option when defining encoding regions                 (w2,w1). This is illustrated at the bottom of Figure 1 using
corresponds to the window-size. For sliding windows, the            the fixed-window encoding regions defined above. Since the
region size can be any positive integer between 2 (which            WW matrix is symmetric, we only show the upper-
encodes the minimal possible information—the co-                    triangular region of the matrix for clarity.1 As shown in
occurrence of a single pair of words) and the length of the         Figure 1, each of the three encoding regions can be mapped
longest document in the corpus. For a fixed window                  to a unique WW matrix of a fixed size. Summing across all
method, a small, medium and large window size might                 of the regions then creates single WW representation of the
correspond to sentences, paragraphs, and documents. A               full corpus. In a WD structure, each encoding region is
sliding-window method will have always have the number              mapped to a unique column. Each word’s frequency within
of encoding regions equal to the number of tokens in a              the region is encoded within the row for of that column. For
corpus, whereas for a fixed-window method the window-               example, the word “D” occurs twice in the third encoding
size will be inversely proportional to the total number of          region, so the row corresponding to “D” is assigned a value
encoding regions.                                                   of two in the third column of the WD matrix.
   At the top of Figure 1 we illustrate the differences                 A key theoretical distinction between the WW and WD
between fixed and sliding windows using a toy corpus                representational structures is that, in a WW structure, words
consisting of three sentences. For the illustration, we use a 6     are represented strictly by their co-occurrence with other
token sliding window (including punctuation), and a fixed-          words, making WW structures akin to other psychological
window method that utilizes sentences as boundaries around          theories that stress the associations between individual
regions of interest.                                                items, stimuli, or responses. In contrast, WD structures posit
2.2 Representational Structure                                          1
Once the encoding regions have been defined, these regions                 Due to size restrictions we limit the discussion to models that
                                                                    do not account for word-order, although the current framework can
are mapped to a representational structure for storing the
                                                                    be generalized to account for such models as well.
                                                                1330

an association between an item and its encoding region                        3. Experiments using artificial datasets
(such as a sentence or document). A second, more practical           Due to the fact that semantic modeling entails choices along
difference between WW and WD representational structures             a number of dimensions, it is difficult to know which of
is that since WW “contexts” are fixed across the corpus, this        these dimensions is responsible for the differences observed
allows the row-representations to be collapsed across all            when comparing any pair of semantic models. For example,
encoding regions. In contrast, the row-representations in a          HAL and LSA employ different encoding regions (sliding
WD encoding structure cannot be collapsed, resulting in a            windows over small regions vs. fixed windows over large
representation who’s size scales as a function of the number         regions), different representational structures (WW vs.
of encoding regions in the corpus.                                   WD), different normalization (conditional probability vs.
2.3. Representational Transformation                                 log entropy) and different dimensionality-reduction methods
The representational transformation corresponds to how the           (no abstraction vs. SVD). In this section we illustrate that by
information encoded within the WW or WD matrix is                    isolating individual modeling components, we can identify
manipulated after it has been stored. This process can               precisely how the components influence a model’s ability to
(optionally) involve a number of difference procedures,              capture different types of word relationships. We employ
including normalization, abstraction, and dimensionality-            artificially constructed datasets designed to capture different
reduction. In the semantic modeling literature, a variety of         types of inter-word relationships, while minimizing the
these methods have been proposed. For example, LSA                   number of confounding variables between models.
(Landauer and Dumais, 1997) employs log-entropy                         We designed datasets that captured three distinct types of
normalization followed by Singular-Value-Decomposition               word relationships, while also limiting the number of
(SVD) for abstraction and dimensionality reduction. Since            possible variables that can contribute to observed
the number of possible transformations is potentially                differences in model performance. In particular, all datasets
infinite, we identify a number of published models with              were constructed such that they consisted of sets of
respect to their encoding regions, representational structures       documents, each of which contained only a single word-
and representational transformations, comprising the space           pair. By limiting each document to a single word-pair, we
we have described thus far in the paper (Table 1).                   eliminated any potential effects caused by the definition of
                                                                     encoding-region; for a 2-word document, a single word-pair
2.4. Similarity Metric / Decision Process                            will be encoded for each document, independent of both the
To compute a similarity between two words w1 and w2,                 encoding region type (sliding vs. fixed) and size. Within the
semantic models apply some function to the transformed               previously defined modeling framework, this limits two key
representational structure. Typically this consists of a vector      modeling choices to (1) whether to use a WW or WD
operation across the row representations for each word. For          representational structure, and (2) whether or not to use an
example, in LSA the cosine similarity is computed between            abstraction algorithm such as SVD.
words’ singular vectors across a subset of dimensions.                  In designing our toy datasets, we wished to explore which
Although there are alternative approaches that could be              types of semantic relationships between words were
employed here (e.g., within a WW matrix a model could                captured by different manipulations in terms of the semantic
directly utilize the value of the matrix element WWw1,w2 as a        models. In particular, we designed each dataset such that it
measure of the semantic relationship between words w1 and            captured (1) associativity: words with which a target word
w2), such alternatives have rarely been used in the literature.      directly co-occurs, (2) substitutability: words that have
2.5. Final Considerations                                            similar co-occurrence patterns to a target word, and (3)
While it is useful (and computationally equivalent) to define        categorical-relationships: words which co-occur with similar
the steps in our framework independently, it is not necessary        types of words to the target word.
that a model perform them in a strictly sequential fashion.             To make this more concrete, consider the example dataset
For the purposes of psychological theories, it is valid to           represented in Figure 2. Words in this dataset belong to one
posit that two (or more) steps actually occur in parallel. For       of two syntactic categories: objects or descriptors. We limit
example, it may be more psychologically plausible for a              the existing word pairs in the dataset such that objects only
model such as LSA to perform dimensionality-reduction                co-occur with descriptors (as in, e.g., the sentences “pet
during the encoding process, such that it does not                   cat”, “pet dog” and “wild wolf”). Of all 16 possible object-
asymptotically require infinite storage (as more and more            descriptor pairs, only the pairs with an indicator value of 1
regions are encoded).                                                in fact co-occur in the dataset. By doing so, we build two
               Encoding Region Representational          Representational Transformation
 Model         Type    Size          Structure     Normalization Dimensionality-Reduction       Reference
 HAL           Sliding 10 Words         WW         Row-sum       None                           Lund & Burgess, 1996
 COALS         Sliding 10 Words         WW         Correlational Singular-Value Decomposition   Rohde, Gonnerman, & Plaut, 2009
 BEAGLE        Fixed Sentence           WW         None          Random Vector Accumulation     Jones, Kintsch, & Mewhort, 2006
 LSA           Fixed Document           WD         Log-Entropy Singular-Value Decomposition     Landauer & Dumais, 1997
 Topic Model   Fixed Document           WD         None          Latent Dirichlet Allocation    Griffiths, Steyvers & Tenenbaum, 2007
                         Table 1: Situating several semantic models within the organizational framework
                                                                1331

types of semantic information into the data: associativity                                          B1 B2 B3 B4        B1 B2 B3 B4         B1 B2 B3 B4        B1 B2 B3 B4            B1 B2 B3 B4
                                                                                                A1 1 1 0 0           A1 1 1 0 0       A1 1 1 0 0         A1 0 1 1 1                A1 1 1 1 0
and substitutability. Words with associative relationships in                                   A2 1 1 0 0           A2 1 0 1 0       A2 1 1 0 0         A2 1 0 1 1                A2 1 1 1 0
the dataset are word-pairs with values of 1 (e.g. dog and pet                                   A3 0 1 1 0           A3 0 1 0 1       A3 1 1 1 0         A3 1 1 0 1                A3 0 1 1 1
are associated, whereas sparrow and furry are not). Words                                       A4 0 0 1 1           A4 0 0 1 1       A4 0 1 1 1         A4 1 1 1 0                A4 1 0 1 1
with substitutable relationships in the dataset are word-pairs                                                                                  B1 B2 B3 B4              B 1 B 2 B 3 B 4 B 5 B1 B 7
that have similar sets of associative relationships (e.g. cat                                       B1 B2 B3 B4 B5        B1 B2 B3 B4 B5     A1 1 1 0 0            A1 1 1 1 0 0 0 0
                                                                                                A1 1 1 1 0 0           A1 1 1 1 0 0          A2 1 1 0 0            A2 1 1 1 0 0 0 0
and dog are perfectly substitutable in this dataset since they                                  A2 1 1 1 0 0           A2 1 1 1 0 0          A3 0 1 1 0            A3 0 1 1 1 0 0 0
both only co-occur with pet and furry, whereas dog and wolf                                     A3 1 0 0 1 1           A3 0 0 1 1 1          A4 0 0 1 1            A4 0 0 0 1 1 1 0
are partially substitutable). Words with a categorical                                          A4 0 1 0 1 1           A4 0 0 0 1 1          A5 0 0 1 1            A5 0 0 0 0 1 1 1
relationship are words that co-occur with the same type of                                      A5 0 0 1 1 1           A5 0 0 0 1 1          A6 1 0 0 1            A6 0 0 0 1 0 1 1
word, regardless of substitutability (e.g. sparrow belongs to                                       Figure 3: Illustration of structure for all artificial datasets
the same category as dog and cat despite it not sharing a
single associate, because it co-occurs with other descriptors                                   Decomposition, because of its rich history in the semantic
and not with other objects).                                                                    model literature (e.g., Landauer & Dumais, 1997; Rohde et
  In Figure 3, we show all dataset structures used in                                           al., 2009). As shown in Table 2, this two-by-two space of
generating our artificial datasets. These corpora were                                          semantic models under consideration thus encapsulates
designed such that they captured a range of associative,                                        three models that have been employed in the psychological
substitutability, and categorical relationships, across a range                                 literature, as well as the Vector-Space Model (VSM) from
of category-sizes. The question of interest here is: what                                       information retrieval (Salton, Wong & Yang, 1975). To
modeling manipulations allow a model to pick up on the                                          control for other possible ways in which the models could
three different relationships captured by the structure of                                      vary, we employ the cosine-similarity metric and row-
these datasets.                                                                                 normalization for all models2.
                        Dataset structure                                 Documents
                                   Descriptors
                                                                                                    Table 2: The experimental models employed, and approximate
                                                                          "Pet Cat"
                                                                                                                corresponding models from literature
                                            "Furry"            "Flying"
                                                                                                                                                      Structure
                                                      "Wild"
                                                                          "Pet Dog"
                                    "Pet"                                 "Furry Cat"                                                               WW        WD
                                    B1      B2        B3       B4         "Furry Dog"                                                No-SVD         HAL      VSM
                                                                                                                      Abstraction
                   "Cat"       A1   1       1         0        0          "Furry Wolf"                                               SVD           COALS      LSA
         Objects
                   "Dog"       A2   1       1         0        0          "Wild Wolf"
                   "Wolf"      A3   0       1         1        0          "Wild Sparrow"        3.1.3. Model Evaluations: To evaluate which models
                   "Sparrow"   A4   0       0         1        1          "Flying Sparrow"      captured the three previously described relationships, we
Figure 2: Example of design and construction of artificial datasets.                            provide formal definitions of each3:
3.1. Methods                                                                                    Associativity: The extent to which a pair of words locally co-
                                                                                                occurs. As a measure of a pair of word’s “true” associativity, we
3.1.1. Dataset generation: Corpora were generated using
                                                                                                use the Pairwise Mutual Information measure. This measure has
the associative structures illustrated in Figure 3. Each                                        been employed previously in both the psychological and machine-
dataset (which we will refer to as a corpus) consisted of a                                     learning literature. Intuitively, it corresponds to the observed
set of documents, each of which contained a single pair of                                      probability with which two words co-occur relative to their
words. Within each corpus, only the word-pairs indicated in                                                                                     !(! ,! )
                                                                                                expected co-occurrence probability: 𝑃𝑀𝐼 = 𝑙𝑜𝑔 ! ! !∗! !!
                                                                                                                                                                     !         !
the figure were represented. Frequencies of each pair of                                        Substitutability: The extent to which two words have similar co-
words were adapted such that all words from category A                                          occurrence patterns. We measure this using the Jensen-Shannon
had equal frequencies (words in category B had equal                                            divergence (a measure of the similarity of two probability
frequencies in about half of the corpora). The nine corpora                                     distributions) between each word’s probability of co-occurrence
were designed such they each capture a range of patterns of                                     across all other words. To give some examples, consider the words
the three distinct types of word relationships described                                        from category “A” in Figure 2. The probability distribution of co-
above, while additionally varying in factors such as category                                   occurrences for “dog” is equivalent to that for “cat” (with p=.5 for
size. This was done to ensure that our findings were                                            both pet and furry); these words’ have a JS-divergence of 0. The
                                                                                                JS-Divergence for dog and wolf (which share one associate) equals
consistent across a variety of data.                                                            .5, and for dog and sparrow (which share no associates) equals 1.
3.1.2. Models: Since each document within our corpora                                           We transform this value into a similarity using:
consisted of only a single word-pair, this eliminated the                                                  JS-Similarity = 1 - JS-Divergence.
need to define or manipulate the encoding regions in our
                                                                                                    2
models. This left us with two primary factors along which                                            The broad trends presented in our results are consistent across
models could vary: the type of encoding structure used and                                      cosine, city-block, and correlational similarity metrics.
                                                                                                   3
whether or not they employed an abstraction algorithm. As                                            We do not wish to argue the case for whether these are the
previously discussed, there are two types of encoding                                           “proper” or “true” definitions of these different types of
                                                                                                relationships. However, the types of relationships we describe have
structures used in semantic modeling (WW and WD), and a
                                                                                                a basis in both statistical measures of text and the psychological
wide variety of abstraction algorithms. We limit our                                            literature e.g., see (Jones et al., 2006), and furthermore capture
exploration of abstraction here to the use of Singular Value                                    intuitive psychological aspects of semantics.
                                                                                             1332

                                                                             Table 3 Average rank-correlation between all model similarities
Categorical: A binary measure of whether the two words belong to                     and the three word relationships across all corpora
the same category. Using the example given in Figure 2, dog has a
                                                                                          Associativity    Substitutability      Categorical
categorical similarity of 1 to all object-words in category A, and a                      WW       WD        WW      WD         WW       WD
similarity of 0 to all descriptor-words in category B.
                                                                            No-SVD        .00     1.00       1.00    .15         .81    -.22
Each model generates only a single set of predictions, and                  SVD           .17      .94       .92     .88        1.00     .15
these predictions may conflate the different relationships
(e.g., a model’s similarity metric might pick up on both                   3. These results indicate clear main effects as well as
substitutability and associativity). However, the design of                interactions between modeling manipulations and the types
our datasets is such that we are able to evaluate each                     of word relationships that a model captures.6
model’s ability to pick up on different relationships                         First, these results illustrate that similarities computed
independently.4 In particular, each word only associates                   from raw WD matrices perfectly capture the associativity
with words from opposite categories, but will only share                   between two words. This is because the word-vector within
associates with words from within its own category.                        a WD matrix simply encodes the instances in which the
   The emphasis of the present experiments is theoretical                  word has occurred, and the extent to which this vector is
(i.e., to determine which models are capable of capturing                  aligned between a pair of words captures the relative
which aspects of similarity), rather than practical. In light of           frequency with which they co-occur. Furthermore, the raw
this, we make two choices with respect to model-evaluation                 WW matrix perfectly captures the extent to which words are
that emphasize ceiling-performance rather than performance                 substitutable. This is because the rows within the WW
that might be expected in real-world conditions. In                        matrix capture each word’s patterns of co-occurrence.
particular: (1) during evaluation, we only evaluate a model’s              Additionally, since the category-membership was defined
ability to pick up on substitutability within categories, and              by the set of valid words with which a word could co-occur
to pick up on associativity between categories (i.e., the                  with in each dataset, the raw WW-matrix picks up on
relationships between model-predictions of PMI and JS-                     category membership to the extent that category-
Similarity are only evaluated for word-pairs relevant to the               membership is correlated with substitutability.
task)5, and (2) we evaluate models that employ SVD with                       Employing SVD as an abstraction method significantly
respect to their best performance on a given task, across all              affects model performance for both the substitutability and
dimensionalities for which the singular values is greater                  category-membership measures. The ability of the WD
than zero (that is, for an SVD model with seven dimensions                 matrix to capture substitutability dramatically improves
that account for variance in the dataset, we compute the best              when SVD is employed, and achieves near perfect
performance among the six sets of predictions generated by                 performance. To give a concrete example of how this is
the model using between two and seven dimensions). For                     achieved, refer back to the design shown in Figure 2. In the
evaluating the category-based relationships, we compare the                raw-document space, the cosine-similarities between words
model’s similarity score and a binary variable—indicating if               within a category are always equal to zero except when
two words belong to the same category—across all words.                    comparing a word to itself (due to the fact that this matrix
   For each of the prediction tasks (predicting associativity,             picks up only on word-associativity). However, the
substitutability, and categories), we evaluate a model’s                   similarities in the first two dimensions of the word-space
ability to pick up on each word’s pattern of relationships                 after performing SVD perfectly capture the relative
using Spearman’s rank correlation. For example, to evaluate                substitutability of all words within their categories except
whether a model picks up on the pattern of associativity for               for A3 and B1. This is due to the fact that the SVD process
the word dog in the dataset shown in Figure 2, we compute                  uses its first dimensions to encode as much variance in the
the rank correlation between a model’s predicted similarities              dataset as possible. In this case the most variance can be
and the PMI in the dataset between dog and all words in                    accounted for by collapsing across documents with partially
category B. These rank-correlations are then averaged                      overlapping object or descriptor words. It is important to
across all words within a dataset. For models employing                    note that within a single choice of dimensionality, the model
SVD, the best-performing model on this task is taken from                  ends up conflating substitutability and associativity; e.g., if
among all dimensionalities.                                                one were to use just the first two dimensions of the SVD to
                                                                           predict word-associativity, the average rank correlation
3.2. Results                                                               between model-similarities and word-associativity on
For all three types of word-similarities we defined, the                   dataset 2 would be just .59 (but using either 4 or 5
average rank-correlation between model predictions and                     dimensions gives the observed performance of .94). This
true word-similarities (across all corpora) is shown in Table              result is consistent across the different datasets; the SVD of
                                                                           the WD matrix picks up primarily on substitutability using
   4                                                                       the first few dimensions, and picks up on associativity in
     Although the extent to which the models may weight different
aspects of similarity is of both theoretical and practical interest, it
                                                                           higher dimensions (as it more closely approximates the
is not the focus of the current experiments
   5                                                                          6
     For JS-Similarity we furthermore do not include the item’s                 We note here that the results were highly consistent across all
self-similarity, as this is greatly over-estimates model-                  nine corpora, and did not interact with corpus features such as
performance, since both values will always equal one.                      category-size.
                                                                       1333

original space, which captures associativity). This is why          words. In particular, without dimensionality-reduction, a
the associativity score does not dramatically worsen when           WW representation captures the substitutability between
moving from a raw to SVD representation.                            two words, whereas a WD representation captures the
   Within the WW matrix, employing an SVD allows the                associativity between the words. Employing an abstraction
model to perfectly capture the category-membership of all           process like SVD on a WW matrix allows it to induce
words. This is an interesting result, since it indicates that       category-level relationships, even when the two words’
this model has the ability to generalize across category            patterns of associativity are orthogonal. Employing an SVD
members, despite the fact that in some cases they have              on the WD matrix allows it to capture the substitutability of
orthogonal patterns of associativity; e.g., in Figure 2 the         words in addition to their associativity. However, both
pattern of sparrow is orthogonal to both dog and cat, but the       structures have their own unique limitations: the similarity
model nonetheless picks up on the fact that this word               between words composed of WW structures can not pick up
associates with only members of category B and is therefore         on associativity, and the WD matrix can not pick up on
a member of category A. As with the WD matrix capturing             categorical-similarities, whether or not an SVD is used.
substitutability, category membership is entirely captured             Our results indicate that a single semantic model’s
within the first few dimensions of the reduced matrix               predictions may be insufficient to capture the full range of
(typically the first 2 dimensions). Since perfectly capturing       semantic relationships that people are able to represent. This
the rank-ordering of category members necessitates that all         suggests that a valuable direction for future research may be
within-category members have equal similarity, the SVD-             in embedding multiple representational structures and
reduced matrix does not pick up on substitutability at these        abstractions within a larger model.
lower-dimensionalities (for a single set of predicted                  It remains to be seen how these findings will interact with
similarities, if performance on the category task is 1,             manipulations along other dimensions in our framework.
performance on the substitutability task is zero). However,         For example, while we have shown that a WW structure is
just as the WD matrix picks up again on associativity as            unable to capture associativity when the model’s encoding
more dimensions are included, the WW matrix picks up on             regions are restricted to individual word-pairs, this should
again on substitutability as more dimensions are included.          change as encoding regions increase in size. For example,
   Since the best performing dimensionality is used                 using a larger encoding region should allow a WW model’s
separately for each task, performance for the SVD-reduced           row representations to indirectly capture word-associativity
WW matrix significantly improves on the category task               via second order co-occurrences; when encoding the phrase
while hardly being impacted on the substitutability task. It is     “pet dog chased”, the word chased would be encoded within
important to note, however, that at any individual                  the rows for both dog and pet. Since WW matrices pick up
dimensionality, the SVD-reduced WW matrix could not                 on co-occurrence patterns, they could indirectly capture the
perform as well as is shown in Table 3 on both the                  associativity between pet and dog via their mutual co-
substitutability and category tasks. Similarly, the SVD-            occurrence with chased. But while a larger encoding region
reduced WD matrix could not perform as well on both                 may increase performance with respect to associativity,
associativity and substitutability tasks using a single             performance with respect to other types of word
dimensionality.                                                     relationships may suffer. This leaves many open questions
   Lastly, we get striking failures for both the WW and WD          regarding how other manipulations within the space of
representational structures in their ability to capture specific    models we described will qualitatively affect performance.
types of relationships. The WD matrix—using either a raw            Additional results such as those presented within this paper
representation or an optimally reduced dimensionality—              should serve to constrain the types of psychological
fails to pick up on category-membership. The WW matrix              processes one might posit for how a model captures
likewise fails to ever pick up on associativity.                    particular aspects of human behavior.
                        4. Discussion                               Acknowledgements
                                                                    This work was supported by NSF BCS-1056744 and NIMH-096906.
In this paper, we presented a general framework for
                                                                    References
organizing the space of semantic models, and identified a           Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics in semantic
number of existing models within this space. We then                   representation. Psychological Review, 114, 211-244.
demonstrated how this framework is useful for guiding               Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces
                                                                       from lexical co-occurrence. Behavioral Research Methods, Instrumentation,
experimental work into modeling semantic structure. In                 and Computers, 28, 203-208.
particular, we showed that by isolating and comparing               Jones, M. N., Kintsch, W., & Mewhort, D. J. K. (2006). High-dimensional
individual components within the framework, we can                     semantic space accounts of priming. Journal of Memory and Language, 55,
                                                                       534-552.
identify how specific manipulations influence a model’s             Landauer, T., & Dumais, S. (1997). A solution to Plato's problem: The Latent
ability to capture different aspects of semantic structure.            Semantic Analysis theory of the acquisition, induction, and representation of
   Using artificial data generated using a known structure,            knowledge. Psychological Review, 104, 211-240.
                                                                    McRae, K., & Jones, M. (2013). 14 Semantic Memory. The Oxford Handbook of
we showed that both a model’s representational structure               Cognitive Psychology, 206.
(WW vs. WD) and its use of dimensionality reduction have            Rohde, D. L. T., Gonnerman, L., & Plaut, D. C. (2009). An improved model of
specific consequences in terms of a model’s ability to                 semantic similarity based on lexical co-occurrence. Cognitive Science.
                                                                    Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic
capture types of different kinds of relationships between              indexing. Communications of the ACM, 18(11), 613-620.
                                                                1334

