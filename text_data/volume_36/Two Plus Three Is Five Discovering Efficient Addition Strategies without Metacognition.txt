UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Two Plus Three Is Five: Discovering Efficient Addition Strategies without Metacognition
Permalink
https://escholarship.org/uc/item/2gx1j1qq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Hansen, Steven Stenberg
McKenzie, Cameron Ross Lloyd
McClelland, James L.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                      University of California

                                                   Two Plus Three Is Five:
                 Discovering Efficient Addition Strategies without Metacognition
                                         Steven S. Hansen (sshansen@stanford.edu)
                                         Cameron McKenzie (crlmck@stanford.edu)
                                      James L. McClelland (mcclelland@stanford.edu)
                                                        Department of Psychology
                                             Stanford University, Stanford, CA 94305 USA
                             Abstract                                  several early models did have this character (Neches, 1987).
                                                                       However, the paucity of solution paths that involved faulty
   When learning addition, children appear to perform a
   remarkable feat: as they practice counting out sums on their        strategies appear to rule out the „take a random step‟ style
   fingers, they discover more efficient strategies while avoiding     exploration used by most reinforcement learning models
   conceptually flawed procedures. Existing models that seek to        (Crawley, Shrager, & Siegler, 1997). Trial and error
   explain how children discover good strategies while avoiding        accounts were thus rejected, and replaced by a theory that
   bad ones postulate metacognitive filters that reject faulty         posited a metacognitive mechanism with explicit, domain-
   strategies. However, this leaves unexplained how the domain-        specific content knowledge to filter out flawed strategy
   specific knowledge required to evaluate a strategy might be
   acquired prior to addition being mastered. We introduce a           proposals. This mechanism allows the discovery of new
   biased exploration model, which demonstrates that new               strategies while producing only reasonable errors. However,
   addition strategies can be discovered without invoking              it remains unclear how children could acquire the complex
   metacognitive filtering. This model provides a fit to data          knowledge required to judge the appropriateness of their
   comparable to previous models, with the considerable                own strategy proposals. The acquisition of the
   advantage of avoiding an appeal to knowledge whose source           metacognitive filter is neither explained nor explicitly
   is not itself explained. Specifically, we fit the pattern of        modeled.
   changes in strategy use over time as children learn addition,
   as well as the overall error rate and error types reported             The approach taken in this paper is to circumvent this
   empirically. The model suggests that the critical element           difficulty by proposing that a metacognitive filter may not
   allowing strategy discovery may be prior learning, rather than      be necessary in the first place. We accomplish this by
   metacognitive strategy evaluation. We close by offering             modifying a standard trial and error, reinforcement-learning-
   several empirical predictions and propose that what others          based paradigm to be biased towards previously learnt
   have called strategies might often be decomposable into             actions. We note that children learning the addition task
   elements that can be assembled on the fly as problem solving        have already learnt to count out numbers of objects, count
   unfolds in real time.
                                                                       on their fingers, and perform addition using a finger-
   Keywords: M athematical Cognition; Strategy Discovery;              counting strategy (Siegler & Jenkins, 1989). As we shall
   Reinforcement Learning; M etacognition.                             demonstrate, instantiating a model with biases towards these
                                                                       actions obviates the need for a metacognitive filter. We also
                          Introduction                                 expand the notion of retrieval – a „strategy‟ that circumvents
   Single-digit addition is one of the first hurdles children          the need to engage in a structured sequence of behaviors by
master on their way to mathematical competence. Given the              simply recalling the correct final answer to a problem – by
importance of mathematics to educational attainment, it is             suggesting that retrieval might also occur for appropriate
unsurprising that the process by which children learn                  subparts of a larger problem. Our model makes several
addition has received considerable attention (eg, Siegler &            novel predictions about the discovery process and questions
Jenkins, 1989). A remarkable observation from these studies            the notion that selection and discovery processes necessarily
is that, once they are equipped with the ability to count out          take place at the level of complete strategies.
sums on their fingers, children spontaneously (without
instruction) exhibit faster strategies. Despite this willingness                               Background
to innovate, children rarely arrive at a strategy that, when              The data our model attempts to account for comes from a
executed correctly, leads to the wrong answer. This poses a            study examining 4 and 5 year olds‟ discoveries of new
real problem for trial and error theories of learning. As they         finger addition strategies (Siegler & Jenkins, 1989).
acquire new, faster strategies, how do children know which             Children are assumed to come to the task knowing a correct
strategies to avoid?                                                   but inefficient strategy, and are observed in a series of
   Several attempts have been made to model the evolution              sessions spread over approximately three months as they
of children‟s approaches to solving simple addition                    solve simple addition problems . Over this time, children
problems. The apparent absence of explicit instruction in the          gradually acquire strategies that lead to faster completion of
use of particular observed s trategies would normally make             the task.
reinforcement learning a candidate mechanism, and indeed
                                                                   583

   In our view, it is important to frame the dis covery process     introduced (i.e. problem with one very large addend), which
against the backdrop of relevant previously learnt                  caused children who had already discovered the min
procedures. The most important to our theory is what we             strategy to expand its usage rapidly.
will call the count-list procedure whereby the child learns to         In the study, the majority of children discovered two new
step through a stable ordering of the number words,                 strategies, and generally did so in the same order. The first
sometimes while counting out fingers or other physical              discovery was the shortcut-sum strategy, and this tended to
tokens. The count list is a prerequisite for learning addition      occur very early on in the study.1 This strategy involves
and is known by all children in the study.                          counting up from one to the sum of the two numbers ,
   We also assume (following Davidson, Eng & Barner,                though the interpretation of this behavior is a key question
2012) children can perform the how many task, in which the          posed by our theory.
child verbally goes through his count list in order,                   The second strategy, the min strategy, consists of counting
simultaneously pointing to the next in a set of physical            from the larger of the two addends up to the sum. For
tokens, then responding with the number reached when the            example, for the problem “2+5”, a possible protocol would
items in the set have been exhausted. It is generally accepted      be: “five, six, seven. The answer is seven.” This strategy
that this behavior is well learnt by the time children are          slowly gains dominance over both the shortcut-sum and sum
taught their first addition procedure. Finally, we assume           strategies. While these transitions are occurring, children
children have mastered the give-a-number or give-n task,            also gradually increase their reliance on the „retrieval
which involves providing a supply of tokens and asking the          strategy‟, simply recalling the correct answer.
child to give the experimenter (or other target) a certain             Given the categorical nature of their coding scheme, the
number of them. Children who can perform this task for              researchers focused their analysis on when new strategies
numbers larger than 4 do so by stepping through the count           were discovered, how often they were used thereafter, and
list as they remove them one by one from the supply,                whether or not any incorrect strategies were ever used. The
stopping when they reach the reques ted number.                     results of the study partially supported the idea that strategy
   In the study we will be considering, children were               change occurred through an exploration-based, incremental
enrolled in a preschool/kindergarten that taught a standard         learning process. Children were not always able to describe
procedure, known as the „sum‟ strategy, for correctly adding        or explain their new strategies to the experimenter.
two numbers together. This procedure begins with the child          However, the authors also found no evidence that incorrect
counting from one up to the value of one of the two                 strategies were ever used and they argued that exploration of
addends, while simultaneously putting up a finger or taking         the space of possible strategies should lead to such errors.
a token from a pile on each count. The remaining addend is          Though children did sometimes answer problems
counted out in the same manner. The child then proceeds to          incorrectly, the authors argued that these errors did not
count off each finger/token that she has previously                 represent the sort of conceptual mistakes one would assume
enumerated. For example, a protocol for the problem “2+3”           children would make if they were randomly exploring the
might read: “one, two (while raising two fingers), one, two,        space of possible strategies.
three, (while raising three more fingers), one, two, three,
four, five (while counting the raised fingers). Five.” Crucial      The SCADS Model (Shrager and Siegler, 1998)
to our later modeling, this protocol can be reframed in terms       In the years following this study, Siegler and colleagues
of previously learnt procedures. The first step is a „give-a-       built several computational models to explain their data,
number‟ task where the number to be given is one of the             culminating in SCADS (Strategy Choice And Discovery
addends. The second step is the same, but targeting the             Simulation), which posits initial knowledge of the „sum‟
second addend. Finally, to produce the answer, a „how               strategy, a retrieval system for recalling answers based on
many‟ task is performed on the fingers/tokens produced by           associative learning, a module that proposes new strategies
the previous two tasks.                                             and another module that filters out proposals that do not
                                                                    meet criteria assuring their adequacy.
   After prescreening sessions where the children‟s                    SCADS captures some aspects of the successive
knowledge of the sum strategy was verified, the children            emergence of strategies shown in the behavioral data.
were asked to solve addition problems across several                However, the transitions in learning are far more rapid than
sessions. The children predominantly used the „sum‟                 in the empirical data, and no account is given for how
strategy at first, but adapted their procedures over time,          children would acquire the posited metacognitive filtering
generally moving to approaches that increased accuracy              mechanism. It is this gap that we attempt to address.
while decreasing time taken. The experimenters coded the
children‟s behavior as falling into one of several discrete
„strategies‟ on a trial by trial basis. At no point was only a      1
                                                                      These transitions are quite gradual on the time scale of the Siegler
single strategy chosen for all problems . Instead, there were       and Jenkins (1989) study, and some participants are already on
„overlapping-waves‟ of strategies. As shown in Figure 1b,           their way into adopting these strategies at the outset of the study,
the distribution of strategies changed quite slowly, though in      but they are easily seen in aggregate data over longer time periods
the last block of trials “challenge” problems were                  (Svenson & Sjöberg, 1983).
                                                                584

                          A                                           B                                         C
         Figure 1: A) The SCADS model B) Behavioral strategy usage data across four blocks of 35 problems, average over 8
        subjects. 60 prescreening trials were also administered but not recorded here. The abrupt change from the third to final
       block was due to a large intervention involving a spike in problem difficulty at the beginning of block 4. C) The biased
       exploration model, with data averaged over 100 runs. The milder strategy transitions are due to the lack of intervention
                                                                     (Shrager & Siegler, 1998; Neches, 1987). Which of these
The Biased Exploration Model                                         two conceptualizations is the better account of human
Our model approaches the problem of strategy evolution               behavior is an empirical question, and the two mechanisms
through the use of a standard reinforcement learning system.         are not necessarily mutually exclusive.
It attempts to do away with the domain specific strategy                The size of the second addend is positively correlated
proposal and filtering modules of SCADS. It avoids                   with error rate when the shortcut-sum strategy is used, and
incorrect strategies because action is biased towards related,       this has been seen as evidence that the strategy was
previously learned, procedures. The key insight arises by            discovered to eliminate redundancy. It has been
breaking down the two main strategy discoveries („shortcut           hypothesized that the increased error rate comes from the
sum‟ and „min‟) into the component steps needed to allow a           child having to hold two numbers in mind (one for the total
new policy to arise from a predecessor.                              count, and another for the count within the second addend).
   For the „shortcut sum‟ this means making two critical             However, our recognition account is also compatible with
exploratory steps away from the existing „sum‟ strategy              this increased error rate, as larger second addends allow
policy. The first is to continue going through the count list        more chances to terminate the counting process, as well as
after reaching the end of the first addend, rather than starting     providing more time to forget the problem, thus lowering
the count over at one for the second addend. The seco nd is          the chance of recognizing the answer.
to stop going through the count list once the correct numeral            The second transition, from „shortcut sum‟ to „min‟
is uttered. This second step can be seen as relying on               involves (a) skipping the counting out of the addend chosen
problem specific knowledge, but avoiding reliance on recall          to be dealt with first and (b) choosing the larger of the
by replacing it with an easier recognition problem whereby           addends as the first one to deal with. Skipping the counting
the child merely has to stop counting when the value                 of an addend can be seen as a sort of retrieval, wherein it is
reached feels like it is correct.                                    the result of the subtask that is recalled rather than the
   Exploration of this shortcut sum strategy can take place          answer to the entire problem. This subtask structure is part
without assuming there is uniform exploration across all             of the „sum‟ strategy, which contains two instances of the
possible states and actions. We propose that the previously          preexisting „give-a-number‟ task, first learned prior to
learnt counting procedure gives children a tendency to               encountering addition. Thus part (a) of the „min‟ strategy
continue counting even when the first addend is reached .            comes about by starting with the „shortcut sum‟ strategy, but
Thus, whilst the majority of the time the model chooses the          instead of performing the full „give-a-number‟ task for the
sum strategy, occasionally a latent tendency to perform the          first addend and then counting on, the child retrieves the end
related counting task takes over and an „exploratory‟ step is        state of this subtask and then counts on. Part (b), choosing
made. Critically, this exploratory step speeds up task               the larger of the two addends first, follows quickly from
performance but does not lead to an error.                           random exploration due to the inclusion of time cost in the
   We should stress here that the discovery process proposed         reinforcement learning algorithm: The reward signal
above is very different from prior proposals, which focus on         associated with producing the answer comes sooner in this
realizing the redundancy in having to recount both addends
                                                                 585

case, so the effective reward for dealing with the larger           known as the TD error, is used to update both the critic and
addend first is greater.                                            the actor.
                                                                       All of the knowledge the model has about what actions to
    A
                                                                    execute is stored in 10 two-dimensional tables, one for the
                                                                    state space of each of the 10 possible problems , as shown in
                                                                    Figure 2b. Each table can be imagined as a 6x6 square,
                                                                    where each cell represents a state of the world relevant to
                                                                    solving the specific problem associated with the table. The
                                                                    first dimension represents the number of fingers/tokens
                                                                    currently raised, from 0 to 5 (we only consider addition
                                                                    problems with sums up to 5, though nothing prevents the
    B                                                               extension to larger problems). The second dimension is an
                                                                    echoic buffer that represents the last numeral uttered.
                                                                       Each cell of the table contains 5 values, each representing
                                                                    the propensity towards taking a specific action in that state.
                                                                    There are 5 possible actions: perform a give-a-number
                                                                    subtask on the first addend, perform a give-a-number
                                                                    subtask on the second addend, perform a how-many subtask
                                                                    on the fingers currently raised, raise one more finger and say
                                                                    the next number in the count list, state that your previous
                                                                    utterance is your final answer. The first three actions are
                                                                    referred to as subtasks, since they involve a mediating
  Figure 2: A) The actor-critic architecture as instantiated in     process before affecting the state. When called upon to
  the biased exploration model. The discrepancy between             perform one of the subtask actions, either the end state is
  the critic‟s predicted cumulative value and reality is used       retrieved, or failing that, the whole subtask is carried out
  to update both the critic and the actor. B) The state space       without interruption.
  of the biased exploration model for the problem „two plus            In addition to these actions, the agent tries to retrieve the
  three‟, with arrows representing a sample trajectory for          sum at the start of each problem, and the action selection
  the sum (red) and shortcut sum (green) strategies as well         process described above only occurs when this initial
  as the count list procedure (blue). Each cell contains            retrieval fails. The assumptions made for retrieving this sum
  propensities for executing each of the five possible              (as well as the subtask end states) are taken straight from
  actions (see main text) at that state. The cell in the inset      SCADS. This memory mechanism, the „distributions of
  shows the propensities when in the „two fingers up, just          associations‟ model, learns by accumulating association
  said two‟ state.                                                  strengths between the task at hand and the various possible
Implementation We implemented the model within the                  end states, with the idea being that it will converge to the
actor-critic reinforcement learning architecture. While this        correct answer as this is the most common end state for an
architecture has traditionally been chosen for its relative         agent competent at the task. When called upon to make a
biological plausibility, here its utility comes from the fact       retrieval, a threshold is stochastically set and compared to
that an actor-critic model learns by modifying the current          each association strength. If no associations are higher than
policy. This feature limits how drastically a single learning       this threshold, then the retrieval fails to return an answer. If
step can affect the behavior of the agent (Sutton & Barto,          multiple associations are above the threshold, the retrieved
1998). This is important in part because it prevents the large      association is randomly chosen from this set (Siegler &
number of errors typically associated with reinforcement            Shrager, 1984).
learning. Since the initial policy (the explicitly taught sum          We give the model the initial „sum strategy‟ policy by
strategy) is accurate, the model avoids a big change unless it      looking at all of the states encountered when executing this
consistently outperforms this existing solution.                    strategy, and setting the model‟s action preferences in these
   As shown in Figure 2a, the actor-critic model consists of        states to be consistent with the actions taken by the strategy.
two main parts: the actor who selects actions to perform            A weak preference for actions consistent with proceeding
based on the current state, and the critic who predicts the         through the count list while putting up fingers is also
expected cumulative reward of the actor at that state. At           encoded. As noted above, this is critical to the discovery of
each time step an action is selected by treating the actor‟s        the shortcut-sum strategy in the model.
action propensities as probabilities (via the softmax                  The model was trained on problems randomly sampled
function). The action is performed, which modifies the state        from cases where the sum was no greater than 5 and
of the agent and produces some reward value. The critic is          averaged over 100 runs. Since a tabular version of the
then able to see whether or not its prediction was better or        Actor-Critic architecture was used, the top-level policy
worse than expected by comparing it against this actual             information (which subtask or primitive action to select)
reward plus its expectation at the new state. This signal,          about each addition problem was learnt independently.
                                                                586

However, the give-a-number subtasks were shared across              then sum‟ strategy has not been reported in children to our
problems (for example, there was a single give-3 subtask,           knowledge, instances might have been lumped together with
which could be carried out by iterating up to 3 or by               the sum strategy, based on their operational definition of the
producing 3 fingers all at once). While strategy learning           sum strategy as putting up fingers for each addend (agnostic
may have a degree of problem specificity, as in the current         as to whether they are simultaneously counted) and then
model, sharing across specific problems seems likely, and           counting them together. On this basis, we lump our model‟s
some proposals on how to do so are put forward in the               data for the „retrieve larger, then sum‟ strategy together with
discussion section.                                                 the prototypical sum strategy for comparison with the
                                                                    microgenetic study data.
                           Results
The primary concern of this article was to demonstrate the          Error Analysis Both the kind and quantity of the model‟s
viability of an exploration-based model of strategy                 errors fell within the bounds of a typical child in Siegler‟s
discovery in addition. Following previous work (Shrager &           microgenetic study or a cross -sectional study covering
Siegler, 1998), we focus on the qualitative fit to the pattern      similar addition problems (Siegler & Jenkins 89, Svenson &
of strategy use as a function of problems encountered. A            Sjöberg 83). The error rate averaged across 100 trials of
principal claim of the model is that avoidance of implaus ible      each problem type was 13.4%, compared to 15% in the
errors can occur without metacognitive filters, so we also          Siegler study. The problems Shrager and Siegler had
examine the model‟s errors, including the overall rate and          assumed went hand in hand with trial and error learning,
types of errors. Whilst preferable, quantitative assessment of      such as counting out a single addend twice, were also
model fit is not possible, as detailed raw data is not              absent. This was determined by examining each erroneous
available (Siegler, personal communication).                        trial and summing across those where the model made
  To ensure a valid comparison between our model and the            identical steps. Categorizing each unique error would be
available data, our model was trained for the same number           quite time prohibitive, but over 50% of errors occurred in a
of trials as the human participants (4 blocks of 35 trials). We     small number of unique action sequences. Of this group, the
trained the model for 2 preliminary blocks (labeled blocks -        vast majority of the errors were in retrieval, with a failure to
1 and 0 in Figure 1c) prior to this to account for the              inhibit counting beyond the correct answer being the only
prescreening trials the participants received.                      other significant category of errors .
                                                                       The retrieval errors have strong empirical support, and
                                                                    occur in our model precisely because its retrieval system is
Strategy Distribution Dynamics Our model‟s strategy                 very closely based on the existing literature (Siegler &
choices over time are shown in Figure 1c, where the number          Shrager, 1984; Siegler & Shipley, 1995). The failures to
of correct trials is plotted for each strategy for each             inhibit counting, or „count on‟ errors, are a consequence of
successive block of 35 trials. Since strategies are not             our approach. Our theory relies on children taking steps in
explicitly represented anywhere in our model, the action            line with procedures related to, but different from, the task
sequence for each trial was examined to specify the strategy.       at hand, specifically the count-list procedure. Occasionally
We omit from figure 1 strategies which never achieved a             the model fails to stop counting upon reaching the sum,
usage rate greater than 5%. Additionally, the min strategy is       which is precisely what is happening in the count on errors.
discovered around the same time as in the study, which co -         Whilst the exact frequency is not reported, Siegler and
occurs with dropping usage of the sum strategy. In the              Jenkins (1989) themselves report that children occasionally
study, there is an abrupt change from the third to final            count past the correct answer. One strong empirical
block; this was due to the inclusion of challenge problems          prediction of our model is that thes e errors should
(not yet addressed in our simulations) at the beginning of          occasionally occur and be indicative of a child in the early
block 4.                                                            stage of learning, before the shortcut sum strategy is
   Some of the model‟s solutions did not fit into one of the        consistently accurate.
preexisting strategies, but played a significant role in the
usage dynamics of the model. Specifically, a strategy                                         Discussion
emerged whereby the larger addend was retrieved (i.e. by            We set out to explore how the problem of strategy discovery
recalling the end state of its give-a-number subtask), but          might be solved without a metacognitive filter, while
then the rest of the solution followed the steps of the sum         avoiding a high error rate and approximating the pattern of
strategy. This „retrieve larger, then sum‟ strategy (which          change in strategy use observed in Siegler and Jenkins,
occurred on 13% of trials averaged across the six blocks)           1989. The biased exploration model showed this to be
played a crucial role in setting up the order dependence later      possible and additionally demonstrated that strategies can be
needed in the min strategy. Having order dependence                 composed by assembling parts on the fly, rather than being
develop here solves the problem previous trial-and-error            selected as units at the start of the problem. This allows fine
accounts have had where min discovery relied on first               grained variations in strategies to be used, and predicts that
discovering the „count from first‟ strategy, which is rarely        such variation, such as the „retrieve larger, then sum‟
used in children (Neches, 1987). While the „retrieve larger         strategy, will be seen in behavioral data.
                                                                587

          Distinguishing our account from that of the               metacognitive       and      associative     mechanisms.
SCADS model and investigating the extent to which                   Developmental Review, 17(4), 462-489.
metacognition plays a role in the discovery process will be       Davidson, K., Eng, K., & Barner, D. (2012). Does learning
another focus of our future work. One area where the                to count involve a semantic induction?. Cognition, 123(1),
models make distinct predictions is in the rationale behind         162-173.
the use of the shortcut sum strategy. The SCADS model             Neches, R. (1987). Learning through incremental refinement
claims that children track the total while also counting out        of procedures. Production system models of learning and
the second addend, while our account relies on habitually           development, 163-219.
counting on until the sum is recognized, avoiding the need        Shrager, J., & Siegler, R. S. (1998). SCADS: A model of
to keep track of the second addend. This may be amenable            children's strategy choices and strategy discoveries.
to empirical exploration. Self-reports might also be used to        Psychological Science, 9(5), 405-410.
differentiate these accounts, but we stress that we do not        Siegler, R. S., & Shrager, J. (1984). A model of strategic
claim children do not eventually discover a rationale for           choice. Origins of Cognitive Skills. Hills dale, NJ:
their actions. Our claim is only that they need not do so           Lawrence Erlbaum.
before the actions themselves emerge.                             Siegler, R. S., & Jenkins, E. (1989). How children discover
          Another area for future work will be to address the       new strategies. Lawrence Erlbaum Associates, Inc.
problem-specific representations of our current model and to      Siegler, R. S., & Crawley, K. (1994). Constraints on
explore the consequences of this for the model‟s predictions.       learning in nonprivileged domains. Cognitive Psychology,
Sharing information between problems might simply                   27(2), 194-226.
accelerate the learning process, but more fundamental             Siegler, R. S., & Shipley, C. (1995). Variation, selection,
changes are also possible. For example, sharing could               and cognitive change. Developing cognitive competence:
increase certain errors due to confusion of one problem with        New approaches to process modeling, 31-76.
another, which would change the pressures that lead to            Sutton, R. S., & Barto, A. G. (1998). Reinforcement
strategy discovery.                                                 learning: An introduction (Vol. 1, No. 1). Cambridge:
          Another approach we are exploring is to let a             MIT press.
neural network control the policy across all of the problems      Svenson, O., & Sjöberg, K. (1983). Evolution of cognitive
(in this case, the problem state would be represented as an         processes for solving simple additions during the first
input feature vector), as this could allow a more nuanced           three school years. Scandinavian journal of psychology,
sharing of discovery information to emerge (it is possible to       24(1), 117-124.
see at least some versions of a table-driven model as an
alternative implementation of this neural-network based
approach).
          Going forward, we plan to extend our model to
account for another stream of evidence that has previously
been used to support the notion of metacognition: the
recognition of never-before seen strategies. Children that
have been shown the min strategy before discovering it still
rate it as better than an incorrect strategy (Siegler &
Crawley, 1994). While this has previously been taken as
support for the proposed metacognitive filter, we suggest
that the biased exploration model can account for this data
as well by using the agent‟s value function to evaluate novel
strategies. Such an extension is indicative of our overall
goal with this model: to set up a new foundation for self-
guided learning that will allow a rethinking of the role of
metacognition in strategy discovery.
                    Acknowledgments
Thanks go out to the whole Parallel Distributed Processing
Lab at Stanford, for their support throughout the research
process and to Robert Siegler and the reviewers for useful
comments.
                        References
Crawley, K., Shrager, J., & Siegler, R. S. (1997). Strategy
   discovery as a competitive negotiation between
                                                              588

