UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Writer recognition in cursive eye writing: A Bayesian model
Permalink
https://escholarship.org/uc/item/9sg2j24h
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Chanceaux, Myriam
Rynik, Vincent
Lorenceau, Jean
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                    Writer recognition in cursive eye writing: A Bayesian model
                                 Myriam Chanceaux (myriam.chanceaux@upmf-grenoble.fr)
                                         Univ. Grenoble Alpes, LPNC, F-38000, Grenoble, France
                                                  CNRS, LPNC, F-38000 Grenoble, France
                                              Vincent Rynik (vincent.rynik@gmail.com)
                                         Univ. Grenoble Alpes, LPNC, F-38000, Grenoble, France
                                                  CNRS, LPNC, F-38000 Grenoble, France
                                              Jean Lorenceau (jean.lorenceau@upmc.fr)
                                          Laboratoire des Systèmes Perceptifs, CNRS UMR 8248,
                                        Département d’études cognitives, Ecole normale supérieure
                                                     29, rue d’Ulm, 75005, Paris, France
                                            Julien Diard (julien.diard@upmf-grenoble.fr)
                                         Univ. Grenoble Alpes, LPNC, F-38000, Grenoble, France
                                                  CNRS, LPNC, F-38000 Grenoble, France
                               Abstract                                   turns out to have a low communication throughput compared
                                                                          with virtual keyboard-based systems, the production of ar-
   Using a novel apparatus coupling a visual illusion with an eye
   tracker device, trained participants are able to generate smooth       bitrary trajectories would potentially help patients conserve
   pursuit eye movements, even without a target to follow. This           artistic and self-expression capabilities longer. Eye writing,
   allows them to perform arbitrary continuous shapes, and, for           and more precisely the recording of eye produced trajecto-
   instance, write letters with their eyes. In a previous study,
   based on data from a single writer (author JL), we developed           ries, also provides an opportunistic window into the state and
   and tested a Bayesian computational model – the BAP-EOL                evolution of motor capabilities in patients and, by proxy, the
   model – able to simulate character recognition. In the present         state and evolution of their disease.
   study, data from different writers provide the opportunity to
   study the signal characteristics of eye-written letters. More             However, since eye writing is such a novel object of study,
   precisely, we extend the model to perform writer recognition.          not much is known about the motor processes involved in
   Experimental results, and high performance we obtained, show           preparing and performing letter traces with the eyes, and
   that eye writing is as writer specific as handwriting is, and that
   motor idiosyncrasies are present in eye-written letters.               the signal characteristics of resulting trajectories. Although
   Keywords: Bayesian modeling; writer recognition; eye writ-             handwriting is widely studied (see Plamondon and Srihari
   ing                                                                    (2000) for a review), it is not known, for instance, whether
                                                                          letters written with the eyes have stable shapes across repeti-
                          Introduction                                    tions, whether they have shapes similar to letters traced with
A novel apparatus was recently designed that, in essence, al-             other effectors (i.e., whether motor equivalence carries over
lows users to write with their eyes (Lorenceau, 2012). It is              to the eyes as a writing effector), or whether they have shapes
commonly admitted that a target is needed to generate smooth              that allow writer recognition (i.e., whether eye writing con-
eye movements, even if previous exceptions have been re-                  tains recognizable user idiosyncrasies, as handwriting does).
ported (Madelain & Krauzlis, 2003). Thanks to the use of a                This last issue is the main topic of the study we present here.
perceptual illusion, smooth pursuit control of the eyes even                 To answer this question, we developed and simulated a
in the absence of a visual target to follow is possible. An               Bayesian model of writer recognition. It is an extension of the
eye tracking device records the user’s eye movements, which               BAP-EOL model (for Bayesian Action-Perception for Eye
can then be visualized on a screen. Using this system requires            On-Line), a Bayesian model that we used previously for char-
some training, as the visual illusion takes some time to be get-          acter recognition in the context of eye writing (Diard, Rynik,
ting used to. Initially, users usually only perceive the illusion         & Lorenceau, 2013). The BAP-EOL model was itself an
and generate smooth pursuit movements for short durations,                adaptation of the BAP model (Bayesian Action-Perception)
so that the resulting trajectories are heavily contaminated by            of reading and writing handwritten letters (Gilet, Diard, &
blinks and spurious saccades. With training however, some                 Bessière, 2011). Thanks to the flexibility of Bayesian infer-
users become able to generate long, smooth trajectories in                ence, character recognition and writer recognition turn out to
any desired shape.                                                        be similar tasks. Then, writer recognition was extended to
   An obvious application, and our long term objective, is to             take as input the letters of a complete word, instead of iso-
provide this system to motor impaired patients, for instance              lated letters, using a sensor fusion approach.
patients with amyotrophic lateral sclerosis (ALS, also known                 In the rest of this paper, we first recall the structure and
as Lou Gehrig’s disease). Even if eye writing, in this manner,            main features of the BAP-EOL model, and introduce its ex-
                                                                      2014

tensions for writer recognition, based on single letters first,
                                                                                           Original Trace                            Via Points
and on sequences of letters second. We then present an exper-                              Filtered Trace                            Filtered Trace
iment with three different writers, its results and their analy-                5                                        5
sis, in terms of performance and information accumulation.
                                                                                0                                        0
                     BAP-EOL Model                                            −5                                       −5
Trajectory analysis                                                           −10                                      −10
A preliminary step consists in extracting, from raw data, dif-            Y   −15                                  Y   −15
ferent variables to summarize the signal.                                     −20                                      −20
   For each input trace, i.e., each written character, recorded
                                                                              −25                                      −25
data is a series of x, y coordinates and, from these, veloci-
ties ẋ, ẏ are computed using a finite difference approximation.             −30                                      −30
Input traces are noisy, of course, and a smoothing filter (bi-
                                                                              −35                                      −35
nomial filter of order 20) is applied to position and velocity                 −20     0         20           40        −20      0         20         40
                                                                                             X                                         X
dimensions. At the beginning of most traces, saccades are ob-
served before the beginning of the letter proper, that is to say,
before smooth pursuit eye movement begins. To remove these            Figure 1: Left: input trajectory with initial and end seg-
saccades, another filter is applied, based on an acceleration         ments filtered out because of intrusive saccades (gray por-
threshold, at the beginning and end of each trace: for the first      tions). Right: via-points positions extracted from the filtered
30 points (resp. last 10 points), if the observed acceleration        trace (velocity information is not shown).
exceeds a certain threshold, empirically fixed at 0.5 unit/s2 ,                  1:25 C1:25 C1:25 C1:25 S S A L). For lack of space,
                                                                      i.e., P(C∆x        ∆y     ẋ     ẏ    x y
all points before this peak (resp. after) are deleted. This effi-
                                                                      we only present here its main features; technical details are
ciently removes most intrusive saccades in the signal.
                                                                      provided in a previous paper (Diard et al., 2013). The BAP-
   We summarize the filtered traces by a sequence of via-
                                                                      EOL model was extended to introduce writer identity W , in a
points (Gilet et al., 2011). We choose, as via-points, the
                                                                      manner similar to the original BAP model of handwritten let-
points of the trace where either x-velocity or y-velocity is ze-
                                                                      ter production and perception. We define W as a set of values
roed (or both); the first and last point of the trace are also
                                                                      {w1 , w2 , . . .}, one for each possible writer.
defined as via-points. For each via-point we memorize the
                                                                          The model is formally defined by its joint probability dis-
displacements ∆x, ∆y (relative to the preceding point) and ve-                          1:25 C1:25 C1:25 C1:25 S S A L W ). In order to
                                                                      tribution P(C∆x          ∆y   ẋ    ẏ     x y
locities ẋ, ẏ: the k-th point (k > 1) is associated with posi-
           k , Ck and velocities Ck and Ck (the first via-point       obtain a computationally tractable model, discrete domains
tions C∆x                           ẋ       ẏ
                 ∆y                                                   of suitable precision are chosen for each variable in the trace
always has position (0, 0), and via-points from the last to the       description: via-point relative positions C∆x    k , Ck have 81 pos-
                                                                                                                            ∆y
25th have special values indicating trace termination). Rel-
                                                                      sible values in the range [−40, 40], via-point velocities Cẋk and
ative positions were used, instead of absolute positions, so
                                                                      Cẏk have 21 values in the range [−10, 10], letter width Sx and
that characters could be written at any location on the dis-
                                                                      height Sy have 51 possible values in the range [0, 50] (not to
play and via-point information recorded as the letter was be-                          k , Ck units), and, finally, the proportion of high-
                                                                      scale with C∆x
ing traced (absolute positions would require some size nor-                                 ∆y
                                                                      frequency components in the eye movements A has 31 possi-
malization process, which is only possible after the trace is
                                                                      ble values in the range [0, 30].
completed). The system treats at most 25 via-points in a tra-
                                                                          Furthermore, conditional independence hypotheses are
jectory, which is more than enough for the current application
                                                                      chosen so as to break down the dimensionality of the joint
(min 3, max 13, mean 5.9 in the learning database). Figure 1
                                                                      probability distribution. We define:
shows an example of a trajectory before and after filtering,
and the corresponding via-points.                                                       1:25 1:25 1:25 1:25
                                                                                     P(C∆x  C∆y Cẋ Cẏ Sx Sy A L W ) =
   After a trace is completed, other variables are added to the
                                                                                           P(L)P(W )
trajectory summary: the letter width Sx , its height Sy , and a                               1:25           1:25
variable that characterizes the density of the signal A, i.e., the                         P(C∆x   | L W )P(C∆y   | L W)                                   (1)
proportion of high-frequency components in eye movements.                                  P(Cẋ1:25   |   L W )P(Cẏ1:25     | L W)
This last variable is based on the Fourier Transform of the                                P(Sx | L W )P(Sy | L W )P(A | L W ) ,
signal in both x and y dimensions.
                                                                      with C∆x1:25 a shorthand for the sequence C1 ,C2 , . . . ,C25 .
                                                                                                                 ∆x ∆x           ∆x
Probabilistic model of isolated letters                                  In this decomposition of the joint probability distribution,
The BAP-EOL model is a probabilistic model of letters,                the terms P(L) and P(W ) are prior probability distributions
where, in a nutshell, each letter l (in the set L of all considered   over letters and writers, and are associated with uniform dis-
letters, in our case, letters ‘a’ to ‘z’) is represented by a prob-   tributions, to represent ignorance of the frequency of letters
ability distribution over all dimensions introduced previously,       and no preference for any writer. The next four terms encode
                                                                   2015

the geometrical form of the trajectory, using a further decom-                Writer recognition from sequences of letters
position (shown below for ∆x positions, but it is similar on                  In this task, input is a sequence T 1 , T 2 , . . . , T k of written
other dimensions):                                                            traces, and we compute the probability distribution over writ-
                                        25                                    ers:                                                 !
      1:25                 1                      i     i−1                                                  k
 P(C∆x     | L W ) = P(C∆x   | L W ) ∏ P(C∆x         | C∆x  L W) .    (2)
                                       i=2                                                P(W | T 1:k ) ∝ ∏      ∑i P(T i | Li W )     .       (4)
                                                                                                            i=1  L
Each of the terms is associated with a conditional probabil-
                                                                              Combining Eqs. (3) and (4), we obtain:
ity table, whose parameters are identified from a learning
database.                                                                                                        k
   Finally, the parameters of the last terms P(Sx | L W ),                                      P(W | T 1:k ) ∝ ∏ P(W | T i ) .                (5)
                                                                                                                i=1
P(Sy | L W ) and P(A | L W ) are also learned from data, but
these terms are associated to Gaussian probability distribu-                  In other words, recognizing the writer given a sequence of
tions (with proper care taken to approximate these distribu-                  letters amounts to a sensor fusion of writer identification tasks
tions over discrete, finite domains). Concerning P(Sx | L W )                 for isolated letters, i.e., probability distributions about writer
and P(Sy | L W ) they are considered, for simplicity, indepen-                identity given each letter are simply multiplied together.
dent of P(C∆x 1:25 | L W ) and P(C1:25 | L W ) conditionally to the
                                     ∆y
learned data.                                                                                              Method
   For simplicity, in the remainder of the paper, we note T the               Participants
conjunction of all probabilistic variables involved in the de-                Three participants (one woman) produced a set of traces. One
                             1:25 ,C1:25 , . . . , A. With this notation,
scription of a trace, i.e., C∆x       ∆y                                      is author JL, and the two others were also involved in the
the structure of the probabilistic model simply becomes:                      project during data collection (JM and MV). All participants
               P(T L W ) = P(L)P(W )P(T | L W ) .                             were French native speakers and reported having normal or
                                                                              corrected to normal vision. After a training phase consisting
In other words, our model describes the most likely shapes                    of practicing how to move their eyes using smooth pursuit
and sizes of traces, for each letter and each writer, in a prob-              with the illusion, they were able to produce data.
abilistic manner.
                                                                              Procedure and Apparatus
Writer recognition from isolated letters                                      In order to obtain voluntary smooth pursuit eye movements,
Once the parameters of all terms in the joint probability distri-             the classic perceptual illusion of “reverse-phi motion” is pre-
bution definition are set, Bayesian inference is used to solve                sented to the participant (Anstis, 1970). The visual stimulus
the task at hand. We are here interested in writer recognition,               consists of a set of pairs of visual patterns presented in strobe
that is to say, given an input trace, identify the writer that pro-           and staggered in space with their polarity contrast reversed si-
duced it (but the letter is unknown). In probabilistic terms,                 multaneously. The perceived motion is the inverse of the shift
this is solved by computing:                                                  of direction. During the illusion the whole display seems to
                                                                              move in the same direction as the eye. This illusion allows
                     P(W | T ) ∝ ∑ P(T | L W ) .                      (3)
                                   L                                          the oculomotor system to generate smooth pursuit eye move-
                                                                              ments without visual target (see Lorenceau (2012) for more
Probabilistic model of sequences of letters                                   details on this principle).
We now extend the previous model so as to take into account                      After a calibration phase, the illusion was displayed. Dur-
sequences of written traces T 1:k = T 1 , T 2 , . . . , T k , as would        ing the presentation of the illusion, participants were moving
be obtained from a written word. We directly consider a se-                   their eyes, writing letters. After each letter, participants had to
quence of isolated letters and do not consider the segmenta-                  blink to indicate segmentation and start writing another letter.
tion problem, since, to this day, very few “eye writers” are                  There was no feedback during the record.
expert enough to produce complete words in a single trace,                       A head-mounted camera EyeTechSensor equipped with
without blinking.                                                             CCD for ocular tracking (Pertech company) was used to
   The probabilistic model is extended and becomes a naive                    record eye positions. The eye tracker had a sampling rate
Bayesian fusion model where letters are assumed to be inde-                   of 75 Hz. The illusion was presented on a monitor with a
pendent given the writer W , and the writer is assumed to be                  screen size of 1024*768 pixels. Stimulus presentation was
the same for all letters:                                                     controlled with the homemade Jeda software. Eye move-
                                       k                                      ments were recorded from the left eye.
         P(T 1:k L1:k W ) = P(W ) ∏ P(Li )P(T i | Li W ) .
                                     i=1                                                                    Results
P(W ) and all P(Li ) are assumed to be uniform probability                    Learned parameters of the probability distributions
distributions, as previously. Each term P(T i | Li W ) is also                The participants produced a database of 933 characters (245
structured and defined as for isolated letters.                               for author JL, 328 for JM, 360 for MV). The characters are
                                                                          2016

letters from ‘a’ to ‘z’, with an average for each letter of 9.8            Confusion Matrix, writer=JL   Confusion Matrix, writer=JM       Confusion Matrix, writer=MV
                                                                                                     1                                 1                             1
samples for JL (min 6, max 10), 13.12 for JM (min 10, max                       a                              a                                a
17) and 14.16 for MV (min 9, max 19). Data collection was                       b                              b                                b
a result of participants’ practice sessions, without an explicit                c                    0.9       c                       0.9      c                    0.9
instruction of systematic alphabetic production.                                d                              d                                d
                                                                                e                              e                                e
   We computed the parameters of the probability distribu-
                                                                                f                    0.8        f                      0.8       f                   0.8
tions using a cross-validation method: for each writer a set
                                                                                g                              g                                g
of 26 letters (one complete alphabet) was randomly selected                     h                              h                                h
as the test database (on which the recognition task perfor-                      i                   0.7         i                     0.7       i                   0.7
mance was assessed), the remaining letters were the learning                     j                               j                               j
database. Thus the test database was of size 3*26, and the                      k                              k                                k
                                                                                 l                   0.6         l                     0.6       l                   0.6
learning database of size 933-3*26. This random procedure
                                                                               m                              m                                m
was repeated 100 times to ensure that each letter was both in
                                                                                n                              n                                n
the learned and the tested database at least once.                              o                    0.5       o                       0.5      o                    0.5
   All results presented below are the average measures over                    p                              p                                p
these 100 repetitions.                                                          q                              q                                q
                                                                                r                    0.4        r                      0.4       r                   0.4
   For each of these measurement, the parameters of the prob-
                                                                                s                              s                                s
ability distributions of Eqs. (1) and (2) were learned: for in-
                                                                                t                               t                                t
stance, the terms about via-point relative positions and veloc-                 u                    0.3       u                       0.3      u                    0.3
ities are Conditional Probability Tables, implemented using                     v                              v                                v
Laplace succession laws (Gilet et al., 2011). They are a vari-                 w                              w                                 w
ant of histograms that start from a uniform distribution and                    x                    0.2       x                       0.2      x                    0.2
converge, when data accumulates, to a histogram. To palliate                    y                              y                                y
                                                                                z                              z                                z
the lack of experimental data compared to the number of free
                                                                                                     0.1                               0.1                           0.1
parameters to identify, a Gaussian filter was applied in order
to smooth the obtained distributions (in effect, simulating a
larger database with additional traces similar in shapes to the                                      0                                 0                             0
                                                                                   JL JM MV                        JL JM MV                        JL JM MV
ones available). The Gaussian filters parameters are of order                                                      Confusion Matrix
                                                                                                                                       1
15 and variance 2 for relative positions and of order 7 and                                                JL
                                                                                                                                       0.9
                                                                                                                                       0.8
variance 1 for velocities.                                                                                                             0.7
                                                                                                                                       0.6
   With these parameters the BAP-EOL model becomes op-                                                    JM                           0.5
                                                                                                                                       0.4
erational, and can be used to perform automatic writer recog-                                                                          0.3
                                                                                                                                       0.2
nition.                                                                                                   MV
                                                                                                                                       0.1
                                                                                                                                       0
                                                                                                              JL         JM         MV
Writer recognition: experimental results
                                                                      Figure 2: Upper panel:Confusion matrices for writer recog-
From isolated letters We performed writer recognition a
                                                                      nition, for each letter and each writer. Each row is the proba-
hundred times for each letter and each writer. Averaging re-
                                                                      bility distribution over writers, computed from Eq. (3), av-
sults over the 100 repetitions, we obtained three confusion
                                                                      eraged over 100 experimental repetitions. Bottom panel:
matrices of size 26*3. They are shown Figure 2 (upper panel).
                                                                      Global confusion matrix.
On these matrices we observe that some letters and some
writers are more easily recognized. For example, input from
writer MV is efficiently recognized, whatever the written let-        From sequences of letters To test this model over se-
ter. On the other hand, writer JL is harder to recognize, with        quences of letters, a dataset of words from a French corpus
a low recognition rate for letters like ‘j’ or ‘t’. This could be     Lexique (New, Pallier, Brysbaert, & Ferrand, 2004) was used.
due to the fact that the learning database is smaller for JL than     2126 words were selected, all were singular nouns (length 3
for MV (245 vs. 360 samples). Another explanation relies on           to 12 letters), with a mean printed frequency greater than 15
letter similarity and distinguishability: for instance, JL usu-       occurrences per million.
ally writes the letter ‘s’ in a very particular way, and they are        We created input trajectories for words by randomly ex-
easy to recognize, whereas JL’s ‘t’s or ‘j’s are not character-       tracting from the database, for a given writer, trajectories for
istic.                                                                the letters of that word, and repeating that procedure 100
   Further averaging over letters, we obtained a 3*3 confu-           times. We obtained a test database of 2126*100*3 words.
sion matrix (Figure 2, bottom), whose diagonal values repre-          As above, for each letter of each word, a probability distri-
sent correct recognition rates of writers, independently of the       bution over writers is computed, and these probability distri-
written letter: these global recognition rates are 70.00 % for        butions are then gradually multiplied to obtain writer recog-
writer JL, 73.82 % for writer JM and 90.54 % for writer MV.           nition from the first two letters, from the first three, etc, until
                                                                  2017

                                          35                                                                                                                          35
                                          30                                                                                                                          30
                                          25                                                                                                                          25
                                          20                                                                                                                          20
                                      y                                                                                                                           y
                                          15                                                                                                                          15
                                          10                                                                                                                          10
                                           5                                                                                                                           5
                                           0                                                                                                                           0
                                               0       20    40   60         80                100        120   140      160    180                                        0                                20      40           60                80          100      120      140
                                                                                   x                                                                                                                                                   x
                                 P(W|Trajectory i [W= JL])                                     P(W|Trajectories 1:T [W= JL])                            P(W|Trajectory i [W= JM])                                                                  P(W|Trajectories 1:T [W= JM])
                                                                       1                                                              1                                                                                      1                                                             1
                                  s                                                                  s                                                        p                                                                                         p
                                                                       0.8                                                            0.8                                                                                    0.8                                                           0.8
                                  t                                                                  st                                                       i                                                                                         pi
                    Trajectory                                                    Trajectory                                                     Trajectory                                                                           Trajectory
                                                                       0.6                                                            0.6                                                                                    0.6                                                           0.6
                                  y                                                              sty                                                          a                                                                                      pia
                                                                       0.4                                                            0.4                                                                                    0.4                                                           0.4
                                  l                                                             styl                                                          n                                                                                     pian
                                                                       0.2                                                            0.2                                                                                    0.2                                                           0.2
                                 e                                                             style                                                          o                                                                                    piano
                                                                       0                                                              0                                                                                      0                                                             0
                                      JL            JM      MV                                             JL    JM        MV                                     JL            JM    MV                                                                      JL      JM    MV
                                                   Writer                                                       Writer                                                         Writer                                                                                Writer
Figure 3: Examples of writer recognition based on words: input word “style” by writer JL on the left, input word “piano” by
writer JM on the right. Upper panels: input trajectory for the words, obtained by sequencing samples of letters that constitute
the word (letters not scaled; notice that participants did not dot the ‘i’ nor cross the ‘t’). Bottom panels: confusion matrices
obtained by writer recognition, from each isolated letter (left sub-panels), and aggregated for growing prefixes, as letters are
fed to writer recognition and Eq. (5) is applied (right sub-panels).
writer recognition from the complete word is obtained. Two
examples are displayed Figure 3, for the input word “style”                                                                                                                                                1
by writer JL, and the word “piano” by writer JM.
   On these examples, one can see that even if individual let-                                                                                                                                            0.8
                                                                                                                                                                               Average Recognition Rate
ters do not always yield correct writer recognition, the fusion
model provides a more reliable estimate of writer identity. Of                                                                                                                                            0.6
                                                                                                                                                                                                                                                                                                 Writer JL
                                                                                                                                                                                                                                                                                                 Writer JM
course this is not always the case, with counterexample words                                                                                                                                                                                                                                    Writer MV
sometimes incorrectly recognized. This mostly happens for                                                                                                                                                 0.4
words that contain several difficult letters. For instance, “ac-
cent” by JL is sometimes recognized as being written by JM,                                                                                                                                               0.2
but still 82.09 % of the times correctly recognized as JL’s.
   Overall results are satisfactory. Averaged on all words and                                                                                                                                             0
                                                                                                                                                                                                                0        1                 2                   3          4            5          6          7
all repetitions, correct recognition rates after the last letter are                                                                                                                                                                                         Number of letters
95.43 % for JL, 98.29 % for JM and 99.92 % for MV. These
results are in line with writer recognition rate in handwritten                                                                              Figure 4: Evolution of the correct recognition rate of writers,
documents (Bensefia, Paquet, & Heutte, 2005). We notice                                                                                      as a function of the number of input letters, averaged for all
that the accuracy is better in term of recognition rate with                                                                                 7-letter words of the test database.
entire words than with only one letter.
   Therefore, we analyzed the evolution of writer recognition
as letters are fed, one by one, to the system. We computed the
average correct recognition rates, as a function of the num-                                                                                 This is shown Figure 5, also for 7-letter words. Recall that
ber of letters. This is shown Figure 4 for 7-letter words. It                                                                                entropy of a discrete probability distribution P(x) is defined
of course starts at chance level (33 %) before the first let-                                                                                as − ∑x P(x) ln P(x). In our case, entropy initially starts from
ter is seen, as the prior probability distribution over writers,                                                                             ln 3 ≈ 1.09 nats, as P(W ) is uniform, and decreases sharply as
P(W ), is uniform. For all writers, correct recognition rate                                                                                 information is gathered and the probability distribution over
then quickly increases, with 2-3 letters being sufficient to ob-                                                                             writers concentrates. Slight differences can be observed be-
tain near final performance.                                                                                                                 tween writers but, overall, 2-3 letters also are sufficient to be
   We also computed the average entropy of probability dis-                                                                                  near final entropy, showing a fast convergence speed of the
tribution over writers, as a function of the number of letters.                                                                              model.
                                                                                                                                          2018

                 1.2
                               Entropy of all distributions for 7−letter words                      recognize writers. Recall that our long term objective is dis-
                                                                                 Writer JL
                                                                                 Writer JM
                                                                                                    ability assessment. Imagine a user that begins to show signs
                  1
                                                                                 Writer MV          of motor deterioration, like micrography for instance. In this
                                                                                                    example, this would affect the letter sizes, which would be-
                 0.8                                                                                come uncharacteristically small. Our system would then not
                                                                                                    recognize the user as the writer anymore, which could raise
       Entropy
                 0.6
                                                                                                    an alarm to the patient’s caregivers. Of course, carefully cal-
                                                                                                    ibrating our model so that it is robust to usual variations in
                 0.4
                                                                                                    eye writing but able to detect such motor deterioration, even
                 0.2
                                                                                                    if there are more writers in the database, would require much
                                                                                                    more data about the use of our system by disabled patients,
                  0                                                                                 than is currently available. However, we believe the writer
                       0   1        2          3          4           5          6           7
                                             Number of letters                                      recognition mechanism we have described here is a promis-
                                                                                                    ing first step in this direction.
Figure 5: Evolution of the entropy of the probability distribu-
tion over writers, as a function of the number of input letters,                                                       Acknowledgments
averaged for all 7-letter words of the test database.                                               This work was supported by the Eye-On-Line ANR Grant
                                                                                                    (ANR-12-TECS-0009-04). We thank MV and JM, who pro-
                                        Discussion                                                  duced and collected trajectories used in this research.
In this paper, we refined the BAP-EOL model to simulate let-                                                                References
ter perception and writer recognition, in the context of writ-                                      Anstis, S. M. (1970). Phi movement as a subtraction process.
ing with the eyes. We showed how Bayesian modeling and                                                Vision research, 10(12), 1411–30.
inference could be used to solve the writer recognition task,                                       Bensefia, A., Paquet, T., & Heutte, L. (2005). Handwrit-
both based on single isolated letters, and on sequences of let-                                       ten document analysis for automatic writer recognition.
ters. We tested our model on a database containing 3 different                                        Electronic Letters on Computer Vision and Image Analy-
writers, and a corpus of several thousands of words. Exper-                                           sis, 5(2), 72–86.
imental results show that the model is quite efficient in the                                       Diard, J. (2009). Bayesian model comparison and distin-
task, especially using the fusion approach with entire words.                                         guishability. In Proceedings of the international conference
   In our experiments, we noted that some letters made writer                                         on cognitive modeling (ICCM 09) (pp. 204–209).
identification more easy, whereas some made it more diffi-                                          Diard, J., Rynik, V., & Lorenceau, J. (2013). A Bayesian
cult, because of similarity in writing styles of some letters be-                                     computational model for online character recognition and
tween writers, as in handwriting. The Bayesian model could                                            disability assessment during cursive eye writing. Frontiers
be extended, however, to include this information. Instead of                                         in psychology, 4(843).
a uniform probability distribution over letters, which, in ef-                                      Gilet, E., Diard, J., & Bessière, P. (2011). Bayesian action-
fect, gives the same weight to all letters, a prior distribution                                      perception computational model: interaction of produc-
reflecting the distinguishability of letters could be used. This                                      tion and recognition of cursive letters. PLoS ONE, 6(6),
could be done by adapting a Bayesian meta-model of the dis-                                           e20387.
tinguishability of models, from previous research in another                                        Lorenceau, J. (2012). Cursive Writing with Smooth Pursuit
domain (Diard, 2009). In a nutshell, given a database, the                                            Eye Movements. Current Biology, 22(16), 1506–1509.
model would test itself and give more weight to letters that                                        Madelain, L., & Krauzlis, R. (2003). Effects of learning on
yield good writer distinguishability, and less weight to letters                                      smooth pursuit during transient disappearance of a visual
that are similar across writers.                                                                      target. Journal of Neurophysiology, 90, 972–982.
   We have presented, in the context of writer recognition, a                                       New, B., Pallier, C., Brysbaert, M., & Ferrand, L. (2004).
first model using sequences of letters. However, in a clas-                                           Lexique 2: a new French lexical database. Behavior re-
sical naive Bayesian fusion approach, we have assumed let-                                            search methods, instruments, & computers: a journal of
ters to be independent, given writer identity W . This could                                          the Psychonomic Society, Inc, 36(3), 516–24.
of course be refined, by introducing knowledge about letter                                         Norris, D. (2006). The Bayesian Reader: Explaining word
frequency in a given language, bigram frequencies, or even                                            recognition as an optimal Bayesian decision process. Psy-
word frequency, or other high-level orthographic, lexical and                                         chological Review, 113(2), 327–357.
semantic information. In a Bayesian framework, introducing                                          Norris, D. (2013). Models of visual word recognition. Trends
such knowledge takes the form of top-down prior probabil-                                             in Cognitive Sciences, 17(10), 517–524.
ity distributions, which can be hierarchically combined. The                                        Plamondon, R., & Srihari, S. N. (2000). On-line and off-
domain of probabilistic visual word recognition is indeed cur-                                        line handwriting recognition: A comprehensive survey.
rently growing along these lines (Norris, 2006, 2013).                                                IEEE Transactions on Pattern Analysis and Machine In-
   Finally, we have shown that the BAP-EOL model is able to                                           telligence, 22(1), 63–84.
                                                                                                 2019

