UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning or Framing?: Effects of Outcome Feedback on Repeated Decisions from Description

Permalink
https://escholarship.org/uc/item/15w6n5j4

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Chen, Yu-Jia
Corter, James

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Learning or Framing?: Effects of Outcome Feedback
on Repeated Decisions from Description
Yuh-Jia Chen (mike_chen@pba.edu)
Rinker School of Business, Palm Beach Atlantic University
West Palm Beach, FL 33401 USA

James E. Corter (jec34@columbia.edu)
Department of Human Development, Teachers College, Columbia University
New York, NY 10027 USA

Abstract
Two experiments document effects of experienced outcome
feedback on risk behavior in a repeated description-based
decision task. In Study 1, participants were initially strongly
risk-seeking in the loss domain, but became less so across 100
repeated trials with outcome feedback. No significant trend
was observed for gain problems.
Participants then
experienced an additional 100 trials of the reflected gain or
loss problem. Trends in risk preferences across these Set 2
trials were similar to those in Set 1, however, initial Set 2
levels of risky-option choices were shifted towards EVmaximization, suggesting a cross-domain effect of prior
experience. Study 2 attempted to distinguish between
reinforcement-learning
and
monetary
reference-point
explanations of these cross-domain effects by “endowing”
participants facing 100 gain (loss) trials with a large starting
loss (gain). Endowment with a large prior gain mimicked the
effects of 100 prior gain trials for loss-domain decisions,
favoring the reference-point account.
Keywords: repeated decisions; decisions from experience; risky
choice; risk-seeking

Behavioral decision research over the past decade has
established that decisions from experience (DFE) differ
from description-based decisions (DBD) in key ways (for
reviews see Hertwig & Erev, 2009; Rakow & Newell,
2010). However, there is a lack of consensus about the
underlying reasons for the description-experience gap (e.g.,
Camileri & Newell, 2009; Hadar & Fox, 2009; Hau, Pleskac
& Hertwig, 2010). In part this may be because in typical
studies of decisions from experience all information about
payoff distributions of the choice options is acquired
through outcome feedback. Thus, comparing typical studies
of DBD and DFE contrasts not only single-trial decisions to
repeated decisions with outcome feedback, the usual focus
of discussion, but simultaneously compares decisions under
risk, with known payoffs and probabilities, to decisions
under uncertainty, with consequences that are initially
unknown (cf. Hadar & Fox, 2009). Thus, simple DBD-DFE
comparisons confound the informational effects of outcome
feedback (i.e., information relevant to estimating
probabilities of outcomes, thus to strategy selection) and the
hedonic effects of experienced outcomes (i.e., possible
reinforcement learning).

Some empirical evidence suggests that repeated
experience with outcome feedback can modify behavior
even in description-based decisions. Yechiam, Barron and
Erev (2005) found that for a description-based decision
problem involving choice between two risky losses, choices
of the riskier option that had slightly higher expected value
(EV) were more common in the “experience” condition
where participants received outcome feedback across 100
trials. Jessup, Bishara, and Busemeyer (2008) studied
behavior in repeated trials of two description-based risky
decision problems in the Gain domain, finding that the
overall proportion of risk-seeking or risk-avoiding choices
differed according to whether the decision maker was given
outcome feedback or not. In a study of probability-matching
behavior where the probabilities of the binary outcomes
were known from the outset, Newell and Rakow (2007)
found that outcome feedback affected the tendency to select
the normative (more likely) outcome.
Several mechanisms could account for effects of
experience on risk preferences. Reinforcement learning
models have been proposed to account for behavior in
repeated decision tasks with initially unknown outcomes
(e.g., March, 1996; Yechiam and Busemeyer, 2005; Erev,
Ert, Roth, et al., 2010). One such account that takes a clear
position on what is learned and how it might generalize is
March’s (1996) proposal that simple reinforcement learning
might impart domain-specific risk attitudes, or generalized
preferences between risky and riskless decision options for
gains and (separately) for losses.
Another possibility is that reinforcement learning
mechanisms act so as to make description-based decisions
more “rational” by encouraging EV-maximization. Such a
shift might occur implicitly, by shifting decision weights to
be more nearly linear with the objective probabilities
(Hertwig, Barron, Weber & Erev, 2004; Jessup et al., 2008;
Hau et al., 2008). Alternatively, effects of experience in
description-based decisions may arise via explicit strategy
choices, if experience promotes more frequent adoption of
EV-maximizing strategies (e.g., Erev & Barron, 2005).
Finally, effects of experience on description-based
decision making might be caused by framing effects as
postulated by Prospect Theory (Kahneman & Tversky,
1979; Tversky & Kahneman, 1992). It is usually assumed

2073

in Prospect Theory that previous outcomes are ignored;
under this default “segregation framing” no changes in risk
preferences across repeated trials are to be expected.
However, under certain task conditions prior gains and/or
losses might be incorporated into the evaluation of a new
problem’s outcomes (“aggregation framing”), thereby
changing an individual’s reference point for the decision
(Kahneman & Tversky, 1979; Redelmeier & Tversky, 1992;
Thaler & Johnson, 1990).
These alternate classes of explanation for possible effects
of experience on DBD tasks are explored in the two
empirical studies and analyses reported below.

facilitate data analysis, trial-by-trial choices were averaged
within 20-trial pseudo-blocks.
Participants in this repeated-trials situation were strongly
risk-seeking for losses. However, the proportion of surething choices changed across the 100 trials: For the three
Loss problems the overall proportion of sure-thing choices
in the initial block of trials was .14, rising to .37 in the final
block. For the three Gain problems the overall trend was
flat, from .49 in the initial block to .48 in the final block.
The trends by individual problem are shown in Figure 1.

STUDY 1
Study 1 was designed to verify if people’s risk
preferences can change over repeated trials with outcome
feedback in a description-based decision task, and if any
observed changes in risk preference would generalize
between the domains of gains and losses.

Method
Materials. Six simple decision problems were used; three
basic problems in the gain domain and corresponding
versions in the loss domain. The three basic gain problems
were: Problem G1 = ($5; $100, .1), Problem G2 = ($9;
$100, .1), and Problem G3 = ($9; $20, .5). So, for example,
Problem G1 offered a choice between receiving $5 with
certainty and a 10% chance of receiving $100. The three
loss versions (L1, L2, and L3) simply substituted losses for
gains.

Figure 1: Proportion of sure-thing choices across blocks of
Set 1 trials, by problem (G1-G3 = Gain domain problems,
L1-L3 = Loss domain problems).

Participants. The participants were 96 undergraduate and
graduate students, recruited by posted flyers or from
courses, and paid for their participation.
Procedure.
The participants responded to decision
problems shown on a computer screen. Each participant
saw 100 trials of one of the Gain (or Loss) problems,
followed by 100 trials of the corresponding Loss (or Gain)
problem. After the first set of trials participants were shown
their cumulative gains or losses. There were six
experimental groups, with 16 participants in each condition.
On each trial, descriptions of the two choice options were
presented, with the left-right position counterbalanced. The
participant made a choice, and the payoff for that trial was
shown. The cumulative amount won or lost could be
checked at any time by clicking an icon.
Actual pay for the participants varied depending on the
outcomes of their decisions. A base payment of $10 was
adjusted by 0.5% of the participant’s total amount of
winnings (for the 100 Gain trials) and losses (for the 100
Loss trials).

Results
The main dependent variable was the probability of
selecting the sure-thing option across the 100 trials. To

The interaction between Block and Domain was
significant, F(4,360) = 7.153, p<.001, using the HuynhFeldt correction for sphericity, ε = .73, thus the trends for
Gains and Losses were tested separately. For Loss
problems, the effect of Block was significant, F(4,188) =
10.721, p<.001, ε = .50, but not for Gain problems, F(4,188)
= 0.217, p=.902, ε = .84. Thus, a significant learning trend is
observed in the Loss domain, consisting of an increasing
probability of selecting the sure thing option (the option
with higher EV). This suggests that the longer-term effect
of outcome feedback in the Loss domain is to reduce riskseeking, perhaps by moving decision makers towards EVmaximization.
For Set 2 trials, Figure 2 shows initial mean levels and the
trend for sure-thing choices compared to Set 1 performance.
The initial-block proportion of sure thing choices for Set 2
Gain trials is .39, compared to .49 for the initial block of Set
1 trials. This difference, though sizeable, is not significant,
t(94) = -1.347, p=.181. For Losses the mean proportion of
sure-thing choices is .29 for the initial block of Set 2 trials,
compared to .14 for the initial block of Set 1 trials; this
effect is significant: t(94) = 2.717, p=.008. These “intercept
effects” demonstrate an effect of prior experience (in the
first set of 100 trials) on initial performance in Set 2 trials.
Figure 2 also shows the overall mean trend across blocks
in Set 2 (the Set 1 trends are shown for comparison) for
Gains and Losses. The learning trends across blocks in Set 2
trials are similar to the patterns for Set 1 trials. For Gain
problems, the proportion of sure-thing choices is essentially

2074

flat across the five blocks of the Set 2 trials (.39 in Block 1
versus .36 in Block 5). However, there is a steady increase
in the proportion of sure-thing choices across blocks for
Loss problems (from .29 in Block 1 to .40 in Block 5). The
Time (Block) effect for Gains is not significant, F(4,188) =
0.227, p=.825 with Huynh-Feldt correction (ε =.57), while
the Time (Block) effect for Loss problems is marginally
significant, F(4,188) = 2.674, p=.057, ε =.659.

a risky response (and the “total” pattern) have a positive
slope, reflecting the general rise in the proportion of surething choices observed across blocks.

Figure 3: Trends in the proportion of sure-thing choices (Y
axis) across the 10 trials following a sure-thing response, an
unreinforced risky-choice response, and a reinforced riskychoice response. Top panel: Gain problems. Bottom panel:
Loss problems.

Figure 2: Mean change in proportion of sure-thing choices
for problems across 100 trials in Study 1-Set 1, Study 1-Set
2, and Study 2. Top panel: overall means for Gain problems.
Bottom panel: overall means for Loss problems.
Sequential Analysis. To further evaluate the viability of
reinforcement-based accounts, a sequential analysis of triallevel responses was conducted for the Study 1 Set 1 data.
This analysis involved computing separate “learning
curves” across trials t+1 to t+10 based on whether trial t
(varied from t =1 to 90) was a sure-thing response resulting
in a fixed gain (or loss, depending on condition), a risky
response that did not result in a gain (loss), or a risky
response that resulted in a large gain (loss). The mean
proportion of sure-thing responses for each trial t+1 to t+10
is shown in Figure 3.
For Gain problems, a sure-thing response on trial t tends
to be followed by another sure-thing response at trial t+1,
and this proportion declines over the next 4-5 trials; a
corresponding increase is observed for several trials
following a risky response. It seems that decisions by
participants to try the sure-thing or the risky-option are
often decisions to try a given option for several trials, not
just one (cf. Biele, Erev, and Ert, 2009). For Losses, there is
some hint of a similar pattern, but the Loss curves following

The primary question, though, concerns whether effects
of reinforcement learning can be detected in these curves.
Such an effect should be discernable in a divergence of the
curves following a reinforced risky response (curve “R-R”)
and an unreinforced risky response (curve “R-U”). For
Gains, a divergence is indeed discernable across trials t+1 to
t+3, however the direction is such that participants seem to
switch away from the risky response for a few trial
following a reward “hit”. This behavior is inconsistent with
reinforcement learning but consistent with the gambler’s
fallacy (cf. Yechiam & Busemeyer, 2006; Barron & Leider,
2010). For Losses, the pattern is different. Across the first
five or so trials following a “punished” risky response
(curve R-R), the mean level of sure-thing responses is
elevated, compared to the curve for unreinforced risky
responses (curve R-U). This pattern is consistent with
reinforcement learning.

Discussion
The results of Study 1 show that risk preferences can
change as a result of experience across repeated trials of a
description-based decision problem, at least for loss-domain
problems. In both Set 1 and Set 2 trials, participants were
initially risk-seeking for Loss problems, but increased their
proportion of sure-thing choices across blocks.

2075

Importantly, the sequential analyses described above
demonstrate that simple reinforcement learning models
cannot fully explain choices in the repeated DBD paradigm,
because participants were actually less likely to select the
risky option for several trials after a successful risky choice,
exhibiting “negative recency” (cf. Barron & Leider, 2010).
The present study is the first to our knowledge to explore
possible cross-domain effects of experience on repeated
decisions. Our results show that experience with 100 prior
trials of a Gain-domain decision problem decreases the
initial level of risk seeking for Set 2 trials in the Loss
domain. The complementary effect, in which prior Loss
trials decreased initial risk aversion for Gain problems, was
sizable (a 10% change) but not significant. This finding
cannot be explained by simple reinforcement learning
models (which do not address generalization of learned
associations between gain and loss domains), nor by
March’s (1996) idea of learned generalized risk preferences.
After all, if participants in the Set 1 Loss condition are
learning a generalized aversion to risk, then the proportion
of sure-thing choices for Set 2 Gain trials should be
increased, not decreased. And if the learned risk preferences
are domain-specific (March’s actual suggestion), then they
could not explain any cross-domain shifts.
Thus we are left with two candidate mechanisms that
could explain both trial-by trial changes in risk preferences
and the cross-domain “intercept” effects: learning of more
rational (EV-maximizing) behavior (perhaps mediated by
learning of more linear decision weights), and framing or
reference-point effects, as postulated by Cumulative
Prospect Theory (Tversky & Kahneman, 1992).
To
elaborate on the latter explanation, under the assumption of
aggregation framing accumulating losses in the Loss domain
trials would move participants away from the reference
point, reducing the curvature of the value function in the
neighborhood of the aggregated decision outcomes, thereby
reducing risk seeking behavior incrementally over trials.
Second, aggregation framing might be triggered especially
for initial Set 2 trials (causing the intercept effects) because
participants were explicitly shown their cumulative Set 1
outcomes before beginning Set 2. Thus, the shift in initial
risk propensity for Set 2 Loss trials might be a “house
money” effect (Thaler & Johnson, 1990) whereby
participants take into account their cumulative Set 1 gains,
inuring them to small certain losses in Set 2.

STUDY 2
Study 2 was designed to compare framing effects and EVlearning as potential explanations for the cross-domain
“intercept” effects observed in Study 1. One way to
distinguish predictions of the two accounts is to offer
participants comparable gains or losses that are not
associated with individual decision trials. If the previous
winnings/losses had their influence through framing effects,
then simply endowing decision makers with a large initial
windfall gain (or a large loss) should have similar effects. In
contrast, if the Study 1cross-domain “intercept” effects were

caused by learning of more linear subjective decision
weights (or any other type of reinforcement learning), then
endowing subjects with a large initial gain or loss should not
lead to shifts in risk preferences.

Method
The procedure of Study 2 was similar to that of Study 1.
However, each participant experienced only 100 trials of a
single decision problem in the domain of either gains or
losses. Before these 100 trials, each participant was
awarded either a large gain (before the Loss domain trials)
or a large loss (before the Gain domain trials). The
proportions of sure-thing choices were then observed across
these 100 decision trials.
Materials. Study 2 used the same six decision problems
used in Study 1.
Participants. The participants (N=78) were recruited in the
same manner as Study 1 participants.
Procedure. Before beginning the learning trials, each
participant was given the choice of three envelopes. In each
envelope was a positive or negative number that was
described to the participant as a monetary gain or loss.
Subjects in the Loss condition were awarded a large initial
gain, and participants in the Gain condition were awarded a
large initial loss. The amounts used for endowment of the
initial large gain or loss were generated for individual
participants based on a normal distribution with mean and
standard deviation matched to the distribution of cumulative
outcomes earned by participants for each corresponding
problem in Study 1 Set 1 trials. It was explained to the
participant that the gain (loss) shown in the envelope was
his/her starting position, and that the outcomes of all
subsequent decision trials would be combined with this
initial stake to determine the participant’s final outcome
position, which would determine his/her pay (as in Study 1).
Participants then responded to each of 100 repeated trials
of a decision problem shown on the screen. There were 1214 participants in each condition (i.e., for each decision
problem). The interface for each decision trial was identical
to that used in Study 1.

Results
The mean proportion of sure-thing choices in the first and
last block of trials is shown in Table 1 for all six decision
problems. For Study 2 Gain trials, the mean first-block
proportion of sure-thing choices was .47, versus .49 for
Study 1 Set 1 Gain trials (see Figure 2). This difference was
not significant, t(86) = 0.322, p=.748. Thus, the
“endowment” manipulation of a large initial loss had
virtually no effect on initial risk preference levels for Gain
problems. However, for Study 2 Loss trials the initial (firstblock) propensity to select the sure-thing option (.29) was
significantly higher than the initial level (.14) shown in
Study 1 Set 1 Loss trials, t(84) = -2.602, p=.011, and was
virtually identical to the initial level of .30 shown by
participants in Study 1 Set 2 Loss trials after they had
experienced 100 Gain trials (see Figure 2).

2076

Table 1. Study 2 trials: Initial and final block proportions
of sure-thing choices for six decision problems, with mean
initial endowments. (N=78 participants total).

Problem:
G1 ($5; $100, .1)
G2 ($9; $100, .1)
G3 ($9; $20, .5)
(Gain overall)
L1 (-$5; -$100, .1)
L2 (-$9; -$100, .1)
L3 (-$9; -$20, .5)
(Loss overall)

Mean
Endowment:
-$718.72
-$975.85
-$922.43
-$872.33
$738.29
$946.82
$951.19
$878.77

Initial
Block
.56
.51
.34
.47
.18
.31
.35
.29

Final
Block
.28
.64
.34
.41
.29
.50
.45
.42

Gain problems, there was no significant effect of
“endowing” participants with a large initial loss (cf. Fatás,
Jiménez, and Morales, 2011).

General Discussion

Thus, the effects of 100 preceding Gain trials on initial
risk preferences for the Set 2 Loss problems (Study 1) is
roughly equivalent to the effects of receiving a single
windfall gain of comparable magnitude (Study 2), nudging
people toward a (maximizing) strategy of favoring small
sure losses. However, a large initial loss does not affect
behavior in repeated Gain trials.
Changes in risk propensity across the five blocks of Study
2 learning trials were also assessed. The interaction between
Block and Domain was significant, F(4,304) = 3.692,
p=.016, Huynh-Feldt ε=.681, indicating that the trends
across blocks differed for Gains and Losses. For Gains,
there was a non-significant decline in the proportion of surething choices, from .47 in Block 1 to .41 in Block 5,
F(4,156) = 0.551, p=.621, ε=.639. For Losses there was a
large (significant) increase in the proportion of sure-thing
choices across blocks, from .29 in Block 1 to .42 in Block 5,
F(4,148) = 4.393, p=.013, ε=.552. These trends closely
resemble those for Study 1 Set 1 trials, though the curves
are shifted in their “intercepts” or initial levels (Figure 2).

Discussion
The results of Study 2 replicate the Study 1 finding that
making repeated decisions with outcome feedback has the
long-term (100 trials) effect of reducing risk-seeking in the
Loss domain. However, the major goal of this study was to
investigate whether the cross-domain “intercept” effects of
prior experience found in Study 1 are due to
framing/aggregation of monetary outcomes or to learning in
the direction of EV-maximization.
The obtained results show that merely endowing
participants with a large initial gain has an effect on initial
risk propensity for Loss trials, significantly increasing the
proportion of sure-thing choices in the first block of Loss
trials. The size of the effect is comparable to the effect
found in Study 1 where participants actually experienced
100 previous gain trials, suggesting that framing processes
are a sufficient explanation for the effects of prior Gain
trials on initial risk-seeking propensity in the Loss domain.
Participants may have felt that they were “playing with the
house money” (Thaler & Johnson, 1990). In contrast, for

The present empirical results confirm and extend previous
findings (e.g., Chen, 2001; Yechiam & Busemeyer, 2005;
Jessup et al., 2008) that experienced outcome feedback can
change people’s choice propensities in a repeated
description-based risky decision task (involving losses),
even though complete information about outcomes and
contingencies is given at the outset (cf. Newell & Rakow,
2007, for similar conclusions involving a prediction task).
It is interesting to compare initial preferences (first-trial
and first-block) in this repeated-trials task with available
data on preferences for the same problems in a standard
single-trial description-based decision paradigm (Chen &
Corter, 2006). In that study, for single trials, the mean
proportion of sure-thing choices for Gain problems G1-G3
was .54. Here, the first-trial mean proportion of sure thing
responses for Gain problems was .52, a nearly identical
level, and .49 for all 20 first-block trials. For Loss
problems, the single-trial level of sure-thing responses (from
Chen & Corter, 2006) was .46, compared to .27 for first-trial
and .14 for first-block trials here. Thus, it seems that in the
Loss domain merely knowing that one will face repeated
trials of a risky decision problem intensifies risk seeking.
Study 1 and Study 2 document a number of other
asymmetries between the Gain and Loss domains:
experiencing outcome feedback for each trial can affect
description-based decisions in the Loss domain, but does
not seem to do so for Gains. Also, the effect of a “windfall”
gain can affect initial decisions under (described) risk in the
Loss domains (decreasing risk seeking), while a large onetime loss does not significantly affect decisions in the Gain
domain. The specific asymmetries echo previous findings
(e.g., Schneider, 1992; Chen & Corter, 2006) that decision
makers show more inconsistency in choice when decisions
are framed in terms of losses, and are consistent with
Dunegan’s (1993) suggestion that Gain and Loss problems
may invoke different types of processing. Finally, our
analysis of sequential effects in Study 1 show that a riskygain payoff does not act as a positive reinforce, but an
experienced risky loss does seem to act as a punishment,
reducing subsequent choices of that option.
These results shed light on possible mechanisms whereby
experience might affect description-based decisions. Simple
reinforcement learning between the sure-thing and risky
options, suggested by March (1996) to create domainspecific risk preferences, cannot account for the full pattern
of our results. One possible mechanism causing EVconsistent responses to increase with substantial experience
is learning affecting the decision-weight function that brings
the function closer to linear, thus making choices more
consistent with EV-maximization. But another possibility
involves conscious selection among strategies (cf. Erev &

2077

Barron, 2005), specifically, that more participants are
explicitly calculating EV and basing their decisions on that
criterion. Both types of processes, implicit learning of more
linear decision weights and conscious selection of strategies,
may in fact affect behavior in sequential description-based
decisions. Thus, dual-systems accounts of cognition (e.g.,
Sloman, 1996; Camerer, Loewenstein, & Prelec, 2005) may
be needed to provide complete descriptions of decision
behavior in such situations.

Acknowledgments
Study 1 reported here was conducted as part of the first
author’s dissertation research (Chen, 2001). He wishes to
thank the members of his dissertation committee, especially
Ido Erev and Elke Weber, for their helpful advice. We also
thank Yi-Chun Chen for her assistance in running Study 2.

References
Barron, G., & Erev, I. (2003). Small feedback-based
decisions and their limited correspondence to descriptionbased decisions. Journal of Behavioral Decision
Making, 16(3), 215-233.
Barron, G., & Leider, S. (2010). The role of experience in
the Gambler's Fallacy. Journal of Behavioral Decision
Making, 23(1), 117-129.
Biele, G., Erev, I., & Ert, E. (2009). Learning, risk attitude
and hot stoves in restless bandit problems. Journal of
Mathematical Psychology, 53, 155–167
Camilleri, A. R., & Newell, B. R. (2009). The role of
representation in experience-based choice. Judgment and
Decision Making, 4(7), 518.
Camerer, C. F., Loewenstein, G. W., & Prelec, D. (2005).
How neuroscience can inform economics. Journal of
Economic Literature, 43(1), 9-64.
Chen, Y.-J. (2001). The role of learning in risk preferences
for single- and multiple-play gambles. Unpublished Ph.D.
dissertation, Teachers College, Columbia University.
Chen, Y.-J., & Corter, J. E. (2006). When mixed options
are preferred in multiple-trial decisions. Journal of
Behavioral Decision Making, 19(1), 17-42.
Dunegan, K. J. (1993). Framing, cognitive modes, and
image theory: Toward an understanding of a glass half
full. Journal of Applied Psychology, 78(3), 491-503.
Erev, I. & Barron, G. (2005). On adaptation, maximization,
and reinforcement learning among cognitive strategies.
Psychological Review, 112, 912–931.
Fatás, E., Jiménez, F., & Morales, A. J. (2011). Controlling
for initial endowment and experience in binary choice
tasks. Journal of Risk and Uncertainty, 43, 227–243.
Hadar L, & Fox C. (2009). Information asymmetry in
decision from description vs. decision from experience.
Judgment and Decision Making, 4(4), 317-325.
Hau, R., Pleskac, T. J., & Hertwig, R. (2010). Decisions
from experience and statistical probabilities: Why they
trigger different choices than a priori probabilities.
Journal of Behavioral Decision Making, 23, 48–68.

Hertwig, R., Barron, G., Weber, E. U., & Erev, I. (2004).
Decisions from experience and the effect of rare events in
risky choice. Psychological Science, 15(8), 534–539.
Hertwig, R., & Erev, I. (2009). The description-experience
gap in risky choice. Trends in Cognitive Sciences, 13,
517-523.
Jessup, R. K., Bishara, A. J., & Busemeyer, J. R. (2008).
Feedback produces divergence from Prospect Theory in
descriptive choice. Psychological Science, 19, 1015-1022.
Kahneman, D., & Tversky, A. (1979). Prospect Theory: An
analysis of decision under risk. Econometrica, 47(2), 263292.
March, J. G. (1996). Learning to be risk averse.
Psychological Review, 103, 309–319.
Newell, B. R. & Rakow, T. (2007). The role of experience
in decisions from description. Psychonomic Bulletin and
Review, 14, 1133-1139.
Rakow, T., & Newell, B. R. (2010). Degrees of uncertainty:
An overview and framework for future research on
experience-based choice. Journal of Behavioral Decision
Making, 23, 1–14.
Redelmeier, D. A., & Tversky, A. (1992). On the framing of
multiple prospects. Psychological Science, 3, 191–193.
Schneider, S. L. (1992). Framing and conflict: Aspiration
level contingency, the status quo, and current theories of
risky choice. Journal of Experimental Psychology:
Learning, Memory, & Cognition, 18, 1040-1057.
Sloman, S. (1996). The empirical case for two systems of
reasoning. Psychological Bulletin, 119, 3–22.
Thaler, R., & Johnson, E. (1990). Gambling with the house
money and trying to break even: effects of prior outcomes
on risky choice. Management Science, 36(6), 643–660.
Tversky, A., & Kahneman, D. (1992). Advances in prospect
theory: cumulative representation of uncertainty. Journal
of Risk and Uncertainty, 9, 195–230.
Yechiam, E., Barron, G., & Erev, I. (2005). The role of
personal experience in contributing to different patterns of
response to rare terrorist attacks. The Journal of Conflict
Resolution, 49(3), 430-439.
Yechiam. E., & Busemeyer, J.R. (2005). Comparison of
basic assumptions embedded in learning models for
experience based decision-making. Psychonomic Bulletin
and Review, 12(3), 387-402.

2078

