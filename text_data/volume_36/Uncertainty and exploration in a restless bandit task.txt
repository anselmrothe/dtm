UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Uncertainty and exploration in a restless bandit task
Permalink
https://escholarship.org/uc/item/5qs1r33b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Speekenbrink, Maarten
Konstantinidis, Emmanouil
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                            Uncertainty and exploration in a restless bandit task
                                      Maarten Speekenbrink (m.speekenbrink@ucl.ac.uk)
                           Emmanouil Konstantinidis (emmanouil.konstantinidis.09@ucl.ac.uk)
                                   Department of Experimental Psychology, University College London
                                               Gower Street, London WC1E 6BT, England
                               Abstract                                “softmax” decision rule). The lack of an effect of uncertainty
   Decision-making in noisy and changing environments requires         on explorative decisions is disappointing, considering that ra-
   a fine balance between exploiting knowledge about good              tionally, this should be a driving factor of exploration (Cohen
   courses of action and exploring the environment in order to         et al., 2007). Knox et al. (2012) used a restless two-armed
   improve upon this knowledge. We present an experiment in
   which participants made repeated choices between options for        bandit task with a simplified structure, which allowed them to
   which the average rewards changed over time. Comparing a            derive the optimal decision strategy. In their task, the rewards
   number of computational models of participants’ behaviour in        of the two arms alternated in their superiority, “leapfrogging”
   this task, we find evidence that a substantial number of them
   balanced exploration and exploitation by considering the prob-      over each other. The results showed that people appeared to
   ability that an option offers the maximum reward out of all the     act reflectively, updating their beliefs that the arms switched
   available options.                                                  in superiority, but that they could not use these beliefs to plan
   Keywords:        Dynamic decision making; Exploration-              further ahead in time than for the immediate decision. More-
   exploitation trade-off; Restless multi-armed bandit task
                                                                       over, in contrast to Daw et al., Knox et al. found evidence
                           Introduction                                that exploration was driven by uncertainty regarding the asso-
In many situations, the expected utility of an action is ini-          ciated rewards. As Knox et al. used a much more constrained
tially unknown and can only be learned from experience. In             task than Daw et al., it is unclear whether this difference is
such situations, we can take actions in order to maximise the          due to the task, or to the way in which the effect of uncer-
utility experienced (exploiting the environment), but also take        tainty on decisions was formalized. In the present paper, we
actions which might not provide as good outcomes, but which            will try to reconcile these conflicting results, by considering
help us to learn more about the outcomes associated with that          an alternative way to incorporate uncertainty into explorative
action (exploring the environment). Performing well in these           decisions than the heuristic strategy used by Daw et al..
situations requires a fine balance between exploration and                 To illustrate our model, consider a relatively simple task in
exploitation. Multi-armed bandit tasks have proven a use-              which, on each trial t, the reward R j (t) associated with arm j
ful paradigm to study the exploration-exploitation trade-off,          is drawn from a Normal distribution, with a mean µ j (t) that
theoretically (e.g., Gittins, 1979; Whittle, 1988) as well as          changes over trials according to a random walk
empirically (e.g., Acuna & Schrater, 2008; Daw et al., 2006;                     R j (t)=µ j (t) + ε j (t)      ε j (t)∼N(0, σε )
Knox et al., 2012; Steyvers et al., 2009).                                                                                            (1)
                                                                                  µ j (t)=µ j (t − 1) + ζ j (t) ζ j (t)∼N(0, σζ )
   For standard bandit problems, in which the average re-
wards of unchosen bandits remain unchanged and future                  An ideal Bayesian learner with knowledge of the properties
rewards are exponentially discounted, the optimal decision             of this process would update her belief about the average re-
strategy can be determined by calculating a “Gittins index”            wards based on the observed rewards; p(µ j (t)|R1:t ,C1:t ), the
for each arm of the bandit, reflecting the expected total future       posterior distribution of arm j’s average reward, conditional
rewards associated with the arm at a particular time (Gittins,         upon the observed rewards R1:t and choices C1:t , can be com-
1979). Acuna & Schrater (2008) showed that, allowing for               puted by the Kalman filter (cf. Daw et al., 2006). This pos-
computational constraints, human decisions follow this opti-           terior distribution, together with the structural model, allows
mal strategy reasonably well. Although standard bandit tasks           the agent to derive a prior distribution p(µ j (t + 1)|R1:t ,C1:t )
have generated useful results, in real-life situations, the ex-        for the next trial, which reflects the current beliefs about each
pected rewards of unchosen options do often change. For in-            arm at the moment of choice. How should these beliefs be
stance, when choosing a restaurant, we should allow for the            used to choose the next arm to play? A “greedy” agent would
possibility that the quality of the food on offer changes over         always choose the arm with the highest prior mean. While
time (cf., Knox et al., 2012). For what is now a “restless”            this is optimal on the last play, if there are more plays left,
bandit problem, optimal decision strategies have proven elu-           it is generally beneficial to sometimes choose a different arm
sive, although heuristic strategies have been proposed (Whit-          in order to check that its average reward has not surpassed
tle, 1988). Daw et al. (2006) investigated human decision              that of the currently favoured arm. The longer an arm has
making in a restless bandit problem, and found that explo-             not been played, the higher the (subjective) probability that it
ration appeared to be unrelated to the uncertainty regarding           now has a higher average reward. If exploration is based on
the average reward of each arm. Exploration was best de-               this probability, the probability of exploration increases with
scribed by a heuristic strategy in which arms are chosen prob-         the time that an arm has not been played. The difference be-
abilistically according to their relative expected rewards (the        tween this explorative strategy and a greedy one is illustrated
                                                                   1491

                                                                        in Figure 1. As shown there, the explorative strategy clearly
                                             greedy
                                                                        outperforms the greedy strategy.
                                                                           The probability that an arm provides the maximum reward
                                                                        naturally combines both the expected value (mean) and the
                              20
                                                                        associated uncertainty (variance) of the prior distributions.
                                                                        While a strategy which bases decisions on this probability is
                                                                        myopic in the sense that it is solely based on the chance of
                                                                        obtaining the highest possible reward for the immediate deci-
                               0                                        sion, it nevertheless allows for a reasonable balance between
                                                                        exploitation and exploration. As the probability of maximum
                                                                        reward increases with uncertainty, the probability of explor-
(estimated) average reward
                             −20
                                                                        ing an arm increases the longer it has not been observed. But
                                                                        the probability of maximum reward is also dependent on the
                                                                        expected reward, such that this increase is larger for arms that
                                            explorative
                                                                        are closer to the currently favoured arm in expected reward.
                                                                        While Daw et al. (2006) did not find evidence that exploration
                                                                        is related to uncertainty, they only considered a heuristic deci-
                              20
                                                                        sion strategy with an exploration bonus which increased lin-
                                                                        early with the standard deviation of the prior distribution. The
                                                                        probability of maximum reward strategy offers a more prin-
                                                                        cipled way in which to combine expectancy and uncertainty
                               0                                        and results of a simulation study showed that it outperforms
                                                                        more heuristic strategies. The strategy generalizes the belief
                                                                        model proposed by Knox et al. (2012) to the more general
                             −20
                                                                        situation of a restless multi-armed bandit.
                                                                           In the present paper, we investigate whether humans per-
                                                                        forming a restless multi-armed bandit task use this strategy to
                                   0   25      50         75   100
                                               trial                    make their decisions. We compare the strategy to a number of
                                                                        heuristic decision strategies proposed for multi-armed bandit
Figure 1: Learning and decision making in a restless two-               tasks, contrasting also Bayesian “model-based” learning and
armed bandit task. Two arms (blue and red) have changing                two popular “model-free” learning strategies.
average rewards (broken lines), generated according to Equa-
tion 1. On each trial, an agent chooses an arm (dots at the                                        Method
bottom of the graphs), observes the associated reward, and              We investigated decision-making in a restless four-armed
then updates her belief about the average reward for that arm.          bandit task similar to that used by Daw et al. (2006). Four
These posterior beliefs form the basis of the prior belief on           versions of the task were constructed in which (a) the average
the next trial (solid lines show the prior means and areas              rewards of the decks changed either completely unpredictably
the 95% highest density intervals of the prior distribution).           or with a small trend, and (b) the volatility of the changes was
The “greedy” agent (top panel) always chooses the arm with              either stable or there were periods of relatively high volatil-
the highest expected reward. After sampling once from both              ity. We expected people who notice an increase in volatility
arms, she always chooses the one with the highest expected              to make more exploratory decisions, due to the associated in-
reward until the prior mean falls below the prior mean of the           crease in uncertainty. People who notice the trends could be
unchosen arm. A problem with this strategy is that it ignores           expected to make relatively less exploratory decisions, as the
the uncertainty in the prior distribution, which increases for          changes are more predictable.
unchosen arms due to the innovations ζ j (t). After not choos-
                                                                        Participants
ing an arm for a prolonged period, the probability that the
mean reward of this arm is higher than the mean reward of the           Eighty participants (41 female), aged between 18 and 56
chosen arm can become substantial. The “explorative” agent              (M = 22, SD = 6.72), took part in this study on a voluntary
(bottom panel) bases her choices on this probability. On each           basis. Participants were randomly assigned to one of the four
trial she chooses an arm randomly according to the probabil-            experimental conditions: stable volatility with trend (ST), sta-
ity that this arm will provide the highest reward in the set of         ble volatility without trend (SN), variable volatility with trend
arms. This clearly gives better results than the greedy strat-          (VT), and variable volatility without trend (VN).
egy and the agent mostly chooses the arm with the highest
                                                                        Task
average reward.
                                                                        All participants completed a restless four-armed bandit task.
                                                                        On each of 200 trials, they were presented with four decks
                                                                     1492

                                                                                            decks of cards and that their task was to select on each trial
                                arm      1       2     3    4
                                                                                            a card from any deck they chose. The only goal of the game
                        no trend                            trend                           was to win as many points as possible. Participants were in-
                                                                                            formed that some decks may be better than others, but that the
         100
                                                                                            amount they tend to give may vary, so that this can change.
                                                                              stable
           0                                                                                They were not informed of the total number of trials in the
        −100
                                                                                            task.
                                                                                               After reading the instructions, participants started the ex-
value
        −200                                                                                perimental task. On each trial, they were presented with the
         100
                                                                                            four decks of cards and selected a card from one deck via a
                                                                                            mouse click. The number of points won or lost was then dis-
                                                                              variable
           0                                                                                played for 1.5 seconds, along with either a smiley or a frown-
        −100                                                                                ing face for wins or losses respectively. Throughout the task,
                                                                                            a counter displayed the total points received thus far.
        −200
               0   50     100      150       200 0
                                              trial
                                                       50       100   150   200                                Behavioural results
                                                                                            One participant (age = 21) in the SN condition was excluded
Figure 2: Example rewards in the four-armed restless bandit                                 from further analysis as she only sampled from one bandit
task.                                                                                       throughout the whole task. All other participants sampled
                                                                                            from each bandit at least once.
                                                                                            Performance
of cards (arms) and asked to draw a card from one of them.
After choosing an arm, they were informed of the reward as-                                 Given the differences between the conditions in obtainable
sociated with their choice. For each arm j = 1, . . . , 4, the                              reward magnitudes (see e.g. Figure 2), total reward obtained
reward R j (t) on trial t was randomly drawn from a Normal                                  is not an unambiguous measure of performance. We there-
distribution with a mean µ j (t) which varied randomly and in-                              fore chose to focus on whether, on a given trial, the bandit
dependently according to a random walk. More precisely, the                                 with the maximum reward was chosen, which we will refer
rewards were generated according to the following schedule:                                 to as an advantageous choice. Average proportions of advan-
                                                                                            tageous choices, by block (4 blocks of 50 trials each) and
    R j (t)=µ j (t) + ε j (t)                         ε j (t)∼N(0, σε )                     condition, are given in Figure 3. Choice behaviour was anal-
                                                                           (2)              ysed with a generalized linear mixed-effects model, using a
    µ j (t)=λµ j (t − 1) + κ j + ζ j (t)              ζ j (t)∼N(0, σζ (t))
                                                                                            binomial distribution for the number of advantageous choices
The averages of the decks were initialized as µ j (1) =                                     in each block. In addition to fixed effects for Block, Volatility,
−60, −20, 20, 60 for decks 1 to 4 respectively. A decay pa-                                 and Trend, subject-specific random intercepts were included.
rameter λ = .9836 was used so that values remained closer to                                Note that this model is structurally similar to a repeated-
0 than with a pure random walk. In the ST and VT conditions,                                measures ANOVA, but takes into account the non-normal dis-
the trend parameter had values κ j = 0.5, 0.5, −0.5, −0.5 for                               tribution of the number of advantageous choices. This analy-
decks 1 to 4 respectively. In the SN and VN conditions, the                                 sis showed a significant main effect of Block, χ2 (3) = 295.97,
values were κ j = 0 for all decks. The reward error variance                                p < .001. Averaging over conditions, the proportion of ad-
was σ2ε = 16 in all conditions. The innovation variance was                                 vantageous choices increased from block 1 to block 3, while
σ2ζ (t) = 16 on all trials in the stable volatility conditions (SN                          there was a small decrease from block 3 to block 4. In ad-
and ST). In the variable volatility conditions, the innovation                              dition, there was a significant Volatility by Block interaction,
variance was the same on half of the trials. On trials 50-                                  χ2 (3) = 148.13, p < .001, as well as a significant Trend by
100 and 150-200, the innovation variance was increased to                                   Block interaction, χ2 (3) = 86.38, p < .001. Post-hoc com-
σ2ζ (t) = 256. The schedule in Equation 2 was used to gener-                                parisons showed that in block 2, performance in the stable
ate four sets of reward sequences in each condition, matching                               volatility conditions was significantly better than in the vari-
the seed in the random number generated used over condi-                                    able volatility conditions (p = .019), and performance in the
tions. One example of the resulting rewards in the four con-                                no trend conditions was significantly better than in the trend
ditions is provided in Figure 2. A similar structure was used                               conditions (p = .004). In block 3, the reverse was true,
by Daw et al. (2006), but they didn’t include trends or changes                             with performance better in the variable volatility conditions
in volatility and used only posive rewards.                                                 (p < .001) and in the trend conditions (p = .029). In the re-
                                                                                            maining blocks, there was no effect of Volatility or Trend.
Procedure                                                                                   The effects of Volatility and Trend on performance are likely
Participants completed the 200 trials of the task individually                              due to their effect on the discriminability between the arms
at their own pace in a single session. At the start of the task,                            in terms of their average rewards. For instance, while high
participants were told that they would be presented with four                               volatility may hinder discrimination between the decks due to
                                                                                         1493

                                                                                                            in the variable volatility conditions, χ2 (1) = 35.77, p < .001,
                                  block        1   2    3       4
                                                                                                            while there is no difference in the stable volatility conditions,
                             no trend                           trend                                       χ2 (1) = 1.89, p = .17.
                                                                                   advantageous choice
             0.75                                                                                                    Modelling exploration and exploitation
             0.50                                                                                           Switching between arms is only a rough measure of explo-
                                                                                                            ration, as one can switch arm because one believes it is now
             0.25                                                                                           optimal (exploitation) or to gain more information about it
proportion
                                                                                                            (exploration). We therefore use computational modelling to
             0.00                                                                                           gain more insight into explorative decisions. In all models
              0.4
                                                                                                            considered here, u(t), the utility of the reward R(t) received
                                                                                                            on trial t, is assumed to be described through the value func-
              0.3
                                                                                                            tion of Prospect Theory (cf. Ahn et al., 2008):
              0.2                                                                  switch
                                                                                                                                    (
              0.1
                                                                                                                                     R(t)α        if R(t) ≥ 0
                                                                                                                             u(t) =
                                                                                                                                     −λ|R(t)|α if R(t) < 0
              0.0
                    stable          variable           stable           variable
                                                                                                            where the parameter α > 0 determines the shape of the utility
                                                                                                            function: when α < 1, the curve is concave for gains (risk
Figure 3: Proportion of advantageous choices and switches                                                   aversion) and convex for losses (risk seeking). The parameter
by block (50 trials each) and condition.                                                                    λ ≥ 0 can account for loss aversion: when λ > 1, a loss of x
                                                                                                            points has a larger negative utility than a win of x points has
                                                                                                            a positive utility.
                                                                                                               After receiving a reward on trial t, participants are assumed
large trial-by-trial variation in average rewards, when volatil-
                                                                                                            to update their expectancies E j (t + 1) regarding the utility
ity reduces again in block 3, the arms are actually more dis-
                                                                                                            they will receive when choosing deck j on trial t + 1. We
criminable than in the stable volatility conditions, as the high
                                                                                                            consider three possible mechanisms through which these ex-
volatility has pushed the means further apart.
                                                                                                            pectancies are updated: Bayesian updating, the delta rule, and
Switching                                                                                                   the decay rule.
Average switching proportions, by block and condition, are
                                                                                                            Bayesian updating This model-based learning strategy as-
given in Figure 3. Switching behaviour was analysed with
                                                                                                            sumes the utility of arms is determined by a Gaussian process
a similar generalized linear mixed-effects model as for the
                                                                                                            as in Equation 1. Optimal Bayesian inference regarding mean
advantageous choices. This analysis showed a significant
                                                                                                            utilities is implemented by the Kalman filter:
main effect of Block, χ2 (3) = 634.75, p < .001, as well as
a Volatility by Block interaction, χ2 (3) = 26.44, p < .001, a                                                        E j (t) = E j (t − 1) + δ j (t)K j (t)[u j (t) − E j (t − 1)]   (3)
Trend by Block interaction, χ2 (3) = 44.15, p < .001, and a
three-way interaction between Volatility, Trend, and Block,                                                 where δ j (t) = 1 if deck j was chosen on trial t, and 0 other-
χ2 (3) = 16.12, p = .001. No other effects were significant.                                                wise. The “Kalman gain” term is computed as
Post-hoc analysis did not show any significant differences be-
tween pairs of conditions within each block. Comparisons of                                                                                      S j (t − 1) + σ2ζ
consecutive blocks within each condition showed that in the                                                                       K j (t) =
                                                                                                                                              S j (t − 1) + σ2ζ + σ2ε
SN condition, there was a significant decrease in switching
from block 2 to 3. In the VN condition, switching decreased                                                 where S j (t) is the variance of the posterior distribution of the
from block 1 to 2 and from block 2 and 3, while there was                                                   mean utility, computed as
an increase from block 3 to 4. In the ST and VT condition
there was a decrease in switching from block 1 to 2 and from                                                    S j (t) = S j (t − 1) + σ2ζ + δ j (t)[1 − K j (t)][S j (t − 1) + σ2ζ ] (4)
block 2 to 3. For all these comparisons, p < .001. As for the
number of advantageous choices, this analysis indicates that                                                Prior means and variances were initialized to E j (0) = 0 and
there were no general effects of volatility or trend on switch-                                             S j (0) = 1000. For simplicity, we did not consider a model
ing behaviour. However, these manipulations did affect how                                                  which learns possible trends or the level of volatility.
switching behaviour developed during the task.
   Of particular interest is whether participants in the vari-                                              Delta rule A popular model-free alternative to Bayesian in-
able volatility conditions show increased exploration in the                                                ference is the delta rule:
blocks with high volatility. Focussing on switching behaviour
in block 3 and 4, we see an increase from block 3 to block 4                                                            E j (t) = E j (t − 1) + δ j (t)η[u j (t) − E j (t − 1)]
                                                                                                         1494

The main difference between this rule and Bayesian updating              Probability of maximum utility (PMU) The probability
(Equation 3) is that the former uses a fixed learning rate 0 ≤           that an arm provides a higher utility than any of the other
η ≤ 1, while the “learning rate” K j (t) of the latter depends on        arms can be computed as the probability that all pairwise dif-
the current level of uncertainty.                                        ferences between the reward of an arm and the rewards of
                                                                         the other arms are greater than or equal to 0. For the current
Decay rule While the two previous learning rules assume                  task and generative model in Equation 1, there are three such
only the expectancy of the currently chosen deck is updated,             pairwise differences scores for each arm which follow a mul-
according to the (model-free) decay rule (e.g. Ahn et al.,               tivariate Normal distribution. Hence, the probability that arm
2008), expectancies of unchosen decks decay towards 0:                    j is chosen on trial t is
                  E j (t) = ηEg (t − 1) + δ j (t)u j (t)                                 P(C(t) = j) = P(∀k : u j (t) ≥ uk (t))
                                                                                                         Z ∞
where the decay parameter 0 ≤ η ≤ 1.                                                                  =      Φ(M j (t), H j (t))
                                                                                                           0
Choice rules
Choice rules describe how the expectancies are used to make              where Φ is the multivariate Normal density function with
a choice C(t) between the arms. We consider six choice rules,            mean vector M j (t) = A j E(t) and covariance matrix H j (t) =
the “probability of maximum utility” rule described in the               A j diag(S(t) + σ̂2ε )ATj , where diag(S(t) + σ̂2ε ) is a diagonal
introduction, and three more heuristic rules popular in rein-            matrix with values S j (t) + σ̂2ε and σ̂2ε the error variance as-
forcement learning (cf. Daw et al., 2006).                               sumed by the learner. The matrix A j computes the pairwise
                                                                         differences between deck j and the other decks. E.g.,
ε-greedy This choice rule exploits the arm with the maxi-                                           
                                                                                                     1 −1 0            0
                                                                                                                          
mum expectancy with probability 1 - ε, and with probability                                  A1 = 1 0 −1 0 
ε chooses randomly from the remaining bandits:                                                       1 0        0 −1
                         (
                           1 − ε if E j (t) > Ek (t), ∀k 6= j
       P(C(t) = j) =                                                     Model estimation and inference
                           ε/3     otherwise
                                                                         For each individual participant, model parameters were esti-
                                                                         mated by maximum likelihood using the Nelder-Mead sim-
Softmax (SM) The softmax rule can vary gradually be-                     plex algorithm implemented in the optim function in R. To
tween a pure exploitation (maximisation) and pure explo-                 evaluate the fit of the models, we computed the Akaike (AIC)
ration through an inverse temperature parameter θ(t):                    and Schwartz (BIC) information criteria, reported as differ-
                                     exp{θ(t)E j (t)}                    ence scores between a null model1 and the model of inter-
                P(C(t) = j) =      4                                     est (cf. Ahn et al., 2008). For these difference scores, neg-
                                 ∑k=1 exp{θ(t)Ek (t)}
                                                                         ative values of ∆(AIC) and ∆(BIC) indicate that the model
The temperature is constant in the fixed softmax (SM f ) mod-            fitted worse than the null model, while increasing positive
els: θ(t) = θ0 , with θ0 ≥ 0. In the dynamic softmax (SMd )              values indicate better fit. Finally, we computed Akaike and
models, the temperature can increase or decrease over trials             Schwarz weights, w(AIC) and w(BIC) (e.g., Wagenmakers &
according to the schedule θ(t) = [t/10]θ0 . In this case, θ0 can         Farrell, 2004). Schwarz (BIC) weights approximate the pos-
take values along the whole real line.                                   terior probability of the models (assuming equal prior proba-
                                                                         bility). Similarly, Akaike (AIC) weights can be interpreted as
Softmax with exploration bonus (SMEB) Exploration                        reflecting the probability that, given the observed data, a can-
can be increased by adding an “exploration bonus” term,                  didate model is the best model in the AIC sense (that it mini-
β j (t), to the Softmax rule:                                            mizes the Kullback-Leibler discrepancy) in the set of models
                                 exp{θ0 E j (t) + β j (t)}               under consideration.
             P(C(t) = j) =     4
                             ∑k=1 exp{θ0 Ek (t) + βk (t)}                                       Modelling results
The exploration bonus increases with uncertainty. For the                Table 1 contains the fit measures for all models. Focussing
Kalman filter model, we use the standard deviation       q   of the      first on the AIC and BIC difference scores, we see that on
prior distribution of mean utility: β j (t) = β0 S j (t) + σ̂2ζ ,        average the best fitting model is decay learning with fixed
with β0 > 0 and S j (t) computed as in Equation 4. As the                softmax choice (Decay-SM f ). In general, the Decay rule al-
Delta and Decay models do not provide measures of uncer-                 ways performs better (on average) than the other two learning
tainty, we used a simple heuristic according to which the un-                 1 The null model used was a simple multinomial model in which,
certainty increases linearly with the number of trials since a           on each trial, the deck choice is assumed to be an independent ran-
particular arm was last observed: β j (t) = β0 [t − T j ], where T j     dom draw from a multinomial distribution. The probabilities of this
                                                                         distribution were estimated for each participant and the null model
is the last trial before the current trial t in which deck j was         has three parameters (the probability for three bandits; the probabil-
chosen.                                                                  ity of other bandit deck follows immediately).
                                                                     1495

Table 1: Modelling results. Values of ∆(·) and w(·) are averages and the standard deviation is given in parentheses. Values of
n(·) are the total number of participants best fit by the corresponding model.
                Learning    Choice          ∆(AIC)         w(AIC)      n(AIC)          ∆(BIC)           w(BIC)       n(BIC)
                Bayesian    ε-greedy    190.39 (95.72)      0 (0)         0        183.79 (95.72)         0 (0)         0
                            SM f        237.37 (90.07)   0.04 (0.09)       2       230.78 (90.07)     0.01 (0.03)       0
                            SMd         236.22 (90.09)   0.06 (0.17)       2       229.62 (90.09)     0.03 (0.13)       2
                            SMEB         226.71 (94.8)   0.01 (0.02)      0         216.81 (94.8)         0 (0)         0
                            PMU        223.43 (107.36)    0.25 (0.4)      21      220.13 (107.36)     0.26 (0.41)      21
                Delta       ε-greedy    192.81 (94.36)      0 (0)         0        189.51 (94.36)         0 (0)         0
                            SM f          237.5 (91.7)   0.06 (0.13)       6         234.2 (91.7)      0.09 (0.2)       8
                            SMd          236.68 (91.4)   0.11 (0.24)      12        233.38 (91.4)      0.15 (0.3)      12
                            SMEB         231.39 (93.4)   0.02 (0.05)      0          224.8 (93.4)     0.01 (0.01)       0
                Decay       ε-greedy    203.76 (94.01)   0.01 (0.11)      1        200.46 (94.01)     0.01 (0.11)       1
                            SM f        244.55 (91.69)   0.19 (0.28)      18       241.25 (91.69)     0.22 (0.33)      18
                            SMd          240.59 (95.7)   0.2 (0.33)       17        237.29 (95.7)     0.22 (0.34)      17
                            SMEB         240.5 (91.99)   0.05 (0.08)      0         233.9 (91.99)     0.01 (0.02)       0
rules. This is a common finding which is likely due to the            ability in a set of competing models. This supports previous
ability of this rule to capture people’s tendency to repeat pre-      findings by Knox et al. (2012) in a simpler task, but contrasts
vious choices (Ahn et al., 2008). While the delta learning rule       with the findings of Daw et al. (2006) who found no influence
performs on average a little better than Bayesian updating            of uncertainty in a task similar to the one used here. Appar-
with the Kalman filter, the differences between these learn-          ently, uncertainty affects decisions in a more refined way than
ing rules are less marked. Out of the different choice rules,         in the model of Daw et al., through its effect on predicted dis-
the ε-greedy rule clearly performs the worst, while the fixed         tributions of the utility associated with the options.
Softmax (SM f ) rule performs best for each learning rule.
    When we look at the number of participants who are best                                 Acknowledgements
fit by each model, a different picture arises: Bayesian up-           We thank James Carragher, Belle Cartwright, Melvin Wan
dating with the probability of maximum utility choice rule            and Tiffany Yung for their assistance in collecting the data.
(Bayesian-PMU) fits more participants best than any of the
other models. This is also the model with the highest aver-                                       References
age Akaike and Schwarz weights. As such, the evidence for             Acuna, D., & Schrater, P. (2008). Bayesian modeling of human
                                                                         sequential decision-making on the multi-armed bandit problem.
this model is more marked than the evidence for the other                In B. C. Love, K. McRae, & V. M. Sloutsky (Eds.) Proceedings
models. The discrepancy between the results for the AIC and              of the 30th Annual Conference of the Cognitive Science Society,
BIC difference scores on the one hand, and the Akaike and                (pp. 200–300).
Schwarz weights on the other, is due to the fact that when the        Ahn, W.-Y., Busemeyer, J. R., Wagenmakers, E.-J., & Stout, J. C.
                                                                         (2008). Comparison of decision learning models using the gener-
Bayesian-PMU model fits best, it fits decidedly better than the          alization criterion method. Cognitive Science, 32, 1376–1402.
other models, resulting in weights close to 1. For participants       Cohen, J. D., McClure, S. M., & Yu, A. J. (2007). Should I stay or
with a different best fitting model, the differences between the         should I go? How the human brain manages the trade-off between
AIC and BIC values of the next best-fitting models are gener-            exploitation and exploration. Philosophical Transactions of the
                                                                         Royal Society B, 362, 933–942.
ally smaller, resulting in less extreme weights. In conclusion,       Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., & Dolan, R. J.
there is strong evidence that uncertainty regarding an arm’s             (2006). Cortical substrates for exploratory decisions in humans.
mean reward influenced a substantial number of participants’             Nature, 441, 876–879.
decisions. While this contradicts the findings of Daw et al.          Gittins, J. C. (1979). Bandit processes and dynamic allocation in-
                                                                         dices. Journal of the Royal Statistical Society. Series B, 41, 148–
(2006), they only considered the heuristic Bayesian-SMEB to              177.
model the influence of uncertainty, for which we also found           Knox, W. B., Otto, A. R., Stone, P., & Love, B. C. (2012). The nature
little evidence. Clearly, the effect of uncertainty on explo-            of belief-directed exploratory choice in human decision-making.
rative decisions is more nuanced (cf. Cohen et al., 2007).               Frontiers in Psychology, 2:398, 1–12.
                                                                      Steyvers, M., Lee, M. D., & Wagenmakers, E.-J. (2009). A Bayesian
                                                                         analysis of human decision-making on bandit problems. Journal
                           Conclusion                                    of Mathematical Psychology, 53, 168–179.
We found evidence that a substantial proportion of our par-           Wagenmakers, E.-J., & Farrell, S. (2004). AIC model selection using
ticipants explored to reduce uncertainty in their beliefs. This          Akaike weights. Psychonomic Bulletin & Review, 11, 192–196.
was evident from the finding that participants switched decks         Whittle, P. (1988). Restless bandits: Activity allocation in a chang-
                                                                         ing world. Journal of Applied Probability, 25, 287–298.
more in periods of increased volatility, and from computa-
tional modelling where a model that balances exploration and
exploitation according to the probability that arms provide
the maximum utility fitted the largest number of individual
participants and, on average, had the highest posterior prob-
                                                                  1496

