UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Social Categorization and Cooperation between Humans and Computers
Permalink
https://escholarship.org/uc/item/5k62p6c3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
de Melo, Celso
Carnevale, Peter
Gratch, Jonathan
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Social Categorization and Cooperation between Humans and Computers
                                                  Celso M. de Melo (demelo@usc.edu)
                                     Peter J. Carnevale (peter.carnevale@marshall.usc.edu)
                                                       USC Marshall School of Business
                                                      Los Angeles, CA 90089-0808 USA
                                                Jonathan Gratch (gratch@ict.usc.edu)
                             USC Institute for Creative Technologies, 12015 Waterfront Drive, Building #4
                                                      Playa Vista, CA 90094-2536 USA
                               Abstract                                   of this categorization, people will conform more to the
   Computers increasingly perform a variety of important tasks
                                                                          values and norms of the group, and tend to favor the in-
   and services that influence individuals and organizations, yet         group to the out-group – a phenomenon referred to as in-
   few studies tell us about how humans interact with computers           group bias. One consequence of this bias is that people trust
   and other non-human decision-makers. In four experiments,              and cooperate more with in-group than out-group members
   we asked people to engage in cooperation tasks with                    (Sherif, Harvey, White, Hood, & Sherif, 1961).
   computers and with humans. Experiment 1 found that people                 We propose that people make a basic distinction between
   gave more money to a human than a computer. We argue this              humans and computers in terms of membership in the
   effect reflects a basic bias in favor of humans, which are
   perceived to be the in-group, when compared to computers,
                                                                          “human social category”. People are capable of
   which are perceived to be the out-group. In Experiment 2, we           dehumanizing and subsequently discriminating against
   varied computer and human ethnicity to be the same or                  others that are perceived to lack certain mental abilities
   different as the participant; results indicated that ethnicity had     (Haslam. 2006). These abilities are of two types (Gray,
   a parallel but additive effect that was independent to the effect      Gray, & Wegner, 2007; Loughnan & Haslam, 2007):
   of the human social category. The data of Experiment 3                 agency, the capacity to act and plan; and, experience, the
   indicate that it is also possible to promote group membership          capacity to sense and feel. Our proposal rests on research
   with computers by creating structural interdependence based
   on shared incentives. Finally, we demonstrate in Experiment            that shows that people perceive computers to possess less of
   4 that our framework based on social categorization theory             these mental abilities than humans (Blascovich et al., 2002;
   can predict situations where people will cooperate more with           Gray et al., 2007; Waytz, Gray, Epley, & Wegner, 2010).
   computers than with humans. We discuss implications for                Further research shows that people tend to show stronger
   understanding people’s decision making with human and non-             activation with humans, when compared to computers, of
   human others.                                                          brain regions associated with mentalizing (i.e., the inferring
   Keywords: Human vs. Computers; Decision Making;                        of others’ mental states) and the experience of affect
   Cooperation; Social Categorization; Group Membership.                  (Gallagher, Anthony, Roepstorff, & Frith, 2002; Krach et
                                                                          al., 2008; McCabe, Houser, Ryan, Smith, & Trouard, 2001;
                           Introduction                                   Rilling et al., 2002; Sanfey, Rilling, Aronson, Nystrom, &
Computers are routinely involved in decisions that affect the             Cohen; 2003). We, therefore, posit that denial of
lives of individuals and organizations (Davenport & Harris,               membership in the human social category to computers is
2005). The remarkable growth of business conducted online                 due to this perceived difference in mental abilities. To test
(U.S. Census Bureau, 2011) and the increasing amounts of                  the existence of a bias in favor of humans, we compared
time people spend interacting via social media (Honignam,                 participants’ money offers to computers with offers to
2012), suggests computers will play an even more pervasive                humans, in prototypical decision making tasks (Experiments
role on people’s social, economic, and political life. As a               1, 2, and 3).
consequence of these changes, people are faced with having                   Human identities, however, are complex and
to make decisions with, not human but, artificial decision                multifaceted. In many settings, more than one social
makers. However, despite the importance of the issue, there               category (e.g., gender, age, ethnicity) may be relevant and
has been remarkably little research on the nature of people’s             influence behavior (Crisp & Hewstone, 2007). On the one
decision making with such non-human counterparts.                         hand, context can prime one category to become more
   We propose that social categorization theory is a useful               dominant (or salient) and effectively exclude the influence
framework for understanding how humans reach decisions                    of the others (e.g., Shih, Pittinsky, & Ambady, 1999). On
with computers (Tajfel & Turner, 1986; Turner, Oakes,                     the other hand, social categories can be simultaneously
Haslam, & McGarty, 1994; Crisp & Hewstone, 2007). One                     salient and have an additive effect on people’s behavior
proposition of the theory is that people categorize others                (Crisp & Hewstone, 2007).
into groups while associating, or self-identifying, more with                Our second proposal, therefore, is that multiple social
some (the in-groups) than others (the out-groups). Because                categorization also applies in human-computer interaction.
                                                                          In particular, we posit that the effects of the human social
                                                                      2109

category can combine in additive fashion with the effects of        matched with counterparts of the same gender. Participants
other social categories. Earlier research has already shown         were debriefed at the end regarding these deceptions.
that people can apply human stereotypes to computers (Nass             To facilitate interpretation, we converted participants’
& Moon, 2000): in one experiment, in line with gender               offers into percentages (over the total amount of 20 tickets).
stereotypes, people assigned more competence to computers           As predicted, the results revealed that participants offered
with a female voice than a male voice on the topic of “love         more tickets to avatars (M = 32.61, SD = 18.21) than to
and relationships”; in another experiment, people perceived         agents (M = 15.43, SD = 15.52), t(46) = 6.702, p = .000, r
computers with a virtual face of the same ethnicity as being        =.703, mean difference = 17.18, 95% CI [12.02, 22.34].
more trustworthy and giving better advice than a computer              The results confirm that people show a bias that favors
with a face of a different ethnicity. Aside from manipulation       humans to computers in the dictator game. We argue this
of characteristics of the computers, research has also shown        occurs because computers are perceived to lack in certain
that it is possible to create group membership with                 mental abilities (Blascovich et al., 2002; Gray et al., 2007;
computers by manipulating characteristics of the situation.         Waytz et al., 2010) and, thus, are perceived as out-group
Nass, Fogg, & Moon (1996) showed that people can favor a            members (in the human social category).
computer that belongs to the team, as defined by
interdependence in the task’s payoffs, when compared to a                                    Experiment 2
non-team computer. Thus, to test our proposal, we present           In Experiment 2, we introduced a new social category –
several experiments where people engaged with humans and            ethnicity of the counterpart – and tested whether its effect
computers but, we also introduced additional social                 dominated, combined, or was dominated by the effect of the
categories based on manipulation of the characteristics of          human social category. Although racial discrimination is on
the computers, namely ethnicity (Experiments 2 and 4), and          the decline (e.g., Ford, 2008), people tend to make
of the situation, namely interdependence through shared             automatic distinctions based on race, which can produce
payoffs (Experiments 3 and 4). We test whether the effect           subtle forms of racial discrimination (Gaertner & Dovidio,
of the human social category on participants’ offers                2005). In human-computer interaction, previous studies had
dominates or combines, in additive fashion, with the effects        also shown that computers with a visual representation
of the other categories.                                            corresponding to the same ethnicity were perceived more
                                                                    favorably (Nass & Moon, 2000; Rossen, Johnsen,
                       Experiment 1                                 Deladisma, Lind, & Lok, 2008). However, this earlier work
Experiment 1 tests whether people make a basic distinction,         focused on subjective impressions, whereas our experiment
in a decision making context, that favors humans when               is the first, to the best of our knowledge, to test the effect of
compared to computers. Participants engaged in a simple             ethnicity on behavioral measures in a decision task.
task, the dictator game (Forsythe, Horowitz, Savin, &                  Participants engaged in the dictator game, in a between-
Sefton, 1994), with computers that were perceived to be             participants factorial design, with (perceived) agents or
controlled by computer algorithms – agents – or computers           avatars that had a virtual face corresponding to either the
that were perceived to be controlled by other participants –        same or different ethnicity as the participant. For instance, if
avatars. The dictator game involves two players: a sender           the participant was Caucasian then, in the ‘different
and a receiver. The sender gets 20 tickets and decides how          ethnicity’ condition, the counterpart’s virtual face would be
many to give the receiver, who has no choice but to accept          randomly chosen from one of the following: African-
it. The tickets had financial consequences, as they would go        American, Hispanic, Southeast Asian, or East Indian. We
into a lottery for a prize in real money. Participants were         tested three competing hypotheses: 1) if the human category
told that agents would participate in the lottery and, if an        is more salient than ethnicity, then people should not
agent won, no one would get the prize. Because there is no          distinguish between agents of the same or different
material incentive to offer anything, the game is seen as an        ethnicity; 2) if the ethnicity category is more salient than the
index of altruism. Previous experiments show that 60% of            human category, then people should not distinguish between
participants tend to offer something and, on average, 28% of        agents and avatars of the same ethnicity; 3) if the human
the pie is offered (Engel, 2011). In our experiment,                and ethnicity categories are independent, then they should
participants always played the role of the sender and               lead to an additive effect on people’s offers. Figure 1 shows
engaged, in a repeated measures design, with agents and             the ethnicities we considered and some of the corresponding
avatars. We recruited 47 participants on Amazon                     virtual faces. We recruited 184 participants at the USC
Mechanical Turk for this experiment. To support the                 Marshall School of Business. Most participants were
deception pertaining to avatars, before starting the task,          Caucasian (32.1%) or Southeast Asian (56.0%). To support
participants were asked to wait for a “server to connect them       the deception related to avatars, we ran 12 participants per
to other participants”; moreover, while they waited they saw        session, and instructed them that “other participants will be
information that several other participants were already            controlling” the avatars. Moreover, we also told them that
connected to the server. In fact, there was no server. Finally,     they “would connect to a server that matches players with
in all our experiments, participants were supposedly                each other”. Participants were debriefed at the end regarding
                                                                    this deception.
                                                                2110

                                                                    individual gets a higher payoff by defecting rather than
                                                                    cooperating, regardless of what others in society do, yet all
                                                                    individuals end up receiving a lower payoff if all defect than
                                                                    if all cooperate (Dawes, 1980). Specifically, the nested
                                                                    social dilemma is a 6-player task where the participant is
                                                                    randomly allocated to position A, B, C, D, E or F and
                                                                    accordingly assigned to group ABC or DEF. The participant
                                                                    is given 30 lottery tickets that can be invested in three
                                                                    accounts: the private, in-group and all accounts. Tickets
                                                                    invested to the private account are multiplied by 1.0 and
                                                                    returned to the participant; tickets invested to the in-group
                                                                    account, which is referred to in the instructions as the
     Figure 1: The ethnicities and some of the virtual faces        “group account”, are multiplied by 2.5 and split equally
                 used in Experiments 2 and 4.                       among all group members; tickets invested to the all
                                                                    account are multiplied by 4.0 and split equally by all six
  To facilitate interpretation, we converted offers into            players. These payoff characteristics create interdependence
percentages (over the total amount of 20 tickets). The              among group members and preserve the defining properties
average offers are shown in Figure 2. To analyze the data           of a social dilemma: irrespective of others’ allocations,
we ran an Other (Agent vs. Avatar) × Ethnicity (Same vs.            shifting points from a higher to a lower level account always
Different) ANOVA. The results replicated the main effect of         increased one’s individual final payoff; however, if
Other reported in Experiment 1: people offered more tickets         everyone is selfish and invests in a lower account, then
to avatars (M = 20.63, SD = 20.27) than to agents (M =              everyone is worse off than if they had invested in a higher
15.44, SD = 17.37), F(1, 180) = 4.08, p = .045, partial η2 =        account.
.022. The results also confirmed a main effect of Ethnicity:           The experiment followed a 2 × 2 between-participants
people offered more to counterparts of the same ethnicity           factorial design: In-Group (Agents vs. Avatars) × Out-
(M = 23.26, SD = 20.21) than counterparts of a different            Group (Agents vs. Avatars). In line with earlier work on the
ethnicity (M = 12.80, SD = 16.20), F(1, 180) = 15.55, p =           in-group bias (Sherif et al., 1961), we expected people to
.000, partial η2 = .080. Finally, the results revealed no Other     favor in-group avatars to out-group avatars. Since a previous
× Ethnicity interaction, F(1, 180) = .347, p = .556.                study had already shown that people can favor a computer
  Thus, the results show that multiple social categories can        that belongs to the team when compared to a non-team
be applied to computers; moreover, the results suggest that         computer (Nass et al., 1996), we also expected people to
the human and ethnicity categories can have an independent          favor in-group agents to out-group agents. When engaging
and additive effect on participants’ money offers.                  with in-group avatars and out-group agents, we expected
                                                                    people to strongly favor the in-group not only because they
                                                                    belonged to the interdependent group but also to the human
                                                                    social category. The last case is more interesting: when
                                                                    engaging with in-group agents and out-group avatars,
                                                                    interdependence favors the agents but people also identify
                                                                    with the human social category of the out-group. Following
                                                                    the results in the previous experiment we expected these
                                                                    two influences to cancel each other out, which would result
                                                                    in no preference between the in- and out-groups. We
                                                                    recruited 116 participants at the USC Marshall School of
                                                                    Business. To support the deception pertaining to avatars, we
                                                                    followed a similar procedure to Experiment 2.
   Figure 2: Average offers in Experiment 2. The error bars            To facilitate interpretation, we converted ticket
                   show the standard errors.                        allocations into percentages. As expected people invested
                                                                    more in the private than the other accounts (private: M =
                       Experiment 3                                 66.23, SD = 26.38; in-group: M = 21.26, SD = 19.60; all: M
                                                                    = 12.51, SD = 16.92); however, to test our hypotheses we
In contrast to Experiment 2, which created group                    focused on a measure for the in-group bias, which we
membership by manipulating a (visual) characteristic of the         operationalize as the difference between allocations to the
counterpart, Experiment 3 manipulated group membership              in-group and the all accounts. Figure 3 shows the means and
by creating payoff interdependence among players. To                confidence intervals for this measure for each condition. We
achieve this we used a decision making task, the nested             then ran an In-Group × Out-Group ANOVA on this
social dilemma (Wit & Kerr, 2002), which splits players             measure. The results revealed a statistically significant In-
into groups and bids group interests against collective             Group × Out-Group interaction, F(1, 118) = 4.13, p = .044,
interests. Generally, a social dilemma is a situation where an
                                                                2111

partial η2 = .034. To further understand this interaction we       procedure to support the deception regarding avatars was
ran one-way t-tests, for each condition, to test if in-group       similar to the one used in Experiment 1.
bias was different than zero. For in-group avatars and out-           To facilitate interpretation, we converted ticket
group avatars, in-group bias was statistically significantly       allocations into percentages. As in the previous experiment,
different than zero, t(28) = 2.63, r = .445, mean difference =     we use the difference between allocations to the in-group
12.64, 95% CI [2.79, 22.50]. For in-group agents and out-          and all accounts as a measure of the in-group bias. Means
group agents, in-group bias was also statistically significant,    and confidence intervals for this measure are shown in
t(29) = 2.644, r = .441, mean difference = 14.33, 95% CI           Figure 4. We ran one-way t-tests to compare the differences
[3.25, 25.42]. For in-group avatars and out-group agents, in-      in each condition to zero: if the difference is statistically
group bias was once more statistically significant, t(31) =        significantly different from zero, then there is evidence for
3.73, r = .557, mean difference = 7.81, 95% CI [3.54,              an in-group bias. The results confirmed our prediction: for
12.08]. However, for in-group agents and out-group avatars,        in-group agents and out-group avatars of the same ethnicity,
in-group bias was not statistically significantly different        the in-group bias was not statistically significantly different
from zero, t(30) = .122, r = .022, mean difference = .65,          than zero, t(22) = .417, p = .681, r = .089; however, for in-
95% CI [-10.15, 11.44].                                            group agents of the same ethnicity and out-group avatars of
   The results, thus, confirmed our prediction that people         a different ethnicity, the in-group bias was statistically
would favor the in-group in all cases, except when the in-         significant, t(23) = 2.13, p = .044, r = .406.
group was composed of agents and the out-group of avatars.            The results, therefore, showed that by associating more
                                                                   positive social categories with computers than with humans,
                                                                   it is possible to overcome people’s bias in favor of humans.
    Figure 3: In-group bias in Experiment 3. The error bars
                   show the standard errors.
                       Experiment 4
The previous experiments showed that it is possible to                  Figure 4: In-group bias in Experiment 4. The error bars
compensate for the fact that computers do not belong to the                            show the standard errors.
human social category by changing the computer’s visual
appearance (Experiment 2) or the structure of the task                                      Discussion
(Experiment 3). In Experiment 4 we wanted to test if it was        As computational systems take an active role in today’s
possible to over-compensate and have people offer more to          society, it becomes important to understand how people
computers than to humans. To accomplish this we had                reach decisions with non-human decision makers. Social
participants engage, in a between-participants design, in the      categorization theory (Tajfel & Turner, 1986; Turner et al.,
nested social dilemma with an in-group that was always             1994; Crisp & Hewstone, 2007) provides important insights.
composed of agents of the same ethnicity as the participant        Accordingly, we argue that the same mechanism whereby
but, with an out-group that was composed of avatars of             people form social categories that include some people (the
either the same or a different ethnicity than that of the          in-groups) and exclude others (the out-groups) extends to
participant. For the case where both the in-group agents and       computers. At first blush, people categorize humans as
out-group avatars had the same ethnicity, we expected to           being in-group members, in the human social category, and
replicate the result in the previous experiment, i.e., no          computers as the out-group. This differentiation is
preference between the in- and out-groups. For the case            motivated by perceptions that humans possess more
where the out-group was composed of avatars of a different         sophisticated mental abilities than computers (Blascovich et
ethnicity, we expected people to favor the in-group agents.        al., 2002; Gray et al., 2007; Waytz et al., 2010).
The rationale is that in this case two categories (ethnicity       Consequently, people tend to favor humans over computers
and payoff-defined group membership) favored agents and            in resource allocation (Haslam, 2006; Loughnan & Haslam,
only one favored avatars (human category). We recruited 47         2007). However, by manipulating the characteristics of
participants on Amazon Mechanical Turk. Most participants          artificial decision makers (e.g., their ethnicity) or of the
were Caucasian (25.5%) or East Indian (59.6%). The                 situation (e.g., payoff interdependence) it is possible to
                                                               2112

bring other social categories into play, which can                simultaneously (Crisp & Hewstone, 2007). Our results show
compensate (and even over-compensate) for people’s bias in        clear evidence for an additive pattern that reflects a positive
favor of humans.                                                  correlation between group differentiation and intergroup
   To support our proposal we presented four experiments          bias. However, researchers have also pointed out that, for
where participants engaged with computers perceived to be         people that strongly identify with the group, this correlation
controlled by algorithms (agents) or by humans (avatars).         can be negative, i.e., the less the differentiation, the higher
Experiment 1 showed that people offered in the dictator           the bias (Brewer, 1991). The rationale is that bringing
game more money to avatars than to agents, supporting the         groups together via shared categories might threaten
existence of a bias in favor of humans. Experiment 2              people’s desire for distinctiveness. Future work, thus,
introduced a second social category – the counterpart’s           should explore if in-group identification can also moderate
ethnicity – and data indicated that people offered more           people’s bias in favor of humans when engaging with
money to any counterpart, avatar or agent, that had the same      computers in other settings.
ethnicity. Experiment 2 also indicated that the effects of the      From a practical point of view, this work emphasizes the
human and ethnicity social categories can be independent          importance of considering appropriate social and cognitive
and additive, thus suggesting that the human/computer             psychological theories of human behavior when designing
categorization is another in the long list of categories that     artificially intelligent decision makers. It is important
people use in social life (Crisp & Hewstone, 2007).               designers understand and compensate for people’s tendency
Experiment 3, in turn, demonstrated that it is also possible      to reach different decisions according to whether they
to promote group membership with computers by creating            perceive computers to be driven by a human or by computer
structural interdependence, based on shared payoffs. The          algorithms. Superficially, designers could try to de-
results showed that in the nested social dilemma when the         emphasize that certain decisions are being made
in-group was composed of agents and the out-group of              autonomously; however, there are ethical and legal concerns
avatars, people would not favor avatars anymore. Finally,         that might limit this type of approach. For instance, the
Experiment 4 showed further that people can favor agents to       UK’s 1998 Data Protection Act gives employees the right to
avatars if the former are associated with more positive           ask for human intervention in the case of any decision made
categories than the latter. Effectively, people offered more      solely by automated means, when personal data is involved.
in the nested social dilemma to in-group agents of the same       Looking instead to the vast literature on intergroup conflict
ethnicity than to out-group avatars of a different ethnicity.     resolution suggests a set of more-principled design
   These results have implications for understanding              guidelines. First, personalizing out-group members and
people’s behavior with computers. The “computers are              increasing intergroup contact can reduce in-group bias and
social actors” theory (Reeves & Nass, 1996) introduced the        prejudice (Pettigrew & Tropp, 2006). In the case of artificial
idea that people can treat computers in a fundamentally           decision makers, this suggests that designers should strive to
social manner. Indeed our results demonstrated that, even in      increase visibility and transparency for the mechanisms and
the absence of financial incentives, people were willing to       reasons behind the decisions that were made. Second,
offer money to computers (Experiments 1 and 2). However,          creating cooperation through shared goals is known to
our results complement this view by demonstrating that            reduce intergroup competition (Sherif et al., 1961). This
there are still important differences in the way people treat     suggests, for instance, emphasizing that the policies that
computers in social settings, when compared to humans.            decision algorithms implement serve the common good or
Everything else being equal, people tended to favor humans        creating payoff interdependence between humans and
(the in-group) to computers (the out-group). The results          computers. Finally, in case of severe conflict, designers can
suggest that social categorization is a useful framework to       resort to negotiation techniques (Pruitt & Carnevale, 1993)
understand people’s decision-making behavior with                 to resolve divergence of interest between humans and
computers.                                                        computers, an approach that is in fact an active topic of
   The results comport with findings that people naturally        research (Lin & Kraus, 2010).
attribute more mind to humans than computers (Blascovich            A multi-disciplinary perspective is critical for
et al., 2002; Gray et al., 2007; Waytz et al., 2010). The         understanding how people adapt to changing conditions in
expectation of less mental abilities could be the fundamental     an evolving world and, in particular, carry the mechanisms
reason people fail to treat computers as in-group members         of human-human interaction into human-computer
and, consequently, show a bias in favor of humans. Future         interaction. With distinct cooperation tasks and with
work should, therefore, test the prediction that proper           different kinds of populations, we showed that the
simulation of appropriate mental abilities suffices to make       mechanisms of social categorization and intergroup
people treat computers in the same manner as humans, at           behavior can explain people’s interaction with computers
least in the context of decision-making tasks with clear          that make decisions. Future work should further explore
financial incentives.                                             more decision contexts, more social categories (e.g., age,
   This work also presents further evidence that people have      gender, culture), more roles (e.g., receiver), other kinds of
the cognitive resources and motivation to classify                machines (e.g., robots), and determine the sufficient
themselves and others along multiple categories
                                                              2113

conditions artificial decision makers should possess in order     Loughnan, S., & Haslam, N. (2007). Animals and androids:
to be treated in the same manner as human decision makers.          Implicit associations between social categories and
                                                                    nonhumans. Psychological Science, 18, 116-121.
                    Acknowledgments                               McCabe, K., Houser, D., Ryan, L., Smith, V., & Trouard, T.
This research was supported in part by the following grants:        (2001). A functional imaging study of cooperation in two-
FA9550-09-1-0507 from the Air Force Office of Scientific            person reciprocal exchange. Proc. National Academy of
Research, IIS-1211064 and SES-0836004 from the National             Sciences, 98, 11832-11835.
Science Foundation. The content does not necessarily reflect      Nass, C., & Moon, Y. (2000). Machines and mindlessness:
the position or the policy of any Government, and no                Social responses to computers. Journal of Social Issues,
official endorsement should be inferred.                            56, 81-103.
                                                                  Nass, C., Fogg, J., & Moon, Y. (1996). Can computers be
                         References                                 teammates? International Journal of Human-Computer
                                                                    Studies, 45, 669-678.
Blascovich, J., Loomis, J., Beall, A., Swinth, K., Hoyt, C.,      Pettigrew, T., & Tropp, L. (2006). A meta-analytic test of
  & Bailenson, J. (2002). Immersive virtual environment             intergroup contact theory. Journal of Personality Social
   technology as a methodological tool for social                   Psychology, 90, 751-783.
   psychology. Psychological Inquiry, 13, 103-124.                Pruitt, D., & Carnevale, P. (1993). Negotiation in social
Brewer, M. (1991). The social self: On being the same and           conflict. (Brooks/Cole, Pacific Grove, CA).
  different at the same time. Personality and Social              Reeves, B., & Nass, C. (1996). The media equation: How
  Psychology Bulletin, 17, 475-482.                                 people treat computers, television, and new media like
Crisp, R., & Hewstone, M. (2007). Multiple social                   real people and places (Cambridge University Press, New
  categorization. Advances in Experimental Social                   York).
  Psychology, 39,163-254.                                         Rilling, J., Gutman, D., Zeh, T., Pagnoni, G., Berns, G. &
Davenport, T., & Harris, J. (2005) Automated decision               Kilts, C. (2002). A neural basis for social cooperation.
  making comes of age. Sloan Management Review, 46, 83-             Neuron, 35, 395-405.
  89.                                                             Rossen, B., Johnsen, K., Deladisma, A., Lind, S., & Lok, B.
Dawes, R (1980). Social dilemmas. Annual Review of                  (2008). Virtual humans elicit skin-tone bias consistent
   Psychology, 31, 169-193.                                         with real-world skin-tone biases. In Proceedings of the
Engel, C. (2011). Dictator games: A meta study.                     Intelligent Virtual Agents Conference, pp. 237-244.
   Experimental Economics, 14, 583-610.                           Sanfey, A., Rilling, J., Aronson, J., Nystrom, L., & Cohen,
Ford, R. (2008). Is racial prejudice declining in Britain?          J. (2003). The neural basis of economic decision-making
   British Journal of Sociology, 59, 609-634.                       in the ultimatum game. Science, 300, 1755-1758.
Forsythe, R., Horowitz, J., Savin, N., & Sefton, M. (1994).       Sherif, M., Harvey, O., White, B., Hood, W., & Sherif, C.
  Fairness in simple bargaining experiments. Games and              (1961). Intergroup conflict and cooperation: The Robbers
  Economic Behavior, 6, 347-369.                                    Cave experiment. Norman, OK: University Book
Gaertner, S., & Dovidio, J. (2005). Understanding and               Exchange.
   addressing contemporary racism: Aversive racism to the         Shih, M., Pittinsky, T., & Ambady, N. (1999). Stereotype
   common intergroup identity model. Journal of Social              susceptibility: Identity salience and shifts in quantitative
   Issues, 61, 615-639.                                             performance. Psychological Science, 10, 80-83.
Gallagher, H., Anthony, J., Roepstorff, A., & Frith, C. 2002.     Tajfel, H., & Turner, J. (1986). The social identity theory of
   Imaging the intentional stance in a competitive game.            intergroup behavior. In S. Worchel & W. Austin (Eds.),
   NeuroImage, 16, 814-821.                                         Psychology of intergroup relations (pp 7-24). Chicago:
Gray, H., Gray, K, & Wegner, D. 2007. Dimensions of mind            Nelson-Hall.
   perception. Science, 315, 619.                                 Turner, J., Oakes, P., Haslam, S., & McGarty, C. (1994)
Haslam, N. (2006). Dehumanization: An integrative review.           Self and collective: Cognition and social context.
   Personality and Social Psychology Review, 10, 252-264.           Personality and Social Psychology Bulletin, 20, 454-465.
Honignam, B. (2012). 100 Fascinating social media                 U.S. Census Bureau. (2011). E-stats - Measuring the
   statistics and figures from 2012. The Huffington Post.           electronic         economy.         Retrieved         from:
   Retrieved from: http://www.huffingtonpost.com/brian-             http://www.census.gov/econ/estats/2011reportfinal.pdf.
  honigman/100-fascinating-social-me_b_2185281.html.              Waytz, A., Gray, K., Epley, N., & Wegner, D. (2010).
Krach, S., Hegel, F., Wrede, B., Sagerer, G., Binkofski, F.,        Causes and consequences of mind perception. Trends in
   & Kircher, T. (2008). Can machines think? Interaction            Cognitive Science, 14, 383-388.
   and perspective taking with robots investigated via fMRI.      Wit, A., & Kerr, N. (2002). “Me versus just us versus us all”
   PLoS ONE, 3, 1-11.                                               categorization and cooperation in nested social dilemmas.
Lin, R., & Kraus, S. (2010). Can automated agents                   Journal of Personality and Social Psychology, 3, 616-
   proficiently negotiate with humans? Communications of            637.
   the ACM, 53, 78-88.
                                                              2114

