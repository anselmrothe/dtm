UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Percentile analysis for goodness-of-fit comparisons of models to data
Permalink
https://escholarship.org/uc/item/21k702vf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Khemlani, Sangeet
Trafton, Greg
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

                                 Percentile analysis for goodness-of-fit comparisons of models to data
                                                                   Sangeet Khemlani and J. Gregory Trafton
                                                                 skhemlani@gmail.com, trafton@nrl.navy.mil
                                                       Navy Center for Applied Research in Artificial Intelligence
                                                      US Naval Research Laboratory, Washington, DC 20375 USA
                                               Abstract                                         A common way of assessing the fit of a model to data is
                     In cognitive modeling, it is routine to report a goodness-of-fit
                                                                                              to employ statistical goodness-of-fit measures. One such
                     index (e.g., R2 or RMSE) between a putative model's                      measure is the coefficient of determination (R2), which is
                     predictions and an observed dataset. However, there exist no             often interpreted as the proportion of variance explained by
                     standard index values for what counts as “good” or “bad”, and            the model. Another is the root mean squared error (RMSE),
                     most indices do not take into account the number of data                 a measure of the residuals between expected and observed
                     points in an observed dataset. These limitations impair the              values. Generally, R2 is used to characterize the precision of
                     interpretability of goodness-of-fit indices. We propose a                a model, and RMSE is used to characterize a model’s
                     generalized methodology, percentile analysis, which
                     contextualizes goodness-of-fit measures in terms of                      accuracy at accounting for a given dataset. They are often
                     performance that can be achieved by chance alone. A series of            reported in concert with one another (Schunn & Wallach,
                     Monte Carlo simulations showed that the indices of                       2005) under the assumption that a good model must be both
                     randomized models systematically decrease as the number of               precise and accurate. The indices are used ubiquitously as a
                     data points to be fit increases, and that the relationship is            method of model evaluation in computational cognitive
                     nonlinear. We discuss the results of the simulation and how              science research (Busemeyer & Diederich, 2010). Indeed, in
                     computational cognitive modelers can use them to place
                                                                                              the Proceedings the 35th Annual Conference of the
                     commonly used fit indices in context.
                                                                                              Cognitive Science Society (CogSci 2013) alone, 51 out of
                     Keywords:     goodness-of-fit,     computational      cognitive          171 papers (30%) self-identified as pertaining to
                     modeling, percentile analysis                                            computational modeling made use of at least one of the two
                                                                                              indices to assess the fit of a cognitive model to empirical
                                           Introduction                                       data. For instance, when Kachergis and Yu (2013) applied a
          A common methodological practice for cognitive science                              computational model of cross-situational word learning
        researchers is to assess the merits of a cognitive model by                           (Kachergis, Yu, & Shiffrin, 2012) to an experiment they
        evaluating its ability to capture the dynamics of a relevant                          conducted, it revealed a high R2 value (= .98), and the
        dataset. For example, an adequate model of list memory                                authors noted that their model “achieved quite a good fit to
        might be one that captures appropriate serial position effects                        the data” (p. 713) and that it “could account for individuals’
        as well as other related psychological phenomena (e.g.,                               behavior in each of the conditions [of their experiment]” (p.
        Anderson, Bothell, Lebiere, & Matessa, 1998). The                                     715). On the assumption that a good model must account for
        predictions derived from such a model can themselves be                               a generous proportion of the variance in a given dataset, R2
        tested experimentally. Thus, comparisons of theoretical                               provides a readily interpretable metric with which to
        predictions to empirical data reflect an alternating dialectic                        evaluate and optimize computational models.
        between theory building and experimentation (cf.                                        Our present analysis focuses on the limitations of using
        McClelland, 2009).                                                                    R2 and RMSE as model evaluation metrics. Consider the fit
                                                                                              of two hypothetical cognitive models, Theory A and Theory
                            R² = 0.93                 R² = 0.2
Proportion correct
                     1.00
                     0.75
                     0.50
                     0.25
                     0.00
                             1   2   3                1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
                            Theory A                                                                     Theory B
           Figure 1. Hypothetical observed data (histograms and error bars) and hypothetical model predictions (circles) for two separate models,
           Theory A and Theory B. Errors bars reflect 95% confidence intervals. Black circles indicate when the predictions fell within the confidence
           interval of the observed proportion of correct responses, while red circles indicate deviations from the predictions and the observation.
           Theory A makes predictions of three separate problems, and Theory B makes predictions of thirty-two separate problems.
                                                                                        737

B, to hypothetical datasets as depicted in Figure 1. Theory A         assessing its complexity, and not the number of data points
is conservative: it attempts to model the proportion of               it attempts to fit.
correct responses to three separate problems, and it accounts            The insensitivity of goodness-of-fit metrics to the number
for 93% of the empirical variance. Theory B is more                   of data points being fit may appear innocuous at first blush.
ambitious: it attempts to model the responses of thirty-two           After all, a model that accounts for 90% of the variance
problems, but accounts for only 20% of the variance. Our              intuitively strikes us as an adequate account of the data
intuitions might therefore suggest that Theory A provides a           regardless of how many points it fits. However, the
close match to the data, and Theory B, for all its ambition,          established conventions for what is considered a “good”
provides a poor match to the data. The goal of the present            model, combined with the insensitivity to the number of
article is to show how both intuitions can be incorrect:              data points under consideration, may discourage cognitive
Theory B, we argue, reflects an excellent account of the              modelers to fit their model at the level of individual items
data, and Theory A’s fit is not particularly impressive. To           and problems. If a computational model makes quantitative
show how this is the case, we turn to address several                 predictions for twenty separate items, but those individual
limitations of common model fitting metrics and discuss               items can be collapsed into four-item sets, the modeler may
those limitations in the context of developing computational          be tempted to optimize the model’s fit to the set-wise
cognitive models. We introduce a novel, theoretically                 analysis (five separate data points) and to ignore or else not
motivated metric, percentile R2, which overcomes                      report the item-wise analysis (twenty separate data points).
limitations of orthodox goodness-of-fit measures. We then             Indeed, we would venture that many cognitive modelers
describe a simulation study that reveals the dynamics of the          would reject our hypothetical Theory B (see Figure 1) on
percentile R2 metric relative to different sorts of                   the basis of how low the R2 is, disregarding how many
hypothetical datasets. Finally, we conclude by addressing             points the theory attempts to fit. To do so would be a failure
constraints of the percentile R2 metric and prescribe its use         to recognize that the number of data points is negatively
in computational cognitive modeling.                                  correlated with the chance probability of obtaining a high
                                                                      R 2.
          The limitations of goodness-of-fit                             One solution to the problem is to consider a metric that is
   The use of goodness-of-fit measures like R2 and RMSE is            both sensitive to the number of data points under
controversial: some researchers argue that they are                   investigation as well as uniformly interpretable and
uninformative and should not be used (Roberts & Pashler,              meaningful. The next section introduces such a metric.
2000) while others suggest that the metrics themselves are
informative, but they are often misused (Schunn & Wallach,                                Percentile analysis
2005; Stewart, 2006). Both sides agree that a major problem              We posit a general methodology of model evaluation,
with goodness-of-fit metrics is that there exist no                   percentile analysis, which contextualizes goodness-of-fit
established measures for how to interpret them (Estes,                indices in terms of performance that can be achieved by
2002): researchers often rely on conventions, such as that an         chance alone. It is sensitive to the number of data points
R2 > .90 reflects a “good” fit, and these conventions can be          under investigation, as well as interpretable. The
misleading. Furthermore, Schunn and Wallach acknowledge               methodology is designed to take into account the strengths
several other factors that affect the quality of a fit, including     of orthodox goodness-of-fit metrics like R2, which are often
the noise in the data and its information-theoretic                   used under the assumption that a model is “good” whenever
complexity.                                                           its predictions explain a large proportion of the data. The
   One of the primary reasons that the conventions for good           term “large” is usually taken to mean upwards of 90%, but
and bad fits are misleading is because R2 does not reflect the        it is merely a convention, and many studies attribute good
number of data points (N) that a model attempts to fit. In            fits to models whose R2 values are lower (e.g., Lassiter &
other words, we might intuitively believe that a model that           Goodman, 2012; Salvucci, 2005) depending on the
accounts for 95% of the variance amongst twenty data                  particular dataset. The threshold for evaluating R2 is
points is stronger than an alternative model that accounts for        therefore ambiguous. A less ambiguous assumption of
the same mount of variance amongst only five data points,             model fitness can be achieved by comparing the model to
provided that the data points are distributed in a non-trivial        hypothetical alternatives. That is, the metric we propose,
way, e.g., in a non-linear fashion. However, there is nothing         percentile R2, assumes that an acceptable model is one that
inherent about the goodness-of-fit metrics themselves that            accounts for more variance than a set of predictions
rewards the fitting of more points (or penalizes the fitting of       produced by chance alone, and it can be scaled and
fewer points). There exist metrics for model selection that           interpreted based on the probability that the results could
combine goodness-of-fit and model complexity measures,                occur by chance.
such as minimum description length (Grünwald, 2001), the                 As an illustration, consider an observed dataset,
Akaike information criterion (AIC; Akaike, 1973;                      DOBSERVED, that consists of N data points that one wishes to
Bozdogan, 2000), and the Bayesian information criterion               model, and a set of predictions as derived from a putative
(BIC; Schwarz, 1978; Wasserman, 2000), but these metrics              model, DPREDICTED. Our goal is to describe how well the
generally take into account a model’s free parameters in              predicted data fits the observed data. We do this by
                                                                  738

exploring the space of possible models as produced by                          (e.g., .95 vs. .92). In this event, the two models have
chance by generating randomized models, i.e., sets of N data                   comparable accounts of the data, i.e., they cannot be
points at random, D1 to D100. We then calculate the                            distinguished on the virtue of model fits alone.
proportion of explained variance, R2, for D1…D100 and for
DPREDICTED relative to DOBSERVED. Finally, we examine the                3.    Model A has a higher R2 value than Model B, but a
percentile rank of our putative dataset, DPREDICTED, relative                  lower percentile R2 value than Model B. In this
to the 100 randomized models. The percentile rank is what                      controversial case, if R2 is used as the only goodness-
is reported as the percentile R2, and we accordingly define it                 of-fit metric, then Model A will be deemed to have a
as follows:                                                                    closer fit to the data than Model B, disregarding the fact
                                                                               that Model B makes more predictions and is susceptible
                                          1                                    to a lower fit by virtue of chance alone. In contrast,
                                𝑓!"#$% + 2 𝑓!"#!!"
                         !
        𝑃𝑒𝑟𝑐𝑒𝑛𝑡𝑖𝑙𝑒  𝑅    =                            ∗ 100                    taking percentile R2s into account yields one of two
                                        𝑁                                      separate conclusions: either Model A is deemed to be
                                                                               not sufficiently ambitious, i.e., not able to account for
where:                                                                         as many data points as Model B, or else Model B is
                                                                               deemed to have poor descriptive power (although it
                                                                               may be more generalizable than Model A; see
     𝑓!"#$%    is the frequency of random models whose R2
                values are less than the R2 value of the putative              Cavagnaro, Myung & Pitt, 2013). The key insight of
                                                                               this controversial scenario is that a cognitive modeler is
                model
                                                                               not justified in dismissing Model B on account of its
     𝑓!"#!!"    is the frequency of random models whose R2
                                                                               inability to account for enough variance, and must
                values are the same as the R2 value of the
                                                                               direct criticisms to other facets of the model (e.g., its
                putative model
                                                                               parsimony, breath, and ability; see Cassimatis, Bello, &
     𝑁          is the number of random models.
                                                                               Langley, 2008).
Thus, a percentile R2 of .93 means that the putative model’s
                                                                         4.    Model A and Model B have identical R2 values relative
R2 value is higher than 93% of the R2 values of the random
                                                                               to the datasets they attempt to fit. In this case, Model B
models. Clearly, the fidelity of the percentile R2 measure
                                                                               is guaranteed to have a higher percentile R2 value than
depends on the number and quality of the datasets generated
                                                                               Model A, and is therefore deemed to have a closer fit to
by the random exploration of the data space. Nevertheless,
                                                                               the data than its competitor.
the law of large numbers guarantees convergence on a stable
percentile R2 as the sample size of random models
                                                                         In the first case, the use of percentile R2 reinforces the
increases.
                                                                         results of employing the orthodox goodness-of-fit metric,
   The percentile R2 is advantageous because it allows
                                                                         R2, alone. The latter cases, however, are those that pose a
researchers to uniformly evaluate theories that differ in the
                                                                         challenge for metrics that disregard the number of data
number of predictions they make on their ability to account
                                                                         points being fit. In those cases, percentile R2 provides
for a given data set. Suppose one model (Model A) makes
                                                                         meaningful, contextually relevant interpretations of model
fewer predictions than another model (Model B).
                                                                         fits, and allows a modeler to adjudicate between putative
Employing percentile R2 as a metric in concert with R2 to
                                                                         models.
evaluate Model A and Model B has four potential outcomes:
                                                                            To explore these latter two cases, we conducted a Monte
                                                                         Carlo simulation of observed datasets (DOBSERVED) of
1.   Model A has a higher R2 value than Model B, and also
                                                                         varying numbers of items in the dataset (N), and calculated
     has a higher percentile R2 than Model B. In this
                                                                         the R2 and RMSE values at informative percentiles. We
     uncontroversial case, Model A is universally preferred
                                                                         expected that R2 would drop and RMSE would rise relative
     over Model B. Assuming that the model’s R2 is high
                                                                         to N. Our goal, however, was to examine these patterns as
     enough to account for an large proportion of the data
                                                                         well as the raw goodness-of-fit values that achieve relatively
     relative to the standards set by other researchers, it is
                                                                         low and relatively high percentile R2s.
     declared to have a fit that is both sufficiently high and
     sufficiently ambitious, i.e., it fits that data better than its
     competitor.                                                                       Monte Carlo simulation study
                                                                            We conducted a series of Monte Carlo simulations to
2.   Model A has a higher R2 value than Model B, but the                 explore the distribution of percentile R2 values as a function
     two models have identical percentile R2 values. In many             of the number of items in the dataset. Random samples were
     cases, this scenario is uncontroversial: in the event that          drawn to numerically investigate the properties of the
     percentile R2 values between two theories match, the                unknown probability distribution defining percentile R2
     model with the higher R2 is interpreted as fitting the              values.
     most data. However, it is possible that Model A has
     only a marginally higher R2 value relative to Model B
                                                                     739

Method and procedure. 127 separate Monte Carlo
simulations were conducted for values of N ranging from 2
to 128. Each of the simulations was run 10,000 times. Every
run comprised three operations:
1.    First, an observation sample, DOBSERVED, is defined by
      drawing N samples from a standard uniform probability
      distribution, U(0, 1).
2.    A model sample, Di, is then constructed by again
      drawing N samples from U(0, 1).
3.    Goodness-of-fit statistics, i.e., R2 and RMSE, are
      calculated between Di and DOBSERVED.
The simulations drew samples from the uniform probability
distribution, which ranges from 0 to 1. Such a distribution
can be used to model proportions, e.g., accuracy data. We
performed an analogous simulation study by drawing
samples from a unit normal distribution, but as its results
were largely similar to those of the analysis performed over
the uniform distribution, we omit them for brevity.
However, the second simulation made evident that the
distribution from which samples are drawn makes a
substantive difference in the analysis of RMSE and other
metrics of deviation from exact location.
   After the simulation was carried out for a given value of
N, the system calculated the values of R2 and RMSE at four
separate percentiles of salience. Three of the percentiles, the
90th, 95th, and 99th, represent those that correspond to
orthodox alpha values in inferential statistics, i.e., they
represent percentiles that could potentially be deemed a
“good” fit. Values of R2 and RMSE at the 99th percentile,
for example, indicate that of 100 random guesses, on
average only one will match or exceed it. A fourth
percentile value, the 70th percentile, provided a value of
what would unequivocally be considered a “poor” fit. In
other words, it described the values of R2 and RMSE that             Figure 2. Mean R2 (top panel) and RMSE (bottom panel) values
could be achieved (or surpassed) 30% of the time if a theory         at the 70th, 90th, 95th, and 99th percentiles of R2 values where the
structured its predictions at random.                                number of data points in the model (N) ranges from 2 to 100
                                                                     (truncated from 128).
Results of the simulation
The Monte Carlo simulation provided a numerical                     when fitting 4 data points, around 10% of random
exploration of the multivariate probability distribution that       predictions (i.e., those at the 90th percentile or higher) yield
defines percentile R2 values. Figure 2 plots average R2 and         R2s ≥ .97. That is, one in ten random guesses can capture
RMSE values as a function of the four percentiles analysed          nearly all of the variance among 4 data points. Increasing
for values of N that range from 2 to 100. Table 1 provides          the number of data points to 8 makes it progressively more
mean R2 and RMSE numerical values at the four percentiles           difficult to achieve a high R2 value by chance alone: the
of interest for values of N at powers of 2. The results of the      model at the 99th percentile yielded an R2 = .71. By the time
simulations reveal the predicted monotonic trends: R2 values        a modeler attempts to fit 100 data points, the probability of
drop as the number of data points in a model increases while        achieving a high R2 by guessing randomly is astronomically
RMSE values increase proportionally to the number of data           low, and the model at the 99th percentile yielded a paltry R2
points in a model. These results serve as manipulation              of .06.
checks and validate the fidelity of the simulation.                    The results of the simulation study provide the basis of a
   As Figure 2 shows, the simulations revealed that when            new measure of goodness-of-fit that is imminently
fitting low numbers of data points, a set of random                 interpretable. They yield a systemized account of what can
predictions could achieve high R2 values. For instance,             be considered an acceptable fit of the data. For example, a
                                                                740

                                      R2                                                          RMSE
                   Poor                         Good                          Poor                          Good
      N            70th            90th          95th        99th             70th              90th         95th          99th
      2            1.00            1.00         1.00        1.00              0.22              0.12         0.09         0.04
      4            0.49            0.85         0.93        0.99              0.38              0.26         0.21         0.13
      8            0.17            0.39         0.51        0.72              0.34              0.27         0.24         0.18
      16           0.08            0.18         0.25        0.39              0.38              0.33         0.31         0.27
      32           0.04            0.09         0.12        0.20              0.37              0.34         0.33         0.30
      64           0.02            0.04         0.06        0.10              0.41              0.39         0.38         0.36
     128           0.01            0.02         0.03        0.05              0.40              0.38         0.38         0.36
        Table 1. Mean R2 and RMSE numerical values at the four percentiles of interest for values of N at powers of 2.
model’s alleged “good fit” (R2 = .95, RMSE = .30) can be               Perhaps a more pressing concern is that modelers are
corroborated or contravened depending on whether its                susceptible to Type I error as well: a purported “good” fit
percentile R2 is .80 or .99. We conclude by discussing the          (e.g., R2 = .93) might be a result of a random allocation of
broader use of percentile analysis in computational                 predictions. In this case too, percentile analysis allows for a
cognitive modeling.                                                 motivated rejection of the model: if the percentile R2 value
                                                                    is low, then its predictions may be dubious.
                   General Discussion                                  In either event, the use of percentile R2 in computational
   A good fit of a model to data can promote refinement; a          modeling promotes the analysis of detailed model fits.
bad fit can encourage correction or dismissal. The analytical       Cognitive modelers often fit their models at the level of sets
technique we propose and implement, percentile analysis,            of data points by aggregating individual problems and items
augments traditional goodness-of-fit measures with                  in some theoretically meaningful way. They rarely fit their
information regarding how well a model performs relative            models at the level of individual items, and it is no surprise:
to what can be achieved by chance alone. A series of Monte          modelers who do so are often guaranteed to yield what
Carlo simulations provide the numerical basis for novel             orthodox goodness-of-fit metrics would consider a “poor”
metrics that can be used to place model fits in context. For        fit. Nevertheless, a good model should be able to fit the data
example, consider the fits of the two theories we introduced        at both the set-wise level and the level of individual items
at the beginning of the paper, Theory A and Theory B (see           (provided that the data collection methodology is robust).
Figure 1). Theory A appears to have a closer fit to the data        Percentile analysis is a tool that allows for the development
(R2 = .93), and despite the fact Theory B makes more                of such models.
predictions, it appears to achieve a poor fit to the data (R2 =        Percentile analysis can also aid in more common
.20). Percentile analysis provides the full context: the            computational modeling practices, such as parameter
percentile R2s of Theory A and Theory B are .81 and .99             optimization. R2s and percentile R2s are separate measures,
respectively. That is, on average, roughly one in five              i.e., the latter is a function of the number of items in a
random predictions can achieve a fit that matches or exceeds        dataset, whereas the former is not. As a result, percentile R2s
Theory A’s performance, whereas less than one in a                  may prove to be a better index of fitness with which to tune
hundred random predictions can exceed Theory B’s                    parameter settings, and optimizing for percentile R2s can
performance.                                                        potentially find parameters that avoid overfitting. That is
   The preceding example demonstrates how percentile                because, for sufficiently large values of N, the parameter
analysis can promote better computational modeling                  space that yields percentile R2s > .90 is larger than the space
practices. Because fit indices are guaranteed to drop as the        that yields R2s > .90, and the additional parameter settings
number of data points increases, we suspect that many               may allow for more generalizability. The end result could be
computational modelers are susceptible to Type II error:            a parameterization of a model that yields a relatively “low”
they disregard models of potential value because they               R2 value (< .90), a high percentile R2 value, and better
understand that an R2 of .20 is unlikely to impress                 cross-validation potential.
reviewers. Percentile analysis provides a more thorough                Are there disadvantages to employing percentile analysis
perspective for evaluating models: a model whose R2 is .20          in computational modeling? Critics may wonder if the
may be worth considering on the basis of how difficult it           introduction of yet another index of model fit is worthwhile:
would be to account for 20% of the variance by chance               they might hold that present methodologies suffice to
alone.                                                              quantify how much variability a models accounts for, and
                                                                741

that metric alone serves as a sufficient metric for evaluating     Bozdogan, H. (1990). On the information-based measure of
models. However, metrics such as R2 and RMSE suffer                  covariance complexity and its application to the
from many limitations as reviewed above, not the least of            evaluation of multivariate linear models. Communications
which is that they are difficult to interpret. As a poor model       in statistics theory and methods, 19, 221-278.
fit is regarded in such disdain as to present an impediment        Busemeyer, J. R. & Diederich, A. (2010). Cognitive
to publication, we argue that percentile analysis is an              Modeling. Sage.
indispensible tool for interpreting goodness-of-fit indices        Cassimatis, N., Bello, P. & Langley, P. (2008). Ability,
and placing them in an appropriate context. Percentile               breadth and parsimony in computational models of
analysis is not a methodology meant to supplant orthodox             higher-order cognition. Cognitive Science, 32, 1304-1322.
goodness-of-fit measures, but rather one that should be used       Cavagnaro, D. R., Myung, J. I., & Pitt, M. A. (2013).
to make them more comprehensible. The same points hold               Mathematical modeling. In Todd D. Little (ed.), The
mutatis mutandis for model selection metrics like minimum            Oxford Handbook of Quantitative Methods, Vol. 1 (pp,
description length, AIC, and Bayesian non-parametric                 438-453), Oxford University Press, New York, NY.
approaches (Karabatsos, 2006): the advantage of these              Estes, W. K. (2002). Traps in the route to models of
metrics is that they take into account model complexity, but         memory and decision. Psychonomic Bulletin &Review, 9,
their disadvantage is that they are hard to interpret and to         3-25.
compare across datasets.                                           Grünwald, P. (2001). Model selection based on minimum
   Dissenters may also hold that percentile analysis is a            description length. Journal of Mathematical Psychology,
vindication for poor modeling: it allows for the publication         44, 133-152.
of models that account for relatively little variance in the       Kachergis, G. & Yu, C. (2013). More naturalistic cross-
data (e.g., R2 = .20). Far from the dissenting position,             situational word learning. In Proceedings of the 35th
however, we believe percentile analysis promotes better              Annual Conference of the Cognitive Science Society.
computational modeling practices, because it contextualizes        Kachergis, G., Yu, C., & Shiffrin, R. M. (2012). Cross-
previous methodologies and allows modelers to reasonably             situational word learning is better modeled by
examine detailed model fits across individual items.                 associations than hypotheses. IEEE Conference on
Therefore, a modeler’s focus needn’t rest on maximizing R2           Development and Learning / EpiRob 2012.
values alone; they can also try to build models that are           Karabatsos, G. (2006). Bayesian nonparametric model
flexible enough to make detailed process predictions.                selection and model testing. Journal of Mathematical
   In summary, we developed percentile analysis as a                 Psychology, 50, 123-148.
methodology for evaluating and interpreting a model’s fit to       Lassiter, D. & Goodman, N. D. (2012). How many kinds of
observed data. It is sensitive to the number of data points in       reasoning? Inference, probability, and natural language
the data, and it operates by comparing a putative model              semantics In Proceedings of the Thirty-Fourth Annual
against hypothetical ones generated by pure noise. We argue          Conference of the Cognitive Science Society.
that percentile analysis should be an essential component of       McClelland, J. (2009). The place of modeling in cognitive
a cognitive modeler’s toolkit.                                       science. Topics in Cognitive Science, 1, 11-38.
                                                                   Roberts, S., & Pashler, H. (2000). How persuasive is a good
                   Acknowledgements                                  fit? A comment on theory testing. Psychological Review,
This research was funded by a National Research Council              107, 358-367.
Research Associateship awarded to SK and a grant from              Schunn, C., & Wallach, D. (2005). Evaluating goodness-of-
ONR awarded to GT. We are also grateful to Bill Adams,               fit in comparison of models to data. In W. Tack (Ed.),
Magda Bugajska, Dan Gartenberg, Tony Harrison, Laura                 Psychologie der Kognition: Reden and Vorträge
Hiatt, Ed Lawson, Frank Tamborello, and Alan Schultz for             anlässlich der Emeritierung von Werner Tack (pp. 115-
their helpful comments.                                              154). University of Saarland Press, Saarbrücken,
                                                                     Germany.
                                                                   Schwarz, G. (1978). Estimating the dimension of a model.
                        References                                   The Annals of Statistics, 6, 461-464.
Akaike, H., & (Eds.). (1973). Information theory and an            Stewart, T. C. (2006). Tools and techniques for quantitaive
   extension of the maximum likelihood principle. In B. N.           and predictive cognitive science. In Proceedings of the
   Petrov & F. Csaki (Eds.), 2nd International Symposium             28th Annual Conference of the Cognitive Science
   on Information Theory (pp. 267-281). Kiado, Budapest:             Society (pp. 816-821). Vancouver, British Columbia,
   Akad.                                                             Canada.
Anderson, J. R., Bothell, D., Lebiere, C. & Matessa, M.            Wasserman, L. (2000). Bayesian model selection and model
   (1998). An integrated theory of list memory. Journal of           averaging. Journal of Mathematical Psychology, 44, 92-
   Memory and Language, 38, 341-380.                                 107.
                                                               742

