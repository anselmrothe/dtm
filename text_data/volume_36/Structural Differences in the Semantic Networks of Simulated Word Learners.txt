UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Structural Differences in the Semantic Networks of Simulated Word Learners
Permalink
https://escholarship.org/uc/item/28q915dx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Nematzadeh, Aida
Fazly, Afsaneh
Stevenson, Suzanne
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

     Structural Differences in the Semantic Networks of Simulated Word Learners
                                    Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
                                                        Department of Computer Science
                                                               University of Toronto
                                                    {aida,afsaneh,suzanne}@cs.toronto.edu
                               Abstract                                     show that the network of vocabulary of late talkers exhib-
   A learner’s semantic network represents the learner’s knowl-
                                                                            ited a small-world structure to a lesser degree than that
   edge of words/concepts and the relations among them. The                 of the normally-developing children. However, while this
   structure of this network is significant as it might reveal aspects      work suggests some preliminary answers to the first ques-
   of the developmental process that leads to the network. In this          tion above, it cannot shed light on the relation between the
   work, we use computational modeling to examine the struc-
   ture of semantic networks of different simulated word learn-             process of word learning and the small-world and scale-free
   ers. We find that the learned semantic knowledge of a learner            properties. Specifically, the networks considered by Beckage
   that simulates a normally-developing child reflects the struc-           et al. (2011) only include productive vocabulary, not the
   tural properties found in adult semantic networks of words. In
   contrast, the network of a late-talking learner — one that sim-          many words a child will have partial knowledge of, and the
   ulates a child with a marked delay in vocabulary acquisition             connections among the words are determined by using co-
   — does not exhibit these properties. We discuss the implica-             occurrence statistics from a corpus, not the children’s own
   tions of this result for understanding the process of vocabulary
   acquisition and delay.                                                   knowledge or use of the words. In order to shed light on
                                                                            how the small-world and scale-free properties arise from the
                            Introduction                                    developmental process of word learning, we need to consider
Semantic knowledge includes the knowledge of word-to-                       the structure of semantic networks formed from the (partially)
concept mappings, as well as relations among the words                      learned meanings of the words in the child’s environment.
and/or concepts. Much research shows the importance of dif-                    In this work, we take advantage of a computational model
ferent aspects of semantic knowledge in vocabulary acquisi-                 to simulate normally-developing (ND) and late-talking (LT)
tion (e.g., Jones, Smith, & Landau, 1991; Colunga & Smith,                  learners, enabling us to examine the properties of seman-
2005; Sheng & McGregor, 2010; Colunga & Sims, 2011). A                      tic networks that include all the vocabulary a learner has
long-standing question is how the overall structural properties             been exposed to (i.e., even those partially learned), and that
of semantic knowledge impact how words are learned and                      has connections based on the actual learned knowledge of
processed in semantic memory (Collins & Quillian, 1969;                     those words. The model is a probabilistic cross-situational
Collins & Loftus, 1975; Steyvers & Tenenbaum, 2005).                        learner which incrementally acquires word-to-meaning map-
   Semantic knowledge is often represented as a semantic                    pings through exposure to naturalistic input. When parame-
network in which the nodes correspond to words or con-                      terized to reflect normal and deficit scenarios, this computa-
cepts, and the edges specify semantic relationships among                   tional model has been shown to replicate several patterns of
them (e.g., Collins & Loftus, 1975; Steyvers & Tenenbaum,                   results observed in normally-developing and late-talking chil-
2005). Steyvers and Tenenbaum (2005) argue that semantic                    dren (Nematzadeh, Fazly, & Stevenson, 2011, 2012), making
networks created from adult-level knowledge of words ex-                    its learned knowledge suitable as a simulation of the knowl-
hibit a small-world and scale-free structure: an overall sparse             edge of such children. We create semantic networks based on
network with highly-connected local sub-networks, where                     the learned knowledge of this model in the ND and LT model-
these sub-networks are connected through high-degree hubs                   ing scenarios, and investigate their structural differences with
(nodes with many neighbours). Through mathematical mod-                     respect to having a small-world and scale-free structure.
eling, they argue that these properties arise from the develop-                We find that the semantic network of the ND learner – cre-
mental process of semantic network creation, in which word                  ated from all the words in the input to the model, and the
meanings are differentiated over time.                                      learned meanings of those words – exhibits a small-world
   The work of Steyvers and Tenenbaum (2005) raises very                    and to some extent a scale-free structure, whereas the cor-
interesting follow-on questions: To what degree does chil-                  responding LT network does not. Moreover, we find that a
dren’s developing semantic knowledge of words exhibit a                     “gold-standard” network – which uses ground-truth mean-
small-world and scale-free structure? How do these proper-                  ings rather than the learned meanings from the model – less
ties arise from the process of vocabulary acquisition in chil-              clearly exhibits these two important properties. This suggests
dren? The work of Beckage, Smith, and Hills (2011) is sug-                  that properties of the learned knowledge may actually aid the
gestive regarding these issues. They compare semantic net-                  learner in making appropriate connections among words. We
works formed from the productive vocabulary of normally-                    conclude that considering the learned knowledge of vocabu-
developing children and from that of late talkers — chil-                   lary is important in understanding how the structure of chil-
dren who show a marked delay in their vocabulary acquisi-                   dren’s semantic networks is related to, and might arise from,
tion (Ellis-Weismer & Evans, 2002). Beckage et al. (2011)                   word learning processes.
                                                                       1072

         apple: { FOOD:1, SOLID:.72, · · · , PLANT- PART:.22,               all possible meaning features, but gradually the features that
                 PHYSICAL - ENTITY :.17, WHOLE :.06, · · · }
                                                                            consistently occur in the presence of w in the input will rise
             Figure 1: true meaning of the word apple.                      in probability, while less relevant features will decline. Note
                                                                            that one consequence of the cross-situational approach is that
          The Simulated Semantic Networks
                                                                            features of other words that frequently occur in the context of
In this section, we first explain how our computational model               w may also have some non-negligible probability mass in the
learns and represents word meanings, and how our normally-                  meaning representation of w.
developing (ND) and late-talking (LT) learners differ in their                 The model incorporates an attentional mechanism that
word learning mechanism. Next, we describe our approach                     gradually improves over time, enabling it to focus (more
to the construction of the semantic networks from the learned               or less) on the relevant features to a word. We simulate
knowledge of the model in these two scenarios.                              normally-developing (ND) and late-talking (LT) learners by
                                                                            parameterizing the rate of development of this mechanism,
Simulating Different Learners
                                                                            such that ND has a faster rate. Because the attentional mech-
The model of Nematzadeh et al. (2011, 2012) learns from                     anism impacts the learning algorithm of the model, the ND
a sequence of input utterance–scene pairs, corresponding to                 and LT learners differ in the quality of their learned mean-
what a child is exposed to in her natural learning environ-                 ings; specifically, the LT meanings tend to have a more uni-
ment. Each input pair consists of an utterance (what a child                form distribution over semantic features at a given point in
hears), represented as a set of words, and its corresponding                development.
scene (what a child perceives upon hearing that utterance),
represented as a set of semantic features; e.g.:                            Constructing a Learner’s Semantic Network
      Utterance: {she, drinks, milk }                                       We train each learner (ND and LT) on an identical sequence
      Scene: {ANIMATE, PERSON, FEMALE, CONSUME, DRINK,                      of utterance–scene pairs, and then use their learned lexicons
              SUBSTANCE, FOOD , DAIRY- PRODUCT , . . . }                    to build a semantic network for each. Unlike Beckage et al.
The utterances are taken from the child-directed speech por-                (2011), we do not want to restrict the network to productive
tion of the Manchester corpus (Theakston et al., 2001, from                 vocabulary, which eliminates much semantic knowledge of
CHILDES MacWhinney, 2000). The corresponding scene                          the learner (e.g., Benedict, 1979; Woodward & Markman,
representation for each utterance is generated using a gold-                1998). We thus assume all the words that the model has been
standard lexicon. In this lexicon the gold-standard meaning                 exposed to during training are part of the learner’s semantic
(true meaning) of each word is represented as a set of fea-                 network. This reflects our assumption that an important as-
tures, taken from a pool of F semantic “primitives” that com-               pect of a learner’s semantic knowledge is that it (perhaps im-
prise the F-dimensional space of semantic meanings. Each                    perfectly) captures connections among even words that can-
feature for a word is associated with a score that reflects the             not yet be fully comprehended or produced.
strength of association of the word and feature, and the speci-                To establish the connections among nodes in the network,
ficity of each feature to that word (see Figure 1 for an exam-              we examine the semantic similarity of the meanings of the
ple).1 Given this gold-standard lexicon, a scene is probabilis-             corresponding words. Specifically, we measure semantic
tically generated by sampling from the full set of features in              similarity of two words by turning their meanings into vec-
the true meanings of the words in the utterance, according to               tors, and calculating the cosine of the angle between the two
the score of each feature.2                                                 vectors. We connect two nodes if the similarity of their cor-
   As a probabilistic cross-situational learner, the model uses             responding words is higher than a pre-defined threshold.
observations of co-occurrences of words and features in the                    This process yields two networks, Net-ND and Net-LT,
utterance and scene inputs to update its hypotheses of the                  each of which contains nodes for all the words in the input,
meaning of each word over time. Specifically, for each word                 with the edges determined by the semantic similarity of the
w in the input, the model maintains a probability distribu-                 word meanings represented within the ND and LT learners,
tion over all F possible semantic features; this probability                respectively.
distribution represents the model’s learned meaning of w at                    For comparison, we also build a gold-standard semantic
any given point in the input sequence. Initially, the probabil-             network, Net-GS, that contains the same words as Net-ND
ity distribution for w will start out uniformly distributed over            and Net-LT (i.e., all the words in the input), but relies on the
                                                                            true meanings of words (from the gold-standard lexicon) to
    1 For details on the semantic features and data generation see          establish the connections. Note that the structure of this net-
Nematzadeh et al. (2012). We also note that a naturalistic word             work does not depend on the learners’ knowledge of word
learning scene contains rich information beyond word meanings
such as social cues and context. We cannot represent this informa-          meanings, but only on the similarity of the true meanings.
tion automatically, and given the scale of our data, it is not feasible        In order to further explore the importance of the knowledge
to manually annotate such information. Our scene representation is          of (partially) learned meanings to the structure of the resulting
thus limited in that it only includes the semantics of words in each
utterance.                                                                  networks, we also consider a variation on Net-ND and Net-
    2 Note that the gold-standard lexicon is only used for generating       LT. Like Beckage et al. (2011), we can consider only a subset
the input data, and is not used in the learning algorithm of the model.     of the best-learned words of the learners, and see whether the
                                                                        1073

 vocabulary itself – as opposed to what the learner has learned       if it forms a complete graph —i.e., there is an edge between
 about that vocabulary – exhibits the small-world and scale-          all node pairs. Thus, the maximum number of edges in the
 free properties. Recall that Beckage et al. (2011) create se-        neighborhood of n is kn (kn − 1)/2, where kn is the number
 mantic networks connected on the basis of corpus-based co-           of neighbors. A standard metric for measuring the connect-
 occurrence statistics that are the same for both groups of chil-     edness of neighbors of a node n is called the local clustering
 dren – i.e., it is the make-up of the vocabulary, rather than        coefficient (C) (Watts & Strogatz, 1998), which calculates the
 the learner’s knowledge of that vocabulary, that differs across      ratio of edges in the neighborhood of n (En ) to the maximum
 the two types of networks. In our approach, this corresponds         number of edges possible for that neighborhood:
 to using the true meanings from the gold-standard lexicon to
                                                                                                     En
 connect the words in the network.                                                        C=                                      (1)
    Hence, we form additional networks, Net-NDacq and Net-                                     kn (kn − 1)/2
 LTacq as follows. We take “productive” vocabulary in our             The local clustering coefficient C ranges between 0 and 1. To
 model to be a subset of words which are learned better than          estimate the connectedness of all neighborhoods in a network,
 a predefined threshold (by comparing the learned meaning to          we take the average of C over all nodes, i.e., Cavg .
 the true meaning in the gold-standard lexicon). We then build           Small-world structure. A graph exhibits a small-world
 semantic networks that contain these acquired words of our           structure if dmedian is relatively small and Cavg is relatively
 ND and LT learners, connected by drawing on the similarity           high. To assess this for a graph g, these values are typically
 of the true meanings (that are the same for both learners). We       compared to those of a random graph with the same number
 can then use these networks to further explore the importance        of nodes and edges as g (Watts & Strogatz, 1998; Humphries
 of the partially learned knowledge of words in our original          & Gurney, 2008). The random graph is generated by ran-
 networks in contributing to small-world and scale-free net-          domly rearranging the edges of the network under consider-
 works.                                                               ation (Erdos & Rényi, 1960). Because any pair of nodes is
    To summarize, we consider the following networks:                 equally likely to be connected as any other, the median of
1. Net-GS: The nodes of the network are all the words in the          distances between nodes is expected to be low for a random
    input, and the edges are based on the similarity of the true      graph. In a small-world network, this value dmedian is ex-
    meanings of the words.                                            pected to be as small as that of a random graph: even though
                                                                      the random graph has edges more uniformly distributed, the
2. Net-ND and Net-LT: The nodes are all the words in the in-
                                                                      small-world network has many locally-connected compo-
    put, and the edges are based on the similarity of the learned
                                                                      nents which are connected via hubs. On the other hand, Cavg
    meanings of the words in each of the modeling scenarios.
                                                                      is expected to be much higher in a small-world network com-
3. Net-NDacq and Net-LTacq : The nodes are the acquired               pared to its corresponding random graph, because the edges
    words (those best learned) in each scenario, and the edges        of a random graph typically do not fall into clusters forming
    are based on the similarity of the true meanings of those         highly connected neighborhoods.
    words.                                                               Given these two properties, the “small-worldness” of a
                                                                      graph g is measured as follows (Humphries & Gurney, 2008):
 Evaluating the Networks’ Structural Properties
 A network that exhibits a small-world structure has certain                                       Cavg (g)
 connectivity properties – short paths and highly-connected                                    Cavg (random)
 neighborhoods – that are captured by various graph metrics                            σg =                                       (2)
                                                                                                  dmedian (g)
 (Watts & Strogatz, 1998). Below we explain these proper-
                                                                                              dmedian (random)
 ties, and how they are measured for a graph with N nodes and
 E edges. Then we explain the requirement for a network to            where random is the random graph corresponding to g.
 yield a scale-free structure.                                        In a small-world network, it is expected that Cavg (g) 
    Short paths between nodes. Most of the nodes of a                 Cavg (random) and dmedian (g) ≥ dmedian (random), and thus
 small-world network are reachable from other nodes via rela-         σg > 1.
 tively short paths. For a connected network (i.e., all the node         Note that Steyvers and Tenenbaum (2005) made the em-
 pairs are reachable from each other), this can be measured as        pirical observation that small-world networks of semantic
 the average distance between all node pairs (Watts & Stro-           knowledge had a single connected component that contained
 gatz, 1998). Since our networks are not connected, we in-            the majority of nodes in the network. Thus, in addition to
 stead measure this property using the median of the distances        σg , we also measure the relative size of a network’s largest
 (dmedian ) between all node pairs (e.g., Robins et al., 2005),       connected component having size Nlcc :
 which is well-defined even when some node pairs have a dis-
                                                                                                       Nlcc
 tance of ∞.                                                                                sizelcc =                             (3)
    Highly-connected neighborhoods. The neighborhood of                                                 N
 a node n in a graph consists of n and all of the nodes that             Scale-free structure. A scale-free network has a relatively
 are connected to it. A neighborhood is maximally connected           small number of high-degree nodes that have a large number
                                                                  1074

of connections to other nodes, while most of its nodes have a
small degree, as they are only connected to a few nodes. Thus,
if a network has a scale-free structure, its degree distribution
(i.e., the probability distribution of degrees over the whole
network) will follow a power-law distribution (which is said
to be “scale-free”). We evaluate this property of a network by
plotting its degree distribution in the logarithmic scale, which
(if a power-law distribution) should appear as a straight line.
                           Evaluation
Set-up
We train our learners on 10, 000 utterance–scene pairs taken
from the input data of Nematzadeh et al. (2012). Recall that                                    (a) Net-GS
our ND and LT learners differ in the rate of attentional de-
velopment that is a parameter of the model (c). Following
Nematzadeh et al. (2011), we use c = 1 for ND and c = 0.5
for LT. We use only nouns in our semantic networks: since
we draw on different sources for the semantic features of dif-
ferent parts of speech (POS), we cannot reliably measure the
similarity of two words from different POS’s. To determine
the subset of “acquired words” for Net-NDacq and Net-LTacq ,
we follow Fazly et al. (2010) and use a threshold of 0.7 for
similarity between the learned and true meaning of a word.
Finally, when building a network, we connect two word nodes
with an edge if the similarity of their corresponding meanings
is higher than 0.6.                                                                             (b) Net-ND
Results and Discussion                                               Figure 2: (a) The gold-standard network, and (b) the network
                                                                     of ND with all words connected by learned meanings.
Table 1 contains the graph measures for all the semantic net-
works we consider here. The table displays the number of             that these hubs are one of the main characteristics of a small-
nodes (N) and edges (E) in each network, as well as the mea-         world structure. The different structures of Net-GS and Net-
sures that capture characteristics of a small-world structure.       ND are evident from their visualizations in Figure 2. We can
We first discuss these measures, and the indicator of scale-         see that in Net-GS there are a number of isolated components
free structure, for our primary networks, Net-GS, Net-ND,            that are not connected to the rest of the network.
and Net-LT, and then consider the networks formed without               We also examine Net-GS and Net-ND for having a scale-
using the learned knowledge of the words, Net-NDacq and              free structure by looking at their degree distributions in the
Net-LTacq .                                                          logarithmic scale (see Figure 3). According to these plots,
Small-world and scale-free structure in the learners’ net-           Net-ND to some degree exhibits a scale-free structure (with
works. We first compare the structure of Net-GS and Net-             the plot roughly following a straight line), but Net-GS does
ND (rows 1 and 2 in the table), and then turn to Net-LT (row         not.
3).
   According to the values of σg , we can see that both Net-
GS and Net-ND yield a small-world structure, although the
structure is more clearly observed in Net-ND: σg (ND) = 5.5
versus σg (GS) = 3.1. This is especially interesting since both
networks have the same nodes (all the words), but Net-ND
uses learned meanings to connect the nodes, whereas Net-GS
uses the true meanings (from the gold-standard lexicon).
   A closer look reveals that Net-ND has a structure in which
many more nodes are connected to each other (sizelcc (ND) =                      (a) Net-GS                        (b) Net-ND
.90 vs. sizelcc (GS) = .72) by using substantially fewer edges
(E(ND) = 12, 704 vs. E(GS) = 26, 663). Net-ND achieves               Figure 3: The degree distributions of Net-GS and Net-ND in
this by a better utilization of hubs: each hub node connects to      the logarithmic scale.
many nodes, and in turn to other hubs, ensuring a high-degree           Now, looking at the characteristics of Net-LT (row 3 of the
of connectivity with a relatively small number of edges. Note        table), we can see that it does not clearly show a small-world
                                                                 1075

                        Networks                      N        E          sizelcc       Cavg        dmedian     σg
                    1   Net-GS (gold-standard)       776     26, 633    0.72 (1)    0.95 (0.09)       7 (2)    3.1
                    2   Net-ND                       776     12, 704    0.90 (1)    0.70 (0.04)       6 (2)    5.5
                    3   Net-LT                       776   239, 736     1.00 (1)    0.97 (0.81)       1 (1)    1.2
                    4   Net-NDacq                    512     12, 470    0.67 (1)    0.96 (0.10)       ∞ (2)      0
                    5   Net-LTacq                     84         423    0.23 (1)    0.81 (0.11)       ∞ (2)      0
Table 1: The calculated graph metrics on each of the semantic networks. The number in the brackets is the measure for the
corresponding random network. The value of N and E are the same for each network and its random graph.
structure. The value of σg (LT) is very close to 1 because the      ings achieved by that learner. The relative deficit in attention
value of Cavg for Net-LT is very similar to its corresponding       in our LT learner entails that the learner cannot focus on the
random graph (cf. Eqn. 2). This is mostly due to the exis-          most relevant meaning features, yielding a network that fails
tence of a very large number edges in this network, which           to distinguish relevant clusters of meaning around “hubs”.
reflects the uninformativeness of the learned meanings of LT           Clearly, this is data from a computational model, and not
for identifying meaningful similarities among words. Specif-        the actual semantic memory representation of children. How-
ically, the meanings that the LT learns for semantically un-        ever, it does lead to interesting predictions about the relation-
related words are not sufficiently distinct, and hence almost       ship between the small-world and scale-free properties and
all words are taken to be similar to one another. Net-LT con-       the process of vocabulary acquisition: specifically, that the
sequently also does not show a scale-free structure, since the      contextualization of otherwise (at least moderately) distin-
nodes across the network all have a similar number of con-          guishable meanings is a crucial outcome of successful vocab-
nections (resulting in a bell-shaped rather than a power-law        ulary acquisition, and one that leads to the formation of se-
degree distribution).                                               mantic networks with the overall structural properties found
What underlies the small-world and scale-free findings?             in representations of adult semantic knowledge.
To summarize, we find that Net-ND shows a small-world               A further look at the role of learned meanings. We sug-
and (to some degree) a scale-free structure, while Net-LT           gest above that the small-world and scale-free properties of
does not. This is consistent with the findings of Beckage           Net-ND arise due to qualitative differences in its learned
et al. (2011) who observed that a network of vocabulary             knowledge of words, compared to both Net-LT or Net-GS.
of normally-developing children had more of a small-world           However, Beckage et al. (2011) found differences in the de-
structure than a network of late-talkers’ vocabulary. However,      gree of small-world structure in their ND and LT networks
by using the simulated knowledge of ND and LT learners, and         that differed only in the vocabulary used as nodes in the net-
comparing it to a representation of the “true meanings” in          work – that is, even though both networks used the same ex-
Net-GS, we can go beyond their work and address the ques-           ternal knowledge to create edges among those nodes. Hence
tion we raised in the introduction: How do these properties         we also examine two additional networks, Net-NDacq and
arise from the process of vocabulary acquisition in children?       Net-LTacq , formed from the best-acquired words of the learn-
   The fact that Net-ND exhibits a small-world and scale-           ers and the similarity of the true meanings of those words.
free structure more clearly than Net-GS suggests that the           This can help reveal whether it is the make-up of the vocab-
probabilistically-learned meanings of our model capture im-         ulary or the specific learned knowledge of words that plays a
portant information beyond the true meanings. Recall that our       role in our results.
model learns the meaning of each word w by gradually associ-           The graph measures for Net-NDacq and Net-LTacq are
ating w with semantic features that consistently co-occur with      shown in rows 4 and 5 of Table 1. We see that neither of these
it across its usages. We noted above that this probabilistic        networks exhibits a small-world structure (σg = 0), mainly
cross-situational approach can lead to a “contextualization”        because they have many isolated sub-networks, resulting in
of meaning representation for w: i.e., if another word w0 con-      dmedian having a value of ∞ (i.e., most node pairs are not con-
sistently co-occurs with w (e.g., due to semantic relatedness),     nected to each other).
then the learned meaning of w can include semantic features            We conclude that in our simulations of child knowledge, it
of w0 . This contextualized meaning representation essentially      is the actual meaning representation that is important to yield-
makes the learned meanings of the two co-occurring words            ing a small-world and scale-free structure, not simply the par-
more similar than their true meanings. This “blurring” of           ticular words that are learned. Our finding differs from that of
meanings entails that, even though Net-ND has fewer edges           Beckage et al. (2011), who found small-world structure even
than Net-GS, those edges form connections across hubs that          when using simple corpus statistics to similarly connect the
achieve a greater small-world structure.                            vocabulary of each type of learner. It could be that our “best-
   On the other hand, the lack of a small-world structure in        learned” words do not correspond to the productive vocabu-
Net-LT clearly arises from the lack of differentiation of mean-     lary of children; we also note that forming network connec-
                                                                1076

tions based on similarity of our true meanings is much stricter           hension and production. Journal of child language, 6(2),
than compared to the simple co-occurrence statistics used by              183–200.
Beckage et al. (2011).                                                  Collins, A. M., & Loftus, E. F. (1975). A spreading-
   More importantly, we think our simulated networks can                  activation theory of semantic processing. Psychological
turn attention around these issues to the actual (developing)             review, 82(6), 407.
knowledge that different learners are bringing to the task of           Collins, A. M., & Quillian, M. R. (1969). Retrieval time from
word learning and semantic network creation. Specifically,                semantic memory. Journal of verbal learning and verbal
Beckage et al. (2011) conclude that the semantic networks of              behavior, 8(2), 240–247.
late talkers might be less connected because they use a word-           Colunga, E., & Sims, C. (2011). Early talkers and late talkers
learning strategy that favors semantically-dissimilar words. It           know nouns that license different word learning biases. In
is not clear, however, how such children could follow a strat-            Proc. of CogSci’11.
egy of semantic dissimilarity when they do not have an ade-             Colunga, E., & Smith, L. B. (2005). From the lexicon to
quate representation of semantic similarity. To the extent that           expectations about kinds: A role for associative learning.
the semantic knowledge of children is similar to the simulated            Psychological Review, 112(2), 347–382.
knowledge in our model – in being partial, probabilistic, and           Ellis-Weismer, S., & Evans, J. L. (2002). The role of process-
contextualized – our experiments point to a different expla-              ing limitations in early identification of specific language
nation of late talkers’ disconnected vocabulary: Not that it is           impairment. Topics in Language Disorders, 22(3), 15–29.
purposefully disconnected, but that due to the lack of mean-            Erdos, P., & Rényi, A. (1960). On the evolution of random
ingful semantic differentiation, it is accidentally so – i.e., late       graphs. Publ. Math. Inst. Hungar. Acad. Sci, 5, 17–61.
talkers have simply failed to exploit the contextualized mean-          Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-
ings that help normally-developing children formulate helpful             tic computational model of cross-situational word learning.
connections among words.                                                  Cognitive Science, 34(6), 1017–1063.
                                                                        Humphries, M. D., & Gurney, K. (2008). Network small-
           Summary and Future Direction                                   world-ness: a quantitative method for determining canoni-
                                                                          cal network equivalence. PLoS One, 3(4), e0002051.
We use a computational model to simulate normally-
                                                                        Jones, S., Smith, L. B., & Landau, B. (1991). Object proper-
developing (ND) and late-talking (LT) learners, and exam-
                                                                          ties and knowledge in early lexical learning. Child Devel-
ine the structure of semantic networks of these learners. We
                                                                          opment, 62(3), 499–516.
compare the networks of ND and LT learners with that of a
                                                                        MacWhinney, B. (2000). The CHILDES project: Tools for
gold-standard (GS) network that has access to ground-truth
                                                                          analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum.
meanings. Our goal is to investigate whether the simulated
                                                                        Nematzadeh, A., Fazly, A., & Stevenson, S. (2011). A com-
learned meanings of words reflected in ND and LT networks
                                                                          putational study of late talknig in word-meaning acquisi-
yield a small-world and scale-free structure, as observed in
                                                                          tion. In Proc. of CogSci’11.
adult semantic networks (Steyvers & Tenenbaum, 2005).
                                                                        Nematzadeh, A., Fazly, A., & Stevenson, S. (2012). Interac-
   Our results show that while Net-GS and Net-ND exhibit a
                                                                          tion of word learning and semantic category formation in
small-world and (to some extent) a scale-free structure, the
                                                                          late talking. In Proc. of CogSci’12.
less differentiated meanings of Net-LT does not. We also ob-
                                                                        Robins, G., Pattison, P., & Woolcock, J. (2005). Small and
serve that Net-ND shows a stronger small-world and scale-
                                                                          other worlds: Global network structures from local pro-
free structure compared to Net-GS. We attribute this inter-
                                                                          cesses1. American Journal of Sociology, 110(4), 894–936.
esting observation to the way our model learns word mean-
                                                                        Sheng, L., & McGregor, K. K. (2010). Lexical–semantic
ings: Unlike the true meanings, the learned meanings capture
                                                                          organization in children with specific language impairment.
contextual semantic knowledge, which brings in an additional
                                                                          J. of Speech, Lang., & Hearing Research, 53, 146–159.
and helpful source of information for identifying semantic re-
                                                                        Steyvers, M., & Tenenbaum, J. B. (2005). The large-scale
latedness among words.
                                                                          structure of semantic networks: Statistical analyses and a
   An interesting future direction is to model the actual devel-
                                                                          model of semantic growth. Cognitive science, 29(1), 41–
opment of a semantic network over the course of word learn-
                                                                          78.
ing. This would allow us to examine the underlying mecha-
                                                                        Theakston, A. L., Lieven, E. V., Pine, J. M., & Rowland, C. F.
nisms that might be involved in the growth of a semantic net-
                                                                          (2001). The role of performance limitations in the acquisi-
work, and how the developing knowledge of word meanings
                                                                          tion of verb–argument structure: An alternative account. J.
interacts with the formation of network connections.
                                                                          of Child Language, 28, 127–152.
                                                                        Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics
                         References
                                                                          of small-worldnetworks. nature, 393(6684), 440–442.
Beckage, N., Smith, L., & Hills, T. (2011). Small worlds and            Woodward, A. L., & Markman, E. M. (1998). Early word
   semantic network growth in typical and late talkers. PloS              learning.
   one, 6(5), e19348.
Benedict, H. (1979). Early lexical development: Compre-
                                                                    1077

