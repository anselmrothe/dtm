UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Self-Organized Artificial Grammar Learning in Spiking Neural Networks
Permalink
https://escholarship.org/uc/item/75j1b2t0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Duarte, Renato
Series, Peggy
Morrison, Abigail
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

         Self-Organized Artificial Grammar Learning in Spiking Neural Networks
                                       Renato Duarte (renato.duarte@bcf.uni-freiburg.de)
                                          Bernstein Center Freiburg, Albert-Ludwigs University
                      Freiburg im Breisgau, 79104 Germany & Institute for Adaptive and Neural Computation,
                             School of Informatics, University of Edinburgh, EH8 9AB, United Kingdom
                                                Peggy Seriès (pseries@inf.ed.ac.uk)
                                             Institute for Adaptive and Neural Computation,
                             School of Informatics, University of Edinburgh, EH8 9AB, United Kingdom
                                           Abigail Morrison (morrison@fz-juelich.de)
                                          Bernstein Center Freiburg, Albert-Ludwigs University
                     Freiburg im Breisgau, 79104 Germany & Institute of Neuroscience and Medicine (INM-6),
                       Computational and Systems Neuroscience, Jülich Research Center, 52425, Germany &
              Institute of Cognitive Neuroscience, Faculty of Psychology, Ruhr-University Bochum, 44801, Germany
                              Abstract                                  plex computational processes to the underlying neuronal pro-
                                                                        cesses and assessing the properties of the neuronal system re-
   The Artificial Grammar Learning (AGL) paradigm provides a
   means to study the nature of syntactic processing and implicit       sponsible for their implementation is not straightforward, but
   sequence learning. With mere exposure and without perfor-            it is likely to yield important insights into the nature of neural
   mance feedback, human beings implicitly acquire knowledge            computation.
   about the structural regularities implemented by complex rule
   systems. We investigate to which extent a generic cortical mi-
   crocircuit model can support formally explicit symbolic com-         Artificial Grammar Learning
   putations, instantiated by the same grammars used in the hu-
   man AGL literature and how a functional network emerges, in          The problem of sequence learning has a long tradition in cog-
   a self-organized manner, from exposure to this type of data.         nitive science and psycholinguistic research. Considerable
   We use a concrete implementation of an input-driven recurrent        effort has been devoted to the question of whether and under
   network composed of noisy, spiking neurons, built according
   to the reservoir computing framework and dynamically shaped          which conditions, the acquisition of complex, rule-governed
   by a variety of synaptic and intrinsic plasticity mechanisms         knowledge can be performed in an incidental or implicit man-
   operating concomitantly. We show that, when shaped by plas-          ner, i.e., “without any requirements of awareness of either the
   ticity, these models are capable of acquiring the structure of
   a simple grammar. When asked to judge string legality (in a          process or the product of acquisition”(A. S. Reber, Walken-
   manner similar to human subjects), the networks perform at a         feld, & Hernstadt, 1991). These studies exploit the fact that
   qualitatively comparable level.                                      our ability to deal with complex sequential structure is most
   Keywords: Sequence Learning; Self-Organization; Plasticity;          evident in language acquisition and processing, transforming
   Artificial Grammar Learning;                                         the problem of sequence learning into the largely equivalent
                                                                        problem of grammar learning, which can be addressed within
                          Introduction                                  the domain of language syntax. In fact, growing evidence
Sequential organization is a ubiquitous facet of adaptive cog-          suggests that language acquisition and processing is medi-
nition and behavior. Many of our more fundamental abilities             ated by implicit sequence learning and structured sequence
reflect some form of adaptation to the structural regularities          processing (K. M. Petersson & Hagoort, 2010), thus involv-
of sensory events, as they unfold over time, and the extraction         ing common mechanisms.
and use of such regularities.                                               A typical AGL experiment consists of a learning or acqui-
   In order to adequately navigate complex, dynamic envi-               sition phase and a test phase. During acquisition, participants
ronments, an agent ought to be able to represent and process            are exposed to a set of symbol sequences generated from a
sequences of information, use this information in a predic-             formal grammar (a complex rule system, whose rules can
tive context, to make inferences about what will happen next,           be described by the allowed transitions of a directed graph,
when it will happen and how to react to it, and assemble ele-           e.g. Figure 1), often in the form of a short-term memory task.
mentary responses into novel action sequences.                          During the subsequent test phase, subjects are informed about
   It is thus of central importance to elucidate how knowl-             the existence of an underlying set of rules and instructed to
edge about sequential structure is acquired, represented in             classify a novel set of sequences as grammatical or not, based
memory and expressed in behavior, and to understand the                 on their immediate intuitive judgement.
nature and characteristics of such knowledge representations                A robust and well replicated finding is that subjects per-
and of the underlying acquisition mechanisms. Importantly,              form significantly above chance, and performance improves
such pursuit must be grounded by the biophysical properties             if subjects are exposed to multiple sessions of implicit acqui-
of the neural processing infrastructure. Mapping such com-              sition. This means that humans are able to acquire knowledge
                                                                    427

                 T                              MTTVT                           Self-Organizing Spiking Network
                        V
                                                VXRRRRM
            M                                   VXVRXRRM             We explore a simplified spiking network model (Zheng, Dim-
                                     T          MTTTTVRXRRM          itrakakis, & Triesch, 2013), whose connectivity structure and
                   X      R                     VXTVRXRRM
                                                VXTTTTTTVT
                                                                     spiking dynamics are shaped by a combination of different
                                     M
            V                                   MTVT                 plasticity mechanisms, inspired by biology and capable of ac-
                        X                       MTTVRXM              curately reproducing the statistics and fluctuations of synaptic
                                R               VXVRXRRM
                                                                     connectivity patterns encountered in the neocortex, while ac-
                                                                     tively maintaining a stable firing activity.
Figure 1: Representation of a grammar commonly used in                  The model consists of a reservoir of noisy, threshold spik-
AGL studies (the Reber Grammar (A. Reber, 1967)) along-              ing neurons. The network is subdivided in excitatory and in-
side some examples of strings it generates.                          hibitory populations (N E excitatory and N I = 0.2 × N E in-
                                                                     hibitory neurons). All synaptic connections onto the exci-
                                                                     tatory population are subjected to adaptation through synap-
about complex rule systems by mere exposure and without              tic plasticity (see below), whereas connections to inhibitory
any performance feedback. The neurobiological correlates             neurons are fixed and static. W EE and W EI are sparse, with
of such ability are just starting to be unravelled and the pre-      connection probabilities of pEE = 0.1 and pEI = 0.2, re-
cise neurocomputational implementation is still largely unex-        spectively, whereas W IE is full (all-to-all connectivity). No
plored.                                                              synapses among inhibitory neurons and no ‘autapses’ are al-
                                                                     lowed. The initial synaptic strengths are randomly drawn
                                                                     from a uniform distribution in [0, 1] and subsequently normal-
Computing with neural circuits                                       ized such that the total synaptic input each neuron receives
                                                                     sums up to 1.
While the myriad of complex features one encounters in neu-
                                                                                                                             Out
robiology hinders a detailed description and understanding of                             W
                                                                                            In
                                                                                                                          W
all the system’s components and their interactions, a certain
                                                                                                 EE
degree of universality in both structure and dynamics can be                                   W
encountered in the neocortical microcircuitry, and ought to
be exploited in order to build simplified models capturing the
                                                                                     ...               EI           IE
                                                                                                                                    ...
essential features that are likely to be most relevant for the                                       W           W
computations it performs.
   One such property is the ubiquitous recurrent connectiv-
ity, making Recurrent Neural Network (RNN) models suit-              Figure 2: Schematic representation of the network used in
able candidates for neurocomputational studies. The ap-              this study. Dashed arrows represent dynamic synapses.
proach used throughout this study is inspired by the Reser-
voir Computing framework (Lukoševicius & Jaeger, 2009),                The network’s activity, at discrete time t is given by the
                                                                                                             E                          I
whose main strength lies in its simplicity. The idea is to pro-      binary vectors xE (t) ∈ {0, 1}N and xI (t) ∈ {0, 1}N , whose
vide a large enough neuronal pool with a time-varying input          dynamics are described by:
and train a set of readout units (receiving convergent synaptic
input from the pool) to perform some spatio-temporal trans-                                       NE                      NI
formation on the input stream based on the high-dimensional                  xiE (t + 1) = Θ      ∑ WiEE        E
                                                                                                           j x j (t) −   ∑ WikEI xkI (t)
                                                                                                  j=1                    k=1
representation generated by the network activity. The neu-                                                                                ! (1)
                                                                                                   NU
rons in the pool (or reservoir) are randomly, sparsely and re-
currently connected and can comprise varying degrees of bi-
                                                                                               +   ∑ Wiinj u j (t) + ξEi (t) − T E (t)
                                                                                                   j=1
ological detail. Apart from being simple to implement and
train, these models have been consistently shown to be very                                         NE
                                                                                                                                      !
                                                                                                                                    I
computationally powerful and readouts from such circuits can                    xiI (t + 1) = Θ     ∑     WiIE  E         I
                                                                                                             j x j (t) + ξi (t) − T         (2)
be trained to perform meaningful, non-trivial computations.                                         j=1
   The main caveat of this approach when it comes to brain-             The neurons’ thresholds (T E and T I ) are random values
inspired modelling, lies in the randomness of the recurrent          initially drawn from a uniform distribution in the interval
                                                                           E ] and [0, T I ], for excitatory and inhibitory units, re-
                                                                     [0, Tmax
connections, which remain fixed and untrained. Biological                                  max
networks are known to be shaped by several adaptation mech-          spectively. The Heaviside step function Θ(.) constrains the
anisms, making them effective processing devices and it is           network activations to a binary representation, i.e., a neuron
reasonable to assume that the nature of the processing task          fires if the total drive it receives exceeds its threshold, other-
will differentially tune the circuit’s properties to the current     wise it stays silent. ξE and ξI introduce a small amount of
needs.                                                               Gaussian white noise with µξ = 0 and σ2ξ = 0.04.
                                                                 428

   The readout neuron’s output is given by:                           Synaptic Normalization Proportionally distributes the
                                                                      strengths of incoming synapses onto an excitatory neuron
                                  NO                                  (W Eα , with α ∈ {E, I}). This rule keeps the weights bounded,
                       zi (t) = ∑ WikOut xkE (t)              (3)     while maintaining their relative distribution, thus implement-
                                  i=1
                                                                      ing a form of competition among synapses:
   The only form of supervised learning is performed to ob-                                WiEα         Eα         Eα
                                                                                             j (t)/∑ Wi j (t) = Wi j (t)              (8)
tain W Out , the synapses from the reservoir to the readout                                         j
units. The learning algorithm we chose to use was the
FORCE algorithm (Sussillo & Abbott, 2009), which is a re-             Structural Plasticity With a small probability pc = 0.001,
cursive algorithm, involving online, error-directed synaptic          a new synapse, with a strength of 0.001 is added between a
changes. The weights onto the readout neurons are updated             random pair of excitatory neurons.
according to:
                                                                      Input Structure
                     ∆W out = −P(t)e(t)xE (t)                 (4)     The input consists of symbolic temporal sequences (St =
                                                                      σ1 , σ2 , ..., σT ), whose symbols σi are drawn from the finite al-
where the N E × N E matrix P(t), contains a running estimate          phabet A = {#, M,V, R, T, X}, following the set of rules speci-
of the inverse of the correlation matrix of xE (t).                   fied by the Reber Grammar (RG) (traversing the allowed tran-
                                                                      sitions of the graph depicted in Figure 1) and concatenating
Adaptation                                                            the strings it generates with the symbol #, thus allowing the
                                                                      generation of potentially infinite symbol sequences.
The most important aspect of this model is the fact that its
structure and dynamics are shaped by the synergistic com-                An input layer (u) consisting of NU neurons is created,
bination of several plasticity mechanisms (see (Zheng et al.,         along with a (full) matrix of connection weights (W in ), whose
2013) for a more complete description of the model and plas-          values are uniformly drawn from [−1, 1], connecting the in-
ticity rules used in this study):                                     put layer to the main reservoir. W in is NU × N dimensional,
                                                                      N being the number of neurons in the main reservoir and
Spike-Timing-Dependent Plasticity The most well-                      NU = |A |. Sequence symbols are encoded in the input layer
known form of activity-dependent synaptic plasticity relies           (u) with a one-hot or exclusive coding scheme, i.e., all activ-
on its dependence on pre- and postsynaptic spike times. To            ities are set to 0, except the one corresponding to the current
implement spike-timing-dependent synaptic modifications,              symbol, which is set to 1.
all the synapses onto excitatory neurons are altered by small,
fixed amounts (η = 0.001), according to:                                            Predictive Modelling for the RG
                                                                      In order to assess the network’s ability to process the sym-
                                                                      bolic sequences and to extract the underlying regularities, we
        ∆WiEE              E       E            E        E
             j (t) = η(xi (t)x j (t − 1) − xi (t − 1)x j (t)) (5)     train it to perform 1-step prediction. Given the nature of the
                                                                      grammar, predictions are not deterministic so, if the network
   In order to balance the excitatory and inhibitory inputs to        is able to acquire the relevant representations from the train-
the excitatory neurons, a form of inhibitory STDP is also in-         ing data, it should activate all the output neurons correspond-
troduced modifying the weights of inhibitory synapses as:             ing to allowed transitions from the current input. Hence, it
                                                                      becomes appropriate for the prediction to take the form of
                                                                    a probability distribution of possible next items, and so the
                                             E          1
        ∆WiEIj (t) =  −ηx   I
                            j (t − 1)  1 − x i (t)(1 +     )  (6)     readout output is rectified (all negative values set to 0) and
                                                       µIP                                             O
                                                                      normalized (P̂i (t) = zi (t)/∑Nj=1 z j (t)).
                                                                         To adequately quantify the network’s performance, these
   By maintaining the balance and stabilizing the firing rate,
                                                                      probability estimates ought to be compared with some
this rule also serves a homeostatic purpose.
                                                                      context-dependent likelihood vector for the desired output,
Intrinsic Plasticity Distributes the network activity evenly          i.e., a target probability distribution for the possible next sym-
throughout the excitatory neurons, by regulating their respon-        bols (Ptarget ). To be able to equate our results with human
siveness and enforcing the maintenance of a constant average          subjects, we must assume that the network is a naive learner,
firing rate. The threshold of a neuron that has just been ac-         and that the ground true probabilities are not accessible. The
tive is decreased, while the threshold of an inactive neuron is       learner is exposed to a training sequence and the underlying
increased by the same amount (ηIP = 0.001):                           goal is to learn a model P̂ that assigns some probability to fu-
                                                                      ture outcomes, given some context. We start by evaluating the
             TiE (t + 1) = TiE (t) + ηIP (xiE (t) − HiIP )    (7)     performance of a series of general-purpose prediction algo-
                                                                      rithms under the same conditions, while accounting for differ-
where H IP ∼ N (µIP , σ2IP ) sets the target firing rates.            ent context lengths. The idea is that, if the network builds an
                                                                  429

appropriate internal model of the data, its predictions should           For that purpose, we measure the Kullback-Leibler diver-
be as good as the best performing model.This way, we can                 gence, which can be interpreted as a measure of the infor-
also disambiguate the amount of contextual information that              mation lost when P̂ is used to approximate Ptarget :
the network should memorize in order to make accurate pre-                                                                       !
                                                                                                                         P i
dictions, which can be fixed (N-gram Markov Models) or vary                                                  i            target
based on the locally available statistics of the training data                     DKL (Ptarget ||P̂) = ∑ Ptarget × log                (10)
                                                                                                        i∈A                 P̂i
(Variable Order Markov Models (VMMs)).
   The literature on VMMs is quite extensive, so we restricted              To adequately compare the results obtained from differ-
our analysis to some well-known algorithms that have been                ent network implementations, we determine prediction per-
tested in discrete sequence prediction tasks whose nature re-            formance as the mean normalized DKL (Per f ormance =
sembled that of our symbolic sequences (Begleiter, El-Yaniv,             − T1 ∑t=1
                                                                                T
                                                                                    1 − exp(−DKL (t))), which results in a value of 1
& Yona, 2004). The prediction performance of these algo-                 if DKL = 0. The results depicted in Figure 3 were obtained
rithms is evaluated using the average log-loss l(P̂, STest ) of          by training networks of different sizes (N E ) with an increas-
the model, with respect to the test sequence STest = σ1 ...σT :          ing number of strings and subsequently testing them on a
                                                                         fixed size test corpus composed of 1 × 104 strings. The bene-
                                 1  T                                    fits of self-organization through plasticity are evident. Given
              l(P̂, STest ) = −
                                 T ∑ P̂(σi |σ1 ...σi−1 )         (9)     enough training data (> 1000 strings), even a small network
                                   i=1                                   of only 200 neurons is capable of adequately representing
   The average log-loss is equivalent to the likelihood                  the temporal relations in the input sequence and predict the
P̂(STest ) = ∏Ti=1 P̂(σi |σ1 ...σi−1 ) and minimizing the average        test sequence with a significantly high performance (close to
log-loss is equivalent to maximizing a probability assignment            0.8, reflecting a very good agreement between the output es-
for the entire test sequence. The results of this preliminary            timates and the target distributions), despite the non-trivial
analysis are provided in Table 1 (see (Begleiter et al., 2004;           complexity of the task, requiring the maintenance of context
Dimitrakakis, 2010) for a complete description of the algo-              information in the neuron’s activities. Static networks, on
rithms). All the models were trained on the same training                the other hand perform barely above chance level, even when
sequence (length of 1 × 105 symbols) and tested on the same              large networks are trained with large amounts data.
test sequence (5 × 104 symbols). Whenever the models con-                   Furthermore, we determine the stability of the solution
tained some hyperparameter that required tuning (such as                 found by the readout algorithm by measuring its norm, the
maximum context depth), these were determined by 10-fold                 rationale for doing so being that a large value of |W Out | cor-
cross-validation over the training data.                                 responds to an output which depends heavily on some dimen-
                                                                         sions of state space, while ignoring others, thus leading to an
                                                                         unstable output which is very sensitive to variations in the
Table 1: Prediction algorithms tested. The acronyms in                   activity of some neurons and does not accurately reflect the
the variable context models stand for: Lempel-Ziv (LZ),                  population dynamics. The results demonstrate that the static
Decomposed Context Tree Weighting (D-CTW), Binary                        networks achieve the higher performances in conditions when
Context Tree Weighting (Bi-CTW), Prediction by Partial                   the solutions are less stable, whereas in the plastic case, the
Match (PPM) and Bayesian Variable Order Markov Model                     global mean squared error of the readout output is very small
(BVMM).                                                                  and the solutions found are stable, thus accurately portraying
     Fixed Context                    Variable Context                   the network dynamics.
     Algorithm Log-Loss               Algorithm Log-Loss                    Additionally, the spiking activity within the reservoir,
     1-Gram          2.5069           LZ             1.2753              while processing the input sequence, retains a healthy dynam-
     2-Gram          1.6702           D-CTW          1.0801              ics (see Figure 4), with asynchronous (low pairwise correla-
     3-Gram          1.0673           Bi-CTW         1.0882              tions) and irregular (coefficient of variation of the inter-spike
     4-Gram          1.0678           PPM            1.0971              intervals close to 1) firing activity. The average population fir-
     5-Gram          1.0679           BVMM           1.0677              ing rate is also actively maintained within the desired value,
                                                                         with small fluctuations around the mean.
                                                                            In order to obtain a better understanding of the impact of
   The results showed that a 3 − gram model is the best model            plasticity in shaping the network activity and it’s ability to ad-
for this data, so the probability estimates given by this model          equately learn the input structure, we systematically ‘knocked
were set as the target (i.e., Ptarget = P(St = σi |St−2 , St−1 )), a     out’ each plasticity mechanism individually and assessed how
result consistent with a careful inspection and analysis of the          much this would impact performance. The results depicted
RG (K. Petersson, Grenholm, & Forkstam, 2005).                           in Figure 5 (top) show that while structural plasticity (SP) is
                                                                         the less relevant for the network’s ability to perform the task,
Prediction Performance                                                   synaptic normalization (SN) and inhibitory STDP (iSTDP)
After setting the target probabilities, we need to compare               have a particularly large influence. If we analyse how activity
them with the estimates provided by the readout output (P̂).             spreads from one time step to the next, as displayed in Fig-
                                                                     430

                       Performance                                            MSE                                                         |W out|                                                                  0.8
                                                                                                                                                                                                                   0.7
                                                                                                                                                                                                                   0.6
                                                                                                                                                                                                     Performance
          100
                                                                                                                                                                                                                   0.5
          300
                                                                                                                                                                                                                   0.4                      ALL               IP                                                                SP
                                                                                                                                                                                                                   0.3
                                                                                                                                                                                                                                                                                     eSTDP
E
 N        500                                                                                                                                                                                                      0.2
                                                                                                                                                                                                                                                                                                          iSTDP                                    SN
                                                                                                                                                                                                                   0.1
          700                                                                                                                                                                                                        0
                                                                                                                                                                                         1                                                                    1                                                                   1
          900                                                                                                                                                                          0.9                                                                  0.9                                                                 0.9
                                                                                                                                                                                       0.8                                                                  0.8                                                                 0.8
                                                                                                                                                                                       0.7                                                                  0.7                                                                 0.7
                                                                                                                                                                              a(t+1)
                                                                                                                                                                                       0.6                                                                  0.6                                                                 0.6
               0.2          0.4        0.6            0.8       0.135       0.125      0.115     0.105                        0.5           1            1.5
                                                                                                                                                                                       0.5                                                                  0.5                                                                 0.5
                                                                                                                                                                                       0.4                                                                  0.4                                                                 0.4
          100                                                                                                                                                                          0.3                                                                  0.3                                                                 0.3
                                                                                                                                                                                       0.2                                                                  0.2                                                                 0.2
          300                                                                                                                                                                          0.1                                                                  0.1                                                                 0.1
                                                                                                                                                                                         0                                                                    0                                                                   0
                                                                                                                                                                                          0   0.1   0.2     0.3    0.4   0.5    0.6   0.7   0.8   0.9   1      0   0.1   0.2   0.3     0.4   0.5    0.6   0.7   0.8   0.9   1      0   0.1   0.2   0.3   0.4   0.5    0.6   0.7   0.8   0.9   1
          500
E
 N                                                                                                                                                                                                                       a(t)                                                                a(t)                                                              a(t)
          700
          900                                                                                                                                                               Figure 5: Impact of the different plasticity rules on predic-
               100         1e3      1e4         1e5           100       1e3          1e4       1e5
                           # Strings (train)                            # Strings (train)
                                                                                                                        100         1e3          1e4
                                                                                                                                    # Strings (train)
                                                                                                                                                               1e5
                                                                                                                                                                            tion performance and network dynamics. Top: Performance
                                                                                                                                                                            results obtained with all mechanisms and in the absence of
Figure 3: Performance of dynamic (top row) and static net-                                                                                                                  those highlighted. Bottom: Spread of activity from one time
works (bottom row), for increasing amounts of training and                                                                                                                  step to the next, in a network with all plasticity mechanisms
different network sizes. Results depict the average of 10 sim-                                                                                                              (left), without iSTDP (middle) and without SN (right).
ulations per condition.
                                                                                                                 0.16
                                                                                                                                                                            previous section, we interpret the network’s output as a prob-
          5                                                                                                      0.14
                                                                                                                 0.12
                                                                                                                                                                            ability distribution, but now we store, on each step, the prob-
         10
                                                                                                     Frequency
         15
                                                                                                                  0.1
                                                                                                                 0.08
                                                                                                                                                                            ability mass that the readout assigned to the correct symbol,
         20
                                                                                                                 0.06
                                                                                                                 0.04
                                                                                                                                                                            the one that actually occurs. We then take the product of all
Neuron
         25                                                                                                      0.02
                                                                                                                    0
                                                                                                                        0            50            100          150
                                                                                                                                                                            symbol predictions on an individual string and normalize it
         30
                                                                                                                                          ISI
         35                                                                                                      0.05
                                                                                                                                                                            by taking the logarithm of this value and dividing it by the
         40                                                                                                      0.04                                                       string length. Formally, consider a string L of length N and
                                                                                                     Frequency
         45
         50
                                                                                                                 0.03
                                                                                                                                                                            the network’s next-symbol predictions P̂, the string ‘likeli-
                                                                                                                 0.02
         0.4
                                                                                                                 0.01
                                                                                                                                                                            hood ratio’ (LR) is given by:
Rate     0.2
                                                                                                                   0
                                                                                                                              0.7   0.8    0.9     1     1.1    1.2                                                                                                           N−1
           0
                     200     400     600       800    1000
                                                 Time (sts)
                                                              1200   1400     1600    1800
                                                                                                                                          CV−ISI                                                                                                                     1
                                                                                                                                                                                                                                 LRL = −                                log2 ( ∏ p̂i (t)),                                                                                               (11)
                                                                                                                                                                                                                                                                   N −1        t=1
Figure 4: Snapshot of network activity in the initial steps
of the test phase. The figure depicts the activity of 50 ran-                                                                                                               where p̂i (t) = P̂(L(t + 1) = σi ), knowing that L(t + 1) = σi .
domly selected excitatory neurons as well as the distributions                                                                                                                 We treat each network simulation as a subject, train and test
of inter-spike intervals and their coefficient of variation.                                                                                                                them using the exact same string set used in a human AGL ex-
                                                                                                                                                                            periment (for a detailed description of the experimental meth-
                                                                                                                                                                            ods used for the acquisition of the behavioral data, including
ure 5 (bottom), a pattern distinctly emerges. The combined                                                                                                                  the characteristics of the stimulus material, see (Forkstam,
action of all the plasticity mechanisms, imposes a contractive                                                                                                              Hagoort, Fernandez, Ingvar, & Petersson, 2006) and refer-
dynamics in the network’s state space, important to create ad-                                                                                                              ences therein). Apart from grammaticality status, the test
equate trajectories that can be easily readout. If either of the                                                                                                            strings are also divided in subsets that account for how fre-
mechanisms with a bigger impact on performance is taken                                                                                                                     quently 2 and 3 letter chunks making up the strings appear
out, the activity spreads and the network is allowed to explore                                                                                                             in the training set (low (L) and high (H) associative chunk
a vaster region of state space, which has a natural deleterious                                                                                                             strength). Human classification performance is assessed in
effect on the separation of the input into distinct trajectories                                                                                                            terms of endorsement rates (i.e. fraction of strings classified
and hinders the network’s ability to acquire and reflect the                                                                                                                as grammatical) and compared with the network likelihood
structure of its input. Besides, the spiking activity becomes                                                                                                               ratios, averaged over all the strings of each type and all the
more regular and synchronized (insets in Figure 5).                                                                                                                         ‘experimental subjects’ (Figure 6). The results of this anal-
                                                                                                                                                                            ysis display a similar pattern of variation from the untrained
                                           Judging String Legality                                                                                                          (baseline) to the trained (grammaticality) conditions between
To complement the analysis, we devised a protocol to clas-                                                                                                                  the network’s normalized likelihood ratio scores and the hu-
sify string sets, in a manner that can be equated and com-                                                                                                                  man endorsement rates, in all conditions and for the differ-
pared to human behavioral experiments. The network is thus                                                                                                                  ent string types. Similar to human subjects, the networks are
expected to discern whether a string was generated by the                                                                                                                   highly sensitive to grammaticality status and, after a training
grammar and adheres to its rules or not (Grammatical (G) or                                                                                                                 or acquisition phase, there is a remarkable difference between
Non-Grammatical (NG)). Following the same line as in the                                                                                                                    G/NG items in both NLR scores and endorsement rates.
                                                                                                                                                                      431

                                                    Baseline
                                                                                        the present model is computationally capable of coping with
Endorsement Rate
                   0.6                                               0.98
                                                    Grammaticality                      the high amount of structural complexity attributed to natural
                                                                            NLR
                                                                     0.97               language in a formal sense (of which we are just skimming
                   0.5
                                                                     0.96               the surface), remains an open question to be addressed in fu-
                   0.4                                               0.95               ture work.
                         G   NG                 L         H
                                  String type
                                                                                                           Acknowledgments
                                                                                        We thank K. M. Petersson for insightful comments and for
Figure 6: Grammaticality classification. Comparison of the
                                                                                        providing the behavioral data as well as the anonymous re-
network’s likelihood ratio (bars enclosed by full lines, right
                                                                                        viewers of the initial submission for the useful comments
axis) with human endorsement rates (bars enclosed by dashed
                                                                                        provided.This work was partially funded by the Erasmus
lines, left axis), before and after training.
                                                                                        Mundus Joint Doctoral Program EuroSPIN, BMBF Grant
                                                                                        01GQ0420 to BCCN Freiburg, the Junior Professor Program
                              Discussion                                                of Baden-Württemberg, the Helmholtz Alliance on Systems
                                                                                        Biology (Germany), the Initiative and Networking Fund of
In this work, we have showed that, through the process of                               the Helmholtz Association and the Helmholtz Portfolio theme
self-organization, mediated by biologically plausible plastic-                          “Supercomputing and Modelling for the Human Brain”.
ity mechanisms, spiking neural networks are capable of ac-
quiring the structure of a simple (regular) grammar and de-                                                     References
veloping a reliable predictive model, whose predictions are                             Begleiter, R., El-Yaniv, R., & Yona, G. (2004). On prediction
comparable to those of the best performing learning algo-                                 using variable order Markov models. Journal of Artificial
rithms. By mere exposure to strings generated by the gram-                                Intelligence Research, 22, 385–421.
mar, the network functionally develops stable, confined tra-                            Dimitrakakis, C. (2010). Bayesian variable order Markov
jectories that can be accurately mapped, readout and used to                              models. In Proceedings of the 13th international confer-
make correct, context-dependent predictions. Furthermore,                                 ence on artificial intelligence and statistics (pp. 161–168).
the magnitude of plasticity-induced changes that the network                            Elman, J. L., & Diego, S. (1991, September). Distributed
is subjected to tends to stabilize, allowing it to develop sta-                           representations, simple recurrent networks, and grammati-
ble and healthy dynamics throughout training and testing, de-                             cal structure. Machine Learning, 7(2-3), 195–225.
spite having some of its components permanently subjected                               Forkstam, C., Hagoort, P., Fernandez, G., Ingvar, M., & Pe-
to adaptation. When asked to judge string legality, based on                              tersson, K. M. (2006). Neural correlates of artificial syn-
a likelihood metric computed on the network’s output esti-                                tactic structure classification. NeuroImage, 32(2), 956–67.
mations, it performs at a qualitatively comparable level to                             Lukoševicius, M., & Jaeger, H. (2009). Reservoir computing
that of human subjects on a similar task, displaying a par-                               approaches to recurrent neural network training. Computer
ticularly high sensitivity to the grammaticality status of the                            Science Review, 3(3), 127–149.
tested strings.                                                                         Petersson, K., Grenholm, P., & Forkstam, C. (2005). Artifi-
   While the focus of this work has been on human perfor-                                 cial grammar learning and neural networks. In Proceedings
mance data as reported in the psycholinguistic literature, it                             of the cognitive science society.
should be noted that, from a purely computational perspec-                              Petersson, K. M., & Hagoort, P. (2010). The neurobiology of
tive, the task of learning a finite-state (regular) language, such                        syntax: beyond string sets. Philosophical transactions of
as that generated by the RG, bears only a tenuous similarity                              the Royal Society of London. Series B, Biological sciences,
to human language learning, which is known to require much                                367(1598), 1971–83.
more complex dependencies. Previous work employing sim-                                 Reber, A. (1967). Implicit learning of artificial grammars.
pler models, such as the Simple Recurrent Network (SRN),                                  Journal of verbal learning and verbal behavior, 6, 855–
has already accounted for the ability of such learning devices                            863.
to acquire the dependencies present in such languages (K. Pe-                           Reber, A. S., Walkenfeld, F. F., & Hernstadt, R. (1991).
tersson et al., 2005) and even extended these results to more                             Implicit and explicit learning: individual differences and
informative classes (e.g. (Elman & Diego, 1991)). How-                                    IQ. Journal of experimental psychology. Learning, mem-
ever, most of these previous studies shows little or no parallel                          ory, and cognition, 17(5), 888–96.
with the biophysical properties of the neuronal system both in                          Sussillo, D., & Abbott, L. F. (2009). Generating coherent
terms of structure (typically employing fully connected recur-                            patterns of activity from chaotic neural networks. Neuron,
rent networks, with real-valued, continuous unit activations)                             63(4), 544–557.
and learning mechanisms (based on some form of error back-                              Zheng, P., Dimitrakakis, C., & Triesch, J. (2013). Net-
propagation). What sets the current approach apart from pre-                              work self-organization explains the statistics and dynamics
vious work is the close parallel with biology at various lev-                             of synaptic connection strengths in cortex. PLoS computa-
els, from the single neurons’ activations to the unsupervised                             tional biology, 9(1), e1002848.
learning mechanisms shaping population dynamics. Whether
                                                                                  432

