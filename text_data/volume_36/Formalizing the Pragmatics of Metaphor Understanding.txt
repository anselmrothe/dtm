UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Formalizing the Pragmatics of Metaphor Understanding
Permalink
https://escholarship.org/uc/item/09h3p4cz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Kao, Justine
Bergen, Leon
Goodman, Noah
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             Formalizing the Pragmatics of Metaphor Understanding
Justine T. Kao1 (justinek@stanford.edu), Leon Bergen2 (bergen@mit.edu), Noah D. Goodman1 (ngoodman@stanford.edu)
                    1 Department    of Psychology, Stanford University. 2 Department of Brain and Cognitive Science, MIT.
                                    Abstract                                  theorists argue that this principle explains how listeners in-
                                                                              fer the meaning of a novel metaphor as well as other forms of
        While the ubiquity and importance of nonliteral language are          loose talk where the meaning of an utterance is underspecified
        clear, people’s ability to use and understand it remains a mys-
        tery. Metaphor in particular has been studied extensively             (Sperber & Wilson, 1985). When interpreting the metaphor
        across many disciplines in cognitive science. One approach fo-        “My lawyer is a shark,” for example, the listener assumes that
        cuses on the pragmatic principles that listeners utilize to infer     the speaker aims to communicate features of “a shark” that
        meaning from metaphorical utterances. While this approach
        has generated a number of insights about how people under-            are relevant to the person under discussion (“my lawyer”) and
        stand metaphor, to our knowledge there is no formal model             do not make use of other shark features—vicious but not has
        showing that effects in metaphor understanding can arise from         fins or swims.
        basic principles of communication. Building upon recent ad-
        vances in formal models of pragmatics, we describe a com-
        putational model that uses pragmatic reasoning to interpret              While many linguists and psychologists have argued for
        metaphorical utterances. We conduct behavioral experiments            the benefits of studying metaphor using a pragmatics frame-
        to evaluate the model’s performance and show that our model
        produces metaphorical interpretations that closely fit behav-         work, to our knowledge there is no formal model showing
        ioral data. We discuss implications of the model for metaphor         that effects in metaphor understanding may arise from basic
        understanding, principles of communication, and formal mod-           principles of communication. On the other hand, a recent
        els of language understanding.
                                                                              body of work presents a series of computational models for
        Keywords: language understanding; metaphor; pragmatics;
        computational models                                                  pragmatic reasoning, where speaker and listener reason about
                                                                              each other to communicate effectively (Frank & Goodman,
                                                                              2012; Jäger & Ebert, 2009). By formalizing principles of
                                Introduction                                  communication, these models are able to make quantitative
     From “Juliet is the sun” to “That woman is a bombshell,” non-            predictions about a range of phenomena in language under-
     literal language is, quite literally, everywhere. Metaphor, hy-          standing, such as scalar implicature and the effect of alter-
     perbole, and sarcasm are ubiquitous features of human com-               native utterances (Goodman & Stuhlmüller, 2013; Bergen,
     munication, often creating poetic or humorous effects that               Goodman, & Levy, 2012). However, a limitation of these
     add rich dimensions to language (Glucksberg, 2001; Pilking-              models is that they are unable to explain the use of utterances
     ton, 2000; Lakoff & Turner, 2009; Roberts & Kreuz, 1994).                which are known to have false literal interpretations. More re-
     Metaphor has inspired a particularly large amount of research            cent work extends these models to consider affective goals in
     in cognitive science, spanning topics such as how metaphors              communication that may be optimally satisfied by nonliteral
     structure and shape our thoughts (Ortony, 1993; Lakoff et                utterances such as hyperbole (under review). In this paper, we
     al., 1993; Thibodeau & Boroditsky, 2011), whether metaphor               will be considering the role of communicative goals in the in-
     processing recruits the same strategies as standard language             terpretation of metaphorical utterances. In our formalization,
     processing (Giora, 1997; Gibbs, 2002; Glucksberg & Keysar,               a listener assumes that the speaker chooses an utterance to
     1993) and what factors determine people’s interpretation a               maximize informativeness about a subject along dimensions
     novel metaphor (Gentner & Wolff, 1997; Blasko & Connine,                 that are relevant to the conversation and consistent with the
     1993; Tourangeau & Sternberg, 1981; Kintsch & Bowles,                    speaker’s communicative goal. This makes it possible for a
     2002). This overwhelming interest in metaphor research is                literally false utterance to be optimal as long as it is informa-
     due to both the ubiquity of metaphor in everyday language                tive along the target dimension. Our framework closely aligns
     and the potential role of metaphor for helping us understand             with the relevance-theoretic view that a listener considers the
     how the mind creates meaning.                                            relevance of a potential meaning to the speaker’s goal in order
        One approach to studying metaphor focuses on the prag-                to infer what the speaker intended to communicate.
     matic principles that listeners utilize to infer meaning from
     metaphorical utterances (Tendahl & Gibbs, 2008). Rather                     To reasonably limit the scope of our work, we focus on
     than view metaphor as a separate mode of communication                   metaphors of the classic form “X is a Y .” We describe a com-
     that requires specialized language processing strategies, this           putational model that can interpret such sentences metaphor-
     approach argues that basic principles of communication drive             ically and conduct behavioral experiments to evaluate the
     the meaning that a listener infers from a metaphor (Sperber              model’s performance. We show that peoples’ interpretations
     & Wilson, 2008; Wilson & Carston, 2006). Relevance the-                  of metaphors are driven by conversational context and that
     ory, in particular, posits that listeners interpret utterances with      our model captures this effect. Finally, we show that our
     the assumption that speakers produced them because they are              model predictions correlate significantly with people’s fine-
     maximally relevant (Wilson & Sperber, 2002). Relevance                   grained interpretations of metaphorical utterances.
                                                                          719

                     Computational Model                                  speaker S1 :
In the basic Rational Speech Act model of (Frank & Good-
                                                                                      U(u|g, ~f ) = log ∑ δg(~f )=g(~f 0 ) L0 (c, ~f 0 |u) (1)
man, 2012; Goodman & Stuhlmüller, 2013), a listener and a
                                                                                                          c,~f 0
speaker recursively reason about each other to arrive at prag-
matically enriched meanings. Given an intended meaning,                   Given this utility function, the speaker chooses an utterance
a speaker reasons about a literal listener and chooses an ut-             according to a softmax decision rule that describes an approx-
terance based on its informativeness. A pragmatic listener                imately rational planner (Sutton & Barto, 1998):
then reasons about the speaker and uses Bayes’ rule to infer
the meaning given the utterance. To account for nonliteral                                                                ~
                                                                                                  S1 (u|g, ~f ) ∝ eλU(u|g, f ) ,           (2)
interpretation, we extend this model by considering the idea
that a speaker may have a range of different communicative                where λ is an optimality parameter.
goals. Intuitively, an utterance is optimally informative and                Imagine that S1 had the goal to convey f1 , scariness, about
relevant if it satisfies the speaker’s communicative goal. Since          John. Based on S1 ’s understanding of L0 ’s prior knowledge,
the speaker’s precise communicative goal may be unknown to                she knows that if she produces the utterance “John is a shark,”
the listener, the listener performs joint inference on the goal           L0 will believe that John is literally a shark and hence very
as well as the intended meaning. By introducing multiple                  likely to be scary. Since S1 ’s goal is satisfied if the lis-
potential goals for communication, we open up the possibil-               tener believes that John is scary, S1 is motivated to produce
ity for a speaker to produce an utterance that is literally false         such a metaphorical utterance. A pragmatic listener, how-
but still satisfies her goal. The speaker achieves this in part           ever, should be able to leverage this pattern to infer that John
by exploiting her own and the listener’s prior knowledge—                 is scary without inferring that John is actually a shark.
their common ground (Clark, 1996)—to reason about what                       The listener L1 performs Bayesian inference to guess the
information the listener would gain if he takes the utterance             intended meaning given prior knowledge and his internal
literally.                                                                model of the speaker. To determine the speaker’s intended
     To illustrate this idea more concretely and demonstrate              meaning, Ln will marginalize over the possible speaker goals
how it is implemented in our model, we will use the metaphor              under consideration:
“John is a shark” as an example. For simplicity, in this model
we restrict the number of possible categories to which a mem-                        L1 (c, ~f |u) ∝ P(c)P(~f |c) ∑ P(g)S1 (u|g, ~f )
ber may belong to ca and c p , denoting an animal category or                                                       g
a person category, respectively. We also restrict the possi-
                                                                          While speaker and listener could continue to reason about
ble features of John under consideration to a vector of size
                                                                          each other recursively, resulting in Ln , we restrict ourselves
three: ~f = [ f1 , f2 , f3 ], where fi is either 0 or 1 (for example,
                                                                          to L1 for present purposes. Past work has shown that this first
the three features could be scary, sleek, and finned).1 The lit-
                                                                          level of pragmatic reasoning is often a good model of human
eral listener L0 will interpret the utterance “John is a shark”
                                                                          comprehension.2 If listener L1 thinks it is likely that speaker
as meaning that John is literally a member of the category
                                                                          S1 ’s goal is to convey scariness but believes it is a priori very
“shark” and has corresponding features. Formally, if u is the
                                                                          unlikely that John is actually a shark, she will determine that
uttered category:
                                                                          S1 is using shark metaphorically—that John is a scary person.
                                                                             Based on this formulation, the listener needs to consider
                                 P(~f |c)
                               
                     ~                         if c = u
               L0 (c, f |u) =                                             the following prior probabilities to arrive at an interpretation:
                                 0             otherwise
                                                                        (1) P(c): the prior probability that the entity discussed belong-
where P(~f |c) is the prior probability that a member of cate-               ings to category c. We assume that the listener is extremely
gory c (in this case “shark” or “person”) has feature vector                 confident that the person under discussion (e.g. John) is a
~f .                                                                         person, but that there is a non-zero probability that John is
     We assume that the speaker’s goal is to communicate the                 actually a non-human animal. We fit P(ca ) to data with the
value of a particular feature—a goal is thus a projection from               assumption that 10−4 ≤ P(ca ) ≤ 10−1 .
the full feature space to the subset of interest to the speaker.
Formally, the goal to communicate about feature i ∈ {1, 2, 3}           (2) P(~f |c): the prior probability that a member of category
is the function gi (~f ) = fi . Following the Rational Speech                c has feature values ~f . This is empirically estimated in
Act model, we define the speaker’s utility as the negative sur-              Experiment 1.
prisal of the true state under the listener’s distribution, given
                                                                        (3) P(g): the probability a speaker has goal g. This prior can
an utterance. However, here we consider only the surprisal
                                                                             change based on the conversational context that a question
along the goal dimension. To do so we project along the goal
dimension, which leads to the following utility function for                  2 The current model does not robustly predict metaphorical in-
                                                                          terpretations at recursion depths greater than 1. Future work will
     1 In principle the model can be extended to accommodate more         investigate which features of the model lead to this prediction, and
categories and features.                                                  whether this remains true under alternative model definitions.
                                                                      720

 Animal       f1 = 1     f2 = 1 f3 = 1      f1 = 0      f2 = 0       f3 = 0       Animal    f1 = 1   f2 = 1   f3 = 1     f1 = 0     f2 = 0       f3 = 0
    ant       small     strong    busy       large      weak           idle        goose     loud    mean annoying        quiet      nice      agreeable
    bat       scary      blind nocturnal unalarming    sighted      diurnal        horse      fast   strong beautiful     slow      weak          ugly
   bear       scary        big   fierce unalarming      small     nonviolent     kangaroo   jumpy   bouncy      cute    relaxed   inelastic   unattractive
    bee        busy      small   angry        idle       large     unangry          lion  ferocious scary     strong nonviolent unalarming        weak
   bird         free   graceful small       unfree    awkward         large       monkey    funny    smart   playful humorless      stupid     unplayful
  buffalo       big     strong    wild      small       weak          tame           owl     wise     quiet nocturnal foolish        loud       diurnal
     cat independent lazy          soft  dependent        fast        hard            ox    strong     big     slow      weak       small          fast
   cow           fat     dumb     lazy        thin      smart          fast      penguin     cold      cute   funny        hot   unattractive  humorless
    dog        loyal   friendly happy     disloyal   unfriendly    unhappy           pig     dirty      fat  smelly      clean       thin       fragrant
 dolphin      smart    friendly playful     stupid   unfriendly unplayful          rabbit     fast    furry     cute      slow     hairless   unattractive
   duck        loud       cute quacking      quiet  unattractive non-quacking      shark     scary dangerous mean unalarming         safe         nice
 elephant      huge      smart  heavy       small       stupid        light        sheep    wooly    fluffy   dumb      hairless     hard        smart
    fish      scaly       wet   smelly     smooth         dry      fragrant         tiger  striped   fierce    scary unpatterned nonviolent   unalarming
    fox          sly     smart  pretty      artless     stupid        ugly         whale     large  graceful majestic    small    awkward       inferior
   frog       slimy      noisy  jumpy nonslippery        quiet      relaxed         wolf     scary   mean     angry unalarming       nice       unangry
   goat       funny     hungry    loud   humorless        full        quiet        zebra   striped   exotic     fast  unpatterned native          slow
Table 1: 32 animal categories, feature adjectives, and their antonyms. Feature adjectives were elicited from Experiment 1a and
indicate when a feature is present ( fi = 1). Antonyms were generated using WordNet and indicate when a feature is not present
( fi = 0). Feature sets shown in Experiment 1b were created with this table, where ~f = [1, 0, 0] for category “ant” is represented
by the words {small, weak, idle}. There are 23 = 8 possible feature combinations for each animal category.
    sets up. For example, if the speaker is responding to a                     popular adjective among a set of synonyms. We then took the
    vague question about John, e.g. “What is John like?”, the                   three most popular adjectives for each animal category and
    prior over goals is uniform. If the question targets a specific             used them as the set of features. In what follows, f1 is the
    features, such as “Is John scary?”, then she is much more                   most popular adjective, f2 the second, and f3 the third. Table
    likely to have the goal of communicating John’s scariness.                  1 shows the animal categories and their respective features.
    However, she may still want to communicate other fea-
    tures about John that were not asked about. We assume                       Experiment 1b: Feature Prior Elicitation
    that when the question is specific, the prior probability that              Materials Using the features collected from Experiment
    Sn ’s goal is to answer the specific question is greater than               1a, we elicit the prior probability of a feature vector given
    0.5, fitting the value to data below.                                       an animal or person category (i.e. P(~f |c)). We assume that
                                                                                the adjective corresponding to a feature (e.g. scary) indicates
                     Behavioral Experiments                                     that the value of that feature is 1 (present), while the adjec-
To evaluate our model’s interpretation of metaphorical utter-                   tive’s antonym indicates that the value of that feature is 0 (not
ances, we focused on a set of 32 metaphors comparing human                      present). We used WordNet to construct antonyms for each of
males to different non-human animals. We conducted Experi-                      the adjective features produced in Experiment 1a. When mul-
ment 1a and 1b to elicit feature probabilities for the categories               tiple antonyms existed or when no antonym could be found
of interest. We then conducted Experiment 2 to measure peo-                     on WordNet, the first author used her judgment to choose
ple’s interpretations of the set of metaphors.                                  the appropriate antonym. Table 1 shows the resulting list of
                                                                                antonyms. For each animal category, eight possible feature
Experiment 1a: Feature Elicitation                                              combinations were constructed from the three features and
Materials We selected 32 common non-human animal cat-                           their antonyms. For example, the possible feature combina-
egories from an online resource for learning English (www                       tions for a member of the category “ant” are {small, strong,
.englishclub.com). The full list is shown in Table 1.                           busy}, {small, strong, idle}, {small, weak, busy}, and so on.
Methods 100 native English speakers with IP addresses in                        Methods 60 native English speakers with IP addresses in
the United States were recruited on Amazon’s Mechanical                         the United States were recruited on Amazon’s Mechanical
Turk. Each participant read 32 animal category names pre-                       Turk. Each participant completed 16 trials in random or-
sented in random order, e.g. “whale”, “ant”, “sheep”. For                       der. Each trial consisted of the eight feature combinations
each animal category, participants were asked to type the first                 for a particular animal category. Using slider bars with ends
adjective that came to mind in a text box.                                      marked by “Impossible” and “Absolutely certain,” partici-
Results Using participants’ responses, we constructed a list                    pants were asked to rate how likely it is for a member of the
of adjectives for each animal category and ordered them by                      animal category to have each of the eight feature combina-
the number of times they were given by a different subject                      tions. Participants also rated the probabilities of the feature
(i.e. their popularity). We removed all color adjectives, such                  combinations for a male person. We only elicited priors for
as “white” and “black,” to eliminate the possibility of inter-                  males to minimize gender variation and to maintain consis-
preting these adjectives as racial descriptions. To avoid re-                   tency with Experiment 2.
dundancy in the feature set, we used WordNet (Miller, 1995)                     Results We normalized each participant’s ratings for the
to identify synonymous adjectives and only kept the most                        eight feature combinations in a trial to sum up to 1 based
                                                                            721

on the assumption that the feature combinations exhaus-
                                                                                                                                  Literal utterance              Metaphorical utterance
tively describe a member of a particular category. Using                                                             1.00                                                           Feature
the Spearman-Brown prediction equation, reliability of the
                                                                                                                                                                                          f1
ratings was 0.941 (95% CI = [0.9408, 0.9414]). Averag-
                                                                            Probability of feature given utterance
                                                                                                                                                                                          f2
ing across participants’ normalized ratings, we obtained fea-                                                        0.75
ture priors P(~f |c) for c = ca (animal) and c = c p (person).                                                                                                                            f3
Since the features were created using the animal categories
in Experiment 1a, by construction features are rated as sig-                                                         0.50
nificantly more likely to be present in the animal category
than in the person category (F(1, 190) = 207.1, p < 0.0001).
These results confirm that participants are fairly confident                                                         0.25
that each animal category has certain distinguishing features
(mean= 0.61, sd= 0.06), while those same features are rated
as appearing in people less often (mean= 0.48, sd= 0.06).                                                            0.00
                                                                                                                            Vague goal        Specific goal   Vague goal       Specific goal
Experiment 2: Metaphor Understanding
Materials We created 32 scenarios based on the animal cat-
egories and results from Experiment 1. In each scenario, a                 Figure 1: Average probability ratings for the three features
person (e.g. Bob) is having a conversation with his friend                 given a vague/specific goal and a literal/metaphorical utter-
about a person that he recently met. Since we are inter-                   ance. Error bars are standard error over the 32 items.
ested in how the communicative goals set up by context af-
fect metaphor interpretation as well as the effectiveness of
                                                                           ure 1 shows the average ratings for each feature across an-
metaphorical versus literal utterances, we created four con-
                                                                           imal categories given a vague or specific goal and a lit-
ditions for each scenario by crossing vague/specific goals
                                                                           eral or metaphorical utterance. When the speaker gives a
and literal/metaphorical utterances. In vague goal conditions,
                                                                           literal statement directly affirming the presence of f1 , par-
Bob’s friend asks a vague question about the person Bob re-
                                                                           ticipants rate f1 as significantly more likely than when the
cently met: “What is he like?” In specific goal conditions,
                                                                           speaker gives a metaphorical statement (F(1, 126) = 52.6,
Bob’s friend targets f1 and asks a specific question about the
                                                                           p < 0.00001). However, participants rate f2 and f3 as signifi-
person: “Is he f1 ?,” where f1 is the most popular adjective
                                                                           cantly more likely when the speaker produces a metaphorical
for a given animal category ca . In literal conditions, Bob
                                                                           utterance than when the utterance is literal (F(1, 126) = 23.7,
replies with a literal utterance, either by saying “He is f1 .”
                                                                           p < 0.0001; F(1, 126) = 13.66, p < 0.0005). Comparing fea-
to the question “What is he like?” or “Yes.” to the question
                                                                           ture probability ratings in Experiment 2 to the feature priors
“Is he f1 ?”. In Metaphorical conditions, Bob replies with a
                                                                           obtained in Experiment 1b, we can measure how literal and
metaphorical statement, e.g. “He is a ca .” where ca is an ani-
                                                                           metaphorical utterances change listeners’ inferences about a
mal category. See Table 2 for examples of each condition.
                                                                           person’s features. Given a literal utterance that directly con-
                                                                           firms the existence of f1 , probability ratings for f1 are signif-
 Goal       Utterance       Example question     Example utterance         icantly higher than the prior probabilities of f1 for a person
 Vague      Literal         “What is he like?”   “He is scary.”            (t(63) = 59.19, p < 0.00001). However, probability ratings
 Specific   Literal         “Is he scary?”       “Yes.”                    for f2 and f3 are not significantly different from their prior
 Vague      Metaphorical    “What is he like?”   “He is a shark.”
 Specific   Metaphorical    “Is he scary?”       “He is a shark.”          probabilities (t(63) = −0.13, p = 0.89; t(63) = 0.03, p =
                                                                           0.97). Given a metaphorical utterance, probability ratings for
Table 2: Example scenarios given the four experimental con-                all three features are significantly higher than the prior proba-
ditions in Experiment 2.                                                   bilities (t(63) = 15.74, p < 0.0001; t(63) = 7.29, p < 0.0001;
                                                                           t(63) = 5.91, p < 0.0001). While preliminary, this analy-
                                                                           sis suggests that metaphorical utterances may convey richer
Methods 49 native English speakers with IP addresses in                    information and update listeners’ beliefs along more dimen-
the United States were recruited on Amazon’s Mechanical                    sions than literal utterances.
Turk. Each participant completed 32 trials in random order.                   We now analyze the effect of the speaker’s communica-
The 32 trials were randomly and evenly assigned to one of the              tive goal on the interpretation of literal or metaphorical ut-
four conditions, i.e. each participant read 8 scenarios for each           terances. When the speaker’s utterance is literal, the proba-
condition. For each trial, participants used sliders to indicate           bility ratings for f1 , f2 , and f3 are not significantly different
the probabilities that the person described has features f1 , f2 ,         given a vague or a specific question ((F(1, 62) = 2.73, p =
and f3 , respectively.                                                     0.1; F(1, 62) = 0.0001, p = 0.99; F(1, 62) < 0.0001, p =
Results For each condition of each scenario, we obtained                   0.99). For metaphorical utterances, however, the question
the average probability ratings for the three features. Fig-               type has an effect on participants’ interpretations: partici-
                                                                     722

pants rate the probability of f1 as significantly higher when
the question is specifically about f1 than when it is vague
(F(1, 62) = 10.16, p < 0.005). The probabilities of f2 and                          0.8                                               Goal
 f3 are not significantly different given a vague question or                                                                                Vague
a specific question about f1 (F(1, 62) = 0.04, p > 0.05;
                                                                                                                                             Specific
F(1, 62) = 0.8285, p > 0.05). This suggests that people’s
                                                                            Human
                                                                                    0.6
interpretation of metaphor may be more sensitive to the com-                                                                          Feature
municative goals set up by context than their interpretation of                                                                              f1
literal utterances.                                                                                                                          f2
                                                                                    0.4
                                                                                                                                             f3
                    Model Evaluation
We used the feature priors obtained in Experiment 1b to com-
pute model interpretations of the 32 metaphors. As discussed                              0.4        0.5           0.6         0.7
                                                                                                           Model
in the previous section, the behavioral results in Experiment
2 show evidence that the context set up by a question changes
participants’ interpretation of a metaphor. Our model nat-                 Figure 2: Model predictions (x axis) vs participants’ probabil-
urally accounts for this using the speaker’s prior over com-               ity ratings (y axis) for 192 items (32 metaphors × 3 features
municative goals P(g). When a speaker is responding to a                   × 2 goal conditions). Shape of points indicates goal condition
vague question, we set the prior distribution for P(g) as uni-             and color indicates feature number.
form. When the speaker is responding to a question specifi-
cally about f1 , we assume that P(g1 ) > 0.5 and equal between
P(g2 ) = P(g3 ). Fitting the goal prior parameter to data yields           the behavioral data. In particular, our model does especially
a prior of P(g1 ) = 0.6 when responding to a specific ques-                well at predicting participants’ judgments of f1 , which are
tion about f1 . We fit the category prior P(ca ) = 0.01 and the            the most salient features of the animal categories and were
speaker optimality parameter λ = 3.                                        targeted by specific questions in Experiment 2. Correlation
   Using these parameters, we obtained interpretation prob-                between model predictions and human judgments for f1 is 0.7
abilities for each of the 32 metaphors under both vague                    (p < 0.0001), while the predicted reliability of participants’
and specific goal conditions. For each metaphor and goal                   ratings for f1 is 0.82 (95% CI = [0.818, 0.823]).
condition, the model produces a joint posterior distribution                  We now compare our model’s performance to a baseline
P(c, ~f |u). We first show a basic but important qualitative re-           model that also considers the feature priors and the conver-
sult, which is that the model is able to interpret utterances              sational context. We constructed a linear regression model
metaphorically. Marginalized over values of ~f , the proba-                that takes the marginal feature priors for the animal category,
bility of the person category given the utterance is close to              the marginal feature priors for the person category, and the
one (P(c p |u) = 0.994), indicating that the pragmatic listener            vague or specific goal as predictors of participants’ ratings.
successfully infers that the person described as an animal is              With four parameters, this model produced a fit of r = 0.45,
actually a person and not an animal. This shows that the                   which is significantly worse than our model (p < 0.0001 on
model is able to combine prior knowledge and reason about                  a Cox test). This suggests that our computational model ad-
the speaker’s communicative goal to arrive at nonliteral inter-            equately combines people’s prior knowledge as well as prin-
pretations of utterances.                                                  ciples of pragmatics to produce metaphorical interpretations
   We now turn to the second component of the interpretation,              that closely fit behavioral data.
P(~f |u). To quantitatively evaluate the model’s performance,                 While our model predictions provide a close fit to behav-
we correlated model predictions with human interpretations                 ioral data, some residual variance can be further addressed.
of the metaphorical utterances. Given a metaphorical utter-                Previous work has shown that alternative utterances—what
ance and a vague or specific goal condition, we computed the               the speaker could have said—can strongly affect listeners’ in-
model’s marginal posterior probabilities for f1 , f2 , and f3 . We         terpretation of what the speaker did say (Bergen et al., 2012).
then correlate these posterior probabilities with participants’            Our model currently does not take into account the range of
probability ratings from Experiment 2. Figure 2 plots model                alternative utterances (both literal and metaphorical) that a
interpretations for all metaphors, features, and goal condi-               listener considers when interpreting a speaker’s utterance. We
tions against human judgments. Correlation across the 192                  posit that this may account for some of the variance in the data
items (32 metaphors × 3 features × 2 goal conditions) is 0.6               that our model does not capture. Consider the metaphor “He
(p < 0.001). The predicted reliability of participants’ ratings            is an ant” and the corresponding features small, strong, and
using the Spearman-Brown prediction formula is 0.828 (95%                  busy. Our model currently assigns a high probability to the
CI = [0.827, 0.829]), suggesting first that people do not agree            feature strong given the metaphor, while participants assign
perfectly on metaphorical interpretations, and second that our             it a lower probability. Indeed, this data point has the high-
model captures a significant amount of the reliable variance in            est residual in our model fit. To demonstrate that alternative
                                                                     723

utterances may account for this discrepancy, we construct a             generally, and we hope that it will continue to shed metaphor-
model that has “He is an ox” as an alternative utterance. “Ox”          ical light on related questions.
has features that roughly align with the features of “ant”:
strong, big, and slow. Since strong is a higher probability                                   Acknowledgments
feature for “ox” than for “ant,” the listener reasons that if the       This work was supported in part by NSF Graduate Research
speaker had intended to communicate the feature strong, she             Fellowships to JTK and LB and by a John S. McDonnell
would have said “He is an ox” since it optimally satisfies that         Foundation Scholar Award and grants from the ONR to NDG.
goal. Since the speaker did not produce the utterance “He is
an ox,” the listener infers that strong is a less probable feature.                                References
                                                                        Bergen, L., Goodman, N. D., & Levy, R. (2012). Thats what she
Adding this alternative utterance to the model indeed lowers               (could have) said: How alternative utterances affect language use.
the marginal posterior probability of strong given the utter-              In Proceedings of the thirty-fourth annual conference of the cog-
ance “He is an ant.” As a result, we posit that adding alterna-            nitive science society.
                                                                        Blasko, D. G., & Connine, C. M. (1993). Effects of familiarity and
tive utterances across all animal categories may significantly             aptness on metaphor processing. Journal of experimental psy-
improve model performance. Constructing a complete set of                  chology: Learning, memory, and cognition, 19(2), 295.
alternative utterances using our current set of metaphors is not        Bowdle, B. F., & Gentner, D. (2005). The career of metaphor. Psy-
                                                                           chological review, 112(1), 193.
possible because feature combinations are not aligned across            Clark, H. H. (1996). Using language (Vol. 4). Cambridge University
animal categories (i.e., different animal categories have dif-             Press Cambridge.
ferent feature sets, and not all features are shared by multiple        Frank, M. C., & Goodman, N. D. (2012). Predicting pragmatic
                                                                           reasoning in language games. Science, 336(6084), 998–998.
animals). We aim to address the role of alternative utterances          Gentner, D., & Wolff, P. (1997). Alignment in the processing of
more specifically in future work.                                          metaphor. Journal of memory and language, 37(3), 331–355.
                                                                        Gibbs, R. (2002). A new look at literal meaning in understanding
                                                                           what is said and implicated. Journal of Pragmatics, 34(4), 457–
                          Discussion                                       486.
                                                                        Giora, R. (1997). Understanding figurative and literal language: The
We have presented a computational model that predicts rich                 graded salience hypothesis. Cognitive Linguistics, 8, 183–206.
metaphorical interpretations using general communicative                Glucksberg, S. (2001). Understanding figurative language: From
                                                                           metaphors to idioms. Oxford University Press.
principles. Besides going beyond the literal meaning of an              Glucksberg, S., & Keysar, B. (1993). How metaphors work.
utterance to infer non-literal interpretations (e.g., John is a         Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge and im-
person and not a shark), our model provides quantitative judg-             plicature: Modeling language understanding as social cognition.
                                                                           Topics in cognitive science, 5(1), 173–184.
ments about the person’s features (e.g., John is very likely            Jäger, G., & Ebert, C. (2009). Pragmatic rationalizability. In Pro-
scary, dangerous, and mean). Furthermore, behavioral results               ceedings of sinn und bedeutung (Vol. 13, pp. 1–15).
show that the interpretation of a metaphor is shaped in part            Kintsch, W., & Bowles, A. R. (2002). Metaphor comprehension:
                                                                           What makes a metaphor difficult to understand? Metaphor and
by the conversational context, which our model naturally ac-               symbol, 17(4), 249–262.
counts for using communicative goals. Together these results            Lakoff, G., et al. (1993). The contemporary theory of metaphor.
suggest that basic principles of communication may be an im-               Metaphor and thought, 2, 202–251.
                                                                        Lakoff, G., & Turner, M. (2009). More than cool reason: A field
portant driver of metaphor understanding.                                  guide to poetic metaphor. University of Chicago Press.
   Our model captures several intuitions about communica-               Miller, G. A. (1995). Wordnet: A lexical database for english.
tion, including the importance of common ground between                    COMMUNICATIONS OF THE ACM, 38, 39–41.
                                                                        Ortony, A. (1993). Metaphor and thought. Cambridge University
listener and speaker, the context-dependence of communica-                 Press.
tive goals, and the idea that speakers choose to produce utter-         Pilkington, A. (2000). Poetic effects: A revelance theory perspective
ances that maximize informativeness about features relevant                (Vol. 75). John Benjamins.
                                                                        Roberts, R. M., & Kreuz, R. J. (1994). Why do people use figurative
to their communicative goal. Each of these components in-                  language? Psychological Science, 5(3), 159–163.
spire research questions that can be further investigated using         Sperber, D., & Wilson, D. (1985). Loose talk. In Proceedings of the
both our modeling framework and experimental paradigm.                     aristotelian society (Vol. 86, pp. 153–171).
                                                                        Sperber, D., & Wilson, D. (2008). A deflationary account of
For example, are listeners less likely to interpret an utter-              metaphors. The Cambridge handbook of metaphor and thought,
ance metaphorically when there is little common ground be-                 84–105.
tween speaker and listener? What additional communicative               Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
                                                                           introduction (Vol. 1) (No. 1). Cambridge Univ Press.
goals are metaphors able to satisfy more effectively than lit-          Tendahl, M., & Gibbs, R. W. (2008). Complementary perspectives
eral utterances? We aim to address these questions in future               on metaphor: Cognitive linguistics and relevance theory. Journal
research to further clarify how communication principles in-               of Pragmatics, 40(11), 1823–1864.
                                                                        Thibodeau, P. H., & Boroditsky, L. (2011). Metaphors we think
teract to produce metaphorical meaning. In addition, previ-                with: The role of metaphor in reasoning. PLoS One, 6(2),
ous work has shown that conventional metaphors such as “He                 e16782.
is a pig” may be processed differently from novel metaphors             Tourangeau, R., & Sternberg, R. J. (1981). Aptness in metaphor.
                                                                           Cognitive psychology, 13(1), 27–55.
(Bowdle & Gentner, 2005), which introduces a set of interest-           Wilson, D., & Carston, R. (2006). Metaphor, relevance and the
ing questions to investigate with our model. We believe that               emergent propertyissue. Mind & Language, 21(3), 404–433.
our computational framework advances understanding of the               Wilson, D., & Sperber, D. (2002). Relevance theory. Handbook of
                                                                           pragmatics.
computational basis of metaphor and of communication more
                                                                    724

