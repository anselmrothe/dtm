UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Does the Emotional Modulation of Visual Experience Entail the Cognitive Penetrability or
Emotional Penetrability of Early Vision?

Permalink
https://escholarship.org/uc/item/0j97t3dt

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Author
Raftopoulos, Athanassios

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Does the Emotional Modulation of Visual Experience Entail the Cognitive
Penetrability or Emotional Penetrability of Early Vision?
Athanassios Raftopoulos (raftop@ucy.ac.cy)
Department of Psychology
University of Cyprus
P.O. Box 20537
Nicosia 1678, Cyprus

Abstract
I argue that emotive states affect perceptual processing either
directly or indirectly with latencies that fall within late vision and
not early vision. These effects differ from the effects of, and are
subserved by different neuronal mechanisms than those that
subserve, attentional effects on perception, although the two sorts
of effects may interact. It follows that the emotive effects found
in perception do not entail either the cognitive penetrability of
early vision or its emotional penetrability.

Introduction
Discussions on the cognitive impenetrability (CI) of
perception almost exclusively concern attentional modulation
of perceptual processing. Pylyshyn (1999) argues that
attention does not modulate a stage of visual processing,
namely, early vision, which is CI. Late vision, in
contradistinction, is cognitively penetrated (CP). Raftopoulos
(2009) reaches the same conclusion based on neuroscientific
evidence on the timing of attention. However, even if attention
does not signify the CP of early vision, other influences might
modulate early visual processing. Since emotional states affect
perception, is early vision penetrated by emotional states?
Siegel (2006) and Stokes (2012) have argued that affective
states affect the phenomenology of perception. A second
interesting question is whether emotional effects are
independent of attention, or whether they should be
interpreted as attentional phenomena in which emotional
stimuli are more attended. (Brown et al. 2010)
Philosophers usually discuss about the CP of perception as
if perception were a unified stage. If they find reason to
believe that some perceptual states are CP, they conclude that
perception is CP. However, perception is not a homogeneous,
undifferentiated process. It consists of two main stages,
namely early vision and late vision, of which only the former
may be CI. Siegel (2006, 501) acknowledges that it is likely
that visual perception has an informationally encapsulated part
that is CI and another part that is influenced by cognitive
processing. These two visual parts represent different
properties of the environment. For example, object
membership to some category, which according to Siegel is
part of the content of perception, may be represented in the CP

stage of perception. Since perception has two stages, any
adequate discussion about CP should make clear to which
stage the claim concerning CP purports to apply, and,
similarly, any claim concerning the penetration of perception
by emotional influences should make clear to which stage of
visual processing it purports to apply.
Affective states modulate perceptual processing and affect
the allocation of processing resources to incoming sensory
stimuli. In this sense, they function as attention does and for
this reason, the difference between attentional and emotional
mechanisms notwithstanding, many researchers talk of
‘emotional attention’. (Vuilleumier 2005) Emotional states
can affect perceptual processing both directly or indirectly.
The indirect effects occur when signals form brain areas
like the OFC (orbitofrontal cortex), and the amygdala that
process the emotional aspects of stimuli are transmitted to
parietal and frontal areas and affect the semantic processing of
the stimulus that takes place there, as when the valence of a
stimulus speeds up or inhibits object recognition. In this case,
the emotional processes co-determine the allocation of
cognitively driven attention and, thus, affect indirectly
perceptual processing through attentional effects; emotional
effects modulate attention and, thereby, perception. (Phelps
2006) Since, I assume that the earliest effects of cognitively
driven attention modulate perceptual processing at 150 ms
after stimulus onset the earliest, and that early vision lasts for
up to 80-120 ms, I take it that the indirect emotional
influences on perceptual processing through attentional
allocation do not threaten the CI of early vision.
There are also direct emotional effects on perceptual
processing through top-down transmission of signals from
either the OFC or the amygdala to the visual processing areas,
signals that are distinct from those generated in parietal and
frontal areas. Should this be the case, early vision despite its
CI, would be emotionally penetrated (EP). There is, for
example, evidence that irrespective of whether or not a face
cue directs covert attention, the fear face cue enhances
contrast sensitivity. (Phelps and LeDoux 2005) There is also
evidence for modulation of the P1 waveform at 120 ms after
stimulus onset by emotional stimuli, a latency that precludes
this modulation being the result of top-down cognitive signals

1216

but can be accounted for by signals from the amygdala
affecting directly visual processing.
In this paper, I argue that emotive states do not affect
directly early vision but only late vision. In the first section, I
discuss early and late vision, as well as CP and EP. In the
second section, I discuss the timing of emotional effects to
determine whether they affect perceptual processing and, to
the extent that the answer is affirmative, which stage of
perceptual processing. Given that early vision lasts for 120ms,
to affect early vision affective affects must be registered
within 120 ms. If they occur later than that and while
perceptual processing still lasts, they affect late vision. I
conclude that direct emotional effects are found in late vision
but not in early vision. Thus, early vision is not EP.

2. Visual Stages, CP, and EP
I assume that perception consists of two stages; early vision
and late vision. The former is CI, while the latter is CP as far
as cognitive effects that are mediated by cognitively driven
attention are concerned. Thus, I assume that cognitively driven
attention does not affect directly early vision.
Early vision includes both a feed forward sweep (FFS) of
signal transmission in which signals are transmitted bottom-up
and which lasts, in visual areas for about 100 ms, and a stage
at which lateral and recurrent connections between neurons
allow recurrent processing. This sort of recurrent
processingLamme (2003) calls it local recurrent processing
(LRP)occurs at 80-120 ms, is restricted within visual areas,
and does not involve signals from cognitive centers. The
unconscious FFS extracts high-level information that could
lead to categorization, determines the classical receptive field
of neurons and their basic tuning properties, and results in
some initial feature detection. The representations formed at
this stage are restricted to including information regarding the
transducable features of objects, that is, information about
spatio-temporal properties, surface properties, viewer-centered
shape, color, texture, orientation, motion, and affordances, in
addition to the representations of objects as bounded, solid
entities that persist in space and time. Parts of this stage’s
contents are at the personal level. By being restricted within
the visual system and by not involving signals from the
cognitive areas of the brain, FFS and LRP are CI.
During early vision no cognitively-driven attentional
effects exist. Neurophysiological evidence for this comes from
various findings (discussed in Raftopoulos 2009, ch. 2) that
strongly suggest that the first signs of cognitively driven
attentional effects on visual areas up to V4 occur at about150
ms. Thus, early vision is a pre-attentional stage of visual
processing that is CI in the sense that its formation is not
directly affected by signals from cognitive centers. It is in
defining what ‘directly’ means that considerations about
attention enter the picture and make necessary some
explication of what “pre-attentional’ means.
First, my claim does not entail that there is no selection
during early vision. There are non-attentional selection

mechanisms that filter information before it reaches
awareness. These mechanisms are not considered to be
attentional because they occur very early and do not involve
higher brain areas associated with attentional mechanisms
(prefrontal cortex, parietal cortex, etc.)
Second, ‘pre-attentional’ should be construed in relation to
cognitively driven attention that affects perceptual processing
directly, the claim being that early vision involves processes
that are not affected directly by this sort of attention.
Cognitively driven attention is opposed both to exogenous or
stimulus-driven attention, and to the effects of either spatial or
feature/object cueing before stimulus onset. The latter do not
affect in a top-down manner visual processing but just rig up
the feedforward sweep. This phenomenon is referred to as the
attentional modulation of spontaneous activity. For example,
attending to a location at which the stimulus will appear may
enhance the base-line activation, that is, the spontaneous firing
rates, of the neuronal assemblies tuned to the attended location
in specialized extrastriate areas. The same phenomenon is
found with respect to feature/object-centered attention.
Late vision is affected by cognitive effects and, thus,
involves higher cognitive areas of the brain (memory etc); late
vision involves the global neuronal workspace. (Dehaene et al.
2006) Such effects start at about 150 ms when information
concerning the gist of a visual scene, retrieved on the basis of
low spatial frequency (LSF) information in the parietal cortex
in about 130 ms, reenters the extrastriate cortex and facilitates
the processing of the high spatial frequency information (HSF)
leading to faster scene and object identification. (Kihara and
Takeda 2010; Peyrin et al. 2010)
Let me explain what I mean by CP and CI of early vision.
CP=The CP of early vision is the nomological
possibility that cognitive states can causally affect in a
top-down, direct, on-line way (that is, while the viewer
has in her visual field and attends to the same location
or stimulus, or is prepared to attend to the same stimulus
when it appears) early vision, in a way that changes the
visual contents that are or would be experienced by a
viewer or viewers with similar perceptual systems,
under the same external viewing conditions.
The reference to direct on-line effects purports to insulate a
process that is indirectly affected by cognitive inferences from
being construed as CP. The indirect effects include both
cognitively driven spatial and feature/object based attention,
and the preparedness to attend that covers cases in which the
viewer expects a certain object or feature to appear either at a
certain cued location or somewhere in her visual field. The
former cases are post-early vision effects. The later cases
constitute a rigging-up of the FFS and are not instances of CP,
which is supposed to affect perception on-line. The term of
causality ensures that any relation between contents occurs as
a result of the causal influences of cognitive states on
perceptual states and contents and is not a matter of
coincidence. Finally, the specification of ‘top-down’ ensures
that the operational constraints, which are at play in perception

1217

to solve the various problems of underdetermination both of
the distal objects from the retinal image and of the percept
from the retinal image do not constitute cases of CP because
by being hardwired in the perceptual system, they cannot be
cognitive effects. (Raftopoulos 2009)
A similar definition applies to EP.
EP=The EP of early vision is the nomological
possibility that affective states can causally affect in a
top-down, on-line way (that is, while the viewer has in
her visual field the same stimulus or is prepared for the
appearance of the same stimulus) early vision, in a way
that changes the visual contents that are or would be
experienced by a viewer or viewers with similar
perceptual systems, under the same external viewing
conditions.
Note that there are some differences from the definition of CP
owing to the fact that, as we shall see, it is likely that
emotional stimuli can be processed and experienced even
when they are outside the focus of attention. As in the case of
CP, the preparedness purports to cover cases in which a cue
regarding the valence of an upcoming stimulus may influence
the base-line activation of the neurons encoding the stimulus.

2. Timing Emotional Effects
When the brain receives information, it generates a hypothesis
based on the input and what it knows from the past to guide
recognition and action. In addition to what it knows, it uses
affective representations, that is, prior experiences of how the
input had influenced internal bodily sensations. In determining
the meaning of the incoming stimulus, the brain employs
representations of the affective impact of the stimulus to form
affective predictions. These predictions are made within ms
and do not occur as a separate step after the object is
identified; rather they assist in object identification. There is
substantial evidence that the OFC, which is the centerpiece of
the neuronal workspace that realizes affective responses, plays
an important role in forming the predictions that support
object recognition. (Barr 2009)
Activation of the OFC owing to bottom-up signals is
observed between 80-130 ms. (Bar 2009) This activity is
driven by LSF information and, hence, magnocellular visual
input. A second wave of activity in the OFC is registered at
200 to 450 ms, probably reflecting the refinement and
elaboration of the initial hypothesis. There is evidence that the
brain uses LSF information to make an initial prediction about
the gist of a visual scene or object, that is, to form a
hypothesis regarding the class to which the scene/object
belongs. This hypothesis is tested and details are filled using
HSF information in the visual brain and information from
visual working memory. (Johnson and Olshausen 2005;
Kihara and Takeda 2010; Peyrin et al. 2010)
Barrett and Bar (2009) argue that the medial OFC directs
the body to prepare a physical response to the input, while the
lateral parts of OFC are integrating the sensory feedback from
the bodily states with sensory cues. The medial OFC has

reciprocal connections to the lateral parietal areas in the dorsal
system, where it receives LSF information transmitted through
magnocellular pathways. Using LSF information, the medial
OFC extracts the affective context in which the object has
been experienced in the past and this information is relayed to
the dorsal system where it contributes to the determination of
the sketchy gist of the scene or object. The lateral OFC, in its
turn, has reciprocal connections with inferior temporal areas
of the ventral stream, whence it receives HSF information
through parvocellular pathways (the pathways that carry
detailed information about a visual scene in the ventral
system). Its role is to integrate sensory with affective
information to create a specific representation of the scene or
object, which eventually leads to conscious experience. Note
that owing to the time delay of the information transmitted
through parvocellular pathways compared to the information
transmitted through magnocellular pathways, information
arrives faster at the medial OFC than at the lateral OFC.
(Ashley et al. 2003)
Emotional stimuli, owing to their intrinsic significance,
have a competitive advantage relative to neutral stimuli and
are more likely to win the biased competition among stimuli
for further processing. However, affecting the biased
competition among stimuli is what attentional effects do too
and, so, the question arises as to the relation between
emotional and attentional influences on visual processing.
Evidence shows that both attention to non-emotional stimuli
and emotional stimuli per se can boost neural responses
(Vuilleumier et al. 2004; Shupp et al. 2003). This suggests that
the net result of both attentional and motivational modulation
of the visual cortex is very similar. Since emotional effects,
like attentional effects, enhance perceptual processing, they
are sometimes referred to as ‘emotional attention’.
(Vuilleumier 2005) However, the neuronal pathways
responsible for attentional and emotional effects are likely
different, since, among other things, differences in size and
duration of the time courses of semantic and emotional
processing and their influences on the visual cortex have been
observed. (Attar et al. 2010; Vuilleumier 2005; Vuilleumier
and Driver 2007) Another reason for being skeptical of the
view that the same mechanism underlies attentional and
emotional effects is that there is mixed evidence concerning
whether unattended emotional stimuli (fearful faces) are
processed. Williams et al. 2005 argue that although
differential amygdala responses to fearful versus happy facial
expressions are tuned by mechanisms of attention, the
amygdala gives preference to potentially threatening stimuli
under conditions of inattention. Moreover, the influence of
selective attention on amygdala activity depends on the
valence of the facial expression. Bishop et al. 2007, on the
other hand, argue that affective modulation of the BOLD
signals occurs only when the task demands low attention.
Studies in humans (Olofsson et al. 2008; Vuilleumier and
Driver 2007) show that emotional vs. neutral faces processing
produces a higher amplitude of VEP (visual evoked potentials)

1218

and an enhancement of the P1 ERP component at about 120
ms. P1 originates in extrastriate areas and is considered to be
the hallmark of the effects of exogenous spatial attention on
visual processing, that is, the effects of the automatic orienting
response to a location where sudden stimulation has occurred.
This entails that the emotion-related modulation of the visual
cortex arises prior to the processing stages associated with
fine-grained face perception indexed by the N170 component
for face recognition. This reinforces the view that emotional
affects are prior to, and help in determining, the categorization
of the stimuli, and that they can collaborate with attentional
effects by enhancing the processing of spatially relevant and
emotionally significant stimuli. The early latency precludes
this modulation being the result of top-down cognitive signals.
Neither can the modulation be accounted for by signals from
the amygdala because the amygdala in humans processes the
emotional content of facial expressions at 140-170 ms after
stimulus onset (Conty et al. 2012), or at 200 ms (Pessoa &
Adolphs 2010). Despite its early latency, the P1’s modulation
by emotion occurs when early vision is almost over (120 ms).
The N170 is also modulated by emotional content and this
modulation occurs at about the same time that amygdala start
processing the emotional content of face expressions. (Conty
et al. 2012) EEG studies that manipulate attentional and
emotional facial expressions orthogonally (Holmes et al.
2003) show that emotional effects start modulating face
processing at the fusiform gyrus closely following the N170
face specific component. Thus, the emotional modulation of
the extrastriate cortex takes place prior to task-related
attentional selection and prior to the full processing of faces in
the cortex. This is also an indication that emotional effects
enhance or inhibit the processes that lead to object
recognition.
ERP results on affective processing show also an early
posterior negativity (EPN) at about 200-300 ms for arousing
vs. neutral pictures, which involves both fronto-central and
temporo-occipital sites and which is thought to index
‘motivated attention’. The motivated attention selects
affectively arousing stimuli for further processing on the basis
of perceptual features. Furthermore, other findings show that
the affective amplitude modulation persists for a prolonged
period of time, which entails that emotionally arousing stimuli
receive enhanced encoding even when they had to be ignored
by being task irrelevant. (Olofsson et al. 2008) Around the
same time (200-300 ms), stimulus valence has been shown to
elicit a decreased N2 negativity (unpleasant compared to
pleasant stimuli). Since at 200-300 ms latencies stimulus
discrimination and response selection are thought to occur,
affective visual stimuli may influence neural activation before
response stages. (Carretie et al. 2004)
The negativity biases of ERP waveforms at these latencies
may reflect rapid activity by amygdala processing of aversive
information and the transmission of this information to frontoparietal areas where it modulates the allocation of attention so
that unpleasant stimuli may receive priority processing. Or,

they may reflect the functioning of an early selective attention
mechanism that does not depend on valence categorization but
on motivational relevance and which facilitates processing of
stimuli with high motivational relevance. (Shupp et al. 2004)
Emotional effects are found at long latencies as well
(>300ms), probably reflecting the impact of emotional signals
to the processing of sensory information in fronto-parietal
areas. Both P3 and the following positive slow wave relate to
the elevated ERP positivity caused by the emotional
modulation of P3 and of the slow wave, and by the valence
value and arousal level of the stimulus (valence influences
P3b but not P3a, while arousal influences both).
In general, valence effects are found predominantly for
early and middle-range ERP components, probably reflecting
the role of emotional intrinsic value of the stimulus for
stimulus selection. Arousal effects, that is, a positive shift in
the ERP waveforms, are found for middle-range and late
components and constitute the primary affective influence at
these latencies (Olofsson et al. 2008). They probably reflect
the allocation of processing resources to the selected stimuli.
The discrepancies found in studies comparing emotional
with attentional effects are probably caused by the fact that in
the various experiments there were different manipulations of
the kind of attention involved (spatial vs. object-based
attention). In ERP studies when non-spatial attentional
manipulations were applied (pictures of fearful faces and
houses were superimposed so that spatial attention could be
controlled and object-based attention could be manipulated) a
sustained positivity in response to fearful faces emerged at
about 160 ms in the fusiform gyrus, which was not affected by
attentional manipulations. (Santos et al. 2008) Similar results
suggesting that emotion-related modulation occurs even when
emotional stimuli were not task relevant have been found with
SSVEP studies. (Muller et al. 2008)
To disentangle this issue, Attar et al. (2010) examined not
the time course of emotional processing of stimuli per se but
its effects on attentional resource allocation in a primary task
with respect to which the emotional stimuli functioned as
distractors. Their findings suggest that highly arousing
emotional pictures consume much more processing resources
than neutral pictures over a prolonged period of time, which
means that emotional distractors receive prioritized processing
despite severe resource limitations. This effect, however, is of
relatively small size when compared to the effects of general
picture processing on task-related activity, where irrelevant
whole pictures without any emotional value that act as
distractors have a detrimental effect on task related activity.
More importantly for this paper, however, Attar et al (2010)
found, at the behavioral level, significant decreases in target
detection rates when emotional compared to neutral pictures
were concurrently presented in the background. At the
neuronal level, the effect was accompanied by a stronger
decrease of SSVEP amplitudes directed to a primary task for
emotional relative to neutral pictures. The earliest onset for the
affective deflective amplitude was at 270 ms. According to

1219

our knowledge about the neural sites at which SSVEP signals
are generated, the deflection observed stems from sources in
early visual areas. (Andersen et al. 2012) Attar’s et al (2010)
work also shows that the presence of a challenging primary
task that limits the availability of processing resources does
not eliminate the observed emotion-induced reduction of
SSVEP amplitudes, which suggests that the effects of
emotional distractors are not contingent on top-down
attentional control. Note that the SSVEP findings accord well
to the findings on the timing of the emotional effects on visual
processing found in the various ERP studies discussed above.
The discussion thus far shows both attentional and
emotional effects on visual processing from brain areas other
than the visual cortex. However, the brain regions and neural
pathways involved in emotional and attentional influences
seem to be different. For example, amygdala is involved in
emotional modulation of perceptual processing, whereas the
FEF and other parietal regions are involved in the modulation
of perceptual processing by spatial attention. Amygdala is
well poised to modulate perception because it receives sensory
inputs from all modalities and sends signals to many cortical
and subcortical regions that can potentially influence
perception. Amygdala is sensitive both to coarse LSF
information that travels fast in the brain and to slow HSF
information. This way an initial appraisal of emotional
significance based on a limited amount of information may
proceed quicker than the elaborate and time consuming
processing associated with conscious awareness of a stimulus.
This may explain why ERPs to fearful expressions in face
selective neurons in monkeys are registered very early (50-100
ms after the initial selective activity), while the fine encoding
of faces that relies on the slower traveling HSF information
starts at 170ms as indexed by the specifically related to faceprocessing N170.
Concerning the relation between affective and attentional
effects, one can make the following general remarks. The
amygdala responds to fearful expressions independent of
attentional modulation. The amygdala can reinforce the
representation of fearful faces in fusiform cortex, an influence
that is disrupted when the amygdala is damaged. (Vuileumier
et al. 2004) Recordings of face-selective neurons in monkeys
(Sugase et al. 1999) suggest that the amygdala modulates
perceptual processing 50-100 ms after the initial face-selective
activity. Since the monkey amygdala neurons respond to
threatening face expressions between 120-250 ms (Pessoa &
Adolphs 2010), the earliest modulation of face selective
neurons by amygdala signals starts at about 170 ms, in
accordance with Holmes et al (2008) findings. The amygdala
activity probably reflects coarse-grained global processing of
the input, while the affective modulation of face processing
reflects affective information contributing to a more finegrained representation of faces at later latencies with a delay
of 50 ms compared to global processing.
Emotional enhancement of the responses of neurons in
visual areas of the brain can operate even when attention is

impaired owing to parietal damage in spatial neglect. fMRI
studies with patients show enhanced fusiform activity for
fearful faces compared to neutral faces even when the faces
were neglected. (Vuilleumier et al. 2004) fMRI studies show
that amygdala feedback to the fusiform area influences visual
cortex additively to the modulation of the same area by
attention (in this case attention and emotion cooperate). Thus,
even though emotional states produce activations analogous to
those of attention, the fact that they enhance the representation
of emotionally task-irrelevant stimuli means that these effects
are probably realized by different neural pathways.
Emotional and attentional effects can also compete.
Emotional modulation of distractors enhances the responses of
the neurons encoding them. This increases the competition
with the targets by reducing the responses of the neurons
encoding them. Emotional signals, however, may be
suppressed by high perceptual competition where spatial
attention filters out very early most of the distractors. (Lavie
2005) Finally, amygdala’s influence can persist in conditions
where cortical responses are reduced, contributing, thus, to the
amplification of cortical processing when sensory inputs are
insufficient. (Vuilleumier 2005)
Emotional effects act separately from attentional effects
and provide an additional bias to the processes of sensory
representations that lead to the selection of some among the
items in the input, either adding to or competing with
attention. The competition that emotional effects pose to
attention is advantageous for an organism since unexpected
events that have a particular emotional value can be detected,
and influence behavioral responses, independently of the
organism’s current attentional loads.

Conclusion
Between 80 and 170 ms after stimulus onset an emotional
effect is found in emotion sensitive areas like the OFC and the
amygdala owing to bottom-up sensory signals. At 120 ms
emotional influences start modulating perceptual processing in
extrastriate cortex and at about 170 ms the processing of face
selective neurons is affected by emotional signals. At 270 ms
SSVEP signals are registered in early visual areas, driven by
top-down emotional signals. In this latency, an EPN and
perhaps a N2 effect due to affective modulation is found.
Information concerning the emotional significance of visual
stimuli reenters visual areas at about 120 ms the earliest, and
continues for up to 1000 ms. Thus, the earliest affective
influences reach visual areas at such latencies that fall outside
the duration of early vision; they affect only late vision.

References
Andersen, S., Muller, M., & Hillyard, S. (2012). Tracking the
allocation of attention in visual scenes with SSEVP. In M.
I. Posner (ed.), Cognitive Neuroscience of Attention. New
York, N.Y: Guilford Press.
Ashley, V., Vuilleumier, P., & Swick, D. (2003). Effects of
orbitofrontal lesions on the recognition of emotional faces

1220

expressions. Paper presented at the Cognitive
Neuroscience Society Meeting.
Attar, C. H., Andersen, S., & Muller, M. M. (2010). Time
course of affective bias in visual attention: convergent
evidence from steady-state visual evoked potentials and
behavioral data. NeuroImage, 53, 1326-1333.
Barr, M. (2009). The proactive brain: memory for predictions.
Philosophical Transactions of the Royal Society, Biology,
364, 1235-1243.
Barrett, L. F., & Bar, M. (2009). See it with feeling: affective
predictions during object perception. Philosophical
Transactions of the Royal Society,, 364, 1325-1334.
Bishop, S.J., Jenkins, R., Lawrence, A. D. (2007). Neural
processing of fearful faces: effects of anxiety are gated by
perceptual capacity limitations. Cerebral Cortex, 17, 15951603.
Brown, Ch., El-Deredy, W., & Blanhette, I. (2010).
Attentional modulation of visual-evoked potentials by
threat: investigating the effect of evolutionary relevance.
Brain and Cognition, 74, 281-287.
Carretie, L., Merecado, F., Hinosoja, J. A., Loeches, M., &
Sotillo, M. (2004). Valence-related vigilance biases in
anxiety studied through event-related potentials. Journal of
Affective Disorders, 78, 119-130.
Conty, L., Dezecache, G., Hugueville, L., & Grezes, J. (2012).
Early binding of gaze, gesture, and emotion: neural time
course and correlates. Neuroimage, 28, 4531-4539.
Dehaene, S., Changeux, J-P., Naccache, L. Sackur, J., &
Sergent, C. (2006). Conscious, preconscious, and
subliminal processing: a testable taxonomy. Trends in
Cognitive Science, 10(5), 204-211.
Holmes, A., Vuilleumier, P., & Eimer, M. (2003). The
processing of emotional facial expression is gated by
spatial attention: evidence from event-related brain
potentials. Brain Research, 16, 174-184.
Johnson, J. S. & Olshausen, B. A. (2005) The earliest EEG
signatures of object recognition in a cued-target task are
postesensory. Journal of Vision, 5, 299-312.
Kihara, K., & Takeda, Y. (2010). Time course of the
integration of spatial frequency-based information in
natural scenes. Vision Research, 50, 2158-2162.
Lamme, V. A. F. (2003). Why visual attention and awareness
are different. Trends in Cognitive Sciences, 7 (1), 12-18.
Lavie, N. (2005). Distracted and confused? selective attention
under load. Trends in Cognitive Science, 9, 75-82.
Muller, M. M., Andersen, S. K., & Keil, A. (2008). Time
course of competition for visual processing resources
between emotional pictures and foreground task. Cerebral
Cortex, 18, 1892-1899.
Olofsson, J. K., Nordin, S., Sequeira, H., & Polich, J. (2008).
Affective picture processing: an integrative review of ERP
findings. Biological Psychology, 77, 247-265.

biological significance. Nature Reviews Neuroscience, 11,
773-783
Peyrin, C., Michel. C. M., Schwartz, S., Thut, G., Seghier, M.,
Landis, Th., Marendaz, Ch., &Vuilleumier, P. (2010). The
neural processes and timing of top-down processes during
coarse-to-fine categorization of visual scenes: a combined
fMRI and ERP study. Journal of Cognitive Neuroscience,
22(12), 2678-2780.
Phelps, E. A. (2006). Emotion and cognition. Annual Review
of Psychology, 57, 27-73.
Phelps, E. A., & LeDoux, J. E. (2005). Contributions of the
amygdala to emotion processing: from animal models to
human behavior. Neuron 48, 175-187.
Pylyshyn, Z. (1999). Is vision continuous with cognition?
Behavioral and Brain Sciences, 22, 341-423.
Raftopoulos, A. (2009). Cognition and Perception: How do
Psychology and the Neural Sciences inform Philosophy.
Cambridge, MA: The MIT Press.
Raftopoulos, A. (forthcoming). The cognitive impenetrability
of perception and theory-ladenness. Journal of General
Philosophy of Science.
Santos, I. M., Iglesias, J., Olivares, E. I., Young, A. W.
(2008). Differential effects of object-based attention on
evoked potentials to fearful and disgusted faces.
Neurophysiologia, 46, 1468-1479.
Shupp, H. T., Junghoffer, M., Weike, A. J., & Hamm, A. O.
(2004). Emotional facilitation of sensory processing in the
visual cortex. Psychophysiology, 41, 441-449.
Siegel, S. (2006). Which properties are represented in
perception? In T. S. Gendler & J. Hawthorne (eds.),
Perceptual Experience. Oxford: Oxford University Press.
Stokes, D. (2012). Perceiving and desiring: a new look at the
cognitive penetrability of experience. Philosophical
Studies, 158 (3), 479-92.
Sugase, Y., Yamane, S., Ueno, S., & Kawano, K. Global and
fine information coded by single neurons in the temporal
visual cortex. Nature, 400(6747), 869-873.
Vuilleumier, P. (2005). How brains beware: neural
mechanisms of emotional attention. Trends in Cognitive
Science, 19(12), 585-595.
Vuilleumier, P., Richardson, M., Armony, J., Driver, J., &
Dolan, R. J. (2004). Distant influences of amygdala lesion
on visual cortical activation during emotional face
processing. Nature Neuroscience, 7, 1271-1278.
Vuilleumier, P. & Driver, J. (2007). Modulation of visual
processing by attention and emotion: windows on causal
interactions between human brain regions. Philosophical
Transactions of the Royal Society, Biology, 362, 837-855.
Williams, M. A., McGlone, F., Abbott, D. F., & Mattingley,
J. B. (2005). Differential amygdala responses to happy
and fearful facial expressions depend on selective
attention. Neuroimage, 24, 417-425.

Pessoa, L. & Adolphs, R. (2010). Emotion processing and the
amygdala: from a 'low road' to 'many roads' of evaluating

1221

