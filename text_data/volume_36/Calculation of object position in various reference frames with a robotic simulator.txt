UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Calculation of object position in various reference frames with a robotic simulator
Permalink
https://escholarship.org/uc/item/3db638mx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Svec, Marcel
Farkas, Igor
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                      University of California

Calculation of object position in various reference frames with a robotic simulator
                   Marcel Švec (svec.marcel@gmail.com) and Igor Farkaš (farkas@fmph.uniba.sk)
                                      Department of Applied Informatics, Comenius University
                                               84248 Mlynská dolina, Bratislava, Slovakia
                              Abstract                                puts for training the model. In this paper, we take advan-
                                                                      tage of the simulated iCub robotic environment (Tikhanoff
   The brain encodes the space in various reference frames. The
   key role in spatial transformations is played by the posterior     et al., 2008) that naturally provides embodied data of higher
   parietal cortex where neurons combine retinal location of vi-      complexity for learning the task. The classical view, com-
   sual stimulus with gaze direction to encode spatial informa-       ing from geometry, applied commonly in robotics, assumes
   tion. This nonlinear dependence of neuronal responses, gain
   modulation, is considered a fundamental computational prin-        that coordinate transformations are computed explicitly and
   ciple used in the brain. The important insight can be obtained     applied sequentially. On the contrary, in a novel view, being
   through computational models, typically artificial neural net-     more consistent with neuroscientific data, coordinate trans-
   works. In this paper, we test the Zipser–Andersen model but
   use more realistic and variable stimuli, employing the sim-        formations are computed implicitly and in parallel (Blohm
   ulated iCub robot. The multi-layer perceptron was able to          & Crawford, 2009). Hence, we analyze the phenomenon of
   successfully perform coordinate transformation from eye- to        coordinate transformations in the context of the progressive
   body-centered reference frame, using gaze information. Model
   achieves high accuracy of 2 to 4 degrees on testing data, de-      cognitive robotics that offers a promising pathway to build-
   pending on the dataset variability. We provide visualisation       ing autonomous systems. We are not aware of this type of
   techniques for analysing the network, and the effects of gain      work with iCub. For coordinate transformation, we use the
   modulation and shifting receptive fields. Our results confirm
   previous findings that hidden neurons use various intermediate     original Zipser–Andersen model described below, and show
   codings that mediate transformations.                              that the implicit transformation can be learned equally well
                                                                      despite using more complex data. We also introduce visual-
   Keywords: reference frames; coordinate transformation;             ization techniques that reveal model behavior.
neural network; robotic simulator; gain field
                                                                                              Background
                          Introduction                                Andersen and Mountcastle (1983) discovered that neurons in
Determining the object position in space is always related to         area 7a of posterior parietal cortex (PPC) of monkeys com-
some point at known location. This relationship is captured           bine retinal location of visual stimulus with gaze direction to
by the concept of reference frame in which we can define              encode spatial information. The role of PPC as a sensorimo-
a concrete coordinate system (Batista, 2002). Humans are              tor interface for visually guided eye and arm movements has
able to use both egocentric and allocentric reference frames,         been also supported by later findings (Buneo & Andersen,
which can be combined to support behavior according to the            2006; Khan, Pisella, & Blohm, 2012). Cells in PPC appear
task (Burgess, 2006). In general, reference frame may be an-          to nonlinearly combine information from different modal-
chored to practically anything, to our head, hand, or any other       ities, while their sensitivity is modulated by one modality
object. Neuroscientists naturally adopted the concept of refer-       (e.g. gaze direction) without changing their selectivity to the
ence frames to better understand how the space is represented         other modality (visual stimuli). This phenomenon was coined
in the brain. Cells in the visual system respond to the stimuli       as gain modulation and the changes in neuron’s sensitivity as
located only in particular location called the receptive field in     gain fields. The subsequent studies of gain modulation have
cases where the response is considered to be strictly sensory.        revealed that it is an extremely widespread mechanism that
The receptive field (also called response field) can also refer       appears to be a fundamental computational principle behind
to the set of patterns that evoke neuron’s activation.                coordinate transformations (Salinas & Thier, 2000; Salinas &
   Research on reference frames that has been progressing for         Sejnowski, 2001; Blohm & Crawford, 2009).
the past 25 years suggests that reference frames do not always           The types of signals that could produce gain fields in-
exist in an explicit form, but rather as some intermediate rep-       clude gaze direction, head position, eye vergence, target dis-
resentations of space that are further processed for specific         tance, chromatic contrast or attention, all together leading to
purposes, for instance to generate reaching commands. The             the suggestion that gain modulation is a general mechanism
process of converting sensory stimuli into the motor plans            for multimodal integrations that underlie important cogni-
is referred to as sensorimotor transformation. These are of-          tive functions like sensorimotor transformation, object recog-
ten formalised in terms of spatial transformations from eye-          nition, motion processing or focusing attention (Salinas &
centred (retinotopic) or head-centered coordinates into hand-         Thier, 2000). The essential feature of gain fields is nonlin-
centered coordinates.                                                 earity. However, the biophysical basis that allows neurons to
   Coordinate transformations are also a key component for            combine information from two sources such that their output
learning the body schema (Hoffmann et al., 2011). In most             is close to the product of two functions is still unclear.
cases, the authors use artificially generated inputs and out-            In the first computational model, Zipser and Andersen
                                                                  2175

(1988) trained an artificial neural network to compute the                 During random eye positioning, we needed to determine
head-centered position of target from eye-centered visual               iCub’s field of view. Two cameras with a resolution 320×240
stimuli and gaze direction. Their network spontaneously de-             pixels use a simple pinhole projection with the focal lengths
veloped visual receptive fields (RF), gain modulated by eye             equivalent to 257 pixel units (in both directions), which yields
position similar to what had been observed in PPC in area 7a            a field of view of ∼64◦ . We set both eyes to be kept parallel in
(Andersen & Mountcastle, 1983). Their artificial simuli used            the simulator (no convergence). In order to generate the data,
for training the network were simple but nicely illustrated the         we randomly placed an object in front of iCub and randomly
concept of gain fields as analysed in the hidden layer of the           moved its eyes by the same angle, regardless of the object
three-layer feedforward network.                                        position. We always checked that the object remained in the
   Since then, computational models have been proposed to               visual field and did not get under the ground. The simulator
account for coordinate transformations. For instance, Xing              has three predefined object types: box, sphere and cylinder.
and Andersen (2000) analyzed simple neural network mod-                 We generated datasets with these 3 types and also datasets
els of the PPC and concluded that the gain field is an effec-           with only spheres. To make the data diverse enough, we gen-
tive mechanism for performing coordinate transformations.               erated objects with random sizes, within a reasonable scale
Blohm and Crawford (2009) studied coordinate transforma-                range. We repeated the object setting procedure 1500 times to
tions in the context of visually guided reaching in 3D. Their           generate a sufficient number of training and testing patterns.
four-layer neural network was successfully trained to per-              These patterns covered the entire visual space quite densely,
form visuomotor transformation from gaze-centered inputs to             such that they could be assumed to be representative.
a shoulder-centered output used for reaching.
                           The model
The neural network performs transformation from the eye-
centered (retinal) object position to the body-centered object
position (attached to robot’s waist),1 modulated by the infor-
mation about eye positions (which determine gaze direction).
Data generation
To generate the data in iCub simulator, we randomly moved
iCub’s eyes and randomly positioned an object in robot’s vi-
sual space. Then we collected the data as shown in Figure 1.            Figure 2: Learned transformation: from eye-centered ob-
                                                                        ject coordinates, and given eye-positions to object position
                                                                        in body-centered frame of reference. Robot’s head is fixed.
                                                                        Model architecture
                                                                        In order to use the dataset as an input for the neural network,
                                                                        we converted each pattern (pixelwise) into the set of real num-
                                                                        bers in the interval [0, 1]. Camera images from the left and
                                                                        right eyes were flipped in both directions and downscaled to
                                                                        64×48 pixels. For better performance, we also removed the
                                                                        background.2 The processed image is illustrated in Figure 1
                                                                        (white ball on black background). The image input was hence
                                                                        represented by 2×64×48 = 6144 neurons. Eye positions
                                                                        and object position were represented by biologically plausi-
                                                                        ble population coding3 that lends itself to robustness and good
                                                                        generalization (Averbeck, Latham, & Pouget, 2006). Eye po-
                                                                        sitions are represented by eye tilt and eye version. Eye tilt
Figure 1: Generating the dataset in iCub simulator involves:
                                                                        is encoded by 11 neurons with preferred directions uniformly
setting the eye position, positioning the object in the scene,
                                                                        distributed over the interval [−35◦ , 15◦ ] and eye version by
and collecting 3 pieces of data: retinal images (shown left),
eye positions and target object position. We removed the                    2 This diminishes our motivation to use more realistic data, but
background to get a B&W image.                                          what is still preserved is the varying object size and the shading. We
                                                                        assumed that the image segmentation component performed figure–
   1 Our  model differs in this detail from Zipser–Andersen model       ground separation.
                                                                            3 In population coding, a value x is represented as a vector of
that uses head-centered coordinates. Howeveer, both models can be
seen as equivalent, since the taget reference frames only differ in     activations yi of neurons with equidistantly shifted centres xi of their
vertical coordinate.                                                    Gaussian RFs with the same width σ: yi (x) = exp(−(x − xi )2 /2σ2 ).
                                                                    2176

21 neurons distributed over the interval [−50◦ , 50◦ ]. The ob-      faster than QuickProp algorithm (Fahlman, 1988). Adding
ject position (network output) is represented by two slopes          the momentum to standard BP dramatically increased the
(shown in Figure 2), horizontal (19 neurons) and vertical (19        speed of training. Inspired by Qian (1999), we used values
neurons), making it a 2.5D model that calculates the direc-          of the learning rate (α = 1.5) and the momentum (µ = 0.9),
tion to the object from iCub’s chest, rather than the distance.      with which the online BP outperformed RPROP. The addi-
The preferred directions of output neurons are for both coor-        tional disadvantage of RPROP and QuickProp algorithms is
dinates uniformly distributed over the interval [−90◦ , 90◦ ].       that they often generated large weights and were thus not suit-
   We used a multi-layer perceptron with full connectivity           able for visualisation purposes. This is an interesting point,
between layers. The input layer consisted of 6144 + 11 +             because it indicates that the right choice of the training al-
21 = 6176 neurons, the hidden layer had 64 neurons (re-              gorithm may be important for the purposes of studying the
sult of experimentation in the range 50–300; performance             internal structures of the network.
with smaller values was limited) and the output layer con-
tained 38 neurons. It turned out to be useful to also opti-            Table 1: Testing errors as a function of dataset variability.
mize the slope k of the neuron’s activation function f (net) =         Data set                                           Error
1/(1 + exp(−k.net)). For the hidden layer we used kH = 0.05            Boxes, spheres and cylinders at various sizes 4◦ ± 3.5◦
and for the output layer kO = 0.1, found experimentally.               Spheres of various sizes                           3◦ ± 3◦
Input balancing                                                        Spheres of fixed size                              2◦ ± 2◦
Since there were more input units encoding the retinal im-
age than those encoding the eyes position (6144 versus 32),             The training dataset contained 1000 patterns, the testing
we decided to proportionally modify the weights of input–            data set 500 patterns which varied according to the variability.
hidden connections in order to guarantee good functioning of         We summarize the testing errors in Table 1. The best results
the model (in the original Zipser–Andersen model this was            were for these model parameters: the population curves with
not an issue). Here we solved this problem by using the ap-          σT = 5 for eye-tilt neurons, σV = 7 for eye-version neurons,
propriate scaling coefficient. This can be expressed as              and σO = 10 for output neurons coding the object position.
                                                                     We did not find any significant correlation between the error
                            Nret          Npos
                                                                     size for a particular pattern and the position of eyes or the ob-
                net = cret ∑ wi ri + cpos ∑ w je j           (1)
                            i=1           j=1
                                                                     ject. The distribution of testing errors was positively skewed
                                                                     (i.e. towards smaller errors). These results reveal that even in
where net is the input to a hidden neuron, cret and cpos are         this more realistic scenario with the iCub robot the modelled
the coefficients used for balancing retinal inputs ri and eye-       transfomation is accurate and generalizes well. The model
position inputs e j , Nret and Npos are the numbers of units en-     performance on the more complex datasets is comparable to
coding the corresponding modality. We calculated the two             that of monkeys trained on saccades (Robinson, Noto, & Be-
coefficients to correspond to the desired ratio R :E, where R        vans, 2003).
is the desired size of retinal inputs contribution and E is the         In the following we analyze the hidden layer of the trained
desired contribution of eye-position inputs. The equations for       network, focusing on three aspects: receptive fields, gain
calculating both coefficients are                                    modulation and the reference frames. We illustrate the model
                                                                     properties using one hidden neuron (unit 4) and show that all
               R(Nret + Npos )            E(Nret + Npos )            hidden units learn various intermediate reference frames.
        cret =                   , cpos =                 .  (2)
                Nret (R + E)                Npos (R + E)
                                                                     Receptive fields
We chose R:E = 2:1, but there was no significant difference          After the network learned to accurately perform the transfor-
in network performance for slightly different ratios. Even the       mation, we examined the hidden layer for the effect of gain
original network (i.e. without balancing) was able to success-       modulation and shifting RFs. For this purpose, we first vi-
fully perform the transformations, but by setting this ratio we      sualised the RFs of hidden units by plotting their incoming
achieved faster training, better accuracy and weight profiles        weights. We found a wide variety of RFs but these could be
that were nicer for visualisation purposes.                          roughly divided into three groups as shown in Figure 3.
                                                                        In group A, we can distinguish continuous area with pos-
                              Results                                itive weights contrasting with an area of smaller or negative
We tested several versions of backpropagation (BP) algorithm         weights (e.g. neuron 4). Group B has RFs divided into two
(using the FANN simulator; (Nissen, 2005)) and stopped the           parts, usually with stronger weights on the sides. In group C,
training when the mean-squared-error decreased below the             we were not able to find any continuous area and the RF was
value 5×10−4 , found experimentally. In the first trials, we         hard to interpret without further investigation. Quantitatively,
achieved the best training performance with RPROP algo-              in our network with 64 hidden units we found 41 units of type
rithm (Riedmiller & Braun, 1993) which performed approx-             A, 15 of type B and 8 of type C. These numbers are specific
imately 8 times faster than standard BP and about 10 times           for the given network and would be slightly different if we
                                                                 2177

Figure 3: Examples of receptive fields: continuous (left), di-
vided into two parts (middle), unspecified (right).
repeated the training process. Based on the analysis of well-
trained models, we may conclude that the majority of units
have developed continuous RFs for particular area(s) in the
visual space.
Gain modulation
Gain modulation is revealed when a modulatory input
changes the response amplitude of a neuron to the other in-
put, without modifying its selectivity (Salinas & Sejnowski,
2001). To examine this effect, we recorded the responses of
hidden units to the fixed visual stimuli and different eye posi-
tions, which were changed in a systematic way with a 10◦ step
                                                                      Figure 4: Gain modulation of hidden neuron’s response. For
in vertical direction (tilt) and 20◦ step in horizontal direction
                                                                      explanation see the text.
(version). Hence, we considered 25 different eye positions
arranged in a 5×5 grid.
   We repeated this process with 9 different retinal images
that depicted the object at particular locations, arranged ana-
logically in a 3×3 grid, from top left to bottom right region
of the image. Thus, all together there were 9×25 different
configurations of visual stimuli and eye positions that lead
to neuron’s response profile (i.e. a vector of 225 responses).
We investigated the response profiles of the hidden units, and
illustrate here the model behaviour using one example: re-
sponse profile of neuron 4, shown in Figure 4.
   Panels A–I indicate the object locations relative to iCub
gazing straight ahead (A means the object was up left; I
means bottom right). Empty magenta circles show how the
neuron would respond only to the visual stimuli without the           Figure 5: Weights of connections between neuron 4 and out-
influence of eye position. In every panel, filled blue circles        put neurons encoding vertical object position.
represent unit responses to the visual stimuli modulated by
corresponding eye position. Top left circle denotes response
when gazing top left, bottom right circle corresponds to gaz-         put neuron thus collects these indications and responds. Let
ing bottom right. The plus sign means that the effect of modu-        us look at the hidden neuron 4 whose RF suggests that it re-
lation is excitatory. The effect of gain modulation is evident in     sponds to objects located on the ground. Let us consider var-
all panels. For instance, in panel D, blue bottom circles illus-      ious combinations of the visual stimulus and eye positions.
trate that the unit is active even though its response to purely      The object cannot be on the ground when iCub looks up and
visual input is weak. Gain modulation has the same direction          sees an object. When iCub gazes straight ahead, the object is
as the RF (see also Figure 6 and the associated text), mean-          on the ground only when its projection falls on the top part
ing that the RF is sensitive to the object at the bottom and the      of the retina. When iCub gazes down, the object is always
effect of gain modulation is highest when gazing down.                on the ground. We would expect that the output neuron in-
   We can arrive at consistent conclusion when looking at             dicating this position will have strong connections from hid-
hidden–output connections and output neurons (see Figure 5).          den neuron 4. This is actually what one can see in Figure 5
Consider an output neuron whose activity indicates that the           when looking at the weights from hidden neuron 4 to the out-
object is located close to the ground. This neuron is fed by the      put units. Looking down corresponds to −35◦ (of the range
population of hidden neurons, each of which can be thought            [−90◦ , 90◦ ]), therefore the strongest connections are in the
of as indicating a specific position of the object. The out-          left part close to the centre (components 6–9).
                                                                  2178

RF–GF differences Following the procedure described in
Xing and Andersen (2000), we also analyzed the hidden neu-
rons in terms of direction differences between the RF and the
gain field (GF). Unit’s RF is defined as the input area that
evokes more than 50% of unit’s maximal response. We ex-
amined the relationship between unit’s gain and the RF by
comparing their directions. The GF direction points to the
best-tuned unit relative to the central eye position, and RF
direction is calculated as the center of mass of the unit’s re-
sponse across the input map. The angle between these two
directions, i.e. RF–GF, serves for testing whether GF and RF
are aligned in the same or opposite way. We show RF–GF
direction differences for all hidden units in Figure 6. We can
see that the majority of hidden units has this difference close
to zero, which implies that RF and GF are aligned. This also
replicates results of Model 1–4 in Xing and Andersen (2000).
                                                                       Figure 8: Response profiles, topographically sorted by the
                                                                       SOM. Every circle represents the response profile of one hid-
                                                                       den unit.
                                                                       Reference frames
     Figure 6: Histogram of RF–GF direction differences.               Here we examined how the centre of mass of the RF shifts
                                                                       for different gaze directions. To determine the centre of RF,
                                                                       we swapped the organizion of units’ response profiles to get
                                                                       a grid of 25×9 responses. In order to determine the refer-
                                                                       ence frames used by the population of hidden neurons, we
                                                                       computed the absolute shifts of RFs and their standard devi-
                                                                       ations for all units and put them into histograms in Figure 9.
                                                                       Absolute shifts close to zero are interpreted as encoding in
                                                                       eye-centered reference frame. Absolute shifts close to one in-
                                                                       dicate body-centered reference frame. Since we observe none
Figure 7: Star plot visualisation of the response profile (neu-
                                                                       of these situations, we conclude that the hidden layer encodes
ron 4). Each neuron response is plotted on one axis.
                                                                       the object position in intermediate coordinates between eye-
                                                                       and body-centered reference frames.
Variability of response profiles We observed that the re-
sponse profiles of many hidden units are more complex than
the one shown in Figure 4. To explore if there are any charac-
teristic profiles or clusters of profiles, we used a visualisation
method based on star plots. For illustration, visualisation of
the response profile of neuron 4 is shown in Figure 7. We
then trained a one-dimensional self-organizing map (SOM)
with 64 units to topographically organize response profiles
(Kohonen, 1982). Figure 8, nicely reveal gradual changes in
these profiles both in terms of (direction/position) selectivity
and in terms of amplitude (line length, gain modulated). This          Figure 9: Histograms of absolute RF shifts (left) and their
suggests that there is a continuum of responses profiles (rather       standard deviations (right). The vertical axis denotes the
than a discrete set) that emerged in the hidden layer as a result      number of units with the given value of the shift. Both hori-
of learned transformation. We can also say that the response           zontal and vertical shifts are superimposed, so the sum of bars
profiles appear to be specialized in similar manner as the RFs         in each histogram is 2 × 64 = 128.
due to the effect of gain modulation.
                                                                   2179

                         Conclusion                                   Burgess, N. (2006). Spatial memory: how egocentric and
We replicated experiments with Zipser–Andersen model, us-                   allocentric combine. Trends in Cognitive Sciences,
ing more complex (more realistic) data generated with the                   10(12), 1–7.
iCub simulator. For learning the model, we achieved the best          Fahlman, S. (1988). An empirical study of learning speed in
results with the standard version of BP with the momentum                   backpropagation networks (Tech. Rep. No. CMU-CS-
term. The network was able to successfully perform the trans-               88-162). Pittsburgh, PA: Carnegie Mellon University.
formation task with testing accuracy within 2◦ in case of a           Hoffmann, M., Marques, H., Arieta, A., Sumioka, H., Lun-
more homogeneous set of objects, and with accuracy of 4◦ in                 garella, M., & Pfeifer, R. (2011). Body schema in
case of more variable object sets. This implies that coordi-                robotics: a review. IEEE Transactions on Autonomous
nate transformations could be successfully realized using the               Mental Development, 2(4), 304-324.
data from iCub simulator. We examined the hidden layer by             Khan, A., Pisella, L., & Blohm, G. (2012). Causal evidence
means of visualisation techniques that revealed the nonlinear               for posterior parietal cortex involvement in visual-to-
effect of gain modulation and shifting receptive fields. The re-            motor transformations of reach targets. Cortex, 49,
sults of the reference frame analysis indicate that the hidden              2439–2448.
neurons encode object position in the intermediate reference          Kohonen, T. (1982). Self-organized formation of topologi-
frames between eye- and body-centered coordinates. It is in-                cally correct feature maps. Biological Cybernetics, 43,
teresting that these reference frames are actually an emergent              59-69.
process that results from error minimization within a super-          Makin, J., Fellows, M., & Sabes, P. (2013). Learning mul-
vised learning task. It is possible that similar emergent pro-              tisensory integration and coordinate transformation via
cesses could take place in the brain, possibly implemented by               density estimation. PLOS: Comput. Biology, 9(4).
mechanisms other than BP that is considered biologically im-          Nissen, S. (2005). Fast artificial neural network library.
plausible. However, alternatives exist that avoid explicit error            Available from http://leenissen.dk/fann/wp/
propagation between layers (O’Reilly, 1996) and share some            O’Reilly, R. (1996). Biologically plausible error-driven
features also with unsupervised Hebbian learning.                           learning using local activation differences: The gen-
   The brain must be able to integrate different sources of in-             eralized recirculation algorithm. Neural Computation,
formation which can significantly differ in terms of the num-               8(5), 895–938.
ber of afferent pathways, to avoid dominance of one modality          Qian, N. (1999). On the momentum term in gradient descent
to the expense of the other. Some theories and computational                learning algorithms. Neural Networks, 12(1), 145-151.
models can be found in Makin, Fellows, and Sabes (2013) and           Riedmiller, M., & Braun, H. (1993). A direct adaptive
references therein. Given the higher dimensionality of input                method for faster backpropagation learning: The Rprop
data we optimized the integration of different modalities in a              algorithm. In IEEE international conference on neural
more straightforward, albeit hardwired manner.                              networks (pp. 586–591). IEEE Press.
                                                                      Robinson, F., Noto, C., & Bevans, S. (2003). Effect of visual
                    Acknowledgments                                         error size on saccade adaptation in monkey. Journal of
                                                                            Neurophysiology, 90(1), 1235-1244.
This work was supported by the projects VEGA 1/0898/14                Salinas, E., & Sejnowski, T. (2001). Gain modulation in the
and KEGA 076UK-4/2013. M. Švec was a graduate student                      central nervous system: Where behavior, neurophysi-
at the Department of Applied Informatics. We thank three                    ology, and computation meet. Neuroscientist, 7, 430–
anonymous reviewers for their comments.                                     440.
                                                                      Salinas, E., & Thier, P. (2000). Gain modulation: A major
                         References                                         computational principle of the central nervous system.
Andersen, R., & Mountcastle, V. (1983). The influence of the                Neuron, 27, 15–21.
       angle of gaze upon the excitability of the light-sensitive     Tikhanoff, V., Fitzpatrick, P., Nori, F., Natale, L., Metta, G.,
       neurons of the posterior parietal cortex. Journal of Neu-            & Cangelosi, A. (2008). The iCub humanoid robot
       roscience, 3(3), 532–548.                                            simulator. Advanced Robotics, 1(1), 22-26.
Averbeck, B., Latham, P., & Pouget, A. (2006). Neural cor-            Xing, J., & Andersen, R. (2000). Models of the posterior pari-
       relations, population coding and computation. Nature                 etal cortex which perform multimodal integration and
       Reviews Neuroscience, 7, 358-366.                                    represent space in several coordinate frames. Journal
Batista, A. (2002). Inner space: Reference frames. Current                  of Cognitive Neuroscience, 12(4), 601–614.
       Biology, 12(11), 380–383.                                      Zipser, D., & Andersen, R. (1988). A backpropagation pro-
Blohm, G., & Crawford, J. (2009). Fields of gain in the brain.              grammed network that simulates response properties
       Neuron, 64(5), 598–600.                                              of a subset of posterior parietal neurons. Nature, 331,
Buneo, C., & Andersen, R. (2006). The posterior parietal cor-               679–684.
       tex: sensorimotor interface for the planning and online
       control of visually guided movements. Neuropsycholo-
       gia, 44(13), 2594–2606.
                                                                  2180

