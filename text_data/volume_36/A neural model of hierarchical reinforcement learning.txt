UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A neural model of hierarchical reinforcement learning
Permalink
https://escholarship.org/uc/item/2w78v3c0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Rasmussen, Daniel
Eliasmith, Chris
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                          A neural model of hierarchical reinforcement learning
                                            Daniel Rasmussen (drasmuss@uwaterloo.ca)
                                              Chris Eliasmith (celiasmith@uwaterloo.ca)
                                       Centre for Theoretical Neuroscience, University of Waterloo
                                                        Waterloo, ON, Canada, N2J 3G1
                               Abstract                                  HRL into a theory of neural function, providing a hypothe-
                                                                         sis for how the brain could achieve its strengths in scaling
   We present the first model capable of performing hierarchical         and knowledge transfer. In the next section we give a brief
   reinforcement learning in a general, neurally detailed imple-
   mentation. We show that this model is able to learn a spatial         introduction to the mathematical underpinnings of HRL. We
   pickup and delivery task more quickly than one without hier-          then describe the implementation of the model we have de-
   archical abilities. In addition, we show that this model is able      veloped, and demonstrate the ability of this model to speed
   to leverage its hierarchical structure to transfer learned knowl-
   edge between related tasks. These results point towards the           learning and transfer knowledge between related tasks. We
   advantages to be gained by using a hierarchical RL framework          conclude with a discussion of some of the testable predic-
   to understand the brain’s powerful learning ability.                  tions that arise from this model, and the next directions for its
   Keywords: neural model; reinforcement learning; hierarchical          continued development.
   reinforcement learning; Neural Engineering Framework
                           Introduction                                                             Background
Reinforcement learning (RL) has a long history of rich in-
                                                                         Reinforcement learning
teraction between computational theories and neuroscientific
understanding. This interaction has led to new understand-
                                                                         Reinforcement learning is concerned with maximizing over-
ings of neural data, as well as new biologically inspired com-
                                                                         all reward across a sequence of decisions. It is usually for-
putational theories. However, basic RL techniques have a
                                                                         mulated as a Markov Decision Process (MDP) where the task
number of challenges, two of the most critical being diffi-
                                                                         has some state space S, available actions A, transition function
culty scaling up to complex problem domains and difficulty
                                                                         T (s, a) (which describes how the agent will move through
transferring knowledge between tasks (Barto & Mahadevan,
                                                                         the state space given a current state s and selected action a),
2003; Taylor & Stone, 2009). These are two areas in which
                                                                         and reward function r(s, a) (which describes the feedback the
the brain excels, which presents a dilemma for neural rein-
                                                                         agent will receive after selecting action a in state s).
forcement learning models. However, new theories have been
proposed to overcome these challenges on the computational                   Temporal difference (TD) learning describes a popular
side, which may again prove fruitful when we apply them in               family of methods for solving the reinforcement learning
the effort to understand and model the brain’s learning ability.         problem.1 It uses the concept of Q values, where Q(s, a) in-
   One active area of research in RL is the field of Hier-               dicates the long term reward to be expected when selecting
archical Reinforcement Learning (HRL; Barto & Mahade-                    action a in state s. This can be expressed recursively as the
van, 2003). HRL introduces higher level actions into the RL              immediate reward r(s, a) plus the value of the next state, that
framework, where selecting one of those actions may drive a              is:
whole sequence of decisions. For example, a high level action
might be “go to the grocery store” or “go to work”, and select-                              Q(s, a) = r(s, a) + γQ(s0 , a0 )          (1)
ing one of those actions then guides a sequence of “right turn”
or “left turn” sub-actions. This helps to address the scaling            (γ is a discount factor applied to future rewards).
problem by imposing more structure on the problem space.                     TD learning is a method for learning those Q values in an
A long sequence of decisions can now be encapsulated in a                environment where the transition and reward functions are
single choice (“go to the grocery store”), and the value of that         unknown, and can only be sampled by exploring the environ-
choice can be calculated in a single learning update (some-              ment. It accomplishes this by taking advantage of the fact that
what) independently of the intervening choices. HRL also                 a Q value is essentially a prediction, which can be compared
addresses the knowledge transfer problem, as the high-level              against observed data. That is, as the agent moves through
actions represent natural, modular components to transfer be-            the state space it is acquiring samples of r(s, a) and Q(s0 , a0 ),
tween tasks. For example, once we have learned how to navi-              and it can use the difference between that observed data and
gate to work, it is easy to see how we could reuse that skill as
a subcomponent in many other work-related tasks.
                                                                             1 TD   learning is also one of the classic examples of cross-
   In this paper we present a biologically plausible neu-
                                                                         fertilization between computational theory and neuroscience, as it
ral model capable of performing hierarchical reinforcement               presented a new framework to understand data on dopamine func-
learning. This allows us to bring the enhanced power of                  tion (Schultz, 1998).
                                                                     1252

Q(s, a) to update the prediction.2 Specifically,
                                                                                                                                s'
             ∆Q(s, a) = r(s, a) + γQ(s0 , a0 ) − Q(s, a)             (2)                             s
   There are many approaches to building neural models of
reinforcement learning, ranging from more abstract artificial
neural networks to detailed spiking neural models (Potjans et                                                             E
                                                                                                                                   r
al., 2009; Frémaux et al., 2013). In Rasmussen & Eliasmith
(2013) we present our own model of reinforcement learning,
which extended previous efforts in ways that will become im-                                                                         environment
                                                                                  a1         a2              ...         an
portant in the next section. We will not go into detail on these
different approaches here, as we wish to focus on the new                            Q(s,a1) Q(s,a2)   Q(s,...)  Q(s,an)    Q(s,a*)
development of hierarchical reinforcement learning.                                                                              a*
Hierarchical reinforcement learning
As RL is based on the MDP framework, HRL is based on the                                       selection
Semi-MDP (SMDP) framework. SMDPs extend MDPs by
adding time into the equation. Mathematically, the transition                Figure 1: Architecture of a model for performing non-
and reward functions—T (s, a,t) and r(s, a,t)—now depend                     hierarchical reinforcement learning. See text and Rasmussen
on time as well as the state/action selected. This means that,               & Eliasmith (2013) for details.
for example, if the agent selects an action they may not get
a reward until several seconds or minutes later (or they could
receive several rewards throughout that period), and they may                However, in previous work (Rasmussen & Eliasmith, 2013)
not arrive in a new state until some time after that.                        we demonstrated a model capable of learning in an arbitrary
   The SMDP framework is important for HRL, as the time                      SMDP environment, thus laying the groundwork for a model
delays can be used to encapsulate the activity of the subpolicy.             of HRL.
That is, after selecting the action of “go to the grocery store”,               There has been little work attempting to integrate this com-
the results cannot be observed immediately. The pertinent                    putational theory with neural modelling. Botvinick et al.
information needs to be preserved over time while that sub-                  (2009) discuss how the actor-critic architecture could be ex-
policy executes, so that the TD error can be computed once                   tended to support HRL, along with the neurophysiological ev-
the grocery store is reached. The SMDP framework allows us                   idence supporting the plausibility of such extensions. How-
to represent that type of environment.                                       ever, their model itself was not implemented at the neural
   Under the SMDP framework, Q values can be re-expressed                    level. In Frank & Badre (2012) the authors modified their
as:                                                                          previous model of corticostriatal action selection to allow for
                          τ−1                                                a hierarchy of actions. However, theirs was a model of a spe-
               Q(s, a) =  ∑ γt r(s, a,t) + γτ Q(s0 , a0 )            (3)     cific hierarchical task, rather than a general model of hierar-
                          t=0
                                                                             chical reinforcement learning that can be applied across tasks.
where τ is the time at which the state transition occurs. This               For example, their model was unable to solve tasks involving
leads to a modified TD update of:                                            temporally extended sequences of actions. We are not aware
                     τ−1                                                     of any other work that presents a general model of how the
        ∆Q(s, a) =    ∑ γt r(s, a,t) + γτ Q(s0 , a0 ) − Q(s, a)      (4)     brain could perform hierarchical reinforcement learning.
                     t=0
   Although computationally simple (the main difference be-                                                   Model
ing that rewards are summed over the delay period), the                      Previous work
SMDP framework significantly complicates a neural imple-                     This work is based on a previous model of SMDP reinforce-
mentation. The main problem is that information on reward,                   ment learning, described in Rasmussen & Eliasmith (2013).
discount, and Q values now needs to be preserved over a po-                  We will briefly review the important features of that model,
tentially unknown, variable, and lengthy delay period. Al-                   but for the sake of brevity focus primarily on how we extend
most all neural RL models rely on some type of “eligibil-                    the model to perform HRL.
ity trace” to preserve information between states, which im-                    The model’s architecture is shown in Figure 1. At the top
poses a fixed and short3 time window on any learning update.                 is a population of neurons that represent the current state, s.
    2 Note that what we describe here is the SARSA (Rummery &                This state can represent any desired information; it is sim-
Niranjan, 1994) implementation of TD learning. The other main                ply an abstract vector, which is encoded into neural activities
approach is Q-learning (Watkins & Dayan, 1992), which operates on            using the principles of the Neural Engineering Framework
a similar principle but searches over possible future Q(s0 , a0 ) values
rather than waiting to observe them.                                         (NEF; Eliasmith & Anderson, 2003). The state can vary con-
    3 Assuming the common biological explanation for eligibility             tinuously over time and space, or, if desired, the system can
traces based on neurotransmitter decay.                                      approximate discrete states by restricting the state to fixed
                                                                         1253

points in the vector space. All components of this model are              cluding a representation of the current context in the vector
implemented using leaky integrate-and-fire (LIF) neurons; in              input to the s population. The output of the s neurons then
the case of the state population, these neurons, combined with            represents context-dependent activity, allowing the system to
the principles of the NEF, take the input state and convert it            produce different Q values in different contexts with a single
into firing activity.                                                     set of connection weights. This allows the system to represent
   The output activity of the state neurons is passed to a sec-           and swap between different policies simply by changing the
ond set of neural populations, corresponding to the different             context representation in the s input, without changing any of
actions available to the agent, an .4 Each of those populations           the structural aspects of the model.
attempts to output the value of its associated action given the              The next question is how to organize the model into a hier-
current state (as represented by the activity of the s popula-            archy, so that higher level decisions (e.g., “go to the grocery
tion). Using the NEF we can interpret the output of the a                 store”) can control the lower level decisions. Given the struc-
neurons as estimated Q values.                                            ture laid out above, this can be accomplished by allowing high
   Next, the system needs to select an action to perform based            level systems to set the context in lower level systems. This
on those Q values. The “selection” network performs this                  architecture is shown in Figure 2. The key feature is that the
function. The core of this component is a neural model of                 action selected by the higher level system, rather than affect-
the basal ganglia and thalamus (for more detail see Stewart et            ing the environment, is used to set the context of the lower
al. 2010), along with several memory components needed to                 level system.5 Thus if the higher level system were to select
preserve information across the SMDP time delay. The end                  the “go to the grocery store” action, it would set the lower
result is that the highest valued action and the Q value of that          level system to be in the “grocery store” context. The lower
action are produced as output.                                            level system would then choose actions according to its “gro-
   The action is delivered to the environment, which com-                 cery store” policy, and the selected actions would be delivered
putes a new state and reward. The system is designed to treat             to the environment to control the movement of the agent.
the environment as a black box; all of its processing occurs                 Note that we have shown a system here with two levels,
in a general, task-independent fashion, so that different envi-           but there is no theoretical restriction on the depth of the hier-
ronments can be swapped out without affecting the rest of the             archy. These systems could be chained together in this way
model.                                                                    to provide as many levels as desired, the only constraint be-
   The value of the selected action is delivered to the error cal-        ing the number of neurons required (the model used in this
culation network, E. This network computes the error shown                work uses approximately 35 000 neurons per level, but that
in Equation 4 through several interconnected neural dynam-                value is affected by the complexity of the task). In addition,
ical systems (its implementation is described in more detail              we have shown the architecture here such that all levels of the
in Rasmussen & Eliasmith 2013). The output of this network                hierarchy receive the same environmental state and reward in-
is used to drive an error-modulated local learning rule (Mac-             formation, which is the simplest case. However, this system
Neil & Eliasmith, 2011) on the connections between the s and              can also operate in the case where different hierarchical levels
a populations, so that over time the the output of the a popu-            use different (e.g., internally generated) state/reward values,
lations will come to represent the correct Q values.                      an example of which is demonstrated in the results section.
                                                                             In regard to the neuroanatomical mapping, it is important
Hierarchical model
                                                                          to note that although we have separate selection networks in
In order to extend this model for hierarchical reinforcement              the different levels, neuroanatomically these are all based in
learning, the first step is to allow the model to represent sev-          the same basal ganglia. The different selection networks in
eral different policies. That is, it needs to be able to represent        each layer correspond to different corticostriatal loops, which
one set of Q values if it is in the “go to the grocery store”             have been shown to be organized in a hierarchical manner
context, and flexibly switch to a different set of Q values if            (Frank & Badre, 2012). In addition, the label for the state rep-
the context changes to “go to work”.                                      resentations, “cortex”, is intentionally ambiguous. As men-
   One approach would be to have multiple sets of connec-                 tioned, this model is designed as a general reinforcement
tions between the s and a populations, and switch between                 learning system that can operate across many tasks. Thus the
them using some gating mechanism. However, this is imprac-                state could take on many forms; it could be visual input, hip-
tical for a number of reasons: it greatly increases the number            pocampal place cell activations, or more abstract prefrontal
of connections needed, it introduces the new problem of how               representations, all of which have efferent projections to the
to switch between connections, and it is inflexible in that the           basal ganglia.
contexts must be directly encoded into the structure of the                  It is not necessary a priori that different hierarchical levels
model. Thus in our model we instead accomplish this by in-                should have different structural components. For example,
    4 The system we describe here uses a discrete action space, where     an alternate implementation would be to have one system,
the agent chooses one of n possible actions. However, that is not an
intrinsic requirement of this architecture; this type of system could         5 This general style of architecture has been employed in several
represent a continuous action space through a weighted sum of the         hierarchical systems throughout the years; for example, it can be
available actions.                                                        traced back to work on feudal RL (Dayan & Hinton, 1993).
                                                                      1254

                                                                                     environment
                                                            s'
                                                                                                                    s'
                                                                                                                                            r
                                                                       r
                                         s              c                                                                    s                  c
                                                                                                                                                                         a*
cortex                                                             E                      a*
                                                                                                                                                           E
                                                                                                                                                               Q(s,a*)
                                                                           Q(s,a*)
                a1             a2                 ...             an                               a1              a2                 ...                 an
basal ganglia
                     Q(s,a1)   Q(s,a2)       Q(s,...)   Q(s,an)                                         Q(s,a1)    Q(s,a2)       Q(s,...)       Q(s,an)
                                 selection                                                                           selection
  Figure 2: Hierarchical reinforcement learning architecture, wherein the actions of the higher level system modify the context
  of the lower level system. See text for details.
  with both the low and high level actions available to it, and
  the high level actions would recursively modify the system’s
  own context. However, the implementation we have chosen
  is consistent with empirical data on reinforcement learning                                                     pickup
  in hierarchical tasks from Badre et al. (2010). They showed
  that learning in the hierarchical setting showed structurally
  distinct activations, with more abstract contexts associated
  with increasingly anterior activation in the prefrontal cortex.
                                                                                                                         agent
  In addition, they showed that subjects were able to learn at                                                                              dropoff
  multiple levels of the hierarchy simultaneously, which is an
  important advantage of the implementation we have chosen.
  That is, if low and high level actions are combined into a sin-                         Figure 3: Schematic representation of the environment in the
  gle system, then that system can only learn at one level at a                           delivery task. The agent must navigate to the pickup location
  time (corresponding to the currently selected action). Sepa-                            to retrieve the package and then move to the dropoff location
  rating out the levels allows this system to learn in parallel at                        to receive reward.
  all levels of the hierarchy.
                                    Results
                                                                                          ure 3. The agent begins at a random location, empty-handed.
  Task                                                                                    It can move in any of the four cardinal directions, unless
  In order to demonstrate the performance of the model, we                                blocked by a wall (shown in black) in which case it will stay
  have chosen to use a delivery task. In this task the agent must                         still. Upon entering the blue region the agent will pick up the
  go to one location to pick up a package, and then a second                              package. Upon entering the red region with the package in
  location to drop it off. This task is commonly used in HRL,                             hand, the agent receives a constant reward of 1.5 for 500ms,
  both in computational and experimental settings, as it natu-                            at which point the package is reset and the agent is moved
  rally lends itself to hierarchical learning: at the low level the                       to another random location. At all other times the agent re-
  system simply learns to navigate to a given location, and the                           ceives a base reward of -0.05. This penalty increases by -0.1
  high level learns which location should be the current target.                          for every second the agent attempts to move into a wall, to en-
     The model operates in continuous time and space. A                                   courage it to complete the task quickly and move throughout
  schematic representation of the environment is shown in Fig-                            the environment.
                                                                                      1255

                         250                                                                        250
                               flat                                                                       transfer
                         200   hierarchical                                                         200   no transfer
                               optimal                                                                    optimal
    accumulated reward                                                         accumulated reward
                         150                                                                        150
                         100                                                                        100
                          50                                                                         50
                          00   1000   2000       3000    4000   5000                                 00   1000   2000      3000    4000   5000
                                              time (s)                                                                  time (s)
Figure 4: Total reward accumulated by a flat versus hierar-                Figure 5: Total reward accumulated by a model initialized
chical reinforcement learning model over the course of the                 with skills learned on a simpler task versus a model with-
delivery task, demonstrating the advantages of a hierarchical              out prior information, demonstrating the ability of the model
system. Displaying 95% confidence intervals.                               to transfer knowledge between tasks. Displaying 95% confi-
                                                                           dence intervals.
   The state representation output from the environment is
constructed to mimic the output of hippocampal place cells.                hierarchical system has begun to markedly improve its per-
Simulated place cells are tuned to random locations through-               formance, as evidenced by the increasing slope of reward ac-
out the environment. Each cell has a Gaussian activation cor-              cumulation. By the end of the training period the average rate
responding to the distance of the agent from that location.                of reward accumulation is 52% of optimal for the hierarchical
These activations are concatenated into an n-dimensional vec-              system and 13% for the flat system.
tor (where n is the number of place cells), which becomes the
state signal input to the model. The environment represents                Knowledge transfer
whether the agent has the package in hand or not by append-
ing one of two orthogonal 2-dimensional vectors to the state               The next important aspect of HRL is its ability to support
representation.                                                            knowledge transfer. To test this ability, we pretrained a flat
                                                                           model (as in Figure 1) for 2000 seconds on a simpler task,
Hierarchical learning                                                      where the agent was rewarded just for moving to one of the
The first result to demonstrate is that a model with hierar-               two targets in Figure 3 (randomly chosen every 60 seconds).
chical learning ability performs better than a standard “flat”             Thus the system learns the low-level skills it needs (how to
reinforcement learning system. For this we trained two sys-                navigate to the targets), but not the high-level policy for how
tems on the delivery task. One had the structure shown in                  to put those skills together to accomplish the delivery task.
Figure 1 and the other had the structure shown in Figure 2. In             We then took the knowledge learned in that system, repre-
the hierarchical system, the lower level receives an internally            sented by the connection weights between the s and a pop-
generated reward rather than reward from the environment—                  ulations, and loaded it into the corresponding actions in the
a reward of 1.5 whenever it achieves the goal set by the high              lower layer of a hierarchical model as shown in Figure 2.
level system. Other than the structural differences, the two                  As can be seen in Figure 5, the model is able to success-
systems were identical: they had the same parameters, and                  fully transfer knowledge between the two tasks. Even though
the same initial conditions (we initialized all Q values to 0.1).          the model has never seen the delivery task before, it is able to
   Figure 4 shows the results of the two systems. We have                  begin with quite high performance due to its previous expe-
plotted the total reward accumulated by each system relative               rience on the simpler task. It is worth noting that this benefit
to the reward accumulated by a randomly moving agent (used                 goes beyond a simple 2000 second head start; even after 2000
as a baseline). We are also showing an upper bound on perfor-              seconds, the untrained model has still not achieved the perfor-
mance, determined by simulating an agent that performed op-                mance (as measured by the slope of reward accumulation) of
timally, always selecting the correct action. It can be seen that          the transfered model. This is because the learning in the high
while both the flat and hierarchical systems begin with near-              level system is significantly aided by the fact that the lower
random performance, after approximately 2000 seconds6 the                  level system can reliably perform the actions selected by the
                                                                           high level system. In the untrained system both layers must
    6 Note that all times shown are simulation time; the model itself      train up simultaneously, which is a more difficult task. By the
takes much longer to run, due to the challenges of simulating large-       end of the training period the average rate of reward accumu-
scale neural models in current software and hardware. Improving
the simulation speed of NEF models is a focus of ongoing work (see         lation is 98% of optimal for the transfered system and 52%
Bekolay et al., 2014).                                                     for the untrained system.
                                                                        1256

                          Discussion                                                             References
                                                                     Badre, D., Kayser, A. S., & D’Esposito, M. (2010). Frontal cortex
We have demonstrated the ability of the model to perform                and the discovery of abstract action rules. Neuron, 66(2), 315–
hierarchical learning, as well as the enhanced reinforcement            26.
learning abilities of such a model. Specifically, we have            Barto, A. G., & Mahadevan, S. (2003). Recent advances in hierar-
shown that the model is able to take advantage of the hier-             chical reinforcement learning. Discrete Event Dynamic Systems,
                                                                        1–28.
archical structure of a task to speed its learning versus a flat
                                                                     Bekolay, T., Bergstra, J., Hunsberger, E., DeWolf, T., Stewart, T. C.,
reinforcement learning model. Another important advantage               Rasmussen, D., . . . Eliasmith, C. (2014). Nengo: a Python
of the HRL approach is that it naturally lends itself to knowl-         tool for building large-scale functional brain models. Frontiers
edge transfer, which we demonstrated through the model’s                in Neuroinformatics, 7(48), 1–13.
                                                                     Botvinick, M. M., Niv, Y., & Barto, A. G. (2009). Hierarchically
ability to benefit from knowledge gained on a related task to           organized behavior and its neural foundations: a reinforcement
speed its learning on the delivery task. This is the the first          learning perspective. Cognition, 113(3), 262–80.
model to present a general and neurally detailed implementa-         Dayan, P., & Hinton, G. (1993). Feudal reinforcement learning.
tion of hierarchical reinforcement learning.                            In Advances in Neural Information Processing Systems (pp. 271–
                                                                        278).
   With the functional capabilities of the model established,        Eliasmith, C., & Anderson, C. (2003). Neural engineering: Compu-
it is now possible to begin to compare it in detail to experi-          tation, representation, and dynamics in neurobiological systems.
mental data. One of the important goals of these models is              Cambridge: MIT Press.
to create predictions, which can be used to verify the model         Frank, M. J., & Badre, D. (2012). Mechanisms of hierarchical re-
                                                                        inforcement learning in corticostriatal circuits 1: computational
as well as to generate new hypotheses for experimental in-              analysis. Cerebral Cortex, 22(3), 509–26.
vestigation. One implication of this model is its hierarchical       Frémaux, N., Sprekeler, H., & Gerstner, W. (2013). Reinforcement
structure; namely, that different layers of the hierarchy are           Learning Using a Continuous Time Actor-Critic Framework with
separated into structurally distinct regions, and that they in-         Spiking Neurons. PLoS Computational Biology, 9(4), e1003024.
teract by the output of higher level regions modifying the state     MacNeil, D., & Eliasmith, C. (2011). Fine-tuning and the stability
                                                                        of recurrent neural networks. PloS ONE, 6(9), e22885.
representations in lower level regions. As mentioned, there is       Potjans, W., Morrison, A., & Diesmann, M. (2009). A spiking
already support for this hypothesis in the work of Badre et             neural network model of an actor-critic learning agent. Neural
al. (2010). However, this model allows us to generate much              Computation, 339, 301–339.
more specific predictions, such as the timecourse and magni-         Rasmussen, D., & Eliasmith, C. (2013). A neural reinforcement
                                                                        learning model for tasks with unknown time delays. In M. Knauff,
tude of error signals in each level over the course of a task.          M. Pauen, N. Sebanz, & I. Wachsmuth (Eds.), Proceedings of the
   Another important prediction from this model is the pres-            35th Annual Meeting of the Cognitive Science Society (pp. 3257–
ence of time-delayed representations. This arises from the              3262). Austin: Cognitive Science Society.
                                                                     Rummery, G., & Niranjan, M. (1994). On-line Q-learning using
switch to the SMDP framework, which requires the model to               connectionist systems (Tech. Rep. No. September).
preserve information on the identity and value of the previ-         Schultz, W. (1998). Predictive reward signal of dopamine neurons.
ous state. This suggests that we should be able to find neural          Journal of Neurophysiology, 80, 1–27.
activity that correlates not just with the current state (which      Singh, S., Barto, A. G., & Chentanez, N. (2005). Intrinsically moti-
is already fairly well established), but also, simultaneously,          vated reinforcement learning. In L. K. Saul, Y. Weiss, & L. Bottou
                                                                        (Eds.), Advances in Neural Information Processing Systems (pp.
activity representing the value of the previous decision point.         1281–1288). MIT Press.
   We will also continue to expand the functional capabili-          Stewart, T. C., Choo, X., & Eliasmith, C. (2010). Dynamic be-
ties of this model. One of the important open questions in              haviour of a spiking model of action selection in the basal gan-
                                                                        glia. In S. Ohlsson & R. Catrambone (Eds.), Proceedings of the
HRL research is how to learn the hierarchical structure (e.g.,          32nd Annual Conference of the Cognitive Science Society (pp.
learning which states should become subgoals), as opposed               235–240). Austin: Cognitive Science Society.
to having that structure built in as it is in this model. We are     Taylor, M., & Stone, P. (2009). Transfer learning for reinforce-
particularly interested in the work of Singh et al. (2005) on           ment learning domains: A survey. Journal of Machine Learning
                                                                        Research, 10, 1633–1685.
intrinsic motivation. This is the idea that the brain has inter-     Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine
nal mechanisms that make novel or surprising states/events              Learning, 8(3-4), 279–292.
rewarding, independent of the external task reward, and that
that internally generated reward signal can be used to guide
the development of useful subpolicies. Extending the model
presented here to include that ability would allow it to provide
a more fully-featured account of the brain’s flexible reinforce-
ment learning ability.
                    Acknowledgements
This work was supported by the Natural Sciences and En-
gineering Research Council of Canada, Canada Research
Chairs, the Canadian Foundation for Innovation, and Ontario
Innovation Trust.
                                                                 1257

