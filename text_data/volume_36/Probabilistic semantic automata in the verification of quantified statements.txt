UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Probabilistic semantic automata in the verification of quantified statements
Permalink
https://escholarship.org/uc/item/5cg4b4zg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Dotlacil, Jakub
Szymanik, Jakub
Zajenkowski, Marcin
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                       University of California

         Probabilistic semantic automata in the verification of quantified statements
                                                  Jakub Dotlačil (j.dotlacil@gmail.com)
                                         Center for Language and Cognition, University of Groningen
                                            Jakub Szymanik (jakub.szymanik@gmail.com)
                                 Institute for Logic, Language and Computation, University of Amsterdam
                                        Marcin Zajenkowski (zajenkowski@psych.uw.edu.pl)
                                                  Faculty of Psychology, University of Warsaw
                                 Abstract                                                      correct      correct, incorrect
     Strategies used by people to verify quantified sentences, like
     ‘Most cars are white’, have been a popular research topic on                                q0    incorrect     q1
     the intersection of linguistics, computer science, philosophy,
     and psychology. A prominent computational model of the task,
     semantic automata, has been introduced by van Benthem in
     1983. In this paper we present a probabilistic extension of          Figure 1: This finite automaton checks whether every sen-
     the model. We show that the model explains counting errors
     in the verification process. Furthermore, we observe that the        tence in the text is grammatically correct. It inspects the text
     variation in quantifier verification data cannot be explained by     sentence by sentence starting in the accepting state (double
     Approximate Number Sense, a prominent approach to proba-             circled), qo . As long as it does not find an incorrect sentence
     bilistic number estimation.
                                                                          it stays in the accepting state. If it finds an incorrect sentence,
     Keywords: quantifier verification; natural language seman-
     tics; automata theory; probabilistic computational modeling,         then it already “knows” that the sentence is false and move to
     Approximate Number Sense                                             the rejecting state, q1 , where it stays no matter what sentence
                                                                          is next.
                            Introduction.
 Subjects’ verification strategies used in rejecting/accepting
 sentences have been a popular research topic in psycholin-               Mathematically speaking, such an algorithm can be realized
 guistics (see, e.g. Clark and Chase, 1972). Together with the            by a push-down automaton, PDA, see Fig. 2. PDAs can not
 turn to more linguistically-complex phenomena the topic has              only read the input and move to the next state, they also have
 also received an increased interest in linguistics, semantics,           access to the stack memory and depending on the top ele-
 logic, and computer science. Especially the computational                ment of the stack they decide what to do next. Graphically,
 and cognitive capacities of recognizing the truth-value of sen-          we represent it by the following labeling of each transition:
 tences with so-called generalized quantifiers (like ‘some’, ‘an          s1 , x, y → s2 , w, where s1 is the current state, x is the current
 even number of’, ‘more than 7’, ‘less than half’ (Peters and             input the machine reads (i.e. the element under considera-
 Westerståhl, 2006)) has been intensively studied (see, e.g.              tion), y is the top element of the stack, and s2 is the final state
 Szymanik, 2009; Lidz et al., 2011)                                       and w shows what element is put on the top of the stack next
     A prominent computational model for verification of quan-            (when the element is added to the previous top element, w is
 tifiers employs semantic automata (van Benthem, 1986). In-               of length 2 and shows both the previous element and the new
 tuitively, to check whether sentence (1) is true:                        element) (Hopcroft et al., 2000).
                                                                              It has been shown that the computational distinction be-
1. Every sentence in this paper is grammatically correct.                 tween quantifiers recognized by finite-automata and push-
 it suffices to read the sentences from this article one by one.          down automata is psychologically relevant, i.e., the more
 If we find an incorrect one, then we know that the statement is          complex the automaton, the longer the reaction time
 false. Otherwise, if we read the entire paper without finding            and working memory involvement of subjects asked to
 any incorrect sentence, then statement (1) is true (see Fig. 1           solve the verification task (see Szymanik and Zajenkowski,
 for a graphical representation). Analogous strategies exist for          2010a,b).McMillan et al. (2005), in an fMRI study, have
 all other natural language quantifiers.                                  shown that during verification, all sentences recruit the right
     However, for recognizing some higher-order quantifiers,              inferior parietal cortex associated with numerosity, but only
 like “less than half” or “most”, we need computational mod-              proportional quantifiers recruit the prefrontal cortex, which is
 els making use of internal memory. Intuitively, to check                 associated with executive resources, such as working mem-
 whether sentence (2) is true we must identify the number of              ory. Zajenkowski et al. (2011) have compared the process-
 correct sentences and store it in working memory to compare              ing of natural language quantifiers in a group of patients with
 with the number of incorrect sentences.                                  schizophrenia and a healthy control group. In both groups,
                                                                          the difficulty of the quantifiers was consistent with the com-
2. Most of the sentences are grammatically correct.                       putational predictions, and patients with schizophrenia took
                                                                      2967

         s0 , correct, # → s0 , Y                                                      Probabilistic semantic automata.
        s0 , incorrect, # → s0 , N                                      Probabilistic finite-state automata (PFSA) can be used to
          s0 , correct, N→ s0 , ε                                       model the verification of counting quantifiers (like, ‘more
        s0 , incorrect, Y→ s0 , ε                                       than k’, ‘less than k’). PFSAs are tuples hS, Σ, s0 , F, M, Probi,
        s0 , correct, Y→ s0 , YY                                        where S = {s0 , . . . , sn } is the finite set of states, Σ the input
      s0 , incorrect, N → s0 , NN                                       alphabet, s0 is the starting state, F is the set of final states, M
                                                                        is a transition relation (S × Σ) × S and Prob assigns a proba-
                                 s0 , ε, Y → s1 , ε                     bility to each element of M, such that for every (s j , a) ∈ S ×Σ,
                     s0                                  s1             ∑si ∈M(s j ,a) Prob(s j , a, si ) = 1 (cf. Rabin, 1963).
                                                                            As an input PFSAs take a string encoding the finite situa-
                                                                        tion (model). They are to decide whether a given quantifier
Figure 2: This push-down automaton for statement (2) reads              sentence, Q(A, B), is true in the model.
the text sentence by sentence. The automaton needs two                      An example of a PFSA used for the verification of more
states and the stack. It starts in the rejecting state, s0 with         than one car is blue is shown in Figure 3, where s0 is the
an empty stack marked by #. If it finds a correct sentence              initial state and s2 is the final accepting state, each transi-
it pushes Y on top of the stack and stays in s0 , if it finds an        tion indicates what happens when a blue car is encountered,
incorrect sentence it pushes N on top of the stack. If it finds         and the superscript on a transition indicates where the tran-
a correct (incorrect) sentence and there is already N (Y ) on           sition originated. Thus, for instance, p00 –p02 all start at s0
the top of the stack, the automaton pops out the top of the             and, given the conditions on probabilities discussed above,
stack (by turning it into the empty string ε). If it ‘sees’ a cor-      exhaust the events in one probability space (probabilities as-
rect (incorrect) sentence and there is Y (N) on the top of the          signed to them sum up to 1). They are abbreviations. p02 , for
stack, then the automaton pushes another Y (N) on the top of            example, is an abbreviation of (s0 , BLUE CAR, s2 ). The deter-
the stack. Eventually, when the automaton has analyzed the              ministic version discussed by van Benthem (1986) could be
whole paper (input = ε) then it looks on the top of the stack.          expressed by assuming Prob(p01 ) = Prob(p11 ) = 1.
If there is a Y it moves to the accepting state, otherwise it
stays in the rejecting state without modifying the stack. For
                                                                                                                 p02
simplification, we omit the situations: s0 , ε, N→ s0 , N , s0 , ε,
#→ s0 , #.
                                                                                                       p01                       p11
                                                                                            s0                   s1                       s2
more time to solve the problems. However, they were signif-                                 p00                  p10
icantly less accurate only with proportional quantifiers, such
as ‘more than half’.                                                    Figure 3: Probabilistic finite state automaton for more than
   All this evidence speaks in favor of the thesis that the             one car is blue.
model employing two types of automata can capture some
cognitive aspect of the semantics for generalized quantifiers.              Probabilistic push-down automata (PPDAs) are tuples
However, the model, while important, is crude and only qual-            hS, Σ, Γ, M, s0 , F, Probi, where S and Σ as previously stand
itative. It distinguishes between quantifier types but it does          for the set of states and the input alphabet, Γ is the stack al-
not offer a cognitive computational story of quantifier pro-            phabet (# ∈ Γ marks the bottom of the stack) and the tran-
cessing. For instance, the semantic automata cannot tell us             sition relation is M ⊆ (S × Σ × Γ) × (S × Γ∗ ), where the
why the verification of the sentence More than half of the              length of Γ∗ is at most 2 elements. Similarly as with PF-
cars are blue leads to more problems when there are 8 blue              SAs, ∑(si ,A∗i )∈M(s j ,a,A j ) Prob(s j , a, A j , si , A∗i ) = 1 for every
cars and 7 cars of other colors than when there are 9 blue              (s j , a, A j ) ∈ S × Σ × Γ. An example of a PPDA modeling the
cars and 6 other cars, as we will show below. The model                 verification of more than half of the cars are blue is given
also leaves the relation between semantics of quantifiers and           in Table 1, where s1 is the final accepting state. Each non-
number sense (Dehaene, 1999) completely unspecified. We                 empty box shows all the transitions in one probability space
show that such problems can be alleviated if one considers              and each row represents rules with identical effects, so the
probabilistic, rather than deterministic automata as a model            rules in the same row are expected to have the same probabil-
of verification strategies. As a result we can directly com-            ity (for example, in Row 2, reading BLUE CAR leads to either
pare semantic automata model with the quantifier verification           adding one Y to the stack or removing N from the top of the
model relying on Approximate Number System (ANS, Piet-                  stack; since N keeps track of non-blue cars and Y keeps track
roski et al., 2009). We believe that introducing the probabilis-        of blue cars, the rules in Row 2 lead to the same effect; simi-
tic version of the model is a first step towards a computational        larly for the other rows). The non-probabilistic variant could
cognitive model of quantifier processing.                               be expressed by stating that the second row in each block has
                                                                    2968

 probability 1, see also Fig. 2 for such a deterministic push-            counted. It also shows that the participants judging PQs be-
 down automaton.                                                          came less accurate as the ratio of the elements of the probed
                                                                          color and of another color got closer to 1 (e.g., the scenarios
             Testing probabilistic automata.                              8 vs. 7 and 9 vs. 8 were more difficult than the scenarios 9
 We tested whether PFSAs and PPDAs could model accuracy                   vs. 6 and 10 vs. 7).
 in verification of counting and proportional quantifiers. We                We now turn to the modeling of the verification of CQs
 applied PFSAs and PPDAs to the data collected in two exper-              (CQ4/5, CQ7/8) and PQs. The models implemented PFSAs
 iments discussed by Szymanik and Zajenkowski (2010b) and                 and PPDAs and captured, among other things, the descrip-
 Zajenkowski and Szymanik (2013).                                         tive observations just mentioned. The PFSAs and PPDAs
    In the experiments participants were asked to verify Polish           themselves were embedded in Bayesian hierarchical models,
 sentences with counting and proportional quantifiers against             which allowed us to include differences in individual partic-
 pictures showing cars in a car park. There was an added dif-             ipants’ responses in our analysis. All models were imple-
 ficulty of a working memory task in Exp. 1 (which we will                mented in JAGS (Plummer, 2003).
 ignore in our models, to keep our study manageable at this                  The model for the verification of CQs is summarized
 point).                                                                  graphically in Figure 5 (see Lee and Wagenmakers 2014 for
    The verification tasks consisted of simple propositions in            an introduction into graphical summaries of Bayesian mod-
 Polish containing a quantifier that probed a color feature of            els). The data to be estimated, kN,i (the number of correct
 cars on display, e.g., ‘Wi˛ecej niż połowa samochodów jest              responses given by a subject in each condition) were taken to
 niebieska’ (More than half of the cars are blue). The par-               come from the binomial distribution with n = 8 (the number
 ticipants were asked to decide if the proposition accurately             of items seen by each subject in each condition). The param-
 described the presented picture. They responded by pressing              eter θN, j is modeled as the addition of the fixed effect SUMN
 the button with the letters ‘p’ or ‘f’ if true or false, respec-         and the subject random effect coming from the normal distri-
 tively. (The letters refer to the first letters of Polish words for      bution with deviation σ. SUMN is the probability calculated
 ‘true’ and ‘false’.) In the first experiment the displayed car           using PFSA.
 park always contained 15 cars, while in the second experi-                  The PFSA we consider has three possibilities at each state
 ment the accompanying picture had either 15 elements or 17               when encountering the car of the probed color (BLUE CAR
 elements. Each picture contained objects in two colors.                  from now on): either it advances to the next state (the ‘cor-
    Four different quantifier types were used in the studies.             rect’ behavior), p01 and p11 in Figure 3, or it loops, p00 , p10 , or
 Here, we only consider data from three groups:                           it moves by two states, p02 in Figure 3. When the final state
                                                                          is sn+1 , or any higher state, the sentence more than n cars
1. PQ: proportional quantifiers (less than half, more than                are blue is accepted and fewer than n+1 cars are blue is re-
    half); PQs were studied in four different scenarios, which            jected, when the final state is lower than sn+1 , the situation is
    varied according to the number of elements of the probed              reversed. We assumed that the probability of pi1 is the same
    color vs. another color (9 vs. 6; 8 vs. 7; 10 vs. 7; or 9 vs.         in every probability space, and that the same is true for pi0 and
    8);                                                                   for pi2 . In other words, the PFSA has only three free parame-
                                                                          ters, pi1 –pi3 , irrespective of the value of i.
2. CQ4/5: counting quantifiers of relatively low rank (less
                                                                             The prior distribution of the parameters is the Dirichlet dis-
    than 5, more than 4); the number of the cars in the probed
                                                                          tribution, in which all the three parameters are equal. This
    color was maximally close to the criterion for validating or
                                                                          represents the fact we have no previous information that one
    falsifying the proposition;
                                                                          transition is more likely than the other ones.
3. CQ7/8: counting quantifiers of relatively high rank (less                 SUMN is the sum of all walks through our probability
    than 8, more than 7); the number of the cars in the probed            spaces that lead to the correct response given the number of
    color was maximally close to the criterion for validating or          blue cars and the CQ used. Different SUMs, SUM4/5 and
    falsifying the proposition.                                           SUM7/8 , were computed since the number of walks differ in
                                                                          case of CQ4/5 and CQ7/8.
    PQs were tested in 8 trials for each scenario type. Each CQ              The model simulating the verification of PQs was similar,
 type also appeared in 8 trials. 50% of the items were designed           the only important difference being that SUMN came from
 to be judged as true, the rest was false.                                the PPDA discussed above (see Table 1). Unlike in case of
    63 participants took part in Experiment 1, 99 participants            PFSAs, two types of transitions are estimated here: either
 took part in Experiment 2.1                                              p0 –p2 , covering the probability space of transitions when en-
    The descriptive summary of the data is presented in Figure            countering a blue car, or q0 –q2 , covering the probability space
 4. It shows that the accuracy in responses to CQs decreased              of transitions when encountering a non-blue car.2 SUMN dif-
 with the increase of the number of elements that had to be
                                                                              2 The model discussed here ignores   p3 and q3 , so the comparison
     1 The number of participants differed slightly from the original     between PFSAs and PPDAs is more straightforward (i.e., the same
 reports as some subjects were excluded from the analysis due to          number of transitions is assumed for each probability space). We
 missing values on other cognitive tasks studied therein.                 also ran the full model, which included p3 and q3 . The full model
                                                                      2969

                    p0       s0 , BLUE CAR, # → s0 , #                            s0 , BLUE CAR,Y → s0 ,Y                                       s0 , BLUE CAR, N → s0 , N
                    p1       s0 , BLUE CAR, # → s0 ,Y                             s0 , BLUE CAR,Y → s0 ,YY                                      s0 , BLUE CAR, N → s0 , ε
                    p2       s0 , BLUE CAR, # → s0 , N                            s0 , BLUE CAR,Y → s0 , ε                                      s0 , BLUE CAR, N → s0 , NN
                    p3       s0 , BLUE CAR, # → s0 ,YY                                                                                          s0 , BLUE CAR, N → s0 ,Y
                    q0       s0 , NON - BLUE CAR, # → s0 , #                      s0 , NON - BLUE CAR, N → s0 , N                               s0 , NON - BLUE CAR,Y → s0 ,Y
                    q1       s0 , NON - BLUE CAR, # → s0 , N                      s0 , NON - BLUE CAR, N → s0 , NN                              s0 , NON - BLUE CAR,Y → s0 , ε
                    q2       s0 , NON - BLUE CAR, # → s0 ,Y                       s0 , NON - BLUE CAR, N → s0 , ε                               s0 , NON - BLUE CAR,Y → s0 ,YY
                    q3       s0 , NON - BLUE CAR, # → s0 , NN                                                                                   s0 , NON - BLUE CAR,Y → s0 , N
                                                                                  s0 , ε,Y → s1 ,Y
                                        Table 1: Probabilistic push-down automaton for more than half of the cars are blue
                         Counting quantifiers                                         Proportional quantifiers                                         Means and SEs of CQs and PQs
               60
                                                                             60                                                                                             ●
                                                                                                                                                                                         ●
                                                                                                                                                     7.2
               40
                                                                                                                                     Means and SEs
                                                                             40                                           PQ_type
                                                          CQ_type                                                           PQ6:9
       count                                                CQ4/5    count                                                  PQ7:10
                                                            CQ7/8                                                           PQ7:8                    6.8    ●                                          ●
                                                                                                                            PQ8:9
               20                                                            20
                                                                                                                                                                                               ●
                                                                                                                                                     6.4            ●
               0                                                             0
                         2         4           6     8                            2           4            6         8                                     CQ4/5   CQ7/8   PQ6:9    PQ7:10    PQ7:8   PQ8:9
                              No_correct_answers                                            No_correct_answers                                                              Quantifier_type
                             Figure 4: Number of correct responses per subject and quantifier type, means and standard errors
                                 p0−2                                                                            creases in case of smaller differences between the number of
                                                          kN,i ∼ Binomial θN,j , 8
                                                                                                                elements compared in PQs.
                                                                                                                    To validate the models further, we sampled the posterior
  σ                                                       θN,j = φj + SUMN
                                                                                                                 predictive distribution using the simulations from the poste-
                               SUMN
                                                                                                                 rior density of σ and p1 –p3 (plus q1 –q3 in PPDAs). Table 2
                                                                                                                compares the means and 95% intervals of the actual responses
                                                          φj ∼ Normal 0, σ
                                                                                                                 (Row 1) and of the predictive distribution (Row 2). The 95%
  φj                             θN,j                                                                            intervals of the posterior predictive always included the ob-
                                                          σ=   √1
                                         j = 1 . . . 63          λ                                               served mean and did not underestimate or overestimate. This
                                                                                                                 is also apparent from the p-values (Row 3), which show that
                                                                                          
                                  kN,i                    λ ∼ Uniform 10, 10000                                  the simulations are not significantly different from the ob-
                                                                                                                 served means. (As is common, the p-values were calculated
                                  i = 1 . . . 252                                 1 1 1
                                                                                            
                                                                                                                 as proportions of simulations which are at least as extreme as
                                                          p0−2 ∼ Dirichlet        3, 3, 3
                                                                                                                 the observed mean.)
               Figure 5: Bayesian model for verification of CQs                                                     We conclude that our hierarchical Bayesian models can
                                                                                                                 successfully model the data of two experiments. In partic-
                                                                                                                 ular, the models correctly capture the fact that higher rank
fers depending on the number of blue and non-blue cars.                                                          CQs decrease accuracy, and the accuracy in verification also
   The posterior distributions of the most relevant parameters                                                   decreases when the ratio of sets compared by PQs is smaller.
are summarized in box-plots of Figure 6. The ‘correct’ tran-                                                     Since the information about number of elements and size of
sition (p1 , q1 ) is narrower in case of PFSA than in PPDA but                                                   sets enter only in PFSAs and PPDAs in our models, we con-
the 95% confidence interval of the PPDA posteriors included                                                      clude that PFSAs and PPDAs constitute valid approaches to
the mean of p1 of the PFSA and thus, we cannot conclude that                                                     modeling the verification of CQs and PQs.
the correct transition significantly differs between automaton
types. The box-plot of θ (probability of success) shows that                                                        Comparing semantic automata with ANS.
we correctly model that the success probability of binomial                                                      It is a plausible hypothesis that the observed variation in the
distribution is lower with CQs of higher ranks, and also de-                                                     data should not be modeled by probabilistic automata but by
did not show any significant divergences from the results discussed                                              probabilistic number estimations. One influential model in
here.                                                                                                            psychophysics is Approximate Number Sense, used to repre-
                                                                                                      2970

                                                        CQ4/5                                        CQ7/8                                      PQ6:9               PQ7:10               PQ7:8                PQ8:9
        Observed                                   6.79 [6.61 - 6.96]                           6.36 [6.15 - 6.55]                        7.46 [7.31 - 7.59] 7.40 [7.24 - 7.54]    6.58 [6.35 - 6.78]   6.77 [6.56 - 6.97]
        PFSA/PPDA                                  7.01 [6.75 - 7.28]                           6.41 [6.03 - 6.76]                        7.43 [7.22 - 7.61] 7.41 [7.19 - 7.59]    6.57 [6.25 - 6.87]   6.74 [6.42 - 7.02]
        p-values                                       p = 0.10                                     p = 0.81                                  p = 0.76              p = 0.91           p = 0.95             p = 0.83
        ANS                                                -                                             -                                7.38 [7.16 - 7.59] 7.36 [7.13 - 7.56]    6.61 [6.29 - 6.92]   6.81 [6.51 - 7.09]
        p-values                                           -                                             -                                    p = 0.49              p = 0.76           p = 0.83             p = 0.78
                                                                                                                                            Mean 95% interval
                                                                                                                                         w 0.06       [0.02 - 0.15]
Table 2: Summaries of the posterior predictive and the actual data and the Weber fraction of the ANS model; means (boldfaced),
95% intervals (in square brackets), and p-values
                                          p1 , q1                                                             σ
                                                                                                                                                       is underestimated.3 Its posterior distribution, shown in Table
              1.000                                                                    0.100             ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                                                                       2, is at odds with previous findings that French adults’s We-
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                                                                       ber fraction is .12 (Pica et al., 2004), i.e., on average French
              0.975
                                      ●
                                      ●
                                      ●
                                      ●
                                      ●
                                                                                       0.075                                                           adults can discriminate the ratio 9:8 but not finer ratios. In
                                                                                                                                                       contrast to that, our model would predict that on average par-
Probability                                                              Probability
                                      ●                                                                                          ●
                                      ●
                                      ●                                                                                          ●
                                      ●
                                      ●
                                      ●                                                                                          ●
                                                                                                                                 ●
                                      ●
                                      ●                                                                                          ●
                                                                                                                                 ●
                                      ●
                                      ●                                                                                          ●
                                                                                                                                 ●
                                      ●                                                                                          ●
                                                                                                                                 ●
                                                                                                                                 ●
              0.950                                                                    0.050                                                           ticipants can discriminate the ratio 15:14.
                                                                                                                                                          Unless Polish and French adults differ in their represen-
              0.925
                                                                                       0.025
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                                                 ●
                                                                                                                                 ●
                                                                                                                                 ●
                                                                                                                                                       tation of imprecise quantities (which we find extremely un-
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●                       ●
                                                                                                                                 ●
                                                                                                         ●
                                                                                                                                                       likely as ANS is know to be language-independent (see De-
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                                                                                                         ●
                          p1−PFSA           p1−PPDA
                                                 Type
                                                         q1−PPDA                                     Sigma−PFSA
                                                                                                                      Type
                                                                                                                             Sigma−PPDA
                                                                                                                                                       haene, 1999)), the estimation of w argues against ANS as a
                                                                                 θ                                                                     suitable model for quantifier verification. Therefore, even
                                                                                                                                                       though ANS explains accuracy data for the verification of
                                                                     ●
                                                                     ●                           ●
                                                                                                                                                       ‘most’ under time pressure, it is unlikely that ANS is em-
                                                                                                                                                       ployed in a similar way in the verification process for related
                                    0.9
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                             ●
                                                                     ●                           ●
                                                                                                                                                       quantifiers without time pressure.
                                                                                                                                     ●
                                                                                                                                                                        Discussion and Outlook
                                                                                                                  ●
                      Probability
                                                                                                                  ●
                                                          ●
                                             ●
                                             ●            ●
                                             ●
                                             ●            ●
                                                          ●
                                             ●
                                             ●            ●
                                                          ●
                                             ●            ●
                                                          ●
                                             ●
                                             ●
                                             ●
                                                                                                                                                       We have introduced the probabilistic semantic automata
                                    0.8
                                                                                                                                     ●
                                                                                                                                                       model. It extends a prominent computational approach from
                                                                                                                  ●
                                                                                                                  ●                                    logic and linguistics. In the paper, we have modeled two
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                                                                                                                       main natural language quantifier classes: counting and pro-
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                                                                                                                       portional quantifiers. Our method of turning semantic au-
                                           CQ4/5        CQ7/8      PQ6/9
                                                                                        Type
                                                                                               PQ7/10        PQ7/8              PQ8/9
                                                                                                                                                       tomata into probabilistic semantic automata can be straight-
                                                                                                                                                       forwardly applied to obtain a probabilistic verification model
Figure 6: Box-plots of posterior of the “correct” transition, σ                                                                                        for any natural language quantifier. Therefore, the paper pro-
and θ                                                                                                                                                  vides a new tool for the semantics of natural language.
                                                                                                                                                           Our experiments have shown that probabilistic semantic
                                                                                                                                                       automata can explain judgment accuracy in sentence-picture
                                                                                                                                                       verification experiments. Moreover, the probabilistic seman-
sent imprecise cardinalities and the comparison of quantities                                                                                          tic automata explain the distance effect in proportional quan-
without counting (see Dehaene, 1999). Recently, ANS has                                                                                                tifier verification: a decrease in verification accuracy as the
been used to model the verification of the quantifier ‘most’                                                                                           numerical distance between the two sets to be compared de-
(Lidz et al., 2011; Pietroski et al., 2009) under 200 and 150 ms                                                                                       creases. Furthermore, we have critically compared proba-
time pressure.We used the ANS model of Lidz et al. (2011)                                                                                              bilistic semantic automata model with the verification model
for the quantifiers more than half and fewer than half to learn                                                                                        based on number estimations and we have argued that ANS
whether our findings could be captured by this model. If this                                                                                          cannot consistently explain the verification process under-
was the case, the strength of our findings would be weakened                                                                                           lying the semantics of proportional quantifiers. Hence, the
as one could argue that there is no need for a specific semantic                                                                                       probabilistic semantic automata model seems to be a neces-
model of verification.                                                                                                                                 sary innovation in cognitive science. Additionally, the pre-
   The posterior predictive of the ANS model is summarized                                                                                             sented approach illustrates fruitful interaction between com-
in the fourth and fifth rows of Table 2. The predictions do                                                                                            putational cognitive modeling and more traditional disci-
not diverge significantly from the observed values or the val-                                                                                             3 The Weber fraction expresses the smallest numerical difference
ues of our PPDA. However, these good predictions come at a                                                                                             between two quantities that participants can distinguish. The Weber
cost: one free parameter of the model, the Weber fraction w,                                                                                           fraction of n1 vs n2 is calculated as (n1 − n2 )/n2 .
                                                                                                                                                   2971

plines: linguistics, logic, and formal semantics.                   Hopcroft, J. E., Motwani, R., and Ullman, J. D. (2000). In-
   As it always happens introducing a new perspective creates         troduction to Automata Theory, Languages, and Computa-
many further questions and research opportunities. In con-            tion. Addison Wesley, 2nd edition.
clusion let us briefly mention a few such themes that we find       Lee, M. D. and Wagenmakers, E.-J. (2014). Bayesian Cogni-
particularly exciting. The main goal of our modeling is to bet-       tive Modeling: A Practical Course. Cambridge University
ter understand cognitive resources underlying quantifier pro-         Press, Camgridge.
cessing. As we recalled in the introduction there is an ample       Lidz, J., Pietroski, P., Halberda, J., and Hunter, T. (2011).
psychological evidence in favor of semantic automata. For             Interface transparency and the psychosemantics of most.
instance, Zajenkowski and Szymanik (2013) have recently               Natural Language Semantics, 19(3):227–256.
argued that the cognitive mechanism of comparing in mem-
                                                                    McMillan, C. T., Clark, R., Moore, P., Devita, C., and Gross-
ory the cardinalities of two sets is crucial when it comes to
                                                                      man, M. (2005). Neural basis for generalized quantifier
the cognitive difficulty of the proportional judgements. How-
                                                                      comprehension. Neuropsychologia, 43:1729–1737.
ever, PPDAs seem to suggest that counting and not compar-
ison is the most important process explaining the accuracy          Peters, S. and Westerståhl, D. (2006). Quantifiers in Lan-
of verification. In fact, in our model it is even hard to dis-        guage and Logic. Clarendon Press, Oxford.
tinguish counting errors from memory slips. Therefore, to           Piantadosi, S. T. (2011). Learning and the language of
further explain the role of cognitive resources in quantifier         thought. PhD thesis, Massachusetts Institute of Technol-
processing we plan to bring probabilistic semantic automata           ogy, Cambridge, Massachusetts.
model more extensively to the lab by confronting its predic-        Pica, P., Lemer, C., Izard, V., and Dehaene, S. (2004). Ex-
tions with more complex behavioral data, like reaction times.         act and approximate arithmetic in an amazonian indigene
On the other hand, probabilistic semantic automata model              group. Science, 306:499–503.
poses also many new theoretical challenges: Do probabilis-          Pietroski, P., Lidz, J., Hunter, T., and Halberda, J. (2009).
tic semantic automata give rise to a new classification of nat-       The meaning of ’most’: semantics, numerosity, and psy-
ural language quantifiers (cf. van Benthem, 1986)? Can the            chology. Mind and Language, 24:54–85.
model be naturally combined with the modeling of the acqui-         Plummer, M. (2003). Jags: A program for analysis of
sition of quantifier meanings (see Gierasimczuk, 2007; Clark,         bayesian graphical models using gibbs sampling. In Pro-
2010; Piantadosi, 2011)? Clearly, such questions are beyond           ceedings of the 3rd International Workshop on Distributed
the topic of this paper but offer interesting venues to explore       Statistical Computing (DSC 2003). March, pages 20–22.
in future applications of probabilistic semantic automata to
                                                                    Rabin, M. O. (1963). Probabilistic automata. Information
cognition.
                                                                      and control, 6(3):230–245.
                     Acknowledgments                                Szymanik, J. (2009). Quantifiers in TIME and SPACE. Com-
                                                                      putational Complexity of Generalized Quantifiers in Natu-
JD acknowledges NWO Veni Grant 275.80.005. JS acknowl-                ral Language. PhD thesis, University of Amsterdam, Am-
edges NWO Veni Grant 639.021.232. The work of MZ was                  sterdam.
supported by a grant no. 2011/01/D/HS6/01920 funded by              Szymanik, J. and Zajenkowski, M. (2010a). Comprehension
the National Science Centre in Poland.                                of simple quantifiers. Empirical evaluation of a computa-
                                                                      tional model. Cognitive Science: A Multidisciplinary Jour-
                          References                                  nal, 34(3):521–532.
van Benthem, J. (1986). Essays in Logical Semantics. D.             Szymanik, J. and Zajenkowski, M. (2010b). Quantifiers and
   Reidel, Dordrecht.                                                 working memory. In Aloni, M. and Schulz, K., editors,
Clark, H. and Chase, W. (1972). On the process of com-                Amsterdam Colloquium 2009, Lecture Notes In Artificial
   paring sentences against pictures. Cognitive Psychology,           Intelligence 6042, pages 456–464. Springer.
   3(3):472–517.                                                    Zajenkowski, M., Styła, R., and Szymanik, J. (2011). A
Clark, R. (2010). On the learnability of quantifiers. In van          computational approach to quantifiers as an explanation for
   Benthem, J. and ter Meulen, A., editors, Handbook of               some language impairments in schizophrenia. Journal of
   Logic and Language, pages 909–922. Elsevier.                       Communication Disorders, 44(6):595 – 600.
Dehaene, S. (1999). The Number Sense: How the Mind Cre-             Zajenkowski, M. and Szymanik, J. (2013). Most intelligent
   ates Mathematics. Oxford University Press, USA.                    people are accurate and some fast people are intelligent:
                                                                      Intelligence, working memory, and semantic processing of
Gierasimczuk, N. (2007). The problem of learning the se-
                                                                      quantifiers from a computational perspective. Intelligence,
   mantics of quantifiers. In Ten Cate, B. and Zeevat, H., edi-
                                                                      41(5):456 – 466.
   tors, Logic, Language, and Computation, 6th International
   Tbilisi Symposium on Logic, Language, and Computation,
   TbiLLC 2005, volume 4363 of Lecture Notes in Computer
   Science, pages 117–126, Batumi, Georgia. Springer.
                                                                2972

