UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Typical use of quantifiers: A probabilistic speaker model
Permalink
https://escholarship.org/uc/item/10z1z670
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Author
Franke, Michael
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                     University of California

                       Typical use of quantifiers: A probabilistic speaker model
                                                 Michael Franke (m.franke@uva.nl)
                              Institute for Logic, Language and Computation, University of Amsterdam
                                 Science Park, Kruislaan 107, 1098 XG Amsterdam, The Netherlands
                             Abstract                                    judgements are crucially different in kind from scalar impli-
                                                                         catures. This allows him to explain away experimental evi-
   Many natural language quantifiers are classically associated
   with a stringent binary semantics, and similarly categorical          dence that might otherwise speak for a grammatical view of
   pragmatic enrichments. But much experimental data shows               scalar implicature (Chierchia, Fox, & Spector, 2012; Sauer-
   that intuitions about appropriateness of use seem to be more          land, 2012). Obviously, then, understanding what typicality
   fuzzy and more subtle, yet highly regular nonetheless. To ac-
   count for these gradient typicality judgements, I sketch a new        judgements are is also of theoretical significance.
   probabilistic model of Gricean speakers that incorporates a              Taking sides with the integrated view informally ex-
   gradient notion of utterance alternatives. Focusing on scalar         pounded by D&T, I present a probabilistic production model
   quantifier some, the model is a proof of concept, against ex-
   pressed views to the contrary, that typicality and scalar infer-      that aims at explaining typicality judgements on a par with
   ences can be treated on a par.                                        scalar implicatures in a Gricean spirit. The model presented
   Keywords: pragmatics of natural language; Gricean reason-             here is a conservative extension of recent Bayesian models of
   ing; Bayesian cognitive modeling; game theory; alternatives           pragmatic reasoning as social cognition (e.g. Frank & Good-
                                                                         man, 2012; Goodman & Stuhlmüller, 2013). The formal ad-
                          Introduction                                   ditions are (i) the integration of a richer context model, bor-
Classically, the meaning of quantifiers is described in terms            rowed from game theory, that allows for more flexibility in
of clearcut binary truth conditions (e.g. Barwise & Cooper,              modeling the implicit question under discussion, i.e., what
1981; Peters & Westerståhl, 2006). For example, the sentence            counts as relevant to a linguistic choice, and (ii) a gradient
schema “Some of the As are Bs” is true just in case there is at          notion of salience of alternative expressions. The latter ex-
least one A that is also a B. On top of that, it is widely held that     tension is the most important one: whereas previous models
the scalar quantifier some, if used in the appropriate contexts,         of pragmatic reasoning look at a single fixed set of alternative
conveys a scalar implicature, roughly, that some but not all of          expressions that compete in production, the present model al-
the As are Bs (c.f. Grice, 1975; Levinson, 1983). Again, this            lows for weaker or stronger activation of different alternatives
is usually treated as a categorical affair: in a given context an        and shows one possibility of integrating such a gradient no-
utterance either does or does not have an implicature.                   tion of alternativeness in a comprehensive production model.
   This beautiful picture, alas, appears to be too coarse-                  More concretely, the model formalizes and tests the idea
grained. A large body of psychological literature has demon-             that typicality ratings reflect pragmatic appropriateness, in
strated that subjects’ use and interpretation of quantifiers is          particular subjects’ considerations as to whether the quanti-
strikingly regular but also rather fuzzy in manifold ways (c.f.          fier some is a good lexical choice in a description of the pre-
Hörmann, 1983; Moxey & Sanford, 1993). Two recent stud-                 sented situation. To determine whether a description is prag-
ies by Degen and Tanenhaus (2011, to appear) and van Tiel                matically well-chosen, a comparison with alternative choices
(2014, to appear) showed that judgements of acceptability of             is needed. I submit that subjects assess various alternatives to
sentences like “Some of the As are Bs” vary systematically               some with a variable latent probability that will be estimated
but smoothly with the size of the set of As that are Bs (hence-          from the observed data. Pragmatic appropriateness of an en-
forth: the target set). Whereas these sentence are atypical              tertained alternative is determined based on its interpretation
descriptions when the target set size is small or when it ap-            by an imagined listener. This presupposes an implicit goal,
proaches its maximum (i.e., the total number of As), this is not         and so I suggest that subjects rate a quantified sentence “Q
expected under the standard categorical picture (see Figure 1            of the As are Bs” based on how good an answer this is to the
and the following section for more on typicality judgements).            question under discussion “How many As are Bs?”
   It is controversial what empirically measured typicality                 The paper is structured as follows. The next section intro-
judgements reflect. Degen and Tanenhaus (D&T) view the                   duces the relevant experimental approaches to typicality of
attested gradient patterns as evidence for a probabilistic ac-           some. After that, I introduce the Bayesian speaker model by
count of pragmatic interpretation. According to their favored            first motivating a parameterized representation of the ques-
constraint-based approach, multiple factors contribute to the            tion under discussion and then spelling out the probabilis-
probability with which a listener will draw a pragmatic infer-           tic production model on top of it. Subsequently, I show that
ence. From this point of view, gradient typicality judgements            the model yields a satisfactory explanation of the data from
result from a fuzzy pragmatic interpretation process, of which           D&T’s and vT’s experiments. The fitted model suggests that
scalar implicature calculation is a part. Unfortunately, D&T             indeed different a priori natural alternatives to some are vari-
do not offer a concrete model with which to corroborate this             ably salient lexical competitors. However, as elaborated in
position. In contrast, van Tiel (vT) maintains that typicality           the concluding section, these results must be relativized to a
                                                                     487

number of modeling choices, some of which should be chal-                                            1                                        Summa vT
                                                                           mean typicality rating
lenged by future work.                                                                                                                       Summa D&T
                                                                                                                                             theor. expect.
                    Typicality of some
                                                                                                    0.5
Independently of each other, D&T and vT probed into the
typicality structure of the quantifier some by having partici-
pants rate the intuitive typicality of a statement like “Some of
                                                                                                     0
the As are Bs” in connection with pictures that differed with
respect to the cardinality of the target set. Here is just a brief                                        0       5         10
description of the most important details of their respective                                       number of elements in target set A ∩ B
experiments (see the original papers for further details).
   Participants in vT’s experiment saw pictures of ten circles             Figure 1: Mean typicality ratings of “Some of the As are Bs”
in total, some number of which were black, the others white.               for different target set sizes in different experiments (see main
They rated the sentence “Some of the circles are black” on                 text). The gray line gives the expected applicability under
a 7-point Likert scale ranging from “bad” to “good” to an-                 standard linguistic theory (for | A | = 14).
swer the question “How well is the picture described by the
sentence?” Judgements were gathered from 30 participants
via Mechanical Turk, where in each session each participant                a description of the presented picture. This presupposes an
judged the critical sentence for all target set sizes in random            implicit goal-structure (appropriate for what?). I submit that
order without interruption by filler material.                             subjects assume that the implicit question under discussion
   In contrast, D&T first showed participants a gumball ma-                is “What is the cardinality of the target set?” with varying
chine with 13 gumballs. After playing a sound, a variable                  degrees of required precision in the answer. Judgements of
number of these 13 gumballs was shown as dispensed and                     pragmatic appropriateness also require a comparison to other
participants were asked to rate the sentence “You got some                 potential descriptions.
of the gumballs” on a 7-point Likert scale ranging from “un-                  Signaling games (Lewis, 1969) provide a rich formal
natural” to “natural.” Additionally, D&T also included an                  model of a context of utterance that includes explicit goals
8th option of choosing “false” on a button separated from                  and choice alternatives. A signaling game captures the inter-
the rating scale. Still, in their analyses the data was treated            action between an informed speaker and an uninformed lis-
as if obtained from an 8-point Likert scale with “false” an-               tener. The speaker observes a state, but the listener does not.
swers coded as lowest. Moreover, D&T ran four versions                     The speaker sends a message with a fixed conventional mean-
of their experiment. For one, they elicited typicality rat-                ing and the listener guesses which state is actual by reasoning
ings separately for sentences with and without the partita-                about the speaker’s motivations for choosing the message in
tive construction some of the, henceforth summa vs. plain                  question over alternative options. Correct guesses are com-
some. For another, while all experiments did include fillers,              municative successes and result in high payoffs for cooper-
they either did or did not include numerical expressions “You              ative interlocutors in the sense of Grice (1975), while false
got one/two/three . . . of the gumballs.” Taken together, there            guesses are failures and yield low payoffs. But when there
were then four experimental versions, which I will refer to                are many possible world states and absolute precision is not
as “Some NoNum”, “Some Num”, “Summa NoNum” and                             strictly necessary, we might also allow for a gradient notion
“Summa Num” in the following. There were 120 and 240 par-                  of communicative success (c.f. Jäger & van Rooij, 2007). If
ticipants for the Num and NoNum experiments respectively,                  t is the actual state and t 0 is the listener’s chosen interpreta-
but the number of judgements collected differed between tar-               tion, then the received payoff is a function of the similarity of
get set sizes (see original paper for details).                            t and t 0 . Depending on how exactly a measure of similarity
   Figure 1 shows the mean normalized typicality ratings                   is mapped onto payoffs, we can capture the idea that almost
from vT’s study (“Summa vT”) and the “Summa NoNum”                         guessing the right state may be an almost perfect outcome,
version of D&T. The general pattern is shared also with the                while farther deviations are increasingly bad.
other versions of D&T’s experiments: the quantifier some is
rather atypical in connection with empty sets, but also with               States & utilities. Applying this idea to the stimuli of the
smaller quantities; typicality ratings peak at slightly below              experimental cases at hand, the states of the game are the re-
half of the maximal cardinality; typicality judgements drop                spective cardinalities of the target set. The maximal number
for larger set sizes, albeit not as steeply as towards smaller             of black balls in vT’s experiment was 10. The maximal num-
set sizes.                                                                 ber of gumballs in D&T’s experiment was     13. Consequently,
                                                                           we get eleven or fourteen states T = t0 ,t1 , . . . ,t10/13 as
Context model: goals & expression alternatives                             the possible information states of the speaker. The idea that
I propose to explain intuitive typicality judgements as reflec-            communication is successful proportional to how close the
tions of pragmatic appropriateness of the given sentence as                listener’s guess matches the actual cardinality can be made
                                                                     488

precise in many ways. For simplicity, I follow Jäger and                  meaning of messages, chooses each true interpretation with
van Rooij (2007) and use a simple one-parameter utility func-              equal probability:
tion in the vein of Nosofsky (1986) (here and below, free pa-
rameters are notationally separated by a semicolon):                                           PLL (t | m) = U (t | m is true in t) ,
                 U(tx ,ty ; π) = exp −π · (x − y)2 .
                                                                          where U the uniform distribution (over state set T ).
                                                                              Given a belief in a literal listener, the sender’s expected
The parameter π captures what level of precision is relevant:              utility for sending m in state t is:
as π goes to infinity, interlocutors want to identify states pre-
                                                                                      EUS (m | t ; π) = ∑ PLL (t 0 | m) · U(t,t 0 ; π) .
cisely, but for lower values of π further deviations from the
                                                                                                             t0
actual state will also count as sufficiently successful. For ex-
ample, with π = 30.25 (roughly the mean of the estimated                   A rational speaker would choose only messages that maxi-
posteriors, see below) utilities come out as:                              mize expected utility. If the context affords precise commu-
                                                                           nication (π → ∞), rational choices under a belief in literal
       U(t0 ,t0 ) = 1                 U(t0 ,t1 ) = 7.29e − 14              interpretation obeys Grice’s Quantity Maxim of choosing the
       U(t0 ,t2 ) = 2.82e − 53        U(t0 ,t3 ) = 5.79e − 119             most informative expression. In this sense, we are assuming
                                                                           a generalization of a Gricean speaker who seeks to maximize
       U(t0 ,t4 ) = 6.33e − 211             ...
                                                                           the informativity of his utterances.
                                                                              Speakers may occasionally fail to choose optimally and
Alternative expressions. Next to the interlocutors’ com-                   be maximally informative. Still, even with errors, slips and
municative goals, another crucial ingredient of a conversa-                limited computational resources, speakers’ choice probabili-
tional context model is a specification of alternative expres-             ties can be expected, on average, to be proportional to their
sions that enter into pragmatic reasoning. Normally, game                  expected utility. A handy implementation of such utility-
theoretic or Bayesian models assume that a handful of con-                 proportional choice is the soft-max rule (Luce, 1959; Sutton
crete alternative expressions compete with each other. But it              & Barto, 1998), which here takes the form:
seems more plausible to consider a large enough base set of
potentially entertained alternative expressions, backed up by                                                   exp(λ · EUS (m | t ; π))
                                                                                   PS (m | t, X ; λ, π) =                                   ,   (1)
a probabilistic measure of how likely each subset of expres-                                                  ∑m0 ∈X λ · EUS (m0 | t ; π))
sions is actually entertained when speakers assess the prag-
matic appropriateness of a particular expression. The latent               where X ⊆ M is the subset of alternative expressions that the
degree of salience should be estimated from the data, not                  speaker takes into account. The parameter λ in Equation (1)
hand-picked by the modeller. This is the approach I would                  controls for the speaker’s rationality in the sense that with
like to take here.                                                         λ → ∞, choices adhere to the standards of pure rationality,
                                                                           while with λ → 0, choices become entirely random.
   As for the base set of alternative expressions to some, I
                                                                              Speakers may not take all conceivable alternative expres-
suggest to consider its obvious scale mates all and none, but
                                                                           sions into consideration equally. The probability of choosing
also small numerals within the subitizing range one, two and
                                                                           a message for a given state is therefore obtained by weighing
three, of which it is prima facie plausible that they compete
                                                                           in the probability that X is entertained:
with some when describing a scene of, say, two out of ten
black circles. As for larger cardinalities, it may seem simi-                     PS (m | t ; λ,~s) =         P(X |~s) · PS (m | t, X ; λ, π) .
larly reasonable to consider most and many as potential alter-
                                                                                                          ∑
                                                                                                         X⊆M
natives as well. Numerals are given a precise semantic inter-
pretation, quantifier some its usual logical semantics, while              On the simplifying assumption that the salience probabilities
most and many receive simple proportional semantics: most                 ~s = s1 , . . . , s| M | of entertaining individual messages are in-
receives a more than 1/2-interpretation, while many receives a             dependent, the probability of entertaining a set X ⊆ M of al-
more than 3/4-interpretation.1                                             ternative expressions is:
            A probabilitic production model                                                      P(X |~s) =   ∏ si · ∏ 1 − si .
                                                                                                             mi ∈X   mi 6∈X
Following recent models of pragmatic reasoning (Benz &
van Rooij, 2007; Franke, 2011; Frank & Goodman, 2012;                                 Linking function for ordinal data
Goodman & Stuhlmüller, 2013; Jäger, 2013), we consider a                 The production model defined above can be used to predict,
speaker’s approximately optimal language use, based on the                 for each cardinality of the target set, how likely a speaker
assumption that the listener interprets messages literally. A              would use the description “Some of the As are Bs.” However,
(hypothetical) literal listener, who only acts on the semantic             the relevant data on typicality judgements is ordinal data ob-
    1 The chosen alternatives and their semantics (especially for most     tained from Likert-scale rating tasks. Instead of assuming flat
and many) are clearly questionable in principle, but picked here also      out that the typicality data can be treated as interval-level data
with practical considerations in mind. See concluding remarks.             (like both vT and D&T did), it is more appropriate to fit the
                                                                       489

model to the ordinal data directly. One option for doing this                   and many (with the semantics assumed here) are strong com-
is to use a probit linking function, like in ordinal probit re-                 petitors to some. On the other hand, high levels of salience of
gression (see Kruschke 2011, Chapter 21).                                       alternatives none, one and all are likely, given the model and
   The probit linking model entertains a vector of non-                         the data.
decreasing thresholds ~θ = θ0 , . . . , θ| D | , with | D | the num-                Furthermore, there is quite some interesting variation be-
ber of items on the rating scale, that determine the relative                   tween experiments. Firstly, as vT’s experiment provided far
sizes of the rating categories. Only θ1 , . . . θ| D |−1 are free pa-           fewer data points, the 95% HDI intervals tend to be bigger.
rameters, while θ0 = −∞ and θ| D | = ∞. The latent categorical                  Secondly, vT’s data is suggestive of higher levels of salience
choice probability PS (m | t, X ; λ, π) is perturbed by Gaussian                of most alternatives. This could be due to the repetitive na-
noise with standard deviation σ. The probability of choosing                    ture of the experiment, where no fillers where included and
degree d j of the ordinal rating scale is the likelihood that the               subjects only judged the critical sentence “Some of the As
noise perturbed value falls in the interval (θ j−1 , θ j ). Taken               are Bs.” Another possibility is that some occurred in subject
together, the likelihood function is:                                           position in vT’s experiments, but in object position in D&T’s.
                                                                                Thirdly, when we compare results for D&T’s experiments, we
  PS (d j | m,t ; λ, π,~s,~θ, σ) =                                      (2)     notice that the summa construction seems to be more strongly
                                                                                associated with the all alternative than the some construction.
                        Z θj
        ∑    P(X |~s) ·         N (x | PS (m | t, X ; λ, π), σ)dx ,             However, in the “Num”-type experiments where additional
       X⊆M                θ j−1
                                                                                numerical expressions were interspersed between critical tri-
where N (x | µ, σ) is the probability density function of a nor-                als, the salience of all seems heavily reduced. Finally, there
mal distribution with mean µ and standard deviation σ.                          is further interesting variation in the estimated salience levels
                                                                                of alternative one. As expected, the presence of number ex-
                    Parameter estimation                                        pressions raised the salience of this expression. But, on top of
The speaker model has 9 free parameters: λ (rationality), π                     that, the increase of salience between “Num” and “NoNum”
(precision), and one salience probability parameter si for each                 versions seems much higher for the partitative summa con-
of the seven alternatives to some that we chose to include.                     struction. We could speculate that this might be due to the
(We should naturally assume ssome = 1.) The linking func-                       fact that the included numerical expressions were partitative
tion comes with additional free parameters: σ (linking noise)                   constructions as well (e.g., one of the). However, all of the
and | D | − 1 threshold parameters θi that determine the rela-                  above remarks must be taken with a grain of salt, as they are
tive sizes and positioning of the | D | ordered categories of the               not based on stringent statistical comparison and are likely
rating scale. Priors were chosen as follows:                                    to be eventually influenced by more realistic modeling (see
                                                                                discussion in conclusion).
    λ ∼ U (0; 30)         π ∼ U (0; 100)                  si ∼ U (0, 1)
    σ ∼ U (0; .4)        θi ∼ N    (i/| D |, 4 · | D |) .                                               Model validation
                                                                                There is no other formalism that predicts typicality ratings
Uniform priors were chosen in order to remain uncommitted,                      to compare the presented model to, but to at least provide a
except for determining a credible range. The priors on the                      crude measure of goodness-of-fit Figure 3 plots the means
thresholds of the linking model reflect the prior assumption                    of 5000 samples from the posterior predictive distribution
that the degrees would be roughly evenly spaced along the                       against the observed data. Each dot in the graph corresponds
unit interval, but allow for significant deviation. Priors also                 to a pair of predicted and observed numbers of choices of a
encoded the ordering of thresholds.                                             degree on the relevant rating scale for each target set cardi-
   We are interested in the joint posterior likelihoods of these                nality. Correlation coefficients between posterior predictions
parameters separately for each of the five experiments. Pos-                    and observations are summarized in Table 1. From this the
teriors were estimated with MCMC sampling using JAGS
(Plummer, 2003). Two chains of 5000 samples were gath-                                                some                   summa
ered for each experiment with a thinning rate of 2 after an                                     NoNum       Num      NoNum       Num     vT
initial burn-in of 2500.                                                                   r    .980        .988     .974        .990    .973
   The most interesting part is the estimated posteriors over
salience degrees si of competing alternatives in different ex-                  Table 1: Pearson’s r of posterior predictions and observations.
periments (see Figure 2).2 Strikingly, if the model is correct,
the data offers little support for the idea that two, three, most
                                                                                model’s posterior predictions seem reasonable but not per-
    2 Estimates  of the posteriors for other parameters, especially             fect. We should bear in mind though, that these results are
those of the linking function, bear no surprises and are also not of
great conceptual interest. There were no noteworthy divergences in              olds θi for D&T’s data show a bigger gap between the first and the
posterior credence for values of parameters λ, π and σ between ex-              second threshold than any other pair of adjacent thresholds. This
periments. Linking noise σ is estimated as slightly higher for vT’s             is likely a result of the distinct category of the lowest rating degree
data, which is not surprising given that the scale used in this exper-          (“false”, as opposed to “unnatural”) which was presented as a sepa-
iment had one degree less. Moreover, the estimated linking thresh-              rate option outside of the Likert-scale.
                                                                            490

                                                                                                                              si
                                                                      1.00
                                                                      0.75
                                                                                                                                                                                                                                                   experiment
                                                                                                                                                                                                                                                                Some NoNum
                                                                                                                                                                                                                                                                Some Num
                                                                      0.50
                                                                                                                                                                                                                                                                Summa NoNum
                                                                                                                                                                                                                                                                Summa Num
                                                                                                                                                                                                                                                                Summa vT
                                                                      0.25
                                                                      0.00
                                                                                         none     one            two         three     most                    many                                          all
Figure 2: Posterior estimates over salience degrees of alternatives some. The plot shows the means of the marginalized posterior
likelihoods for each alternative expression (barplot), together with 95% HDI intervals (error bars).
                                                                                                        ●
                                                                                                    ●
                                                                                                                                                                                                                                                                                      ●           ●       ●
                                                                                                                                                                                                                                                                                      ●       ●       ●
                                                                                                                                                                                                                                                           ●                      ●
                                                                                                                                                                                                                                     ●                                  ●    ●●
               100                                                                                          experiment                                                                                                                                 ●                ●
                                                                                                                                                                                                                                                                                                              experiment
                                                                                                                                                          20                                                                                       ●
                                                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                                               ●●   ●
                                                                                                                                                                                                                                                                            ● ●●
                                                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                                                             ●
                                                                                                                                                                                                                                                                                          ●
                                                                                                                 Some NoNum                                                                                                                                                                                        Some NoNum
 observation                                                                                                                                observation
                                                                                                             ●                                                                                                               ●             ●
                                                                                                                                                                                                                                           ●       ●            ●   ●●
                                                                                                                                                                                                                                                                     ● ●                                       ●
                                                                                     ●                                                                                                                                                         ●                                  ●
                                                                                 ●                                                                                                                                           ●        ● ●●●● ● ●
                                                                                                             ●
                                                                                                                 Some Num                                                                                                        ●● ● ●●               ●
                                                                                                                                                                                                                                                                                                               ●
                                                                                                                                                                                                                                                                                                                   Some Num
                                                                                                                                                                                                                    ● ●      ●       ● ●       ●       ●
                                                                                 ●                           ●
                                                                                                                 Summa NoNum                                                                           ●     ● ●         ●
                                                                                                                                                                                                                    ●●● ● ●
                                                                                                                                                                                                                  ● ●
                                                                                                                                                                                                                             ●   ●
                                                                                                                                                                                                                          ● ●●●● ●●
                                                                                                                                                                                                                                           ●       ●                                                           ●
                                                                                                                                                                                                                                                                                                                   Summa NoNum
                                                                            ●●
                                                                      ●●                                                                                                                              ● ●●
                                                                                                                                                                                                     ●●  ●       ●
                                                                                                                                                                                                                 ●●
                                                                                                                                                                                                              ● ●● ●●
                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                    ●● ●● ● ●
                50                                       ●
                                                                 ●
                                                                 ●
                                                                  ●
                                                                                                             ●
                                                                                                                 Summa Num                                10                                 ●         ●
                                                                                                                                                                                                       ●     ● ●●●●●● ●●
                                                                                                                                                                                                                 ●●                                                                                            ●
                                                                                                                                                                                                                                                                                                                   Summa Num
                                                                                                                                                                                      ●      ●   ●
                                                                                                                                                                                                 ●●
                                                                                                                                                                                             ●●● ●         ●●
                                                                                                                                                                                                            ●●● ●● ● ●●
                                                    ●        ●
                                                   ●                                                                                                                                ● ● ●●
                                                                                                                                                                                    ●            ●
                                                                                                                                                                                                 ●    ●
                                                                                                                                                                                                      ●● ●    ●   ●●     ●
                                                      ●● ●
                                                         ●
                                                                                                             ●
                                                                                                                 Summa vT                                                       ●● ● ● ●      ●
                                                                                                                                                                                              ●●●●● ●
                                                                                                                                                                                             ●●     ●● ●             ●
                                                                                                                                                                                                                                                                                                               ●
                                                                                                                                                                                                                                                                                                                   Summa vT
                                                    ●●
                                        ●        ● ●●
                                                    ● ●                                                                                                                               ●●
                                                                                                                                                                                 ●● ● ●
                                                                                                                                                                                ●●      ●
                                                                                                                                                                                        ●
                                                                                                                                                                                        ●●●
                                                                                                                                                                                          ● ●● ● ●           ●
                                               ●
                                                 ●
                                                ●●
                                                 ●
                                                 ●
                                                 ● ●●
                                                  ●● ●
                                                                                                                                                                           ●●● ●
                                                                                                                                                                             ● ●
                                                                                                                                                                               ●●
                                                                                                                                                                                ●●●●●● ●
                                                                                                                                                                                     ●●●
                                                                                                                                                                                       ●●
                                                                                                                                                                                        ● ●● ● ● ● ●                         ●
                                      ● ●● ●● ●●●●
                                              ●
                                        ● ●●●●●●
                                              ●                                                                                                                          ●●
                                                                                                                                                                         ● ●●●●●
                                                                                                                                                                               ●
                                                                                                                                                                               ●●●
                                                                                                                                                                                 ●
                                                                                                                                                                                 ●●● ●●
                                                                                                                                                                                   ● ● ●
                                                                                                                                                                                       ●●●
                                                                                                                                                                                         ●●
                                                                                                                                                                                         ● ● ●
                                   ● ● ●●●●
                                          ●● ●
                                            ●●●
                                    ●●● ●
                                       ●●
                                       ●                                                                                                                             ●●●
                                                                                                                                                                       ●●●● ● ●
                                                                                                                                                                          ●●● ●
                                                                                                                                                                              ●●●
                                                                                                                                                                                ●
                                                                                                                                                                                ●●●●
                                                                                                                                                                                   ●●●● ●        ●
                              ●●●●●●
                                  ●
                                     ●
                                    ●●
                                     ●
                                      ●
                                      ●
                                      ●
                                      ●●●●
                                        ●●
                             ●
                             ●●●●
                                ●
                                ●●
                                 ●
                                 ●●
                                  ●
                                  ●●●
                                   ●
                                   ●●
                                   ●
                                   ●●●
                                     ●
                                     ●
                                     ●●                                                                                                                            ●
                                                                                                                                                                   ●●●●
                                                                                                                                                                     ●●●
                                                                                                                                                                       ●●
                                                                                                                                                                        ●●
                                                                                                                                                                         ●●●●●
                                                                                                                                                                           ●● ●●
                                                                                                                                                                              ● ●●         ● ●       ●●             ●
                          ● ●
                            ●
                            ●●
                             ●
                             ●●
                              ●●●
                                ●
                                ●●
                                 ●
                                 ●
                                 ●
                                 ●●
                                  ●
                                  ●●
                         ●
                         ●
                         ●●
                          ●
                          ●●
                           ●●
                            ●●
                             ●
                             ●●
                              ●
                              ●
                              ●●●
                                ●●
                                 ●●
                        ●
                        ●
                        ●●
                         ●
                         ●
                         ●
                         ●●
                          ●
                          ●
                          ●●
                           ●
                           ●
                           ●●
                           ●
                           ●●
                            ●
                            ●●
                             ●
                             ●●
                              ●
                              ●●●●●                                                                                                                            ●●●●
                                                                                                                                                                 ●●●  ●
                                                                                                                                                                      ●
                                                                                                                                                                   ●●●●●
                                                                                                                                                                       ●
                                                                                                                                                                       ●●
                                                                                                                                                                        ●●●●●     ●
                                                                                                                                                                            ● ●●● ●
                       ●●
                        ●
                        ●
                        ●●
                         ●●
                          ●●
                           ●
                           ●●●
                 0   ●
                     ●
                     ●
                     ●
                     ●
                     ●
                     ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                      ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                       ●
                        ●
                        ●
                        ●
                        ●
                       ●●
                        ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                         ●
                          ●
                          ●
                          ●●
                           ●
                           ●●●
                             ● ●
                                                                                                                                                          0    ●
                                                                                                                                                               ●
                                                                                                                                                               ●●
                                                                                                                                                                ●
                                                                                                                                                                ●
                                                                                                                                                                ●●
                                                                                                                                                                 ●
                                                                                                                                                                 ●●
                                                                                                                                                                  ●
                                                                                                                                                                  ●
                                                                                                                                                                  ●●●
                                                                                                                                                                  ●●
                                                                                                                                                                   ●●
                                                                                                                                                                    ●
                                                                                                                                                                   ●●
                                                                                                                                                                    ●●
                                                                                                                                                                     ●●
                                                                                                                                                                      ●
                                                                                                                                                                      ●
                                                                                                                                                                      ●●
                                                                                                                                                                       ●  ●●●
                                                                                                                                                                       ●● ●     ●
                     0                                                50                    100                                                                0                                                  10                                                    20
                                                                           prediction                                                                                                                                    prediction
Figure 3: Prediction-observation plots. See main text for description. The gray lines are the diagonals, included for orientation.
The right plot zooms in on the low number region and also includes the best linear fits (for the low-number subset).
based on the full posterior predictive distribution, not just on                                                                           sidered preliminary at best. The presented model contains a
the maximum likelihood estimates and that we attempted to                                                                                  few simplifying assumptions that should be scrutinized more
predict exact numbers of choices of each Likert-scale item.                                                                                carefully. Lifting these assumptions may change at least the
                                                                                                                                           quantitative results obtained. The most striking simplifying
                                                                                 Conclusion                                                assumptions are: (i) a small stipulated set of potential alter-
The main theoretical contribution here is a proof of concept                                                                               natives, (ii) independence of salience of alternatives, and (iii)
that it is possible to explain graded typicality judgements and                                                                            uniform payoff structure over varying cardinalities.
scalar implicatures with a unified Gricean speaker model. To
do so, the model extended existing approaches to pragmatic                                                                                    Ad (i). It may appear unmotivated to fix the rather small set
reasoning as social cognition by including (i) a pragmatic pre-                                                                            of potential alternatives that I considered here. What about
cision parameter that regulated how strictly the question un-                                                                              larger numerals, or expressions like a few, almost all or more
der discussion (“How big is the target set?”) needs to be an-                                                                              than half ? My choice of alternatives here has been guided en-
swered; and (ii) a gradient notion of alternativeness. If the                                                                              tirely by practical considerations. Firstly, vague expressions
model is true, the data supports the conclusion that none, one                                                                             like a few and almost all do not have a clearcut uncontro-
and all are the most serious competitors, while two, three,                                                                                versial semantics (the same does hold for many). Secondly,
most and many appear less strongly associated with some.                                                                                   their inclusion would, if anything, have led to tighter predic-
   Still, there are reasons why these conclusions must be con-                                                                             tive fit (with more uncertainty in the posteriors), because there
                                                                                                                                     491

would have been more free parameters. The same applies to               matic reasoning in language games. Science, 336(6084),
the inclusion of larger number terms. Essentially, we should            998.
not even try to justify any armchair choice, but to look for         Franke, M. (2011). Quantity implicatures, exhaustive inter-
an empirical measure of gradient alternativeness. The modest            pretation, and rational conversation. Semantics & Pragmat-
purpose here was to provide a modeling framework in which               ics, 4(1), 1–82.
to make sense of such future data.                                   Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge
   Ad (ii). The model assumes that whether a speaker is aware           and implicature: Modeling lanuage understanding as social
of alternative two is independent of whether she is aware of            cognition. Topics in Cognitive Science, 5, 173–184.
one and three. This is unintuitive, but it is difficult to model     Grice, P. H. (1975). Logic and conversation. In P. Cole &
potential dependencies in this sense. Eventually the modeling           J. L. Morgan (Eds.), Syntax and semantics, vol. 3, speech
of salience in alternatives should be corroborated anyway by            acts (pp. 41–58). Academic Press.
other empirical measures of lexical association.                     Hörmann, H. (1983). Was tun die Wörter miteinander im
   Ad (iii). The model also assumes, again counterintuitively,          Satz? oder wieviele sind einige, mehrere und ein paar?
that the utility structure is uniform. Similarity between tx and        Göttingen: Verlag für Psychologie, Dr. C.J. Hogrefe.
ty only depends on the differences between x and y, not on           Jäger, G. (2013). Rationalizable signaling. Erkenntnis.
their absolute values. This might not respect the intuitive          Jäger, G., & van Rooij, R. (2007). Language structure: Psy-
and empirically attested perceptual similarity ratings between          chological and social constraints. Synthese, 159(1), 99–
pairs of cardinalities: low cardinalities that differ by a fixed        130.
amount appear less similar to each other, than higher cardinal-      Kaufman, E. L., Lord, M. W., Reese, T. W., & Volkmann, J.
ities that differ by the same amount (e.g. Logan & Zbrodoff,            (1949). The discrimination of visual number. The Amer-
2003). In general, it seems clear that future work in this di-          ican Journal of Psychology, 62(4), 498–525. Retrieved
rection should try to combine models of pragmatic reasoning             from http://www.jstor.org/stable/1418556
with models of (approximate) number sense and size estima-           Kruschke, J. E. (2011). Doing Bayesian data analysis.
tion (e.g. Kaufman, Lord, Reese, & Volkmann, 1949; Atkin-               Burlington, MA: Academic Press.
son, Campbell, & Francis, 1976).                                     Levinson, S. C. (1983). Pragmatics. Cambridge, UK: Cam-
                                                                        bridge University Press.
                     Acknowledgments                                 Lewis, D. (1969). Convention. a philosophical study. Cam-
                                                                        bridge, MA: Harvard University Press.
I would like to thank Judith Degen and Bob van Tiel for shar-
                                                                     Logan, G. D., & Zbrodoff, N. J. (2003). Subitizing and sim-
ing their data with me and for many helpful comments, as
                                                                        ilarity: Toward a pattern-matching theory of enumeration.
well as Jakub Dotlaĉil, Mike Frank, Noah Goodman, Dan
                                                                        Psychonomic Bulletin & Review, 10(3), 676–682.
Lassiter, Roger Levy and Jakub Szymanik for discussion.
                                                                     Luce, D. R. (1959). Individual choice behavior: A theoretical
This work was supported by NWO-VENI grant 275-80-004.
                                                                        analysis. New York: Wiley.
                          References                                 Moxey, L. M., & Sanford, A. J. (1993). Communicating
                                                                        quantities. Hillsdale, NJ: Lawrence Erlbaum.
Atkinson, J., Campbell, F. W., & Francis, M. R. (1976). The          Nosofsky, R. M. (1986). Attention, similarity, and the
   magic number 4 +/- 0: A new look at visual numerosity                identification-categorization relationship. Journal of Ex-
   judgements. Perception, 5(3), 327–334.                               perimental Psychology: General, 115(1), 39–57.
Barwise, J., & Cooper, R. (1981). Generalized quantifiers and        Peters, S., & Westerståhl, D. (2006). Quantifiers in language
   natural language. Linguistics and Philosophy, 4, 159–219.            and logic. Clarendon.
Benz, A., & van Rooij, R. (2007). Optimal assertions and             Plummer, M. (2003). JAGS: A program for analysis of
   what they implicate. Topoi, 26, 63–78.                               Bayesian graphical models using Gibbs sampling. In
Chierchia, G., Fox, D., & Spector, B. (2012). Scalar im-                K. Hornik, F. Leisch, & A. Zeileis (Eds.), Proceedings of
   plicature as a grammatical phenonenon. In C. Maienborn,              the 3rd international workshop on distributed statistical
   K. von Heusinger, & P. Portner (Eds.), Semantics. An in-             computing.
   ternational handbook of natural language meaning (pp.             Sauerland, U. (2012). The computation of scalar implica-
   2297–2332). Berlin: de Gruyter.                                      tures: Pragmatic, lexical or grammatical. Language and
Degen, J., & Tanenhaus, M. K. (2011). Making inferences:                Linguistics Compass, 6(1), 36–49.
   The case of scalar implicature processing. In L. Carl-            Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning.
   son, C. Hölscher, & T. Shipley (Eds.), Proceedings of the           MIT Press.
   33rd annual conference of the cognitive science society (pp.      van Tiel, B. (2014). Quantity matters: Implicatures, typical-
   3299–3304).                                                          ity, and truth. Unpublished doctoral dissertation, Radboud
Degen, J., & Tanenhaus, M. K. (to appear). Processing scalar            Universiteit Nijmegen.
   implicatures: A constraint-based approach. Cognitive Sci-         van Tiel, B. (to appear). Embedded scalars and typicality.
   ence.                                                                Journal of Semantics.
Frank, M. C., & Goodman, N. D. (2012). Predicting prag-
                                                                 492

