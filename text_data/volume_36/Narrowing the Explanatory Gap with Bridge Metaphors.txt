UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Narrowing the Explanatory Gap with Bridge Metaphors
Permalink
https://escholarship.org/uc/item/5x72z7j1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Author
Yoshimi, Jeffrey
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          Narrowing the Explanatory Gap with Bridge Metaphors
                                                 Jeff Yoshimi (jyoshimi@ucmerced.edu)
                                               Cognitive and Information Sciences, UC Merced
                                                             Merced, CA, 95343 USA
                                 Abstract                                     2. The relationship between consciousness and neural activity
                                                                                   cannot be made intelligible.
     A central problem in philosophy of mind concerns the rela-
     tionship between subjective experiences and the physical pro-
     cesses that subserve them. There seems to be an unbridgeable             3. The relationship between consciousness and neural activity
     “explanatory gap” between the two. Whereas other scientific                   cannot be explained.
     explanations (e.g. the explanation of temperature in terms of
     kinetic energy) involve determinate relationships between two             Let us consider the two premises and conclusion of this argu-
     kinds of phenomena, correlations between patterns of neural
     activity and conscious experiences seem to be arbitrary. I argue          ment in more detail.
     that by developing computer models of embodied agents, and                    The first premise describes an intelligibility condition on
     interpreting them using the tools of philosophical phenomenol-
     ogy, the relationship between neural and conscious processes              explanation.2 According to Levine, when something is made
     can be seen to be systematic, and non-arbitrary. Visualiza-               intelligible, “There doesn’t seem to be anything ‘left out”’
     tions of these models serve as “bridge metaphors” that further            (Levine, 1983, p. 358). In the explanatory gap literature this
     emphasize how systematic neuro-phenomenological relations
     are. By showing that links between brain states and conscious             has also been characterized as a feeling of necessity or non-
     states are non-arbitrary, the explanatory gap is narrowed.                arbitrariness. If an explanation makes a phenomenon intelli-
     Keywords: Explanatory Gap; Hard Problem of Conscious-                     gible, we feel that the explanandum is necessitated by the ex-
     ness, Dynamical Systems Theory; Neural Networks; Phe-                     planans. To use Levine’s example, that water boils at 100 de-
     nomenology; Reinforcement Learning; Husserl                               grees Celsius at sea level, seems necessary, given the laws of
                                                                               chemistry. Or, to use an example from Tye, “Once one learns
                      The Explanatory Gap                                      that in solid things, the molecules are not free to move around
                                                                               as they are in liquids, one immediately grasps that solid things
 The familiar mind-body problem is rooted in the difficulty
                                                                               do not pour easily, that they tend to retain their shape and vol-
 we have understanding how subjective feelings could be re-
                                                                               ume. Having been told the physical story, to ask: ‘Yes, but
 ducible to (or identical with) objective physical facts. How
                                                                               why are things with molecules that are fixed in place solid?
 could my rush of excitement at the birth of a child “just be” a
                                                                               Why shouldn’t such things not be solid?’ is to show a concep-
 distributed pattern of spiking neurons? This problem is asso-
                                                                               tual confusion” (Tye, 1999, p. 705). In these cases it does not
 ciated with a kind of explanatory challenge. When it comes to
                                                                               seem like the phenomenon explained (boiling water, solidity)
 consciousness, there seems to be an explanatory gap (Levine,
                                                                               could be any other way, given the explanation. We could not
 1983), or a “hard problem” (Chalmers, 1995), that does not
                                                                               just change the mapping from low level physical states of a
 arise when inter-theoretic reductions are pursued in other ar-
                                                                               thing to temperatures, or to states of matter (solid, liquid, gas,
 eas of science. The problem is a matter of ongoing discussion,
                                                                               etc.) in any arbitrary way.
 and there is a fairly well-developed taxonomy of responses to
                                                                                   The intelligibility condition can be summarized as follows:
 it; for review see (Tye, 2013).1
     The explanatory gap argument can be presented as a modus                      If an explanation of some phenomenon p in terms of phys-
 tollens:                                                                          ical state s and laws L is intelligible, then the relationship
                                                                                   between s and p is felt to be necessary, rather than arbitrary.
1. To explain a phenomenon x requires making x intelligible.
                                                                               More formally, if we describe the relationship between phys-
      1 The relationship between the explanatory gap argument and the          ical states s and higher level phenomena p using a function f ,
 mind-body problem is subtle. The explanatory gap argument is epis-
 temological: it concerns our ability to understand how brain states           then if an explanation using f is intelligible, we must feel that
 are related to conscious states. The mind-body problem is a meta-              f is fixed by the relevant laws, and is not just arbitrarily stipu-
 physical problem: it concerns how brain states and mental states are          lated.3 For example, the mapping between low-level physical
 actually related to each other (in particular: are they identical or sep-
 arate?). So they are distinct issues. Some have used the explanatory               2 I refer to explanations, the phenomena explained in an expla-
 gap argument as a metaphysical argument against materialism (and
 in support of dualism), by adding a premise to the effect that what           nation, and the theoretical relationships involved in an explanation,
 cannot be explained in terms of physics is not itself physical. Others        as being “intelligible.” These usages are related as follows: if a re-
 defend the view that physicalism is true, and that the explanatory            lationship is intelligible it is determinate and non-arbitrary (details
 gap is just a problem we human cognizers have with understanding              are in the main text); if a phenomenon is made intelligible it is de-
 how physicalism is true. For an overview see (Chalmers, 2003). I              scribed in terms of an intelligible relationship; if an explanation is
 am officially neutral on these issues. I argue that there is less of          intelligible, it makes the phenomenon it explains intelligible.
 an explanatory gap than the authors of the original arguments sup-                 3 Notice the reference to “feeling”: intelligibility is, at least in
 pose. This naturally supports physicalism, but is nonetheless strictly        part, a phenomenological concept. Indeed, the phrase “felt contin-
 speaking compatible with dualism and even other forms of monism.              gency” occurs five times in Levine’s paper.
                                                                           3143

states of a system and states of matter like solidity and liquid-     (his Ph.D. was in mathematics, and he wrote a dissertation
ity is not arbitrary, but is fixed by the laws of physics. In a       on the calculus of variation with Karl Weierstrass as his advi-
similar way, the mapping between configurations of particles          sor). Rather than emphasizing individual conscious states he
in a volume and temperatures is (given a temperature scale)           emphasized sets of possible conscious states, what he called
fixed, and not arbitrary.                                             “manifolds” of possible experience. He also described laws
   The second premise states the main problem. The problem            (which he thought of as invariant structures in these mani-
is that, when it comes to explanations of subjective conscious        folds) constraining the kinds of processes that must occur if a
experiences in terms of physical states, it does not seem the         person is to experience a stable world.4
intelligibility condition is fulfilled. Suppose it is shown that           Varela argued that by drawing on the methods and results
activity in the insula and orbito-frontal cortex is correlated        of phenomenology and integrating them with neuroscience
with the taste of chocolate. Even if the correlational evidence       and dynamical systems theory (via a system of “mutual con-
is strong, the relationship between these kinds of brain states       straints”), we can make progress on the hard problem. In his
and taste experiences does not feel necessary. It seems like we       response, Chalmers was enthusiastic: “the idea of ‘neurophe-
could have just as easily been told that activity in these brain      nomenology’ sounds eminently sensible to me. The test will
areas correlates with the taste of strawberries. We have this         be whether it can be cashed out in the form of detailed results”
feeling that the brain states and conscious states “are stuck         (Chalmers, 1997). I think we can pass this test.
together in an arbitrary manner” (Levine, 1983, p. 359). This              Recall that the key issue with the explanatory gap prob-
arbitrariness does not apply in the other cases: we could not         lem concerns the intelligibility of the mind-brain correlations
just as well say that a crystal lattice of atoms in a diamond is      discovered by cognitive neuroscience. According to the ar-
“liquid” as we could say it is “solid”. We could not just as          gument, any mapping discovered between neural states and
well say a given gas is 20 degrees Celsius as we could say it         conscious states will feel arbitrary and thus fail to meet the
is 30 degrees Celsius.                                                intelligibility condition. I argue that the intelligibility condi-
   So whereas in the physical cases we have explanations that         tion can be met.
fulfill the intelligibility condition, in the case of conscious-           Let B be the set of possible brain states for an agent and C
ness we do not. So there is an explanatory gap. Or so the             be theset of possible conscious states for that agent. Consider
argument goes.                                                        a function f : B → C that maps the agent’s brain states to their
   I will make a case that the argument is unsound, because           conscious states. Suppose a neural process occurs, which we
the second premise is false. The relationship between activ-          characterize by a discrete approximation b1 , . . . , bn . Using
ity in visual cortex and visual experience may seem arbitrary          f , we can consider the conscious process this neural process
when states of the visual cortex are considered in isolation.         gives rise to: f (b1 ), . . . , f (bn ). We can do other work with f
A given brain state considered in isolation could just as well        as well. For example, given a way of saying how probable
have given rise to an auditory experience as a visual expe-           future brain states are given current brain states, we can use f
rience. However, when we shift attention to the dynamics              to say how probable future conscious states are, given current
of the visual cortex (in a plausible environment) and consider        conscious states.
what kinds of phenomenological processes “match” those dy-                 In general, we can use the mapping f to take structures in
namics, our choice of mapping from brain states to experi-            B, and associate them with structures in C. Call the result-
ences will no longer seem arbitrary.                                  ing structures “induced phenomenological structures.” I will
                                                                      argue that we can test the arbitrariness and hence the intelli-
    Narrowing the Gap with Bridge Metaphors                           gibility of our neural explanations of consciousness by ask-
                                                                      ing how coherent induced phenomenological structures are,
The explanatory gap problem was made prominent by                     relative to different choices of f . If the degree to which in-
Levine’s paper as well as (Chalmers, 1995), which was sub-            duced phenomenological structures are coherent varies with
sequently responded to by numerous philosophers and cog-              our choice of f , then our choice of f is not arbitrary, and so
nitive scientists. Francisco Varela’s response was called             our neural explanations of consciousness are intelligible.
“Neurophenomenology–A Methodology Remedy to the Hard                       A number of questions arise here. First, we don’t know
Problem” (Varela, 1996). Varela’s strategy was to draw on             what the function f is. Second, it is not clear what the “co-
the area of philosophical phenomenology to try to solve the           herence” of an induced phenomenological structure amounts
hard problem and close the explanatory gap. Phenomenol-               to. Finally, it’s not currently possible to measure brain activ-
ogy originates in the work of the German philosopher Ed-              ity in a manner that is precise enough to allow us to refer in
mund Husserl (1859-1938). Husserl developed a complex                 any detailed way to specific neural processes b1 , . . . , bn .
technical apparatus for studying the structure and dynamics
                                                                           We can address these questions by making use of simula-
of consciousness. He did not merely describe the contents
                                                                      tions and theoretical constructs, which are sufficiently well
of his experience introspectively, using something like a lit-
                                                                      motivated to make a case that our choice of f is not entirely
erary stream of consciousness or autobiography of his own
thoughts. Rather, he drew on his training in mathematics to                 4 For an overview of the ideas in phenomenology I draw on here,
develop a more rigorous, formalized theory of consciousness           see (Yoshimi, 2012).
                                                                  3144

 arbitrary. We can begin with a neural network simulation of
 an agent in an environment, using theories and methods of
 computational neuroscience. We can then posit a mapping
 from the states of this simulation to conscious states. As we
 will see, this can (at least for the cases I consider, which focus
 on sensory states), be done in a fairly natural way: for exam-
 ple, the states of an agent’s neural network when it is near a
 flower stimulus can be thought of as experiences of smelling
 the flower. We do not need to assume this must be the way the
 mapping works (in fact, the key step in the argument will be
 to imagine variations on the function). It will be enough for
 our purposes just to start with something plausible. Finally, to
 determine the coherence of induced phenomenological struc-
 tures, we can ask to what extent those structures are consistent
 with an independently developed theory of conscious experi-
 ence, namely, Husserlian phenomenology.
                                                                        Figure 1: An agent in a 2d world with three objects, and the
    The approach I am describing can be thought of as a step-
                                                                        5 excursions it takes. The dashed gray curves trace out the
 wise “bridge metaphor” strategy:
                                                                        paths taken by the agent on the five excursions.
1. Develop a neural network model of an agent in a virtual en-
    vironment, using existing principles of computational neu-
    roscience.                                                          They are also relatively concrete and easy to interact with in
                                                                        computer simulations, and can be interpreted using a well de-
2. Project patterns of activity from the neural network’s state         veloped body of theoretical work. The concrete structure of
    space to a visualizable 2d or 3d space using dimensionality         neural network dynamics is a kind of source domain that can
    reduction techniques. Observe the sets of points or “point          be used to give structure to the domain of phenomenological
    sets” that occur in the projected neural network state space.       dynamics, which is more abstract and thus stands to be ben-
                                                                        efit by this linkage. I call these “bridge metaphors” and the
3. Propose a mapping f from network states to conscious
                                                                        strategy above a “bridge metaphor strategy.”
    states.
                                                                           Whereas conceptual metaphors are mappings that happen
4. Use f to induce phenomenological structures. Ask whether             de facto in the course of our everyday experience, I envi-
    these are coherent relative to the principles of Husserlian         sion bridge metaphors as tools that are actively tuned in sci-
    phenomenology.                                                      entific practice, to maximally harmonize intuitions between
                                                                        disparate domains. We use the neural network dynamics to
5. Consider alternative mappings f 0 , and ask whether they in-         understand the phenomenological dynamics. We then use the
    duce phenomenological structures that are more or less co-          phenomenological dynamics to interpret the neural dynam-
    herent than f .                                                     ics. By proceeding back and forth in this way, we begin to
                                                                        develop a detailed understanding of neuro-phenomenological
 In the last step, if the alternative mappings f 0 induce phe-
                                                                        connections. We build a bridge between the two domains, and
 nomenological structures that are less coherent than f , we
                                                                        begin to narrow the gap of intelligibility between them.
 have evidence that f is not felt to be arbitrary, and thus that
 explanations of consciousness can be intelligible. To that ex-
                                                                                                  Example
 tent we narrow the explanatory gap.
    We can think about this process using the theory of con-            I now illustrate the 5 steps of the bridge metaphor strategy
 ceptual metaphors (Lakoff, 1993). Conceptual metaphors are             with a concrete example.
 cognitive structures that allow one conceptual domain (the                (Step 1). We begin with an idealized model of a network
 source domain) to be understood in terms of another (the tar-          in a virtual environment. The model was developed using
 get domain). Typically the source domain is more concrete              principles familiar from a wide range of models in cognitive
 and better understood, and in this way provides structure to           science, including reinforcement learning models (Sutton &
 the target domain. Examples are time is money, love is a               Barto, 1998), and Bayesian models of the brain as a predic-
 journey, and argument is war. The structure of money, jour-            tion machine (Clark, 2013). It is also consistent with ideas
 neys, and wars are relatively concrete and embodied, and can           from the earlier literature on animal learning going back to
 thus be used to provide structure to the more abstract domains         (Tolman, 1948), according to which behavioral data imply
 of time, love, and argument. So too here. We begin on the              that agents’ navigate environments using internal maps. A
 physical side, with a neural network model of an agent in an           component of most models in these areas is a mathematical
 environment. Neural network models can be directly con-                structure that learns to predict what future states will occur,
 strained by anatomical, physiological, and behavioral data.            often using an error based learning rule like the delta rule
                                                                    3145

(Bishop, 1996).
    The agent and its virtual environment are shown in Figure
1. The network controlling the agent consists of several sub-
networks: a sensory network consisting of neurons that re-
spond to objects in the environment; a motor network consist-
ing of neurons which, when activated, make the agent move
straight or turn (or both); and a prediction network (trained
using the delta rule) that predicts the next state of the sensory
network based on the current activations of the sensory and
motor neurons. Based on what the agent senses and how it
moves, it predicts what it will sense next. Each sensory neu-
ron responds to a specific object via a linear scaling function
of the distance between the sensor and that object. For ex-
ample, if a cheese-sensing neuron is on top of cheese, then
it is maximally activated. As the agent moves away from the
cheese that neuron’s activation diminishes to 0.5
                                                                          Figure 2: The “spandrelled bouquet” point set after all five
    (Step 2). To get a feel for the way the agent represents              excursions have occurred. Shown is the fifth excursion. The
its simple environment, five “excursions” were run, where                 current point (shown in green) corresponds to the point when
the motor neurons were clamped to specific values at suc-                 the agent has just passed the cheese and is turning towards the
cessive times according to one of five scripts (see Figure 1).            flower.
At each iteration of the simulation, the network was updated,
the agent was moved on the basis of the states of the motor
neurons, and the current state of the sensory networkwas pro-             being active at once. In the fifth excursion, for example, we
jected to a 2d plot using principal component analysis (Abdi              have a sequences of points that moves from the arc corre-
& Williams, 2010). The points of the plot were then col-                  sponding to the cheese alone, to a new connecting path or
ored according to the predictions of the prediction subnet-               “spandrel” corresponding to mixed representations of flower
work. The current state was colored green, and states that                and cheese, to the arc corresponding to the flower alone. As
were predicted to occur next were colored red at varying de-              with the first three excursions, as the agent moves, a halo of
grees of saturation (and clipped to gray below some thresh-               red follows the current point in state space.
old), in proportion to the Euclidean distance between those                   After the agent has gone on these five excursions it can be
states and the next predicted state.                                      sent back on any of them again. In these cases the point set
    On the first three excursions, the agent moves past each              does not change (no new sensory states occur) but the struc-
of the three objects individually. As the agent passes an ob-             ture of the halo moving through the point set does (the net-
ject a single arc (in these cases line segments) unfolds in the           work’s predictions improve). For example, after the first five
state space. Intuitively the agent is representing (for exam-             excursions if the agent is sent on the fifth excursion again,
ple) a “faint flower smell, a little more flower smell.... max-           then at the moment the agent begins turning towards the
imal flower smell ... a little less flower smell...” etc. until it        flower, it correctly predicts that it will soon be in a mixed
is back to smelling nothing. As it moves up to each of the                cheese / flower state even though it occupies the same point
objects a kind of “halo” of red (predicted next states) moves             in the point set as when it is about to have the mixed cheese
with the current point up to the end of the forming arc. As it            / fish state of the fourth excursion. Figuratively, when it is at
passes the objects the halo moves back down the just-formed               the “fork in the road” (shown in Figure 2), it correctly predicts
arc (you are encouraged to download the software, run the                 which path (which spandrel) it will follow, based on what it
simulation, and see it yourself). After the agent has passed              is sensing in its sensory nodes and what action it is taking.6
the three objects its state space contains the three-pronged                  (Step 3). Our provisional mapping f from points in the net-
structure shown at the core of Figure 2, what can be called a             work’s state space to associated conscious states was already
“bouquet of three arcs,” where each arc corresponds to one of             suggested above. We can simply assume that states of the
the represented objects.                                                  sensory network produce conscious perceptions of the repre-
    In the fourth and fifth excursions, the agent visits two ob-          sented objects. For example, the network state that occurs
jects in succession, moving forward past the cheese and then              when the agent is right on top of Swiss cheese corresponds
turning either left or right. An example is shown in Figure 2.            to a conscious perception of Swiss cheese as being near the
This creates new mixed representations that did not occur in              agent, whereas the network states that occur when the agent
the first three excursions, corresponding to two sensory nodes            is at various distances from the cheese correspond to percep-
     5 For a complete specification of the algorithm, the code can be          6 This is hard to see in the figure, though it is the case that the
requested from the author and run via Simbrain, a freely available        output of the prediction network corresponds to the first point on the
simulation package (Yoshimi, 2008).                                       upper spandrel.
                                                                      3146

tions of the Swiss cheese at corresponding distances from the         ential process. The agent could not, in Husserl’s terms, have
agent. When the agent is near multiple objects it has a mixed         any kind of experience of stable things or “posited unities.”
conscious state encompassing all the objects it perceives, at         A subjective process would occur, but it would be a kind of
corresponding distances from it. When it is sufficiently far          “maelstrom”, a chaos of unrelated perceptual states. Husserl
from the objects it consciously perceives no object.                  admits that something like this is imaginable, but says that in
   (Step 4). Thus, under f , in each of the first three excur-        such a case all sense of reality would be lost:
sions a sequence of conscious perceptual states occurs be-
                                                                         Could it not be that, from one temporal moment on...
ginning with faint perception, rising to maximal perception
                                                                         the series of appearances would run into one another
of one object, then falling back to faint perception. Each of
                                                                         in such a way that no posited unity could ultimately be
the arcs corresponds to a perceptual processes of moving past
                                                                         maintained... Thus we arrive at the possibility of a phe-
one of the three objects. The origin of the bouquet, where the
                                                                         nomenological maelstrom... it would be a maelstrom so
three arcs intersect, corresponds to perceiving no objects. In
                                                                         meaningless that there would be no I and no thou, as
the fourth and fifth excursions, the agent first perceives one
                                                                         well as no physical world—in short, no reality (Husserl,
object, then perceives two objects (while it is on the span-
                                                                         1997, 249-250)
drels), then perceives the second object alone. This is the
phenomenological structure induced by the spandrelled bou-               Though I think this is enough to show that our choice of
quet of arcs under the mapping f .                                     f is not arbitrary, let us briefly consider the prediction net-
   We can also consider the phenomenological structure in-            work and the phenomenological structures it induces. If we
duced by the prediction subnetwork under f . After some               consider an expanded f that not only maps sensory states to
training, the states of the prediction subnetwork—visible as          conscious perceptions, but also maps from error states of the
a moving halo of red—accurately predict what the agent                network to experiences of fulfillment and frustration, then the
willsense next. These predictions correspond to phenomeno-            only way this map would make sense of the phenomenology
logical predictions about what the agent will consciously per-        would be if low error corresponded to fulfillment and high er-
ceive at successive times. We can also associate the mean             ror corresponded to frustration. For example, when the agent
error at each time step (the discrepancy between the states           begins to turn right, having just sensed the cheese (excursion
predicted to occur and the states that actually occur, which is       5), it predicts it will sense the flower. When it actually does
used by the delta rule to update the weights), with feelings of       sense the flower, the prediction is confirmed and error is low.
rightness or wrongness.                                               If this mapped to a feeling of surprise under an expanded f 0 ,
   All of this is consistent with Husserlian phenomenology.           that would make no phenomenological sense. We are not
According to Husserl, we consciously perceive objects as              surprised when our subsequent perceptions match our previ-
“identities in manifolds”, such that when we move our bodies          ous expectations: we experience something like the kind of
in a smooth, continuous way relative to an object, we perceive        smooth feeling of fulfillment Husserl describes.
it as changing in a correspondingly smooth, continuous way.              Thus, our choice of f is not arbitrary. We cannot simply
Husserl also develops a complex phenomenology of expecta-             associate brain states with conscious states however we like,
tion (or as he also says, “protention”), whereby as we move           and expect the result to make phenomenological sense. Some
with respect to a familiar object we constantly have expec-           choices of f produce phenomenological structures that are
tations about how the object will look relative to our move-          consistent with Husserlian phenomenology; other choices of
ments. These expectations are “fulfilled” or “frustrated” in           f produce phenomenological structures that are inconsistent
varying degrees depending on what we actually perceive in             with Husserlian phenomenology. So premise 2 is false, the
successive moments. As we learn more about an environ-                explanatory gap argument is unsound, and there is hope yet
ment these expectations become more specific, in the sense            for an intelligible explanation of consciousness in terms of
that there are fewer subsequent perceptions that would fulfill        neural structures.7
our expectations. This corresponds to the idea that the halo
of predictions surrounding the current point narrows as the
                                                                                                 Conclusion
agent repeatedly goes on the same excursions.                         I have used the bridge metaphor strategy to address a specific
   (Step 5) We can now ask how coherent these induced phe-            philosophical argument, the explanatory gap argument. Like
nomenological structures are, relative to other choices of f . If     Varela, I think neuro-phenomenology can be used to narrow
 f were different, we would get different induced phenomeno-              7 The upshot of all this is that the explanatory gap can be nar-
logical structures. Would these different structures still make       rowed. I do not say it can be “closed”, because I believe that there is
sense relative to Husserlian phenomenology? For many such             some latitude in how we specify f . For some range of f ’s I think that
                                                                      induced phenomenological structures can remain coherent. In fact
differences, the answer is obviously “no”. Suppose, for ex-           some fairly radical permutations of f , akin to color inversion cases
ample, that we chose an f 0 such that on the first excursion          (Byrne, 2014), may preserve phenomenological coherence. So, I am
the agent experienced a haphazard sequence of perceptions             not sure a unique mapping from brain states to conscious states can
                                                                      be achieved. But I do think I have shown that there are substantial
of the fish, cheese, and flower, in rapid succession as it passes     constraints on how f is specified, and to that extent I believe I have
the cheese. This would not be a sensible or coherent experi-          narrowed the explanatory gap.
                                                                  3147

the explanatory gap (though I do not think it can close the gap       Lakoff, G. (1993). The contemporary theory of metaphor.
altogether), and make progress on the hard problem.                     Metaphor and thought, 2, 202–251.
   In developing this argument, I have been motivated by              Levine, J. (1983). Materialism and qualia: The explanatory
a broader research agenda: to show how these two exten-                 gap. Pacific philosophical quarterly, 64(4), 354–361.
sive bodies of research—computational and cognitive neu-              Sutton, R., & Barto, A. (1998). Reinforcement learning: An
roscience on the one hand, and Husserlian phenomenology                 introduction (Vol. 1) (No. 1). Cambridge Univ Press.
on the other—can be integrated using the tools of computer            Tolman, E. (1948). Cognitive maps in rats and men. Psycho-
simulation and dynamical systems theory. We have seen                   logical review, 55(4), 189.
that computer simulations can be used to visualize an agent’s         Tye, M. (1999). Phenomenal consciousness: The explanatory
model of its environment. The point-set that develops as an             gap as a cognitive illusion. Mind, 108(432), 705–725.
agent explores its environment has a specific and meaning-            Tye, M. (2013). Qualia. In E. Zalta (Ed.), The stanford
ful shape. In the simple case considered above, where three             encyclopedia of philosophy (Fall 2013 ed.).
objects were repeatedly explored in five ways, the point set          Varela, F. (1996). Neuro-phenomenology: a methodologi-
was a spandrelled bouquet of three arcs.8 As the agent ex-              cal remedy to the hard problem. Journal of consciousness
plores its environment it traverses this structure, and a halo          studies, 3(4), 330-349.
of predictions, which narrows over time, moves with it. Us-           Yoshimi, J. (2008). Simbrain: A visual framework for neural
ing a mapping from brain states to conscious states, we can             network analysis and education. Brains, Minds, and Media,
interpret this structure as being simultaneously a picture of           3.
the agent’s neural representation of its environment, and as a        Yoshimi, J. (2012). Two dynamical themes in husserl. In Be-
phenomenological structure, which shows how well the agent              ing in time: Dynamical models of phenomenal experience
knows its way around its environment, what it perceives at              (Vol. 88, pp. 165–184).
each moment, what it expects at each moment, and the de-
gree to which it is surprised by what it perceives.
                      Acknowledgments
Thanks to Rick Dale, Scott Hotton, David Noelle, Bodo Win-
ter, and several referees, for helpful feedback.
                          References
Abdi, H., & Williams, L. J. (2010, July). Principal component
   analysis. Wiley Interdisciplinary Reviews: Computational
   Statistics, 2(4), 433–459.
Bishop, C. M. (1996). Neural networks for pattern recogni-
   tion. Oxford : New York: Oxford University Press.
Byrne, A. (2014). Inverted qualia. In E. N. Zalta (Ed.), The
   stanford encyclopedia of philosophy (Summer 2014 ed.).
Chalmers, D. (1995). Facing up to the problem of conscious-
   ness. Journal of consciousness studies, 2(3), 200–219.
Chalmers, D. (1997). Moving forward on the problem of
   consciousness. Journal of Consciousness studies, 4(1), 3–
   46.
Chalmers, D. (2003). Consciousness and its place in nature.
   Blackwell guide to the philosophy of mind, 102-142.
Clark, A. (2013). Whatever next? predictive brains, situated
   agents, and the future of cognitive science. Behavioral and
   Brain Sciences, 1–73.
Husserl, E. (1997). Thing and space: Lectures of 1907
   (Vol. 7; R. Rojcewicz, Trans.). Springer.
    8 This approach can be extended in various ways. Adding more
objects to the environment adds more arcs to the bouquet. Adding
more excursions fills in the region between the arcs in different
ways. Adding decay to the neurons creates more curved paths in
the state space, some of which look like curlicues. These kinds
of additions can be understood in terms of a more complex repre-
sentations of the environment and a correspondingly more complex
phenomenology.
                                                                  3148

