UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Smart Human, Smarter Robot: How Cheating Affects Perceptions of Social Agency
Permalink
https://escholarship.org/uc/item/2jh800n1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Ullman, Daniel
Leite, Lolanda
Phillips, Jonathan
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                               Smart Human, Smarter Robot:
                               How Cheating Affects Perceptions of Social Agency
          Daniel Ullman1, Iolanda Leite2, Jonathan Phillips1,3,4, Julia Kim-Cohen3, and Brian Scassellati1,2
 1
   Program in Cognitive Science | 2Department of Computer Science | 3Department of Psychology | 4Department of Philosophy
                                               Yale University, New Haven, CT 06520 USA
                               Abstract
    Human-robot interaction studies and human-human
    interaction studies often obtain similar findings. When
    manipulating high-level apparent cognitive cues in robots,
    however, this is not always the case. We investigated to what
    extent the type of agent (human or robot) and the type of
    behavior (honest or dishonest) affected perceived features of
    agency and trustworthiness in the context of a competitive
    game. We predicted that the human and robot in the dishonest
    manipulation would receive lower attributions of
    trustworthiness than the human and robot in the honest
    manipulation, and that the robot would be perceived as less
    intelligent and intentional than the human overall. The human
    and robot in the dishonest manipulation received lower
    attributions of trustworthiness as predicted, but, surprisingly,             Figure 1. Snapshot of the human manipulation.
    the robot was perceived to be more intelligent than the
    human.
    Keywords: social robotics; trustworthiness; intelligence;
    intentionality; agency; human-robot interaction
                           Introduction
The importance of recognizing social agentic features is not
confined to humans, but extends to other living beings and
to nonliving social agents. Inferences about the behavior and
cognitive capabilities of an entity greatly influence
ascriptions of intelligence (Beer, 1990). Human-like
properties related to intelligence can be attributed to
animated shapes (Scholl & Tremoulet, 2000), virtual agents
(Bickmore & Cassell, 2001), and social robots (Bainbridge,
Hart, Kim, & Scassellati, 2011; Short, Hart, Vu, &                               Figure 2. Snapshot of the robot manipulation.
Scassellati, 2010). While the concept of intelligence has
been studied extensively with respect to humans, the                     no cheat condition. Furthermore, the results pointed toward
properties that contribute to perceptions of other animated              greater attributions of mental state to the robot in the cheat
beings as intelligent, in particular social robots, are still            conditions than in the no cheat condition. The work by
unclear. A better understanding of how people make social                Short et al. (2010) directly motivates the present research.
attributions to robots will not only allow roboticists to                We seek to further this line of research by benchmarking an
design robots with better social interactive capabilities, but           analysis of agentic cues in a human-robot interaction against
also will add to the knowledge base on features of social                a comparable analysis of agentic cues in a human-human
agency.                                                                  interaction. Ultimately, we aim to examine perceptions of
    Previous research by Short et al. (2010) showed that                 intelligence and intentionality in a context of cheating
manipulating high-level behavioral cues, specifically                    behavior.
cheating versus not cheating, causes attributions of different             There are a number of factors that contribute to
mental states to a robot. The researchers investigated                   perceptions of entities as agentic. As stated by Bandura
attributions of mental state and intentionality to a cheating            (2001), “To be an agent is to intentionally make things
robot in a game of rock-paper-scissors, a high-level                     happen by one’s actions.” Researchers over the years have
examination that explored how variations in robotic                      identified features important to ascriptions of agency,
behavior affected perceptions of a robot’s agency.                       including intentionality (Bandura, 2001) and self-propelled,
Participants in the two cheat conditions rated the interaction           purposeful-looking movement (Premack, 1990; Scholl &
as less fair and honest than those in the third condition, the           Tremoulet, 2000). The concept of agency extends beyond
                                                                         humans; as argued by Takayama (2011), “Regardless of the
                                                                     2996

absolute status of an entity’s agency, it is our perceptions of        H2: The robot will be perceived as less intelligent and
agency that influence how we behave.” Several studies have             intentional than the human in both the honest and
shown that nonhuman entities that act in ways that appear to           dishonest manipulations.
be goal-directed are likely to be perceived as agentic. For
example, a study by Scholl and Tremoulet (2000) showed                 Hypothesis H1 stems from the expectation that humans
that goal-directed motion exhibited by small shapes moving          who cheat are perceived to be less trustworthy than humans
around a visual field caused humans to attribute features of        who do not cheat, and is motivated by findings from Short
animacy and causality to the shapes. Research into the role         et al. (2010) that indicate that robots that cheat are perceived
of goal-directed action in robots has found that infants            as less fair and honest than robots that do not cheat.
positively attribute goals to humanoid-robot motion                 Hypothesis H2 stems from the expectation that robots
(Kamewari, Kato, Kanda, Ishiguro, & Hiraki, 2005). In a             display fewer features associated with agency than do
study that manipulated the physical presence of a robot to          humans, resulting in lower attributions of associated
understand the effect of the robot’s embodiment on                  features of intelligence and intentionality to robots than
attributions of goal-directed behavior, Bainbridge et al.           humans.
(2011) found that participants were more willing to comply
with a physically present robot than a robot displayed via a                                   Method
live video feed. Overall, intentionality is a feature that can      Each participant watched two videos in which two agents, a
be ascribed to agents that display sufficient cues of agency        human and a human or a human and a robot, were playing a
(Mutlu, Yamaoka, Kanda, Ishiguro, & Hagita, 2009). Our              modified version of the board game Battleship (Figure 1 and
beliefs about what an agent can do, and how an agent should         Figure 2). In Battleship, two players sit facing each other
act, greatly affect our perceptions of an agent.                    with a visual divider in-between the players’ ocean grids.
   Cheating is a classification of intentional behavior that        The divider hides the opponent’s ship locations. The
can be attributed to social agents. Perceptions of                  objective of the game is to be the first player to sink the
trustworthiness are linked to perceptions of intelligence           opponent’s ships by calling out shot locations.
(Goleman, 1995) and affect how willing one agent is to
interact with another, as well as how one agent actually            Materials
interacts with another. Research has shown that cheating
                                                                    The experimental setup replicated the game of Battleship,
affects trustworthiness, an important feature of social
                                                                    with the game and experimental setup modified to
relationships (Rotter, 1980). Robots are becoming
                                                                    accommodate the physical capabilities of the robot. We used
increasingly present in contexts that demand relationships
                                                                    the robot Nao V3.2, a humanoid robot from Aldebaran
built on trust, from robots that deliver medicine in hospitals
                                                                    Robotics (pictured on the right in Figure 2). We recorded
and robots that provide company for the elderly (Broadbent,
                                                                    four video clips of in-progress games of Battleship, one for
Stafford, & MacDonald, 2009; Zhang et al., 2010), to robots
                                                                    each condition. The humans in the videos tracked game
that team up with workers on factory lines (Desai et al.,
                                                                    progress using a sheet with a grid, while participants were
2012). With the potential to be social agents, robots thus
                                                                    told that the robot would track shots using its memory.
also have the potential to be perceived as trustworthy, or
even untrustworthy (Vazquez, May, Steinfeld, & Chen,
                                                                    Procedure
2011). In fact, robots appear to be held accountable for
moral harm that they cause (Kahn et al., 2012).                     The study employed a mixed design, with participants
   As robots become further integrated into social situations,      randomly assigned to one of four conditions. The first
it is important to understand how robotic behavior can affect       independent variable, presented between subjects, was
the perception of socially relevant traits. In this paper, we       player type: human or robot. The second independent
investigate whether manipulating the behavior of an agent in        variable, presented within subjects, was behavior type:
situations involving trust affects perceptions of agentic           honest, the absence of cheating, or dishonest, the presence
features of a robot similarly to how it affects perceptions of      of cheating.
a human. Specifically, we evaluate how manipulating the                Participants were presented with the rules of the game
type of behavior displayed by a human in a competitive              they were going to observe, which explicitly stated: “Do not
game affects attributions of trustworthiness, intelligence,         change the position of any ship once the game has begun.”
and intentionality to the human (Figure 1), as compared to          The cheat behavior consisted of the human or robot cheater
identical manipulations of the behavior displayed by a robot        moving a ship out of the line of fire and categorizing a shot
(Figure 2). We posited the following hypotheses:                    as a miss instead of a hit, directly violating the stated game
                                                                    rules. Each participant was presented with both the honest
   H1: The human and robot in the dishonest manipulation            and dishonest videos of the player type they were randomly
   will receive lower attributions of trustworthiness than          assigned. The order of the videos was counterbalanced so
   the human and robot in the honest manipulation.                  that half of the participants viewed the honest video first and
                                                                    half viewed the dishonest video first. David was the name
                                                                    given to the human opponent on the left, while Kevin was
                                                                    the name given to the human or robot opponent on the right.
                                                                2997

                                  Figure 3. Series of snapshots of the honest human condition.
                   Human-human interaction with human on the right placing piece, following game rules.
                                 Figure 4. Series of snapshots of the dishonest robot condition.
                Human-robot interaction with robot moving ship and then placing piece, violating game rules.
For each video, participants were told that David, on the          Participants were excluded if they self-reported
left, is playing against Kevin, on the right, and that it is       experiencing technical trouble watching or hearing either
Kevin’s turn. The game proceeded as follows: Kevin took a          video, if they reported being a non-native English speaker,
turn, David took a turn and Kevin either did not cheat             or if they reported a 4 or below on a 7-point Likert item
(honest manipulation) or did cheat (dishonest manipulation),       concerning how well they understood the rules of the game
and then Kevin took a final turn and won the game. For each        they were observing. Participants were also excluded if they
condition, participants were presented with the first video,       failed to correctly answer a control question on the number
answered survey questions for the first video, were                of humans present in each video. Participants’ IP addresses
presented with the second video, and answered the same set         were checked to ensure that there were no repeat
of survey questions for the second video. Participants were        participants; none were found. After exclusions, a total of
then asked to answer optional demographic questions.               179 participants remained: 87 in the human manipulation
   Figure 3 shows a series of snapshots of the honest human        and 92 in the robot manipulation. Participants were paid
condition, portraying the no cheating behavior. Figure 4           $0.50 each.
shows a series of snapshots of the dishonest robot condition,
portraying the cheating behavior.                                  Measures
                                                                   Participants were presented with the prompt “How would
Participants                                                       you rate Kevin in terms of the following:” and these 7-point
A total of 200 adults (137 male, 63 female) participated in        Likert items: Intelligence, Cleverness, Intentionality,
the study. The mean age of participants was 31.24 years (SD        Fairness, Honesty, Trustworthiness.
= 10.60). Participants reported race/ethnicity as follows: 153
“Non-Hispanic White or Euro-American”; 14 “Black, Afro-            Trustworthiness Data for the dependent variable of
Caribbean, or African American”; 4 “Latino or Hispanic             trustworthiness were computed by combining three ratings
American”; 16 “Asian or Asian American”; 2 “Native                 on fairness, honesty, and trustworthiness, with internal
American or Alaskan Native”; 6 “Multi-racial”; 0 “Other”;          consistency for the honest conditions (Cronbach’s α = .93, n
5 “I would prefer not to answer.”                                  = 3) and the dishonest conditions (Cronbach’s α = .96, n =
   We recruited participants via the web-based resource            3).
Amazon Mechanical Turk. Participants were directed to a
survey designed using the web-based resource Qualtrics. To         Intelligence Data for the dependent variable of intelligence
ensure that workers completed the survey, participants             were computed by combining two ratings on intelligence
received an end of survey completion code in order to              and cleverness, with internal consistency for the honest
receive payment through Mechanical Turk.                           conditions (Cronbach’s α = .76, n = 2) and the dishonest
   Participants were recruited through Amazon Mechanical           conditions (Cronbach’s α = .82, n = 2).
Turk with the following restrictions in place: having an
overall approval rating > 95% and being geographically
located in America (determined by IP addresses).
                                                               2998

Intentionality Data for the dependent variable of
intentionality were taken from the one rating on
intentionality.
                           Results
Three 2 x 2 ANOVAs were conducted. All ANOVAs
compared the two independent variables of player type
(human or robot) and behavior type (honest or dishonest).
Trustworthiness
A 2 x 2 ANOVA was conducted to investigate the impact of
player type and behavior type on perceived trustworthiness
(Figure 5). There was a significant main effect of player
type, with the human conditions (M = 3.63, SE = 0.12) rated          Figure 5. Mean perceptions of trustworthiness. Error bars
lower than the robot conditions (M = 3.99, SE = 0.12), F(1,                               show SE mean.
177) = 4.70, p = .03, ηp2 = .03. There was also a significant
main effect of behavior type, with the honest conditions (M
= 5.50, SE = 0.11) rated higher than the dishonest conditions
(M = 2.12, SE = 0.13), F(1, 177) = 433.90, p < .001, ηp2 =
.71. There was no significant interaction effect between
player type and behavior type, F(1, 177) = 0.52, p = .47, ηp2
= .00. Participants’ perception of trustworthiness was
greater for the human in the honest condition (M = 5.38, SE
= 0.15) than in the dishonest condition (M = 1.88, SE =
0.18), and participants’ perception of trustworthiness was
greater for the robot in the honest condition (M = 5.63, SE =
0.15) than in the dishonest condition (M = 2.36, SE = 0.18).
Intelligence
A 2 x 2 ANOVA was conducted to investigate the impact of
player type and behavior type on perceived intelligence            Figure 6. Mean perceptions of intelligence. Error bars show
(Figure 6). There was a significant main effect of player                                    SE mean.
type, with the human conditions (M = 4.58, SE = 0.12) rated
lower than the robot conditions (M = 5.09, SE = 0.12), F(1,
177) = 8.89, p < .01, ηp2 = .05. There was no significant
main effect of behavior type, with no statistically significant
difference between the honest conditions (M = 4.82, SE =
0.09) and the dishonest conditions (M = 4.85, SE = 0.11),
F(1, 177) = 0.12, p = .73, ηp2 = .00. There was a significant
interaction effect between player type and behavior type,
F(1, 177) = 6.63, p = .01, ηp2 = .04. Participants’ perception
of intelligence was greater for the human in the honest
condition (M = 4.68, SE = 0.12) than in the dishonest
condition (M = 4.48, SE = 0.15), whereas participants’
perception of intelligence was greater for the robot in the
dishonest condition (M = 5.22, SE = 0.15) than in the honest
condition (M = 4.95, SE = 0.12).                                      Figure 7. Mean perceptions of intentionality. Error bars
                                                                                          show SE mean.
Intentionality
A 2 x 2 ANOVA was conducted to investigate the impact of           honest conditions (M = 4.97, SE = 0.11) rated lower than the
player type and behavior type on perceived intentionality          dishonest conditions (M = 5.55, SE = 0.13), F(1, 177) =
(Figure 7). There was no significant main effect of player         23.93, p < .001, ηp2 = .12. There was a marginally
type, with no statistically significant difference between the     significant interaction effect between player type and
human conditions (M = 5.41, SE = 0.15) and the robot               behavior type, F(1, 177) = 2.85, p = .09, ηp2 = .02.
conditions (M = 5.10, SE = 0.15), F(1, 177) = 2.17, p = .14,       Participants’ perception of intentionality was much greater
ηp2 = .01. There was a main effect of behavior type, with the      for the human in the dishonest condition (M = 5.81, SE =
                                                               2999

0.19) than in the honest condition (M = 5.02, SE = 0.16),            the honest condition. In light of the possibility that
whereas participants’ perception of intentionality was only          participants rated the human and robot relative to their
slightly greater for the robot in the dishonest condition (M =       experiences of a typical human and robot, the interaction
5.29, SE = 0.18) than in the honest condition (M = 4.91, SE          effect appears logical. People perceived the robot as more
= 0.16).                                                             intelligent when it was dishonest, while the human was
                                                                     rated as less intelligent when dishonest; these ratings may
                          Discussion                                 partially stem from the actual cheat behavior itself, which
The examination of trustworthiness provides the most                 participants might have considered an intelligent behavior
straightforward results, supporting our first hypothesis.            for a robot, but an unintelligent behavior for a human.
Hypothesis H1 posited that participants would make lower             Participants likely considered the cheat behavior of the
attributions of trustworthiness to the human and robot in the        robot a novel, surprising behavior for a robot, adding to the
dishonest manipulation than in the honest manipulation.              perceived intelligence of the robot. It might be possible to
There was indeed a large, significant main effect of                 tease out such a novelty effect in a future repeated
behavior type on trustworthiness, such that the human and            interactions study.
robot received lower attributions of trustworthiness in the             As for intentionality, there was no significant main effect
dishonest manipulation than in the honest manipulation.              of player type on perceived intentionality, but there was a
This is both an expected and logical finding, especially             significant main effect of behavior type. The significant
given the comparable results on fairness and honesty for a           main effect of behavior type seems to align with findings
robot obtained by Short et al. (2010). There was an                  associated with the side-effect effect (Knobe, 2003), such
additional small, significant main effect of player type on          that participants attributed greater intentionality when the
trustworthiness, such that the robot was perceived to be             agent performed an immoral action than when the agent
more trustworthy than the human. A possible explanation              performed a morally neutral action. It is interesting to note
for this effect may be that participants rated the human and         that the dishonest robot was rated as less intentional than the
robot relative to their experiences of a typical human and           dishonest human, and was not rated as low as the human on
robot, with the findings suggesting that participants                trustworthiness; that is, it appears participants held the robot
perceived the robot to be slightly more trustworthy                  less accountable for its cheating behavior than the human.
compared to their experience of a typical robot. As most             There was a marginal interaction effect, p = .09, such that
people do not often interact with robots, especially not in          participants’ perception of intentionality was much greater
contexts involving honest or dishonest behaviors, this might         for the human in the dishonest condition than in the honest
have contributed to slightly inflated attributions in the robot      condition, whereas participants’ perception of intentionality
conditions. An additional possible explanation is that while         was only slightly greater for the robot in the dishonest
people may readily infer that a person is, in general,               condition than in the honest condition. It is possible,
untrustworthy, they may be less willing to infer that a robot        however, that the item on intentionality was confusing, as
is untrustworthy from a single interaction.                          the results do not align with previous research that
   Hypothesis H2 posited that participants would perceive            implicates perceptions of intentionality as correlated with
the robot as less intelligent and intentional than the human.        other perceptions of agentic behavior (Premack, 1990;
This hypothesis was not supported. Rather, the results               Scholl & Tremoulet, 2000). Alternatively, it is also possible
suggested a different interpretation, and warranted                  that this is a novel finding and will incite future research
separately considering intelligence and intentionality. We           into a potential separation of perceived intelligence and
first analyzed the results from intelligence. There was              intentionality in social robotics. Further research is
indeed a significant main effect of player type, however it          ultimately required to tease out such an interaction effect
was in the opposite direction of the prediction; participants        and to better understand the results indicated by the item on
rated the robot higher on intelligence than the human. At            intentionality.
first glance, this finding seems to contradict expectations
suggested by Short et al. (2010). However, upon closer                                        Conclusion
examination, this result in fact seems to parallel the effect of     As robots become increasingly present in everyday settings,
player type on trustworthiness explained earlier; that is, the       and especially as they take on roles that necessitate greater
agent rated as more intelligent, the robot, was also rated as        social interaction with humans, the field of social robotics
more trustworthy.                                                    requires a more thorough understanding of the features that
   While there was no significant main effect of behavior            influence people's perceptions of robots. This trend
type on intelligence, there was a significant interaction            necessitates research into how humans perceive robots'
effect between player type and behavior type. The                    social traits, as well as to what extent robotic behavior
interaction effect was a crossover interaction, such that            affects attributions of agency.
participants’ perception of intelligence was lower for the              In this paper, we investigated the extent to which the type
human in the dishonest condition than in the honest                  of agent (human or robot) and the type of behavior (honest
condition, whereas participants’ perception of intelligence          or dishonest) affected perceptions of trustworthiness,
was greater for the robot in the dishonest condition than in         intelligence, and intentionality in the context of a
                                                                 3000

competitive game. The results on trustworthiness were               Desai, M., Medvedev, M., Vazquez, M., McSheehy, S.,
expected, while interpretation of the results on intelligence         Gadea-Omelchenko, S., Bruggeman, C., … & Yanco, H.
and intentionality yielded unexpected but intriguing                  (2012). Effects of changing reliability on trust of robot
findings. Participants’ perceptions of intelligence trended in        systems. Proceedings of the 7th ACM/IEEE International
opposite directions for the human and robot; the robot was            Conference on Human-Robot Interaction, 73-80.
perceived as more intelligent in the dishonest manipulation,        Goleman, D. (1995). Emotional intelligence: Why it can
while the human was perceived as less intelligent in the              matter more than IQ. New York, NY: Bantam Books.
dishonest manipulation. An interesting implication of the           Kahn, P. H., Jr., Kanda, T., Ishiguro, H., Gill, B. T.,
study and a potential for follow-up research concerns the             Ruckert, J. H., Shen, S., … & Severson, R. L. (2012). Do
results on intentionality. The marginal interaction effect for        people hold a humanoid robot morally accountable for the
intentionality suggests that attributions of intentionality to        harm it causes?. Proceedings of the 7th ACM/IEEE
agents are affected by the type of agent (human or robot) in          International Conference on Human-Robot Interaction,
combination with the type of behavior (honest or dishonest);          33-40.
future work focused on the role of intentionality in this           Kamewari, K., Kato, M., Kanda, T., Ishiguro, H., & Hiraki,
paradigm will hopefully better illuminate the observed                K. (2005). Six-and-a-half-month-old children positively
effect.                                                               attribute goals to human action and to humanoid-robot
   Most human-robot interaction studies obtain results in the         motion. Cognitive Development, 20, 303-320.
same direction as human-human interaction studies. Our              Knobe, J. (2003). Intentional action and side effects in
findings, with respect to perceived intelligence, did not align       ordinary language. Analysis, 63, 190-193.
with this trend. Rather, our results suggest that the behavior      Mutlu, B., Yamaoka, F., Kanda, T., Ishiguro, H., & Hagita,
of a robot not only affects to what extent it is perceived as         N. (2009). Nonverbal leakage in robots: Communication
an intelligent agent, similarly to a human, but also                  of intentions through seemingly unintentional behavior.
demonstrate that individuals perceive a robot differently             Proceedings of the 4th ACM/IEEE International
from a comparably performing human. These results                     Conference on Human-Robot Interaction, 69-76.
indicate that robot designers cannot simply transfer findings       Premack, D. (1990). The infant's theory of self-propelled
from social human-human interaction to human-robot                    objects. Cognition, 36, 1-16.
interaction, but instead must further investigate features that     Rotter, J. B. (1980). Interpersonal trust, trustworthiness, and
affect human-robot interaction in their own right.                    gullibility. American Psychologist, 35, 1-7.
                                                                    Scholl, B. J., & Tremoulet, P. D. (2000). Perceptual
                    Acknowledgments                                   causality and animacy. Trends in Cognitive Sciences, 4,
The authors gratefully acknowledge Alex Litoiu for help               299-309.
recording the stimuli, Henny Admoni and Elena Corina                Short, E., Hart, J., Vu, M., & Scassellati, B. (2010). No
Grigore for feedback on drafts of this paper, and anonymous           fair!!: An interaction with a cheating robot. Proceedings
reviewers for their insightful comments. This material is             of the 5th ACM/IEEE International Conference on
based upon work supported by the National Science                     Human-Robot Interaction, 219-226.
Foundation award #1117801 (Manipulating Perceptions of              Takayama, L. (2011). Perspectives on agency: Interacting
Robot Agency) and award #1139078 (Socially Assistive                  with and through personal robots. In Zacarias, M. &
Robots).                                                              Oliveira, J. V. (Eds.), Human-Computer Interaction: The
                                                                      Agency Perspective. Berlin: Springer.
                                                                    Vazquez, M., May, A., Steinfeld, A., & Chen, W. H. (2011).
                         References                                   A deceptive robot referee in a multiplayer gaming
Bainbridge, W. A., Hart, J. W., Kim, E. S., & Scassellati, B.         environment. International Conference on Collaboration
   (2011). The benefits of interactions with physically               Technologies and Systems, 204-211.
   present robots over video-displayed agents. International        Zhang, T., Kaber, D. B., Zhu, B., Swangnetr, M., Mosaly,
   Journal of Social Robotics, 3, 41-52.                              P., & Hodge, L. (2010). Service robot feature design
Bandura, A. (2001). Social cognitive theory: An agentic               effects on user perceptions and emotional responses.
   perspective. Annual Review of Psychology, 52, 1-26.                Intelligent Service Robotics, 3, 73-88.
Beer, R. D. (1990). Intelligence as adaptive behavior: An
   experiment in computational neuroethology. San Diego,
   CA: Academic Press.
Bickmore, T., & Cassell, J. (2001). Relational agents: A
   model and implementation of building user trust.
   Proceedings of the SIGCHI Conference on Human
   Factors in Computing Systems, 396-403.
Broadbent, E., Stafford, R., & MacDonald, B. (2009).
   Acceptance of healthcare robots for the older population:
   Review and future directions. International Journal of
   Social Robotics, 1, 319-330.
                                                                3001

