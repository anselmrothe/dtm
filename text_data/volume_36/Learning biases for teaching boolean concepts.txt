UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning biases for teaching boolean concepts
Permalink
https://escholarship.org/uc/item/3jm856bs
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Searcy, Nicholas
Shafto, Patrick
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 Learning biases for teaching boolean concepts
                                               Nick Searcy (nick.searcy@louisville.edu)
                                               Patrick Shafto (p.shafto@louisville.edu)
                               Department of Psychological and Brain Sciences, University of Louisville
                                                         Louisville, KY 40292 USA
                               Abstract                                  infinitely many features. The model also indicates a natural
                                                                         representation for teaching—Disjunctive Normal Form—and
   According to previous accounts, teaching is the helpful sam-
   pling of examples according to a learner’s known biases. Us-          the resulting learning bias is consistent with the best mod-
   ing the domain of Boolean concepts, we show that biases are           els of human complexity of learning (Feldman, 2000; Good-
   necessary, there is no single rational bias, and teaching is not      man, Tenenbaum, Feldman, & Griffiths, 2008; Goodwin & P.
   possible when the teacher does not know the learner’s bias.
   Taken together, these results suggest that teaching via sam-          Johnson-Laird, 2011).
   pling would be either ineffective or impossible for Boolean
   concepts. We offer an alternative account of teaching based on              The limitations of teaching by sampling
   cooperation and the teacher’s omission of irrelevant features.
   The result is a model of teaching that is computationally effi-       To analyze sampling-based teaching, we use an extensively
   cient, effective in concept spaces with infinitely many features,     studied model of algorithmic teaching: the teaching set model
   and suggestive of a natural concept representation based on co-       (Shinohara & Miyano, 1991; Anthony, Brightwell, Cohen, &
   operation.
                                                                         Shawe-Taylor, 1992; Goldman & Mathias, 1993). The pa-
   Keywords: Concept learning; representation; teaching dimen-
   sion; relevant features                                               per will use the following notation for Boolean features, in-
                                                                         stances, and concepts.
                           Introduction                                  Definition 1 (Preliminaries) Let F = { f0 , f1 , . . . } be the
Learning from a cooperative source offers significant advan-             feature space. Let the instance space be the function space
tages over learning from other sources (e.g. disinterested, ran-         from F to Boolean labels X = {0, 1}F . And let the concept
dom, or adversarial) (Shafto & Goodman, 2008; Csibra &                   space be the function space from X to the Boolean labels
Gergely, 2009). Previously, cognitive science and machine                C = {0, 1}X . A concept class is some subset of the concept
learning researchers have used a sampling account to explain             space C ⊆ C .
the advantage of teaching (Xu & Tenenbaum, 2007; Shafto                      Ordered pairs formed of features and Boolean labels such
& Goodman, 2008; Balbach, 2008; Zilles, Lange, Holte, &                  as those found in an instance ( f , b) ∈ x are referred to as spec-
Zinkevich, 2009): the teacher selects examples to give to the            ifications. Ordered pairs between instances and Boolean la-
learner in order to maximize the probability of learning the             bels such as those found in concepts (x, b) ∈ c are referred to
correct concept, given limitations on the number of exam-                as examples. Finally, let the sample space be any set of ex-
ples. These approaches assume that learners have a known                 amples that can be found in a concept, S = {s ⊆ c | c ∈ C }.
bias—that, a priori, learners are more inclined toward some              For abbreviation, samples and concepts may be represented
concepts than others (e.g for ‘red’ over ‘red or square’ when            as strings over {0, 1, ∗} such that (xi , A[i]) ∈ s for all A[i] 6= ∗.
both are consistent with the evidence).                                      A concept is consistent with a sample if each example in
   In this paper, we analyze the role of a prior bias in the             the sample is also in the concept.
domain of Boolean concepts (Shepard, Hovland, & Jenkins,                                   Cons(s,C) = {c ∈ C | s ⊆ c}                      (1)
1961) and show three results. First, without a bias, sampling-
based teaching requires the observation of each and every ex-                For an intuitive illustration of these definitions, consider
ample. Second, there are many optimal biases, and hence no               the two features ‘red’ and ‘square’. These two features can be
single rational choice. Third, as the number of features grows,          combined to form four instances: ‘red and square’, ‘not red
teaching is not possible if the teacher and learner have a dif-          and square’, ‘red and not square’, ‘not red and not square’.
ferent bias. These results indicate that sampling alone is an            A concept can be thought of as a definition for an unknown
incomplete account of the effectiveness of teaching.                     category such as ‘fep’. The four instances allow for 16 con-
   We offer a novel account of teaching Boolean concepts                 cepts from ‘feps are red’ to ‘feps are either not red and not
called cooperative inference that uses a notion of cooperation           square or red and square’ to ‘feps are nothing’ (i.e. all four
(such as the idea that a teacher may omit unnecessary features           instances are false). A teacher samples evidence in the form
from examples) rather than biases to explain the effectiveness           of labeled examples to teach a concept; in order to teach the
of teaching. We show that this method allows teaching with-              concept ‘feps are red’ a teacher might use the following ex-
out prior communication of the learner’s bias. For the coop-             amples ‘red and square is a fep, not red and square is not a
erative inference model, the difficulty of teaching depends on           fep’.
the complexity of the target concept irrespective of the con-                Within this framework, a learner is both consistent and
cept space, and thus permits teaching in concept spaces with             class-preserving. Consistent means that learners will only
                                                                     1401

learn a concept that is consistent with the teaching sample.                Many optimal biases
Class-preserving means that learners will only learn a con-                 If a learner needs a bias in order for teaching to be effective,
cept that is in the concept class. Equivalently, the learner can            the most sensible choice for the bias is the one that would, on
be thought of as beginning with all concepts in the concept                 average, lead to the most efficient teaching.
class and ruling out concepts that are inconsistent with the                   We will consider two types of biases: The first, ordered
sample.                                                                     bias, adopts a total pre-order2 on the concepts in C such that
   Assuming that the teacher knows the concept class of the                 lower ordered concepts are given priority. One example of
learner, the teaching set is the sample with the fewest exam-               this is the Occam’s Razor bias (Balbach, 2008; see also Good-
ples that will teach the target concept for any consistent, class-          man et al., 2008) where concepts are judged by the number
preserving learner—i.e. that will rule out all other concepts in            of terms (separated by ‘or’s) in the description, e.g. ‘feps are
the concept class.                                                          red and square’ which has one term would be given prior-
Definition 2 (Teaching Set) The teaching set of a target con-               ity over ‘feps are red or square’ which has two. The second,
cept is the minimal sample that teaches the target concept to               functional bias, permits a redefinition of the set of concepts
all consistent and class-preserving learners.                               that are consistent with any sample. This bias is more com-
                                                                            plex but also opens up many more possibilities. An example
           TS(c,C) = arg min{|s| | Cons(C, s) = {c}}                 (2)
                            s∈S                                             of this is the Subset Teaching Set (Zilles et al., 2009) where a
                                                                            learner rules out concepts based on a prediction of the teach-
The teaching dimension for a target concept is the size of the              ing set rather than based on whether or not an example is
teaching set.                                                               consistent (see Shafto & Goodman, 2008, for a probabilistic
   One learner’s concept class might include only three con-                example of such a bias). For both kinds of bias, we show that
cepts, ‘feps are red’, ‘feps are not square’ and ‘feps are not              there are many optimal biases and that the selection of a bias
red and not square’. Then, a teacher could teach ‘feps are red’             through prior communication would allow arbitrarily efficient
with a single labeled example, ‘red and square is a fep’, be-               teaching—effectively equivalent to telepathy.
cause that example rules out the other two concepts. However,                  For an ordered bias, the learner learns the lowest-order con-
the teacher would require more than one example to teach                    sistent concept. Thus, an ordered bias amounts to a mod-
‘feps are not square’ because none of the examples in that                  ification of the Cons() function. Given c and , we use
concept rule out both other concepts.                                       Cc = {c0 ∈ C | c0  c} to refer to the set of concepts of the
                                                                            same or lower order as c and
Ineffective without a bias
To this point, models of teaching have assumed various learn-                   Cons(s, C , ) = {c0 ∈ Cc | for all c ⊇ s} and                   (4)
ing biases. Consider learning from an unknown source where,
                                                                                   TS(c, C , ) = arg min{|s| | Cons(s, C , ) = {c}} .           (5)
without a bias, all consistent concepts are equally likely and                                        s∈S
thus there can be no learning (Watanabe, 1969). We show that
without a bias, there can also be no sampling-based teaching.                  Next, we develop a novel ordered bias called the Hamming
                                                                            distance bias and show that it minimizes the average teach-
Theorem 1 (Teaching without bias) Given a set of n fea-                     ing dimension and is thus optimal. Informally, the proof is as
tures Fn and a concept class, Xn = {0, 1}Fn , Cn = {0, 1}Xn .               follows. Each ordered bias includes a least-element concept
The teaching set for any c ∈ Cn must include every instance in              that is of equal or lower order to all other concepts in the con-
Xn labeled according to the concept such that TS(c,Cn ) = c.                cept class. Any ordered bias would, at minimum, require the
                                                                            teaching set of a target concept to include examples sufficient
P ROOF Let s, the teaching set for c, label all but m instances             to rule out the least-element concept (because it is necessarily
in c, such that |c \ s| = m.1                                               in the set of concepts of lower order than the target concept).
   The set of unlabeled instances can be used to form a set                 For the Hamming distance bias, we show that the teaching set
                                                               ∗
of concepts X ∗ = {x | (x, b) ∈ c \ s}, C∗ = {0, 1}X . These                of each concept is the minimal set of examples sufficient to
concepts may be used to construct Cons(s,Cn ), the set of all               rule out the least-element concept and therefore it is a mini-
concepts in Cn consistent with the s.                                       mal ordered bias.
                           n                                 ∗
                                                               o
         Cons(s,Cn ) = c = s ∪ c0 for all c0 ∈ {0, 1}X               (3)    Theorem 2 (Hamming distance bias) Let h(c1 , c2 ) be the
                                                                            Hamming distance between c1 and c2 such that h(c1 , c2 ) =
   It follows that Cons(s,Cn ) = {c} iff Xn = 0—i.e./       m = 0 and       |c1 \ c2 |. Given an origin concept, c∗ , the Hamming distance
no instances are left unlabeled—and thus TS(c, C ) = c.                    bias is h(c∗ ) = {(c1 , c2 ) ∈ C × C | h(c∗ , c1 ) ≤ h(c∗ , c2 )} and
   Without a bias, a teacher must label every instance in order             is an optimal ordered bias.
to teach the target concept—teaching is no better than other                    2 A total-order 4 on a set X is a partial order such that any two
                                                                                                  t
sampling methods, all of which trivially teach a concept when               elements in X are comparable (i.e. for all a, b ∈ X either a 4t b or
allowed to sample the entire example space.                                 b 4t a). A pre-order 4 p on a set X is a total order that is both reflex-
                                                                            ive (a 4 p a) and transitive (a 4 p b and b 4 p c implies a 4 p c) for all
    1A \ B is the set difference such that A \ B = {x ∈ A | x ∈/ B}.        a, b, c ∈ X.
                                                                        1402

P ROOF Let c0 be an ordered bias with c0 as a least element,                st = TS(c,Ct ), while the learner’s class Cl is the class that is
i.e. c0 c0 c for all c ∈ C . Then c0 is in Cc0 c for all c ∈ C and         preserved when the learner uses the sample to rule out other
in order to rule out c0 , the teaching set for any c must include            concepts, C0 = Cons(TS(c,Ct ),Cl ). Given Ct and Cl , we say
c \ c0 ,                                                                     that a concept c is teachable iff {c} = Cons(TS(c,Ct ),Cl ) and
                        TS(c, C , c0 ) ⊇ c \ c0 .                   (6)     we use the following indicator function such that Teach(c) =
                                                                             True if the concept c is teachable and False otherwise.
   For h(c∗ ) and any c1 , c2 ∈ C , c1 6= c2 if c1 h(c∗ ) c2 (i.e.
                                                                                To begin with, concepts not in either Ct or Cl are trivially
|c1 \ c∗ | ≤ |c2 \ c∗ |) then (c2 \ c∗ ) * c1 . So c2 \ c∗ is sufficient
                                                                             unteachable. Of the concepts in the intersection of the teacher
to rule out any concept of a lesser order, i.e.
                                                                             and learner’s classes c ∈ Ct ∩ Cl , a concept is teachable iff
               Cons(c2 \ c∗ , {c1 , c2 }, h(c∗ ) ) = {c2 }          (7)     each example in the teaching set from the learner’s perspec-
                                                       ∗                     tive TS(c,Cl ) is included in the teaching set from the teacher’s
                            TS(c, C , h(c∗ ) ) = c \ c .            (8)     perspective TS(c,Ct ). That is, each example in TS(c,Cl ) rep-
                                                                             resents a necessary condition for the teachability of c.
   From eq. (6), the Hamming distance bias results in the min-
imal size teaching set for each concept and is thus optimal.                   For example, if Ct = {00, 01, 10} and Cl = {00, 01, 11},
                                                                             then the teaching set from the teacher’s perspective includes
   For example, if the origin concept is the concept with all                both examples TS(00,Ct ) = 00. From the learner’s perspec-
negative examples, c∗ = 000 . . . , the teaching set for each tar-           tive, 00 includes the teaching set TS(00,Cl ) = ∗0, so 00 is
get concept c will include only the examples that differ from                teachable. On the other hand, 01 is not teachable because the
c to c∗ , or, in other words, the set of positive examples in c.             teaching set from the learner’s perspective requires both ex-
Thus the average teaching dimension for the Hamming dis-                     amples while the teacher’s includes only one, TS(01,Ct ) =
tance bias is the average number of positive examples in each                ∗1.
                n
concept, or 22 = 2n−1 for a concept space with n features.                      The teaching set from the teacher’s perspective includes
   The Hamming distance bias is optimal without regard to                    an example only when it is necessary to rule out a concept
the origin concept, so the average teaching dimension would                  that hasn’t already been ruled out by other examples in the
not change if the origin concept were changed. What does                     teaching set. We refer to such concepts as adjacent concepts.
change, however, is the number of examples required to teach                 Given a sample, we say that a concept is adjacent along an
particular concepts—especially the origin concept which can                  example when the concept would be ruled out if the example
be taught with an empty teaching sample.                                     is included in the teaching set but not otherwise. To illustrate,
   Rational selection of a functional bias is similar to that                imagine that the teaching set for a target concept from the
of an ordered bias, with the exception that, because a func-                 learner’s perspective is TS(c,Cl ) = 001∗. The teacher’s class
tional bias may modify which concepts are consistent with                    must include at least one concept adjacent to the third exam-
which samples in any way, there is no analogous constraint to                ple (either 0000 or 0001) otherwise TS(c,Ct ) will not include
eq. (6). Thus, a functional bias may use any set of examples to              that example (e.g. TS(c,Ct ) = 00 ∗ ∗).
teach a target concept so long as each concept is taught with                   To determine the probability that a concept is teachable we
a different set of examples.                                                 model a process where the teacher’s class is determined by
   The minimal functional bias would assign a concept to                     randomly drawing without replacement from the set of con-
each unique s ∈ S , beginning with the smallest. The first                   cepts. Then, for each example, the hypergeometric distribu-
concept would be taught with s = 0,        / the following concepts          tion gives the probability that all adjacent concepts are re-
would be taught with samples |s| = 1, and so on. For |s| = i,                moved. Let U = |C | be the size of the universe of concepts
and the number
            n
                    of features, n, the number of unique samples             and R be the number of concepts removed from C to get the
in S is 2i 2i . It is outside of the scope of this paper to de-              teacher’s class of size T = |Ct | such that U = R + T . Given an
termine the average teaching dimension for the optimal func-                 example in the teaching set from the learner’s perspective, let
tional bias, though it is clearly smaller than that of the optimal           Ae be the number of adjacent concepts
ordered bias.
   Both the optimal ordered and functional biases have a free                                                             Ae  U−Ae 
parameter in the choice of an origin concept. So there are at                                                             Ae R−Ae
                                                                                      P(Teach(c) = False | R,Cl , e) =         U
                                                                                                                                      .    (9)
least as many distinct optimal choices as there are concepts                                                                   R
and for each bias there is at least one concept that can be
taught with an empty teaching sample.                                           As the number of features becomes very large, n → ∞,
                                                                             the number of concepts U does as well. If R remains con-
Impossible for unknown bias                                                  stant, then limn→∞ P(Teach(c) = False) = 0 meaning that a
First, the different roles of the two biases should be clarified.            target concept will be teachable in the limit. But, because
For the following analysis, we will use classes Ct ,Cl ⊆ C to                U = R + T , a constant R would mean that the teacher’s con-
                                                                                                                                        n
stand in for more complex biases without a loss of generality                cept class increases superexponentially, i.e. O(T ) = 22 , and
(see eq. (4)). The teacher’s class Ct is used to determine a                 this implies an implausible lack of constraints on the size of
teaching set that is consistent only with the target concept,                a concept space. If T increases less than superexponentially
                                                                         1403

such that R approaches U in the limit R = U − T → U, the              A partial example (x0 , b), may match some subset of con-
target concept will not be teachable.                                 sistent instances Cons(x0 , X). Matching instances are given
   Stirling’s approximation of the factorial—appropriate be-          the label b, allowing a partial example to effectively stand
cause both U and R are very large—provides a simplification           in for one or more typical examples. Based on cooperation,
of the hypergeometric distribution in eq. (9) for n → ∞.              a learner infers that all consistent examples should match a
                                                                      partial example3
                    Ae  U−Ae 
                    Ae R−Ae        (U − Ae )U RU
                                                                                                              Cons(x0 , X ) × b .
                                                                                                       [
                         U
                                ∼                            (10)                   Match(s,C) =                                       (12)
                                   U U (R − Ae )U
                         R                                                                          (x0 ,b)∈s
Then, because R → U, limn→∞ P(Teach(c) = False) = 1.                     Imagine the addition of a third feature to our intuitive ex-
   This result depends on a set of reasonable assumptions: that       ample, so that we have ‘red’, ‘square’, and ‘small’. For con-
the set of features is large, that the set of concepts considered     cepts such as ‘feps are red or small and square’, a teacher
by the teacher and learner is much smaller than the set of all        would likely wish to use all three features, but for others, such
concepts, and that the teacher cannot predict which concepts          as ‘feps are red and square’, a teacher might find it helpful not
are in the learner’s concept class. Given these assumptions,          to include the feature ‘small’. Equation (12) represents our in-
the likelihood that a concept could successfully be taught via        terpretation of cooperation for this case; when a cooperative
sampling approaches zero. The fact that more complex biases,          source omits features from examples, the learner assumes that
such as ordered, reduce to a concept class (see eq. (4)) means        the partial example matches all consistent instances. So, ‘red
that this result applies to all such biases.                          and square is a fep’ would imply that ‘red and square and
                                                                      small is a fep’ as well as ‘red and square and not small is a
                   Cooperative Inference                              fep’.
Previous accounts of teaching leverage the idea that teachers            This use of partial examples allows for a powerful im-
purposefully choose which examples to provide to the learner          provement in the efficiency of teaching. First, we show that
but the above resuls show that this is an incomplete account of       with cooperative inference a teacher only needs to use fea-
the effectiveness of teaching. Such an account cannot explain         tures known to be relevant in order to teach a concept. A fea-
the effectiveness of teaching in realistic situations, such as        ture is relevant if, for at least one pair of differently-labeled
when the bias of the learner is unknown or the set of features        instances in the concept, the feature is the only feature to
is large.                                                             change4
   In what follows, we propose a solution. Building on previ-
ous approaches that analyze the consequences of the teacher’s                       Relevant(c) = { f | (x0 , 0), (x1 , 1) ∈ c}        (13)
ability to select examples, we analyze the consequences of the                            where x0 4 x1 = {( f , 0), ( f , 1)} .       (14)
teacher’s ability to select features. When a teacher communi-
cates an example, the teacher may omit unnecessary features,          Theorem 3 (Relevant instance space) Given a concept c let
resulting in what we call a partial example. We show that             FR be a subset of F that contains all features that are relevant
this extension leads to a number of important consequences:           with respect to c and XR be the set of partial instances formed
teachers may successfully teach when ignoring all irrelevant          of FR , XR = {0, 1}FR . Using cooperative inference, a teacher
features, the complexity of teaching depends on the complex-          may successfully teach c by labeling each partial instance
ity of the target concept irrespective of the concept space, and      x0 ∈ XR according to any consistent full instance, x ⊇ x0 , x ∈
there is a natural representation for teaching.                       X.
   Let a partial instance be any subset of a typical instance         P ROOF The theorem follows immediately from the defini-
X 0 = {x0 ⊆ x | x ∈ X }. A partial example is a partial instance      tions.
paired with a Boolean label.                                             Given an example in the teaching set, (x0 , b) ∈ s,
   To understand how we will use partial examples, recognize          x0 ∈ XR , consider the set of matching examples sx0 =
that the relationship between partial instances and typical in-       {(x, b) | x ∈ Cons(x0 , X )}. Note that each example in sx0 is a
stances is analogous to the relationship between samples and          superset of x0 and so must have the same label for each rel-
concepts; partial instances are a subset of a typical instance        evant feature. Assume for the sake of contradiction that two
and teaching samples are a subset of concepts. In both cases,         examples in sx0 are differently labeled, (x, 0), (x∗ , 1) ∈ sx0 . We
inference is needed to relate the incomplete version to the           may build a series of examples x0 , x1 , . . . xi beginning with
complete version. The following application of partial exam-          x0 = x, and for each step, changing the label for one feature
ples takes advantage of the fact that the teacher and learner         in x0 to match x∗ such that i = 12 |x 4 xi |.
are mutually cooperative.
                                                                          3 Others (e.g. Csibra & Gergely, 2009) have informally proposed
   First consider the set of instances that may be consistent
                                                                      a similar idea, that helpful teachers offer generic or semantically
with a particular partial example                                     generalizeable examples.
                                                                          4 Here, 4 refers to the symmetric difference such that A 4 B =
                   Cons(x0 , X ) = {x | x0 ⊆ x} .            (11)     (A \ B) ∪ (B \ A).
                                                                  1404

   Because, x0 and xn are differently labeled, there must exist                        Through a similar process, the negative examples can be
some i such that (xi , b), (xi+1 , b) ∈ c. This implies that the                   ‘and-ed’ together to form a concept that is negative for all of
feature f such that xi 4 xi+1 = {( f , 0)( f , 1)} is relevant and                 the instances in negative examples and positive otherwise.
is a contradiction.                                                                                                                                
So, a teacher who knows that only ‘red’ and ‘square’ are rel-                                                 ^           _                 _
                                                                                           ext2int- (c) =                       f   ∨             f         (17)
evant to the concept ‘fep’, may omit the entire universe of
                                                                                                           (x,0)∈c    ( f ,0)∈x         ( f ,1)∈x
irrelevant features.
   Until this point, our formal discussion of concepts used an                         If we use the set of examples in equation eq. (15), we
extensional sense, where a concept is defined by the set of                        can form an intensional concept both ways. From the posi-
outputs for all inputs. The alternative, intensional sense, is                     tive examples we have c+ = ( f0 ∧ f1 ) ∨ ( f0 ∧ f1 ) ∨ ( f0 ∧ f1 )
to represent concepts as a rule that generates the appropriate                     and from the negative examples we have c− = ( f0 ∨ f1 ). In
output label based on the content of input and is more simi-                       the case where the provided examples are partial examples,
lar to our intuitive discussion. For example, the concept with                     the learner can infer the concept label for instances not cov-
intension c = f0 ∨ f1 (i.e. ‘feps are red or square’) has the                      ered by the examples. If the intensional form of a concept
following extension over two features                                              is known, the output label can be predicted for any instance
                                                                                 defined over the same features and thus intensional concepts
                                   ({( f0 , 0), ( f1 , 0)}, 0)
                                                                                   conveniently provide a set of potentially relevant features as
                                
                                                               
                                   ({( f0 , 0), ( f1 , 1)}, 1)
                                                               
                  f0 ∨ f1 =                                         .     (15)     in theorem 3. Thus, a teacher can communicate an intensional
                                   ({( f0 , 1), ( f1 , 0)}, 1)
                                                                                   concept through the use of partial examples.
                                
                                                               
                                   ({( f0 , 1), ( f1 , 1)}, 1)
                                                               
                                                                                       Just as intensional concepts can be derived from the exten-
   The intensional concept f0 ∨ f1 will label instances out-                       sional definition, a sample compatible with cooperative infer-
side of eq. (15), whereas the extensional concept is only de-                      ence can be derived from an intensional concept. Let cd and
fined for the included examples. For f0 ∨ f1 , the instance                        cc be concepts in DNF and CNF form, respectively.
{( f0 , 0), ( f1 , 1), ( f2 , 0)} would be labeled positive and the                                                                                       
instance {( f0 , 0), ( f1 , 0), ( f2 , 0)} would be labeled negative                                     [ [                                                
                                                                                      int2ext+ (cd ) =
                                                                                                                                        [
while both would be undefined for the extensional definition
                                                                                                                         {( f , 1)} ∪         {( f , 0)}, 1
                                                                                                        t∈c
                                                                                                               f ∈L                                         
in eq. (15).                                                                                                d           t              f ∈Lt
   Given a set of examples, an intensional representation can                                                                                                 (18)
                                                                                                                                                          
be derived and we briefly describe this process.                                                                                                             
                                                                                                        [  [                           [
Definition 3 (Intensional form) A literal is a negated or un-                         int2ext- (cc ) =                   {( f , 1)} ∪         {( f , 0)}, 0
                                                                                                                                       f ∈L
                                                                                                       cl∈c
                                                                                                                                                            
negated variable, e.g. l1 = f is true for ( f , 1) and l2 = f is                                            c      f ∈Lc                     c
true for ( f , 0). A term is a conjunction (i.e. ‘and’) of literals                                                                                           (19)
such as t = l0 ∧ l1 ∧ . . . and is false unless all of the literals
are true. A clause is a disjunction (i.e. ‘or’) of literals such as                    These two equations imply a logical correspondence be-
cl = l0 ∨ l1 ∨ . . . and is true unless all of the literals are false.             tween a teaching set for a concept constructed of partial ex-
Each term       and clause is associated           with a set of literals such     amples and intensional forms of that concept. If a teacher has
            V                         W
that t = l∈Lt l and cl = l∈Lcl l. A concept is in Disjunc-                         a concept stored intensionally, such as c = f1 ∨ f0 ‘feps are
tive Normal Form (DNF) if it is a disjunction of terms such                        red or square’, the teacher easily convert this definition to the
as (l ∧ l ∧ . . . ) ∨ (l ∧ l ∧ . . . ) ∨ . . . and a concept is in Con-            teaching sample {(( f0 , 1), 1), (( f1 , 1), 1)} ‘red is a fep, square
junctive Normal Form (CNF) if it is a conjunction of clauses                       is a fep’.
(l ∨ l ∨ . . . ) ∧ (l ∨ l ∨ . . . ) ∧ . . .                                            This means that logical operations on one form can be
                                                                                   leveraged for the other, e.g. simplifying the intensional def-
   This allows us to derive an intensional concept from an ex-                     inition results in an equivalent simplification of the exten-
tensional definition. The first method is to collect all of the                    sional teaching set. For example the rule used to combine
positive examples and from each, form a term that is true                          terms and clauses in the classic Quine-McCluskey algorithm
when the specifications for that example are true and false                        (Roth, 2013)—e.g. ‘feps are either red and small or red and
otherwise. At this point, each positive example has an anal-                       not small’ → ‘feps are red’—has a logical equivalent with
ogous term that evaluates to true for all instances in positive                    cooperative inference.
examples. To form an intensional concept, these terms then
                                                                                       If a teacher has a concept in both DNF and CNF form, a
need to be ‘or-ed’ together. The resulting concept would be
                                                                                   concept can be inferred from the results of eqs. (18) and (19)
one that is true for all instances in positive examples and false
                                                                                   using partial examples, c = Match(s+ , X ) ∪ Match(s− , X )
otherwise.
                                                                                   where s+ = int2ext+ (cd ) and s− = int2ext- (cc ) and vice versa
                                                                     
                                                                                   for eqs. (16) and (17). A teacher may not need both DNF and
        ext2int+ (c) =
                                _            ^                ^
                                                  f    ∧           f    (16)     CNF forms; a very large feature space combined with sparse
                              (x,1)∈c    ( f ,1)∈x        ( f ,0)∈x                positive instances would support a principle of truth (see e.g.
                                                                               1405

P. N. Johnson-Laird, 2001). In such a case, if a teacher were        ertheless, both problems require a priori assumptions about
to only present positive examples a cooperative learner would        the other party, and a similar approach may yield insights in
infer that the teacher was helpfully omitting the vast set of        the other domain as well.
negative examples.                                                      A critical component of any model of human behavior is
   This correspondence between the intensional form of a             the choice of bias. Before now, these choices have been made
concept and its teaching set suggests that DNF is a natural          by a combination of intuition and informal argument (see es-
representation for teaching. When a concept is stored in this        pecially Anderson, 1990; Feldman, 2000; Goodman et al.,
way, teaching no longer involves searching a space of teach-         2008; Goodwin & P. Johnson-Laird, 2011). We have pre-
ing samples for the solution; rather, the solution is the rep-       sented a formal analysis that provides an a priori justifica-
resentation itself. In this way, teaching via cooperative in-        tion for choice of bias in the case of teaching. However, the
ference would inform sampling biases through representa-             promise of this work is in the generality of the approach, and
tion. A DNF-based representational bias is consistent with           we are optimistic that similar methods can be applied to more
some of the most successful models and experiments in con-           general learning problems.
cept learning (Feldman, 2000; Goodwin & P. Johnson-Laird,
2011; Goodman et al., 2008).                                                                      References
                                                                     Anderson, J. R. (1990). The adaptive character of thought. Psychol-
                                                                             ogy Press. (Cit. on p. 6).
                         Discussion                                  Anthony, M., Brightwell, G., Cohen, D., & Shawe-Taylor, J. (1992).
                                                                             On exact specification by examples. In Proceedings of the
In cognitive science and machine learning, previous models                   fifth annual workshop on computational learning theory
have sought to explain teaching through the helpful sampling                 (pp. 311–318). New York: ACM. (Cit. on p. 1).
of examples with respect to a known bias. We have argued             Balbach, F. J. (2008). Measuring teachability using variants of the
                                                                             teaching dimension. Theoretical Computer Science, 397(1),
that there is no single rational bias, that if the teacher does              94–113. (Cit. on pp. 1, 2).
not know the learner’s bias, the probability of teaching goes to     Clark, H. H. & Wilkes-Gibbs, D. (1986). Referring as a collaborative
zero as the number of features increases, and that a sampling                process. Cognition, 22(1), 1–39. (Cit. on p. 6).
                                                                     Csibra, G. & Gergely, G. (2009). Natural pedagogy. Trends in cog-
account is ineffective without a bias. Thus, sampling alone is               nitive sciences, 13(4), 148–153. (Cit. on pp. 1, 4, 6).
an incomplete explanation of teaching.                               Feldman, J. (2000). Minimization of boolean complexity in human
                                                                             concept learning. Nature, 407(6804), 630–633. (Cit. on pp. 1,
   We have proposed a solution that begins with a notion of                  6).
cooperation instead of biases. In the cooperative inference          Goldman, S. A. & Mathias, H. D. (1993). Teaching a smarter learner.
model, a teacher not only samples examples but is able to                    In Proceedings of the sixth annual conference on computa-
                                                                             tional learning theory (pp. 67–76). ACM. (Cit. on p. 1).
omit unnecessary feature specifications. The most immediate          Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths, T. L.
effect of cooperative inference is a novel, powerful, and real-              (2008). A rational analysis of rule-based concept learning.
istic account of teaching.                                                   Cognitive Science, 32(1), 108–154. (Cit. on pp. 1, 2, 6).
                                                                     Goodwin, G. P. & Johnson-Laird, P. (2011). Mental models of
   In the sampling-based accounts of teaching, the teacher                   boolean concepts. Cognitive psychology, 63(1), 34–59. (Cit.
searches the space of teaching samples for one that rules out                on pp. 1, 6).
all other concepts in the concept space—so the computational         Johnson-Laird, P. N. (2001). Mental models and deduction. Trends
                                                                             in Cognitive Sciences, 5(10), 434–442. (Cit. on p. 5).
complexity of the model increases with the size and complex-         Roth, J. C. H. (2013). Fundamentals of logic design. Cengage Learn-
ity of the concept space. This represents a significant limita-              ing. (Cit. on p. 5).
tion, as the size of the concept space increases superexpo-          Shafto, P. & Goodman, N. D. (2008). Teaching games: statistical
                                                                             sampling assumptions for learning in pedagogical situations.
nentially with the number of features and a realistic world                  In Proceedings of the thirtieth annual conference of the cog-
includes many—if not infinitely many—features. For cooper-                   nitive science society (pp. 1632–1637). (Cit. on pp. 1, 2).
ative inference, the complexity of teaching increases with the       Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961). Learning
                                                                             and memorization of classifications. Psychological Mono-
complexity of the target concept rather than the complexity of               graphs: General and Applied, 75(13), 1–42. (Cit. on p. 1).
the concept space. Additionally, the model indicates a natural       Shinohara, A. & Miyano, S. (1991). Teachability in computational
representation for teaching such that once a concept has been                learning. New Generation Computing, 8, 337–347. (Cit. on
                                                                             p. 1).
learned through teaching or represented for teaching, minimal        Tomasello, M., Carpenter, M., Call, J., Behne, T., Moll, H., et al.
work is needed in order to teach the concept again.                          (2005). Understanding and sharing intentions: the origins
   Our work is related to a variety of trends in cognition and               of cultural cognition. Behavioral and brain sciences, 28(5),
                                                                             675–690. (Cit. on p. 6).
cognitive development (Csibra & Gergely, 2009; Tomasello,            Watanabe, S. (1969). Knowing and guessing: a quantitative study of
Carpenter, Call, Behne, Moll, et al., 2005; Clark & Wilkes-                  inference and information. Wiley New York. (Cit. on p. 2).
Gibbs, 1986). Most notably, the assumption of common                 Xu, F. & Tenenbaum, J. B. (2007). Word learning as bayesian infer-
                                                                             ence. Psychological review, 114(2), 245. (Cit. on p. 1).
knowledge and common ground is key to theories of language           Zilles, S., Lange, S., Holte, R., & Zinkevich, M. (2009). Teaching
and communication (Clark & Wilkes-Gibbs, 1986). Our prob-                    dimensions based on cooperative learning. (Cit. on pp. 1, 2).
lem differs from the standard formulation in that theories of
language have focused on how to link utterances to referents.
In contrast, we have focused on the case where referents are
clear, and the challenge is linking examples to concepts. Nev-
                                                                 1406

