UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Semantics of SIT, STAND, and LIE Embodied in Robots
Permalink
https://escholarship.org/uc/item/60f5v9br
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Loetzsch, Martin
Spranger, Michael
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       The Semantics of SIT, STAND, and LIE Embodied in Robots
                                                Michael Spranger (spranger@csl.sony.fr)
                               SONY Computer Science Laboratory Paris, 6, Rue Amyot, 75005 Paris, France
                                             Martin Loetzsch (martin.loetzsch@gmail.com)
                                 VUB AI Lab, Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium
                                  Abstract                                are less obligatory in their use of posture verbs and allow for
                                                                          both coding strategies in this example.
    In this paper we demonstrate (1) how a group of embodied ar-
    tificial agents can learn to construct abstract conceptual rep-          As suggested by (Lemmens, 2002a), the use of posture
    resentations of body postures from their continuous senso-            verbs can be grouped into three basic scenarios: First, pos-
    rimotor interaction with the environment, (2) how they can
    metaphorically extend these bodily concepts to visual expe-           tural uses (describing human postures) are permitted in most
    riences of external objects and (3) how they can use their ac-        Germanic languages and are even obligatory in some of them
    quired embodied meanings for self-organizing a communica-             (Oosten, 1984). They are grouped around the three central
    tion system about postures and objects. For this, we endow
    the agents with cognitive mechanisms and structures that are          meanings SIT, STAND, and LIE, which have been exten-
    instantiations of specific ideas in cognitive linguistics (namely     sively analyzed with respect to their underlying semantics.
    image schema theory) about how humans relate motor and vi-            Almost all authors have found visual features such as ‘max-
    sual space. We show that the agents are indeed able to perform
    well in the task and thus the experiment offers a concrete oper-      imally vertically elongated’ or ‘resting on one’s feet’ to be
    ationalization of these theories and increases their explanatory      connected to STAND, ‘maximally horizontally elongated’ or
    power.                                                                ‘on one of the sides’ to LIE, and ‘more or less square’ or
    Keywords: cognitive semantics; image schemas; metaphor;               ‘on the buttocks’ to SIT (Lemmens, 2002a; Borneto, 1996;
    autonomous robots; lexicon acquistion; language games                 Newman, 2002). Second, locational uses are semantic exten-
                                                                          sions of these anthropocentric perceptual schemas. The vi-
                              Introduction                                sual property of vertical/horizontal elongation, for example,
Speakers of Germanic languages are strongly committed to                  has been extended to locate any entity in space (see the ex-
using posture verbs for describing the location of human sub-             amples above). Another example is the use of posture verbs
jects whereas for example speakers of Romance languages do                for conveying orientation (see Borneto, 1996 for an analysis
not. Speakers of English for example would rather say “He                 of the German liegen and stehen). Third, metaphorical ex-
sits on the couch” than “He is on the couch”. Some of these               tensions are found in some languages for locating concrete
languages have also extended the usage of these words to the              entities in an abstract space (Lemmens, 2002a). The previ-
posture of animals and even to describe the ”posture” of ob-              ously mentioned use of SIT for being in a financial crisis is
jects. Furthermore, Dutch has an extremely productive use of              an example of the latter.
these verbs in a metaphorical sense. In Dutch one sits in an                 Cognitive linguists have argued that these verbs are
economical crisis.                                                        metaphorically extended from the domain of bodily posture
    The following example from Lemmens (2002b) highlights                 (SIT/LIE/STAND) using notions such as image schemas (e.g.
the typological differences in the usage of lie across some               Johnson, 1987; Croft & Cruse, 2004): Image schemas are
example languages (* denotes unacceptable usage):                         pieces of knowledge that are represented as patterns of re-
1.(a)   There are clothes on the counter. (English)                       occurring bodily experiences. They emerge from continuous
  (b)   Il y a des vêtements sur le comptoir (French)                    sensorimotor activity, that is, they are developed and extended
  (c)   Det är/finss kläder pådisken. (Swedish)                        as we move through the world, direct our attentional focus,
  (d)   * Er zijn kleren op de toonbank. (Dutch)                          manipulate objects, orient ourselves spatially and temporally,
                                                                          and so on. Furthermore, they are agnostic to sensory modal-
2.(a)   The clothes are lying on the counter. (English)
  (b)   * Les vêtements couchent sur le comptoir (French)
                                                                          ities and structure not only bodily but also non-bodily expe-
  (c)   Det ligger kläder pådisken. (Swedish)                           rience via metaphors. Using image schema theory, Gibbs et
  (d)   Er liggen kleren op de toonbank. (Dutch)                          al. (1994) considered for instance BALANCE, VERTICALITY
                                                                          and RESISTANCE as the key image schematic accounts for the
In Dutch as a Germanic languages it is more or less obligatory            polysemous meanings of the English stand and showed that
to use a posture verb for describing the location or posture              “... people tacitly believe there are significant connections
of the clothes, whereas in French it is unacceptable to use               between their recurring bodily experiences and the various
the corresponding form coucher (”lie”). On the other hand,                meanings of the polysemous word stand. We argue that the-
Dutch speakers will feel uncomfortable about using zijn (”to              ories of psychological semantics should account not only for
be”) to express the position of the clothes on the table, where           the organization of polysemous words in the mental lexicon,
in French the usage of être (”to be”) or se trouver (”be found”)         but must also be capable of explaining why different senses
is clearly the preferred coding strategy. English and Swedish             of a word make sense to people in the way they do.” Others
                                                                      2546

                                                                                            500   1000  1500  2000   2500  3000
Figure 1: Experiment phase I. Left: A robot performs a series of actions in the world and is observed by the other robot on
the left. Middle: Examples of the actions performed by the robot. Right: The resulting raw data stream, containing both the
proprioceptive data of the performing robot and the visual features extracted by the observing robot.
like Lemmens (2004) have stressed the productivity of ad-
ditional image schemas such as ‘resting on ones base’,
CONTAINMENT and CONTACT to explain some of the encoun-
tered extensions.
   Although these explanations are often very detailed and
closely linked to questions of representation and processing,
only few have tried to operationalize the underlying mech-
anisms and to turn them into a computational model (e.g.
Amant et al., 2006) and nobody has tackled the question of
grounding in actual physical robots – which brings us to the
main topic of this paper. We will show how robotic agents
can construct image schematic categories from sensorimotor
data and extend them to other objects. In order to validate        Figure 2: Example objects in the environment of the agents.
whether the emerging representations are indeed meaning-           The objects have different verticality and horizontality fea-
ful, we will give them the task to agree on a set of names         tures depending on the view point of the agent. However,
for these meanings in series of communicative interactions.        some always have a strong verticality component, like the red
This methodology is very similar to other experiments of our       cone in the back.
group on the emergence of linguistic communication systems
in physical robots, of which recent examples are on flexible
lexicon formation (Wellens, Loetzsch, & Steels, 2008), mark-
                                                                   mental phase (see Figure 1), the robots interact with their en-
ing of spatial perspective (Steels & Loetzsch, 2009), learning
                                                                   vironment and form posture categories from that. For this,
of case grammars (van Trijp, 2008), or the relation of visual
                                                                   one robot performs a series of actions over a time span of
and motor space through language (Steels & Spranger, 2008).
                                                                   10 minutes. These actions include walking and turning mo-
Throughout the next sections we will outline the experimental
                                                                   tions, arm gestures while the robot is standing, lying, getting
setup, perception and categorization mechanisms, the com-
                                                                   up after falling, some sitting motions and means to switch
munication task and finally the results.
                                                                   between these actions. During that, the robot continuously
                                                                   perceives proprioceptive data from its internal sensors. In or-
                   Experimental setup
                                                                   der for the robot to also have access to visual appearances
We use the ”A-series” humanoid robots developed in the AI          of the performed actions, a second robot perceives the scene
lab of the Humboldt University Berlin as an experimental           through his camera and we provide the first robot with that
platform. They are equipped with pan/tilt cameras, servo mo-       data stream. Giving one robot access to what another one
tors and acceleration sensors. For onboard processing and          sees might seem very unnatural, but it overcomes our robots’
thus autonomous operation, the robots feature a PDA on the         lack of a geometric body model that would allow them to de-
back and distributed sensor and processing boards spread out       termine how it looks when they perform an action (another
across the body, linked via a system bus. The robot’s soft-        possibility to provide a robot with proprioceptive and visual
ware architecture provides integrated mechanisms for balanc-       data would be to let the robot perform actions in front of a
ing motion control, vision and behavior.                           mirror, as it was done in Steels & Spranger, 2008). We will
   The experiment consists of two stages: In a first develop-      describe further below how image schemas are extracted from
                                                               2547

                                                                                                             0.3
                                                                                                           0.25
                                                                                                             0.2
                                                                                                           0.15
                                                                                                             0.1
                                                                                                           0.05
                                                                                                               0
                                                                                                           −0.05
                                                                                                                 1  2  3 4  5   6   7
Figure 3: Extraction of visual features. First: the original image stemming from the onboard cameras of the robot. Second:
foreground/background subtracted image. Third: the connected component processing unit has identified a single connected
area, depicted by the bounding box. Fourth: the seven visual features computed for the connected region, shown as a line plot.
this combined data stream. Then, in a second communication
phase, the robots test their acquired categories in two types of     with x̄ and ȳ following from the raw moments M pq
language games: first about robot postures that they perceive
                                                                                                  M10              M01
and then about objects that a human experimenter presents to                                x̄ =       , ȳ =
them (see Figure 2).                                                                              M00              M00
                                                                     with M pq being the raw moment of order (p + q) defined as
                    Sensory experiences
                                                                                          M pq = ∑ ∑ x p yq I(x, y),
We call the raw uncategorized proprioceptive and visual data                                       x  y
stream that the robots perceive as they move around in their
environment sensory experiences. The proprioceptive fea-             where in all formulas I is a function equal to 1, if the pixel
tures are gathered in every time step from the acceleration          x, y is part of the shape the feature is computed for (or 0 oth-
sensors and the motors. The acceleration sensors report two          erwise). Given these definitions, the normalized central mo-
dimensional values that not only reflect the movement of arms        ment η pq of order (p + q) is computed (Mukundan & Ra-
and the acceleration of limbs, but also gravitational forces,        makrishnan, 1998)
which allows the robot to directly sense its orientation in                                            µ pq
space. Furthermore, for each of the motors the actuated and                                 η pq =
                                                                                                      1+ p+q
the actual position of the motor as well as the torque applied                                       µ00 2
to the motor is sensed. Actuated position is the value that was
requested by the behavior control programs, and it is often          The shape descriptor is consequently agnostic of the exact po-
different from the actual position because the motor does not        sition of the object in the image and in the visual field, due to
necessarily reach it. Notice that humans have very similar           being centralized with respect to the center of the shape and
proprioceptive capabilities. When we use muscles to reach            normalized with respect to the total number of pixels (mass of
certain positions we have means to detect whether this po-           the area). Nevertheless these features are powerful for quanti-
sition was reached and we have the inner ear for orienting           fying basic relationships such as the correlation between ver-
ourselves in space.                                                  tical and horizontal elongation η20 , η02 .
                                                                        A sensory experience s at time t is a vector st =
   For the visual part of sensory experiences, we do not use                         T
the whole image but compute a set of seven translation and             f1 · · · fn , where n is the dimensionality of the sensory
scale-invariant shape features for objects found in the image        experience, and f1 , ..., fn are the values of the proprioceptive
(see Figure 3). Every 80 ms the digital camera of the robot          sensors and the computed visual features.
provides a new two dimensional image. The vision system
                                                                                            Categorization
then first uses running average background subtraction to de-
tect robots or other objects as connected regions that suffi-        To organize the continuous flow of sensory experiences into
ciently differ from background. These connected areas are,           categories of bodily related meaning, we employ an un-
after being noise filtered with morphological operators, pro-        supervised machine learning clustering technique called K-
cessed as belonging to separate objects in the world.                means (Lloyd, 1982). Unsupervised means that K-means au-
   Shape descriptor features are then computed from the im-          tonomously finds clusters in the incoming data without re-
age pixels contained in a connected area using centralized           quiring labeled sets of training data created by a human ex-
normalized moments features: The central moment of order             perimenter. The computed categories are hence grounded in
p + q is computed as follows (Hu, 1962):                             the preconceptual, raw stream of sensory experiences and in
                                                                     nothing else.
               µ pq = ∑ ∑(x − x̄) p (y − ȳ)q I(x, y),                  K-means receives as input the number k of clusters that one
                      x  y                                           would like to find and m unlabeled, unclassified data points.
                                                                 2548

 a)                                             b)                                              a)                                                          b)
                                                                                                     240                                                          0.5
                                                                                                                                                                                                category 1
      250                                            0.5                                                                                                         0.45                           category 3
                                                                           category 1                220
                                                                                                                                                                                                category 2
                                                                                                                                                                  0.4
                                                                           category 2                200
                                                     0.4
      200                                                                  ideal rectangle                                                                       0.35
                               category 1                                                            180
                                                                                                                                         category 3               0.3
                               category 2            0.3                                             160
 height
                                                                                                                                         category 2
                                                                                                height
                                                                                                                                                            η02  0.25
      150                                       η02
                                                                                                     140                                 category 1
                                                                                                                                                                  0.2
                                                     0.2
                                                                                                     120
                                                                                                                                                                 0.15
      100
                                                     0.1                                             100                                                          0.1
                                                                                                         80                                                      0.05
          50                                          0
           50   100     150    200      250                                                                                                                        0
                                                       0   0.1   0.2   η20 0.3    0.4    0.5             60                                                         0   0.1   0.2         0.3   0.4          0.5
                       width                                                                              50         100           150            200                               η20
                                                                                                                           width
Figure 4: Categorization results K-means k = 2 of the raw                                      Figure 5: Categorization. K-means of the raw sensory ex-
sensory experience stream (see Figure 1). We focus on four                                     perience stream (see Figure 1) for k = 3. We center on four
dimensions: bounding rectangle width and height (which are                                     dimensions: bounding rectangle width and height (which are
not part of the centroid, Figure a), as well as two visual fea-                                not part of the centroid, Figure a), as well as two visual fea-
tures, that quantify vertical and horizontal elongation (Figure                                tures, that quantify vertical and horizontal elongation (Figure
b).                                                                                            b).
The result is a set of k categories, which are represented as                                  to the n dimensional stimulus case, we define
centroids (the mean/prototype of a cluster) and that can be
used to partition the data in a metric space. The algorithm                                                                class(s0 ) = arg min ||Ci0 − s0 ||,
                                                                                                                                                        i
starts by first randomly selecting k seed centroids. All data
points of the same class are closest to the same centroid. In                                  where           Ci0
                                                                                                           is constructed from the i-th centroid by select-
a second phase the algorithm iterates until convergence. Un-                                                                                                0
                                                                                               ing the feature
                                                                                                   i        i
                                                                                                               dimensions present in the stimulus Ci =
labeled data points are classified as belonging to the centroid                                  fl1 .. fln̂ . In other words, focussing only on the dimen-
with the smallest Euclidian distance and the centroids are up-                                 sions present in the stimulus it is possible to infer the closest
dated by shifting the mean value given all data points of the                                  centroid with respect to those dimensions. However, since
same class. The algorithm terminates as soon as centroids are                                  centroids are in fact central points in the complete sensori-
not moved anymore, which means that the class of every data                                    motor space this also activates the other features not included
point in the unlabeled data is not subject to change. The out-                                 in the stimulus. For example a robot perceiving another robot
come of the algorithm is a set of k centroids, which cluster the                               performing bodily actions, can classify the visual stimulus
sensorimotor space into k disjoint sets of experience. In other                                with respect to his inventory, thereby effectively activating
words, every point in the sensorimotor space, every sensory                                    the proprioceptive part of the best matching centroid. The
experience belongs to exactly one category.                                                    perceiving robot ergo has a sense of what the other robot is
   The computed centroids can immediately be used to cat-                                      doing or what the internal proprioception of the performing
egorize new incoming sensory experiences that were previ-                                      robot could be like.
ously not encountered by the system (see Figures 4 and 5).                                        Each agent of the population (for the rest of the paper we
Given a set of k centroids C1 , ..,Ck , a new sensory experience                               deal with 10 agent populations) is presented with different but
can be classified using a Minimum Euclidean Distance Clas-                                     similar sensorimotor streams. Consequently, the categories
sifier:                                                                                        constructed by agents are similar but not identical. The cat-
                                                                                               egorization results for K-means clustering (k = 2) of the raw
                                                                                               sensory data stream for a single agent are displayed in Figure
                      class(s) = arg min ||Ci − s||
                                            i                                                  4. The graphs clearly show that the resulting categories (cen-
                                                                                               troids) establish a width-height correlation, linearly separable
where the Ci are the k centroids computed by the K-means                                       well above the square dimension line. The blue line in Fig-
algorithm and s denotes a new sensory experience. No-                                          ure 4b shows an ideal rectangle in the visual field of the robot
tice that partial experiences/stimuli can be classified       as well.                         changing its width and height while keeping its area constant,
That is, given the centroids Ci = f1i .. fni , where n is
                                                       
                                                                                               for illustration purposes. The point in the cusp of that curve
the dimensionality of the centroid and f ji is the value of the                                is an ideal square. Category 2 (which we would call LYING)
feature channel j of centroid i, we can, for        instance, cate-                           ranges from strong horizontal elongation to square, to a little
gorize a visual stimulus s = fl01 .. fl0n̂ with n̂ ≤ n and                                     vertical elongation. Category 1 covers sensory experiences
l1 , .., ln̂ ∈ {1, .., n} being the index subset of visual features of                         which are strongly vertically elongated, and thus visually cor-
the d feature channels of the centroids, using the feature di-                                 responds to STANDING.
mensions of the centroids relevant to that stimulus. Similarly                                    For k = 3 (see Figure 5, categories are further split by cat-
                                                                                        2549

                                                                      drawn from the population and assigned the roles of speaker
                                                                      and hearer. Both robots are shown either a robot in a specific
                                                                      posture or another object (one out of a set of colored bricks
                                                                      of different sizes and each time in a different orientation, see
                                                                      Figure 2 for examples). The speaker then uses the Minimum
                                                                      Euclidian Distance Classifier (as described above) to find the
                                                                      category (which was learnt in the developmental phase) that
                                                                      is closest the sensory experience of the robot/object and re-
                                                                      trieves the name for that category with the highest score from
                                                                      his lexicon and speaks it out to the hearer. When he does not
                                                                      have a name yet for this meaning, he invents a new random
                                                                      word for it and stores it in his lexicon. The hearer looks up
                                                                      that word in his own lexicon and checks whether the category
                                                                      that he associates to the word (with the highest score) is the
                                                                      same as the category that is for him closest to the sensory ex-
Figure 6: Experimental results for a population of 10 agents.         perience. If that is the case, he signals agreement with the
Average communicative success and lexicon size for series of          description and the interaction is a communicative success,
5000 language games, averaged over 25 experimental runs. In           otherwise it is a failure. When the hearer does not know the
the first 1500 interaction agents play language games about           word, he also signals a communicative failure and associates
postures. Starting from interaction 1500 every second lan-            the new word to his conceptualization of the scene. Depend-
guage games is about objects.                                         ing on the outcome of the game, both speaker and hearer in-
                                                                      crease the score of the word used by 0.1 in case of success
                                                                      and decrease it by 0.2 on failure. Words with higher scores
                                                                      are preferred by the agents and words with a score of 0 are re-
egory 3 (which resembles SIT) acting as a separator between           moved from the lexicon, which leads to a conventionalization
the two basic postures LIE and STAND. Category 3, which               of names in the population because words that are success-
lends itself to the SITTING interpretation covers a narrow            fully used by many agents will ‘win’ over other words with
margin with a little bit stronger vertical elongation than hor-       the same meaning.
izontal elongation, which stems from the fact that indeed sit-
ting for this robot looks like a thick vertical elongated rectan-        Since it would be impractical to do hundreds of language
gle. However, there are certain problems with category 3 (in          games with real robots and in order to be able to do repeat-
the k = 3 case) not shown here. The SIT category is almost            able and controlled experiments, we pre-recorded data sets of
useless since it is cramped between the two major and clearly         visual experiences and feed one of them to the agents in each
separable categories LIE and STAND. Moreover, when look-              interaction. But in principle it is possible to do the language
ing at the postures categorized with that category in the pro-        games on-line on the robots – it would just take very long (in
prioceptive space, quite a number of postures are not what we         the range of hours) before they reach convergence.
would call SIT, but are closer to lying or even standing.                Agents first play series of 1500 language games about body
                                                                      postures (see Figure 6). The graph shows that indeed the
                      Language Games                                  agents can reach a consensus on how to name posture cate-
Categorization itself is useless unless the categories provide        gories stemming from the categorization processes and com-
some benefit for the agent in its interaction with the envi-          municate successfully after a period of invention and align-
ronment. The testbed we are going to use here is linguistic           ment. Depending on different categorizations (two or three
communication: the agents learn to use words that denote              bodily posture categories), alignment in the population is
their acquired categories in a communicative scenario, which          reached on different time scales. In the two posture case the
could be for example useful for commanding each other to              population converges much faster than in the three posture
perform a certain action or for drawing attention to an object        case because in the latter the number of meanings is higher (3
in the environment. In order to be able to do that effectively,       instead of 2) and thus there is a longer phase of word inven-
agents have to develop a shared lexicon, linking categories           tion. But more importantly the cramped nature of the ”sit”
and words in similar ways across all agents. How this can             category (category 3 in Figure 5) leads to a significant in-
be achieved is nowadays well known (Steels, 1995, 2001):              crease in time to alignment. Points that lie on the border
Populations of agents engage in series of language games,             of the category might be conceptualized differently by the
which are local communicative interaction with a routinized           interacting robots, in turn leading to a different word used
dialogue pattern. Each agent in the population maintains its          and leading to communicative failure, which eventually de-
own private (initially empty) lexicon, which is learnt and up-        creases the score of that word-category link in the lexicon of
dated as a side effect of a game.                                     the agent. Nevertheless both in the k = 2, as well as in the
   At the beginning of an interaction, two agents are randomly        k = 3 cases the population reaches agreement and well above
                                                                  2550

95% communicative success.                                              experience as motivation for polysemy. Journal of Seman-
   Then, after 1500 interactions, the agents are presented with         tics(11), 231–251.
sensory experiences of objects instead of robots. None of the         Hu, M. (1962). Visual Pattern Recognition by Moment In-
agents has seen any of the objects before. As shown Figure              variants. Information Theory, IEEE Transactions on, 8(2),
6 (last 3500 games), they continue to have the same com-                179–187.
municative success as in the previous games and the size of           Johnson, M. (1987). The body in the mind: the bodily basis
their lexicons does not change, which indicates that the agents         of meaning, imagination and reason. Chicago: University
readily extend their previously learnt posture words to addi-           of Chicago Press.
tional objects. In fact, there is no difference in performance        Lemmens, M. (2002a). The semantic network of dutch pos-
of the agents, when confronted with objects or postures. This           ture verbs. In J. Newman (Ed.), The linguistics of sitting,
shows that the visual features used for categorization are suf-         standing and lying. Amsterdam/Philadelphia: John Ben-
ficient for the extensional use on objects.                             jamins.
                                                                      Lemmens, M. (2002b). Tracing referent location in oral
                         Conclusion                                     picture descriptions. In A. W. et al. (Ed.), A rainbow of
In this paper we have presented a concrete operationaliza-              corpora-corpus linguistics and the languages of the world.
tion of image schema theory in a computational embodied                 Lincom-Europa.
model. Processes hypothesized by cognitive linguists about            Lemmens, M. (2004). Metaphor, image schema and gram-
how speakers of Germanic languages extend the use of pos-               maticalisation: a cognitive lexical-semantic study.
ture verbs to non-living objects have been implemented in             Lloyd, S. (1982). Least squares quantization in PCM. Infor-
humanoid robots. We showed how semantics for postures                   mation Theory, IEEE Transactions on, 28(2), 129–137.
can emerge from recurrent and repeated interactions of the            Mukundan, R., & Ramakrishnan, K. (1998). Moment Func-
agents with their environment and how these semantics can               tions in Image Analysis: Theory and Applications. World
be used in repeated interactions between intelligent agents as          Scientific.
the basis for successful communication. One particularly in-          Newman, J. (Ed.). (2002). The linguistics of sitting, standing
teresting explanation offered by the model is the account for           and lying. Amsterdam and Philadelphia: John Benjamins.
the high cross-linguistic variety and vagueness in the usage of       Oosten, J. V. (1984). Sitting, standing and lying in dutch:
“sit” (when compared to ”lie” and ”stand”, which also seems             A cognitive approach to the distribution of the verbs zitten,
to be the reason for its broad semantic extension, see Lem-             staand and liggen. In Dutch linguistics at berkeley. CA:
mens, 2002a).                                                           UCB.
   We see this work as a support for image schema theory              Steels, L. (1995). A self-organizing spatial vocabulary. Arti-
and as an example of how cognitive modeling can be sub-                 ficial Life, 2(3), 319–332.
stantiated with formal methods and thus provide insights for          Steels, L. (2001). Language games for autonomous robots.
theories of cognition.                                                  IEEE Intelligent Systems, sept-oct 2001, 17-22.
                                                                      Steels, L., & Loetzsch, M. (2009). Perspective alignment in
                    Acknowledgments                                     spatial language. In K. R. Coventry, T. Tenbrink, & J. A.
This research has been carried out at the Sony Computer                 Bateman (Eds.), Spatial language and dialogue. Oxford
Science Laboratory in Paris and at the Neurorobotics Re-                University Press. (to appear)
search Laboratory at the Computer Science Department of the           Steels, L., & Spranger, M. (2008). The robot in the mirror.
Humboldt-Universität zu Berlin with partial support from the           Connection Science, 20(4).
ALEAR project, funded by the EU Cognitive Systems pro-                van Trijp, R. (2008). The emergence of semantic roles in
gram. We would like to thank Christian Thiele for the help              fluid construction grammar. In A. D.M. Smith, K. Smith,
with the robotic setup.                                                 & R. Ferrer i Cancho (Eds.), Proceedings of the 7th inter-
                                                                        national conference on the evolution of language (evolang
                         References                                     7) (pp. 346–353). Singapore: World Scientific Publishing.
Amant, R., Morrison, C., Chang, Y., Cohen, P., & Beal, C.             Wellens, P., Loetzsch, M., & Steels, L. (2008, June). Flexible
   (2006). An image schema language. In 7th international               word meaning in embodied agents. Connection Science,
   conference on cognitive modelling (ICCM 2006) (pp. 292–              20(2 & 3), 173–191.
   297).
Borneto, S. (1996). Liegen and stehen in German: A study
   in horizontality and verticality. Cognitive linguistics in the
   redwoods: The expansion of a new paradigm in linguistics,
   459–506.
Croft, W., & Cruse, D. (2004). Cognitive Linguistics. Cam-
   bridge University Press.
Gibbs, R. W., Beitel, D., Harrington, M., & Sanders, P.
   (1994). Taking a stand on the meanings of stand: Bodily
                                                                  2551

