UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Imagery as Compensation for an Imperfect Abstract Problem Representation
Permalink
https://escholarship.org/uc/item/0q682906
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Laird, John E.
Wintermute, Samuel
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Imagery as Compensation for an Imperfect Abstract Problem Representation
                                           Samuel Wintermute (swinterm@umich.edu)
                               Electrical Engineering and Computer Science Department, 2260 Hayward
                                                      Ann Arbor, MI 48109-2121 USA
                                                  John E. Laird (laird@umich.edu)
                               Electrical Engineering and Computer Science Department, 2260 Hayward
                                                      Ann Arbor, MI 48109-2121 USA
                              Abstract
   In this paper, we investigate the issues that arise when spatial
   abstractions do not capture all the details necessary for correct
   internal reasoning. We argue that in a general-purpose
   reasoning system, an imperfect abstract problem
   representation might be all that is available for any given
   problem. We propose that some forms of such imperfect
   representations are still useful in problem solving and can be            Figure 1. A simple blocks world problem.
   the basis for heuristic transfer of learning between problem              Left: spatial representation
   instances. However, there are cases when they are inadequate,             Right: abstract representation
   such as for tasks where improper actions might have dire
   consequences. To compensate, an agent can use a concrete              problem. However, as we shall argue later, creating an ideal
   problem representation based on imagery in parallel with the          representation of every problem may be difficult or even
   abstract representation to predict the consequence of actions,        impossible for a generally intelligent agent faced with novel
   thereby avoiding mistakes. A model is presented showing the           problems in complex, diverse environments. Thus, an agent
   usefulness of imagery to handle aspects of problem solving
   that the available high-level representation cannot.
                                                                         must have the capabilities to succeed with problem
                                                                         representations that are less than ideal, which we call
   Keywords: Mental imagery; spatial reasoning; transfer                 imperfect representations. The first objective of this paper is
   learning; cognitive architecture.                                     to further elaborate this point: that a generally intelligent
                                                                         agent is unable to create ideal problem representations in
                          Introduction                                   every case, but imperfect representations can be
   In many AI systems and theories of cognition, perception              productively used instead.
is a process of taking raw sensory information, manipulating                Using an imperfect representation has costs, since there
it, and creating an abstract representation that concisely               can be important details of the problem that are abstracted
captures useful properties of the world. Using this                      away. To compensate, an agent must have a way of
representation, internal reasoning can be performed without              retaining and using these details for precise inference. As an
risking potentially costly or dangerous action in an external            agent builds up its most abstract problem representation,
environment (e.g., Newell, 1990). In traditional AI systems,             intermediate representations may be built. In spatial
these problem representations resemble those used in                     perception, for example, sensory data might be used to build
problems like the blocks world (Figure 1). Here, the world               a representation of objects in 3D space, before being further
is described by objects (the blocks and the table), along with           processed into abstract symbols. Alone, this spatial
properties relating objects, such as on(A,B). These objects              representation is not very useful. Without some form of
and properties can be treated as logical variables and                   abstraction, knowledge must be learned or encoded about
predicates. General rules can then be encoded, such as that              each specific spatial state, decreasing the generality of the
moving any block X is possible if X is clear, rather than                agent. However, when used in concert with an abstract
enumerating that fact for each block. These rules can be                 representation, a spatial representation can fulfill the need
used to solve the problem internally. In addition, when the              for precise inference. Operations within concrete
agent solves a problem, it can remember that solution, and if            representations such as this have been viewed as equivalent
another problem is encountered with the same initial state               to human mental imagery (Lathrop, 2008). The second
and goal, the representation allows it to perfectly transfer             objective of this paper is to demonstrate how imagery can
the learned solution to the new problem. The precise size                compensate for some of the consequences of using an
and position of the blocks may differ in the new problem,                imperfect abstract representation.
but that information is abstracted out of the problem                       To date, research on mental imagery has focused on
representation. It is important that block A is on the table,            testing for its existence as a distinct phenomenon in humans
not that it is located 2.12 cm to the left of block C.                   and determining broad characteristics of mental processes
   We will call a representation like this that allows accurate          involving imagery (e.g., Kosslyn et al., 2004). The issue of
internal search and transfer an ideal representation of the              why imagery is useful, and thus why evolution has given us
                                                                     631

the capability at all, instead of solely abstract reasoning, has
received less attention. A common argument is that imagery
allows problems to be represented in a form where certain
types of inferences are computationally more efficient, as
different information is directly available in an image than is
available in a more abstract representation (Larkin and
Simon, 1987, Huffman and Laird, 1992, Kosslyn et al.,
                                                                           Figure 2. A modified blocks world problem.
2004, Kurup and Chandrasekaran, 2006, Lathrop, 2008).
                                                                           Left: spatial representation
For example, systems for solving geometry problems have
                                                                           Right: abstract representation
been built, and used to compare abstract and imagery-like
problem solving (Larkin and Simon, 1987, Lathrop, 2008).             when the block is placed. The effect is that all blocks must
In these experiments, the inferences possible with either            be centered relative to one of the pegs. Blocks cannot be
representation were the same, but the imagery system was             placed on the table, but can be moved out of the way to a
shown to be more efficient at making them.                           storage bin (not shown). In addition, the blocks are not all
   This sort of comparison between systems is possibly a             the same size, but vary slightly in their width and height.
legacy of the imagery debate, where abstract reasoning and           The pegs are close enough together that this variation can
imagery are posed as alternative hypotheses to explain some          cause the blocks to interact in fairly complicated ways:
capability. While the efficiency argument is persuasive (and         whether or not a given block will fit on a given peg depends
seems to be true), using imagery to compensate for                   on the exact widths and heights of all the other blocks. We
imperfections in abstract representations adds another,              assume there is a fairly high cost to moving the blocks to
possibly more fundamental role for imagery: it can do                and from the bin, so it is best to solve the problem by
things that cannot be done with the available abstract               moving blocks between pegs, rather than simply moving
representation. Imagery and abstract reasoning are                   them all to the bin and then building the goal configuration
complements, not alternatives.                                       block-by-block. In addition, we assume that there is a very
   To explore these points, a domain and representation              high cost to attempting to place a block on a peg where it
exhibiting imperfection will be presented. In this case, the         will not fit, possibly the block jams in place and cannot be
imperfection is that objects with the same properties in the         removed or might chip if it hits another block.
abstract representation are not completely interchangeable;             Consider the problem of encoding this domain abstractly,
it is not guaranteed that an action in the real world involving      in terms of symbolic rules. The states of the problem can be
one object will have the same consequence as one involving           described in similar terms to the states of a normal blocks
another object, even though both objects have similar                world problem: which blocks are on top of which other
abstract descriptions. The objects and properties available          blocks, which are clear, with the addition of a list of which
are not completely equivalent to logical variables and               pegs align which blocks. However, there is no compact
predicates. Thus, basing action selection in the world on            symbolic description of the consequence of actions. It
knowledge learned in terms of the abstract representation is         cannot be assumed that moving a block X to the top of a
not guaranteed to lead to success. However, it will be shown         clear block Y will result in on(X,Y), it may instead result in
that despite its imperfection, the representation is still           jammed(X), depending on the exact sizes of the blocks
useful. In addition, to compensate for using heuristic               below and in the other tower. The abstract representation of
imperfect knowledge, imagery is used to predict the                  the problem does not have enough detail to accurately
consequence of actions, so that mistakes are avoided.                capture these relationships in anything more concise than a
   The model presented here is not intended to be a precise          lookup table of the consequence of every action in every
model that can be compared to human data, but can serve as           state.
the starting point for such a model.                                    Contrast this with the abstract problem representation
                                                                     used in the classic blocks world (Figure 1). The two are
        The Modified Blocks World Domain                             similar on the surface, but differ in an important way. In
                                                                     both problems, the state consists of a set of objects and a set
   The argument we are presenting here is intended to be
broadly applicable to any agent (human or AI) that reasons           of properties of those objects. In the first case, the problem
about suitably complicated spatial problems. To show the             can be completely solved in terms of this representation;
argument in the clearest possible terms, an exceedingly              rules can be written such that all future states of any
                                                                     problem instance can be predicted based on the initial state:
simple domain is necessary. Accordingly, we will use a
modified version of the blocks world (Figure 2).                     the representation is ideal. However, this is not the case in
   The basic problem is the same as in Figure 1, there is a set      the modified problem: the representation is imperfect.
of blocks, and the agent’s goal is to place them in a certain           The important difference between these two
                                                                     representations is that in the first case identities of objects
configuration. However, in this domain, the blocks are not
                                                                     can be treated as variables, where they cannot in the second.
freely stackable on the table. Instead, there are two fixed
pegs attached to the table. Each block has a groove down             In the standard blocks world, the actual identities of the
the center of its back, which must be aligned with a peg             objects do not matter in determining the solution; what
                                                                     matters are the properties asserted about them. In this way,
                                                                 632

the objects can be treated as variables. This is not the case in       encounter (see also Wintermute, 2009). This is related to
the modified blocks world, here, objects cannot be treated as          the cognitive substrate hypothesis of Cassimatis (2006): it is
variables. Accordingly, we will call this form of                      more plausible to consider theories for general intelligence
representation object-dependent.                                       that use a small set of basic elements in different ways,
   While imperfect, this form of abstract representation is            rather than many different elements.
still more useful than none at all. In general, abstract                  Theoretically, the best solution is then to develop a fixed,
problem representations eliminate irrelevant details,                  ideal abstract qualitative representation that can be used for
eliminate the need for detailed inference, allow faster                any problem. If that is possible, the mechanism which
learning, and allow learned knowledge to apply across                  creates that representation from perception does not need to
different problem instances.1 Consider if the only                     change from problem to problem. However, the poverty
representation of the problem was at a perceptual level, such          conjecture of Forbus et al. (1991) states that this is
as a set of pixel-like points with no higher-level                     impossible: if the conjecture is true (which it seems to be),
interpretation. An abstract representation, even if imperfect,         “there is no purely qualitative, general-purpose,
provides a good measure of similarity between different                representation of spatial properties”.
states, where a concrete representation does not. Two states              When a qualitative representation is augmented with a
where the blocks are all on the table are probably very                quantitative representation, though, a more complete
similar, even if they are not exactly the same, but could be           reasoning system can be built. This idea has been previously
represented by very different raw perceptions. Using an                linked to a need for mental imagery capability by Forbus
imperfect abstract representation can provide a valuable               (1993), who argued that a quantitative representation is
(although heuristic) measure of state similarity, as will be           necessary to compute problem-specific qualitative
explored in detail in the next section.                                representations as needed. Our goal is to build a system that
   Another, more basic, property of a good problem                     is as problem-independent as possible, though, so we take a
representation is that it can properly differentiate the states        slightly different approach, where an imperfect
of the problem. That is, there should be no alternate paths            representation is built from problem-independent parts, and
of actions that can lead the agent to states that have the same        interaction with imagery supplements abstract reasoning.
abstract representation but are different in a way that will
affect future actions. This is the Markov property, a property                                  The Model
required for the proper use of reinforcement learning                     To demonstrate these points, a simple example model has
algorithms (Sutton and Barto, 1998). An imperfect, object-             been implemented. Using this model, it will first be shown
dependent representation can be Markovian, as long as                  that an imperfect abstract representation can still be useful
object identities are part of the state: this is the case in the       to an agent, as it can provide a Markovian state
modified blocks world, the agent needs only to know the                representation and a heuristic basis for transferring
present state of the problem to make a decision; its action            knowledge between problem instances. Building on this, it
history is unimportant. If the problem representation used             will then be shown that imagery capability can overcome
were too simple, for example, if it just included the ‘clear’          some of the problems inherent in using an imperfect
predicate, this would not be true.                                     representation. Imperfections in the representation can lead
                                                                       the agent to mistakenly believe dangerous actions are good,
  Abstract Representation in a General System                          but if action outcomes can be inferred through imagery,
   It might be possible to create an ideal abstract                    actual execution of those actions can be avoided.
representation of the modified blocks world, either by                    Consider a model where the abstract state is represented
adding more properties, or by introducing new objects to the           as a set of objects, and properties describing qualitative
model that represent important areas of empty space.                   relationships between those objects, as on the right-hand
However, such a representation would be much more                      side of Figures 1 and 2. In addition, there is a concrete
complicated than that needed in the unmodified problem in              representation describing the same situation, similar to the
Figure 1, and would be specific to the problem at hand. If             left side of each Figure. In this case, this is a set
we are designing a general purpose AI system to solve                  polyhedrons described by 3D coordinates.
spatial problems, or proposing a theory of human spatial                  A model of this form for the modified blocks world task
reasoning, it seems inappropriate to require a completely              in Figure 2 has been implemented using the Soar cognitive
different abstract representation for every problem: it takes          architecture (Laird, 2008). Symbolic processing in Soar
complicated calculations to induce each object and property            provides a basis for reasoning with an abstract
from a lower-level representation, and it is difficult to see          representation. A recent extension to Soar, SVS, provides
how an AI system or person could perform exactly the                   specialized processing for spatial and visual information
calculations needed for any arbitrary problem it might                 (Wintermute and Lathrop, 2008, Wintermute, 2009). SVS
                                                                       contains the concrete problem representation, from which
                                                                       the abstract representation is built.
   1
     In addition, employing abstract representations is implicitly
required in any psychological model connecting language and
spatial reasoning (e.g., Ragni and Steffenhagen, 2007).
                                                                   633

                                                                           Figure 4. Performance on the problem in Figure 3,
                                                                         total reward (y) vs. episode (x). Results are averaged
       Figure 3. Optimal solution to a modified blocks                   over 10 trials, error bars show standard deviation.
    world problem. The reward for the next state is shown
    at each state transition.                                       that representation? We will consider two dimensions in
                                                                    evaluating the usefulness of a representation: if it can be
   All of the primitive operations needed to construct the          used by the agent to learn to solve the problem well, and if it
abstract representation are provided by built-in, problem-          can be used to transfer learned knowledge to other, similar
independent processes in Soar/SVS. These include the                problems. To better separate the effects of using an
ability to extract simple properties such as object                 imperfect representation from the effects of using imagery,
connectivity, distance, and direction from the spatial              imagery is not initially used for these experiments.
representation. The needed abstract properties can be built            The model described above has been used to solve the
from these simpler properties, for example, on(A,B) is true         modified blocks world problem shown in Figure 3. The goal
if A is adjacent to B in the vertical direction.                    state of the problem is a tower, A on top of B on top of C on
   The actions available to the agent at each state are to          top of D, all on peg2. The optimal solution is shown in the
move each clear block (those at the top of a tower or in the        figure; it achieves a total reward of 87. Ten trials of this
bin) to the top of either tower or to the bin. Moving the           problem were run, each trial consisting of 100 episodes over
same block twice in a row is prohibited, unless that is the         which an RL policy was learned.
only action possible, or a collision has occurred, in which            Results for this experiment are shown in Figure 4. The
case the colliding block is the only block that can be moved.       agent solved the problem optimally as early as the 50th
   Symbolic rules in Soar encode knowledge about how the            episode, although average performance always remains
abstract problem representation is built from the primitives        slightly sub-optimal due to the exploration policy. From this
provided by SVS, and the knowledge needed to execute in             data, it is clear that the representation available to the agent
the problem: which actions are available based on the               was sufficient to allow the problem to be solved. 2
current state, and whether or not the goal has been achieved.          A long-lived agent will encounter many different
   To learn to solve the problem faster, reinforcement              problems in its lifetime, and it is undesirable that each
learning (RL) is used over the abstract representation in           encountered problem must be solved completely from
Soar’s working memory (Nason and Laird, 2005). Through              scratch, as in the previous experiment. Rather, a better
experience, the agent learns the value of executing a given         strategy is to identify a new problem as similar to a
action in a given abstract state. This value is in terms of         previously-solved problem, and transfer the solution of the
rewards received for environmental interactions. In this            old problem to the new problem. One of the benefits of
case, the agent gets a reward of 100 for solving the problem,       using an abstract problem representation is that irrelevant
-1 for a normal action moving a block to one of the two             details are discarded, so this mapping between problem
possible towers, -4 for moving a block to the bin, and -200         instances is simple. Mapping would be extremely difficult
for causing a collision by placing a block where it cannot fit.     without an abstract representation. In an ideal
This reward structure encourages the agent to solve the             representation, if the abstract state of the current problem is
problem in the fewest number of steps, avoiding the bin if          the same as that in an old problem, the problems can be
possible, and avoiding collisions.
                                                                       2
Learning and Transfer with an Imperfect Abstract                         Soar-RL’s q-learning algorithm was used, with a learning rate
Representation                                                      of .3 and discount factor of .9, and epsilon-greedy exploration with
                                                                    an epsilon of .1. The actual results obtained are particular to the
   If the abstract representation available to the agent is         Soar agent involved, though, since Soar-RL takes into account
object-dependent, and therefore imperfect, how useful is            other minor factors outside of the description provided here.
                                                                634

solved in exactly the same way, even though low-level
details might differ.
   In this case, the representation is object-dependent, but in
order to map problems, it can be assumed that the objects
are variables – that the blocks in the new problem are
functionally the same as those in the old. The mapping is
not completely reliable, but can still provide a substantial
benefit. To show this, an agent was trained on a simpler                    Figure 5. Performance on the problem in Figure 4,
instance of the problem, where no blocks were wide enough                after transferring a policy from another instance.
to cause collisions. The initial state and goal were otherwise           Results are averaged over 10 trials. (same axes and
the same. After 250 episodes, the policy learned was                     scales as Figure 4)
transferred to the problem in Figure 4, by assuming the              result and inspecting the imagined scene (using the same
objects involved were the same. The agent was again run for          means as regular perception) and detecting any collisions.
100 instances, as shown in Figure 5. As can be seen,                    Accordingly, the agent has been modified to imagine the
although the policy initially caused a large negative reward,        consequence of each action before executing it. If a collision
the agent quickly learns to solve the problem well, much             results, it chooses the next-best action (according to the RL
faster than when it is not provided with transferred                 policy) instead. This agent was tested on its first encounter
knowledge (in Figure 4). This shows that, even though the            with the Figure 3 problem, after learning a policy over 250
agent does not create an ideal representation of the problem,        episodes of a simpler (collision-free) problem. 35 trials were
the abstract nature of its representation can provide a              done, each with its own learned policy. The same policies
substantial transfer benefit. Including objects in the               were also tested without imagery. In 14 trials, the imagery
representation, even though they can’t be treated exactly as         agent performed optimally in its first encounter with the
variables, is still very valuable.                                   new problem. For 33 of 35 trials, more reward was received
                                                                     by using imagery than not. In one trial, both performed
Imagery Compensates for an Imperfect Abstract                        equally, and in one trial the non-imagery agent performed
Representation                                                       better (exploration was possible; in this case, the imagery
                                                                     agent explored a very unproductive path).
   While the transfer performance above is much better than
what is possible with no prior knowledge, there is certainly
room for improvement. For a long-lived agent encountering                                      Discussion
many problems, the common case for performance might                 To summarize the argument presented above:
well be the far-left data point on Figure 5, the very first time     - Abstract, qualitative representations of spatial problems
a new problem instance is encountered. There does not seem           are useful.
to be a straightforward ideal abstract representation of the         - A generally-intelligent agent must solve many different
modified blocks world problem that could be built by a task-         types of spatial problems.
independent perception system. Because of this, there is no          - The poverty conjecture implies that there is no single
reason to expect the agent could somehow improve its                 qualitative representation that could perfectly capture all of
abstract representation to be able to optimally solve any new        those problems.
instance of the problem on the first try.                            - It is unlikely that wildly different problem-specific abstract
    There is some high-level knowledge the agent could use           representations could be built every time a new problem is
outside of the RL policy, though. It is clear that causing a         encountered.
collision by attempting to move a block where it cannot fit          - This leads us to look at the possibility of using imperfect
is never a good idea; the action always results in a huge            abstract representations which might be built out of
negative reward and must be undone. The RL policy                    problem-independent parts.
captures this implicitly through the learned values of certain       - One type of such representation is an object-dependent
actions, but the knowledge is not well-transferred between           representation, where it cannot be reliably assumed that
problems, since whether or not a collision will occur                objects with the same properties are interchangeable.
depends on the exact sizes of the blocks in the problem,             - An object-dependent representation can still be useful to
information thrown out when making the abstract                      differentiate states, and to provide a basis for heuristically
representation. However, if the agent has some means for             transferring knowledge between problem instances.
making collision predictions at a concrete spatial level, it is      - However, since this transfer is heuristic and inexact, high-
possible that collisions can be foreseen and avoided.                cost, useless actions could still be performed in new
   In this case, the agent described above can use the               problem instances.
imagery functions in SVS. SVS has built-in means to                     - If an imagery system is used, and the consequences of
interpret simple commands such as “imagine block B on top            actions can be predicted, those actions can be avoided.
of block A” – it simply copies the polyhedron describing
the block to the new location. In this way, the consequences            The example agent provides a simple illustration of these
of an action can be determined by creating an image of its           points, but much more work must be done to determine how
                                                                 635

generally the principles apply, and what bearing this has on        specifically for the problem tested in the experiment, but
psychological models.                                               instead to cover a wide variety of tasks.
AI Concerns                                                                            Acknowledgments
   Many of the principles behind the design of this system          This research was funded by a grant from US Army
result from the goal of building a general, problem-                TARDEC.
independent AI system. Two of the most important claims
toward this goal are that a fixed system can build a useful                                 References
(though possibly imperfect) qualitative representation for
any arbitrary problem, and that relevant imagery operations           Cassimatis, N. L. (2006). A Cognitive Substrate for
exist for arbitrary problems.                                       Achieving Human-Level Intelligence. AI Magazine, 27(2).
   Substantial further work is needed to adequately prove             Forbus, K. D. (1993). Image and substance.
these claims. For the first claim, we know that building an         Computational Intelligence, 9(4), 377-378.
imperfect representation is much simpler than building a              Forbus, K. D., Nielsen, P., & Faltings, B. (1991).
perfect representation. In addition, the same basic system          Qualitative spatial reasoning: the CLOCK project. Artificial
used here has been successfully used for several different          Intelligence, 51(1-3), 417-471.
tasks (e.g., car motion planning in Wintermute, 2009) where           Huffman, S., & Laird, J. E. (1992). Using Concrete,
states have been built out of the same basic elements (object       Perceptually-Based Representations to Avoid the Frame
intersections, directions, and distances) without the need for      Problem. In AAAI Spring Symposium on Reasoning with
architectural modification. How far this same system can            Diagrammatic Representations.
continue to be used remains to be seen.                               Johnson, S. H. (2000). Thinking ahead: the case for motor
   For the second claim, that imagery can be used in a              imagery in prospective judgements of prehension.
problem-independent manner, again, substantial further              Cognition, 74(1), 33-70.
work is needed. A general argument can be made that                   Kosslyn, S., Thompson, W., & Ganis, G. (2006). The
complicated actions can be represented more easily through          Case for Mental Imagery. New York: Oxford U. Press.
precise forward simulation in imagery than through more               Kurup, U., & Chandrasekaran, B. (2006). Multi-modal
abstract means (Wintermute and Laird, 2008, Wintermute,             Cognitive Architectures: A Partial Solution to the Frame
2009). However, in the case covered here, motion                    Problem. In Proceedings of The 28th Annual Conference of
simulation was not used. Rather, the agent used a simple            the Cognitive Science Society .
fixed language to describe the consequence of an action               Laird, J. E. (2008). Extending the Soar Cognitive
(“imagine the A centered on top of B”). The implications of         Architecture. In Proceedings of the First Conference on
this approach have yet to be fully characterized.                   Artificial General Intelligence.
                                                                      Larkin, J. H., & Simon, H. A. (1987). Why a Diagram is
Imagery in Psychological Models                                     (Sometimes) Worth Ten Thousand Words. Cognitive
                                                                    Science, 11(1), 65-100.
   The model presented here is not intended to be a precise           Lathrop, S. D. (2008). Extending Cognitive Architectures
psychological model, but even without precision, the model          with Spatial and Visual Imagery Mechanisms. PhD Thesis,
does make some basic psychological predictions: people              University of Michigan.
will tend to imagine the consequences of actions when the             Nason, S., & Laird, J. E. (2005). Soar-RL: integrating
problem cannot be easily captured in a simple abstract              reinforcement learning with Soar. Cognitive Systems
representation, and when the wrong move could be costly.            Research, 6(1), 51-59.
Imagery in this way has a functional role in planning.                Newell, A. (1990). Unified theories of cognition. Harvard
   A similar hypothesis, motivated by behavioral data from a        University Press Cambridge, MA, USA.
motor planning experiment, was presented by Johnson                   Ragni, M., & Steffenhagen, F. (2007). Qualitative Spatial
(2000). Johnson’s hypothesis is that “movement selection            Reasoning: A Cognitive and Computational Approach. In
involves mentally simulating candidate response options in          Proceedings of the 29th Annual Conference of the Cognitive
order to evaluate their consequences”. While Johnson’s              Science Society.
work involves judgment over intrinsic properties of motor             Sutton, R. S., & Barto, A. G. (1998). Reinforcement
imagery (the comfort of a certain grip), and we have instead        Learning: An Introduction. MIT Press.
looked at spatial aspects of imagery, the principles here still       Wintermute, S., & Laird, J. E. (2008). Bimodal Spatial
apply. Specifically, a plausible argument for why people            Reasoning with Continuous Motion. In Proceedings of the
would use motor imagery in planning is that the abstraction         Twenty-Third AAAI Conference on Artificial Intelligence
of the problem available to the human reasoning system              (AAAI-08). Chicago, IL.
does not contain enough information by itself to determine            Wintermute, S., & Lathrop, S. D. (2008). AI and Mental
whether a certain action in the experiment will be                  Imagery. In AAAI Fall Symposium on Naturally Inspired AI.
comfortable or not. This imperfection in the abstract                 Wintermute, S. (2009). Integrating Reasoning and Action
representation is present because the human brain’s                 through Simulation. In Proceedings of the Second
abstraction-performing machinery has not evolved                    Conference on Artificial General Intelligence.
                                                                636

