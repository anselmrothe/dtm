UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Evidence for Efficient Language Production in Chinese
Permalink
https://escholarship.org/uc/item/5wm6r6g0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Jaeger, T. Florain
Qian, Ting
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Evidence for Efficient Language Production in Chinese
                                             Ting Qian (ting.qian@rochester.edu)
                                        T. Florian Jaeger (fjaeger@bcs.rochester.edu)
                               Department of Brain and Cognitive Sciences, University of Rochester
                                                   Rochester, NY 14627 USA
                            Abstract
  Recent work proposes that language production is organized                                            10
  to facilitate efficient communication by means of transmitting                                                                                                              10
                                                                           contextualized information                                            out−of−context information
  information at a constant rate. However, evidence has almost                                          9
  exclusively come from English. We present new results from                                            8
  Mandarin Chinese supporting the hypothesis that Constant En-                                                                                                                9
  tropy Rate is observed cross-linguistically, and may be a uni-                                        7
  versal property of the language production system. We show                                            6                                                                     8
  that this result holds even if several important confounds that
  previous work failed to address are controlled for. Finally, we                                       5
  present evidence that Constant Entropy Rate is observed at the                                                                                                              7
  syllable level as well as the word level, suggesting findings do                                             2     4        6         8   10                                         2    4        6         8   10
  not depend on the chosen unit of observation.                                                                     sentence position                                                      sentence position
  Keywords: constant entropy rate; efficient language produc-                                                (a) direct prediction                                                 (b) indirect prediction
  tion; Chinese; cross-linguistic study; information theory.
                        Introduction                                                                    Figure 1: The direct and indirect predictions of CER
The idea that language can be mathematically described just
like any other communication system goes back at least to
Shannon (1951), who suggests that anyone speaking a lan-                   dependent nature of human communication. Utterances in a
guage should also possess a statistical knowledge of that lan-             discourse build on each other. The information encoded in
guage. According to Shannon, this statistical knowledge en-                a string of words (or a stream of sounds) is co-determined
ables us to use language probabilistically, as evidenced by                by its context. In a situation where context is not properly
our ability to to fill in missing or incorrect letters in proof-           provided, such as sentences that are randomly extracted from
reading, to complete an unfinished phrase in a conversation,               a well-structured discourse, the content will seem surprising
or to perform other common tasks.                                          without context. If speakers are efficient, they should thus
   Recent work has proposed that language users exploit this               encode less out-of-context information in sentence early in
statistical knowledge for efficient language production (Gen-              discourse than late in discourse, where more preceding dis-
zel & Charniak, 2002; Aylett & Turk, 2004; Jaeger, 2006;                   course will (on average) lower the actual (contextualized) in-
Levy & Jaeger, 2007; van Son et al., 1998). Genzel and Char-               formation content. However, a reverse pattern – too much in-
niak (2002) hypothesize that speakers use their probabilistic              formation at the beginning and too little at the end – may turn
knowledge of language to maintain a constant entropy rate in               a discourse overwhelmingly difficult to understand at the be-
language production. According to the information theory,                  ginning and barely informative toward the end. This is hardly
transmitting information at a constant rate through a noisy                efficient from the speaker’s perspective since it is likely to
channel is communicatively optimal (Shannon, 1948).                        result in unsuccessful communication to the listener.
   If speaker follow the principle of Constant Entropy Rate                   Corpus studies have provided evidence for the indirect pre-
(hereafter, CER, Genzel and Charniak, 2002), we should ob-                 diction of CER. For articles in the Wall Street Journal cor-
serve that the sentences they produce carry on average the                 pus, Genzel and Charniak (2002) found that average out-of-
same amount of information. This direct prediction of CER                  context sentence information increases throughout the dis-
is illustrated in Figure 1a. However, the direct prediction of             course (see also Keller, 2004). Piantadosi and Gibson (2008)
CER is difficult to examine because it is difficult to derive es-          found that data from spoken English also follow the predic-
timate of sentences’ information content in context. Current               tion of CER.
natural language processing techniques (e.g. n-grams, proba-                  In this paper, we build on these previous findings. We
bilistic context-free grammar models) only assess the a priori,            address certain methodological shortcomings and extend the
or out-of-context, information of a sentence. To circumvent                scope of the empirical investigation of CER in three impor-
this problem, Genzel and Charniak tested an indirect predic-               tant ways. First, previous studies reported only gross correla-
tion of CER: out-of-context sentence information should in-                tion between sentence information and sentence position. We
crease throughout discourse. This indirect prediction of CER               use a linear mixed model to analyze the relation between out-
is illustrated in Figure 1b.                                               of-context information content and sentence position, while
   To understand why out-of-context information ought to in-               controlling for possible confounds such as potentially non-
crease throughout discourse, one needs to look at the context-             linear effects of sentence length. Study 1 replicates Genzel
                                                                     851

and Charniak’s studies with this new method to validate CER                         In (1) and (2), c() refers to the joint count (the number
in English.                                                                      of collocations), and P refers to unsmoothed probability esti-
   Second, if CER is a result of rational language produc-                       mates.
tion, its effects should be widely observable in any lan-
guage. Study 2 presents the first test of CER on a non-Indo-                     Sentence Information Estimates The Shannon informa-
European language, Mandarin Chinese. Mandarin Chinese                            tion content of a word is defined as the logarithm of the re-
differs typologically from previously studied languages. If                      ciprocal of the word’s probability (i.e. log2 p(w)   1
                                                                                                                                          ), a quantity
the prediction of CER can be observed in Mandarin Chinese,                       whose unit is called bits when the logarithmic base is 2. The
this would provide cross-linguistic support for CER as a prin-                   information content of a sentence S is the sum of information
ciple of efficient language production.                                          of all its words:
   Third, previous studies are limited to study CER at the
word level. That is, they have tested whether out-of-context                                                               1
                                                                                                I(S) =  ∑ log2 Pkatz (wi |wi−2 , wi−1 ) .           (3)
per-word entropy is correlated with the sentence position in                                           wi ∈S
discourse. However, nothing about the hypothesis of Con-                            Since the a sentence’s information content increases with
stant Entropy Rate attributes a special status to words com-                     additional words, previous work (Genzel & Charniak, 2002,
pared to other linguistic units. If constant entropy rate is a                   2003; Keller, 2004) has tested CER by calculating entropy
pervasive effect in language production, it should be observed                   rates for all sentence in the k-th position of a discourse:
even if entropy is calculated over units other than words.
Study 3 provides a test of this prediction by extending Study                                                   1      1
                                                                                                       H(Sk ) =
                                                                                                                N ∑ |s j | I(s j )                  (4)
2 over toned syllables.                                                                                            j
                               Methods                                              In Equation (4), H(Sk ) is the entropy rate, N is the total
                                                                                 number of sentences at Position k, |s j | is the length of a sen-
This section describes the computational model used to de-
                                                                                 tence j, and I(s) is its information content (as in equation 3).
rive sentence information based on a language model. First,
                                                                                 Computing entropy rates for sentence positions conveniently
an n-gram model is used to compute probability estimates of
                                                                                 allows for correlation tests between sentence position and en-
sentences. Then, Shannon information (Shannon, 1948) esti-
                                                                                 tropy rates reported by early studies.
mates, which are used as the measure for sentence informa-
                                                                                    However, correlations over averages do not control for pos-
tion, are derived from probability values. Finally, the regres-
                                                                                 sible non-linear effects of sentence length on sentence infor-
sion model used in current studies is described.
                                                                                 mation. This approach also does not extend well to the in-
                                                                                 vestigation of additional controls. Here, we use linear mixed
Language Model An n-gram language model can be used
                                                                                 regression models to circumvent this problem.
to make predictions about the probability of a word given the
n-1 preceding words. Here, we use a trigram model. That
is, the probability of a word is conditioned on two preced-                      Regression Analysis Individual sentence information esti-
ing words. While a trigram model is unlikely to reflect hu-                      mates are regressed against sentence position (the position of
man probability estimates, there is no reason to believe that                    a sentence in discourse) and several additional control predic-
this simplification introduces a confound. Additionally, Gen-                    tors.
zel and Charniak (2002) showed that the results derived by                          1. Sentence length is an important predictor of sen-
trigram models closely matched those derived by probabilis-                      tence information, because, mathematically, sentence con-
tic phrase structure grammars. Good-Turing discounting and                       tains more information when they have more words (for an
                                                                                                              1
Katz back-off (Katz, 1987) were used to smooth probability                       additional word w, log2 p(w)    > 0 is always true). This idea
estimates. Katz discount coefficients i.e. α values are also de-                 is also intuitive since each word must serve to convey some
rived from training data and used for backing off to bigrams                     information in a sentence. 2. Out-of-vocabulary words
or unigrams when a specific trigram is not in the model. This                    (OOVs) are words that have not been observed by the lan-
methods yield more reliable estimates for low count events                       guage model during the training phase. Since it is unlikely to
and provide probabilities even for words that have not been                      find a corpus large enough to include all words of a language
observed in the training corpus (out-of-vocabulary words,                        for the language model to learn, OOVs are a common prob-
OOVs).                                                                           lem in fitting n-gram models. We include a separate control
                                                                                 for OOVs in our model to ascertain that any effect of sentence
                                                                                 position is not purely driven by OOVs. Due to the smoothing
                      
                      P(w3 |w1 , w2 )               if c(w1 , w2 , w3 ) > 0
Pkatz (w3 |w1 , w2 ) = α(w2 |w1 ) ∗ Pkatz (w3 |w2 ) if c(w1 , w2 ) > 0           algorithm (see above), these words may be assigned a random
                         P(w3 )                                                  (but uniform) probability value and thus have superficially
                      
                                                                         (1)     high information content. While OOV words are typically
   where                                                                         low frequency (and hence high information words), some-
                           
                            P(w2 |w1 )         if c(w1 , w2 ) > 0                times even relatively frequent words may not be observed in
         Pkatz (w2 |w1 ) =                                               (2)     the training data.
                            α(w1 ) ∗ P(w2 )
                                                                             852

   To test for super- or sublinear relations, we model all             tion, even when it is measured out of context, does not in-
controll effects and the effect of sentence position using 2nd-        crease indefinitely. This suggests that the average amount of
order polynomials. In future work, we plan to explore addi-            a priori information speakers convey per word (entropy rate)
tional means of modeling non-linearities.                              converges against some as of yet unknown maximum (see Pi-
   The random effects of document-level differences and                antadosi & Gibson, 2008 for a similar observation). It is pos-
agency differences, if such information is available, are also         sible that information is distributed across natural language
controlled for to make sure any potential effects hold beyond          use in such a way that more local contextual cues are more
the particular texts observed in the sample.                           informative (as hypothesized by Piantadosi, p.c.). This would
                                                                       indeed result in the observed convergence against a maximum
        Study 1: Evidence for CER in English                           entropy rate. Future work is necessary to test this hypothesis.
The purpose of Study 1 is to test whether the results reported            In summary, Study 1 provides evidence that CER holds for
in Genzel and Charniak (2002) hold after all potential con-            English, after further controls. We also find that the additional
founds discussed above are addressed.                                  controls we introduced are justified (e.g. the weak, but highly
                                                                       significant superlinear effect of sentence length). Study 2 in-
Data                                                                   vestigates whether CER also holds in Mandarin Chinese.
The trigram model is trained on Sections 0-20 of the Wall
Street Journal subset of the Penn Treebank, and Sections 21-                   Study 2: Evidence for CER in Chinese
24 are used for hypothesis testing. We apply the same cut-off
value for sentence position as in Genzel and Charniak (2002)           Only one previous study has investigated the extent to which
– only the first 25 sentence positions are considered. There           CER holds cross-linguistically. Genzel and Charniak (2003)
are a total of 42,075 sentences in the training set, and 7,133         analyzed the novel War and Peace in both Russian and Span-
sentences in the test set.                                             ish and found that sentence entropy correlates with sen-
                                                                       tence position in all three languages, as predicted by CER.
Prediction and Results                                                 However, Genzel and Charniak’s results suffered from the
If English speakers produce language efficiently, according            methodological drawbacks mentioned above. We extending
to CER, they should encode more information in late sen-               the study of CER to Mandarin Chinese, a language typologi-
tences than in early ones. This is indeed observed: on aver-           cally and areally unrelated to previously studied languages.
age, the sentence information increases by 0.29 bits for each
increase in sentence position (linear: β = 0.29, t(5456) = 3.87,       Data
p < 0.001, quadratic: β = −0.03, t(5456) = −3.03 p < 0.003).
                                                                       The Chinese Treebank v6.0 (Xue, Xia, Chiou, & Palmer,
Unsurprisingly, longer sentences encode more information:
                                                                       2005) corpus is used in Study 2. The corpus consists of 2,036
each additional word corresponds to 7.76 bits of informa-
                                                                       documents, containing 28,295 sentences, and 781,351 words.
tion1 . Interestingly, the effect contains a significant quadratic
                                                                       The content of the corpus is a mixture of news articles from
component (linear: β = 7.76, t(5456) = 169.83, p < 0.0001;
                                                                       four news agencies: Xinhua news, Hong Kong news, Taiwan
quadratic: β = 0.01, t(5456) = 9.13, p < 0.0001). The number
                                                                       Sinorama, and ACE broadcast news. Corpus articles are pre-
of OOV words did not reach significance (p > 0.1) (we will
                                                                       dominantly in the form of news reports and differ little in style
discuss the role of OOV in detail in the next study, where it
                                                                       across agencies.
has a significant effect on sentence information).
                                                                          All headlines, author lines, and ending lines (i.e. the word
Discussion                                                             “-fine-” typically written at the end of a Chinese news report)
Our results provide converging evidence to findings in previ-          are ignored. Abstract-style news reports are also excluded
ous studies (Genzel & Charniak, 2002, 2003; Keller, 2004).             from the corpus. Then, we create a balanced dataset with the
Keller (2004) discovered that each increase in sentence po-            following sampling method: we select articles with more than
sition correlates with an increase of 0.64 bits of information         10 sentences, and then extract the first 10 sentences from each
per word (using the same corpus data). However, our results            (i.e. there are an equal number of sentences in each position).
show a smaller effect. English speakers add only 0.29 bits of          6,240 sentences are used for training and 320 sentences for
information to each subsequent sentence. That is, the effect           testing CER2 .
approximately corresponds to an increase of only 0.01 bits                Our sampling method described as above is adopted from
per word on average, given that the mean sentence length is            Genzel and Charniak (2003). It ensures that probability pat-
25 words in this data. This suggests that previous studies may         terns of n-grams that are only typical of sentences after the
have overestimated the effect of sentence position (and hence          cut-off position (i.e. 10 in our work) will not be learned by
the role of CER in discourse planning).                                the language model. Therefore, it avoids another potential
   Note that the data have shown an interesting sublinear ef-          confound.
fect of sentence position, suggesting that sentence informa-
                                                                           2 Study 1 did not follow this method in order to be as close to
    1 This is also the average per-word information.                   Genzel and Charniak (2002) in design as possible.
                                                                   853

Results                                                                                               Out-of-vocabulary words are, as a matter of fact, low-
Due to the small size of the Chinese Treebank as well as our                                       frequency words and therefore should have higher-than-
selection threshold (no. of sentences ≥ 10), testing of the                                        average information. Unfortunately, the n-gram model used
CER hypothesis is limited to a relatively small dataset (n =                                       here cannot distinguish them from each other and treat them
320). The hypothesized effect of sentence position fails to                                        as a group. Consider that the amount of information of each
reach significance (linear: p > 0.3; no quadratic effects). The                                    OOV word is the same under such a model. This obscures the
amount of information that a word contributes to a sentence                                        real information those words would be encoded with by hu-
is 9.35 bits (linear: β = 9.35, t(282) = 61.58, p < 0.0001;                                        man speakers. This distinction is particularly problematic to
quadratic: β = −0.01, t(282) = −2.82, p < 0.01). Since sen-                                        the current study, since Chinese word boundaries marked in
tence length is already controlled for, the slope parameter for                                    corpus data tend to make words seem more informative to a
the predictor OOV is really about how many bits of informa-                                        language model than they really are to speakers. For example,
tion will be changed if a known word in a sentence is made                                         “     ” means “red flag”, and is typically tagged as a single
unknown. That is, it is the average difference between infor-                                      word consisting of two characters in corpus data. “     (white
mation of regular words and OOVs. In this case, each ad-                                           flag)”, a similar word whose meaning only differs in the color
ditional OOV word has 11.81 more bits than a known word                                            property of that flag being referred to (whiteness instead of
on average (linear: β = 11.81, t(282) = 6.01, p < 0.0001; no                                       redness), will nevertheless be marked as an OOV word if the
quadratic effect: p > 0.6; see Figure 2).                                                          language model has not seen “white flag” explicitly during
                                                                                                   training. On the contrary, this word is unlikely to be of high
                                                                                                   information to native Chinese speakers.
                                                                                    Counts            As a result, we adopt two alternative methods. Method 1
                                                                                       15
                                                                                       14
                                                                                                   tests the CER hypothesis on the training data (nine times as
                                                                                       13
                                                                                       12
                                                                                                   much data). Although the ideal scenario would be to have a
                                   600
                                                                                       12          language model that recognizes every word in the test data,
                                                                                       11
            Sentence information
                                                                                       10          training data do have an OOV rate close to 0. By definition, a
                                   400
                                                                                       9
                                                                                       8
                                                                                                   trained language model is overfitted to its training data. Prob-
                                                                                       7           ability estimates will be superficially high, resulting in under-
                                                                                       6
                                                                                       5           estimation of words’ information content. However, under-
                                   200                                                 4
                                                                                       4
                                                                                                   estimated information content in itself does not lead to the
                                                                                       3
                                                                                       2
                                                                                                   conclusion that out-of-context sentence information will in-
                                                                                       1           crease throughout discourse. When using training data to test
                                         0   2   4         6          8   10   12
                                                     Number of OOVs                                the CER hypothesis (6,240 sentences), we find a strongly sig-
                                                                                                   nificant effect of sentence position (β = 0.45, t(5612) = 9.46,
                                                                                                   p < 0.0001) and once again, a sublinear effect (β = −0.08,
Figure 2: The number of OOV words has a significant effect                                         t(5612) = −4.33, p < 0.0001). Interestingly, the sublinear ef-
on sentence information.                                                                           fect coincides with the case of English: there is a limit of
                                                                                                   out-of-context sentence information yet to be investigated in
                                                                                                   future work.
Discussion                                                                                            Method 2 drops the control for OOV words in the lin-
These results seem to suggest that later sentences in discourse                                    ear mixed model to test whether the information content of
are not encoded with more information than earlier ones. The                                       all words (out-of-vocabulary as well as within-vocabulary)
increase of sentence information seems to be related to the                                        in a sentence correlates with sentence position. However, the
number of unknown words in a sentence rather than to an                                            trade-off is that the information content of all unknown words
increase in sentence position. However, what the predictor                                         has to be approximated to a uniform quantity (21.16 bits).
of sentence position in this model really tests is whether in-                                     The predicted effect of sentence position reaches marginal
formation content of known words in a sentence correlates                                          significance in this model: Chinese speakers increase sen-
with sentence position. Although this hypothesis is not sup-                                       tence information by 1.02 bits for each subsequent sentence
ported by results obtained in this model3 , it is also much more                                   (linear: β = 1.02, t(284) = 1.62, p = 0.10; no quadratic ef-
conservative than the original hypothesis of CER by viewing                                        fect; see Figure 3). Additionally, longer sentences contain
OOVs merely as a confounding factor.                                                               significantly more information than short sentences (linear:
                                                                                                   β = 10.22, t(284) = 64.25, p < 0.0001; quadratic: β = −0.01,
   3 The Chinese data set is relatively small, which results in a lan-
                                                                                                   t(284) = −1.91, p = 0.05; see Figure 4).
guage model that does not recognize 7.1% of the words in test data.
To understand this number in context, the English language model                                      From the results produced by Method 1 and the marginally
built on the much larger WSJ corpus contained only 3.4% OOV                                        significant linear effect reported using Method 2, we tenta-
words. In other words, probability estimates in the current study
are likely to be more noisy, and OOV words have bigger influence                                   tively conclude that Chinese speakers also produce language
on our result.                                                                                     efficiently.
                                                                                             854

                                                                                        Counts                                                                                                   Counts
                                                                                           3                                          1000
                                                                                                                                                                                                    17
                                                                                           3                                                                                                        16
                                                                                           3                                                                                                        15
                                                                                           3                                          800                                                           14
                                                                                           2                                                                                                        13
                                     300
                                                                                           2                                                                                                        12
              Sentence information                                                                             Sentence information
                                                                                           2                                          600                                                           11
                                                                                           2                                                                                                        10
                                                                                           2                                                                                                        9
                                                                                           2                                          400                                                           8
                                                                                           2                                                                                                        7
                                     250
                                                                                           2                                                                                                        6
                                                                                           2                                                                                                        5
                                                                                                                                      200
                                                                                           1                                                                                                        4
                                                                                           1                                                                                                        3
                                                                                           1                                                                                                        2
                                                                                                                                         0
                                                                                           1                                                                                                        1
                                                                                                                                             0   20           40         60        80      100
                                            2      4              6        8     10
                                                                                                                                                      Sentence lengths (number of words)
                                                       Sentence position
                                      Figure 3: Effect of sentence position                              Figure 4: Effect of sentence length (number of words)
         Study 3: Syllable-Level Evidence                                                                Predictions and Results
This study presents yet another way of reducing sparsity in                                              Given the decrease in data sparsity, the hypothesized effect of
Chinese data: applying a syllabic transformation to the test                                             sentence position is expected to reach significance, while the
part using the Pinyin system. The one-to-many mapping rela-                                              previously observed OOV effect may become non-significant.
tion from Pinyin to characters is useful in reducing the dimen-                                          This is indeed observed. For each subsequent sentence in test
sionality of the feature space needed to model the language.                                             documents, sentence information independently and signifi-
                                                                                                         cantly increases by 2.28 bits (linear: β = 2.28, t(284) = 12.04,
Data                                                                                                     p < 0.02; no quadratic effects). The effect of OOV words is
Study 3 uses the same dataset as in Study 2. However, cor-                                               weakened (linear: p > 0.4; quadratic: p > 0.6). Again, we
pus data are first converted to a syllabic representation before                                         sentence length affects sentence information in the expected
being used to build a language model. We use the Pinyin sys-                                             way, although this time only a linear effect is observed (linear:
tem to represent the syllabic characteristics of Chinese. Tones                                          β = 6.82, t(284) = 237.13, p < 0.0001; no quadratic effects).
are coded with numerals 1(flat), 2(rising), 3(falling-rising),
                                                                                                         Discussion
4(falling), and 5(neutral). Furthermore, word boundaries are
removed from the corpus so that this study effectively models                                            With improved information estimates, we are able to con-
the probabilistic relation between Chinese syllables.                                                    firm that Chinese speakers seem to follow the predictions of
   Table 1 confirms our expectation that the syllabic model                                              CER. That is, Mandarin Chinese production distribute infor-
fare better than the word-level model used in Study 1 in                                                 mation across sentence in a way that is efficient for commu-
terms of reducing the number of OOV words. The results                                                   nication. The results also show that the predictions of CER
are obtained by evaluating the same test data under different                                            hold not only for per-word information, but also at the syl-
representations. While there are more than 7 unknown                                                     labic level. Efficient language production on the syllabic level
words for every hundred words in the word-level model, this                                              is not improbable since Chinese characters/syllables are gen-
percentage is close to 0 in the syllabic model. Improvement                                              erally meaningful by themselves. Although the meaning of a
in OOV rate correlates with an increase in the number of                                                 syllable is often ambiguous out of context, syllables still carry
observed trigrams as well as a reduction in entropy. In                                                  information from which succeeding characters are highly pre-
summary, the influence from OOV words on the effect of                                                   dictable. A more careful look at the results reveals that
sentence position is minimized, and reduced entropy rate                                                 the increment of information between connected sentences is
also suggests improved accuracy in estimating information                                                more than double the size in the word-level model (2.28 bits
content.                                                                                                 vs. 1.02 bits). In other words, more information appears to
                                                                                                         be added in each subsequent sentence when production effi-
                                                                                                         ciency is measured on the syllable level.
                                       3-gram coverage                OOV rate        Perplexity
                                                                                                                                        Conclusions and Open Questions
  Character                                15.84%                      7.14%           460.29
   Pinyin                                  43.84%                      0.02%            79.92            The present work has demonstrated through a series of three
                                                                                                         studies that language production of Mandarin Chinese is ef-
                                                                                                         ficient independent of whether sentence information is esti-
Table 1: The quality of Pinyin model is better than that of                                              mated over words or syllables. We find that the principle of
character model in terms of higher 3-gram coverage, and                                                  Constant Entropy Rate (CER) is observed by speakers of En-
lower OOV rate as well as perplexity (perplexity=2entropy ).                                             glish (Study 1) and speakers of Mandarin Chinese (Studies 2
                                                                                                   855

                                                                                     mize the efficiency of language production according to their
                                                                      Counts         grammar and vocabulary of the target language. It is only
                                                                         2
                                                                         2           that this knowledge of the target language differs from native
                                                                         2           speakers’ version.
                                                                         2
                                                                         2
        Sentence information
                               300
                                                                         2                                   References
                                                                         2
                                                                         2           Aylett, M. P., & Turk, A. (2004). The smooth signal redun-
                                                                         2
                                                                         1             dancy hypothesis: A functional explanation for relation-
                               250
                                                                         1             ships between redundancy, prosodic prominence, and dura-
                                                                         1
                                                                         1             tion in spontaneous speech. Language and Speech, 47(1),
                                                                         1
                                                                         1
                                                                                       31-56.
                                                                         1           Genzel, D., & Charniak, E. (2002). Entropy rate constancy
                                                                         1
                                     2    4        6         8   10
                                                                                       in text. In Proceedings of the 40th annual meeting of the
                                         Sentence position                             association for computational linguistics (pp. 199–206).
                                                                                       Philadelphia, PA.
    Figure 5: The effect of sentence position in Study 3.                            Genzel, D., & Charniak, E. (2003). Variation of entropy and
                                                                                       parse trees of sentences as a function of the sentence num-
                                                                                       ber. in. In Proceedings of EMNLP (pp. 65–72). Sapporo.
and 3) even after several confounds not addressed by previous                        Jaeger, T. F. (2006). Redundancy and syntactic reduction
work (Genzel & Charniak, 2002, 2003; Keller, 2004) are ac-                             in spontaneous speech. Unpublished doctoral dissertation,
counted for: language producers seem to distribute informa-                            Stanford University.
tion uniformly across the sentences they produce. Findings                           Katz, S. M. (1987). Estimation of probabilities from sparse
in this paper lend support to the rational cognition argument                          data for the language model component of a speech recog-
in general and its application in optimizing information dis-                          nizer. IEEE Transactions on Acoustics, Speech and Signal
tribution in language communication.                                                   Processing, 35(3), 400–401.
   Note that the evidence for CER in Chinese found in above                          Keller, F. (2004). The entropy rate principle as a predictor of
studies only indicates that Chinese speakers produce the lan-                          processing effort: An evaluation against eye-tracking data.
guage efficiently in writing. Do the same results hold for                             In Proceedings of EMNLP (pp. 317–324). Barcelona.
speech too? Another work of ours has affirmed this hypoth-                           Levy, R., & Jaeger, T. F. (2007). Speakers optimize informa-
esis by testing CER on a speech corpus of Chinese broadcast                            tion density through syntactic reduction. In Proceedings of
news transcripts with the same syllable-level modeling frame-                          NIPS.
work (part of Qian, 2009). Chinese speakers are shown to in-                         Piantadosi, S., & Gibson, E. (2008). Uniform information
crease information for each subsequent sentences in sponta-                            density in discourse: a cross-corpus analysis of syntactic
neous speech as well, just as English speakers do (Piantadosi                          and lexical predictability. CUNY Presentation.
& Gibson, 2008). Research on more languages is needed to                             Qian, T. (2009). Efficiency of language production in native
further strengthen the argument that CER holds for both writ-                          and non-native speakers. (University of Rochester, Unpub-
ing and speech.                                                                        lished thesis)
   Finally, in an independent line of research, one of us (TQ)                       Shannon, C. E. (1948). A mathematical theory of com-
has been investigating to what extent second language users’                           munication. The Bell System Technical Journal, 27, 379–
performance can be considered efficient (Qian, 2009). With                             423,623–656.
regard to CER, it makes sense to ask the question whether                            Shannon, C. E. (1951). Prediction and entropy of printed
non-native speakers who are sufficiently well-trained in us-                           english. The Bell System Technical Journal, 30, 50-64.
ing the target language are able to produce it efficiently. For                      van Son, R. J. J. H., Beinum, F. J. K., & Pols, L. C. W. (1998).
example, when English L2 speakers whose native language                                Efficiency as an organizing principle of natural speech. In
is Chinese produce English, do they distribute less informa-                           ICSLP. Sydney.
tion in early discourse and more later on? On the one hand,                          Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. (2005). The
they should do so because they are capable of producing lan-                           Penn Chinese TreeBank: Phrase structure annotation of a
guage efficiently. On the other hand, they apparently have an                          large corpus. Natural Language Engineering, 11, 207–238.
imperfect knowledge of language statistics of English. Pre-
liminary evidence leads us to believe it is a matter of per-
spective: non-native language production seems efficient if
the language model is also trained on non-native corpus data;
however, it violates the prediction of CER (i.e. in the form of
putting more information at the beginning and less later on)
if the language model is trained on native corpus data (Qian,
2009). In other words, non-native speakers also try to maxi-
                                                                               856

