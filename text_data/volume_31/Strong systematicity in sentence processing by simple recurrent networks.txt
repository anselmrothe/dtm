UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Strong systematicity in sentence processing by simple recurrent networks
Permalink
https://escholarship.org/uc/item/2cm442hx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Brakel, Philemon
Frank, Stefan
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        Strong systematicity in sentence processing by simple recurrent networks
                                               Philémon Brakel (pbpop3@gmail.com)
                                                 Stefan L. Frank (S.L.Frank@uva.nl)
                             Institute for Logic, Language, and Computation, University of Amsterdam
                                       Science Park 904, 1098 XH Amsterdam, The Netherlands
                             Abstract                                                b. Women follow dogs
   Providing explanations of language comprehension requires                         c. Men follow women
   models that describe language processing and display strong
   systematicity. Although various extensions of connectionist             Moreover, the model should also be able to process sentences
   models have been suggested in order to account for this phe-
   nomenon, we found that even a simple recurrent network that             that contain embedded clauses with words occurring in novel
   had been trained in a way that can be considered ‘standard’,            positions.
   could display strong systematicity. This ability was found to              Hadley (1994) argues that humans display strong system-
   result from informative word representations that developed in
   the network.                                                            aticity. He continues that connectionist models have been
   Keywords: Simple Recurrent Network; Systematicity; Sen-                 shown to display weak systematicity but that it has yet to be
   tence Processing; Word Representations; Hierarchical Cluster            shown that they display strong systematicity as well. Strong
   Analysis.                                                               systematicity is the type of systematicity that we will test neu-
                                                                           ral networks on in this paper.
                         Introduction                                         It has been argued that merely demonstrating systematicity
Models that want to account for natural language sentence                  in a neural network does not suffice to disprove the claims by
processing also have to explain how humans can produce a                   Fodor and Pylyshyn (1988). Rather, it needs to be shown that
large number of novel sentences after being exposed to a fi-               connectionist systematicity (without merely implementing a
nite number of utterances. This ability of a model to com-                 symbol system) is not only possible, but necessarily follows
bine words in new ways is often referred to as systematic-                 from the network’s architecture (Aizawa, 1997). As Fodor
ity1 . Fodor and Pylyshyn (1988) argued that connectionist                 and McLaughlin (1990) put it, it is a law (and not just an
networks are intrinsically unable to display systematicity un-             accident) that cognition is systematic.
less they implement a symbol system.                                          Although this law-requirement has been questioned (e.g.,
   To display systematicity, a network needs to generalize. A              Hadley, 1997), we do agree that, for demonstrations of con-
model generalizes successfully if it learns regularities from a            nectionist systematicity to be convincing, they should not
set of training data that allow it to increase its performance             depend on some arbitrary arrangement of the system or the
on processing data that has not been encountered before. Ac-               training regime, nor on architectures or representations that
cording to Hadley (1994), one can divide the extent to which               were developed for obtaining systematic behavior. Moreover,
this generalization is necessary for sentence processing into              systematicity should arise robustly and in a variety of circum-
weak and strong systematicity. Weak systematicity is the                   stances, rather than showing up in just one or a few instances.
ability to process sentences that are different from the sen-                 Many demonstrations of connectionist systematicity have
tences the system was trained on but have words occurring in               not met these standards. For example, Christiansen and
the same positions as during training. An example of this is               Chater (1994) showed some indication of strong systematic-
when someone learns from sentences 1a and 1b to understand                 ity with a simple recurrent network in a single simulation, but
sentence 1c as well.                                                       without carrying out a thorough quantitative analysis. Ad-
   (1)     a. Dogs follow men                                              ditionally, Hadley, Rotaru-Varga, Arnold, and Cardei (2001)
           b. Women follow dogs                                            showed systematicity with a hebbian competitive network
                                                                           while explicitly adding semantic information to obtain this
           c. Women follow men                                             goal. Bodén (2004) used a specially designed cascaded archi-
Strong systematicity requires more abstraction and is defined              tecture to obtain systematic behavior with recurrent networks.
as the ability to process test sentences with words occurring              Recently, Frank and Čerňanský (2008) showed sytematicity
in different positions than in the training sentences. An ex-              in a recurrent neural network but only when supplying it with
ample of strong systematicity is when someone is exposed                   pre-arranged input representations. Again, this extension was
to sentences 2a and 2b and learns from those how to process                included just to obtain systematic behavior. Besides the re-
sentence 2c.                                                               search of Christiansen and Chater (1994), all these earlier at-
                                                                           tempts utilised network architectures or representations that
   (2)     a. Dogs follow men                                              had been recruited for the purpose of showing systematicity.
    1 Not to be confused with the possibility of an infinite number of     By doing so, little evidence is provided for a more general
grammatical sentences in a language which would be generativity.           display of generalization that is robust and independent of the
                                                                       1599

specific circumstances.                                               upcoming word depends on a specific number of preceding
   In the current paper, we show that even a standard neu-            words in the current sentence. Formally this means that, ac-
ral network architecture for sentence processing can display          cording to an nth order Markov model, the probability that a
strong systematicity in a large number of test inputs, without        certain word i will follow a sequence that is n words long is
requiring a special training regime or any information that is        defined as
not available from the training data. We argue that it does so
without implementing a symbol system.                                                                       N(wt−n , . . . , wt− , i)
                                                                               Pr(i|wt−n , . . . , wt− ) =                            , (1)
                                                                                                            N(wt−n , . . . , wt− )
Simple recurrent networks
The best known neural network architecture that can process           where N(·) gives the number of occurrences of its argument
sequential information is the simple recurrent network (El-           in the training data. This definition entails that a 0th order
man, 1990) (SRN). SRNs can learn various types of depen-              Markov model (unigram) is just based on the frequency of
dencies in the presented data and represent quite complex             the word itself. A 1st order Markov model (bigram) depends
automata. We chose to use this architecture because it is             just on the relative frequency of the upcoming word and the
very common in language modeling and able to update both              number of times that it was seen given the current word in the
the way it represents data and the way it uses data to pre-           sequence. Larger values for n do not always result in better
dict new words. This is unlike the echo state network Frank           performance.
and Čerňanský (2008) used. In echo state networks, only the           Whenever a Markov model processes a combination of
weights to the output layer are being trained. Therefore, the         words that did not appear in the training data, it can only base
echo state network does not change the way it represents data         its prediction on the last words that form a combination that
and Frank and Čerňanský (2008) had to explicitly construct         did occur in the training data and on the frequencies of the dif-
input representations in advance in order to obtain strong sys-       ferent words. This makes it impossible for the Markov model
tematicity. By using an SRN in the current study, we will not         to use words that appear earlier in the sentence. For this rea-
use any external means to obtain input representations that           son Markov models don’t take into account dependencies that
are based on prior intuitions.                                        are based on discontiguous word co-occurrences. The perfor-
                                                                      mance of the best Markov model can be seen as the best a
Prediction and systematicity                                          well-trained but non-systematic model can do with the cur-
One way to investigate the ability of SRNs to generalize (and         rent data. This implies that obtaining a higher performance
display strong systematicity) is to present sentences to them         than the best Markov model requires the ability to display
and train them to predict upcoming words. It is not possible to       systematicity and learn aspects of the data that are useful for
predict the next word of a sentence with certainty, so the goal       dealing with new unobserved combinations.
of the prediction tasks is to estimate a probability distribution        The bigrams, trigrams or greatest nth order Markov models
over the words in the lexicon.                                        might not always be the Markov models with the highest per-
   We used a probabilistic context-free grammar to generate           formance. For this reason, comparing with the performance
training and test sentences. After this, we used the prediction       of just one of these Markov models might result in classify-
task to investigate how well a network does at discovering the        ing other models as systematic too easily if another Markov
underlying generator of the artificial language. In the original      model with better performance exists. To classify a model as
grammar of this language, all nouns are treated equally and           systematic, it is not sufficient to just compare it with bigrams
can appear in all positions that a noun can normally appear in.       and trigrams or with the greatest nth order Markov model as
During training however, a distinction is made among differ-          done by Tong, Bickett, Christiansen, and Cottrell (2007) and
ent types of nouns based on gender. This restricts the precise        Frank (2006), respectively. Systematicity (and particularly
grammatical roles the nouns can take in a sentence. The test          strong systematicity), should be impossible to achieve for any
sentences contain the same nouns but in grammatical roles in          nth order Markov model. The Markov model that serves as a
which they never appeared during training. The only way to            baseline should be the one that achieves the best performance
still perform well on word prediction, is to discover that there      for the current sequence. This requires calculating all nth or-
is a general category of nouns that can appear in all those po-       der Markov models for the current sequence until the begin-
sitions regardless of gender. The model has to display strong         ning of the sentence has been reached and deciding which
systematicity by learning that the words can also form sen-           n-gram performs best afterwards.
tences by appearing in different positions than the ones they            Normally one could not use the best nth order Markov
appeared in during training.                                          models to find regularities in sequential data as described
                                                                      above because it is impossible to select the best performing
Measuring systematicity                                               model without already knowing the true underlying probabil-
As a criterion for strong systematicity, Frank and Čerňanský       ities. However, in the context of the current research, this
(2008) argued that a model should outperform the best nth or-         information is available. The best Markov model should not
der Markov model that has been constructed from the train-            be seen as a language model but as a baseline model that has
ing data. In a Markov model, the probability of each possible         been made very strict by providing it with prior knowledge.
                                                                  1600

Table 1: The Probabilistic grammar that underlies the arti-           Table 2: Production rules replacing Nr that were used for gen-
ficial language used in the experiment. Variable r denotes            erating the training sentences.
grammatical role. When the probabilities of different pro-
ductions are not equal they are indicated within parentheses.                           Nsub j   →       N f em | Nanim
                                                                                        Nob j    →       Nmale | Nanim
  S         →    NPsub j V NPob j [end]
  NPr       →    Nr (.7) | Nr SRC (.06) | Nr ORC (.) |              Table 3: The four types of sentences that were used for testing
                 Nr PPr (.)                                         on systematicity.
  SRC       →    that V NPob j
  ORC       →    that NPsub j V                                         SRC1: Nmale     that     V        N f em   V     N f em [end]
  PPr       →    from NPr | with NPr                                    SRC2: Nmale     V        N f em   that     V     N f em [end]
  Nr        →    N f em | Nmale | Nanim                                 ORC1: Nmale     that     Nmale    V        V     N f em [end]
  N f em    →    women | girls | sisters                                ORC2: Nmale     V        N f em   that     Nmale V      [end]
  Nmale     →    men | boys | brothers
  Nanim     →    bats | giraffes | elephants | dogs | cats | mice
  V         →    chase | see | swing | love | avoid | follow |        of training sentences would therefore, never completely con-
                 hate | hit | eat | like                              verge towards the underlying distribution as specified by the
                                                                      grammar in Table 1.
                                                                      Test sentences
    Even with the availability of prior knowledge, the Markov
models might not perform well because of their discrete way           The test sentences had male nouns in the subject positions, fe-
of handling unseen information. Normally, the Markov mod-             male nouns in the object positions (see Table 3) and contained
els would assign a probability of zero to word combinations           no animal nouns. They could not have been generated by the
that have not been encountered yet. This could allow a nois-          more restricted grammar that generated the training sentences
ier model to outperform the Markov models in these cases              and strong systematicity is required to process them correctly.
without truly displaying systematicity.                               Note for example, that due to this reversal of the grammatical
    For these reasons, and unlike Frank and Čerňanský (2008),      roles for the male and female nouns, the first word of the test
we will use the smoothed best nth order Markov models as a            sentences is always a male noun. This is in contrast with the
baseline. We applied Laplace smoothing where the frequency            training sentences that always started with either animal or
of unseen combinations was assumed to be one instead of               female nouns.
zero. Earlier tests showed that the application of smoothing             The four types of test sentences are shown in Table 3 and
resulted in slightly better performance for the best Markov           are labeled with the abbreviations SRC1, SRC2, ORC1 and
models so it makes the baseline even more strict.                     ORC2 that refer to two types of subject relative clauses (SRC)
                                                                      and object relative clauses (ORC). All these types of sen-
                          Experiment                                  tences contained three nouns that had to be either male or
                                                                      female (chosen from three possible variants) and two verbs
Language
                                                                      (chosen from a set of ten). The total number of systematicity
To generate training and test sentences, the same probabilistic       test sentences equaled 10800.
context-free grammar was used as in the experiment by Frank
and Čerňanský (2008). This language has 26 different words.        Network architecture
From these words, 12 are nouns, 10 are transitive verbs and           We used the same architecture for the SRNs as described in
2 are prepositions. There are also a relative clause marker           Elman (1990) but with different numbers of hidden units.
and an end-of-sentence marker. Its production rules (see Ta-          Words get presented at the input layer from which the
ble 1) allowed for complex sentences, containing for example          weighted activation is fed forward to the recurrent layer. The
center-embedded clauses.                                              recurrent layer’s activation is computed from a projection of
                                                                      the input activation, a bias vector and a transformation of its
Training sentences                                                    activation at the previous time-step. This last component al-
Training sentences were created following the rules of the ar-        lows the network to learn sequential information. The final
tificial grammar in Table 1 but with some extra restrictions          activation of the recurrent layer is calculated with the logistic
so that only a subset of all possible sentences was generated.        sigmoid activation function which is defined as
This was done by replacing the Nr production rule with two
                                                                                                                 
rules stating that the subject of a sentence always had to be                            arec,i = f (xi ) =            ,            (2)
                                                                                                               + e−xi
either a female noun or an animal noun and the object al-
ways a male noun or an animal noun (see Table 2). These               where xi is the total activation arriving at unit i. The output
rules are not part of the original underlying grammar. The set        layer receives input from the recurrent layer and its own bias
                                                                  1601

vector after which the softmax activation function is applied.
                                                                      Table 4: Average performance scores on the training and test
This function is defined as
                                                                      sentences for the SRNs and the Markov models.
                                             exi                               Data set              SRN                 Markov
                     aout,i = f out (xi ) =          .        (3)
                                            ∑ j ex j                                       best   worst averaged           best
                                                                              Train set    .964    .950        .959        .944
The softmax function results in positive real values for the ac-               Test set    .927    .856        .906        .792
tivation vector of the output layer that sum to one. This makes
it possible to interpret them as probabilities. More precisely,
the activation vectors per layer are given by
                                                                      Comparing the models
       arec (t) = f(Win ain (t) + Wrec arec (t − ) + brec )          As Table 4 shows, the average performance of the ten SRNs
                                                                      on the test sentences was substantially higher than that of the
       aout (t) = fout (Wout arec (t) + bout ),               (4)
                                                                      smoothed best Markov models. Even the worst performing
                                                                      network outperformed the Markov models. The average per-
where ain (t) is a 26 dimensional vector that contains zeros in       formance on the training data was also investigated but since
all of its elements except for a 1 in the element that corre-         the training sentences were generated at random, these results
sponds to the input word at time t. Respectively, the vectors         are not directly comparable.
arec (t) and aout (t) contain the activation values of the recur-        As Fig. 1 shows, the performance patterns per sentence
rent and output layers at time t, Win is the matrix containing        type of the SRNs and the smoothed best Markov models are
the input weights and Wrec and Wout are similarly the matri-          quite different. Wilcoxon matched-pairs signed-rank tests
ces containing respectively the recurrent and output weights.         showed that these differences were highly significant (with
The vectors brec and bout contain the bias vectors that are used      p < .5 · 10− for all tests) at each position of the test sen-
for respectively the recurrent layer and the output layer.            tences.
                                                                         Especially when the Markov models scored low, the net-
Training                                                              works did quite well. The SRNs reached a near perfect perfor-
                                                                      mance on the first word of every sentence, while the Markov
Backpropagation and stochastic gradient descent were used
                                                                      models had trouble with it. The first word was a male noun
to train the network by minimizing the cross-entropy error-
                                                                      and these never occurred in the first position of a sentence
function instead of the well-known sum-squared-error that
                                                                      during training. To predict the next word, the Markov mod-
was being minimized in Frank and Čerňanský (2008). Mini-
                                                                      els could only rely on bigrams (or unigrams) that look at the
mizing the cross-entropy for a multi-class classification prob-
                                                                      frequencies of words following a male noun in the object po-
lem is equivalent to maximizing a log-likelihood function that
                                                                      sition. In the training data, a male noun in the object position
is based on the categorical distribution. Simard, Steinkraus,
                                                                      preceded an end-of-sentence marker. Predicting the end-of-
and Platt (2003) found faster convergence and better general-
                                                                      sentence marker is clearly incorrect when the first word of the
ization properties by classification networks that were trained
                                                                      sentence was presented. The networks seem unaffected by
by minimizing cross-entropy instead of the sum-squared-
                                                                      this problem. Their predictions seem independent of the gen-
error.
                                                                      der of the first word but dependent on its grammatical role.
   A group of ten networks with 41 hidden units (only dif-               This performance pattern can also be seen in for example
fering in the random2 values of their initial weight matrices)        the fifth word of the ORC2 sentences. The Markov models
was trained for 10 epochs on 25000 sentences with an aver-            do badly at predicting the verb that follows this male noun.
age length of 5.9 words. The training sentences were pre-             In the training data, verbs mainly followed female nouns as
sented word by word and the networks were trained to asso-            those were always subjects. The SRNs however, reach a near
ciate each word with the next word in the sequence. A learn-          perfect performance on this word. Similar results were found
ing rate of 0.1 was used and decreased with 0.01 after each           for the sixth word of the SRC1 sentences, as well as for sev-
training epoch.                                                       eral other points in test sentences.
                                                                         In contrast, the Markov models did a lot better than the
                   Results and discussion                             networks on the fourth word of the SRC1 sentences but this
                                                                      simply indicates that no systematicity was required to do well
To obtain performance scores for the SRNs and the Markov              on this word. The combination of a verb followed by a female
models, the cosines of the angles between the vectors with            noun never appeared in the training data. Consequently, the
the probability estimations of these models and the true prob-        best Markov model could only be a bigram or a unigram. Fe-
ability distributions derived from the underlying probabilistic       male nouns were always subjects in the training data. It is
context-free grammar were calculated. This distance measure
is close to 1 if the vectors are highly similar and close to 0 if         2 The weight matrices were initialised with random values that
they are nearly perpendicular.                                        were uniformly distributed between the interval [−1, 1].
                                                                  1602

Figure 1: Average performance on each test sentence type, for the SRNs (averaged over all ten networks) and the smoothed
best Markov model. Examples of the sentence types are given by the labels on the x-axis
therefore not surprising that the most common bigram pre-            biggest difference is between the nouns and the further re-
dicts that a verb would be likely to follow even though the          maining words. Within the noun cluster, it seems that further
actual subject of that verb is the male noun that appeared ear-      groupings are quite arbitrary and reflect hardly any relation
lier in the sentence. This means that in this case the high          with the gender of the nouns. What is important here is that
performance of the best Markov model results from a lack of          even though the male and female nouns never appeared in the
systematicity rather than the opposite.                              same grammatical roles, the network was still able to place
   The Markov models performed slightly better at other              them in the same general category of nouns together with the
points but it can be seen that these differences were always         animal words. Somehow the network learned to use the gram-
very small. Moreover, the performance of the Markov models           matical category of the words rather than the exact grammat-
was relatively high in these cases, reflecting less necessity to     ical roles in which they appeared.
display strong systematicity when predicting the next word.
Analysis of the input weights
To get a better understanding of how the recurrent networks
might succeed in displaying systematic behavior, we per-
formed a hierarchical cluster analysis (HCA) on the weights
that connect the inputs to the recurrent layer. This gives more
insight in how the network formed representations of its in-
puts. We refer to these weight vectors as ‘word representa-
tions’ because the localist coding in the inputs causes only a
single weight vector at a time to influence the recurrent layer.
Note that this is a different notion of word representation than
Elman (1990) used.
   We took the network with the best average performance
for this analysis as it might be most clear from this network
how the systematicity came about. Fig. 2 shows the hierar-
chical clustering of the words based on their distances in the
representational space of the input weight vectors. It turned        Figure 2: Hierarchical cluster analysis of the input weights of
out that the first clustering separates the word that and the        the SRN with the best average performance
rest of the words, followed by a distinction between the end-
of-sentence marker and the remaining words. After that, the
                                                                 1603

                         Conclusion                                    mance on tasks that require strong systematicity. It would be
SRNs trained to do next word prediction were able to outper-           particularly interesting to see how varying the number of an-
form a baseline, that was based on the best smoothed Markov            imal nouns might influence performance. If our explanation
models, on processing sentences that required strong system-           of how connectionist systematicity arises is correct, increas-
aticity. Hierarchical cluster analysis showed that SRNs were           ing the number of animal nouns should improve performance
able to group the words they learned in a way that seemed              on test sentences.
based on more abstract properties than just their exact gram-                              Acknowledgments
matical positions.
   We conclude that the SRNs displayed strong systematic-              We would like to thank Rens Bod for his support. The re-
ity and that this provides some evidence against the claim             search presented here was funded by grant 277-70-006 of the
of Fodor and Pylyshyn (1988). We showed that strong sys-               Netherlands Organization for Scientific Research (NWO).
tematicity can be displayed by simple recurrent networks that                                   References
have been trained using a standard methodology. No addi-
tional modifications in architecture, training regime or way of        Aizawa, K. (1997). Explaining systematicity. Mind & Lan-
representing information were done and no information that               guage, 12, 115–136.
was not in the training data was used to train the networks.           Bodén, M. (2004). Generalization by symbolic abstraction
   According to Fodor and Pylyshyn (1988), combinatorial                 in cascaded recurrent networks. Neurocomputing, 57, 87–
syntax and semantics are the core properties of a symbol sys-            104.
tem. Marcus (1998) states that a symbol system is a system             Christiansen, M., & Chater, N. O. (1994). Generalization
that performs operations over variables. A standard SRN is               and connectionist language learning. Mind & Language, 9,
not a combinatorial system and contains no distinction be-               273–287.
tween instantiations of variables and operations over them.            Elman, J. L. (1990). Finding structure in time. Cognitive
We therefore argue that some notions of similarity are suffi-            Science, 14, 179–211.
cient for displaying strong systematicity, without the need for        Fodor, J. A., & McLaughlin, B. (1990). Connectionism and
any discrete symbols.                                                    the problem of systematicity: Why Smolensky’s solution
                                                                         does not work. Cognition, 35, 183–204.
How SRNs generalize                                                    Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and
                                                                         cognitive architecture: A critical analysis. Cognition, 28,
An analysis of the input weights showed that the SRNs were
                                                                         3–71.
able to form a general category of nouns instead of separate
                                                                       Frank, S. L. (2006). Strong systematicity in sentence process-
gender based categories. The ability to display systematicity
                                                                         ing by an echo state network. In Proceedings of ICANN
seems to arise from this representational level. Input words
                                                                         2006, part I (Vol. 4131, pp. 505–514). Athens, Greece:
that are represented near each other in the representational
                                                                         Springer.
space result in similar network states. If the female, male
                                                                       Frank, S. L., & Čerňanský, M. (2008). Generalization and
and animal nouns are represented close together, they will be
                                                                         systematicity in echo state networks. In Proceedings of the
treated similarly when predicting the next word.
                                                                         30th annual conference of the cognitive science society (pp.
   This similarity plays an important role during training. If           733–738). Austin, TX: Cognitive Science Society.
animal nouns appear in the same roles as male nouns, they              Hadley, R. F. (1994). Systematicity in connectionist language
will result in similar states in the network due to the context          learning. Mind & Language, 9, 247–272.
information that is fed back into the recurrent layer. The net-        Hadley, R. F. (1997). Cognition, systematicity and nomic
work can improve its performance directly by representing                necessity. Mind & Language, 12, 137–153.
them closer together as it follows from the training data that         Hadley, R. F., Rotaru-Varga, A., Arnold, D. V., & Cardei,
they result in similar context states and require a similar treat-       V. C. (2001). Syntactic systematicity arising from semantic
ment. The same applies to the female nouns and the animal                predictions in a hebbian-competitive network. Connection
nouns. If the male and female nouns both get represented                 Science, 13, 73–94.
closer to the animal nouns, it follows (given that the vectors         Marcus, G. F. (1998). Rethinking eliminative connectionism.
live in a euclidean space) that the distance between the male            Cognitive Psychology, 37, 243–282.
and female nouns also has to decrease during training.                 Simard, P., Steinkraus, D., & Platt, J. C. (2003). Best prac-
   A language learner who uses frequency information might               tices for convolutional neural networks applied to visual
never be exposed to all positions in which a single noun can             document analysis. In ICDAR (p. 958-962). IEEE Com-
occur. However, one could perhaps still learn to use the noun            puter Society.
in other positions by representing it closer to other nouns            Tong, M. H., Bickett, A. D., Christiansen, E. M., & Cottrell,
based on overlap of contextual properties.                               G. W. (2007). Learning grammatical structure with echo
   Of course the findings of this experiment are based on                state networks. Neural Networks, 20, 424–432.
one simulation and further research is required. Future stud-
ies could investigate the conditions that influence the perfor-
                                                                   1604

