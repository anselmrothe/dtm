UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Sparsity of Simple Recurrent Networks in Musical Structure Learning
Permalink
https://escholarship.org/uc/item/97f2k25h
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Agres, Kat R.
DeLong, Jordan E.
Spivey, Michael
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        The Sparsity of Simple Recurrent Networks in Musical Structure Learning
                                                Kat R. Agres (kra9@cornell.edu)
                                     Department of Psychology, Cornell University, 211 Uris Hall
                                                          Ithaca, NY 14853 USA
                                            Jordan E. DeLong (jed245@cornell.edu)
                                     Department of Psychology, Cornell University, 211 Uris Hall
                                                          Ithaca, NY 14853 USA
                                            Michael Spivey (spivey@ucmerced.edu)
                            School of Social Sciences, Humanities, and Arts, UC Merced, P.O. Box 2039
                                                         Merced, CA 95344 USA
                              Abstract                                 trained on input stimuli and attempts to extract two key
  Evidence suggests that sparse coding allows for a more
                                                                       features: which notes in the scale are musically appropriate,
  efficient and effective way to distill structural information        and which of those selected notes is the best stylistically.
  about the environment. Our simple recurrent network has              While ratings of this network were better than compositions
  demonstrated the same to be true of learning musical                 chosen from a transition table, they still were "compositions
  structure. Two experiments are presented that examine the            only their mother could love" (Mozer, 1994).
  learning trajectory of a simple recurrent network exposed to            Other approaches have included aspects such as
  musical input. Both experiments compare the network’s                evolutionary algorithms (Todd, 1999) as well as utilizing
  internal representations to behavioral data: Listeners rate the
  network’s own novel musical output from different points             self-organizing networks instead of relying on learning rules
  along the learning trajectory. The first study focused on            (Page, 1993). While most studies have concentrated on the
  learning the tonal relationships inherent in five simple             success of these networks’ compositions, the studies in this
  melodies. The developmental trajectory of the network was            paper will concentrate on the internal state of the network as
  studied by examining sparseness of the hidden layer                  it learns. Additionally, subjects’ ratings of the network’s
  activations and the sophistication of the network’s                  compositions over time will be examined, as well as other
  compositions. The second study used more complex musical
                                                                       network statistics, such as sparse coding.
  input and focused on both tonal and rhythmic relationships in
  music. We found that increasing sparseness of the hidden                Sparse coding is a strategy in which a population of
  layer activations strongly correlated with the increasing            neurons completely encode a stimulus using a low number
  sophistication of the network’s output. Interestingly,               of active units. Taken to an extreme, this strategy is similar
  sparseness was not programmed into the network; this                 to the concept of a ‘Grandmother Cell’ that responds
  property simply arose from learning the musical input. We            robustly to only one stimulus, and thus has a very low
  argue that sparseness underlies the network’s success: It is the     average firing rate. This is directly in contrast to a fully
  mechanism through which musical characteristics are learned
  and distilled, and facilitates the network’s ability to produce
                                                                       distributed system where every neuron takes part in
  more complex and stylistic novel compositions over time.             encoding every stimulus and fires an average of half of the
                                                                       time.
   Keywords: Musical structure; Simple Recurrent Network;                 Sparse coding allows for the possibility that as a
   Sparsity.
                                                                       distributed system learns the structure of the world, it begins
                                                                       encoding in a more sparse and efficient manner. The
                         Introduction                                  benefits of sparse coding have been reviewed in depth
Work in the field of neural network modeling has been                  (Field, 1994; Olshausen and Field, 2004), however this
useful in creating simulations of functional machinations of           paper will concentrate on two of them. The first reason is
human cognition and behavior. While many different                     that encoding stimuli using fewer neurons allows for a
architectures and learning algorithms exist, this paper will           complete representation without the biological demands of
primarily focus on Elman’s Simple Recurrent Network                    having every neuron firing (Levy, 1996). The second
(SRN) (1990), which was originally developed to process                reason, which is highlighted in these studies, is that a sparse
and predict the appearance of sequentially ordered stimuli.            code develops in order to efficiently mirror the structure of
This feature makes the SRN a prime candidate for                       the world.
processing the structure of music.                                        By examining the neural network architecture over the
  Modeling aspects of musical composition has shown that               learning trajectory, we can investigate how network sparsity
networks can be trained to ‘compose’ music after learning              changes with experience. Given the conventions of Western
from many examples. One such network is Mozer’s                        tonality in music (e.g. common chord progressions), as
CONCERT, which is a modified Elman network that is                     outlined by music theory, the progression of tones in music
                                                                   3099

obeys rules and patterns. These standard transitions impose             The input and output layers of the network consisted of 15
order; notes do not skip randomly around the musical state           nodes each, while the context and hidden layers contained
space. When a SRN receives this structured musical input, it         30 nodes (see Figure 2). The format of the input was such
learns how best to efficiently code the information therein.         that one note (which was represented by turning on a
   The developing internal structure of the network is of            corresponding node of the 15 present in the input layer)
prime concern, but of equal importance is how the                    would be presented per timestep. For every timestep, the
network’s output reflects that internally changing structure.        network predicted the next note in the training series, and
For external validation of the network’s ability to produce          each epoch of learning was comprised of 32 timesteps. The
increasingly stylistic output over training, listeners were          network randomly selected one of the five training melodies
recruited to rate the sophistication of the network’s novel          for every epoch. Hidden and output layer activations were
compositions. This external evaluation confirms the                  transformed using a logistic function, 1/(1+e^(-x)), and
network’s internal measures of sparsity and learning.                varied between 0 and 1. Because the last note of one
                                                                     training melody is not musically related to the first note of
                         Experiment 1                                the next training melody, the context layer activations were
In this study, we tested how a Simple Recurrent Network              reset after each epoch of training.
learns tonal structure over time by asking: What internal               Sparsity was measured in the hidden layer of each
changes occur in order to produce increasingly more                  network by looking at the proportion of hidden layer nodes
sophisticated compositions? This experiment explores how             with an activation value greater than .3. These values were
a SRN learns to predict the next note in a musical sequence          averaged over six iterations of the network, and were
by looking at the sparsity of its hidden layer activations. To       measured at 5, 25, 75, 150, 300 and 450 epochs.
elucidate the relationship between sparsity and the
sophistication (complexity and style) of the network's
compositions, participants rated the novel compositions
from several points along the learning trajectory. We
hypothesize that the sparsity of the network will increase as
it is trained, and that subject ratings will similarly increase.
                            Method
Network Architecture                                                       Figure 2: SRN architecture used in Experiment 1.
Matlab software was used to program and run the SRN. The
network was given one note at a time during training; it             Behavioral study 1
learned musical structure by predicting the next note in the         External validation is required to draw any conclusions
sequence, and then compared its prediction with the actual           regarding the relationship between increasing sparsity over
next note in the training melody. The error signal                   training and improvement in the quality of the network’s
(difference between predicted and actual) was then                   compositions. Therefore, listeners rated ten sample
backpropogated through the network.                                  compositions from epochs 5, 25, 75, 150, 300, and 450.
    The network was trained on five simple, 8-measure long           These compositions were created by inputting the note
melodies composed specifically for this study (see Figure            ‘Middle C’ at each of these benchmark epochs. The network
1). They were monophonic, of a piano timbre, and contained           then predicted the next note, which was in turn fed back into
no rhythmic variation (all of the tones were quarter notes).         the network as input. This method of sequence prediction is
Notes were held at equal duration in order investigate the           a strength of the SRN architecture, and has been used
probabilistic distribution of tonal relationships during             primarily to study grammatical aspects of language (Elman,
training.                                                            1991).
                                                                     Participants Twenty Cornell undergraduates volunteered to
                                                                     participate in the experiment for extra credit in a psychology
                                                                     class. All participants had normal hearing, and had an
                                                                     average of 6.2 ± 3.7 years of musical training.
                                                                     Materials After completing a particular number of epochs
                                                                     of training, sixteen notes of the network’s compositional
                                                                     output were recorded. Ten examples were recorded from
                                                                     each level of training (5, 25, 75, 150, 300, or 450 epochs).
                                                                     Each compositional sample was manually transferred from
    Figure 1: Examples of training melodies used as input.           Matlab to Finale, a music software program, and converted
                                                                     into .wav sound files. All compositions were set to a piano
                                                                 3100

timbre, and rhythm was kept constant (each tone was one           epoch in question). After rapidly distilling structure from
quarter note in duration). Each trial consisted of a 16-note      the training melodies, this decreasing trend begins to plateau
composition (four-measures in 4/4 time), and was 8 seconds        around 150 epochs of training.
in duration. The experiment was administered on a Dell
Inspiron laptop running E-Prime software, and participants        Behavioral study 1
wore Bose Noise Canceling headphones set to a comfortable         To assess how well the internal measure of sparsity
listening volume.                                                 corresponds to the sophistication of the network’s
                                                                  compositions, we tested whether sparsity was an
Procedure After reading the instructions, a brief practice        informative predictor of listeners’ goodness ratings. Indeed,
session consisting of four trials preceded the experiment. No     listeners displayed a general preference for melodies
feedback was given during the practice or experimental            produced after more epochs of training (see Figure 4).
trials; the practice session simply functioned to familiarize
participants to the types of melodies they would be rating.
The practice trials were drawn from different points along
the learning trajectory, including 5, 75, 150, and 450
epochs, and were different from those included in the
experiment. The sixty experimental trials were completed
without interruption and presented in random order using E-
Prime software. After listening to each trial, the listener
rated the composition on a ‘goodness’ scale from 1 to 7,
where ‘1’ represented a “poor example of classical music”
and ‘7’ represented an “excellent example of classical
music”. Participants were urged to use the whole scale as
they found appropriate.
                 Results and Discussion
Network Internal Structure                                        Figure 4: Average of listeners’ goodness ratings over epochs
By examining the activations of the hidden layer at different                               of training.
stages along its learning trajectory, we see that sparsity
increases over time. In other words, as the network                  Because the sparsity measurements and goodness ratings
completes more epochs of training, the internal structure of      followed roughly the same trend over time, sparsity did
the hidden layer becomes more sparse (see Figure 3).              prove to be an excellent predictor of how sophisticated the
                                                                  melodies sounded to listeners, R2 = .95, F = 84, p < .001.
                                                                                         Experiment 2
                                                                  The second experiment examines the same network
                                                                  structure as the first, but utilizes more complex input
                                                                  stimuli, many more training epochs, and employs a new
                                                                  sparsity metric. Three movements from J.S. Bach's Suite
                                                                  No.1 in G Major for Unaccompanied Violoncello were
                                                                  selected for the network’s training input because they are
                                                                  musically complex and sophisticated, yet monophonic (there
                                                                  is a single, unaccompanied voice). The Prelude, Allemande,
                                                                  and Courante were chosen because they can all be
                                                                  performed at a similar tempo. These pieces are more
      Figure 3: The proportion of active hidden layer nodes       complex than those used in the first experiment because
             (sparsity) over the learning trajectory.             each features different note durations and musical themes.
                                                                      In addition to musical changes, a new sparsity metric was
   As shown above, during the early stages of the network’s       adopted from single-cell recording (Rolls and Tovee, 1995),
development, there is a dramatic increase in the sparsity of      in which the square of the mean is divided by the mean of
the hidden layer representations, as indicated by a reduction     the squares (Figure 5). While the metric used in Experiment
in the proportion of hidden nodes with activations greater        1 is mostly equivalent, the Rolls sparsity metric is used
than .3 (note inverted Y axis). Again, these values are           pervasively in the literature. Both the previous sparsity .3
derived by taking the average over six networks of the            criterion and the Rolls sparsity metric will be used to assess
proportion of hidden activations above .3 (for each training
                                                              3101

the sparsity of the hidden layer activations in this               class. All participants had normal hearing, and had an
experiment.                                                        average of 2.4 ± 2.7 years of musical training.
                                                                   Materials For each level of training tested (5, 50, 500, 5
                                                                   thousand, 50 thousand, 500 thousand, and 5 million epochs),
                                                                   ten 32-note compositions were recorded for both the Normal
                                                                   and Bigram networks. Each compositional sample was
                                                                   manually transferred from Matlab to Finale and converted
                                                                   into a wav sound file. The compositions were all of a piano
                                                                   timbre, and the compositions’ rhythmic variation was
   Figure 5: Equation for Rolls sparsity metric, where n is        included. Because of the increased complexity of the
     defined here to be the number of hidden layer nodes,          musical material, each trial consisted of a 32 tones. Due to
          and r is the rate of activation for each node.           some variation in note duration, the trials were of slightly
                                                                   different lengths (average length = 12 sec). The experiment
                            Method                                 was administered on a Dell Inspiron laptop running E-Prime
                                                                   software, and participants wore Bose Noise Canceling
Network Architecture
                                                                   headphones set to a comfortable listening volume.
The same basic SRN architecture from Experiment 1 was
used in this study. Because of the increased complexity of         Procedure The same procedural protocol was used as in the
the musical input, MIDI numbers and note durations were            first study: After reading the instructions, a brief, four-trial
combined into the input for each timestep. This was                practice session preceded the experiment. These practice
encoded in the input and output by turning on one pitch            trials included an example from 50, 5k, 500k, and 5m
node and one duration node per note. Duration values were          epochs, and were different from any test trials in the
represented by sixteen nodes, with each node being                 experiment. A total of 140 test trials were presented, with
representative of a note duration ranging from a 16th note to      the 70 trials from the Normal network and 70 trials from the
a whole note. Due to this increase in complexity of the input      Bigram network combined into one large block of trials and
(a larger pitch range and rhythmic information), the number        presented in random order. Listeners rated each composition
of nodes in each layer was increased. The input and output         on a goodness scale from ‘1’ to ‘7’ as outlined for the first
layers now consist of 144 nodes (128 MIDI notes and 16             experiment.
durations), and the hidden and context layers contain 64
nodes.                                                                              Results and Discussion
   This same network architecture was used for two different
training techniques. The Normal network was fed a 32-note          Network Architecture
sequence, randomly selected from one of the movements of           As predicted, the internal representations of both networks
Bach, for each epoch of training. A second network, the            do become more sparse as the network learns structural
Bigram network, was also trained on 32 notes per epoch, but        relationships inherent in the music (see Figure 6). This
the sequence of notes lacked musical structure: After an           pattern continues until roughly 1 million training epochs,
initial note was randomly chosen from one of the                   even while adopting the alternative Rolls (1995) metric of
movements of Bach, the network’s predictions of the next           sparsity.
note in the sequence were compared with the actual next
note. Then, however, the Bigram network skipped to
another random note within the musical corpus (thus, the
network was only able to learn musical structure via a series
of bigrams). This effectively limits the Bigram network's
predictive capability to the note played immediately prior,
thereby reducing the amount of structure the network is able
to learn. Context layer activations were reset in both the
Normal and Bigram networks after each training epoch.
   A sample of the network’s hidden layer was captured
every 10 training epochs and used to measure the network’s
sparse structure. The entire network was captured at each
level of training in order to compose novel melodies using
sequence prediction as in Experiment 1.
                                                                      Figure 6: Rolls sparsity metric over epochs of training for
Behavioral study 2                                                          the Normal (blue) and Bigram (red) networks.
Participants Ten Cornell undergraduates volunteered to
                                                                      The Normal network displays more sparsity in its hidden
participate in the experiment for extra credit in a psychology
                                                                   layer activations than the Bigram network. In order to shed
                                                               3102

light on the nature of the hidden layer activations of the              A comparison was made between the .3 criterion sparsity
network while composing, sparsity was also examined while            measure and the Rolls sparsity metric (from training) in
the network produced output. Both networks display an                predicting the behavioral data. The sparsity criterion was not
increase in sparsity at 5,000 epochs, but return to a less           a significant predictor of goodness ratings for the Normal
sparse state by 5 million epochs. Though both networks               network, R2 = .57, F = 3.93, p = .14, but was significant for
display similar degrees of sparsity, the Bigram network              the Bigram network, R2 = .81, F = 12.65, p < .05. The Rolls
exhibited sparser coding during composition at 50,000 and            sparsity metric was performed similarly: It was not a
500,000 epochs (see Figure 7). The Bigram network also               significant predictor of ratings for the Normal network, R2 =
created simpler melodies than those of the Normal network.           .62, F = 4.87, p = .11, but was significant for the Bigram
This is mainly due to the fact that while the Normal network         network, R2 = .77, F = 9.99, p = .05.
is more efficient at encoding the stylistic structure from
which it is trained, it has more difficulty encoding its own                            General Discussion
output during composition. The Bigram network does not               Examining the way in which neural networks learn musical
have this limitation, as the structure it learns during training     structure can point to ways in which humans learn music. In
is similar to what it is capable of composing. In addition, the      both the human cortex and neural network models, a
Mean Squared Error (MSE) of both networks decayed                    distributed, sparse structure appears to be an optimal way to
quickly and reached a plateau with little variation by 30,000        encode musical information.
epochs of training. The Bigram network’s MSE was slightly               In comparing the Normal and Bigram data, both networks
lower than that of the Normal network.                               displayed increasingly sparse internal representations over
                                                                     their developmental trajectory. Listeners’ ratings follow a
                                                                     general increase that corresponds with the amount of
                                                                     training that a network has received as well as the sparsity
                                                                     of the network's hidden layer while learning. While we
                                                                     expected that subject ratings would increase with training,
                                                                     the fact that sparsity also increased with training shows that
                                                                     the learning algorithm of the networks picked up sparse
                                                                     structure in the input. While many models attempt to build
                                                                     sparsity into their network, sparse coding simply arises in
                                                                     these networks as they learn.
                                                                        The structure of music may in fact lend itself to sparse
                                                                     coding. Of the vast number of notes that could be used to
     Figure 7: Rolls sparsity metric while composing after           compose a musical work, only a subset of them are selected
                 different amounts of training.                      given the harmonic structure from which the tonal
                                                                     relationships are determined. In other words, tonality has a
Behavioral study 2                                                   hierarchical structure, and its foundation is centered around
Interestingly, the compositions of the Bigram network are            a particular group of tones. This inherent organization can
better rated by participants than those of the Normal                be optimally encoded with a sufficient amount of training.
network, R2 = .95, F = 19.30, p < .01, as shown below in                The Normal and Bigram networks from Experiment 2
Figure 8.                                                            show the difference in hidden layer sparsity that results from
                                                                     differing amounts of structure in the network's input. The
                                                                     Bigram network did exhibit less sparsity while training, a
                                                                     hallmark of less structure in the signal (because transitional
                                                                     relationships between bigrams were random). While the
                                                                     Normal network is more sparse during training, the Bigram
                                                                     network interestingly shows more sparsity during some
                                                                     stages of composition, and receives better ratings overall.
                                                                     This may be because while the Normal network has a more
                                                                     sparse representation during training, it is more likely than
                                                                     the Bigram network to enter into a repetitive series of notes
                                                                     while composing (such as the tonic triad) because it was
                                                                     trained on melodies with a longer musical context (it can
                                                                     utilize information from more previous timesteps when
                                                                     training).
                                                                        There are many possible directions for future study. For
      Figure 8. Participant mean response over epochs of             example, follow-up experiments can implement more recent
        training for the Normal and Bigram networks.                 advances in recurrent neural network architectures that
                                                                     encode for time information in different ways. Some of the
                                                                 3103

newer models used to generate and predict musical output            Levy W.B., Baxter R.A. (1996). Energy efficient neural
are Long Short Term Memory networks (Eck &                            codes. Neural Computation, 8, 531-543.
Schmidhuber, 2002) and Echo State Networks (Jaeger,                 Mozer, M. (1994). Neural Network Music Composition by
2001). Additionally, the network could use an interval-based          Prediction: Exploring the Benefits of Psychoacoustic
representation rather than a pitch-based representation to            Constraints and Multi-scale Processing. Connection
examine whether differences in learning and composition               Science, 6, 247-280.
would arise.                                                        Olshausen B.A., and Field D.J. (2004). Sparse Coding of
   Future iterations of this study will also examine to what          Sensory Inputs. Current Opinion in Neurobiology, 14,
extent the network over-learns the training music.                    481-487.
Overfitting could be investigated by testing how quickly the        Page, M.P.A. (1993). Modeling Aspects of Music
network can learn a novel melody after various amounts of             Perception Using Self-organizing Neural Networks.
training. Also to this end, participants could rate how               Unpublished doctoral dissertation. University of Wales.
similar the network compositions were to the training               Rolls, E.T. and Tovee, M.J. (1995). Sparseness of the
music. It is possible that differing levels of musical training       neuronal representation of stimuli in the primate temporal
between participants in Experiment 1 and Experiment 2                 visual cortex. Journal of Neurophysiology, 73(2), 713-726
contributed to different rating strategies for the                  Todd, P.M. (1989). A connectionist approach to algorithmic
compositions. A t-test comparing the participants’ training           composition. Computer Music Journal, 13(4), 27-43
across the two studies demonstrated a significant difference        Todd, P.M. (1999). Evolving musical diversity.
in musical training, t = -3.08, p < .01. Because this may             In Proceedings of the AISB’99 Symposium on Creative
have contributed to rating differences, musical training will         Evolutionary Systems, 40-48. Sussex, UK: Society for the
be controlled in future work.                                         Study of Artificial Intelligence and Simulation of
   Furthermore, continuing to explore the different internal          Behavior.
characteristics of a network that is composing versus one
that is learning may yield interesting results. The
counterintuitive fact that the Bigram network in the second
study exhibited greater sparsity and higher subject ratings
shows that the process of composition in a SRN may be
more multifaceted than previously appreciated. When a
network feeds itself its own output during composition, the
inherent complexity of the recurrent loop generates highly
variable output that warrants further investigation.
                      Acknowledgments
We would like to thank Professor David Field for his
helpful advice regarding the measure of sparsity. Also, we
wish to thank our Christine Lee for her assistance in
formatting stimuli and running participants in the second
study.
                          References
Eck, D., & Schmidhuber, J. (2002). A First Look at Music
   Composition using LSTM Recurrent Neural Networks.
   Technical Report IDSIA-07-02, Instituto Dalle Molle di
   studi sull intelligenza artificiale, Manno, Switzerland.
Elman, J. L. (1990). Finding structure in time. Cognitive
   Science, 14(2), 179-211.
Elman, J. L. (1991). Distributed representations, simple
   recurrent networks, and grammatical structure. Machine
   Learning, 7,195-224.
Field DJ. (1994). What is the Goal of Sensory Coding?
   Neural Computation, 6, 559-601.
Jaeger, H. (2001). The "echo state" approach to analysing
   and training recurrent neural networks. In GMD Report
   148, German National Research Center for Information
   Technology, 2001.
Lerdahl, F., & Jackendoff, R. (1983). A Generative Theory
   of Tonal Music. Cambridge, MA: MIT Press.
                                                                3104

