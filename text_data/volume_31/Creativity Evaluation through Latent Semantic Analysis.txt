UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Creativity Evaluation through Latent Semantic Analysis
Permalink
https://escholarship.org/uc/item/4wp633ph
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Dunbar, Kevin
Forster, Eve
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        Creativity Evaluation through Latent Semantic Analysis
                                             Eve A. Forster (eve.forster@utoronto.ca)
                                    University of Toronto Scarborough, Department of Psychology
                                           1265 Military Trail, Toronto, ON M1C 1A4 Canada
                                           Kevin N. Dunbar (dunbar@utsc.utoronto.ca)
                                    University of Toronto Scarborough, Department of Psychology
                                           1265 Military Trail, Toronto, ON M1C 1A4 Canada
                              Abstract                                 in a person’s internal and external environment, causing
   The Uses of Objects Task is a widely used creativity test. The
                                                                       different subscales such as drawing or writing to fluctuate in
   test is usually scored by humans, which introduces                  different ways. The psychometric approach accommodates
   subjectivity and individual variance into creativity scores.        these multiple factors by administering a large battery of
   Here, we present a new computational method for scoring             short tests, to encapsulate all aspects of creativity. Most of
   creativity: Latent Semantic Analysis (LSA), a tool used to          the tests require people to generate or manipulate a large
   measure semantic distance between words. 33 participants            number of ideas. Guilford and Hoepfner (1966) provide 57
   provided creative uses for 20 separate objects. We compared         tasks that ask participants to do things such as grouping and
   both human judges and LSA scores and found that LSA
   methods produced a better model of the underlying semantic          regrouping objects according to common properties, listing
   originality of responses than traditional measures.                 the consequences of unlikely situations, and the UoO Task.
                                                                          While it is almost 60 years since Guilford’s original
   Keywords: latent semantic analysis; creativity; natural             address to APA, the scoring of creativity tasks still remains
   language processing
                                                                       problematic. One way of addressing these problems would
                                                                       be to use an automated measurement tool that uses
Creativity research has had a short, but interesting history in
                                                                       underlying semantic knowledge to assess creativity.
the Cognitive Sciences. Beginning with Guilford’s
                                                                       Although initially developed to model language learning,
presidential address to the American Psychological
                                                                       Latent Semantic Analysis (LSA) has since proven itself as a
Association in 1950, researchers have sought ways of
                                                                       flexible tool with a variety of sophisticated uses. In this
discovering creative individuals (Guilford, 1947) that
                                                                       article we test the hypothesis that it can be used as a
provide an alternative to the long and laborious methods
                                                                       consistent and completely automated creativity scoring
used by the Gestalt psychologists.
                                                                       method. Here, we use LSA to score creativity of participants
   Gestalt research methods often consisted of extensive
                                                                       who perform the Uses of Objects task.
interviews with creative individuals (such as Albert
Einstein; Wertheimer, 1945), which offered fascinating
                                                                       The Uses of Objects Task
accounts of creative moments of some creative people, but
was not amenable to discovering vast numbers of creative               The UoO Task is a psychometric test that requires people to
individuals. Guilford advocated the use of the psychometric            generate multiple, original uses for a given object.
approach for this purpose, and over the subsequent decade a            Quantitative scores count the number of ideas (a measure of
number of new creativity tests were devised. By the mid-               fluency) or number of words per response (elaboration), and
1960s, the Guilford Alternate Uses test (Guilford, 1967) and           subjective scores judge creativity and category switching.
the Torrance Test of Creative Thinking (Torrance, 1998)                The task is widely used (Dunbar, 2008; Guilford, 1967;
were widely used measures of creativity across the world.              Guilford & Hoepfner, 1966; Hudson, 1968; Torrance,
On the surface, these tests were ideal; they were easy to              1998). Scoring of the UoO task can be easily automated, but
administer and quick to score: the more responses made, the            doing so strips the responses of their meaning. The only two
more creative the individual.                                          scoring options at present are meaningful but subjective and
   Researchers soon discovered that there was more to                  slow, or consistent and fast but meaningless. The ideal
creativity than number of responses. New measures were                 scoring method should be meaningful, consistent and
proposed that counted number of categories employed, and               completely automated; such a method may be devised by
measured response elaboration and novelty. These measures              combining a traditional elaboration measure with a novel
brought new problems to creativity assessment: they are                assessment of originality.
inherently subjective, have large variances in coding, and
take a considerable amount of time to score.                           The need for consistent measurement
   Sternberg and Lubart (1992) write that creativity is a              Popular scoring systems such as the Torrance Test of
function of six factors: intelligence, knowledge, thinking             Creative Thinking (Torrance, 1998) require a trained person
style, personality, motivation and environmental context.              to assess productions, but this option is not always practical.
Each of these can fluctuate from day to day due to changes             Such assessment is slow, expensive, and subjective (and
                                                                   602

therefore potentially biased). Creativity ratings can vary          the dataset, singular, local relationships between words are
substantially from one judge to another, and depend on the          amplified into consistent, global word associations.
individual interpretations and knowledge of judges. Judges             The process operates by Singular Value Decomposition of
may use particular heuristics to save time (awarding high           a large lexical co-occurrence matrix. The frequency of each
ratings to longer sentences or unusual words), and may              word within each passage of text is stored in the matrix,
adjust their rating methods over time as they learn which           with each row corresponding to a word and each column to
responses are common. This results in a highly inconsistent         a passage. The frequencies are scaled by an inverse entropy
assessment, which is why such testing is still controversial.       measure to reflect the probabilities of those frequency-
   It is difficult to include creative thinking goals in school     context associations, estimating the importance of the word
curricula, as proposed scoring methods are not consistent           to the overall meaning of the passage. The matrix is then
enough for evaluation in results-oriented systems such as           decomposed into 3 matrices ( X = W " S " P T ), where S is a
those in North America and the UK. If used in hiring, testers       diagonal matrix of scaling coefficients or singular values.
must worry about consistency to avoid discrimination                To reduce the dimensionality of the original matrix to rank k
claims. In both industry and education, efficiency,                 (typically, k = 300), all but the highest k singular values are
consistency and cost are extremely important factors, and                                !
                                                                    deleted. The matrix is then reconstructed with the new
ordinarily one of those factors would suffer for the sake of        singular value matrix, transforming the original matrix of
others. The proposed method allows all of the above factors         word-document associations into a matrix relating words to
to be maximized. It allows for a consistent and meaningful          abstract contexts. The dimensionality reduction reduces the
assessment of creativity without sacrificing efficiency.            noise in the data, allowing the latent relationships between
   Subjective measures can be used, requiring independent           words to be revealed. Words are represented as vectors in
judges to score the creativity or originality of a response or      this high dimensional space, and word similarity can be
count the number of changes in the category of use between          calculated by taking the cosine of the angle between vectors.
responses (referred to as the category switch score).               See Landauer and Dumais (1997) for more details.
Psychometric assessment methods are less subjective,                Applications LSA has been shown to behave like humans
scoring people by number of responses or reaction times             with respect to category membership (Laham, 1997), word-
(Guilford & Hoepfner, 1966).                                        word and passage-word priming (Landauer, Foltz & Laham,
                                                                    1998), vocabulary growth and performance on the TOEFL
Scoring creativity by elaboration and originality                   synonym test (Landauer & Dumais, 1997), and metaphor
Despite disagreements at finer grains of detail, there is a         comprehension (Kintsch, 2000). It has been repurposed as a
general consensus that originality and practicality are two of      method of indexing and retrieving documents (Latent
the most important components of creativity (Runco &                Semantic Indexing; Deerwester et al., 1990), grading essays
Pritzker, 1999). In other words, creativity depends on              (Rehder et al., 1998), and distinguishing between humorous
generation of diverse ideas and their subsequent pruning.           and non-humorous texts (Mihalcea & Strapparava, 2006).
   Runco and Pritzker (1999) suggest that refining and              Variations on the system have also been used to uncover the
elaborating on a particular idea improves the quality of the        latent relationships between chemicals and between genes
idea, so level of elaboration may be an indicator of quality.       (Hull et al., 2001; Kim, Park & Drake, 2007).
It may also mean that the idea is more practical or                    LSA’s successful employment in essay evaluation and
applicable; one requirement of elaboration is that the idea be      language modeling—as well as a wide assortment of
elaborate-able in the first place, which is only possible if        additional knowledge representation mechanisms—suggests
there is a tangible association between the object and its use.     great potential for use in creativity evaluation. The
Because the responses will not be seen or edited by a               following study pits LSA against a large group of human
human, there is also the danger that some ideas may not be          judges, who—although potentially inconsistent in the
legitimate uses at all; highly elaborated statements may            smaller numbers used for traditional studies—represent a
correspond to more appropriate responses.                           more consistent assessment due to the higher numbers used.
   A word count can be used to approximate elaboration, but         To determine whether use of LSA would be an adequate
an originality judgment requires much greater complexity. It        replacement for human assessment, two groups of people
must involve an understanding of the meaning of a                   were instructed to produce either creative or uncreative uses
response, or failing that, the capability to determine the          of objects and their scores according to LSA and human
conceptual distances between two responses or a response            assessments were compared. LSA was also evaluated for
and its prompt. This may be possible with the use of LSA.           internal consistency and its ability to predict human
                                                                    judgments, and the human judges were assessed in their
A New Method of Originality Assessment: LSA                         susceptibility to bias. Because of the potential subjectivity
LSA is a model of language learning in which word                   of the judges' ratings, it was important to determine how and
meaning is inferred from statistical analyses of large batches      by which dimensions the judges were assessing creativity;
of text. Word relatedness is inferred according to which            originality and practicality were measured independently to
words often co-occur, and lack of relatedness according to          isolate those dimensions, and several LSA measures were
which words are rarely together. By reducing the noise in           calculated to model suspected evaluation strategies.
                                                                603

                              Method                                     Stimuli
                                                                         40 objects were chosen to maximize variety in word
Participants                                                             frequency (in the British National Corpus; Burnard, 2000),
Creative Responders 61 participants began the study,1 and                distinctiveness (using feature production norms in McRae et
of these 33 participants completed all 20 objects (21 women              al., 2005) and homogeneity. Six object categories were
and 12 men). Online participants had a mean age of 18.24                 included (e.g. furniture and vehicles). Stimuli were divided
(SD = .60) and in-lab participants a mean age of 18.75                   into two groups of 20, and participants were randomly
(SD = .89). All were first year psychology students at the               assigned to view either group. 16 participants were assigned
University of Toronto at Scarborough, taking part in return              to Stimulus Group (SG) 1 and 17 to SG 2.
for course credit. 24 took part in the lab (8 of which
completed the experiment), in a bare room devoid of                      Procedure
inspiration. 37 took part through an on-line interface outside           Participants were encouraged to generate as many original
the lab (which 25 completed). Only those who completed all               uses as they could for each object. They were told that they
20 objects were included in the analyses. To determine any               could manipulate the objects any way they wanted,
bias due to exclusion, analyses were conducted on the 33                 including taking the objects apart, and that they could fall
included participants and subsequently with the addition of              back on obvious uses if they were unable to generate an
a subset (33) of the excluded participants. There were no                original use. They were instructed to fixate on an image of
differences between the results of the two analyses. The                 crosshairs, which was displayed for 2 seconds, followed by
authors elected to report data from completed participants.2             an object slide. An image of each object was displayed on a
Common Use Responders 28 participants (23 female, 3                      white background, with the word for the object below it.3 To
male, 2 declining to answer) were specifically instructed to             the right of the object was a text box where participants
provide common uses for all 40 objects in the study. This                could enter their responses. They were given 2 minutes for
group had two purposes: not only could their responses be                each object, after which time the text box disappeared and
used as a non-creative control for comparison to the creative            they were directed to continue to the next object. In-lab
responders, but their responses were also used in the                    participants performed the task in a room that was bare
calculation of the Common Use LSA score (described in the                except for a large desk, coat rack and filing cabinet. Online
Data Analysis section below). All participated through the               participants were instructed to perform the task wherever
online interface, and all but one resided in either the United           they were comfortable.
States of America or Canada. All spoke English as their first               The Common Use Judges were asked to state the most
language. Their mean age was 35.25 (SD = 11.44).                         common way they would use each object. They were
Creative Response Judges 26 participants (7 male, 19                     instructed to give only a single answer for each object.
female) judged the individual responses provided by the                  Responses were coded into broad general categories by the
Creative Responders. 14 judges judged responses by their                 author (e.g. “play with my daughter”, “to play in”, and “play
creativity, 7 by their originality, and 5 by their practicality.         in it” were all coded as “play”). Of the most common use
The mean age of judges was 21.09 (SD = 2.15).                            category for each object, the least specific expression (e.g.
                                                                         “play”) was recorded as the object’s most common use.
Apparatus                                                                   The Creative Response Judges were then asked to judge
All interaction with participants (except Creative Response              each response by the creative and common use responders
Judges) occurred through a web browser. In the lab, data                 on a scale from 1 to 3 (where 1 = uncreative/ unoriginal/
was collected on a 20-inch 2.4GHz Apple iMac. Creative                   impractical, 2 = somewhat creative/etc. and 3 = very
Response Judges typed their responses into an Excel                      creative/etc.). The judges were given 1 hour to evaluate as
spreadsheet rather than the online system.                               many as possible. 7 judges were instructed to evaluate
                                                                         responses (for objects from SG 1) by creativity; 4 others
                                                                         evaluated the originality of responses and 2 evaluated
                                                                         practicality. The remaining judges (7 creativity, 3 originality
                                                                         and 3 practicality) rated the responses to objects from SG 2.
                                                                         The order of objects was counterbalanced.
   1
     An additional 112 participants were recruited through the
Internet. All took part in an online version of the task. From this                               Data Analysis
group only 13 participants completed all 20 tasks, and due to the
high level of attrition, these participants were not included in the     Traditional Response Scoring
analyses. Results were the same regardless of whether these
subjects were included.                                                  Several scores were calculated for each participant’s
   2
     The lab room was kept purposefully bare, which may have             responses. Elaboration was measured by counting the
adversely affected the creativity of the in-lab participants. This
                                                                            3
may in turn have resulted in a higher number of responses left                In some cases, objects had to be depicted as a pile of “stuff,”
blank, and thus a lower number of in-lab participants completing         but no containers were used. Object categories were depicted using
all 20 objects.                                                          several pictures, each a possible object within the group.
                                                                     604

number of words in each response, and fluency was                   calculations were performed for the human scores with each
measured by counting the number of responses for each               individual judge as a subscale, to determine how much
object. Scores by the Creative Response Judges were                 consensus there was between human judges (Table 2). SG 2
averaged for each response’s subjective creativity score, as        reliabilities were very similar to those for SG 1, so
well as an originality and practicality score.                      reliability was reported only for that group.
Developing Scores using Latent Semantic Analysis                         Table 1: Consistency of each scale over 20 objects.
Similarity measures were calculated using 300 factors (the
most typical number used in LSA), with a corpus consisting          Measure                       Cronbach’s α      Cases           N
of the expected reading experience of a 1st year college            Quantitative
student (general-reading-up-to-the-first-year-in-college).            Elaboration                            .93       16          20
   Responses were spellchecked against a list of words in the         Fluency                                .91       16          20
corpus. Very common words such as “the” and “and” were              LSA
eliminated, as well as common phrases such as “I would use            Category Switch                        .86       16          20
it to….” Not only did this mean that noisy responses such as          Variety                                .92       16          20
“use it to throw food” and “throw food” were treated as the           Originality                            .90       16          20
same response, but it also allowed the two responses to be            Pruned Originality                     .85       16          20
handled simultaneously, reducing total processing time.               Common Use                             .87       16          20
Several scores were calculated with the help of LSA:                Human Judges
Category Switch The similarity scores between successive              Creativity                             .90       16          20
response pairs were averaged for each object. This was                Originality                            .87       16          20
intended to be similar to a category switch score.                    Practicality                           .85       16          20
Variety The similarity scores between every single pair of
responses for an object were also averaged, as a measure of                   Table 2: Consistency of each human judge.
the variety of responses produced by each person.
Originality For each response, 25 responses produced by             Measure                       Cronbach’s α      Cases           N
other people for the same object were selected at random              Creativity (SG 1)                      .61       16           7
and the similarities between the participant’s response and           Originality (SG 1)                     .84       16           4
each of the other 25 responses were averaged.4 This                   Practicality (SG 2)                    .94       17           3
provided a measure of originality compared to responses of
others.                                                                Scores by most of the individual judges correlated
Pruned Originality Of the originality scores calculated             significantly with the average subjective creativity score
previously, the three highest scoring responses were                (.58 < rs < .92, ps < .05; mean r = .62, SD = .35), although
averaged for each object, to simulate an originality score if       four did not (.01 < rs < .34, ps > .13). These four were
the participant were asked to report their three most creative      removed from subsequent calculations of the average
uses. While this did not necessarily prune ideas the way the        subjective creativity score. This did not affect the
participant would, it allows an estimation of how original          significance of any of the following statistics.
they would be if they were more selective.
Common Use Each response was compared to the most                   Relationships among Traditional and LSA Scores
common use of the corresponding object (collected                   Significant correlations between LSA and traditional scores
previously from Common Use Judges).                                 are shown in Table 3. Although there was no connection
   Each of these calculations produced a similarity score,          between elaboration and fluency averaged across
which was then subtracted from 1.0 to produce a                     participants, there was a significant correlation when scores
corresponding novelty score.                                        were examined for each object (r(852) = .13, p < .001).
                            Results                                         Table 3: Correlations between LSA scores and
                                                                                          traditional measures.
Consistency of Creativity Scores
To determine how consistently each type of score was able                           Com- Pruned Origin- Switch Variety
to gauge the creativity of the people generating ideas                               mon                 ality
(independent of the object being shown to them), the                Fluency           .41**      .66**      .31*
reliability of each score was calculated with each object as a                         46         46         46
subscale (as shown in Table 1). Additional reliability              Elaboration                –.42** –.46** –.60** –.63**
                                                                                                  46       46        46         46
   4
     Scores composed of comparisons to 25 responses and scores
                                                                    ** Correlation is significant at the 0.01 level (2-tailed).
compared to every single response produced (both for the scotch     * Correlation is significant at the 0.05 level (2-tailed).
tape object) were very strongly correlated (r = .98, p < .001).
                                                                605

Validity of LSA Scores                                                    motivation of the responders (whether they were told to
Of the LSA scores, only the comparison to common use                      generate common or creative uses) had quite a strong effect
score had a significant correlation with the human-judged                 on both human and LSA creativity scores. This motivation
creativity score (r(33) = .60, p < .001). Both fluency                    had similarly strong effects on both scores, which may
(r(33) = –.07, p > .60) and elaboration (r(33) = .22, p > .20)            imply that both humans and LSA are equally capable of
were unrelated to the subjective creativity measure.                      differentiating creative from uncreative ideas. The
   T-tests comparing the creativity responders in the lab to              prominence of the common use score as the central
outside the lab were not significant for both human scores                predictor to the creativity scores also suggests that the
and the LSA comparison to common score. However, when                     humans were assessing responses by comparing responses
online creativity responders were compared to online                      to the common uses for the objects.
common use responders, the differences were equally
significant for both scores (LSA: t(51) = 10.79, p < .001;                Idea Generation
Human: t(51) = 11.12, p < .001).                                          Fluency and elaboration of ideas correlated strongly with
   Four backwards regressions were performed, to determine                each other, but this was a tendency that disappeared when
which variables best approximated the subjective creativity               they were averaged into overall participant scores; the
scores. Potential factors in the regressions were: 1) the                 relationship existed within objects but was not characteristic
traditional quantitative measures, elaboration and fluency,               of particular people. Surprisingly, elaboration was
2) the LSA measures, 3) a combination of traditional                      negatively related to the pruned, originality, category switch
quantitative measures and LSA measures, and 4) the                        and variety scores, suggesting that people who gave longer
subjective measures, originality and practicality.                        responses also had less variety in their responses, did not
   The model generated in a backwards regression on the                   make significant changes in the themes of their responses,
two traditional measures was non-significant.5 The model                  and/or gave similar responses to those of other people. A
generated using the LSA measures had greater significance                 likely explanation could be that when participants were
(F(2,43) = 14.66, p < .001). Common use and pruned                        spending time thinking through an elaborate answer, this
originality were most significant to the model, which                     hampered their ability to give additional responses of the
accounted for 43% of the variation in creativity (R2adj = .39).           same quality. It may also have prevented them from giving
   Combining the traditional and LSA measures improved                    enough answers to generate a high variety and category
the model further (F(3,29) = 19.99, p < .001), this time                  switch score. This may have been the reason why these
including the measures of elaboration, fluency and common                 scores did not correlate with the subjective creativity score,
use. This accounted for 67% of the variation (R2adj = .64).               and why comparison to a common use was more realistic
   Practicality was not significant to the model of creativity;           for this particular style of evaluation. They may still have
thus, a regression model of subjective score included only                potential as creativity measures, but would correspond
originality as the main factor. This was the most significant             better to subjective category switch measures.
model (F(1,19) = 245.44, p < .001), and accounted for 93%
of the variance in the creativity scores (R2adj = .92).                   Idea Evaluation
   Because average scores from approximately 3 judges is a                Those who were asked to judge creativity most often
generally acceptable number for more creativity studies,                  reported using originality as a guide to their creativity
three additional score averages were calculated and used to               judgments, which was confirmed by the regression model of
model the average creativity score, to illustrate the kind of             creativity on subjective scores. A model including the LSA
predictive power a randomly chosen group of 3 judges may                  common use score and elaboration score was the best
have. An average score from the 3 judges with the highest                 predictor of human creativity measures, suggesting that the
correlation with average creativity accounted for 93% of the              strategies that judges used for deriving the “originality” and
variance (R2adj = .92). The 3 with the lowest correlation,                “creativity” scores involved comparing ideas to a self-
however, accounted for 6% (R2adj = .002) and a group                      generated prototypical use for the object in question. It is
composed of the highest, middle and lowest correlation with               suspected that the contribution of elaboration to the model
average creativity accounted for 44% (R2adj = .43).                       was indirectly related to practicality; uses that were more
                                                                          practical may have also been more elaborated.
                            Discussion                                       The LSA assessment had no major advantage over human
LSA scores (the common use score in particular)                           judges in its consistency over objects, but the low consensus
successfully predicted the average human creativity scores,               between the human judges in their assessments suggested
and were capable of differentiating the uses generated by                 that the LSA measures had a great advantage due to
participants in the creative and common use conditions. The               standardization (in that the algorithm used was the same
                                                                          each time). This study used a much higher number of judges
                                                                          than are usually used in creativity studies (generating an
   5
     It should be noted that a model including non-local participants     average score from judges with the most agreement), in an
was significant (F(1,44) = 10.29, p < .005), accounting for 19% of        effort to provide a creativity evaluation that may be more
the variation in creativity (R2adj = .17). Elaboration was the only
                                                                          representative of overall human approval. The LSA scores
variable significant to the model.
                                                                      606

were compared to a creativity score of high judge                  Helson, R. (1996). In search of the creative personality.
consensus, rather than a less-consistent score driven by the         Creativity Res J, 9(4), 295–306.
deviations of a minority. The decision to remove judges did        Hudson, L. (1967). Contrary imaginations: A psychological
not affect the primary results of the study, but it is argued        study of the English schoolboy. Harmondsworth, England:
that it improved the consistency of the human score and              Penguin Books.
allowed for a better estimation of model performance.              Hull, R.D., Singh, S.B., Nachbar, R.B., Sheridan, R.P.,
   A model of LSA measures was more successful in                    Kearsley, S.K. & Fluder, E.M. (2001). Latent semantic
predicting creativity than a traditional scoring method. A           structure indexing (LaSSI) for defining chemical
combination of traditional and LSA scoring produced the              similarity. J Med Chem, 44, 1177–1184.
best model of automated measures; it accounted for over            Hutchison, K. (2003). Is semantic priming due to
two-thirds of the variation in creativity, which was twice the       association strength or featural overlap? A “micro-
average performance of the judges in the study.                      analytic” review. Psychon B Rev, 12, 82–87.
                                                                   Kim, H., Park, H., Drake, B.L. (2007). Extracting
                         Summary                                     unrecognized gene relationships from the biomedical
Measures such as fluency and elaboration may be simple to            literature via matrix factorizations. BMC Bioinf, 8(Suppl.
quantify, but they can sacrifice realism. This study allowed         9), S6.
participants to respond with elaboration in a familiar and         Kintsch, W. (2000). Metaphor comprehension: A
anonymous environment, and showed that a consistent                  computational theory. Psychon B Rev, 7, 257–266.
measurement scheme can be possible with Latent Semantic            Laham, D. (1997). Latent semantic analysis approaches to
Analysis. The success of this measurement technique was              categorization. In M.G. Shafto & P. Langley (Eds.)
confirmed with a scale independently judged by humans,               Proceedings of the 19th annual meeting of the Cognitive
and shown to be a better approximation of human responses            Science Society (p. 979). Mawhwah, NJ: Erlbaum.
than traditional measures.                                         Landauer, T.K. & Dumais, S.T. (1997). A solution to
                                                                     Plato’s problem: The Latent Semanctic Analysis theory of
                   Acknowledgments                                   the acquisition, induction, and representation of
                                                                     knowledge. Psychol Rev, 104, 211–140.
This research was funded by two grants from the University         Landauer, T.K., Foltz, P.W. & Laham, D. (1998).
of Toronto to both the first and second authors.                     Introduction to latent semantic analysis. Discourse
                                                                     Process, 25, 259–284.
                         References                                McCrae, R.R. (1987). Creativity, divergent thinking, and
Amabile, T.M., Conti, R., Coon, H., Lazenby, J. & Herron,            openness to experience. J Pers Soc Psychol, 52, 1258–
   M. (1996). Assessing the work environment for creativity.         1265.
   Acad Manage J, 39, 1154–1184.                                   McRae, K., Cree, G.S., Seidenberg, M.S., McNorgan, C.
Burnard, L. (2000). British national corpus user reference           (2005). Semantic feature production norms for a large set
   guide V2.0. Oxford: Oxford University Computing                   of living and nonliving things. Behav Res Meth Ins C, 37,
   Service. Retrieved fall 2007 from http:// natcorp.ox.ac.uk        547–559.
Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K.         Mihalcea, R. & Strapparava, C. (2006). Learning to laugh
   & Harshman, R. (1990). Indexing by latent semantic                (automatically): Computational models for humor
   analysis. J Am Soc Inform Sci, 41, 391–407.                       recognition. Comput Intell, 22(2), 126–142.
Dunbar, K. (2008). Arts, education, the brain and language.        Plucker, J.A. & Renzulli, J.S. (1999). Psychometric
   Learning, Arts, and the Brain. The Dana consortium                approaches to the study of human creativity. In R.J.
   report on Arts and Cognition. New York: Dana Press.               Sternberg (Ed.), Handbook of Creativity. New York:
Foltz, P.W., Laham, D. & Landauer, T.K. (1999). The                  Cambridge University Press.
   intelligent essay assessor: Applications to educational         Rehder, B., Schreiner, M.E., Wolfe, M.B.W., Laham, D.,
   technology. Interactive Multimedia Electronic J of                Landauer, T.K. & Kintsch, W. (1998). Using Latent
   Computer-Enhanced Learning, 1(2).                                 Semantic Analysis to assess knowledge: Some technical
Glenberg, A.M. & Robertson, D.A. (2000). Symbol                      considerations. Discourse Process, 25, 337–354.
   grounding and meaning: A comparison of high-                    Runco, M.A. Pritzker, S.R. (1999). Encyclopedia of
   dimensional and embodied theories of meaning. J Mem               Creativity. San Diego: Academic Press.
   Lang, 43, 379–401.                                              Sternberg, R.J. & Lubart, T.I. (1992). Creativity: Its nature
Guilford, J.P. (1947). The discovery of aptitude and                 and assessment. School Psychol Int, 13(3), 243–253.
   achievement variables. Science, 106, 279–282.                   Torrance, E.P. (1998). Torrance tests of creative thinking:
Guilford, J.P. (1967). The Nature of Human Intelligence.             Norms. Bensenville, IL: Scholastic Testing Service.
   New York: McGraw-Hil1.                                          Wertheimer, M. (1945). Productive Thinking. New York:
Guilford, J.P. & Hoepfner, R. (1966). Sixteen divergent-             New York: Harper.
   production abilities at the ninth-grade level. Multivar
   Behav Res, 1(1), 43–66.
                                                               607

