UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Category Generation
Permalink
https://escholarship.org/uc/item/2xq7g551
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Jern, Alan
Kemp, Charles
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                                        Category Generation
                                                       Alan Jern and Charles Kemp
                                                           {ajern,ckemp}@cmu.edu
                                                           Department of Psychology
                                                          Carnegie Mellon University
                               Abstract
                                                                                                           salad
   People exhibit the ability to imagine new category instances
   and new categories, with examples ranging from everyday ac-
   tivities like cooking to scientific discovery. This ability, which
   we call category generation, is not addressed by standard mod-                           Caesar         Greek          Cobb
   els of category learning, which focus on classifying instances
   rather than generating them. We develop a probabilistic ac-
   count of category generation and evaluate it using two behav-                  x1    x2     x3    x4   x5     x6       x7    x8
   ioral experiments. Our results confirm that people find it nat-
   ural to generate new category instances and suggest that our             Figure 1: Category generation may take place at any level in a
   model accounts well for this ability.
                                                                            concept hierarchy. Two cases are illustrated here. Existing or
   Keywords: category learning; category generation; generative             observed knowledge is denoted by solid nodes and generated
   models; Bayesian modeling
                                                                            instances and categories are represented by dashed nodes.
   Humans exhibit a wide variety of creative abilities, includ-             The Caesar salad branch illustrates a situation in which some-
ing the ability to imagine entirely new objects never before                one observes several instances of a Caesar salad and then gen-
observed. Evolutionary biologists predict transitional species              erates a new instance (x4 ). The Cobb salad branch illustrates
on the basis of gaps in the fossil record (e.g., Tiktaalik, a               the simultaneous creation of a brand new type of salad and
species with features characteristic of both aquatic and land               several instances of it.
animals); designers develop new products that combine and
improve upon the strengths of existing products (e.g., the
spork); professional and amateur chefs create new recipes                   that category. This case is illustrated in Figure 1 by the Cae-
by swapping and mixing ingredients (e.g., the Cobb salad,                   sar salad branch of the hierarchy: after observing instances x1
invented by Robert H. Cobb by combining a collection of                     through x3 , a new Caesar salad, x4 , is generated. This paper
ingredients that happened to be available in his restaurant’s               explores a Bayesian approach, which proposes that categories
kitchen). Henceforth, we will refer to this capability as cate-             are represented as probability distributions, and that people
gory generation. 1                                                          can generate new instances of categories by sampling from
   In addition to inventing new categories of objects, people               these distributions.
create new instances of existing categories relatively com-                     Although category generation has received relatively lit-
monly. While the invention of the Cobb salad might be char-                 tle attention, it has been addressed by some previous studies.
acterized as the creation of a new category of salad, people                Ward (1994) asked participants to invent and draw animals
frequently create new instances of existing salads—swapping                 from a distant planet, requiring them to essentially create a
romaine lettuce for iceburg lettuce to obtain a variation on a              new category of animal. Feldman (1997) showed people a
Caesar salad, for example. This hierarchy of category gener-                single instance of category—a line segment with a circle on
ation problems is illustrated in Figure 1. Although the figure              it, for example—and asked them to generate new examples
only shows a few levels in a hierarchy, category generation                 of the category. Both studies confirm that people are able to
could in principle take place at any level.                                 generate new instances of a category, but neither provides a
   These examples cannot be captured by standard accounts                   comprehensive formal account of this ability.
of categorization that focus on classification (e.g., deciding if               We describe a computational account of category genera-
a new dish is a Caesar salad or a Greek salad). Whereas clas-               tion that relies on Bayesian inference. Previous authors (An-
sification involves assigning an object to an existing category,            derson, 1991) have developed Bayesian models of categoriza-
category generation involves creating a new instance of an ex-              tion, but most of these models focus on classification. Our
isting category or creating a brand new category. In this paper,            approach uses some of the same methods as previous models,
we focus on one case of category generation: the generation                 but focuses on category generation rather than classification.
of new instances of a category after observing examples of                      We begin by reviewing some general approaches to classi-
                                                                            fication, and explain why a Bayesian approach is well suited
    1 The term “category generation” is sometimes used to describe          for category generation. We then describe a specific model
tasks in which participants provide a category label, like “snacks”,        of category generation and compare its behavior with human
given instances, like “pretzels” (Ross & Murphy, 1999). The prob-
lem that we consider involves the creation of new categories or cat-        responses. We conclude with some general remarks about the
egory instances, rather than the retrieval of familiar category labels.     efficacy of the Bayesian approach to category generation.
                                                                        130

                                                                            Discriminative
                 (a) Classification
                                                                      P (xnew |y = 1) P (xnew |y = 2)
                                                                0.2                                                        1
                                                                                                        P (y = 2|xnew )
                                                                0.1                                                       0.5
                                     y=1        y=2
                                                                 0                                                         0
                                                                                   xnew                                                     xnew
                                                                                 Generative
                 (b) Generation
                                                                0.2
                                                                                         P (xnew )
                                                                0.1
                                     x1    x2   x3                                      x4
                                                                 0
                                                                                   xnew
Figure 2: Discriminative classification, generative classification, and category generation. (a) Given three instances each of
categories 1 and 2, a discriminative model (solid arrow) directly learns a classification distribution P(y = 2|xnew ) that can
be used to assign category labels to new instances xnew . A generative model (dashed arrows) learns generation distributions
P(xnew |y = 1) and P(xnew |y = 2) for each category, and these distributions induce a classification distribution via Bayes’ rule.
(b) Given three instances of a single category, our model learns a generation distribution P(xnew ), here assumed to be Gaussian.
New instances such as x4 can then be generated by sampling from this distribution.
                             Classification                                               Our distinction between generative and discriminative ap-
The standard classification problem can be formulated as fol-                          proaches is standard in the machine learning literature, but
lows. A set of training exemplars, x̄ = {x1 , . . . , xn }, and a                      terms like “generative” and “discriminative” are sometimes
corresponding set of category labels, ȳ, are provided. Each xi                        used differently by psychologists. Some authors reserve the
is a vector of feature values. After seeing how the instances                          term “generative” for approaches that make infinite use of fi-
in the training set are labeled, the classification task involves                      nite means, and use “discriminative” to refer to settings where
assigning a category label, ynew , to a novel instance, xnew .                         participants must learn to distinguish between stimuli. Note
   There are two standard approaches to classification:                                that neither usage maps perfectly onto our own.
the generative approach and the discriminative approach.                                  Generative and discriminative models are both able to
A generative model learns a probability distribution                                   make predictions about human behavior on classification
P(xnew |ynew , x̄, ȳ), which we call a generation distribution, and                   problems. By contrast, tasks that depend on the genera-
then computes a classification distribution, P(ynew |xnew , x̄, ȳ),                   tion distribution, P(xnew |ynew , x̄, ȳ), are naturally much better
using Bayes’ rule:                                                                     suited to a generative approach. We propose that category
                                                                                       generation is one such task, and that learning a generation
       classification distribution    generation distribution                          distribution allows people to generate novel instances of cat-
        z       }|           {   z       }|           {
        P(ynew |xnew , x̄, ȳ) ∝ P(xnew |ynew , x̄, ȳ) P(ynew |ȳ)        (1)         egories.
By contrast, a discriminative model learns the classification                                A Bayesian Model of Category Generation
distribution directly (Bishop, 2006). The difference between                           The generation distribution, P(xnew |ynew , x̄, ȳ), is defined for
the two types of models is illustrated in Figure 2a. As the                            multiple values of ynew and can be used to generate instances
figure shows, discriminative models directly learn the clas-                           of multiple categories. Here, however, we consider the case
sification distribution, which corresponds to a soft decision                          where there is a single category of interest. Because all ex-
boundary, while generative models begin with the intermedi-                            emplars have the same category label y, we drop the labels
ate step of learning the underlying distribution that generated                        and work with the generation distribution, P(xnew |x̄). Given
the training data.                                                                     training examples in x̄, new examples can be generated by
   Most formulations of exemplar models (Nosofsky, 1985)                               sampling from this distribution.
and prototype models (Reed, 1972) are discriminative                                      Suppose that the single category of interest is characterized
models—they can classify new instances without needing to                              by a vector of parameters θ̄ that is not observed. Integrating
learn the generation distribution over new instances. Ander-                           over all possible values of θ̄, we have
son’s (1991) rational model of categorization, however, fol-                                                                    Z
lows a generative approach.                                                                          P(xnew |x̄) =                       P(xnew |θ̄)P(θ̄|x̄)d θ̄   (2)
                                                                                                                                    θ̄
                                                                                 131

                       1           1        1                  1            the category can now be created by sampling a vertical piece
        (a) S ∈ { 1        2 , 2     1 , 2       2   , ..., 4     2 }
                                                                            from η1 and a horizontal piece from η2 .
                       2           2        1                  3
                                                                               To formalize these generative assumptions, we assume that
                       1
        (b) S :    2       2
                                                                            structure S is drawn from a uniform distribution over the 15
                       1                                                    possible partitions, that each distribution ηi is drawn from a
                               η1                        η2                 Dirichlet prior with parameter α, and that each piece xi is
                 0.5                         0.5
                                                                            sampled from a multinomial distribution ηi :
                   0
                       VR    XF   MG RB
                                               0
                                                   LP   HN  KS   DZ                                   S ∼ Uniform([1 : 15])
                                        (S, η
                                            ~)                                                  η |S ∼ Dirichlet(α)
                                                                                                    i
                                                                                                                                              (3)
                                                                                                 i    i                      i
                                                                                                x |η ∼ Multinomial(η )
                         V           V             X           X
                     L       P     H    N      H       N    L     P ...        We assume that the alphabet of symbols is fixed in advance,
                         R           R             F           F            and that the distribution ηi is defined over all possible per-
Figure 3: Stimuli for the category generation task described                mutations of symbols that could fill slot i. For example, if
in the text. (a) A set of stimuli is created by first selecting             the slot includes m cells and there are k symbols, then there
a structure S—a partition of features into slots. The number                are km possible pieces that could fill the slot. We set the pa-
in each feature position signifies the partition it belongs to.             rameter α by assuming that the prior probability that any two
(b) Given S, the stimuli are generated by sampling from a                   category instances have the same piece for a given slot is 0.5.
distribution ηi over pieces for each slot i. Here, S specifies              Anderson’s (1991) model of categorization makes a related
one slot made up of the top and bottom features, and one slot               assumption, and refers to the parameter 0.5 as a “coupling
made up of the left and right features.                                     probability.” It follows that α = km1−2 , . . . , km1−2 , where the
                                                                            α value for a given slot depends on the size m of that slot.
                                                                               Now that we have formally specified our assumptions
                                                                            about the category we can use Equation 2 to model how novel
   Our account of category generation is illustrated in Fig-
                                                                            instances of the category are generated. We set θ̄ = (S, η̄) and
ure 2b for the case of a single category. Here, θ̄ represents
                                                                            expand the second term in the integral by applying Bayes’
the mean and variance of a Gaussian distribution. The model
                                                                            rule:
first infers these parameters from a set of examples and then                                  Z
generates new instances by sampling from that distribution.                    P(xnew |x̄) = ∑       P(xnew |S, η̄)P(S, η̄|x̄)d η̄
Although this procedure is simple, it cannot be carried out by                               S   η̄
a standard exemplar model, which provides a way to classify,                                   Z
but not generate, new instances. Note, however, that in this                               =∑        P(xnew |S, η̄)P(x̄|S, η̄)P(η̄|S)P(S)d η̄ (4)
                                                                                             S   η̄
simple setting, new instances can be created by an approach
that takes an existing exemplar and slightly varies some of its                Each distribution on the right hand side of Equation 4 is
feature values. We therefore move to a richer setting where                 specified by the generative assumptions in Equation 3.
this “copy and tweak” strategy is likely inadequate.
   Instead of considering cases where category instances are                                          Experiment 1
characterized by values along a single dimension, suppose                   We designed a category generation experiment using stim-
that category instances are now represented as feature vectors.             uli like the circles in Figure 3 in order to test two main hy-
Furthermore, suppose that there are one or more latent causes               potheses: (1) that people are capable of category generation,
that generate multiple features simultaneously, which leads to              evidenced by their ability to generate new instances of the
groups or clusters of features.                                             category, and (2) that the model presented here approximates
   Here we work with a case where category instances are cre-               human performance on the task.
ated by filling four locations in a circular figure with letters.
Four of these instances are shown at the bottom of Figure 3b.               Method
The four locations are partitioned into one or more slots, and              Participants. Seventeen Carnegie Mellon undergraduates
we refer to this partition as a structure. There are 15 possible            completed the experiment for course credit.
partitions, a subset of which are shown in Figure 3a. Given                 Design and Materials. Three different sets of of stimuli were
the structure S of a category, instances of the category are cre-           created using the first three structures in Figure 3a, resulting
ated by filling each slot with a piece. Figure 3b shows a case              in three conditions. Each participant was exposed to two of
where the structure includes a horizontal slot and a vertical               these conditions in a randomized order.
slot, each of which includes two locations. The parameter η̄                   For each set, 16 different capital letters were chosen as
specifies a distribution over pieces for each slot. In Figure 3b,           features. All vowels were eliminated from consideration to
η1 is a distribution over pieces that can fill the vertical slot,           avoid the possibility of accidental formation of pronounce-
and η2 is a distribution for the horizontal slot. An instance of            able syllables or actual words. The letters, A, C, T, and G
                                                                        132

    (a)                       (b)                   (c)                      (a)                  Humans                Model
          0
          1
          1
          0
          10
           1
           01
           1
              2   3   4             1
                                      0
                                     01
                                     0
                                     1
                                     11
                                      00
                                       1
                                       1
                                       0
                                        2   3   4
                                                               D
          0
          100
          01                        1
                                    01
                                     00
                                      1
                                      01
                                       0
                                                                                            0.4               0.4
                                                                               Proportion
     1         ZN              1                          Z        N
          1 1
            0                       1
                                    0 1                        B
     2
           0
          01
          101
           101
             0
             0
             1
               QJ              2
                                    1
                                    0
                                    0
                                    11
                                     0
                                     10
                                     01                       (1,1)                         0.2               0.2
     3
           10
            01
           01
            10
             1
               VS
             0 HF
                               3
                                    1
                                    0
                                    1
                                    00
                                     1
     4
            10
            01                 4
                                    0
                                    1                                                        0                 0      (1,3)
                                                                                                                      (1,4)
                                                                                                                      (2,2)
                                                                                                                      (2,4)
                                                                                                                      (3,1)
          D   R   X W                                                                                                 (3,3)
                                                                                                                      (4,1)
                                                                                                                      (4,2)
                                                                             (b)
                                                                                                                    Others
          B   L   M K
                                                                                            0.4               0.4
                                                                               Proportion
Figure 4: The stimuli used in Experiments 1 and 2. In each
grid, the rows represent the possible pieces for one slot and
                                                                                            0.2               0.2
the columns represent the possible pieces for the other slot.
The rows and columns are numbered so they may be identi-                                     0                 0      (1,1)
fied in the text. The hatched cells indicate which combina-
                                                                                                                      (2,2)
                                                                                                                      (3,3)
                                                                                                                      (2,4)
                                                                                                                      (3,4)
                                                                                                                      (4,2)
                                                                                                                      (4,3)
tions were shown to participants. (a) Experiment 1 stimuli.                                       Responses
                                                                                                                      (4,4)
                                                                                                                    Others
                                                                                                                       Responses
An example set of feature values are also shown along the
right and bottom edges of the grid. (b) Experiment 2 stimuli.                Figure 5: Comparison of human responses and model pre-
(c) An example stimulus corresponding to item (1,1) in (a).                  dictions for (a) Experiment 1 and (b) Experiment 2. The
                                                                             black bars indicate the frequency of the eight most popular
                                                                             responses, which are equivalent to the eight most probable
                                                                             responses according to the model. The white bars show the
were also eliminated because of their semantic significance
                                                                             combined frequencies for all other responses. The human re-
within the context of the experiment, which included a story
                                                                             sponses in both cases are averaged over the three conditions
about genomes, described below. Letters were grouped into
                                                                             within the groups shown in brackets.
pairs to make a total of eight pairs, four of which made up
the possible values of pieces for slot 1, and the other four of
which made up the possible values of pieces for slot 2. As a
result, there were 16 possible combinations of pieces for each               ing them with a pen on paper or with a graphics tablet on the
set of stimuli, of which participants were shown half. 2 The                 computer.
exact set of items shown to participants is indicated by the                    After making their guesses, they proceeded to a rating task
hatched cells in Figure 4a.                                                  in which they were shown a series of new genomes and asked
                                                                             to rate the likelihood (on a scale from 1 to 7) that each one
   In addition to the training stimuli, a set of testing stim-
                                                                             represented a flu virus that would be observed this year. Thus,
uli were prepared for a rating task. These items included
                                                                             the first phase of the experiment was a category generation
some valid but unseen combinations of letter pairs (i.e. the
                                                                             task and the second phase was a classification task.
unhatched cells in Figure 4), some seen and unseen combi-
                                                                                Participants were then given a new set of cards with a dif-
nations rotated 90 degrees (thus violating the structure of the
                                                                             ferent structure and repeated the preceding procedure.
category), and some distortions of seen items that matched
between one and three individual features but were not con-                  Results
sistent with the structure of the set. The rating task therefore
was a typical classification task in which participants had to               The model learns a category distribution that assigns nonzero
decide which novel items belonged in the category. The ex-                   probabilities to training items. To produce our predictions,
                                                                             we set these probabilities to 0, normalized the resulting dis-
act rating stimuli and the order in which they were presented
were both randomized across participants.                                    tribution, and sampled from it.
                                                                                These predictions and human responses are summarized in
Procedure. Participants were presented with the stimuli
                                                                             Figure 5a. The model predicts that the eight most probable
printed on index cards and were told that each item repre-
                                                                             responses correspond to the white cells in Figure 4a. These
sented the genome of a strain of flu virus that had been ob-
                                                                             items constitute a majority (53%) of human responses. The
served in the current year. They were encouraged to spread
                                                                             cells in the grid are not uniquely identifiable across condi-
the cards out on a table and rearrange them as they exam-
                                                                             tions, which used different sets of letters, so the results shown
ined them. They were told that enough funds existed only
                                                                             in Figure 5a are averaged across all possible alignments of
to produce a flu vaccine for one additional strain of flu and
                                                                             cells. This averaging procedure is the reason for the remark-
were instructed to make their three best guesses of a flu virus
                                                                             ably uniform appearance of the behavioral data. A break-
genome that was likely to be observed but was not already in
                                                                             down of responses per condition is shown in Figure 6a. Al-
the current set. Participants made their guesses by illustrat-
                                                                             though these results are noisy, two important observations can
    2 Similar stimuli were used by Fiser and Aslin (2001), in which          be made. First, with the exception of structure 2, the major-
participants successfully learned to differentiate between “chunks”          ity of participants’ responses (46% for structure 2) were valid
of symbols arranged in ambiguous grid.                                       recombinations of letter pairs. Second, among the most prob-
                                                                       133

able items, participants do not appear to favor any item in           piece in each slot appeared three times, two pieces in each
particular, again predicted by the model.                             slot appeared two times, and one piece in each slot appeared
   Due to the small training set and the highly unconstrained         once. Eighteen Carnegie Mellon undergraduates completed
nature of the task, the model also predicted a fairly large num-      the experiment for course credit.
ber of other responses, indicated by the white bar. However,
the predicted likelihoods for individual responses beyond the         Results
top eight are nearly negligible (∼ 3 × 10−4). The human re-           The model predictions were generated the same way as in
sponses were consistent with this prediction, and no response         Experiment 1. The predictions and experimental results are
other than the top eight most frequent items was generated            summarized in Figure 5b. Again, not all responses were
more than once.                                                       alignable across the different structures, and the averaged
   Responses to the rating task (see Figure 7a) provide addi-         groups are indicated by brackets. Unlike in Experiment 1,
tional evidence that participants understood the structure of         some responses were uniquely identifiable across conditions.
the category. Each participant’s set of responses were con-           For example, item (1, 1) is the only item made of pieces that
verted to z-scores and then the mean scores for the differ-           each appeared three times in the training set. Items (2, 2)
ent types of rating items were compared. There was a sig-             and (3, 3), however, are each made up of pieces that were
nificant difference between the mean scores per participant           seen twice, and therefore must be averaged across conditions.
for valid (M = 0.64, SD = 0.60) and invalid (M = −0.26,               With the exception of a small deviation from the model’s pre-
SD = 0.24) items, t(33) = 6.26, p < .001. The figure also             diction for the frequency of item (4, 4), human responses are
shows mean scores for some specific types of distractors—             well predicted by the model.
namely, those that included between one and three previously             A breakdown of responses per condition is shown in Fig-
observed pairs of features. Of particular interest are the items      ure 6b. In all three cases, the most frequently generated item
with three previously seen pairings (3 SP in the figure). If          was the most probable item according the model. In two of
participants had based their judgments only on observed pair-         the three cases, the top three most frequently generated items
wise correlations, they would give higher ratings to the 3 SP         were the model’s three most probable items. Individual re-
items than the valid items, which only contain two previously         sponses that did not match the top eight most probable items
seen pairings. There was a significant difference between the         were generated no more than twice.
scores for these items (M = −0.42, SD = 0.59) and valid                  Again, data from the rating task were analyzed (see Fig-
items, t(33) = 6.25, p < .001. These results suggest that peo-        ure 7b). Two sets of ratings were excluded because the par-
ple’s responses are not primarily driven by a simple notion of        ticipants did not rate every item. There was a significant
feature similarity.                                                   difference between the mean scores per participant for valid
   Taken together, our results for Experiment 1 suggest that          (M = 0.55, SD = 0.61) and invalid (M = −0.22, SD = 0.24)
people were able to generate new members of the category              items, t(32) = 5.23, p < .001.
we considered, and that this ability cannot be explained by a            These results replicate our previous finding that people are
simple similarity-based account. The two main predictions of          able to discover the structure of a category and generate new
our model were supported: people generate valid items more            category members that fit this structure. Our data also suggest
frequently than invalid items, but invalid items account for          that people are sensitive to frequency differences, a finding
some proportion of responses.                                         that is predicted by our probabilistic approach but appears
                                                                      less compatible with alternative rule-based accounts.
                        Experiment 2
Although Experiment 1 provides some initial support for our                                    Conclusion
model, our results are broadly consistent with an alternative         This paper was motivated by the observation that people are
model that learns rules (e.g., the rule that items are created by     able to generate new instances of a category. Our experimen-
combining two pieces) but that does not rely on probability           tal results confirmed this observation even in cases involving
distributions in any fundamental way. We therefore designed           relatively small training sets. These results also provide sup-
a second experiment that tests the probabilistic aspect of our        port for our computational approach to category generation,
approach more directly. The training stimuli in Experiment            which is general enough that it can be applied to many differ-
1 were created using pieces that appeared equally frequently.         ent cases of category generation.
In Experiment 2 we replaced this balanced set of frequencies             We focused on category generation at the exemplar level,
with a skewed set (see Figure 4b), and explored whether peo-          but the same basic approach may help to explain how entirely
ple would respond to these frequency differences as predicted         new categories are generated. For example, suppose one first
by our model.                                                         learns categories that can be characterized by bivariate Gaus-
                                                                      sian categories with different means but equal covariances.
Method                                                                Then, if asked to generate a new category in the same fea-
The materials and procedure in Experiment 2 were identical            ture space, we might expect people to choose a new mean but
to Experiment 1. The two experiments differed only in which           preserve the covariance of the training categories. The ap-
set of items were shown to participants. In Experiment 2, one         proach presented in this paper can account for such behavior
                                                                  134

               (a)                               Structure 1                                  Structure 2                 Structure 3                 Model
                     Proportion
                                  0.4                                          0.4                                0.4                       0.4
                                  0.2                                          0.2                                0.2                       0.2
                                     0                                           0                                 0                          0     (1,3)
                                                                                                                                                    (1,4)
                                                                                                                                                    (2,2)
                                                                                                                                                    (2,4)
                                                                                                                                                    (3,1)
                                                                                                                                                    (3,3)
               (b)                                                                                                                                  (4,1)
                                                                                                                                                    (4,2)
                                                                                                                                                  Others
                                  0.4                                          0.4                                0.4                       0.4
                     Proportion   0.2                                          0.2                                0.2                       0.2
                                     0                                           0                                 0                          0     (1,1)
                                                                                                                                                    (2,2)
                                                                                                                                                    (3,3)
                                                                                                                                                    (2,4)
                                                                                                                                                    (3,4)
                                                                                                                                                    (4,2)
                                                                                                                                                    (4,3)
                                                                                                                                                    (4,4)
                                                                                                                                                  Others
Figure 6: Comparison of human responses and model predictions for the three conditions in (a) Experiment 1 and (b) Experi-
ment 2. In all cases, the black bars correspond to the eight most probable responses according to the model.
(a)                                                            (b)                                                demand more creativity. The task modeled in this paper
                1                                                     1                                           is not especially creative, but future applications of our
Mean Rating
               0.5                                                   0.5                                          approach can consider tasks that require more imagination.
                0                                                     0                                           Characterizing the computational basis of creativity is obvi-
              −0.5                                              −0.5                                              ously a challenging problem, but a generative probabilistic
               −1                                                    −1
                                                                                                                  approach may provide part of the solution.
                      V
                                  All D   3 SP   2 SP   1 SP
                                                                           V
                                                                               All D   3 SP   2 SP   1 SP         Acknowledgments.      We thank Faye Han for help in
                                                                                                                  running the experiments and coding the data. We also
Figure 7: Mean ratings (converted to z-scores) for the test                                                       thank Blair Armstrong, Michael Lee, Dan Navarro and two
items in Experiments 1 and 2. V = Valid items; All D = All                                                        anonymous reviewers for helpful comments on an earlier
Distractors; 3 SP = Distractor with three seen pairings; 2 SP                                                     draft of this paper.
= Two seen pairings; 1 SP = One seen pairing.
                                                                                                                                         References
                                                                                                                  Anderson, J. R. (1991). The adaptive nature of categorization.
with a model that learns a distribution over the means and                                                          Psychological Review, 98, 409-429.
covariances of the categories and then samples from that dis-                                                     Bishop, C. M. (2006). Pattern recognition and machine
tribution to create a new category. It may then sample from                                                         learning. New York, NY: Springer.
the new category to generate instances of it (e.g., generating                                                    Bloom, P. (1994). Generativity within language and other
the first Cobb salad ever created).                                                                                 cognitive domains. Cognition, 51(2), 177-189.
                                                                                                                  Feldman, J. (1997). The structure of perceptual categories.
   Bloom (1994) has explored the hypothesis that the gen-
                                                                                                                    Journal of Mathematical Psychology, 41(2), 145-170.
erative properties of natural language are inherited by other
                                                                                                                  Fiser, J., & Aslin, R. (2001). Unsupervised statistical learn-
cognitive systems. Although we adopt a slightly different
                                                                                                                    ing of higher-order spatial structures from visual scenes.
definition of “generative”, it is clear that the ability to gen-
                                                                                                                    Psychological Science, 12(6), 499-504.
erate new items and ideas extends well beyond the domain
                                                                                                                  Nosofsky, R. (1985). Overall similarity and the identification
of language. Consequently, the generative approach may also
                                                                                                                    of separable-dimension stimuli: A choice model analysis.
have applications beyond category learning—for example, to
                                                                                                                    Perception & Psychophysics, 38(5), 415-432.
imagination and mental imagery, or to problem solving situa-
                                                                                                                  Reed, S. K. (1972). Pattern recognition and categorization.
tions in which people must devise a new solution to a problem
                                                                                                                    Cognitive Psychology, 3, 383-407.
after being shown several other solutions. In the case of men-
                                                                                                                  Ross, B. H., & Murphy, G. R. (1999). Food for thought:
tal imagery, people may have some notion of a distribution
                                                                                                                    Cross-classification and category organization in a complex
over visual scenes and sample from that distribution when,
                                                                                                                    real-world domain. Cognitive Psychology, 38, 495-553.
say, picturing a setting described in a novel.
                                                                                                                  Ward, T. B. (1994). Structured imagination: The role of
   Although many examples of category generation (e.g.,                                                             category structure in exemplar generation. Cognitive Psy-
generating a new instance of a Caesar salad) seem fairly                                                            chology, 27, 1-40.
ordinary, others (e.g., inventing a Cobb salad) seem to
                                                                                                            135

