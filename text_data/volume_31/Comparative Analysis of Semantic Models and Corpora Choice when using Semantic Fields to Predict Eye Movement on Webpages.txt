UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Comparative Analysis of Semantic Models and Corpora Choice when using Semantic Fields
to Predict Eye Movement on Webpages
Permalink
https://escholarship.org/uc/item/30z1k0hz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Dennis, Simon
Stone, Benjamin
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Comparative Analysis of Semantic Models and Corpora Choice when using
                        Semantic Fields to Predict Eye Movement on Webpages
                                  Benjamin P. Stone (bpstone@psychology.adelaide.edu.au)
                            School of Psychology, Level 4, Hughes Building, The University of Adelaide
                                                       Adelaide, SA 5005 Australia
                                              Simon J. Dennis (dennis.210@osu.edu)
                                   Department of Psychology, Ohio State University, 1827 Neil Ave
                                                        Columbus, OH 43210 USA
                             Abstract                                 tajima, & Polson, 2005; Kaur & Hornof, 2005; Pirolli, 2005;
                                                                      Stone & Dennis, 2007).
   Nine models are compared in their ability to predict eye-
   tracking data that was collected from 49 participants’ goal-          Several researchers have highlighted the importance of
   oriented search tasks on a total of 1809 webpages. Forming         combining display-based and semantic information when
   the basis of six of these models, three semantic models and        modelling user’s navigation through web sites (Blackmon,
   two corpus types are compared as components for the Seman-
   tic Fields model (Stone and Dennis, 2007) that estimates the       Polson, Kitajima, & Lewis, 2002; Chi et al., 2003; Pirolli
   semantic saliency of different areas displayed on webpages.        & Fu, 2003; Kaur & Hornof, 2005; Stone & Dennis, 2007).
   Latent Semantic Analysis, Sparse Non-Negative Matrix Fac-          Research that has combined display and semantic informa-
   torization, and Vectorspace were used to generate similarity
   comparisons of goal and webpage text in the semantic compo-        tion when predicting web users’ behavior include the Cogni-
   nent of the Semantic Fields model. Surprisingly, Vectorspace       tive Walkthrough for the Web (CWW, Blackmon, Kitajima &
   was consistently the best performing model in this study. Two      Polson, 2005), the Bloodhound Project (Chi et al., 2003), and
   types of corpora or knowledge-bases were used to inform the
   semantic models, the well known TASA corpus and other cor-         the Latent Semantic Analysis - Semantic Fields model (LSA-
   pora that were constructed from the Wikipedia encyclopedia.        SF, Stone & Dennis, 2007). For a detailed description of the
   In all cases the Wikipedia corpora out performed the TASA          CWW and Bloodhound Project the reader is directed to Stone
   corpora. Three other baseline models: Flat, Non-Flat, and
   No-Model were included as a point of comparison to evaluate        and Dennis (2007).
   the effectiveness of the Semantic Fields models. In all cases         One major unanswered question when integrating display-
   the Semantic Fields models outperformed the baseline models        based and semantic information is the nature of the seman-
   when predicting the participants’ eye-tracking data.
                                                                      tic model itself, and the data upon which it should rely. As
   Keywords: LSA; Vectorspace; SpNMF; Semantic Fields; web            discussed previously, the computational semantics literature
   pages; eye-tracking, goal-directed visual search
                                                                      provides multiple options for semantic models, and similarly
                                                                      there are several approaches one might take in specifiying a
                          Introduction                                corpus relevant to the task (Stone, Dennis, & Kwantes, sub-
The exponential increase in Internet usage over the last              mitted). In this paper, we aim to shed light on these issues,
decade has motivated psychological researchers to examine             by comparing different models and corpora on their ability to
web users’ behavior in this virtual environment. Research fo-         predict human eye movements during web browsing.
cussing on user behavior in web page environments can gen-
erally be delineated into two main streams: display-based and         Semantic Fields (SF)
semantics-based research. While both methods to some de-              In a previous article, we presented the LSA-SF model (Stone
gree attempt to predict the area on a webpage that a user will        & Dennis, 2007), which was used to predict the eye move-
focus their attention on, they approach this task in different        ments of 49 participants recorded during goal-oriented search
ways. Display-based research has focused on perceptual as-            tasks on three websites. The LSA-SF model used LSA to cal-
pects of the webpage, components such as element and menu             culate the similarity between a textual representation of the
position, color usage, and font style. Alternatively, when            users’ goal and each of the textual elements displayed on a
attempting to predict user’s webpage navigation, semantic-            webpage. Using a decay function, these LSA estimates of
based research matches web user’s information needs to the            similarity were then distributed and summed over each pixel
concepts displayed within the textual content of webpages.            position for all of the textual elements contained on a web-
Display-based and semantics-based research into web user’s            page (see Equation 1). Combining the semantic information
visual search of web page hyperlinks has indicated that the           (L) with distance (di(x,y) ) from its display position using a de-
user’s search processes are influenced by: text semantics, el-        cay function, enabled the production of maps of information
ement position, aesthetic qualities of elements, and environ-         density for each web page in our study (see Figure 1).
mental learning (Faraday, 2000, 2001; Ling & Van Schaik,
2002, 2004; Chi et al., 2003; Cox & Young, 2004; McCarthy,                                SF( x, y) = ∑ Li e−λdi(x,y)                (1)
Sasse, & Rigelsberger, 2003; Pearson & Van Schaik, 2003;                                               i
Pirolli & Fu, 2003; Rigutti & Gerbino, 2004; Blackmon, Ki-               Initially, the TASA corpus was used as a knowledge based
                                                                  1924

                                                                        Secondly, the calibration of the eyedata was performed in
                                                                     a different way in this study. During the course of the partici-
                                                                     pants’ search tasks, after each webpage they viewed, a screen
                                                                     containing nine calibration points was displayed on the mon-
                                                                     itor. The eye data recorded while the participants were view-
                                                                     ing these calibration points was used to adjust their eye data,
                                                                     correcting for head movements that occurred during the ex-
                                                                     periment. In the previous study, a complex algorithm was
                                                                     used to adjust the eye-positions relative to participants’ move-
                                                                     ment during experimentation. However, to make this process
                                                                     more transparent in the current study, the participants’ eye-
                                                                     points were repositioned using the average offsets recorded
                                                                     over all nine calibration points.
                                                                     Websites
                                                                     Three websites were chosen from the Internet:
Figure 1: LSA-SF Map with participant’s eye tracking data               www.greencorps.com.au1 (Green Corps Australia)
super imposed using black dots.                                         www.missionaustralia.com.au (Mission Australia)
                                                                        www.whitelion.asn.au (White Lion Australia)
for the LSA component of the LSA-SF model. However, re-
cent research has suggested that a better fit with the knowl-           Static versions of these sites2 were pre-fetched in Decem-
edge domains required to model human similarity judgments            ber 2005 to avoid changes created by website updates. These
in document comparison tasks may be achieved using doc-              websites are all similar in the type of service they provide,
ument sets which are retrieved from the online encyclope-            such that they all offer services to disadvantaged members of
dia Wikipedia (Gabrilovich & Markovitch, 2007; Stone et al.,         the community. The websites were chosen because they were
submitted). Also, while LSA is certainly the best studied sta-       sufficiently complex that searching for information on these
tistical semantic model, Stone et al. (submitted) found that         sites would be a non-trivial task for participants.
other models such as Vectorspace and Sparse Nonnegative
                                                                     Models
Matrix Factorization (SpNMF) out-performed LSA when es-
timating human judgments of document similarity. Based on            The SEMMOD3 semantic models package was used to incor-
these findings, in this paper we present a comparison of three       porate the Vectorspace model (Salton, Wong & Yang, 1975),
semantic models (LSA, SpNMF, and Vectorspace), and two               Latent Semantic Analysis (Kintsch, McNamara, Dennis, &
types of knowledge based (TASA and Wikipedia), when used             Landauer, 2006), and Sparse Nonnegative Matrix Factoriza-
as components in the generation of Semantic Fields.                  tion (Xu, Liu, & Gong, 2003) into the Semantic Fields model.
                                                                     The Vectorspace model (Salton, Wong & Yang, 1975):
                            Method                                   The Vectorspace model assumes that terms can be repre-
Human Data                                                           sented by the set of documents in which they appear. Two
                                                                     terms will be similar to the extent that their document sets
Eye-movement data generated by 49 university participants
                                                                     overlap. To construct a representation of a document, the vec-
on three websites was recorded during nine goal-oriented
                                                                     tors corresponding to the unique terms are multiplied by the
search tasks. The human eye-movement data used in this re-
                                                                     log of the frequency within the document and divided by their
search is fully described in Stone & Dennis (2007). There are
                                                                     entropy across documents and then added. Using the log of
only two differences in the preparation of the data used in this
                                                                     the term frequency (TF) within documents identifies higher
study to the methods used in the original study.
                                                                     frequency or important words in those documents. While di-
   Firstly, the original study used eye-tracking data recorded
                                                                     viding by the entropy or inverse document frequency (IDF)
on 1842 webpages, in this study only 1809 webpages are
                                                                     reduces the impact of high frequency words that appear in
used. It was found that 33 page views included in the original
                                                                     many documents in a corpus. Similarities are measured as
dataset were webpages designed by the researcher to catch
user clicks on PDF files. These “catch-pages” only contained             1 The Green Corps Australia URL is no longer used, the Aus-
one textual element that informed the participants that they         tralian Government has change both the website and its URL, which
had either found their goal or had not and should click the          can be viewed here: http://www.greencorps.gov.au
                                                                         2 The static versions of these websites can be found here:
“back” button. These pages have been removed because they
                                                                     http://www.psychology.adelaide.edu.au/mall lab/lsa-sf sites/
were not part of the original websites, and their simple one             3 The SEMMOD semantic models package is release under
element construction with black text on a white background           the GNU Licence and can be found here: http://mall.psy.ohio-
probably favored the Semantic Fields model.                          state.edu/wiki/index.php/Semantic Models Package %28SEMMOD%29
                                                                 1925

the cosines between the resultant vectors for different docu-        complete articles removed were identified if they contained
ments.                                                               the phrases like “help Wikipedia expanding” or “incomplete
Latent Semantic Analysis (LSA, Kintsch, McNamara,                    stub”.
Dennis, & Landauer, 2006): LSA started with the same rep-               To enable the creation of sub-space corpora, Lucene4 (a
resentation as the Vectorspace model - a term by document            high performance text search engine) was used to index each
matrix with log entropy weighting. In order to reduce the            document in the Wikipedia corpora. Lucene allows the user
contribution of noise to similarity ratings, however, the raw        to retrieve documents based on customized boolean queries,
matrix is subjected to singular value decomposition (SVD).           and wildcard operators like ’the star’ (*) which return mul-
SVD decomposes the original matrix into a term by factor             tiple results from word stems. Like the more well known
matrix, a diagonal matrix of singular values and a factor by         search engine Google, the documents returned by Lucene are
document matrix. Typically, only a small number of factors           ordered by relevance to a query.
(e.g., 300) are retained. To derive a vector representation of a        As mentioned above, three search tasks were given to par-
novel document, term vectors are weighted, multiplied by the         ticipants for each website. A sub-space of 1000 documents
square root of the singular value vector and then added. As          was created for each website using simple Lucene queries that
with the vector space model, the cosine is used to determine         contained keywords from the three tasks set to a specific web-
similarity.                                                          site. For example, a textual representation of the tasks given
                                                                     to participants for the Green Corps website are:
Sparse Nonnegative Matrix Factorization (SpNMF, Xu,
                                                                        1) You want to know more about Green Corps manage-
Liu, & Gong, 2003): Nonnegative Matrix Factorization is a
                                                                     ment. Find out who is the National Program Manager of
technique similar to LSA, which in this context creates a ma-
                                                                     Green Corps.
trix factorization of the weighted term by document matrix.
This factorization involves just two matrices a term by fac-            2) Find what environmental and heritage benefits are con-
tor matrix and a factor by term matrix - and is constrained to       tributed by Green Corps.
contain only nonnegative values. While nonnegative matrix               3) Find the online Expression of Interest form to apply to
factorization has been shown to create meaningful word rep-          become a Green Corps Partner Agency.
resentations using small document sets, in order to make it             Using keywords from these tasks the following Lucene
possible to apply it to large collections we implemented the         query was used to create the 1000 document subspace for the
sparse tensor method proposed by Shashua and Hazan (2005).           Green Corps website:
As in LSA, log entropy weight term vectors were added to                (“green corps”) OR (“national” AND “program” AND
generate novel document vectors and the cosine was used as           “manager”) OR (“environment*” AND “benefit*”) OR
a measure of similarity.                                             (“heritage” AND “benefit*”) OR (“partner” AND “agency”)
Corpora                                                              Appending Webpages as Documents - The creation of
                                                                     TASA-WEB and WIKI-WEB corpora. When creating
TASA The Touchstone Applied Science Associates                       corpora for each of the three websites in this study, the textual
(TASA) corpus was constructed to represent the reading ma-           content of each of the webpages viewed by participants on
terial covered by American students up to first year college.        that website were appended to the TASA and Wikipedia sub-
Moreover, the documents contained in the TASA corpus                 corpora. For example, when creating a corpus for the Mission
range over nine content areas: language arts and literature,         Australia website, overall 57 unique webpages were viewed
social science, science and math, fine arts, home economics          by participants during the experiment. So, the textual content
and related fields, trade, service and technical fields, health,     from these webpages was used to construct a mini-corpus of
safety and related fields, business and related fields, popular      57 documents, which was then appended to both the TASA
fiction and nonfiction (Budiu, Royer, & Pirolli, 2007).              corpus and Wikipedia sub-spaces for Mission Australia cor-
Wikipedia Wikipedia was used as a generic set of doc-                pora5 . Highlighting this appended data, the naming conven-
uments from which smaller targeted sub-spaces could be               tion TASA-WEB and WIKI-WEB is used through out this
sampled and compiled. Wikipedia is maintained by the                 paper.
general public, and has become the largest and most fre-
quently revised or updated encyclopedia in the world. Crit-          Baseline models to estimate eye-position
ics have questioned the accuracy of the articles contained in        Three alternative models were designed as a baseline to assess
Wikipedia, however research conducted by Giles (2005) did            the success of the Semantic Fields models when predicting
not find significant differences in accuracy of science based        participants’ eye-positions when they are engaged in goal-
articles in Wikipedia and similar articles contained in the En-      oriented search tasks on the three websites. These baseline
cyclopedia Britannica. In total 2.8 million Wikipedia entries            4 PyLucene, is a Python extension that allows access to the Java
were collected current to March 2007, however the document           version of Lucene: http://pylucene.osafoundation.org/
number was reduced to 1.57 million after the removal of in-              5 Note: Green Corps and White Lion websites’ textual data was
complete articles contained in the original corpus. The in-          not used when creating corpora for the Mission Australia data.
                                                                 1926

models are the Flat, Non-Flat, and No-Model.
Flat Model The Flat model is the simplest model, it as-
sumes that the eye-position has equal likelihood of being fo-
cused on all pixels contained on the webpage. Given the to-
tal number of eye-points (EP), and the total number of pix-
els that an eye-point (p) could be located in (1280 x 1024),
for each page (i) that is viewed (V ) by participants, the Flat
model calculates the log-likelihood of the eye-points on any
given webpage as LLwebpage (see Equation 2). The sum of
these webpage log-likelihoods (LLFLAT ) calculates the fit of
the Flat Model to all eye-points recorded for participants.
               LLFLAT =      ∑ LLwebpagei
                            i∈V
                                      
                                               1
                                                                (2)
            LLwebpage =       ∑   log
                             p∈EP        1280 × 1024                     Figure 2: Textual webpage elements are highlighted in red,
                                                                         images that have “ALT” or descriptive text are included.
Non-Flat Model The Non-Flat model is similar to the Flat
model, except it gives more weight to the probability esti-
mates for those eye-points found in textual elements6 . The
Non-Flat model is displayed in Equation 3, where for each
                                                                                             LLSF =       ∑ LLwebpagei
webpage (i) that is viewed (V ) by participants, N is the num-                                            i∈V
ber of pixels in text elements, M is the number of pixels                                                            
                                                                                                                            SFp
                                                                                                                                             (4)
outside text elements, A is the number of eye-points in text                           LLwebpage =         ∑    log
                                                                                                          p∈EP          SFwebpage
elements and B is the number of eye-points outside text el-
ements. Furthermore, ŵ is the optimized probability of an
eye-point being in a text element (see Equation 3). The maxi-            Calculating the log-likelihood for Semantic Fields and
mized log-likelihood (ML) over all webpages viewed by par-               No-Model conditions The log-likelihoods for the Seman-
ticipants occurred at a MLE of ŵ = 3.41 for this sample.                tic Fields models and No-Model are calculated in the same
Therefore participants were 3.41 times more likely to focus              fashion. The Semantic Field Value7 (SF(x,y) , see Equation 1)
their eyes on the textual elements on webpage than focusing              for each eyepoint (p) is divided by the summed total of the
on other areas, moreover the calculation of log-likelihoods for          Semantic Field Values for all pixels that webpage (SFwebpage ).
eye-points in these textual areas have been assigned greater             This process calculates the probability that a single eye point
weight in accordance with this finding.                                  is viewed. Then, the log of these probabilities (LLwebpage ) is
                                                                         calculated and summed over all eye-points (EP) on a web-
                                                                         page viewed by a participant. This process of summing
            MLNONFLAT =         ∑ MLwebpagei                             the log-likelihoods is repeated for each webpage (i) that
                                i∈V                                      was viewed (V ) by participants to calculate the overall log-
                                                
                                           ŵ                            likelihood (LLSF ) for both the Semantic Fields and No-Model
               MLwebpage =      A log                            (3)
                                        ŵN + M                          conditions (see Equation 4).
                                                      
                                                 1
                               +     B log                                                               Results
                                             ŵN + M
                                                                         The log-likelihoods (LL) of nine models constructed to pre-
No-Model The No-Model condition has been created to test                 dict participants eye movements are compared in this section
the theory that the Semantic Fields model is driven only by              using the Bayesian Information Criterion (BIC, Schwartz,
the structure of the webpage, and that the semantic models               1978) for assessing model fit. The nine models include six
do not add to the Semantic Fields model’s capacity to predict            Semantic Fields (SF) models, half of these SF models used
participants’ eye-positions. It takes the same parameters as             the TASA-WEB corpus while the other half used WIKI-WEB
the Semantic Fields model, however the semantic model co-                corpora as a knowledge base. Furthermore, three semantic
efficient is kept constant at one (Li = 1.0) in the calculation          models (LSA, SpNMF, and Vectorspace) were used to com-
of the Semantic Fields (see Equation 1).                                 pare goal text to element text in these SF models. Therefore,
                                                                             7 No-Model holds the semantic model coefficient constant at 1.0,
    6 An example of text elements (including images that have a tex-     so the value it returns for a pixel can still be thought of as a Semantic
tual description) on a webpage are shaded in red in Figure 2             Field Value
                                                                     1927

for the SF models, there is a two (corpora) by three (semantic      to compare the similarity of textual information. That said,
model) experimental design. The other three models com-             the generation of Wikipedia sub-spaces in this research was
pared here are the Flat, Non-Flat, and Non-Model conditions.        based on very simple boolean queries. Greater focus on the
While the calculation of most model LL only takes one pa-           formulation of these Lucene queries may increase the per-
rameter, the maximized log-likelihood (ML) calculated for           formance of semantic models when calculating text similar-
the the Non-Flat model has two. The BIC is the most appro-          ity and thereby conceivably produce better estimates of eye-
priate method of comparing the fit of these LL to the eye-data,     tracking data by the Semantic Fields model.
because it adjusts for the number of parameters going into the         LSA has been the focus of much of the statistical lexical
model. Moreover, higher BIC score indicate a better fitting         semantics research in recent years. That LSA has success-
model to the data.                                                  fully been used to grade essays (Foltz, Laham, & Landauer,
                                                                    1999) is testament to the overall usefulness of this model.
Table 1: Comparison of Bayesian Information Criteria (BIC)          It was therefore surprising that a much simpler model like
statistics calculated from LL generated for all nine models.        Vectorspace, would consistently out-perform more complex
                                                                    models such as LSA and SpNMF when generating similar-
    Corpora         Model          BIC             Difference       ity comparison of text in this study. It is also interesting to
    WIKI-WEB        Vectorspace    -10983963       0                note that Vectorspace is the first step in the calculation of both
    WIKI-WEB        SpNMF          -10989551       -5588            LSA and SpNMF, which begs the question as to whether the
    WIKI-WEB        LSA            -10990633       -6670            extra complexity introduced by these latter models when em-
    TASA-WEB        Vectorspace    -10993390       -9427            ploying dimension reduction is of benefit when performing
    TASA-WEB        SpNMF          -11001041       -17078           textual comparisons of user goals and webpage content. Fur-
    TASA-WEB        LSA            -11001286       -17323           thermore, the simplicity of Vectorspace’s calculation allows
    *               No-Model       -11005111       -21148           for quick and efficient construction of “on-the-fly” semantic
    *               Non-Flat       -11287790       -303827          knowledge spaces that could be incorporated into more ap-
    *               Flat           -11417562       -443599          plied models of semantic saliency on webpages.
* No corpus used                                                       As outlined previously, incorporating additional compo-
                                                                    nents such as colour and contrast information may improve
                                                                    the performance of the Semantic Fields model (Stone &
   The results displayed in Table 1 are presented in descend-
                                                                    Dennis, 2007). Another possible approach would be to use
ing order of their BIC scores. There were three interesting
                                                                    the Explicit Semantic Analysis (ESA) model (Gabrilovich &
trends displayed in the results. Firstly, BICs were higher for
                                                                    Markovitch, 2007) to make textual comparisons in the Se-
the six Semantic Fields models than the other models, there-
                                                                    mantic Fields model. The ESA model also uses Wikipedia as
fore the Semantic Field models provided a better fit for the
                                                                    a knowledge base and has performed very well on document
eye tracking data than the other baseline models. Moreover,
                                                                    comparison tasks. Using human judgments of the document
as would be expected, the simple Flat model performed the
                                                                    similarity of 50 headlines and précis taken from the ABC on-
worst, followed by the Non-Flat model, and then the No-
                                                                    line news mail service that were collected by Lee, Pincombe,
Model; the latter two baseline models were expected to per-
                                                                    and Welsh (2005), Gabrilovich and Markovitch (2007) re-
form better as they contain information about the structure
                                                                    port that ESA has generated estimates of document similarity
of the webpage display. Secondly, corpus choice appeared
                                                                    that correlate strongly (0.72) with human performance on this
to affect SF model performance. The SF models using the
                                                                    task. Given the ESA’s strong performance at this task, in fu-
WIKI-WEB corpora outperformed the TASA-WEB corpora
                                                                    ture research on the Semantic Fields model may use the ESA
in all instances. Thirdly, the semantic models that were used
                                                                    to generate estimates of similarity between webuser’s goals
in the SF models produced a main effect. Using either corpus
                                                                    and textual elements contained on webpages.
as a knowledge base, Vectorspace was consistently the best
performing model, followed by SpNMF and then LSA.
                                                                                              Summary
                         Discussion                                 Nine models were used to try and predict the eye-movements
While the Semantic Fields models provided the best fit to the       of 49 participants who were performing goal-oriented search
human eye-tracking data in this study, there were some inter-       tasks on three websites. All six Semantic Fields models pro-
esting performance differences within each of the Semantic          duced a better fit than three baseline models at this task. Both
Fields models that were introduced by the manipulation of           choice of corpora and semantic model produced main effects
corpora and semantic models. Corpora like the TASA have             when estimating the human eye-tracking data. Such that,
been hand-picked to broadly represent the expected general          the corpora drawn from Wikipedia out-performed the TASA,
knowledge of a first-year American college student. How-            and Vectorspace consistently outperformed both SpNMF and
ever, the findings in this study indicate that for some seman-      LSA. Particularly encouraging was finding that the Semantic
tic models (LSA, Vectorspace and SpNMF) semi-automated              Fields models outperformed the No-Model condition, indi-
corpora generation using Wikipedia provides a better base           cating that the semantic component of the Semantic Fields
                                                                1928

model is providing more to the fit of this model to the eye-           tors in computing systems (pp. 51–60). New York, NY,
tracking data than can be produced by the display component            USA: ACM.
alone.                                                               Landauer, T., McNamara, D., Dennis, S., & Kintsch, W.
                                                                       (2006). Handbook of latent semantic analysis. Mahwah,
                    Acknowledgements                                   NJ: Lawrence Erlbaum Associates.
Many thanks to Dan Navarro for taking the time to read               Lee, M. D., Pincombe, B. M., & Welsh, M. B. (2005). An em-
through this paper and for his excellent suggestions.                  pirical evaluation on models of text document similarity. In
                                                                       Proceedings of the 27th annual conference of the cognitive
                          References                                   society (pp. 1254–1259). Mahwah, NJ: Lawrence Erlbaum
                                                                       Associates.
Blackmon, M. H., Kitajima, M., & Polson, P. G. (2005).               Ling, J., & Van Schaik, P. (2002). The effect of text and back-
   Tool for accurately predicting website navigation prob-             ground color on visual searches of web pages. Displays, 23,
   lems, non-problems, problem severity, and effectiveness of          223–230.
   repairs. In Chi ’05: Proceedings of the sigchi conference         Ling, J., & Van Schaik, P. (2004). The effect of link format
   on human factors in computing systems (pp. 31–40). New              and screen location on visual search of web pages. Er-
   York, NY, USA: ACM.                                                 gonomics, 47, 907–92.
Blackmon, M. H., Polson, P. G., Kitajima, M., & Lewis, C.            McCarthy, J., Sasse, M. A., & Rigelsberger, J. (2003). Could
   (2002). Cognitive walkthrough for the web. In Chi ’02:              i have the menu please? an eye tracking study of design
   Proceedings of the sigchi conference on human factors in            conventions. In Proceedings of hci 2003 (pp. 401–414).
   computing systems (pp. 463–470). New York, NY, USA:                 London, UK: Springer-Verlag.
   ACM.                                                              Pearson, R., & Van Schaik, P. (2003). The effect of spatial
Budiu, R., Royer, C., & Pirolli, P. L. (2007). Modeling infor-         layout of and link colour in web pages on performance in a
   mation scent: a comparison of lsa, pmi and glsa similarity          visual search task and an interactive search task. Interna-
   measures on common tests and corpora. In 8th riao confer-           tional Journal of Human-Computer Studies, 59, 327–353.
   ence. Pittsburgh, PA.                                             Pirolli, P. (2005). Rational analyses of information foraging
Chi, E. H., Rosien, A., Supattanasiri, G., Williams, A., Royer,        on the web. Cognitive Science, 29, 343–373.
   C., Chow, C., et al. (2003). The bloodhound project:              Pirolli, P., & Fu, W. (2003). Snif-act: A model of informa-
   automating discovery of web usability issues using the              tion foraging on the world wide web. In 9th international
   infoscentπ simulator. In Proceedings of the sigchi con-             conference on user modeling (pp. 45–54). Johnstown, PA:
   ference on human factors in computing systems (pp. 505–             Springer-Verlag.
   512). New York, NY, USA: ACM.                                     Rigutti, S., & Gerbino, W. (2004). Navigating within a web
Cox, A., & Young, R. M. (2004). A rational model of the                site: the webstep model. In Proceedings of the sixth inter-
   effect of information scent on the exploration of menus. In         national conference on cognitive modeling (pp. 378–379).
   Proceedings of the sixth international conference on cog-           Mahwah, NJ: Lawrence Erlbaum Associates.
   nitive modelling (pp. 82–87). Mahwah, NJ: Lawrence Erl-           Salton, G., Wong, A., & Yang, C. S. (1975). A vector
   baum Associates.                                                    space model for automatic indexing. Communications of
Faraday, P. (2000). Visually critiquing web pages. In Pro-             the ACM, 18, 613–620.
   ceedings of the 6th conference on human factors and the           Schwarz, G. (1978). Estimating the dimension of a model.
   web.                                                                The Annals of Statistics, 6, 461–464.
Faraday, P. (2001). Attending to web pages. In Chi ’01:              Shashua, A., & Hazan, T. (2005). Non-negative tensor fac-
   Chi ’01 extended abstracts on human factors in computing            torization with applications to statistics and computer vi-
   systems (pp. 159–160). New York, NY, USA: ACM.                      sion. In Proceedings of the 22nd international conference
Foltz, P. W., Laham, D., & Landauer, T. K. (1999). The in-             on machine learning (pp. 792–799). New York, NY.
   telligent essay assessor: Applications to educational tech-       Stone, B., & Dennis, S. (2007). Using lsa semantic fields
   nology. Interactive Multimedia Electronic Journal of                to predict eye movement on web pages. In Proceedings
   Computer-Enhanced Learning, 1(2).                                   of the 29th annual conference of the cognitive society (pp.
Gabrilovich, E., & Markovitch, S. (2007). Computing se-                665–670). Mahwah, NJ: Lawrence Erlbaum Associates.
   mantic relatedness using wikipedia-based explicit semantic        Stone, B., Dennis, S., & Kwantes, P. J. (submitted). Compar-
   analysis. In Proceedings of the 20th international joint con-       ing methods for document similarity analysis.
   ference on artificial intelligence (pp. 6–12). Menlo Park,        Xu, W., Liu, X., & Gong, Y. (2003). Document clustering
   CA: AAAI Press.                                                     based on non-negative matrix factorization. In Proceedings
Giles, J. (2005). Internet encyclopaedias go head to head.             of the 26th annual international acm sigir conference on
   Nature, 438, 900–901.                                               research and development in informaion retrieval (pp. 267–
Kaur, I., & Hornof, A. J. (2005). A comparison of lsa, word-           273). New York, NY: ACM Press.
   net and pmi-ir for predicting user click behavior. In Chi
   ’05: Proceedings of the sigchi conference on human fac-
                                                                 1929

