UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Belief Propagation and Locally Bayesian Learning
Permalink
https://escholarship.org/uc/item/9bx2c0gb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Sanborn, Adam
Silva, Ricardo
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                           Belief Propagation and Locally Bayesian Learning
                                        Adam N. Sanborn (asanborn@gatsby.ucl.ac.uk)
                               Gatsby Computational Neuroscience Unit, University College London
                                                               London, UK
                                              Ricardo Silva (ricardo@stats.ucl.ac.uk)
                                    Department of Statistical Science, University College London
                                                               London, UK
                             Abstract                                 proximation embodies the compelling idea that local calcula-
                                                                      tions are optimal, but the global prediction is only approxi-
   Highlighting, a conditioning effect, consists of both primacy-
   like and recency-like effects in human subjects.          This     mately optimal because information is lost in the communi-
   combination of effects are notoriously difficult for Bayesian      cation between regions.
   models to produce.       An approximation to probabilistic             The approximation introduced in LBL was successful in
   inference, Locally Bayesian learning (LBL), can predict
   highlighting by partitioning the model into regions during         fitting conditioning data, but the approximation itself has not
   learning and passing messages between these regions. While         been thoroughly studied. The goal of this paper is to connect
   the approximation matches behavior in this task, it is unclear     the approximation used in LBL with algorithms in computer
   how LBL compares to other approximations used in Bayesian
   models, and what behaviors this approximations will predict        science and statistics. First we introduce LBL and compare it
   in other paradigms. Our contribution is to show LBL is closely     to the probabilistically correct updating algorithm, Globally
   related to the statistical algorithms of Assumed Density           Bayesian Learning. Next we introduce Belief Propagation
   Filtering (ADF), which simplifies calculations by assuming
   independence, and Belief Propagation, which identifies how         and Assumed Density Filtering (ADF) and show how LBL
   to make these calculations through message passing. We             relates to both these algorithms. Next we describe the effect
   propose that people use ADF to learn and show how this             of highlighting and show that ADF can produce a highlight-
   model can produce highlighting behavior. In addition, we
   demonstrate how the degrees of approximation used in LBL           ing effect. Finally we show that LBL predicts a highlighting
   and ADF cause the models to make very different predictions        effect for alternating trials, while ADF does not.
   in a proposed experimental design.
                                                                                       Locally Bayesian Learning
   Keywords: machine learning; conditioning; Bayesian models;         The probabilistic model underlying LBL was a generaliza-
   belief propagation; assumed density filtering
                                                                      tion of a feedforward neural network with one hidden layer.
   Bayesian approaches have often been successful in pre-             This structure and the variable names used are shown in Fig-
dicting human data as the result of optimal behavior (e.g.,           ure 1a. The neural network was generalized from having sin-
Körding & Wolpert, 2004), but people can also produce be-            gle estimates of hidden weights, hidden nodes, and output
havior that is very difficult to explain with Bayesian ap-            weights by putting a distribution over the values of these hid-
proaches (Daw, Courville, & Dayan, 2008; Kruschke, 2006a,             den variables. The generalization required different calcula-
2006b). Additionally, Bayesian approaches can be hampered             tions than a neural network to update the weights, and the
by computational complexity in practical applications (An-            correct probabilistic update was termed Globally Bayesian
derson, 1991).                                                        Learning (GBL),
   Modeling human cognition as an approximation to optimal
behavior is an approach that has been used to both explain              p(Whid ,Wout , y | x,t) ∝ p(t | Wout , y)p(y | Whid , x)p(Wout ,Whid ) (1)
deviations from optimality as well as managing the computa-
tional complexity of the solution (Gigerenzer & Todd, 1999;           where the variables in the model are described in Figure 1 and
Kahneman, Slovic, & Tversky, 1982; Kruschke, 2006b).                   p(t|Wout , y) and p(y|Whid , x) were the standard linear combi-
Many of these algorithms were invented to match human                 nation of weights plus a sigmoid nonlinearity.
behavior, but recent work has proposed that candidate al-                 The network graph was then split into regions to imple-
gorithms for human cognition could be drawn from work                 ment LBL, as in Figure 1b. Psychologically, LBL was meant
in computer science and statistics (Sanborn, Griffiths, &             to represent two stages: an early attentional phase that took
Navarro, 2006). These algorithms have been developed to               stimulus cues from the input nodes and converted it into at-
efficiently produce results faithful to the full model, and can       tended cues on the hidden nodes, and weights from the at-
come with guarantees on the quality of the approximation.             tended cues to the output. Correct probabilistic calculations
   Conditioning has been a testbed for approximations to              were used within regions, but LBL uses messages between
Bayesian models. Conditioning effects such as blocking and            regions that result in an approximation to GBL. Informa-
highlighting depend on the order of the stimuli. An approx-           tion was passed between the two regions of LBL in two par-
imation that has successfully fit these types of effects is Lo-       ticular ways. The expected value of the attentional nodes
cally Bayesian Learning (LBL; Kruschke, 2006b). This ap-              E(y|x) in the first region given the stimulus cues was passed
                                                                  389

      A     Globally Bayesian
                Learning
                                   B   Locally Bayesian
                                            Learning
                                                                                  A      Globally Bayesian
                                                                                              Learning
                                                                                                                   B       Locally Bayesian
                                                                                                                               Learning
                         t                                t                                                             f1        f2     f3
                                                            Wout                         Wout           Whid
                           Wout                           y
                                          Forward: E(y|x)                                                                   Wout     y       Whid
                         y              Backward: argmax y*
                                                                                                 y
                                                         y
                           Whid
                                                            Whid
                         x                                x              Figure 2: Factor graphs of the variables in the Globally and
                                                                         Locally Bayesian Learning. In A, the factor graph of GBL is
                                                                         a loop, while in B, the factor graph of LBL is a chain.
Figure 1: Network diagrams of Locally and Globally
Bayesian Learning. The underlying models are feedforward
neural networks with input vector x, hidden weight matrix                computation for Bayesian models is one of its main applica-
Wh , hidden node vector y, output weight matrix Wo , and out-            tions.
put vector t. In GBL, all of the hidden parameters are inferred               Typically, inference by message passing assumes that the
together. In LBL, the network graph has been sectioned into              distribution of interest is represented as a product of factor
regions with copies of the hidden nodes y in each region.                functions fa (·)
Messages are passed back and forth between the copies of the
hidden nodes y, the expected value E(y|x) is passed upward                                                      1
                                                                                                       p(s) =          fa (sa )                    (3)
and the single y∗ that best maximizes the output is passed                                                      Z∏  a
backwards.                                                               where s denotes a set of random variables and sa the subset of
                                                                         variables that are arguments to the function fa (·). Constant Z
to the second region, and computations depending on y in                 is the normalizing constant of the distribution.
the second region were calculated based on E(y|x). Once the                   BP schemes are most commonly used as approximations
feedback response t was received by the system, the output               to marginals of distributions. In these schemes, one defines a
weights Wout were updated to be p(Wout |E(y|x),t), instead of            set of belief functions br (sr ) that will encode marginals of a
the p(Wout |x,t) as they would be under GBL.                             subset of variables sr ⊆ s: that is, belief functions are prob-
                                                                         ability functions, being non-negative and integrating to one.
   The second approximation was in the information passed
                                                                         The goal is to project the marginals of the (exact) distribution
back from the second region to the first region. In this ap-
                                                                         p(hidden | observed) into the corresponding belief functions
proximation the value of y that maximizes the probability of
                                                                         in a iterative and computationally cheap way – a brute-force
the feedback response t was computed,
                                                                         procedure that computes these marginals directly is in general
                                                                         not feasible.
        ŷ = arg max ∑ p(t|Wout , y∗ )p(Wout |E(y|x),t)          (2)          In most cases, the belief functions will only be approxi-
                 y∗   Wout
                                                                         mations to the true marginals, and the set of belief functions
and finally the hidden weight prior p(Whid ) was updated with            do not need to be globally coherent (in the sense there exists
p(Whid |x, ŷ).                                                          a single joint distribution with marginals given by the belief
   The local computation and message passing were moti-                  functions). Message passing can be applied to many types of
vated from several perspectives, including the desire to sim-            probabilistic models, but we focus on the special case that is
plify calculations (Kruschke, 2006b). The use of local com-              relevant to the interpretation and generalization of LBL.
putation and message passing to simplify computation has                 Message Passing for a Factored Prior
been echoed in computer science and statistics, and next we
describe a version of this approach and connect it to LBL.               In the probabilistic model, the complete set of random
                                                                         variables used when observing the cue-target pair {x,t} is
                     Belief Propagation                                  given by {x, y,t,Whid ,Wout }. When a cue-target pair {x,t}
                                                                         is revealed, Bayesian updating will consist of computing
Dividing a model into components that perform local up-                  p(Whid ,Wout , y | x,t). Within LBL and GBL, the posterior
dating, based only on the information available from con-                can be factored similarly to Equation 1. Using the framework
tiguous components, is an idea explored in machine learning              of Equation 3, the posterior is the result of multiplying (and
and statistics under the name Belief Propagation (BP; Minka,             renormalizing) these factors,1
2001; Yedidia, Freeman, & Weiss, 2005). BP is a message
                                                                               1 There   are multiple ways to construct the factor graph for
passing scheme for inference. The joint distribution is di-
                                                                         these variables. We could have instead treated the product
vided into regions, messages are passed between regions, and              f1 (Wout ) f2 (Wout , y) as a single factor, but this version better matches
updating is performed based on these messages. Reducing                  LBL.
                                                                     390

                                                                                  m f1 →Wout = f1 (Wout ). The marginal for Wout given {x,t} can
                  f1 (Wout ) ≡      p(Wout )                                      be rewritten in message passing notation as the belief function
               f2 (Wout , y) ≡      p(t | Wout , y)                       (4)     bWout (Wout ),
               f3 (Whid , y) ≡      p(y | Whid , x)p(Whid )
    Notice that we are considering stimulus {x,t} as fixed, as                              bWout (Wout ) ∝ m f1 →Wout (Wout )m f2 →Wout (Wout )        (7)
such, they are not arguments to the factor functions. We
                                                                                      In general, the belief function resulting from a message
also assume that the prior for the parameters factorizes as
                                                                                  passing scheme will produce only an approximation to the
p(Wout )p(Whid ) at any stage of our experiment, an assump-
                                                                                  desired marginal. The belief function would be approximate
tion we will justify in the next section.
                                                                                  for GBL, because the GBL factor graph in Figure 2a contains
    Consider a chained propagation scheme for defining be-                        a loop. This is not the case for the simple chain model of
liefs. The LBL graph of Figure 2b depicts our target distribu-                    LBL in Figure 2b, which is guaranteed to calculate the cor-
tion using blocks to denote factors and circles to denote vari-                   rect marginal. As we just verified, bWout (Wout ) = p(Wout | x,t).
ables, a representation also known as a factor graph (Yedidia
                                                                                      Calculating the marginal belief bWhid (Whid ) can be done by
et al., 2005). In constrast, GBL might also have as a prior a
                                                                                  an analogous process starting from the other end of the chain
function p(Wout ,Whid ) for all data points. This implies that a
                                                                                  (i.e., starting from message m f1 →Wout ). Computing the mes-
factor between Wout and Whid is in general also needed, which
                                                                                  sages toward Whid , our first summation comes as we pass in-
results in the loop in Figure 2a.
                                                                                  formation to node y,
    In LBL, we define messages that will pass from variable
nodes (the bottom, circled ones, in Figure 2b) to factor nodes
                                                                                                  m f2 →y (y) ∝  ∑   f1 (Wout ) f2 (Wout , y)           (8)
(top, squared nodes), and vice-versa. As we will see, such                                                      Wout
messages will stand for different partial summations over
the distribution function of interest, allowing us to compute                         As before, the message from y to f3 will again be just
marginals in a more efficient way than by brute-force summa-                      a copy of the incoming message from f2 . The message
tion. Consider the expression for the marginal of Wout given                      passing scheme will be finalized by the message from f3
the evidence after we sum out the other hidden variables,                         to Whid , m f3 →Whid . This message will be the factor function
                                                                                   f3 weighted by the incoming message from y, marginalizing
                                                                                  over y so that we obtain the marginal information for Whid ,
   p(Wout | x,t)       ∝    ∑∑      f1 (Wout ) f2 (Wout , y) f3 (Whid , y)
                             y Whid                                                           m f3 →Whid (Whid ) ∝ ∑ m f2 →y (y) f3 (Whid , y)          (9)
                      =     f1 (Wout ) ∑ f2 (Wout , y) ∑ f3 (Whid , y)                                               y
                                        y              Whid
                                                                          (5)         Because there are no other messages into Whid , its be-
    The inner summation over Whid can be cached as a func-                        lief function bWhid (Whid ) and the posterior p(Whid |x,t) will be
tion of the corresponding values of y. Expressing that as a                       identical to the above message.
message passing algorithm, we denote the value of this inner
summation as a message. Since the message goes from factor                        Approximations to Belief Propagation in Locally
 f3 to variable y and depends on the particular value of y, we                    Bayesian Learning
denote it as m f3 →y (y) 2 .                                                      We can now understand LBL algorithm as a belief propaga-
    Node y repeats the message from f3 to f2 , since it has                       tion algorithm with three approximations. Perform the first
no other neighbors but these two. We denote this copy as                          stage as we described, passing messages from Whid towards
my→ f2 (y). Given this message, a message from f2 to Wout can                     Wout . The first difference is that LBL replaces the message
be defined in terms of the summation over y using the cached                      m f3 →y with the “collapsed” message,
message,
                                                                                                    m0f3 →y (y) = δ(y = Ey∼m f3 →y (y))                (10)
             m f2 →Wout (Wout ) ∝ ∑ f2 (Wout , y)my→ f2 (y)               (6)
                                    y
                                                                                  where Ey∼m f3 →y (y) is the expectation of y with respect to
    For simplicity of interpretation, we assume we normalize                      m f3 →y (y). Function δ(·) is one if its argument is true, zero
all messages − in general, this is not necessary. Notice that                     otherwise. The rest of the message passing scheme towards
because we cached the previous summation, we are summing                          Wout stays the same, with the modified bW          0     (Wout ) being an
                                                                                                                                       out
over a reduced space now. Message passing can be seen as a                        approximation to p(Wout | x,t).
dynamic programming approach to probabilistic inference.                              The second and third approximations change the informa-
    Finally, since factor node f1 has no neighbors but Wout , it                  tion used to update Whid . Instead of using the prior distri-
passes its factor function to Wout with no summation required:                    bution of Wout in the update of Whid , LBL uses the posterior
     2 In general message passing situations, we expect f to first get            of Wout in the update. This approximation corresponds to an
                                                                3
a message from Whid , but since node Whid in Figure 2b has no other               overcounting of the evidence in the updating of Whid . The
neighbors, this message is empty.                                                 overcounting is reflected in the new message,
                                                                              391

                                                                             in LBL produces a non-standard projection of the true poste-
                                                                             rior into the space of factorized distributions, so the ADF us-
      m0f2 →y (y)   ∝     ∑ p(Wout | E(y|x),t)p(t | Wout , y) (11)           ing the exact marginals will always be as good or better than
                          Wout                                               LBL in the KL divergence sense.
                    =     ∑ b0wout (Wout ) f2 (Wout , y)            (12)        Updating the parameters using ADF is accomplished by
                          Wout                                               using the complete messages from Equations 4-9 in place of
                                                                             the approximated versions used in LBL. As in the LBL, learn-
   Instead of using the full message, LBL further approxi-
                                                                             ing is the result of local computations and message passing
mates this distribution by collapsing it to its maximum value:
                                                                             between regions.
                          m00f2 →y = δ(y = ŷ)                      (13)                             Highlighting
where ŷ = argmaxy∗ ∑Wout b0wout (Wout ) f2 (Wout , y∗ ). The mes-           We have connected LBL to ADF and BP and have proposed
sage passing is finalized by (9) using m00f2 →y instead of m f2 →y .         an alternative model based on this formulation. Now we can
LBL approximation algorithm is recovered, since the new                      explore how well these models predict human behavior in a
message from f3 to Whid is now                                               conditioning experiment. Space constraints prevent us from
                                                                             exploring the range of conditioning effects LBL has been ap-
             m0f3 →Whid (Whid ) ∝ p(ŷ | Whid , x)p(Whid )          (14)     plied to, so we focus instead on the most troublesome effect
                                                                             for Bayesian models, highlighting.
which given the normalization will be exactly p(Whid | ŷ, x).
                                                                                Highlighting (Kruschke, 1996, 2006b; Medin & Edelson,
   In contrast, even if the prior in GBL is factored, its fac-
                                                                             1988) is a conditioning effect that results from exposing sub-
tor graph changes after processing the first data point. The
                                                                             jects to two types of trials. The first, which is notated at
dependency between Wout and Whid will transform the graph
                                                                             I.PE → E, presents the subject with two cues: cue I and cue
from Figure 2b into the graph in Figure 2a, starting from the
                                                                             PE. Following the cues is the outcome E. The second type
second data point. The side-effect of the collapsing done in
                                                                             of trial is I.PL → L, in which subjects are trained on cues I
Equation 13 is that Wout and Whid will be marginally indepen-
                                                                             and PL and which leads to outcome L. In this task, cue I is an
dent. This approximate posterior distribution is then used as
                                                                             imperfect predictor of the outcomes, while cues PE and cue
the new prior for the next data point and the process repeated.
                                                                             PL are perfect predictors of E and L respectively.
An Alternative Approximation                                                    The highlighting effect occurs in designs in which subjects
                                                                             initially learn I.PE → E trials, and later learn I.PL → L tri-
Once we understand LBL as a special case of a message pass-
                                                                             als. When presented at test with cue I, subjects tend to choose
ing algorithm in a chain-ordered factor graph, we can exper-
                                                                             outcome E. But when presented at test with cues PE and PL,
iment with other approximations to the full Bayesian model.
                                                                             subjects tend to choose outcome L. This effect is usually ex-
The consequence of the LBL approximations that we focus
                                                                             plained in terms of attention: subjects first learn to associate
on is the independence that results from the collapsed mes-
                                                                             outcome E equally with cues I and cues PE, because they are
sages. This independence can also be produced using the full
                                                                             both equally predictive of this outcome in the I.PE → E tri-
messages, which is a type of Assumed Density Filtering ap-
                                                                             als. However, when later learning the I.PL → L trials trials,
proximation (ADF; Boyen & Koller, 1998). In ADF, message
                                                                             subjects realize the cue I is not informative, so ignore it and
passing is used to approximate a posterior over parameters for
                                                                             heavily weight the association between cue PL and outcome
a single data point, and this approximate posterior is used as
                                                                             L. During test, I has a stronger association with outcome E
the prior when processing the next point.
                                                                             and when cues PE and PL compete against each other, out-
   Instead of using the expected value E(y|x) or the maximum
                                                                             come L is chosen because PL has a stronger association to L
y∗ to remove dependencies between Wout and Whid , we will
                                                                             than PE has to E.
adopt the standard procedure of ADF: considering all factor-
ized distributions q(Wout ,Whid ) ≡ qo (Wout )qh (Whid ), find the              Kruschke (2006a, 2006b) demonstrated that highlighting
one that is closest to the true posterior p(Wout ,Whid | x,t) ≡              was an extremely challenging effect for Bayesian models of
px,t (Wout ,Whid ) according to the KL-divergence criterion,                 conditioning. The key challenge to these models is the canon-
                                                                             ical design: an equal number of I.PE → E and I.PL → L trials
                                                                             over the entire experiment. The design that accomplishes the
                                         qo (Wout )qh (Whid )                balance between global trial equality and local trial imbalance
               Z
KL(p || q) =       px,t (Wout ,Whid ) ln                      dWout Whid
                                          px,t (Wout ,Whid )                 is shown in Table 1. Using this design, the prediction from a
                                                                    (15)     stationary Bayesian model is indifference to the outcomes for
   Minimizing Equation 15 with respect to qo (·) and qh (·) re-              both cue I and cues PE.PL. However both human data (Kr-
sults in qo (Wout ) = p(Wout | x,t) and qh (Whid ) = p(Whid | x,t).          uschke, in press) and LBL (Kruschke, 2006b) predict a robust
In this way we obtain the exact marginals under indepen-                     highlighting effect.
dence, and use this approximation to the posterior as the prior                 We show the predictions of GBL and LBL for highlighting
when processing the next data point. The approximation used                  in Figure 3 as well as the results for human subjects estimated
                                                                         392

                                                                         nection to propose an alternative to LBL. Casting LBL in the
      Table 1: Kruschke’s Canonical Highlighting Design
                                                                         framework of BP identified several deviations from this mes-
             Phase     Number of Trials          Items
                                                                         sage passing algorithm. The deviations cause the posterior
             First            2 ∗ N1          I.PE → E
                                                                         distribution for the weights to be factorized. We use ADF to
            Second            3 ∗ N2          I.PE → E
                                                                         retain this factorization of the posterior weights, while pro-
                              1 ∗ N2           I.PL → L
                                                                         ducing the best possible approximation (as measured by KL
             Third        1 ∗ (N2 + N1 )      I.PE → E
                                                                         divergence) to the full posterior distribution. Like LBL, ADF
                          3 ∗ (N2 + N1 )       I.PL → L
                                                                         uses local message passing and it provides an interpolation
                                                                         between the more computationally complex full Bayesian
in Kruschke (in press). In the first three panels, the model             model and the more approximate LBL.
was trained on the same stimuli used in Kruschke (2006b): 7                 Highlighting has proven to be a difficult experimental ef-
trials of I.PE → E followed by 7 trials of I.PL → L. GBL                 fect to predict using Bayesian models (Daw et al., 2008; Kr-
shows no highlighting effect because of its insensitivity to             uschke, 2006a, 2006b) and LBL predicts these data, as does
order. LBL shows a highlighting effect that is almost exactly            our alternative, ADF. The differences in the approximation
the size of human effect. ADF shows a highlighting effect                are revealed, however, when they are trained on alternating
that is between GBL and LBL, but is not as large as in the               highlighting trials. ADF predictions are very close to the full
human data.                                                              Bayesian model, while the stronger approximations in LBL
   However, the human data were collected using many more                continue to predict a strong highlighting effect. This result
trials, N1 = 10 and N2 = 5 in the canonical design resulting             may only hold if learning is applied to the parameter spaces
in 50 trials of each type3 . When we train LBL and ADF on                used in (Kruschke, 2006b), as a preliminary investigation us-
the number of trials used in Kruschke (in press), then both              ing larger hypothesis spaces shows that LBL does not pro-
models produce highlighting effects as large or larger than              duce highlighting for alternating trials (Kruschke & Denton,
human highlighting effects (Figure 3).                                   2009). Further investigations are needed to test the predic-
                                                                         tions of these approximations against human highlighting ef-
Predictions for Alternating Trials                                       fects.
The highlighting effect is assumed to be caused by early                    In addition to highlighting, LBL produces human-like be-
blocks with a high proportion of I.PE → E trials and late                havior in a variety of conditioning paradigms. We have fo-
blocks with a high proportion of I.PL → L trials. An ex-                 cused on the highlighting effect in this paper due to space
treme example of the highlighting design is alternating trials,          constraints. Our initial simulations do show that ADF is able
in which single I.PL → L trial is followed by a I.PE → E                 to produce forward blocking, backward blocking, and a larger
trial. The early block and late block are both one trial long            effect for forward blocking than backward blocking.
and there can be many repetitions of the blocks. As far as
we are aware, the highlighting effect has not been tested for                                    Conclusions
in this design, and it would be surprising if it were found.             Kruschke (2006b) introduced the idea that conditioning ef-
A highlighting effect for alternating stimuli would mean that            fects, such as highlighting, could be produced by using local
subjects are extremely sensitive to small changes in order, in           message passing to approximate full Bayesian models. Our
fact, sensitive to the position of a single trial in the entire run      work builds on this approach by connecting it to message
of the experiment. Shifting the first trial to the end of the            passing algorithms used in computer science and statistics,
experiment in this design produces the opposite alternating              and develops an alternative that can also predict highlight-
order.                                                                   ing behavior. Connections between existing models and ma-
   Figure 4 shows the predictions for the models on the al-              chine learning algorithms give cognitive scientists access to a
ternating trials. As in the highlighting design, GBL predicts            rich resource for developing alternative models that produce
equal preference for the two alternatives when test with cue I           a range of behavior. Through matching these models to be-
or with cues PE.PL. For both 14 and 100 trials, LBL predicts             havior it is hoped that the approximations used in the mind
a highlighting effect that is as strong as the effect predicted          can be determined.
for the highlighting stimuli. ADF closely mimics GBL and
predicts nearly indifferent performance for both 14 and 100                                  Acknowledgments
alternating trials.                                                      Adam Sanborn was supported by a Royal Society USA Research
                                                                         Fellowship and the Gatsby Charitable Foundation.
                           Discussion
Our paper provides a connection between LBL and approxi-                                          References
mations in computer science and statistics and uses this con-            Anderson, J. R. (1991). The adaptive nature of human cate-
    3 As in Kruschke (in press), a canonical highlighting design was        gorization. Psychological Review, 98(3), 409–429.
used, but to simplify the results the simulations used only a single     Boyen, X., & Koller, D. (1998). Tractable inference for com-
copy of the canonical design.                                               plex stochastic processes. In Proceedings of the fourteenth
                                                                     393

                                Globally Bayesian                  LBL                       LBL                           ADF                         ADF
                                    14 Trials                    14 trials                 100 trials                    14 Trials                   100 Trials
                           �                           �                            �                            �                            �
                          ���                         ���                         ���                           ���                         ���
      �����������������
                          ���                         ���                         ���                           ���                         ���
                          ���                         ���                         ���                           ���                         ���
                          ���                         ���                         ���                           ���                         ���
                           �                           �                            �                            �                            �
                                ���� ����   � �����         ���� ����   � �����         ���� ����   � �����           ���� ����   � �����         ���� ����   � �����
                                      ���������                   ���������                   ���������                     ���������                   ���������
Figure 3: Highlighting results for a selection of models and experimental designs compared to human data. The mean human
preference is plotted on each graph with a circle. Bars around the circle to show 95% confidence intervals on the human data.
The bar plots show the model predictions of outcome E, where the line marks equal preference between outcomes E and L. A
standard set of cues is tested in each model: the original training sets of cues I.PE and I.PL, as well as the critical tests of cue
I and cues PE.PL. The set of models and stimuli used to train the models are described in the text.
                                Globally Bayesian                  LBL                       LBL                             ADF                       ADF
                                    14 Trials                    14 trials                 100 trials                      14 Trials                 100 Trials
                           �                           �                           �                             �                           �
                          ���                         ���                         ���                           ���                         ���
      �����������������
                          ���                         ���                         ���                           ���                         ���
                          ���                         ���                         ���                           ���                         ���
                          ���                         ���                         ���                           ���                         ���
                           �                           �                           �                             �                           �
                                ���� ����   � �����         ���� ����   � �����         ���� ����   � �����           ���� ����   � �����         ���� ����   � �����
                                      ���������                   ���������                   ���������                     ���������                   ���������
Figure 4: Alternating trial predictions for a selection of models and experiment lengths. The bar plots show the predictions of
various models of outcome E, where the line marks equal preference between outcomes E and L. The original training cues
I.PE and I.PL, as well as the critical tests of cue I and cues PE.PL. The set of models and stimuli used to train the models are
described in the text.
  conference on uncertainty in artificial intelligence (p. 33-                                        Psychological Review, 113, 677-699.
  42).                                                                                              Kruschke, J. K. (in press). Attentional highlighting in learn-
Daw, N. D., Courville, A. C., & Dayan, P. (2008). Semi-                                               ing: a canonical experiment. In B. Ross (Ed.), The psychol-
  rational models of conditioning: the case of trial order. In                                        ogy of learning and motivation.
  N. Chater & M. Oaksford (Eds.), The probabilistic mind (p.                                        Kruschke, J. K., & Denton, S. (2009). personal communica-
  431-452). Oxford, UK: Oxford University Press.                                                      tion.
Gigerenzer, G., & Todd, P. M. (1999). Simple heuristics that                                        Medin, D. L., & Edelson, S. M. (1988). Problem structure and
  make us smart. Oxford: Oxford University Press.                                                     the use of base-rate information from experience. Journal
Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982).                                               of Experimental Psychology: General, 117, 68-85.
  Judgment under uncertainty: Heuristics and biases. Cam-                                           Minka, T. (2001). A family of algorithms for approxi-
  bridge: Cambridge University Press.                                                                 mate Bayesian inference. Unpublished doctoral disserta-
Körding, K., & Wolpert, D. M. (2004). Bayesian integration                                           tion, MIT, Boston.
  in sensorimotor learning. Nature, 427, 244-247.                                                   Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2006).
                                                                                                      A more rational model of categorization. In Proceedings
Kruschke, J. K. (1996). Base rates in category learning. Jour-
                                                                                                      of the 28th Annual Conference of the Cognitive Science
  nal of Experimental Psychology: Learning, Memory, and
                                                                                                      Society. Mahwah, NJ: Erlbaum.
  Cognition, 22, 3-26.
                                                                                                    Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Con-
Kruschke, J. K. (2006a). Locally Bayesian learning. In Pro-
                                                                                                      structing free energy approximations and generalized belief
  ceedings of the 28th annual meeting of the cognitive sci-
                                                                                                      propagation algorithms. IEEE Transactions on Information
  ence society. Hillsdale, NJ: Erlbaum.
                                                                                                      Theory, 51, 2282–2312.
Kruschke, J. K. (2006b). Locally Bayesian learning with
  applications to retrospective revaluation and highlighting.
                                                                                            394

