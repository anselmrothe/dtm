UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The hierarchical prediction network: towards a neural theory of grammar acquisition
Permalink
https://escholarship.org/uc/item/4dn9w49s
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Bod, Rens
Borensztajn, Gideon
Zuidema, Willem
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        The hierarchical prediction network: towards a neural theory of grammar
                                                               acquisition
                                                 Gideon Borensztajn (gborensztajn@uva.nl)
                                                     Willem Zuidema (zuidema@uva.nl)
                                                        Rens Bod (rens.bod@uva.nl)
                             All authors: Institute of Logic, Language and Computation, 904 Science Park
                                                  1098 XH, Amsterdam, The Netherlands
                              Abstract                                  but everywhere in the brain. Why would the brain invest such
                                                                        an effort in developing topologies? The most plausible an-
   We present a biologically inspired computational framework
   for language processing and grammar acquisition, called the          swer, we find, is that the topology in the brain serves to en-
   hierarchical prediction network (HPN). HPN fits in the tradi-        code graded category membership. The brain infers the cate-
   tion of connectionist models, but it extends their power by al-      gory of a neural assembly from its topological position on the
   lowing for a substitution operation between the nodes of the
   network. This, and its hierarchical architecture, enable HPN         cortical surface, whether it is for seeing, walking or talking.
   to function as a full syntactic parser, able to emulate context         Another inspiration for our work comes from theories of
   free grammars without necessarily employing a discrete notion        cortical information processing that stress the hierarchical
   of categories. Rather, HPN maintains a graded and topologi-
   cal representation of categories, which can be incrementally         and columnar organization in the (visual) neocortex, such
   learned in an unsupervised manner. We argue that the forma-          as the fragment-based hierarchy approach (Ullman, 2007)
   tion of topologies, that occurs in the learning process of HPN,      and the Memory Prediction Framework (MPF) (Hawkins &
   offers a neurally plausible explanation for the categorization
   and abstraction process in general. We apply HPN to the task         Blakeslee, 2004). The key elements of the latter framework
   of semi-supervised ‘grammar induction’ from bracketed sen-           are that (i) categories in the neocortex encode temporal se-
   tences, and demonstrate how a topological arrangement of lex-        quences (of patterns), (ii) the neocortex stores categories in
   ical and phrasal category representations successfully emerges.
                                                                        an hierarchical fashion, (iii) as one goes up in the hierarchy,
                                                                        categories are formed that are progressively more invariant
                          Introduction                                  and more temporally compressed, (iv) the main function of
Our understanding of the world around us is organized in                the cortex is prediction of future events, and this is achieved
categories. There is nothing more basic than categorization             by ‘unfolding’ the temporally compressed categories to the
to our thought, reasoning, perception, action, learning and             lower levels.
speech. Categories are necessary for generalization from con-              In the current paper these ideas are generalized for lan-
crete observations to general rules that apply to novel situa-          guage, and integrated in a self-organizing network. The re-
tions, and to productively use language.                                sulting model, besides accounting for the gradedness of cate-
   Of course, categories also play a central role in linguistic         gories, also incorporates the notion of phrase structure.
theory. The sets of rules that describe grammars are essen-
tially systems of relations between categories. Yet, within                             A neural theory of syntax
formal linguistics no convincing account exists of how syn-             Starting point of the research in this paper is the assumption
tactic categories are acquired. Within the tradition of genera-         that there exists a uniform cortical mechanism that under-
tive grammar there has been little interest in this question, as        lies categorization and processing within different modalities.
humans are believed to be born equipped with a full fletched            The analogy between visual and linguistic processing leads to
language faculty (a universal grammar) with abstract adult              a proposal for a neural theory of language processing and ac-
syntactic categories already in place. A major obstacle for             quisition, and a computational implementation thereof, which
explaining the incremental acquisition of categories is their           we call the Hierarchical Prediction Network (HPN). The the-
status as set theoretical entities, where category membership           ory is founded on the following hypotheses about the neural
is either true or false, implying that a category exists before it      representation of a grammar:
has any members.
   Cognitive linguistics, in contrast, proposes that syntactic            There exist cell assemblies in the language area of the cor-
categories are prototype-based, and that category member-                  tex that function as neural correlates of graded syntactic
ship is graded (Lakoff, 1987). The acquisition of prototypical             categories1 .
categories can be modelled using a localist connectionist ap-             These ‘syntactic’ cell assemblies represent temporally com-
proach, such as exemplified by the Kohonen self-organizing                 pressed word sequences (phrases) which can ‘unfold’ to
map (Kohonen, 1995).                                                       predict words or assemblies in a lower level.
   One often cited advantage of Kohonen networks is that                  The topological and hierarchical arrangement of ‘syntactic’
they account for the development of topologies, such as have               assemblies in the cortical hierarchy constitutes a grammar.
been demonstrated not just in the visual system (e.g.. the ori-             1
                                                                              We do not actually adhere to a strict separation between syntax
entation columns of complex cells, Hubel & Wiesel, 1968),               and semantics, but in this work we focus on syntax alone.
                                                                   2974

Still, there is one more important aspect of human language
that our model has to deal with. It is often claimed that
the only thing special about human language is recursion
(Hauser, Chomsky, & Fitch, 2002). Recursion happens to
be a major stumbling block for most connectionist models
of language. We argue, that all that is needed to explain the
phenomenon of recursion from a cognitive point of view is a
basic ability for neural systems to perform substitution, and
that recursion is an epiphenomenon. It is clear what substitu-
tion means in a symbolic framework, because it is implicitly
assumed that variables are global and their content can be re-
placed. Within a biological context however, which lacks the
notion of variables, it is not so obvious what substitution is,
nor is it trivial, as with the related problem of binding, how
                                                                                        Figure 1: HPN architecture.
it can be instantiated in a connectionist model for language
processing (Fodor & Pylyshyn, 1988).
   In this paper we propose a neural network solution for sub-      through bottom-up activation of the slot. An HPN production
stitution that renders HPN potentially much more powerful           thus resembles a rewrite rule with the exception that its slots
than traditional connectionist networks. The price is that we       are not associated with any particular non-terminal. When
have to postulate a capacity for the nodes in our network to        all of its slots in turn have received bottom-up activation, the
transmit and store richer information than just activation lev-     production is completed, and the compressor node fires.
els, as will be detailed in the next section.
                                                                       In HPN, the slots fullfill the role of physical substitution
   Whereas in a symbolic framework substitution relations           sites where nodes are coupled to each other. Such a local cou-
between category members are typically assumed to be be in-         pling mechanism serves as a neural correlate for substitution
nate, in a neurally motivated model of grammar and category         in formal grammars, where substitution is defined globally.
learning the acquisition of substitutability relations (between     Every slot in HPN offers an independent substitution site in
nodes in the network) becomes the central question. We hy-          the context of which a set of nodes are more or less substi-
pothesize that a graded measure of substitutability is realized     tutable. Thus, the slots form a basis for a substitution space,
as a distance between neural assemblies within a meaning-           with respect to which the internal representations of both the
ful topology. The central claim of this work is thus that lan-      input and compressor nodes in HPN are defined. Our imple-
guage acquisition is in essence equivalent to learning a net-       mentation circumvents the need for full connectivity, and at
work topology.                                                      the same time enables node substitution: when a node fires,
                                                                    its internal representation is serially transmitted over a single
                      HPN architecture                              link to a central switch board (substitution space), where it is
The hierarchical prediction network (HPN) consists of an in-        transferred to the slots that best fit the node’s representation.
put layer composed of input nodes on top of which there are         This is illustrated in Figure 1.
one or more compressor layers with so-called compressor                Substitutablity between two nodes is given as some dis-
nodes. The nodes have a fixed position in the network, but          tance measure on the metric of substitution space. Thus, re-
they also develop representations in a virtual space, which         gions in substitution space define a continuum of categories,
we call the substitution space. The internal representation         and a node’s representation in substitution space defines its
is learned through the interactions that take place in the net-     graded membership to one or more categories.
work, and will eventually reflect the distribution of the input        Another feature that distinguishes HPN from conventional
data.                                                               neural networks is the fact that HPN keeps track of (serial)
   Input nodes interact with the external environment; in the       activation paths through the network as an input sequence is
language domain every input node is assumed to correspond           processed. Once (and provided) the neural network has ac-
to a unique word from the lexicon, and it fires whenever this       cess to the activation path, it can benefit from information
word is presented within a sentence.                                that is relevant for the internal interpretation of the sequence
   The compressor nodes correspond to sequences of words            with respect to its components, such as feature grouping (for
(phrases). The name compressor nodes implies that they tem-         visual processing) and phrase structure. These are tasks that
porally compress a sequences of nodes from the lower layers         conventional neural networks are not good at.
(as was hypothesized by the MPF). Compressor nodes have                Typically, many processes run in parallel in the network,
two or more ordered slots, with which they form a fixed unit.       and each is associated with a distinctive path. Different paths
   An ordered set of slots on a compressor node constitutes a       compete with each other for the best internal interpretation
production. A production is executed by matching each one           of the sequence. In order to differentiate between alternative
of the slots in a fixed order to an input or compressor node        paths (possibly crossing the same node multiple times), HPN
                                                                2975

 keeps so-called ‘path connectors’ in the slots of every visited     2. every slot of every compressor node in the parse is bound
 compressor node along the activation path. The path connec-             either to an input node (a word), or to the root of another
 tors store a pointer to the node that has bound to the slot (the        compressor node (a phrase); there is only a single compres-
 ‘sender node’), together with the ‘time of activation’, which           sor node with an unbound root.
 equals the processed part of the input sequence (the current        3. adjoining slots within a production can only be bound to
 position). The path connectors allow to distinguish different           nodes that processed adjoining portions of the sentence.
 active states of a single HPN node, much like the states in an
 Earley chart parser (Earley, 1970). The information needed           A derivation is an ordered sequence of time indexed produc-
 to update the state of the nodes (current sentence position and      tions (states) and bindings, that fully determines the followed
 sender node identity) is transmitted along the activation path,      trajectory through the network space upon a successful parse.
 through the links mentioned before.                                  As an example we give a derivation for the sentence Sue eats
    The solution for the binding problem in HPN can be as-            a sandwich in Table 1.
 cribed to the ability of compressor nodes to temporarily store         X1    →      12     1   >=<       Sue
 (multiple) path connectors. By virtue of locally stored point-         X2    →      34     3   >=<       eats
 ers from slots to ‘bound’ nodes, HPN is able in principle to           X3    →      56     5   >=<       a
                                                                        2     >=<    X2     6   >=<       sandwich
 represent parse trees of unlimited depth. An ordered sequence
                                                                        4     >=<    X3
 of such pointers corresponds to a stack in symbolic parsers,
 since it constitutes a distributed memory of the productions         Table 1: A derivation of the sentence Sue eats a sandwich.
 and input nodes involved in the parse.                               Capital letters indicate compressor nodes, italics indicate in-
                                                                      put nodes, numbers indicate slots, >=< indicates a binding.
                 HPN as a syntactic parser
 Parsing in HPN takes the form of an interaction between par-
 allel bottom-up activation of input and/or compressor nodes               Representation of context free grammars
 and serial top-down execution of productions, as ordered se-
                                                                      Context-free grammars (CFG) are a special case of an HPN
 quences of predictions. The parse starts with bottom-up ac-
                                                                      grammar with a single compressor layer, and they can be
 tivation of productions whose left slot matches the mean-
                                                                      represented as a set of input nodes and compressor nodes
 ing representation of an activated input node. Slots must be
                                                                      with appropriate meaning representations. We show this by
 matched in the correct order, and adjoining slots within a sin-
                                                                      sketching a conversion procedure from a CFG grammar to an
 gle production accept only adjoining portions of the sentence.
                                                                      HPN representation, such that those and only those sentences
 (This is why we need to keep track of the ‘time of activation’
                                                                      that are successfully parsed by the CFG grammar are success-
 in the slots). When all slots of a production are matched, the
                                                                      fully parsed by the HPN grammar. For convenience, we will
 compressor node fires, and transmits its internal representa-
                                                                      represent the nodes and slots as vectors with respect to a ba-
 tion, such that it in turn can be matched with a slot of another
                                                                      sis of slots. We can then compute the match between a firing
 production. Figure 2 shows some typical parsing scenario’s.
                                                                      node and a slot as the inner product between their represen-
                                                                      tations (if the inner product equals 0 then there is no match).
                                                                        S       →    NP VP (1.0)
                                                                        NP      →    PropN (0.2) k N (0.5) k N RC (0.3)
                                                                        VP      →    VI (0.4) k VT NP (0.6)
                                                                        RC      →    WHO NP VT (0.1) k WHO VP (0.9)
 Figure 2: Scenario’s of three HPN parses. i) ((a b c) d e),            VI      →    walks (0.5) k lives (0.5)
 ii) (a b (c d e)), iii) (a b ((c d e) f g)).                           VT      →    chases (0.8) k feeds (0.2)
                                                                        N       →    boy (0.6) k girl (0.4)
                                                                        PropN   →    John (0.5) k Mary (0.5)
    Although it is possible to implement a parallel version of          WHO     →    who (1.0)
 the HPN parser, we have implemented a serial version of
 HPN that roughly parallels left corner parsing, and uses back-       Table 2: Example probabilistic context-free grammar.
 tracking to search the space of possible parses. Left corner         Probabilities are indicated in brackets.
 parsing is a cognitively quite plausible incremental parsing
 strategy that combines top-down and bottom-up parsing. For
 details, see (Rosenkrantz & Lewis II, 1970).                        1. Create a separate HPN production for every non-
    A parse in HPN is a trajectory through the nodes of the              unary rule expansion, and assign unique and orthogo-
 physical network that binds a set of productions together               nal representations to its slots. (For example, S →
 through path connectors. A parse is called successful if                (1000000000) (0100000000)). The representation of the
                                                                         compressor nodes will be determined later.
1. every successive input word in the sentence is predicted by       2. For every non-unary production in the CFG, change the
    one of the productions involved, in the correct order.               representations of all non-terminals occurring on its right
                                                                  2976

    hand side by adding the slot vectors (as assigned in step 1)        Compressor nodes
                                                                        X2 (NP) (.3 0 0 0 0 .3 0 .3 0 0 0)
    with which they are associated (using vector addition);             X3 (VP) (0 .6 0 0 0 0 0 0 0 0 .6)
3. For every unary production in the CFG, copy or add the               X4 (RC) (0 0 0 .1 0 0 0 0 0 0 0)
    representation of the non-terminal on the left hand side to         X5 (RC) (0 0 0 .9 0 0 0 0 0 0 0)
                                                                        Input nodes
    the representation of the non-terminal or terminal on the           John = Mary = (.1 0 0 0 0 .1 0 .1 0 0 0)
    right hand side (do this recursively).                              lives = walks = (0 0 0 0 .5 0 0 0 .5 0 0)
4. Assign the appropriate non-terminal representation to the            boy = (.3 0 0 .6 0 .3 0 .3 0 0 0); girl = (.2 0 0 .4 0 .2 0 .2 0 0 0)
                                                                        chases = (0 0 0 0 .8 0 0 0 .8 0 0); feeds = (0 0 0 0 .2 0 0 0 .2 0 0)
    roots of HPN productions, and create input nodes using              who = (0 0 0 0 0 0 1 0 0 1 0)
    the representations computed in step 3. Discard all unary
    productions, and unused non-terminals.                                  Table 3: Node representations for probabilistic HPN.
 The conversion procedure is illustrated in Figure 3 (using in-
 formal notation, and with unary productions added for clar-
                                                                      egories from context is a discrete process: a familiar strategy
 ity), for the CFG grammar with recursive relative clauses
                                                                      is to merge two words into a single category when they ap-
 shown in Table 2.
                                                                      pear in similar contexts (Stolcke & Omohundro, 1994). One
                                                                      of the advantages of defining category membership by means
                                                                      of a topology is that it allows category meanings to change
                                                                      continuously while learning.
                                                                          In order to induce a meaningful topology, one must some-
                                                                      how induce the meaning representations from the distribution
                                                                      implicit in the corpus. HPN uses a similar strategy as above
                                                                      to identify meaningful relations between two words, but it
                                                                      makes the words more substitutable in a gradual way, rather
                                                                      than discretely, by decreasing their distance in substitution
                                                                      space upon encountering similar contexts.
                                                                          In HPN parsing and learning are complementary, as node
                                                                      representations are dynamically adapted after every parse.
                                                                      Following is a sketch of the algorithm:
 Figure 3: Conversion procedure from CFG to HPN. The slot
 indices reflect their only non-zero component.                      1. Initialization. Create input nodes with random represen-
                                                                          tations for every distinct word in the corpus. Specify the
    One can easily check that given these representations there           number of productions of each size (i.e., number of slots),
 is only one way for HPN to parse for example the sentence                and create a compressor node with random representation
 boy who lives chases Mary.                                               for each production. Initialize all slot representations or-
    It is easy to modify the conversion procedure such that it            thogonally to each other.
 converts a probabilistic context free grammar (PCFG) into a         2. Parsing. For every sentence, let HPN compute the most
 probabilistic HPN. To do so, during the construction of node             probable parse (as explained above). Recover the produc-
 representations (step 2 and 3) one must multiply the repre-              tions and bindings involved, using the path connectors.
 sentation of the left hand side by the probability of their re-     3. Learning. For every binding adjust the representation of
 spective expansion in the PCFG. The inner product between                ‘winning node’ n that participated in slot s according to
 a node and a slot representation now gives the probability of            ∆n = λ ∗ s, with (decreasing) learning rate λ. Also, ad-
 their binding, and the product of the probabilities of all bind-         just the representations of the nodes in the neigborhood h
 ings involved in an HPN derivation gives the HPN parse prob-             of the ‘winning nodes’, in proportion to their distance in
 ability. Table 3 gives the ‘probabilistic’ node representations          substitution space.
 (assuming the HPN productions of Figure 3). It can be shown
 that, using this conversion procedure, for any sentence gen-         As can be inferred from the learning step, the internal rep-
 erated by the example artificial grammar, every PCFG parse           resentations of words and syntactic categories are gradually
 has an HPN counterpart with the same phrase-structure and            changed from concrete (i.e., not correlated with other node
 the same probability.                                                representations) to abstract, when they participate in more
                                                                      slots. Two lexical nodes that often participate in the same
                            Learning                                  slot(s) will be gradually merged into a single part of speech
 While in the previous section we have shown that a syntax can        category. By shrinking the neighborhood, and decreasing
 be represented in HPN, the added value of the connectionist          λ with time, as in a Kohonen network, a topology over
 approach is in its ability to actually learn syntactic represen-     node representations is incrementally induced in substitution
 tations. Within the symbolic paradigm learning syntactic cat-        space, based on the corpus distribution.
                                                                  2977

                          Evaluation
The learning algorithm was evaluated on the artificial lan-
guage of Table 2. We generated at random 1000 distinct sen-
tences with until 5 levels of recursion from the CFG grammar,
and split these in a training corpus of 800 sentences (with
brackets included) and a test corpus of 200 sentences (with-
out brackets). We initialized the HPN network by creating 10
productions with 2 slots, and 5 productions with 3 slots, and
we set all compressor and input node representations to ran-
dom initial values. The learning rate decreased from λ = 0.3
to λ = 0.05 and the neighborhood from h = 10 to h = 0.01.
Figure 4 shows the representations of the input nodes after
learning has completed (scaled to two dimensions using Mat-
                                                                       Figure 5: Representations of Eve’s 100 most frequent words
lab’s cmdscale). Also shown are the average compressor
node representations that map to the symbolic labels of the
gold standard parses (NP, etc.). From the figure it can be seen       Model Merging (Stolcke & Omohundro, 1994; Borensztajn
                                                                      & Zuidema, 2007). HPN shares the division of work be-
                                                                      tween the syntagmatic and the paradigmatic components of
                                                                      the model (e.g., the chunk operator is replaced by compressor
                                                                      nodes). A drawback of all symbolic approaches to categoriza-
                                                                      tion, which is inherent to the discrete nature of the categories,
                                                                      is that they suffer from the lack of well-motivated criteria for
                                                                      preventing over-generalization, and for deciding on category
                                                                      boundaries. These problems are mitigated in HPN, with the
                                                                      assumption of a continuous substitution space.
                                                                         Most connectionist work on sentence processing has been
                                                                      carried out in the tradition of Elman’s Simple Recurrent Net-
                                                                      work (SRN) (Elman, 1991), using variations of fully dis-
 Figure 4: Substitution space with HPN node representations           tributed multi-layer perceptron networks and error backprop-
                                                                      agation. Such networks do not employ a notion of categories
that the nouns, proper nouns, intransitive and transitive verbs       at all, and consequently are not suited for capturing phrasal
cluster into postag categories, and some of the higher order          structure. Much attention has been paid to the abilities of
categories are mapped in substitution space between words             the SRN to deal with ‘quasi-recursion’ (Tabor, 2000; Chris-
with which they are intuitively substituted. To test whether          tiansen & Chater, 1999). Typically, a word prediction task is
the induced HPN grammar had indeed developed useful rep-              used, and a similar artificial grammar as ours. There is reason
resentations, we let it parse the test corpus with, and evaluated     to believe, that because of its architecture HPN will perform
precision (UP) and recall (UR) of the found constituents. The         well on this task with highly recursive sentences, but bench-
resulting values of UP=UR=0,864 indicate that the induced             mark tests still remain to be done.
HPN grammar approximates the original grammar to a fair                  HPN and SRN take a very different approach in their treat-
degree.                                                               ment of time: the SRN learns temporal sequences by feeding
   Unlike connectionist models that are trained with error            the activation of the hidden layer at time t-1 back to a so-
back-propagation, training with HPN is scalable to realistic          called context layer. As a consequence of the linear history,
size corpora, and can be done in a single pass through the            the internal (word) representations formed in the hidden layer
data. This created the opportunity to evaluate HPN on the Eve         of the SRN are approximately Markovian. In contrast HPN,
corpus from the CHILDES database (MacWhinney, 2000).                  inspired by (Hawkins & Blakeslee, 2004), uses hierarchical
Figure 5 shows very preliminary results on 2000 consecu-              temporal compression, and therefore maintains a non-linear
tive child utterances from the second half of the Eve corpus,         representation of a word’s history. HPN constructs its word
where we used the brackets available from the dependency              representions depending on an optimal phrasal decomposi-
annotation. The interesting clusters are accentuated. Note,           tion of the sentence: the context of a word depends on the
that for this experiment an earlier version of the algorithm          slot position that the word is bound to within a certain HPN
was used, that did not employ a neigborhood function.                 production. Moreover, unlike SRN’s, HPN maintains explicit
                                                                      representations of higher order syntactic constituents, which
          Relation to other modelling work                            in turn constitute part of the context for the terminals.
Unsupervised induction of syntactic labels has been done in              Yet another class of connectionist models of syntactic
the ‘symbolic’ paradigm, with best results through Bayesian           processing emulates symbolic parser behavior by proposing
                                                                  2978

mechanisms for translating the concept of compositionality           syntactic categories can be bootstrapped from scratch.
into a connectionist setting, such that compositional struc-            Empirical research in child language acquisition seems
tures can be represented. Examples of these are Harmony              largely to support such an approach to learning.                        In
Theory (HT) (Prince & Smolensky, 1997), and derivatives of           Tomasello’s theory of Usage Based Grammar (UBG)
RAAM (Pollack, 1990), such as SPEC (Miikkulainen, 1996).             (Tomasello, 2003) the gradual acquisition of syntactic cat-
In our opinion, in brief, the problem with the proposed archi-       egories is referred to as item-based learning: in the early
tectures is that, in contrast to HPN, they are not capable of au-    developmental stages linguistic constructions are typically
tonomously finding structural (tree) representations, without        learned case-by-case, often around specific verbs, without
relying on external or innate knowledge of the node mean-            reference to a general syntactic category. Only gradually chil-
ings. HPN, rather than constructing compositional represen-          dren learn to generalize across constructions, when they ac-
tations, deconstructs sentences into tree-like representations       quire an adult-like grammar that uses system-wide syntac-
because it is confined to map their interpretations onto the hi-     tic categories. HPN seems to offer a promising direction for
erarchical and topological structure of the network. As such,        modeling grammar acquisition in this tradition.
HPN offers a biologically motivated explanation for the ori-
gin of compositionality, that suggests that the superficially                               Acknowledgements
compositional structure of language (as well as vision) arises       We want to thank Stefan Frank and Remko Scha for valuable
from the hierarchical processing that takes place in the brain.      discussions and comments.
                                                                                                     References
               Discussion and Conclusions                             Borensztajn, G., & Zuidema, W. (2007). Bayesian model merging for
                                                                            unsupervised constituent labeling and grammar induction (Tech.
We have presented a novel connectionist network, the Hier-                  Rep.). (ILLC Technical Report PP-2007-40)
archical Prediction Network. A key innovation of HPN is               Christiansen, M. H., & Chater, N. (1999). Toward a connectionist model
                                                                            of recursion in human linguistic performance. Cognitive Science,
its ability to temporarily bind the nodes in the network (by
                                                                            23(2), 157-205.
means of pointers), and thereby provide a neural substrate            Earley, J. (1970). An efficient context-free parsing algorithm. In Commu-
of substitution. This is a significant achievement, because                 nications of the acm (Vol. 13, p. 94-102).
it equips HPN with a mechanism for some kind of ‘vari-                Elman, J. L. (1991). Distributed representations, simple recurrent net-
able’ manipulation, and with the ability to represent system-               works, and grammatical structure. Machine Learning, 7, 195-225.
                                                                      Fodor, J. D., & Pylyshyn, Z. W. (1988). Connectionism and cognitive
atic relations over ‘variables’ (i.e., HPN exhibits systematic-             architecture: A critical analysis. Cognition, 3-71.
ity). The lack of this ability has been a major source of cri-        Hauser, M. D., Chomsky, N., & Fitch, W. T. (2002). The faculty of lan-
tique against the suitability of classical connectionist models             guage: what is it, who has it, and how did it evolve? Science, 298,
to deal with language processing (Fodor & Pylyshyn, 1988).                  1569-1579.
The fact that the nodes of the network can engage in meaning-         Hawkins, J., & Blakeslee, S. (2004). On intelligence. New York: Henry
                                                                            Holt and Company.
ful relations makes HPN much more powerful than conven-               Hubel, D. H., & Wiesel, T. N. (1968). Receptive fields and functional
tional neural networks, since it transforms HPN into a struc-               architecture of monkey visual cortex. Journal of Physiology, 215-
tured knowledge base (i.e., a grammar) rather than a loose                  243.
collection of nodes (e.g., see (Marcus, 2001)).                       Kohonen, T. (1995). Self-organizing maps. Berlin: Springer Verlag.
                                                                      Lakoff, G. (1987). Women, fire, and dangerous things: What categories
   By virtue of a mechanistic explanation for substitution,                 reveal about the mind. Chicago, IL: University of Chicago Press.
HPN creates a synthesis between the traditional connection-           MacWhinney, B. (2000). The childes project: Tools for analyzing talk.
ist and symbolist frameworks. Yet, even though HPN bor-                     third edition. Mahway, NJ: Lawrence Erlbaum Associates.
rows some notions from the parsing field, its implementation          Marcus, G. F.(2001). The algebraic mind: Integrating connectionism and
is fully connectionist: HPN’s internal representations are in-              cognitive science. Cambridge, MA: MIT Press.
                                                                      Miikkulainen, R. (1996). Subsymbolic case-role analysis of sentences
duced from arbitrary initial representations, and all interac-              with embedded clauses. Cognitive Science, 20, 47-73.
tions are local. We believe that the main contribution of our         Pollack, J. B. (1990). Recursive distributed representations. Artificial
work is therefore that it formulates sufficient conditions for              Intelligence, 46, 77-105.
a connectionist solution of the structure encoding problem.           Prince, A., & Smolensky, P. (1997). Optimality: from neural networks to
                                                                            universal grammar. Science, 275(5306), 1604-10.
Major departures from the symbolic paradigm are that the
                                                                      Rosenkrantz, D., & Lewis II, P. (1970). Deterministic left corner parsing.
classical notion of a category is replaced by that of a graded              In 11th annual symposium on switching and automata theory (p.
topology, within which neighboring nodes are substitutable,                 139-152). New York: IEEE Press.
and that ‘rules’ (the HPN productions) have local rather than         Stolcke, A., & Omohundro, S. M. (1994). Inducing probabilistic gram-
global scope.                                                               mars by bayesian model merging. In Proceedings of the second in-
                                                                            ternational colloquium on grammatical inference and applications
   HPN simulates the fact that categories emerge gradually,                 (icgi’94) (Vol. 862, pp. 106–118). Berlin: Springer Verlag.
and their meaning grows incrementally with time from con-             Tabor, W. (2000). Fractal encoding of context free grammars in connec-
crete to abstract, as the nodes associated with the category in-            tionist networks. Expert Systems, 17(1), 41-56.
tegrate in the network topology. Such a strategy makes HPN            Tomasello, M.(2003). Constructing a language: A usage-based theory of
                                                                            language acquisition. Cambridge, MA: Harvard University Press.
very well suited as a model for language acquisition, because
                                                                      Ullman, S. (2007). Object recognition and segmentation by a fragment-
it offers a plausible explanation for the tricky question of how            based hierarchy. Trends in Cognitive Science, 11(2), 58-64.
                                                                 2979

