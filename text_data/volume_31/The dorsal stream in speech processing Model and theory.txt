UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The dorsal stream in speech processing: Model and theory
Permalink
https://escholarship.org/uc/item/2jb587sf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Keidel, James L.
Lambon Ralph, Matthew A.
Welbourne, Stephen
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                      The dorsal stream in speech processing: Model and theory
                       James L. Keidel, Stephen R. Welbourne and Matthew A. Lambon Ralph
                                    School of Psychological Sciences, University of Manchester
                                               Manchester, M13 9PL United Kingdom
                            Abstract                                  Whether or not speech perception is mediated by neural
   The ability to produce and comprehend spoken language
                                                                   representations of intended gestures, the many findings
   requires an internal understanding of the complex relations     inspired by MT provide important insight into the nature of
   between articulatory gestures and their acoustic                phonetic categorization. While there exist myriad cues to
   consequences. Recent theories of speech processing propose a    phonetic identity, these cues necessarily covary due to the
   division between the ventral stream, which involves the         constraints imposed on the signal by the physics of the
   mapping of acoustic signals to lexical/semantic                 articulatory apparatus. Thus while a low F1 onset frequency
   representations, and the dorsal stream, which mediates the      and a lack of F1 cutback are signals to voicing in word-
   mapping between incoming auditory signals and articulatory
   output. We present a connectionist model of the dorsal stream   initial stops, these two features tend to trade off as the
   of speech processing that utilizes a novel schematic            supraglottal articulations in voiced and voiceless stops are
   representation of time-varying acoustics and a featural         highly similar. When voicing commences at the release of
   mapping of articulation. The model successfully learns a        the stop closure, F1 will be low because, as an index of jaw
   large training vocabulary, accurately produces novel items      position, it will reflect the fact that it is still relatively
   and demonstrates patterns of perceptual errors highly similar   closed. Additionally, due to the presence of energy (from
   to those observed in human subjects.                            the voicing source) in the vicinity of the first formant, F1
   Keywords: speech perception; speech production; dorsal          will be audible and thus there will be little to no F1 cutback.
   stream; neural networks                                            In spite of this extensive variability, the speech perception
                                                                   system displays an astonishing ability to recover the
                         Introduction                              speaker's intended phonetic message and thereby her
In few instances do our intuitions more thoroughly deceive         meaning. Many studies have demonstrated just how difficult
us than in the 'common sense' picture of speech perception.        it is to render a speech signal unintelligible. Remez, Rubin,
Phenomenologically, the experience of hearing speech is            Pisoni and Carrell (1981) showed that participants could
similar to that of reading text; empty gaps set off each word,     understand speech signals composed only of sine waves
and these words consist of individual stretches of sound           tracking the movement of the first three formants of a
corresponding more or less perfectly to the written character      sentence. Listeners in Shannon et al.â€™s (1995) study quickly
s. In reality, things are not nearly so clean-cut; the silent      learned to transcribe signals composed of four bands of
stretches that do appear in the signal (for instance, during       white noise modulated by amplitude within each filtered
voiceless stop closures) do not typically correspond to word       band, a manipulation which all but removes temporal cues
boundaries, and one would be hard pressed to point to the          to phonetic identity. At the other end of the spectrum, Saberi
instant in the word 'deed' where the signal switches from /d/      and Perrott (1999) split speech waveforms into 50-100 ms
to /i/. To the extent that phonetic segments do exist, they are    chunks which were then each reversed in time; again,
highly blurred together by the effects of the preceding and        participants quickly adapted to this manipulation and could
following articulatory gestures. In some cases the same            reproduce the distorted sentences.
stretch of sound induces different percepts in different              Given the complexity of the stimulus, then, we should not
contexts; thus the same burst of noise will produce a /p/          be surprised that the corresponding neural activation is so
percept before a back vowel but a /k/ percept preceding a          extensive and difficult to pin down. The perception of a
front vowel. In addition, each segment can be signaled by a        single word activates an extensive bilateral network
constellation of cues, none of which is guaranteed to be           centered around the primary auditory cortices and extending
present in a given utterance.                                      both anteriorly and posteriorly along the superior temporal
  This apparent lack of acoustic invariants for phonetic           gyri (Binder et al., 2000). Similarly, speech production
identification led Liberman and his colleagues to propose          activates a left-lateralized network consisting of IFG
the Motor Theory (MT) of speech perception, which holds            (Broca's area), insula, and primary and supplementary motor
that the true objects of speech perception are not acoustic        cortices (Hickok & Poeppel, 2007). Unsurprisingly, there is
but instead articulatory. Under this view, listeners employ a      also a significant amount of overlap between the two
potentially innate understanding of vocal tract physics to         systems, both in terms of the underlying representations that
recover the speaker's intended gestures from the acoustic          mediate the sound to articulation mapping, as well as the
waveform. The lack of invariance at the acoustic level is          perceptuo-motor systems that provide online monitoring of
thereby resolved through reference to the putatively               one's own utterances and the ability to shadow heard speech
invariant underlying articulations.                                at extremely short latencies.
                                                                 1024

  While most researchers would accept primary auditory           way as to make them believe they had produced an incorrect
cortex as an appropriate starting point for analysis of the      vowel when reading single words aloud (e.g., when a
speech processing system, the consensus appears to end           participant produced 'head' she heard herself saying 'heed').
there as well. The advent of functional neuroimaging has         Participants in this experiment quickly adjusted their
rendered even the most general questions concerning speech       articulations to reflect the altered feedback concerning
processing open for debate. On the basis of lesion data, the     vowel height; a function which Hickok and Poeppel ascribe
classical Wernicke-Geschwind model posited a few very            to the dorsal stream.
basic tenets about the organization of language in the brain:      While the neuroimaging findings discussed above have
1) Language in right-handed individuals is generally left-       certainly increased our understanding of the anatomical
lateralized; 2) The posterior portion of the left superior       substrates of speech processing, there remain a number of
temporal lobe ('Wernicke's area') is primarily responsible for   open questions concerning the nature of the underlying
language comprehension; 3) The left IFG ('Broca's area') is      conceptual representations. That is, while we may agree that
primarily responsible for guiding language production            area Spt is involved in the mapping between representations
(Geschwind, 1970). All of these statements have been             of sensory data (e.g., heard speech) and those underlying
subject to some reevaluation; we focus here on the               production (e.g., articulatory sequences), we would also like
sensorimotor transformations required to compute the             to know what the representations in Spt actually look like.
mapping between audition and articulation. As such, we           What stimuli are considered similar/equivalent in this area?
will first examine receptive processing of the speech signal,    Are there nonlinearities in the encoding of stimuli along
and then integrate this discussion into an understanding of      perceptual or articulatory dimensions known to be relevant
the mechanism of speech production. In this discussion we        in speech processing? While imaging studies certainly can
will follow the terminology of Hickok and Poeppel (2007),        contribute to this goal (perhaps especially through the use of
who pose a distinction between speech perception and             fMRI adaptation paradigms), it is likely that work in other
speech comprehension. Speech comprehension involves              methodologies will provide key insights as well.
recovering the speaker's intended message from the acoustic        Parallel Distributed Processing (PDP) provides an ideal
signal, and more or less aligns with our everyday use of         framework for the exploration of the internal representations
speech. Speech perception, on the other hand, refers             that guide behavior. This is perhaps especially true in
primarily to the sorts of behaviors beloved by speech            speech perception, as the signal consists of multiple
researchers, such as identification and discrimination of        interacting probabilistic cues that likely require highly
speech sounds and other explicitly phonological or phonetic      nonlinear weightings for correct stimulus classification. An
tasks. When no distinction need be made; viz., when a            important early precursor to the work presented here is the
statement is true of both perception and comprehension, we       TRACE model of speech perception (McClelland & Elman,
refer to both under the aegis of speech processing.              1986), which was designed to account for a number of key
  Hickok and Poeppel (2007) propose a dual-stream model          phenomena in speech perception and lexical access. One of
of speech processing, in which a ventral stream projecting       the key insights of this model was the highly interactive
bilaterally from A1 to posterior STG and MTG mediates the        nature of speech processing: processing of the early part of
mapping between signal and lexicon, while a left-lateralized     the signal strongly constrains the interpretation of the latter
dorsal stream involving the left temporo-parietal junction       part. However, there exist key differences between TRACE
(area Spt) and IFG links speech sounds to articulations. To a    and the model presented here: while the input to TRACE
first approximation, then, this model fits well with the         was an acoustic featural description of the signal, in our
classical picture. However, on the basis of a number of          model we use schematic spectrograms derived from actual
neuroimaging studies, these authors propose some important       recordings of English. In addition, the weights in TRACE
elaborations to the Wernicke-Geschwind model. Key among          were set by hand, while in our simulations the weights were
these is the proposal that area Spt plays a central role in      adjusted as a function of the mismatch between the target
computing the mapping between incoming auditory                  and actual output.
information and articulatory gestures.                             A closer precedent to our simulations can be found in the
  That there might be some region of cortex keyed into the       work of Kello and Plaut, who explored phonological
relation between acoustics and articulation seems quite          development in the context of neural network models. Plaut
likely; Hickok and Poeppel offer two strong motivations for      and Kello (1999) trained a multi-layer PDP network on the
the necessity of such an area. First, when a child is learning   mappings between a schematic, feature-based acoustic input
to speak, the disparity between the intended and actual          and both articulation and semantics, demonstrating the
output provides the learning signal that drives organization     feasibility of the approach presented here. In addition,
of the articulatory output system. The importance of this        Kello and Plaut (2004) focused specifically on the forward
function is not limited to development, however; the adult       mapping from articulation to acoustics, by training a
speaker must also monitor her output for errors. The quick       network on the mapping between actual articulatory
and efficient operation of this system is perhaps best           recordings (EMA, laryngography, electropalatography) and
exemplified by the work of Houde and Jordan (1998), who          their associated acoustic output. A key insight of both these
manipulated the acoustic feedback to participants in such a      papers is the idea that the representations that underlie
                                                               1025

speech processing are best viewed as neither purely acoustic      closure into the vowel, and 3) the steady state vowel itself.
nor purely articulatory in nature, but rather are shaped by       Values for the bursts followed the characterization found in
the covariance structure of the articulation-audition             the work of Blumstein and Stevens (1979) and Repp and
interface.                                                        Lin (1989). Schematically, labial bursts were flat and
                                                                  diffuse, alveolar bursts were diffuse and rising, and velar
                         Methods                                  bursts were compact. To prevent the model from tracking
Model Architecture                                                idiosyncratic features of production by our single speaker,
                                                                  the F1 transition was always calculated as the trajectory
The simulations described in this paper employed a 4-layer        from a start value of 200 Hz to the steady-state F1 of the
PDP network trained with continuous recurrent                     following vowel. The onset frequencies for F2 and F3
backpropagation (Pearlmutter, 1995). The input layer              transitions were measured for each vowel context, and these
contained 46 units, divided into two separate filter banks        served as the basis for the model's input representations.
and two task units. The first 22 units represented the            Values for the first three formants of each vowel were
presence of acoustic energy in 1 Bark bands, corresponding        calculated as the averages of F1, F2 and F3 across the
to the region of the spectrum from 0-8 kHz, spaced                recorded consonant contexts.
according to auditory acuity as a function of frequency. The        Fricatives were represented as a steady-state frication
second bank of 22 units had the same frequency spacing, but       period followed by formant transitions into the syllable
their activation corresponded to the presence or absence of       nucleus. Values for the frication spectra were taken from
periodicity in each frequency range. The task units               spectrographic measurements of the model speaker. The
indicated the delay at which the model was required to            formant transition values from the stop measurements were
produce the response; these units were employed in the            also used in some fricative contexts; thus the alveolar and
delayed repetition test described below. The input layer          post-alveolar fricatives received the transitions from the
projected to a hidden layer of 150 units, which itself was        alveolar stops and the labiodental fricatives received the
recurrently connected to a second hidden layer containing         transitions of bilabial stops. For the interdental fricatives,
150 units as well. This second hidden layer projected             transition values from the model speaker were measured and
recurrently to the output layer. All simulations used a           used to create the stimuli. Nasals were modeled by an
learning rate of 0.01, and a Euclidean distance metric was        initial murmur followed by transitions appropriate for the
used for scoring model performance.                               POA taken from the stop productions described above.
  The output layer contained 21 units, which represented a        Generally, coda consonants were represented as the time-
schematic articulatory mapping based on that reported in          reversed versions of their onset counterparts. However, in
Keidel, Zevin, Kluender and Seidenberg (2003). Each word          the case of voiceless stops, the distinction was produced by
was coded as a series of articulatory targets, in line with the   shortening the vocalic portion of the word, as this is the
work of Browman and Goldstein (1992). Individual bits in          predominant cue to coda stop voicing in English.
each segment vector corresponded to constructs such as              The measured values described above were then converted
place of articulation (POA), constriction degree, tongue          into time-varying acoustic input vectors to the model
tip/body position and velar lowering.                             (hereafter referred to as the 'acoustic matrix', reflecting the
                                                                  time x frequency nature of the stimuli). The matrix for each
Stimulus Design The input to the model consisted of               CVC word was 36 x 44, with 36 20 ms time steps and 44
schematic acoustic representations of CVC stimuli. To             frequency coefficients (22 for energy in a filter and 22 for
create these representations, we first recorded a native          presence or absence of periodicity). Stimuli were first
Southern British English speaker (SRW) producing tokens           vowel-centered, so that similar formant transitions (e.g.,
of 16 English consonants (six stops, eight fricatives, and        those in /d/ and /z/) would overlap in time. Next, measured
two nasals) in onset position before 11 different vowels.         acoustic values were inserted into the proper filters,
These recordings were then analyzed to determine values           according to a linearly weighted split of energy between the
for key acoustic parameters known to affect phonetic              units representing the closest Bark values on either side of
identification, such as burst spectrum, direction and             the given frequency. Formant transitions were created by
magnitude of formant transitions, vowel formant values, and       linear interpolation between onset frequencies for transitions
others described below. These values were then employed           and steady-state values for the relevant vowel over a 60 ms
to create schematic time-varying acoustic input for the           time window, equivalent to 3 events along the time axis of
network to classify.                                              the acoustic matrix.
  The perception of stop consonants is perhaps the best             To simulate effects of speaker variability, 15 versions of
studied field in speech research, as they embody the              each token were created by adding noise to the vowel
interaction of multiple probabilistic cues perhaps better than    formants, then calculating transitions with respect to the
any other type of segment. In our stimuli, voiced stops in        new values (e.g., /da/ was specified with a 400 Hz falling F2
onset position were represented as three separate events: 1)      transition, so regardless of the formant values chosen in the
a burst portion corresponding to the release of pressure built    noise procedure, F2 would fall 400 Hz into the steady state
up behind the constriction; 2) formant transitions resulting
from the movement of the primary articulator from the
                                                                1026

F2 value). Additionally, Gaussian noise with an SD of 0.1             hidden unit representations generated by three consecutive
was added to the activation of the input units.                       stimuli in the series.
   In order to validate the similarity of our acoustic
representations to the actual speech signal, we trained the                   1
network on all possible combinations of the 16 consonants                   0.9
in both onset and coda position with the 11 vowels in the                   0.8
syllable nucleus (a total of 2816 stimuli; 10% of these                     0.7
stimuli were withheld to test generalization performance).                  0.6
                                                                                                                                                                ID
At the conclusion of training, we tested the model on the                   0.5
                                                                                                                                                                Discrim
trained stimuli in varying levels of noise, for comparison                  0.4
with the perceptual confusion data presented in Miller and                  0.3
Nicely (1955).                                                              0.2
   In this classic paper, the authors present identification data           0.1
from four human listeners labeling thousands of syllables of                  0
the form /Ca/, where C ranged over the six stops, eight                                                    1   2   3      4   5   6        7   8   9 10
fricatives, and two onset nasals (/m/ and /n/) of the English                                                                 Step
sound system. The experiment was carried out under a
number of different noise levels and bandpass filter widths,            Figure 1. Identification and discrimination of a /ba/-/da/
illustrating the gradual breakdown of the boundaries                   series. Y-axis represents percent /ba/ identification for ID
between phonetic categories as listening conditions                      curve and correct discrimination of tokens (see text).
deteriorate.      The      comparison      of    the      model's
misidentifications of words in noise with Miller and Nicely's           Figure 2 shows the correlation between the errors of
(MN) data allows for an independent validation of the match           human listeners perceiving speech at a signal-to-noise ratio
between our acoustic representations and the statistical              (SNR) of -6 dB, and the average of 10 models'
structure of the actual speech signal. That is, while any             identifications of the training stimuli presented in Gaussian
mapping is in principle learnable by a PDP network, it is by          input noise with an SD of 0.35. This value was chosen to
no means given that the breakdown in function induced by              produce the same proportion of errors as the MN subjects
the addition of noise will follow that exhibited by human             independent of the errors' distribution. Because most of the
listeners.                                                            cells of the confusion matrix are empty we only entered
                                                                      cells with error rates greater than .05 for one of the groups.
                           Results                                    The results of the analysis demonstrate a good fit to the
All results presented represent the average of 10 runs of the         human data: r(27) = .54, p < .05. Similar results were found
model. After 1.5M trials the models reached asymptote,                for the other noise levels tested by Miller and Nicely; in all
identifying an average of 99% of the input patterns correctly         cases the correlations between human and model data were
based on a Euclidean distance criterion. Generalization               significant.
performance was somewhat poorer, with an average of 93%                                              0.3
of items named correctly.
                                                                           MN errors (proportion)
                                                                                                    0.25
  Clearly, any model of speech perception must account for
the foundational finding of MT: categorical perception. In                                           0.2
the terminology of Liberman, Harris, Hoffman, & Griffith
(1957), categorical perception occurs when identification                                           0.15
predicts discrimination: if two stimuli from the same
                                                                                                     0.1
speaker are both identified as /ba/ then the listener will not
be able to tell them apart. However, if the same amount of                                          0.05
acoustic difference straddles a category boundary (such as
                                                                                                      0
that between /ba/ and /da/), then the two stimuli will be                                                  0           0.05          0.1           0.15   0.2        0.25
discriminable. To simulate this in the model, we                                                                              Model errors (proportion)
interpolated the initial formant values used to create /ba/ and
/da/ stimuli and generated a 10-step series between these             Figure 2. Correlation of model identification performance in
stimuli. Importantly, the model had only been exposed to                noise and human participant data from Miller and Nicely
the endpoint stimuli in training; thus, the identification of
the intermediate stimuli represents true generalization.               As a further test of the match between human and model
Figure 1 shows the results of this test: as in human subjects,        behavior, we introduced high levels of noise to the second
discriminability peaks sharply at the category boundary, and          hidden layer, and tested the modelâ€™s ability to repeat words
intermediate stimuli are identified as the closest endpoint in        (i.e., items from the modelâ€™s training set) and nonwords
auditory space. Discrimination performance was calculated             (items from the generalization set). Performance on this task
as the normalization of the Euclidean distance between                was evaluated in the same manner as above, viz. Euclidean
                                                                    1027

distance. Jefferies, Crisp and Lambon Ralph (2006)                                      Dorsal stream speech processing is a highly complex
demonstrated that patients with phonological impairments                              behavior, requiring analysis of information at multiple
following cerebrovascular accident showed an interaction                              levels of specificity. At the acoustic level, listeners are
between lexicality and delay, such that nonword repetition                            exquisitely sensitive to small variations in the signal (e.g.,
was significantly more impaired than word repetition at                               VOT changes on the order of 10s of milliseconds), yet are
longer delays. As can be seen in Figure 3, the model                                  able to decode highly degraded signals in which most or all
demonstrates a very similar lexicality effect as unit noise                           of these fine-grained cues have been destroyed. At the same
increases, with lexical items much more resilient to                                  time, speech perception is also strongly shaped by higher-
increasing noise.                                                                     level linguistic influences. While the many cues to phonetic
                                                                                      identity are largely independent of meaning, listeners' online
                        1
                                                                                      processing of the signal is demonstrably affected by their
                                                                                      knowledge of the lexicon.
                       0.9
                                                                                        Further, the speech processing apparatus must fulfill two
                       0.8
                                                                                      main functions. On the one hand, we listen to understand,
                       0.7                                                            and thus speech perception must provide a mapping from
  Proportion correct
                       0.6                                      words-no delay        the acoustic signal to semantic representationsâ€”the
                       0.5
                                                                words-delay           function of the ventral stream in Hickok and Poeppelâ€™s
                                                                nonwords-no delay     model. However, both during linguistic development and in
                       0.4                                      nonwords-delay
                                                                                      the adult state, we must also process speech signals in such
                       0.3
                                                                                      a way as to be able to produce articulations similar to those
                       0.2                                                            which gave rise to the input. An important question, then, is
                       0.1                                                            at what point in the processing pathway do the cortical
                        0                                                             representations diverge?
                             0   0.2    0.4    0.6    0.8   1                           Unsurprisingly, the answers that have been offered to this
                                  SD of noise in hidden2                              question reflect certain theoretical commitments that lead
                                                                                      different researchers to different conclusions. For instance,
Figure 3. Model performance as a function of lexicality and
                                                                                      proponents of MT and direct realism propose that this is
                      naming delay.
                                                                                      simply a distinction without a difference, as the key tenet of
                                                                                      MT is that it is recovery of the underlying articulations that
                                              Discussion                              allows lexical access. In this sense, then, we get the sound
   The results from identification in noise demonstrate that                          to meaning mapping 'for free': since the brain has inbuilt
our acoustic representations accurately reflect the structure                         structure that allows us to know what the speaker intended
of the speech signal: error patterns in the simulations closely                       to do with their vocal tract, the acquisition of a lexicon is to
matched those observed in human listeners. At a general                               a first approximation simply a matter of rote memorization.
level, place errors were much more common than errors in                                While MT and its descendants possess a great deal of
voicing, and perception of nasal consonants was very robust                           intuitive appeal, they face a number of issues at both the
even at high noise levels. At a more fine-grained level of                            implementational and theoretical levels. For any given
detail, the models captured the high degree of confusability                          speech waveform, there are an infinite number of vocal tract
for interdental and labiodental fricatives, both voiced and                           configurations that could have given rise to the signalâ€”a
voiceless. Additionally, the model captured an important                              relation known as the 'inverse problem'. The work presented
dissociation in misidentification of stop consonants in noise.                        here does not directly address this issue, as the
Specifically, for voiced stops it is /d/ and /g/ which are most                       preprocessing of the acoustic signal and the provision of
likely to be confused, since listeners rely on the direction of                       veridical articulatory targets renders the mapping learnable.
F2 and F3 transitions, which are rather similar for these two                         Nonetheless, the work spawned by MT has greatly increased
segments. On this basis, then, one might expect a similar                             our understanding of the speech processing apparatus, and
pattern for the voiceless stops, with a high degree of                                the model presented here represents an attempt to reify
confusion between /t/ and /k/. However, it is actually /p/                            many of the insight that this work has provided.
and /k/ that are more confusable in this case, since the                                 While we believe that this model represents an important
formants in voiceless stops are excited by low-energy                                 first step toward a mechanistic implementation of recent
aspiration noise and thus listeners appear to focus more on                           theory in the study of speech processing, there obviously
the release burst in identifying these sounds. In the case of                         remain a number of issues to address. Chief among these is
/t/, the release burst is a very strong cue to a coronal POA,                         the addition of lexical/semantic knowledge, which would
as it is high-energy and high-frequency. For the other two                            permit the exploration of the many interactions between
stops, however, the bursts are not quite as robust and thus it                        bottom-up and top-down interaction in speech perception.
is the labial and the velar that are more commonly mistaken                           Further, while the added noise to the input pattern prevents
for one another when perceived in noise.                                              the model from learning entirely speaker-specific
                                                                                      information, the results from this word do not directly
                                                                                    1028

address the rather vexatious question of speaker variability.               trained on recordings of articulatory parameters.
In this, however, we are not alone; even the most advanced                  Journal of the Acoustical Society of America,
commercial automatic speech recognition systems have not                    116(4), 2354-2364.
yet achieved the ideal of large-vocabulary speaker-                Liberman, A. M., Harris, K. S., Hoffman, H. S., & Griffith,
independent identification.                                                 B. C. (1957). The discrimination of speech sounds
   In future work, we intend to employ the multi-layer                      within and across phoneme boundaries. Journal of
architecture implemented here in order to investigate the                   Experimental Psychology, 54(5), 358-368.
internal representations that arise in layers whose input and      McClelland, J. L., & Elman, J. L. (1986). The TRACE
output is impacted more by perception or production. For                    model of speech perception. Cognit Psychol, 18(1),
instance, the first hidden layer in our model receives direct               1-86.
input from the acoustic layer, while the second hidden layer       Miller, G. A., & Nicely, P. E. (1955). An analysis of
receives a transformed version of this input which it must                  perceptual confusions among some English
use to drive the articulatory output. It is likely that the                 consonants. Journal of the Acoustical Society of
internal representations within these layers (investigated                  America, 27, 338-352.
with multivariate tools such as multidimensional scaling)          Pearlmutter, B. A. (1995). Gradient calculation for dynamic
reflect the different processing demands associated with                    recurrent neural networks: a survey. IEEE
these tasks.                                                                Transactions on Neural Networks, 6(5), 1212-
                                                                            1228.
                    Acknowledgements                               Plaut, D. C., & Kello, C. T. (1999). The emergence of
This work was supported by a grant from the Gatsby                          phonology from the interplay of speech
Foundation (GAT2831).                                                       comprehension and production: A distributed
                                                                            connectionist approach. In B. MacWhinney (Ed.),
                                                                            The Emergence of Language. Mahwah, NJ:
                         References                                         Erlbaum.
                                                                   Remez, R. E., Rubin, P. E., Pisoni, D. B., & Carrell, T. D.
Binder, J. R., Frost, J. A., Hammeke, T. A., Bellgowan, P.                  (1981). Speech-Perception without Traditional
          S. F., Springer, J. A., Kaufman, J. N., et al. (2000).            Speech Cues. Science, 212(4497), 947-950.
          Human temporal lobe activation by speech and             Repp, B. H., & Lin, H. B. (1989). Acoustic Properties and
          nonspeech sounds. Cerebral Cortex, 10(5), 512-                    Perception of Stop Consonant Release Transients.
          528.                                                              Journal of the Acoustical Society of America,
Blumstein, S. E., & Stevens, K. N. (1979). Acoustic                         85(1), 379-396.
          Invariance in Speech Production - Evidence from          Saberi, K., & Perrott, D. R. (1999). Cognitive restoration of
          Measurements of the Spectral Characteristics of                   reversed speech. Nature, 398(6730), 760.
          Stop Consonants. Journal of the Acoustical Society       Shannon, R. V., Zeng, F. G., Kamath, V., Wygonski, J., &
          of America, 66(4), 1001-1017.                                     Ekelid, M. (1995). Speech recognition with
Browman, C. P., & Goldstein, L. (1992). Articulatory                        primarily temporal cues. Science, 270(5234), 303-
          Phonology - an Overview. Phonetica, 49(3-4), 155-                 304.
          180.
Geschwind, N. (1970). Organization of Language and Brain.
          Science, 170(3961), 940-&.
Hickok, G., & Poeppel, D. (2007). The cortical organization
          of speech processing. Nat Rev Neurosci, 8(5), 393-
          402.
Houde, J. F., & Jordan, M. I. (1998). Sensorimotor
          adaptation in speech production. Science,
          279(5354), 1213-1216.
Jefferies, E., Crisp, J., & Ralph, M. A. L. (2006). The
          impact of phonological or semantic impairment on
          delayed auditory repetition: Evidence from stroke
          aphasia and semantic dementia. Aphasiology, 20(9-
          11), 963-992.
Keidel, J. L., Zevin, J. D., Kluender, K. R., & Seidenberg,
          M. S. (2003). Modeling the role of native language
          knowledge in perceiving nonnative speech
          contrasts. Proceedings of the 15th International
          Congress of Phonetic Sciences, 2221-2224.
Kello, C. T., & Plaut, D. C. (2004). A neural network model
          of the articulatory-acoustic forward mapping
                                                                 1029

