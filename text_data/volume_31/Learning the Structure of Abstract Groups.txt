UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning the Structure of Abstract Groups
Permalink
https://escholarship.org/uc/item/0rb9m2jt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Schlimm, Dirk
Shultz, Thomas
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    Learning the Structure of Abstract Groups
                                             Dirk Schlimm (dirk.schlimm@mcgill.ca)
                             Department of Philosophy and School of Computer Science, McGill University
                                         855 Sherbrooke St. W., Montreal, QC H3A 2T7, Canada
                                           Thomas R. Shultz (thomas.shultz@mcgill.ca)
                            Department of Psychology and School of Computer Science, McGill University
                                         1205 Penfield Avenue, Montreal, QC H3A 1B1, Canada
                               Abstract                                   Artificial neural networks most always learn from scratch,
   It has recently been shown that neural networks can learn
                                                                       unlike people who almost always try to build new learning
   particular mathematical groups, for example, the Klein 4-           on top of their existing knowledge. Our results show how
   group (Jamrozik & Shultz, 2007). However, there are groups          neural networks can also learn from existing knowledge. In
   with any number of elements, all of which are said to               particular, we are curious to know whether it helps the
   instantiate the abstract group structure. Learning to               learning process to have previously learned the structure of
   differentiate groups from other structures that are not groups      smaller groups. This is especially interesting, because
   is a very difficult task. Contrary to some views, we show that      similarities between two different groups and differences
   neural networks can learn to recognize finite groups
   consisting of up to 4 elements. We present this problem as a        between groups and other domains cannot in general be
   case study that exhibits the advantages of knowledge-based          characterized by structure-preserving mappings, but only by
   learning over knowledge-free learning. In addition, we also         formulating their underlying laws (Schlimm, 2008).
   show the surprising result that the way in which the KBCC              Learning of group structure based on previous knowledge
   algorithm recruits previous knowledge reflects some deep            of smaller groups is a natural area of application for
   structural properties of the patterns that are learned, namely,     constructive algorithms that build their learning on top of
   the structure of the subgroups of a given group.
                                                                       existing knowledge. One such algorithm is knowledge-
                                                                       based cascade-correlation (KBCC), which constructs a
   Keywords: Mathematical groups; neural networks; KBCC;               neural-network topology by recruiting previously learned
   CC.                                                                 networks and single hidden units (Shultz & Rivest, 2001).
                                                                       Its performance can be readily compared to ordinary
                           Introduction                                cascade-correlation (CC), which builds a network only by
Mathematical groups are remarkable for two reasons: On                 recruiting single hidden units (Fahlman & Lebiere, 1990).
the one hand, they pervade many areas of mathematics and               Past comparisons have revealed that KBCC tends to recruit
also everyday life, and on the other hand, their abstract              relevant knowledge whenever it can and that this speeds
structure is quite difficult to learn from instances. In this          learning (Shultz & Rivest, 2001) and in some cases makes
paper we use the task of learning the abstract structure of            learning possible (Egri & Shultz, 2006).
small finite groups as a case study to make the following                 Finally, we investigate the relation between the group to
three points: (a) We provide an argument against purported             be learned and the kinds of networks that are recruited from
limitations of neural network approaches. (b) We show how              the knowledge base: Does only the size of the recruits
neural networks can learn from previously existing                     matter, or does the quality of their knowledge also play a
knowledge and that this increases the speed of their                   role? Our results provide a surprising answer to this
learning. (c) Our results reveal that the neural networks              question that points to promising new lines of work on
show some ‘deep understanding’ of the abstract structure of            knowledge and learning.
the problems they are learning.
   Some authors have argued that ordinary artificial neural                        The Abstract Group Structure
networks are inherently unable to learn systematically                 A mathematical group consists of a set of elements together
structured representations such as mathematical groups                 with an associative operation * (i.e., where a*(b*c) and
(Phillips, & Halford, 1997; Marcus, 1998). However, a first            (a*b)*c yield the same result), for which the following three
step has recently been made to invalidate this claim, because          properties hold: (i) the objects are closed under the group
it has been demonstrated that neural networks can learn                operation, i.e., for elements a and b, a*b is also an element
particular mathematical groups, like the Klein 4-group, in a           of the group; (ii) there exists a neutral element n, such that
fashion that simulates learning by humans (Jamrozik &                  a*n=a for all elements a of the group; and (iii) that for
Shultz, 2007). We now extend the task to learning the                  every element a there exists an inverse element a', such that
abstract structure of small finite groups with three and four          a*a'=n.
elements and we compare knowledge-based learning with
knowledge-free learning.
                                                                   2950

   Typical examples of groups are the first n-1 natural             elements and rearranging the rows and columns. In
numbers with addition modulo n as the group operation (for          mathematical terms, these two groups are not isomorphic.
n=3, see Figure 1; for n=4, see the cyclic 4-group in
Figure 3); the positions of a cube with the operation of
                                                                           *    0   1    2   3              *   0    1    2  3
performing rotations in space; all possible permutations of a
                                                                           0    0   1    2   3              0   0    1    2  3
given sequence of objects with successive application of the
permutation as the group operation (Rothman, 1985); and,                   1    1   2    3   0              1   1    0    3  2
the different configurations that can be obtained by flipping              2    2   3    0   1              2   2    3    0  1
a mattress (Hayes, 2005). The group operation is usually                   3    3   0    1   2              3   3    2    1  0
                                                                              Cyclic 4-group                   Klein 4-group
presented by a multiplication table, or Cayley table, as in the
examples shown in Figure 1.                                             Figure 3: Two different 4-element groups, the cyclic 4-
                                                                               group (left) and the Klein 4-group (right).
          *   0   1   2                   *   b   a   c
          0   0   1   2                   b   a   c   b                Both 4-element groups have the 2-element group as a
          1   1   2   0                   a   c   b   a             subgroup. In other words, within their 4x4 multiplication
          2   2   0   1                   c   b   a   c             tables one can identify pairs of elements that behave like a
                                                                    group with 2 elements. For the cyclic 4-group these are the
  Figure 1: Addition modulo 3 and alternative presentation.         elements 0 and 2, while the Klein 4-group has three such
                                                                    pairs: elements 0 and 1, 0 and 2, and 0 and 3. Notice also,
   The multiplication table on the left in Figure 1 represents      that the 3-element group (Figure 1) does not have the 2-
a 3-element group with elements 0, 1, 2, where the group            element group as a subgroup. This difference between 3-
operation * is interpreted as addition modulo 3; the neutral        and 4-element groups has interesting implications for
element is 0; as one can see by inspecting the table, the           interpreting the results of knowledge-based learning of
inverse of 0 is 0, the inverse of 1 is 2, and the inverse of 2      groups. We return to this difference in the discussion of our
is 1. Because an abstract group is characterized only by the        results.
structure induced by the group operation and not by the                For our research, we trained neural networks with all
names of the elements or the order in which they are                possible multiplication tables for the 3-element and the 4-
presented in a multiplication table, renaming the elements          element groups using knowledge-free (CC) and knowledge-
and/or rearranging the rows and columns yields a                    based (KBCC) learning algorithms in order to assess their
representation of the same abstract group (see, for example,        abilities, and, in the case of the knowledge-based algorithm,
the multiplication table on the right in Figure 1 for an            also to study the quality of the recruited knowledge.
alternative presentation; here c is the neutral element). In
fact, one can show that there is only one abstract group with                                   Method
three elements (Rotman, 1996). In other words, any set with
three elements and an operation that satisfies the conditions       CC Learning
(i)–(iii) for groups listed above can be put into the form of       CC learning alternates between two phases: input phase and
the multiplication tables in Figure 1 by renaming the               output phase (Fahlman & Lebiere, 1990). Each phase is
elements and rearranging the rows and columns.                      composed a number of epochs, where an epoch is a pass
   There is also exactly one abstract group with two                through the set of training examples. CC networks begin
elements, which is represented by the two multiplication            learning without any hidden units. They start training in
tables in Figure 2 (the group on the left has the neutral           output phase, by adjusting connection weights entering
element 0, while the group on the right has the neutral             output units to reduce as much error as possible. In input
element 1; these two groups are again instances of the same         phases, the input weights to candidate hidden units are
abstract group).                                                    trained in order to maximize the covariance between unit
                                                                    activation and network error. The candidate unit with the
            *   0   1                       *   0   1               highest absolute covariance is selected and installed into the
            0   0   1                       0   1   0               network with random input connection weights of the same
            1   1   0                       1   0   1               sign as those that were just learned, the other candidates are
                                                                    discarded, and training shifts back to output phase. The
      Figure 2: Two alternative representations of the same         algorithm shifts from one phase to the other when the
                    abstract 2-element group.                       current phase fails to improve the solution of the problem on
                                                                    which the network is being trained. That is, it fails to reduce
   There are two different abstract groups with four                error in output phase, or fails to increase covariances in
elements: the cyclic 4-group, and the Klein 4-group (see            input phase. CC and related algorithms have been used to
Figure 3). They are considered different because the                simulate many aspects of cognition and its development
multiplication table of a Klein 4-group can never be made to        (Shultz, 2003; 2006).
coincide with that of a cyclic 4-group by renaming the
                                                                2951

KBCC Learning                                                        leftmost column and the top row, as in the table on the left
KBCC differs from CC mainly by having the potential to               in Figure 5. However, we did not employ this convention
recruit previously-learned networks in competition with              for the inputs in order to prevent networks from learning
single hidden units (Shultz & Rivest, 2001). The                     some feature that is purely an artifact of this particular mode
computational device that gets recruited is the one whose            of presentation, e.g., that 0*0 is always 0. As a consequence,
output covaries best with residual network error. An                 the learning task itself becomes much more difficult. To
example of a KBCC network is shown in Figure 4,                      grasp this, consider the task of determining whether the two
illustrating that a recruited source network can have                multiplication tables presented in Figure 1 or the three tables
multiple inputs and outputs, thus requiring connection-              shown in Figure 5 are indeed instances of the same abstract
weight matrices rather than vectors. Mathematical details            group. Even most trained mathematicians would not be able
for KBCC are available elsewhere (Shultz & Rivest, 2001),            to solve this at a glance.
along with evidence of its use in cognitive simulations                 It is noteworthy that only very few multiplication tables
(Shultz, Rivest, Egri, Thivierge, & Dandurand, 2007).                actually represent instances of an abstract group. A 3x3
                                                                     multiplication table, or matrix, can be filled with at most
                                                                     three elements in 39=19,683 different ways. If only those
                                                                     tables are considered in which each element occurs exactly
                                                                     three times, this reduces the number of different
                                                                     multiplication tables to 1,680. If the tables are further
                                                                     restricted to only those in which each element occurs
                                                                     exactly once in each row and column, 12 different tables
                                                                     remain, of which only the three shown in Figure 5 represent
                                                                     the abstract group structure.
                                                                        The candidate units for the KBCC algorithm consisted of
                                                                     multiple instances of a sigmoid unit, a network that was
                                                                     trained to learn the 2x2 group using the CC algorithm
                                                                     (genuine 2x2), and a network of the same structure as the
        Figure 4: Drawing of sample KBCC network after               previous one, but with random weights chosen with uniform
  recruiting two source networks and one sigmoid unit. The           distribution from the range -1 to 1 (random 2x2). As
 dashed line represents a single connection weight, the light        training data for the genuine 2x2, we used all 16 possible
 solid lines represent weight vectors, and the dark solid lines      configurations of the elements 0 and 1 in a 2x2
    represent weight matrices. Activation flow is upward.            multiplication table, only two of which are in fact groups
                                                                     (these two are shown in Figure 3).
The 3x3 Learning Task                                                   For the comparison of the CC and the KBCC algorithms
                                                                     we used in the training set 900 positive matrices, i.e., 300
As inputs we encode the group multiplication tables                  copies of the three alternative presentations of the 3-element
discussed earlier, in which the names and the order of the           group, and 1000 random 3x3 matrices, in which every
group elements are kept fixed. In this case the 3-element            element occurs exactly three times, that are not groups.
group can be presented by three different multiplication
tables, as shown in Figure 5. (In the first multiplication table     The 4x4 Learning Task
0 is the neutral element, while it is 1 and 2 in the second and
third table, respectively). Because these tables can be              Each of the two abstract groups with four elements can be
transformed into each other by renaming the elements and             represented by 16 different 4x4 matrices (if the names of the
rearranging the rows and columns, they all represent the             elements and their order are kept fixed). Of these, 15 were
same abstract group. Also, because the names of the                  used in the training sets and one was used for testing for
elements and their order are kept constant, we can omit the          generalization. As in the 3x3 case, we again used 900
labels of the columns and rows from the inputs, so that the          positive instances, i.e., 60 copies of the 15 4x4 group
multiplication table of a 3-element group can be given as a          multiplication tables and 1000 random 4x4 matrices that are
3x3 matrix.                                                          not groups in which every element occurs exactly four
                                                                     times.
      *    0   1   2     *   0   1    2      *    0   1  2              The pool of possible recruits by the KBCC algorithm
                                                                     consisted of multiple instances of the same units as in the
      0    0   1   2     0   2   0    1      0    1   2  0
                                                                     3x3 learning task (sigmoid, genuine 2x2, and random 2x2),
      1    1   2   0     1   0   1    2      1    2   0  1
                                                                     plus two additional 3x3 networks. The genuine 3x3 network
      2    2   0   1     2   1   2    0      2    0   1  2
                                                                     had learned the 3-element group structure using a training
                                                                     set consisting of 1090 matrices in total, 90 of which were
 Figure 5: Alternative presentations of the 3-element group.         copies of the 3 different multiplication tables for the 3x3
                                                                     group. The random 3x3 network had the same structure as
   When presenting a group multiplication table it is
customary to always write the neutral element in the
                                                                 2952

the genuine 3x3, but again with random weights chosen                more important than its mere complexity. See Table 1 for
with uniform distribution from the range -1 to 1.                    the proportions of the units recruited by KBCC in both
   Both the 3x3 and the 4x4 learning tasks were run 20 times         learning tasks.
to enable statistical comparisons. Each network was trained
until the single output unit activation was within the score-                              Table 1: Proportions of recruits by KBCC.
threshold parameter of its target value on every training
pattern. Because the output decision was a binary one, i.e.,                                          Dimensions of group to be learned
the input matrix is a group or is not a group, the score-                     Recruits                     3x3                4x4
threshold was the default value of 0.4. The sigmoid                           Sigmoid                     .077               .147
activation function has an output in the range -.5 to .5.                     Genuine 2x2                 .788               .427
                                                                              Random 2x2                  .135               .067
                                  Results                                     Genuine 3x3                  n/a               .227
We analyzed learning speed, number of recruits, the quality                   Random 3x3                   n/a               .133
of KBCC recruits, and generalization ability.
  For learning speed, we performed a task dimensions x                  For learning 4x4 patterns, the counts of genuine and
algorithm factorial ANOVA of the epochs required to learn            random 2x2 and 3x3 recruits were subjected to a type of
the training patterns. There were main effects of dimensions         recruit x dimensions of recruit repeated measures ANOVA.
F(1, 76) = 7.37, p < .01 and algorithm F(1, 76) = 15.54, p <         The associated means are plotted in Figure 7. There was a
.001, but no significant interaction. The means are presented        main effect of type of recruit and an interaction between that
in Figure 6. The 4x4 patterns took longer to learn than the          and the dimensions of the recruit. The main effect of type of
3x3 patterns, and CC took longer to learn than did KBCC.             recruit, F(1, 19) = 10.34, p < .005, reflects that recruits were
The speedup provided by KBCC was significant for both                more often genuine than random, again the expected pattern
3x3 patterns, t(38) = 3.51, p < .001, and 4x4 patterns t(38) =       if knowledge trumps mere complexity. The interaction, F(1,
2.31, p < .03. Thus, KBCC learned these groups faster than           19) = 8.64, p < .01, was further explored with dependent t
did knowledge-free CC.                                               tests. Among genuine network recruits, there were more
                                                                     2x2s than 3x3s, t(19) = 3.00, p < .01, an interesting finding
                  500                                                discussed more fully in the Discussion section. Whereas
                                       CC                            among random network recruits, there was no significant
                  400
                                       KBCC                          difference related to group dimensions of the recruit, t(19) =
    Mean epochs
                  300                                                -1.42, ns.
                  200
                                                                                          2
                  100                                                                                                  Type of recruit
                   0                                                                     1.5                               Random
                                                                         Mean recruits
                            3x3                            4x4                                                             Genuine
                                                                                          1
                             Group dimensions of learning task
                                                                                         0.5
             Figure 6: Mean epochs to learn groups of different
                   dimensions by two algorithms ±SE.                                      0
                                                                                                      2x2                                 3x3
   Number of recruits was also subjected to a dimensions x                                                  Group dimensions of recruit
algorithm factorial ANOVA. There were two main effects
and no interaction. The main effect of dimensions reflected              Figure 7: Mean genuine and random network recruits
more recruits for the 4x4 task (M = 3.98) than for the 3x3                of different group dimensions in KBCC learning of
task (M= 3.2), F(1, 76) = 5.34, p < .03. The main effect of                                4x4 groups ±SE.
algorithm revealed that CC used more recruits (M = 4.00)
than did KBCC (M = 3.17), F(1, 76) = 6.05, p < .02.                     For the 4x4 learning task we tested generalization in 20
Generally in CC-type algorithms, the more recruits that are          fresh CC and 20 fresh KBCC networks by recording each
required, the longer it takes to learn.                              network’s error on 1 of the 16 sets of patterns that was not
   For quality of recruits, we analyzed the 3x3 task                 used in training. The mean errors are plotted in Figure 8. A
separately from the 4x4 task because some types of recruits          repeated-measures ANOVA on these errors with algorithm
were available in the latter but not in former. Learning 3x3         as a between network factor and training as a within
patterns, KBCC networks recruited many more genuine 2x2              network factor yielded only a main effect of training, F(1,
source networks (M = 2.05) than randomized 2x2 source                38) = 71.42, p < .0001. Error dropped sharply on these test
networks (M = 0.35), t(19) = -6.47, p < .001. This is the            patterns during training with either algorithm.
pattern one would expect if the recruits’ actual knowledge is
                                                                  2953

                                                                         a sort of building block for both the cyclic 4-group and the
                  1.6
                  1.4
                                                                         Klein 4-group, and the KBCC algorithm picks up on these
                  1.2                           CC                       subtle structural relationships. In other words, in addition to
                                                                         being sensitive to the abstract group structure, KBCC is also
     Mean error
                    1                           KBCC
                  0.8                                                    able to exploit deeper structural relationships between what
                  0.6                                                    is being learned and what is already present in the
                  0.4
                                                                         knowledge base. In the present case this has the effect that
                  0.2
                    0
                                                                         those candidate networks are preferred that represent
                        Before                         After             subgroups of the group that is being learned. To summarize,
                                     Training                            our results suggest that KBCC does not recruit networks
                                                                         primarily based on some quantitative measure (e.g., size and
       Figure 8: Mean error on test patterns before and after            complexity), but based on the relevance and quality of the
                         training ±SE.                                   knowledge they encode.
                                                                            In future work we would like to investigate if this
                          Discussion                                     observation can also be substantiated when larger groups are
It is evident that both CC and KBCC algorithms are able to               being learned. Based on our experiences so far we expect
learn the structure of the abstract 3-element and 4-element              that in these cases further structural properties, like that of
groups, and that they generalize successfully to untrained               being a cyclic group or being a commutative group, will
test patterns. Thus, claims that such tasks are impossible to            play a role for the selection of the candidate recruits.
learn by neural networks are successfully refuted by our                    As anticipated, the learning of the abstract group structure
results.                                                                 proved to be a worthy and interesting challenge for
   Comparison between the CC and KBCC learning                           constructive network algorithms. CC is known to be a
algorithms reveals the following: (a) KBCC consistently                  strong learner because of the way it searches simultaneously
learns faster than CC, and (b) KBCC consistently has fewer               in both topology space and weight space to find, not only
recruits. These two differences are due to the fact that                 appropriate connection weights, but also the right topology
KBCC chooses recruits that possess knowledge that is                     for the task it is learning (Fahlman & Lebiere, 1990; Shultz,
relevant for the task at hand. Thus, our results confirm                 2003). Thus, CC naturally, without intervention of
similar learning speedups due to knowledge that were                     programmers, builds networks of about the right size for the
obtained previously with very different learning tasks                   learning task. Consequently, it can learn to distinguish
(Shultz & Rivest, 2001).                                                 groups from non-groups, given enough learning time and
   In addition, if we look at the types of networks that are             examples. But KBCC, by recruiting simpler relevant
recruited by KBCC, we notice very distinctive preferences                knowledge than its training task, can learn these distinctions
(see Table 1), which cannot be explained in terms of the                 even faster and with fewer recruits than CC can. There is
complexities of the recruited networks alone. When learning              little question that these constructive neural learners can,
the 3x3 group, KBCC recruited in the majority of cases the               from examples alone, learn important features about
genuine source network, i.e., the network that had already               abstract, systematic structures.
learned the 2x2 group. This shouldn’t come as too big a                     We make no claim that these neural networks understand
surprise, because both groups share those structural features            groups in the same explicit fashion as mathematical
that are characteristic of all groups and that distinguish them          specialists do. But these algorithms could well serve to
from other patterns that do not instantiate the abstract group           model how ordinary people acquire complex, abstract,
structure. (These features are the properties (i), (ii), and (iii),      highly-structured knowledge (Jamrozik & Shultz, 2007;
presented earlier.) This also explains why, in the 4x4                   Egri & Shultz, 2006). Modeling expert explicit
learning task, KBCC recruited the two networks that were                 mathematical knowledge would require in addition that this
trained with the smaller groups (Genuine 2x2 and Genuine                 implicit learned knowledge be converted into an axiomatic
3x3) more frequently than the networks with random                       characterization that expresses the three group properties
weights or single sigmoid units.                                         (i)–(iii) listed in our earlier section on The Abstract Group
   But, why did KBCC recruit the genuine 2x2 network                     Structure, to allow for a concise representation of both finite
significantly more often than the genuine 3x3 network?                   and infinite groups (Schlimm, 2008).
After all, because the 3x3 network is larger it should be able
to encode more information than the 2x2 network. Some                                      Acknowledgements
insight into the structural relations between the different 2,              This research is supported by a grant to DS from the
3, and 4-element groups can resolve this mystery and                     Social Sciences and Humanities Research Council of
explain the prima facie irrational behavior of the KBCC                  Canada, and a grant to TRS from the Natural Sciences and
algorithm.                                                               Engineering Research Council of Canada. We also would
   As mentioned earlier, both of the abstract 4x4 groups                 like to thank James Olchowski for helping to run the
have 2x2 groups as their subgroups. Thus, the 2x2 group is               learning tasks.
                                                                      2954

                        References
Egri, L., & Shultz, T. R. (2006). A compositional neural-
  network solution to prime-number testing. In R. Sun & N.
  Miyake (Eds.), Proceedings of the Twenty-eighth Annual
  Conference of the Cognitive Science Society (pp. 1263–
  1268). Mahwah, NJ: Lawrence Erlbaum.
Fahlman, S. E., & Lebiere, C. (1990). The cascade-
  correlation learning architecture. In D. S. Touretzky (Ed.),
  Advances in neural information processing systems 2 (pp.
  524–532). Los Altos, CA: Morgan Kaufmann.
Hayes, B. (2005) Group theory in the bedroom. American
  Scientist, 93, 395–399.
Jamrozik, A., & Shultz, T. R. (2007). Learning the structure
  of a mathematical group. In D. McNamara & G. Trafton
  (Eds.), Proceedings of the Twenty-ninth Annual Con-
  ference of the Cognitive Science Society (pp. 1115–1120).
  Mahwah, NJ: Lawrence Erlbaum.
Marcus, G. F. (1998). Rethinking eliminative connection-
  ism. Cognitive Psychology, 37, 243–282.
Phillips, S., & Halford, G. S. (1997). Systematicity:
  Psychological evidence with connectionist implications.
  In M. G. Shafto & P. Langley (Eds.), Proceedings of the
  Nineteenth Annual Conference of the Cognitive Science
  Society. Mahwah, NJ: Erlbaum.
Rothman, T. (1982). The short life of Évariste Galois.
  Scientific American, 246 (4), 136–149.
Rotman, J. J. (1996). A first course in abstract algebra.
  Upper Saddle River, NJ: Prentice Hall.
Schlimm, D. (2008). Two ways of analogy: Extending the
  study of analogies to mathematical domains. Philosophy
  of Science, 75 (2), 178–200.
Shultz, T. R. (2003). Computational developmental psy-
  chology. Cambridge, MA: MIT Press.
Shultz, T. R. (2006). Constructive learning in the modeling
  of psychological development. In Y. Munakata & M. H.
  Johnson (Eds.), Processes of change in brain and
  cognitive development: Attention and performance XXI.
  (pp. 61–86). Oxford, UK: Oxford University Press.
Shultz, T. R., & Rivest, F. (2001). Knowledge-based
  cascade-correlation: Using knowledge to speed learning.
  Connection Science, 13, 1–30.
Shultz, T. R., Rivest, F., Egri, L., Thivierge, J.-P., &
  Dandurand, F. (2007). Could knowledge-based neural
  learning be useful in developmental robotics? The case of
  KBCC. International Journal of Humanoid Robotics, 4,
  245–279.
                                                               2955

