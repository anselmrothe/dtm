UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Temporal Recalibration in Audio-Visual Speech Integration Using a Simultaneity Judgment
Task and the McGurk Identification Task
Permalink
https://escholarship.org/uc/item/2q4164q6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Askawa, Kaori
Imai, Hisato
Tanaka, Akihiro
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                 University of California

                       Temporal Recalibration in Audio-Visual Speech Integration
          Using a Simultaneity Judgment Task and the McGurk Identification Task
                                        Kaori ASAKAWA1 (kaori@ais.riec.tohoku.ac.jp)
                   Division of Psychology, Graduate School of Humanities, Tokyo Woman's Christian University
                                         2-6-1 Zempukuji, Suginami-ku, Tokyo 167-8585, Japan
                                                Akihiro TANAKA2 (a.tanaka@uvt.nl)
       Department of Psychology, Faculty of Letters/Graduate School of Humanities and Sociology, University of Tokyo
                                             7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
                                                Hisato IMAI (hisato@lab.twcu.ac.jp)
                   Department of Psychology, College of Arts and Sciences, Tokyo Woman's Christian University
                                         2-6-1 Zempukuji, Suginami-ku, Tokyo 167-8585, Japan
                              Abstract                                   & Spence, 2006a; Vatakis & Spence, 2006b). These studies
                                                                         have revealed that the audio-visual temporal window
   Audio-visual synchrony is important for comfortable speech            depends on the characteristics of the stimulus (e.g.,
   communication. Previous studies have revealed a temporal              complexity, duration, and ecological validity) and/or tasks
   window during which human observers perceive physically               (e.g., asynchrony detection, simultaneity/temporal order
   desynchronized auditory and visual signals as synchronous in
   both speech and nonspeech signals. This temporal window of
                                                                         judgment, and speech identification).
   audio-visual integration is re-calibrated after adaptation to a          Recent studies have shown that the audio-visual temporal
   constant timing difference between auditory and visual                window is recalibrated after adaptation to a constant timing
   signals in nonspeech. In this study, we investigate whether or        difference between auditory and visual signals. Both
   not the after-effects of the consequences of temporal                 Fujisaki, Shimojo, Kashino, and Nishida (2004) and
   recalibration occur even in speech stimuli. Our results suggest       Vroomen, Keetels, de Gelder, and Bertelson (2004)
   that the temporal recalibration occurs not only in nonspeech          demonstrated shifts in the point of subjective simultaneity
   signals but also in monosyllabic speech.
                                                                         (PSS: the amount of time between the auditory and visual
   Keywords: audio-visual integration; speech perception;                stimuli required for perceptual simultaneity) following
   temporal recalibration; simultaneity judgment; the McGurk             exposure to a series of desynchronized simple audio-visual
   effect                                                                pairs toward the exposure lag.
                                                                            The above studies used simple nonspeech pairs of light
                           Introduction                                  and sound. However, the temporal discrepancy that we
Human beings perceive light and sound generated from an                  encounter in the real world is more complex (e.g., speech,
event via their respective organs, that is, eyes and ears. The           music, or action). Especially, speech signals play an
light and sound are usually asynchronous. For example, a                 important role in daily communications. Previous studies
distant sound arrives later than a visual component because              have investigated the temporal window of audio-visual
of their different velocities. People can detect a temporal              speech integration in both synchrony perception (Dixon &
mismatch due to a technical limitation in a live satellite               Spitz, 1980; Conrey & Pisoni, 2006; Vatakis & Spence,
broadcast. In addition, there is a difference between the                2006a; Vatakis & Spence, 2006b) and speech identification
sensory processing latencies of audition and vision (Spence              (Grant & Greenberg, 2001; Munhall, Gribble, Sacco, &
& Squire, 2003). Thus, audio-visual temporal asynchrony                  Ward, 1996; van Wassenhove, Grant, & Poeppel, 2007). In
                                                                         addition to containing linguistic information, a speech signal
occurs due to factors in the environment and in the brain
                                                                         has different characteristics from a simple nonspeech signal
processing.
                                                                         in terms of structural factors (e.g., physical complexity,
   Audio-visual temporal asynchrony is tolerated to some
                                                                         saliency of onset/offset) and cognitive factors (e.g.,
extent for auditory and visual stimuli to be perceived as a
                                                                         ecological validity, familiarity). The question of whether
single event. This temporal tolerance, the so-called
                                                                         speech is special has been discussed (see Jones & Jarick,
“temporal window,” has been examined through various
                                                                         2006; Vatakis, Ghazanfar, & Spence, 2008; Tuomainen,
kinds of stimuli and tasks (e.g., Dixon & Spitz, 1980;
                                                                         Andersen, Tiippana, & Sams, 2005; Bernstein, Auer, &
Conrey & Pisoni, 2006; Grant & Greenberg, 2001; Vatakis
                                                                         Moore, 2004). However, it has not been settled. Given these
   1
     Present affiliation: Research Institute of Electrical Communication and Graduate School of Information Sciences, Tohoku University,
2-1-1 Katahira, Aoba-ku, Sendai 980-8577, Japan
   2
     Present affiliation: Cognitive and Affective Neurosciences Laboratory, Tilburg University, PO Box 90153, 5000 LE Tilburg, The
Netherlands
                                                                       1669

aspects of speech, in order to reveal the temporal                  speech identification task as an indirect measure. We used a
recalibration of audio-visual speech, it is proper to use a         phenomenon which is known as the McGurk effect
speech signal, not a simple nonspeech signal.                       (McGurk & McDonald, 1976). The McGurk effect is
   Several studies have investigated temporal recalibration         created by dubbing an auditory /pa/ onto a visual /ka/. The
using a speech signal (Navarra, Vatakis, Zampini, Soto-             participants observed this clip experience hearing /ta/ as a
Faraco, Humphreys, & Spence, 2005; Vatakis, Navarra,                result of the integration of an auditory /pa/ and a visual /ka/.
Soto-Faraco, & Spence, 2007; Vatakis, Navarra, Soto-                The more synchronous the visual and auditory speech are,
Faraco, & Spence, 2008). Vatakis et al. (2007) conducted a          the stronger the McGurk effect is (Munhall, et al., 1996; van
study in which the participants had to make a temporal order        Wassenhove, et al., 2007). We focused only on the aspect of
judgment (TOJ) regarding pairs of asynchronous vowel-               the audio-delay of audio-visual asynchrony, given the
consonant-vowel (VCV) speech-sounds and visual-speech               ecological validity of audio-delay timing rather than visual-
gestures while monitoring a continuous background                   delay.
consisting of an audio-visual speech stream of words (i.e.,
an on-line adaptation method). The continuous (adapting)               Experiment 1: Simultaneity Judgment Task
speech stream could either be presented in synchrony or
with the auditory stream lagging. Participants conducted the        Methods
TOJ task either in a single task condition (i.e., devoted
themselves to perform the TOJ task) or in a dual-task               Participants Ten participants (mean age of 23.4 years) took
condition (i.e., conducted the TOJ task in parallel with            part in Experiment 1. All of them reported normal hearing
counting the number of male names included in the                   and normal or corrected-to-normal visual acuity. All were
asynchronous background speech stream). A significant               native Japanese speakers.
PSS shift was observed in the direction of the adapting
stimulus only when participants made a TOJ in the dual-task         Materials The stimulus was based on digital audio and
condition. This result suggests that the temporal perception        video recordings of a female native speaker of Japanese.
for audio-visual speech is pulled toward the monitoring             The visual materials were recorded using a DV camera
asynchrony only during a dual-task situation.                       (HDR-A1J, Sony). Auditory stimuli were collected using a
   Although the above studies have demonstrated temporal            condenser microphone (ECM-77B, Sony). The audio
recalibration using a speech signal, more consideration             uttering of /pa/ was dubbed onto an audio track of a video
appears to be necessary for revealing the temporal                  uttering of /ka/ (frontal view, including head and shoulders)
recalibration for speech. First, their results were limited to a    using Sound Forge 8.0 (Sony) to get precise synchronization.
dual-task situation. Given these results, it is unclear what        The video clip (640 × 480 pixels, Cinepak Codec video
affects the temporal realignment in the on-line adaptation          compression, 30 frames/s) and the auditory speech (16-bit
method; it might be the attention paid to background speech         and 48-kHz audio signal digitization) were synthesized and
and/or a decline of an attentional resource for the target          desynchronized using Adobe Premiere Pro 2.0. A still image
stimulus (Navarra et al. (2005) also showed, however, the           was extracted from the first and the last frame of the video
temporal window was not affected when the background lag
                                                                    clip and added to the beginning and end of the video clip to
was too large (1000 ms). This suggests that it seems
                                                                    fill the blank field.
implausible to explain solely in terms of the reduced
attentional source). In the latter case, the results from the
                                                                    Design The experiment had two within-participants factors:
on-line adaptation method might tap a different mechanism
                                                                    Adaptation lag (audio delay: 0, 233 ms) and stimulus onset
from those from an off-line adaptation method. Thus, it is
                                                                    asynchronies (SOA) between the visual-speech and speech-
not clear whether the temporal recalibration following
                                                                    sound of the test stimulus (audio delay: 0, 66, 133, 166, 233,
exposure to asynchrony (i.e., off-line adaptation method),
                                                                    300, 433 ms). The audio-delay lag was used in terms of
not in a dual-task situation, occurs for an audio-visual
                                                                    ecological validity based on the velocities of light and sound.
speech signal. Second, previous studies for temporal
                                                                    This provided a half of the temporal window.
recalibration in speech have investigated using only a direct
measure in which participants can judge the simultaneity
                                                                    Procedure The experiment was conducted in a sound-proof
explicitly. Because most of the information in any direct
task is open to conscious inspection, the responses can             room. The participant was seated at a distance of
easily reflect response strategies rather than a perceptual         approximately 50 cm from a 17-inch CRT monitor (CPD-
process (see Bertelson & de Gelder, 2004; Fujisaki et al.,          E220, Sony), wearing headphones (HDA 200, Sennheiser).
2004). In order to reduce this possibility, it is useful to adopt   The speech-sound was presented at the sound pressure level
an indirect (implicit) measure as well as a direct measure.         of approximately 65 dB. The pink noise overlapped with the
   Therefore, in this study, we investigated the temporal           speech-sound at 65 dB (i.e., S/N 0 dB). The video clip was
recalibration for audio-visual speech following exposure to         presented on a black background using Real Player Ver.
asynchrony using both direct and indirect measures. We              10.5 (RealNetworks).
used a simultaneity judgment task as a direct measure and a            Each session started with an adaptation phase of 3 min
                                                                    with a constant time lag between the visual speech and
                                                                  1670

                   100                       233 ms adaptation condition                      100                     233 ms adaptation condition
                    90                       0 ms adaptation condition                         90                     0 ms adaptation condition
  % simultaneous responses
                    80                                    n = 10                               80
                                                                                  % /ta/ responses
                    70                                                                         70
                                                                                                                                     n =8
                    60                                                                         60
                    50                                                                         50
                    40                                                                         40
                    30                                                                         30
                    20                                                                         20
                    10                                                                         10
                     0                                                                          0
                             0   50 100 150 200 250 300 350 400 450                                  0   50 100 150 200 250 300 350 400 450
                                        Audio delay (ms)                                                         Audio delay (ms)
Fig.1 Percentages of simultaneous responses as a function                   Fig.2 Percentages of /ta/ responses as a function of adapted
of adapted lag and audio delay.                                             lag and audio delay.
speech sound. The adaptation phase was followed by test                           Experiment 2: The McGurk Identification
trials, each preceded by a 10-s re-adaptation. After a 2-s red-                                    Task
tinted still image as a cue of the test trial, a test stimulus was
presented with various SOAs. The participants’ task was to                  Methods
judge whether auditory and visual stimuli were presented
synchronously or asynchronously. Participants were                          Participants Eight participants (mean age of 22.3 years)
instructed to respond accurately rather than quickly. The                   took part in Experiment 2. All of them reported normal
various SOAs of the test stimuli were randomly presented in                 hearing and normal or corrected-to-normal visual acuity. All
each session using the method of constant stimuli. The                      were native Japanese speakers.
experimental session, which lasted approximately 20
minutes, consisted of 42 test trials (6 repetitions of the 7                Materials, Design and Procedure The participants’ task
SOAs). Four experimental sessions were run for each                         was to choose what they heard while looking at a mouth.
adaptation condition. They participated in one adaptation                   They had to give their answers among four choices (pa, ta,
condition per day.                                                          ka, or other). Except for this, the materials, design and
                                                                            procedure were the same as in Experiment 1.
Results and discussion
Figure 1 shows the percentages of simultaneous responses
as a function of adapted lag and SOAs. The simultaneous                     Results and discussion
responses decreased as the amount of SOAs increased. The                    We adopted /ta/ responses as a typical McGurk illusion
percentages of simultaneous responses were higher in the                    resulting from the integration of an auditory /pa/ and a
audio delay (233 ms) adaptation condition than in the                       visual /ka/ (see van Wassenhove et al., 2007). Figure 2
synchronous (0 ms) adaptation condition except when the                     shows the percentages of /ta/ responses (the McGurk
SOAs were 0 and 433 ms. In a two-way analysis of variance                   illusion) as a function of adapted lag and SOAs. The
(ANOVA), there were significant main effects of adapted                     temporal range within which McGurk illusion is obtained is
lag [F(1, 9) = 5.31, p < .05] and test lag [F(6, 54) = 131.32,              up toaround 200 ms audio-delay (267 ms in van
p < .01]. Interaction between those two factors was only                    Wassenhove et al. (2007) and240 ms in Munhall et al.
marginal [F(6, 54) = 1.87, p < .10]. These results suggest                  (1996), although these studies adopteddifferent criteria for
that adaptation to audio delay timing affects the synchrony                 the McGurk effect). Consistent with thesestudies, the
perception toward the adapted (audio-delay) lag.                            percentage of /ta/ responses was significantly lower whenthe
  Thus, our results suggest that the temporal window for                    audio-delay was 233 ms in our results. As the simultaneous
speech was modulated toward the adapted lag using a direct                  responses in Experiment 1, the /ta/ responses decreased as
measure (i.e., simultaneity judgment). In experiment 2, we                  the amount of SOAs increased. Also, the percentages were
used an indirect measure to reduce the possibility that this                higher in the audio delay adaptation condition than in the
modulation of the temporal window resulted from post-                       synchronous adaptation condition. In a two-way ANOVA,
perceptual influences.                                                      there were significant main effects of adapted lag [F(1, 7) =
                                                                            9.28, p < .01] and test lag [F(6, 42) = 18.59, p < .01].
                                                                            Interaction was not significant [F(6, 42) = 0.29, p > .10].
                                                                           1671

These results suggest that adaptation to audio delay timing                              Conclusion
alters the McGurk illusion.
                                                                 In this study, we investigated temporal recalibration for
   Our results showed that the temporal window changes not
                                                                 audio-visual speech following exposure to asynchrony using
only with the direct measure but also indirect measure (i.e.,
                                                                 both a simultaneity judgment task and the McGurk
speech identification). This finding supports the idea that
                                                                 identification task. The results showed that the temporal
temporal recalibration occurs not only for nonspeech signals
                                                                 window modulated toward exposed (audio-delay) timing in
but also for monosyllabic speech. In addition, this temporal
                                                                 both direct and indirect measures. These results suggest that
recalibration could be regarded as a perceptual phenomenon,
                                                                 temporal recalibration occurs for speech signals at the
not a post-perceptual change, because temporal recalibration
                                                                 perceptual level.
occurred with both direct and indirect measures.
                                                                                    Acknowledgments
                   General Discussion
                                                                 A part of this work was supported by a Grant-in-Aid for
In this study, we investigated the temporal recalibration for
                                                                 Specially Promoted Research No. 19001004 from the
audio-visual speech following exposure to asynchrony (i.e.,
in an off-line adaptation method) using both a direct            Ministry of Education, Culture, Sports, Science and
measure (i.e., the simultaneity judgment) and an indirect        Technology, Japan.
measure (i.e., the McGurk identification). The results
showed that the temporal window for audio-visual speech is                               References
modulated in the direction of adapted lag (audio-delay) with     Bernstein, L.E., Auer, E.T., & Moore, J.K. (2004).
both measures. These results suggest that the audio-visual         Audiovisual speech binding: convergence or association?
temporal recalibration following exposure to audio-delay           In Calvert, G.A., Spence, C., & Stein, B.E. (eds) The
timing occurs not only for nonspeech signals but also for          handbook of multisensory processing. MIT, Cambridge,
speech signals at the perceptual level.                            pp. 203–223.
   Our results were consistent with those of Fujisaki et al.     Bertelson, P., & de Gelder, B. (2004). The psychology of
(2004) in that the audio-visual temporal window changed            multimodal perception. In Spence, C., & Driver, J. (eds)
toward the adapted lag in the off-line adaptation method           Crossmodal space and crossmodal attention. Oxford
using both direct and indirect measures. Fujisaki et al.           University Press, Oxford, pp. 141–177.
demonstrated temporal recalibration using both the               Conrey, B., & Pisoni, D.B. (2006). Auditory-visual speech
simultaneity judgment and a stream/bounce illusion                 perception and synchrony detection for speech and
(Sekuler, Sekuler, & Lau, 1997) using simple nonspeech             nonspeech signals. Journal of the Acoustical Society of
signals. The similar results of these studies could suggest        America, 119, 4065–4073.
that audio-visual temporal recalibration occurs perceptually     Dixon, N., & Spitz, L. (1980). The detection of Audiovisual
both for simple nonspeech and speech. That is, the temporal        desynchrony. Perception, 9, 719–721.
window for speech might be recalibrated, nonetheless             Fujisaki, W., Shimojo, S., Kashino, M., & Nishida, S.
speech appears to have several characteristics, e.g.,              (2004). Recalibration of audiovisual simultaneity. Nature
relatively high ecological validity, physical complexity,          Neuroscience, 7, 773-778.
familiarity, and relatively low saliency of onset/offset. In a   Grant, K.W., & Greenberg, S. (2001). Speech intelligibility
future study, it should be clarified whether the temporal          derived from asynchronous processing of auditory-visual
window shifts toward the adapted lag and/or widens.                information. Paper presented at the ISCA International
   Using the off-line adaptation method, our results showed        Conference on Auditory-Visual Speech Processing
that the temporal window changes even in a single-task           Jones, J.A., & Jarick, M. (2006). Multisensory integration of
condition. A previous study using the on-line adaptation           speech signals: The relationship between space and time.
method showed that the temporal window changes only                Experimental Brain Research, 174, 588–594.
under the dual-task condition (Vatakis et al., 2007). This       McGurk, H., & McDonald, J. (1976). Hearing lips and
discrepancy suggests either that attention to the adaptation       seeing voices. Nature, 264, 746–747.
stimulus is needed for temporal recalibration or that the        Munhall, K. G., Gribble, P., Sacco, L., & Ward, M. (1996).
responses in these two adaptation methods reflect different        Temporal constraints on the McGurk Effect. Perception
mechanisms. Another possibility is that these two methods          and Psychophysics, 58, 351–362.
reflect different developmental phases of recalibration.         Navarra, J., Vatakis, A., Zampini, M., Soto-Faraco, S.,
Namely, the on-line method reflects an initial phase of            Humphreys, W., & Spence, C. (2005). Exposure to
recalibration while the off-line method reflects a later phase     asynchronous audiovisual speech increases the temporal
(cf. Navarra et al., 2005; Vatakis, Navarra, Soto-Faraco, &        window for audiovisual integration of non-speech stimuli.
Spence, 2008). In order to investigate these possibilities, it     Cognitive Brain Research, 25, 499–507.
is necessary to reveal temporal recalibration in audio-lead      Roberts, M., & Summerfield, Q. (1981). Audiovisual
timing, and development in temporal recalibration.                 presentation demonstrates that selective adaptation in
                                                                   speech perception is purely auditory. Perception and
                                                                   Psychophysics, 30, 309–314.
                                                               1672

Sekuler, R., Sekuler, A. B., & Lau, R. (1997). Sound alters
  visual motion perception. Nature, 385, 308.
Spence, C., & Squire, S.B. (2003). Multisensory integration:
  maintaining the perception of synchrony. Current Biology,
  13, R519–R521
Tanaka, A., Sakamoto, S., Tsumura, K., & Suzuki, Y.
  (2009). Visual speech improves the intelligibility of time-
  expanded auditory speech. NeuroReport, 20, 473–477.
Tuomainen, J., Andersen, T.S., Tiippana, K., & Sams, M.
  (2005). Audio-visual speech perception is special.
  Cognition, 96, B13–B22.
Vatakis, A., Ghazanfar, A.A., & Spence, C. (2008).
  Facilitation of multisensory integration by the 'unity
  effect' reveals that speech is special. Journal of Vision, 8,
  1–11.
Vatakis, A., Navarra, J., Soto-Faraco, S., & Spence, C.
  (2007). Temporal recalibration during asynchronous
  audiovisual speech perception. Experimental Brain
  Research, 181, 173–181.
Vatakis, A., Navarra, J., Soto-Faraco, S., & Spence, C.
  (2008). Audiovisual temporal adaptation of speech:
  Temporal order versus simultaneity judgments.
  Experimental Brain Research, 185, 521–529.
Vatakis, A., & Spence, C. (2006a). Audiovisual synchrony
  perception for music, speech, and object actions. Brain
  Research, 1111, 134–142.
Vatakis, A., & Spence, C. (2006b). Audiovisual synchrony
  perception for speech and music using a temporal order
  judgment task. Neuroscience Letters, 393, 40–44.
Vroomen, J., Keetels, M., de Gelder, B., & Bertelson, P.
  (2004). Recalibration of temporal order perception by
  exposure to audio-visual asynchrony. Cognitive Brain
  Research, 22, 32–35.
van Wassenhove, V., Grant, K.W., & Poeppel, D. (2007).
  Temporal window of integration in bimodal speech.
  Neuropsychologia, 45, 598-607.
                                                                1673

