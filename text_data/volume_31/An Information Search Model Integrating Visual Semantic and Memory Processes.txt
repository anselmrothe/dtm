UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Information Search Model Integrating Visual, Semantic and Memory Processes
Permalink
https://escholarship.org/uc/item/41t9z6df
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Baccino, Thierry
Chanceaux, Myriam
Guerin-Dugue, Anne
et al.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

         An Information Search Model Integrating Visual, Semantic and Memory
                                                             Processes
                                       Myriam Chanceaux (myriam.chanceaux@imag.fr)
                               University of Grenoble, Laboratoire TIMC-IMAG, Domaine de la Merci
                                                      38700 La Tronche FRANCE
                                    Anne Guérin-Dugué (anne.guerin@gipsa-lab.inpg.fr )
                                 University of Grenoble, Laboratoire Gipsa-Lab, Domaine universitaire
                                                  38402 Saint Martin d’Hères FRANCE
                                            Benoı̂t Lemaire (benoit.lemaire@imag.fr)
                               University of Grenoble, Laboratoire TIMC-IMAG, Domaine de la Merci
                                                      38700 La Tronche FRANCE
                                           Thierry Baccino (baccino@lutin-userlab.fr)
                                  Laboratoire LUTIN, Cité des sciences et de l’industrie de la Villette
                                                     75930 Paris cedex 19 FRANCE
                              Abstract                                collected during two experiments where participants had to
                                                                      search for information.
   This study aims at presenting a computational model of visual
   search including a visual, a semantic and a memory process in
   order to simulate human behavior during information seeking.                           Model architecture
   We implemented the memory process, which is the most im-           In this section we will describe our model. This model sim-
   portant part of the model based on the Variable Memory Model
   (Arani, Karwan, & G., 1984; Horowitz, 2006). To compare            ulates human eye movements during a simple task of infor-
   model and humans, we designed two experiments where par-           mation seeking involving text and visual features. We devel-
   ticipants were asked to find the word among forty distributed      oped an architecture in 3 parts, (see Figure 1), corresponding
   on the display, which best answers a question. The results
   showed good fits on different features extracted between em-       to 3 main cognitive processes involved in a task of informa-
   pirical and simulated scanpaths.                                   tion search. These three processes are respectively related to
   Keywords: Visual Memory; Information Seeking; Computa-             bottom-up visual information, top-down semantic informa-
   tional Model; Eye Movements; Semantic Similarities                 tion and memory. These processes will be detailed thereafter.
                                                                      The principle of the model is to predict from a fixed location
                          Introduction                                and an history of previous fixations where will be the next
Nowadays information seeking, on a Web page for exam-                 fixation. It is assumed that from a given point each location
ple, is a very common task. That is why for several years             of the image has a visual weight (calculated by the visual pro-
research has been conducted to try to answer this question:           cess) and a semantic weight (calculated by the semantic pro-
what guide user’s attention in this particular task, especially       cess). These values are modulated by the process of memory.
on the web? The literature contains theoretical models of in-         The basic process of memory is that if a location has already
formation seeking activity (Marchionini, 1997), especially in         been visited it has less interest than if it never was. At each
electronic documents, and also computational models which             iteration the location of the fixation is thus determined until
simulate navigation between pages, with cognitive architec-           the end of the scanpath. The number of fixations is fixed.
tures like ACT-R (Pirolli & Fu, 2003). There are also a lot
of studies based on the Feature Integration Theory (Treisman          Visual part
& Gelade, 1980) with models which take into account the vi-           The visual part of the model is itself divided into 2 parts. The
sual features of stimuli: colors, orientation, contrast (Itti &       first part is a very simple retinal filter which provide a good
Koch, 2000) to determine the most salient part of the stim-           fit to acuity limitations in the visual human field (Zelinsky,
ulus. However even if some models take into account the               Zhang, Yu, Chen, & Samaras, 2006; Geisler & Perry, 2002)
semantic information of the material, in addition to visual in-       . The filtering output has a maximal resolution in areas near
formation (Navalpakkam & Itti, 2005), experiments and mod-            the fixation point, and the resolution decreases with eccen-
elling are missing in this field. The purpose of this paper is        tricity. More practically, locations close to the current fixa-
to describe a cognitively plausible model that takes into ac-         tion will have a strong visual weight, and those further away
count semantic and visual features of stimuli when searching          a low weight. The second part of this process concerns visual
for information. This model is an improvement of a simpler            stimuli. As in many models of visual attention (Itti & Koch,
one (Chanceaux, Guérin-Dugué, Lemaire, & Baccino, 2008).            2001) we also take into account this information in a pseudo
It has been implemented and compared to experimental data             saliency map. In the first experiment, this information is the
                                                                  2831

                                                                      Figure 2: Memory weight as a function of intervening items
                 Figure 1: Model’s architecture                      if the similarity between the current fixated word and the in-
                                                                     struction is under 0.21 , the weights are decreased following
                                                                     a Gaussian around the current fixation. If the similarity is
size of items, and in the second one, their color. Our visual        above 0.2, the weights are increased still following a Gaus-
model is defined as such:                                            sian around the current fixation.
   VisualWeight(i) = VisualSaliency(i) ∗VisualAcuity(i, c)
                                                                     Memory part
                                                                     Most computational models of attention have implemented
i represents each location in the display and c the current fix-     an Inhibition Of Return (IOR) mechanism for driving atten-
ation.                                                               tion through a scene (Klein & MacInnes, 1999). However,
                                                                     evidence for IOR during scene viewing is inconclusive (Mot-
Semantic part
                                                                     ter & Belky, 1998; Hooge, Over, Wezel, & Frens, 2005). In
It is well known that the semantic meaning of the search has         these studies, results indicate that there is a tendency for sac-
an impact on eye movement (Yarbus, 1967), but this informa-          cades to continue the trajectory of the previous saccade, but
tion is tricky to implement. That is why our semantic model          contrary to the foraging facilitator hypothesis of IOR, there is
takes into account the semantic similarity between a word            also a distinct population of saccades directed back to the pre-
and the goal of the search. This similarity is calculated by         vious fixation location. To capture the pattern of saccadic eye
LSA (Latent Semantic Analysis). This method, popularized             movements during scene viewing we need to model the dy-
by Landauer and Dumais (1997), takes a huge corpus as input          namics of visual encoding. That is why we implemented the
and yields a high-dimensional vector representation for each         Variable Memory Model instead of using the simple mecha-
word, usually about 300 dimensions. It is based on a singu-          nism of IOR.
lar value decomposition of a word × paragraph occurrence
matrix, which implements the idea that words occurring in            Variable Memory Model The memory part is based on the
similar contexts are represented by close vectors. Such a vec-       Variable Memory Model (VMM), developed by Arani et al.
tor representation is very convenient to give a representation       (1984) and revisited by Horowitz (2006). VMM is a non-
to sentences that were not in the corpus: the meaning of a           deterministic model originally designed to accomodate eye
sentence is represented as a linear combination of its word          movements data. It is based on 2 parameters: θ the proba-
vectors. Therefore, we can virtually take any sentence and           bility of encoding and ϕ the probability of recovering infor-
give it a representation. Once this vector is computed, we           mation in memory2 . When an item is attended, the location
can compute the semantic similarity between any word and             of that item could be encoded by the model, or not, depend-
this sentence, using the cosine function. The higher the co-         ing of the encoding probability θ. The second parameter, ϕ,
sine value, the more similar the words are. The results of           simulates a forgetting mechanism. In fact when a stimulus is
this method have been experimentally tested and are close to         encoded, it will be gradually deteriorated following the curve
the capabilities of human in judging of semantic similarities        drawn in Figure 2. When the memory weight is 1 the item
(Landauer, McNamara, Dennis, & Kintsch, 2007). Specif-               is perfectly remembered, when it is 0 there is no trace of this
ically in our model, our assumption is that elements which           item in memory. When an item is attended, its weight is 1 if
are spatially close are also semantically close, as it is often      it is encoded then progressively goes back to 0: at each step,
the case in reality, and it is the case by construction in our       its memory weight decreases as a function of the number of
stimuli. If a fixated word is semantically close to the goal,        intervening items.
elements nearby will receive a high weight. On the contrary              1 This value of 0.2 is usually considered in the LSA literature as a
if that word is far from the goal, elements close to it will re-     threshold under which items are unrelated.
ceive weak weight. The model will therefore tend to move                 2 A third parameter is used in the original model, representing the
away from the areas considered unrelevant. More precisely            probability of correctly identifying the target.
                                                                 2832

   At the ith fixation the model will remember or not that a
particular item was encoding during the kth fixation, as fol-
lowing:
                  MemoryWeight(i, k) = ϕi−k
The last point about this part is the importance of the meaning
of the stimulus: θ is modulated by semantics. If the stimulus
is really close (semantically similar) to the user’s goal, the
encoding is more difficult, because the treatment of this stim-
ulus takes more time and cognitive resources. In contrary, if
the stimulus has anything to do with the information search
the encoding will be easier. This point explains why when a
stimulus is interesting we sometimes need to go back to it. In
fact in the observed data there are more return on words se-
mantically close to the information seeking goal, than others.
Integration
Each part of the model (visual, semantic and memory) gener-          Figure 3: Example of image. The instruction is Find the
ates a map. On this map each location has a weight for this          biggest animal. The answer is whale (”baleine” in french)
component. The integration of maps to form the main map is
a weighted sum of these three components:
                                                                     on a 19 inch CRT monitor at a viewing distance of 50 cm.
     Mgeneral = αV .Mvisual + αS .Msemantic + αM .Mmemory ,          Participants were told to read the instruction, to fix a fixation
                                                                     cross and to view the display in order to find the best answer
                       αV + αS + αM = 1                              to the instruction, with no time limitation. Participants were
                                                                     presented 18 1024 x 768 pixels images (subtending 42 hori-
The weights αV , αS and αM are not fixed a priori. We are            zontal deg. of visual angle). These 2 experiments have the
instead looking at the respective role of each component.            same methods, but differ in one point: the visual features of
                                                                     the stimuli.
                        Experiments
In order to test the model, we will now present two exper-           Experiment 1 In the first experiment the visual feature is
iments, which enabled to compare the computational model             the font size of the stimuli. Each word has a font size be-
with experimental data. We formalized the goal the user is           tween 13 and 19, allocated in this way (19: 5 words, 18: 5
pursuing by considering that this user is seeking a particu-         words, 17: 6 words, 16: 7 words, 15: 6 words, 14: 6 words
lar piece of information. Our goal is to apply our model to          and 13: 5 words). They can be grouped into two classes: V+
complex Web pages, but before that we tested it on a simpler         (size 18-19) and V- (size 13-17). There are three visual con-
task. The user is asked to find the best answer to a question.       ditions which are (1) random assignments of visual features
40 words are spread on the display, and the target is defined        to words; (2) no visual features at all; (3) visual features con-
by the class it belongs to and its specific features within this     gruent to spatial locations: words that are close to the target
class. For instance, the instruction could be: Find the biggest      have bigger font size. In this experiment the visual feature is
animal (class: animal and feature: big), as in Figure 3. Each        multi-varied and gradual. 43 students of Grenoble University
of the 40 words of each image has a visual feature and a se-         participated (30 female; mean age = 20.9 years). All par-
mantic feature. We also organized the 40 words in order to           ticipants had normal or corrected to normal vision and were
reproduce the fact that in our world objects that are semanti-       naı̈ve with respect to the purposes of the study.
cally similar are also often close to each other; in supermar-       Experiment 2 In the second experiment the visual feature
ket, vegetables are in the same place and they are close to          is the color of the stimuli. Each word is black or red (black:
fruits. In each image, seven words, including the target word,       30 words, red: 10 words). There are then two classes: V+
belong to the same category (i.e. category of the instruction,       (red) and V- (black). There are two visual conditions which
in our example animal), and all 33 other words are of de-            are (1) random assignments of color; (2) visual features con-
creasing semantic similarity with the instruction. To calculate      gruent to spatial locations: words that are close to the target
these similarities we used the LSA method, briefly described         are red. In this experiment the visual feature is bimodal and
above.                                                               dichotomic. 29 students of Grenoble University participated
Methods                                                              (14 female; mean age = 21.2 years). All participants had nor-
                                                                     mal or corrected to normal vision and were naı̈ve with respect
In both experiments, eye movements were monitored by an              to the purposes of the study.
SR Research Eyelink 2 eyetracker. Viewing was binocular,
but only one eye was tracked. The images were presented
                                                                 2833

                                                                   Figure 5: Theoritical and observed percentage of fixations
                                                                   depending on visual features
Figure 4: Number of words and rate of progression saccades
according to visual conditions for both experiments                ously described on these quite different tasks.
                                                                         Comparisons between model and human
Results                                                                                      scanpaths
We have here two different experiments, with different results     Weights combination
and we will see if the model can account for both data. In         We have done 50 iterations of the model for each experiment,
both experiments we identified three features that allow us to     like a simulation of 50 participants. The total number of fix-
characterize the performance of participants and to compare        ations was fixed for each image and equals to the average
them afterwards with model data. The selected features are         number of fixations made by the participants who saw this
the number of words seen before reaching the target, the rate      picture. We made these simulations for all combinations of
of progression saccades, i.e. the number of fixations closer to    αV , αS and αM from 0 to 1 with 0.05 steps. For the exper-
the target than the previous one divided by the number of all      iment with the factor color we also varied the weight of the
fixations and the angles in the scanpath. These features were      red words for the visual map (representing the visual saliency
chosen for their discriminating power between the different        of the words). We also varied the parameters of the memory
conditions of the experiment. We are interested in the values      model ϕ and θ, to compare them thereafter with data from the
of all these features for the last 6 maps seen by the partici-     literature.
pants, once they were well aware of the task and the semantic         To select the best parameters of the model we compared the
organization of words on the display.                              average relative errors between the model and participants for
   The difference between the 2 experiments was the visual         the two features described above, the number of words seen
factor. The results show (see Figure 4) gains in performance       before the target and the rate of progression saccades. The
for the experiment color that are not in experiment size be-       average angle of the scanpath gives us similar results that we
tween the two visual conditions (colored words help, but big       do not present here. We took into account for each feature the
words do not). For the feature number of words: T(17)=2.06,        10 best combinations, to average the parameters. In fact the
p=0.055 against T(17)=0.96, p=0.349 and for the feature            difference in the relative errors are too weak to only take the
rate of progression saccades T(17)=4.07, p≤0.001 against           best one. These results are shown in Table 1.
T(17)=0.73, p=0.474.
   To see why there are differences in performance between                      Table 1: Best combination of weights
the 2 experiments, and especially, why the factor size does
not affect performance unlike the factor color, we looked at                     Experiment size           Experiment color
whether the words in color were over fixated compared to the                  Rate prog Nb words         Rate prog Nb words
words in black and similarly if the biggest words were more            αV         0,4         0,34         0,38         0,31
fixated than smaller words (in random visual condition) Re-            αM        0,50         0,62         0,45         0,58
sults show (Figure 5) that the words with more salient visual           αS       0,11         0,05         0,18         0,12
features are not more fixated than others. There is no sig-             θ        0,83         0,84         0,83         0,85
nificant difference between observed and theoretical values             ϕ        0,84         0,86         0,84         0,86
(factor size: χ2 = 1,26, dl = 1, p = 0.26 and factor color: χ2
= 2.36, dl = 1, p = 0.12). The gradualness of factor size is
certainly the reason for the difference previously observed.          We can first notice that the best parameters are very close
   We will now see what is the behavior of the model previ-        for the 2 experiments, i.e. the model is suitable for 2 types of
                                                               2834

                                                                                                                                                                                                       25
                                                                                                                                                                     Percentage of all fixations (%)
                                                                                                                                                                                                                                                                                                    Model
                                                                                                                                                                                                       20                                                                                           Participants
                                                                                                                                                                                                       15
                                                                                                                                                                                                       10
                                                                                                                                                                                                        5
                                                                                                                                                                                                        0
                                                                                                                                                                                                            0    5             10            15         20           25                30           35             40
                                                                                                                                                                                                                                              Saccade amplitude (degre)
                                                                                                                                                                                                       15
                                                                                                                                                                     Percentage of all fixations (%)
                                                                                                                                                                                                       10
                                                                                                                                                                                                        5
Figure 6: Number of words seen before the target for model
and observed data                                                                                                                                                                                       0
                                                                                                                                                                                                            0   20        40            60          80         100          120         140         160            180
                                                                                                                                                                                                                                    Angular deviation from previous fixation (degre)
                                                                                                                                                                                                       10
                                                                                                                                                                     Percentage of all fixations (%)
                                                                                                                                                                                                        8
visual feature, bimodal (color) or gradual (size). If we now                                                                                                                                            6
look at the meaning of these values the most important weight                                                                                                                                           4
is for the memory component, which enables the model to re-                                                                                                                                             2
                                                                                                                                                                                                        0
member where it has already been. The visual component                                                                                                                                                      0        50         100            150           200
                                                                                                                                                                                                                                           Saccade absolute angle (degre)
                                                                                                                                                                                                                                                                              250             300            350
also plays an important role, because it takes into account the
visual acuity and guide the attention on the closest words, as
human do. Finally the semantic part is less important, espe-
cially in the first experiment. Second the parameters θ and ϕ                                                                                                 Figure 8: Comparisons between model and data on the exper-
are also close to those found in literature which are: θ=0,82                                                                                                 iment color
et ϕ=0,86 (Horowitz, 2006).
   Finally looking at the distinction between images with red
                                                                                                                                                              Angle and saccade distributions
words around the target and images with red words randomly
displayed both models and participants have differences in                                                                                                    To have a more complete comparison between humans and
performances, as shown for example in Figure 6 for the fea-                                                                                                   model, we studied two distributions which are typical of hu-
ture number of words.                                                                                                                                         man scanpath: saccade length distribution, and angle distribu-
                                                                                                                                                              tions (both relative and absolute angles). A relative angle is
                                                                                                                                                              an angle between two saccades, an absolute angle is between
                                         25
                                                                                                                                                              a saccade and the horizontal line.
       Percentage of all fixations (%)
                                         20
                                                                                                                                      Model
                                                                                                                                      Participants               Figure 7 describes the first experiment comparisons and
                                         15                                                                                                                   Figure 8 the second ones. There is a good fit of the data for
                                         10                                                                                                                   the saccade amplitude distribution curve (upper panel of both
                                          5
                                                                                                                                                              figures) with a peak at about 4 degrees of saccade amplitude
                                          0
                                              0    5             10            15         20           25
                                                                                Saccade amplitude (degre)
                                                                                                                         30           35             40       in all cases.
                                         15
                                                                                                                                                                 In the case of relative angles the results show a curve for
       Percentage of all fixations (%)
                                                                                                                                                              participants with more return saccades (0 ˚ ) and forward sac-
                                         10
                                                                                                                                                              cades (180 ˚ ), same results that those found by Tatler and
                                          5
                                                                                                                                                              Vincent (2008). For the model, the curve is more horizon-
                                                                                                                                                              tal (middle panel of both figures).
                                          0
                                              0   20        40            60          80         100          120
                                                                      Angular deviation from previous fixation (degre)
                                                                                                                          140         160            180         The values that we get in the case of absolute angles are
                                         10
                                                                                                                                                              very interesting. They show very clearly an horizontal trend
       Percentage of all fixations (%)
                                          8
                                                                                                                                                              (peaks at 0 ˚ and 180 ˚ ) rather than vertical (small peaks at
                                          6                                                                                                                   90 ˚ and 270 ˚ ) for both humans and model.
                                          4                                                                                                                      χ2 tests give us no significant differences for each compar-
                                          2
                                                                                                                                                              ison (all p≥0.9), meaning the model distributions are similar
                                          0
                                              0        50         100            150           200
                                                                             Saccade absolute angle (degre)
                                                                                                                250             300            350            to human’s ones for experiments 1 and 2.
                                                                                                                                                                                                                                        Conclusion
Figure 7: Comparisons between model and data on the exper-                                                                                                    We have considered two experiments in order to validate a
iment size                                                                                                                                                    model. This model takes into account both semantic and
                                                                                                                                                              visual information, associated with a model of visual mem-
                                                                                                                                                              ory. There are many parameters we had to determined, and
                                                                                                                                                           2835

to do that we tried first of all to make cognitively plausible       Itti, L., & Koch, C. (2000). A saliency-based search mecha-
choices. In fact the visual part takes into account both visual         nism for overt and covert shifts of visual attention. Vision
human acuity and the saliency of the stimuli. This saliency             Research, 40, 1489-1506.
is here really simple because of the simplicity of the stimuli,      Itti, L., & Koch, C. (2001). Computational modeling of visual
but could be more complex if necessary. For the semantic                attention. Nature Reviews Neuroscience, 2, 194-203.
part we used a well known method and theory which pro-               Klein, R. M., & MacInnes, J. W. (1999). Inhibition of re-
vides us a good measure of similarity between the aim of the            turn is a foraging facilitator in visual search. Psychological
information search and the items. Finally the memory com-               Science, 10, 346–352.
ponent, which is the most important according to the results         Landauer, T., & Dumais, S. (1997). A Solution to Plato’s
enables the model to remember items previously seen and to              Problem: The Latent Semantic Analysis Theory of Acqui-
forgetting them without being too strict, unlike a classical in-        sition, Induction, and Representation of Knowledge. Psy-
hibition of return, which is generally used in such a model.            chological Review, 104, 211–240.
In fact it is more realistic, because humans often go back to        Landauer, T., McNamara, D., Dennis, S., & Kintsch, W.
the previously fixated item, as shown in the relative angles            (Eds.). (2007). Handbook of latent semantic analysis.
distribution.                                                           Lawrence Erlbaum Associates.
   We chose two different scales for visual saliency of stimuli:     Marchionini, G. (1997). Information seeking in electronic
bimodal for color and linear for size. The color is not linear,         environments (Cambridge series on Human-Computer in-
the reverse would have been difficult to control on effects of          teraction). Cambridge University Press.
saliency. This choice allowed us to see differences in perfor-       Motter, B. C., & Belky, E. J. (1998). The guidance of eye
mance, and even if the two experiments are not similar, the             movements during active visual search. Vision Research,
best parameters for the model are almost the same, meaning              38(12), 1805-1815.
that this model is robust to, at least, such a change.               Navalpakkam, V., & Itti, L. (2005). Modeling the influence
   In further experiments with text paragraphs instead of sin-          of task on attention. Vision Research, 45, 205-31.
gle words, more similar to Web pages, we will test this model        Pirolli, P., & Fu, W. (2003). SNIF-ACT: a model of infor-
again, to see if in a new task the weight of these components           mation foraging on the world wide web. In P. Brusilovsky,
are always the same or not. This model, able to explain simple          A. Corbett, & F. de Rosis (Eds.), User modeling 2003, 9th
stimuli, tends to be more complex, integrating for example a            International Conference, UM 2003 (Vol. 2702, p. 45-54).
reading model.                                                          Johnstown, PA: Springer-Verlag.
                                                                     Tatler, B. W., & Vincent, B. T. (2008). Systematic tenden-
                    Acknowledgements                                    cies in scene viewing. Journal of Eye Movement Research,
                                                                        2(2):5, 1-18.
We would like to thank Gelu Ionescu for providing us
                                                                     Treisman, A., & Gelade, G. (1980). A feature-integration
LisEyeLink software; and Nicolas Betton for his work dur-
                                                                        theory of attention. Cognitive Psychology, 12, 97-136.
ing the experiment. We also thank participants who accepted
                                                                     Yarbus, A. L. (1967). Eye movements and vision. New York:
to pass the experiment.
                                                                        Plenum Press.
                                                                     Zelinsky, G. J., Zhang, W., Yu, B., Chen, X., & Samaras, D.
                          References
                                                                        (2006). The role of top-down and bottom-up processes in
Arani, T., Karwan, M. H., & G., D. C. (1984). A variable-               guiding eye movements during visual search. In Y. Weiss,
   memory model of visual search. Human factors, 26, 631-               B. Scholkopf, & J. Platt (Eds.), Advances in neural infor-
   639.                                                                 mation processing systems (Vol. 18, p. 1609-1616). Cam-
Chanceaux, M., Guérin-Dugué, A., Lemaire, B., & Baccino,              bridge, MA: MIT Press.
   T. (2008). Towards a model of information seeking by in-
   tegrating visual, semantic and memory maps. In Proceed-
   ings of the 4th international cognitive vision workshop (pp.
   65–78). Lecture Notes in Computer Science 5329, Berlin:
   Springer Verlag.
Geisler, W. S., & Perry, J. S. (2002). Real-time simulation of
   arbitrary visual fields. In Proceedings of the 2002 sympo-
   sium on eye tracking research & applications (pp. 83–87).
   ACM New York, NY, USA.
Hooge, I. T., Over, E. A., Wezel, R. J. van, & Frens, M. A.
   (2005). Inhibition of return is not a foraging facilitator
   in saccadic search and free viewing. Vision Research, 45,
   1901–1908.
Horowitz, T. (2006). Revisiting the variable memory model
   of visual search. Visual Cognition, 14, 668–684.
                                                                 2836

