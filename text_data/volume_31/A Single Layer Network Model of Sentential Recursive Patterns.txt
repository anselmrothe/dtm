                                                                         2. Cross-serial recursion (derived from identity recursion):
                                                                             e.g., Ns N pVsVp       the boy girls runs like
                                                                         3. Center-embedding recursion (derived from mirror recur-
                                                                             sion)
                                                                             e.g., Ns N pVpVs       the boy girls like runs
                                                                          Therefore, |V | = 4 and K = 2 (only verbs are predicted). As
                                                                          an example of calculating lexical contexts, for the following
                                                                          center embedded sentence
                                                                                   Ns              Np             Vp      Vs
                                                                               z }| { z            }|      { z }| { z}|{
                                                                               T he sea the mountains overlook is blue,
 Figure 1: A schematic view of our network architecture with
 a single bank described in text. The left lexical context shown          the left lexical context vector centered at “is” for Ns , N p , Vs
 here is centered at “Mary”. “SS” and “EE” refer to the start             and Vp is: [γe−3γ , γe−2γ , 0, γe−γ ].
 and the end of a sentence respectively.                                     The rest of this paper is organized as follows: We introduce
                                                                          the pattern subspaces occupied by certain context vectors and
                                                                          their dimensionality in the next section, followed by a formal
 word while assigning more importance to those closer to it.
                                                                          characterization of learnability. We then present our compu-
    Specifically, let a sentence be w1 w2 w3 · · · wi · · · wn−1 wn ,
                                                                          tational results on the learnability and relative hardness of a
 where wi is a word and n is the total length. Then the left lex-
                                                                          single level of recursion (Ns NsVsVs ), and of two levels of re-
 ical context vector x for wi is the following |V |-dimensional
                                                                          cursion (Ns Ns NsVsVsVs ). We conclude by summarizing the
 vector, where |V | is the vocabulary size, and the jth word in
                                                                          results and comparing them to human performance.
 the vocabulary is V j :
                                                                                 Pattern Subspaces and Dimensionality
                     |V |  i−1
                                     −γ(i−k)                              For a single level of recursion, center embedding patterns are:
                     ∑ ∑          γe         b j,                (1)
                     j=1 k=1
                          wk =V j                                                               Ns NsVsVs , Ns N pVpVs ,
                                                                                              N p NsVsVp , N p N pVpVp .
 in which b j is a unit basis vector in the jth dimension, which
 has 1 in the jth dimension and 0 elsewhere.                              Noting that only the verbs are predictable, we generate the ex-
    Outputs of the network are produced by multiplying the                ponential distribution weighted left lexical context vectors at
 input vector x (of |V | dimensions) with the learned weight              both the verbs for each pattern, leading to a total of 8 context
 matrix W (of size |V | × K), and applying a multinomial logit            vectors. The same is true for cross serial and right branch-
 function (Hastie, Tibshirani, & Friedman, 2001), which ren-              ing patterns with one level of recursion. For double center
 ders the output y a probability distribution over the K word             embedding, the patterns are:
 types:
                                                                                          Ns Ns NsVsVsVs , Ns Ns N pVpVsVs ,
                                   ex·Wi                                                 Ns N p NsVsVpVs , Ns N p N pVpVpVs ,
                        yi = K x·W ,                             (2)
                               ∑ j=1 e j                                                 N p Ns NsVsVsVp , N p Ns N pVpVsVp ,
                                                                                       N p N p NsVsVpVp , N p N p N pVpVpVp .
 where Wi is the ith column of the matrix W . An important ad-
 vantage of restricting our attention to a single layer network is        We generate the left lexical context vectors at all the verbs for
 that it allows us to analyze the learnability of different gram-         each pattern, leading to a total of 24 vectors. Similarly, we
 matical patterns in terms of the linear separability of the input        generate 24 vectors for the cross serial and right branching
 patterns.                                                                cases. We use Di to refer to the set of vectors for i ∈ {r, s, c}
                                                                          denoting double right branching, cross serial and center em-
              Sentential Recursive Patterns                               bedding respectively (e.g. Dc refers to the set of 24 lexical
 We systematically study the left lexical context vectors de-             context vectors from double center embedding).
 rived from three common types of recursive structures found                 Despite the fact that all the members in Dr , Ds and Dc are
 in human languages (where N p and Ns stand for a plural and              4 dimensional, an analysis with singular value decomposition
 a singular noun respectively, and Vp and Vs stand for a plural           (SVD, Hastie et al., 2001) reveals a lower intrinsic dimen-
 and a singular verb respectively), namely:                               sionality. Table 1 summarizes our findings. Similar results on
                                                                          dimensionality are observed for the patterns of a single level
1. Right-branching recursion:                                             of recursion. Specifically, the set of right branching vectors
    e.g., N pVp NsVs      girls like the boy that runs                    lie on a three-dimensional subspace (hyperplane), as do cross
                                                                      462

              [NVNV] right branching vectors                 [NNVV] cross serial vectors                      [NNVV] center embedding vectors
                                                   Vs                                            Vs                                              Vs
                                                   Vp                                            Vp       0.2                                    Vp
        0.1                                             0.2                                               0.1
                                                        0.1                                                 0
          0                                               0                                             −0.1
                                                       −0.1
       −0.1                                                                                             −0.2
                                             0.5                                          0.5            0.2
        0.2                                            −0.2
                                                        0.5
                   0                  0                                              0                            0                          0.5
                                                                 0                                                                 0
                         −0.2 −0.5                                     −0.5 −0.5                                     −0.2 −0.5
            [NVNVNV] right branching vectors                [NNNVVV] cross serial vectors                  [NNNVVV] center embedding vectors
                                                   Vs                                            Vs                                              Vs
                                                   Vp                                            Vp                                              Vp
        0.2                                             0.5                                              0.5
        0.1
          0                                               0                                                 0
       −0.1
       −0.2                                            −0.5                                             −0.5
        0.2                                             0.5                                              0.5
                                             0.5                                          0.5                                                0.5
                 0                                             0                                                   0
                                    0                                             0                                                 0
                     −0.2 −0.5                                    −0.5 −0.5                                            −0.5 −0.5
Figure 2: The three dimensional representations of the lexical context vectors from one level (top) and two levels (bottom) of
recursion with the decay parameter being 0.5.
                            Set              Card.   Dim.                         In addition, for the inseparable cases, we present an approach
                            Dr                  24    3                           to identify the patterns that break the separability.
                            Ds                  24    3
                            Dc                  24    3                           Linear Separability. A survey of linear separability tests is
                        Dr ∪ Ds                 48    4                           provided by Elizondo (2006). We use a simple method: Let
                        Dr ∪ Dc                 48    4                           D1 and D2 be the matrices of row vectors for the two classes
                 Ds ∪ Dc (γs = γc )             48    3                           respectively, and 1 be a vector of ones. A is defined as:
                 Ds ∪ Dc (γs 6= γc )            48    4                                                          µ                  ¶
                    Dr ∪ Ds ∪ Dc                72    4                                                                D1       1
                                                                                                          A=                           .
                                                                                                                      −D2 −1
Table 1: Cardinality and dimensionality of context vectors. γs
and γc are the decay parameters for Ds and Dc respectively.                       Linear separability is equivalent to {x|Ax > 0} is nonempty.
                                                                                  Let p = (1, 1, · · · , 1)T , α ∈ R. A linear program for testing
                                                                                  linear separability is:
serial vectors and center embedding vectors (see Figure 2).
Thus, when we look at the linear separability of those lexi-                                                    min         α
                                                                                                                x,α
cal context vectors, it is equivalent to examine visually their
                                                                                               subject to        Ax + αp ≥ 1, α ≥ 0.                (3)
3D subspaces. Collectively, cross serial and center embed-
ding vectors with the same decay parameter lie on the same                        Then the original sets D1 and D2 are linearly separable ⇐⇒
hyperplane. However, the right branching vectors exist on a                       The optimal α∗ = 0. The linear program is efficiently solved
different hyperplane.                                                             by using the Simplex method (Dantzig, 1963).
                Characterizing Learnability
Our setting for characterizing learnability is that we have two                   Minimum Interclass Distance. The minimum distance be-
classes of vectors D1 and D2 (in our case left lexical context                    tween two classes of vectors is defined as:
vectors of Vs and Vp ). We want to see if we can find a hyper-
plane that separates points in the two sets, and when they are                                          dm =         min     dist(x, y),            (4)
                                                                                                                 x∈D1 ,y∈D2
separable, the minimum interclass distance indicates how far
apart the two classes are in the space and also might indicate                    which characterizes how far the two classes are apart given
how easy it is to find a hyperplane that separates the classes.                   that they are linearly separable.
                                                                           463

        Minimum interclass distance
                                      0.5
                                      0.4
                                      0.3                         Right branching
                                                                  Center embedding
                                      0.2
                                      0.1
                                       0
                                            0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9      1
                                                       Decay parameter
Figure 3: The minimum interclass distance with respect to                                      Figure 4: A schematic view of our model with two banks for
the decay parameter for a single level of recursion (4 words).                                 inputs. The left lexical context shown is centered at “Mary”.
Which Patterns Break the Separability? To gain addi-                                                                                      0.7
tional insight into the model we can examine which patterns
                                                                                                            Minimum interclass distance
                                                                                                                                          0.6
are responsible for preventing the pattern sets from being lin-                                                                           0.5
                                                                                                                                          0.4
early separable. We invoke linear Support Vector Machines                                                                                 0.3
(SVMs, Cortes & Vapnik, 1995), which are now a set of well-                                                                               0.2
studied techniques for regression and classification in the field                                                                         0.1
                                                                                                                                           0
of machine learning. The simplest form of SVMs for separat-                                                                                1
                                                                                                                                                                                                     1
ing context vectors is:                                                                                                                            0.5
                                                                                                                                                                                0.4
                                                                                                                                                                                       0.6
                                                                                                                                                                                              0.8
                                                                                                                                                                         0.2
                                                                                                                           Decay parameter γ1                   0   0
                                                 1                                                                                                                              Decay parameter γ2
                                              min kwk2 +C ∑ ξi
                                                 2        i
                                subject to w · xi − b ≥ 1 − ξi , i ∈ Vp                        Figure 5: The minimum interclass distance with respect to the
                                                                                               decay parameters. From top down, the surfaces are for right
                                          w · xi − b ≤ −1 + ξi , i ∈ Vs                        branching, cross serial and center embedding respectively.
                                                               ξi ≥ 0,
where xi ’s are the lexical context vectors, w is the linear                                   Two Banks of Inputs
weight vector, b is the bias and ξi ’s are the slack variables                                 Unlike in slot-based networks, where the inputs from one set
for the inseparable case. This SVM strives for a separating                                    of units are transferred into adjacent slots as time progresses,
hyperplane with a reasonably large C = 107 for instance. If                                    in our model the two banks operate independently (see Figure
there exists no hyperplane that can split the examples from                                    4). Representations are never transferred between the banks.
two classes, it will choose a hyperplane that splits the exam-                                    With two input banks, the outputs of the network are pro-
ples as cleanly as possible, while still maximizing the dis-                                   duced by multiplying the input vector (of 2|V | dimensions),
tance between the nearest cleanly split examples. Therefore,                                   which is the concatenation of x1 (with a decay parameter γ1 )
to find out which patterns break the separability, we can run                                  and x2 (with a decay parameter γ2 ), with the learned weight
the SVM on the context vectors and identify those vectors                                      matrix W (of size 2|V | × K), and applying a multinomial logit
that the SVM makes a mistake on.                                                               function on the products, which does the following:
                                       Results on Recursive Patterns                                                                                        e[x1 ,x2 ]·Wi
                                                                                                                                                yi =                            ,                        (5)
As plotted in Figure 2, for one level of recursion the right                                                                                             ∑Kj=1 e[x1 ,x2 ]·W j
branching and center embedding vectors are linearly separa-
ble, while cross serial vectors are not, for decay parameters                                  where [x1 , x2 ] represents the vector formed by concatenating
between 0.01 to 1. The change of the minimum interclass dis-                                   x1 and x2 , and Wi is the ith column of the matrix W .
tance of right branching and center embedding vectors with                                        For a single level of recursion, we find that when using two
respect to the decay parameter is plotted in Figure 3. For                                     banks of inputs with the decay parameters γ1 and γ2 , cross
two levels of recursion, right branching vectors are separable,                                serial vectors are linearly separable except when γ1 = γ2 , and
while the other two kinds of vectors are not (see Figure 2).                                   right branching and center embedding vectors are always sep-
   Clearly, the pattern of results differs from the empirical                                  arable, for 0.01 ≤ γ1 ≤ 1 and 0.01 ≤ γ2 ≤ 1. We then turn
data in important ways. In particular, the cross serial patterns                               to the comparison of their minimum interclass distances. As
are not separable even for a single level of recursion. Con-                                   plotted in Figure 5, the minimum interclass distance of right
sequently, we augmented the model by adding another set of                                     branching vectors is larger than that of cross serial vectors,
input units, which operated in the same fashion as the first set                               which is larger than or equal to that of center embedding vec-
except that the decay rate was varied indepedently.                                            tors, given the same decay parameters. Our results suggest
                                                                                         464

                             Cross serial (white: separable)                                            Center embedding (white: separable)                                                              Cross serial (inseparable configurations plotted)
                      1                                                                            1
                     0.9                                                                          0.9
                     0.8                                                                          0.8
Decay parameter γ1
                                                                                          1
                                                                              Decay parameter γ
                     0.7                                                                          0.7                                                                                  1
                     0.6                                                                          0.6                                                                                0.9
                                                                                                                                                                                     0.8
                                                                                                                                                              Decay parameter γ3
                     0.5                                                                          0.5
                                                                                                                                                                                     0.7
                     0.4                                                                          0.4                                                                                0.6
                     0.3                                                                          0.3                                                                                0.5
                                                                                                                                                                                     0.4
                     0.2                                                                          0.2
                                                                                                                                                                                     0.3
                     0.1                                                                          0.1                                                                                0.2
                                                                                                                                                                                     0.1
                           0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1                                        0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
                                   Decay parameter γ                                                            Decay parameter γ                                                    1
                                                                      2                                                             2
                                                                                                                                                                                      0.9
                                                                                                                                                                                         0.8                                                                                              1
                                                                                                                                                                                            0.7                                                                                     0.9
                                                                                                                                                                                               0.6                                                                            0.8
                                                                                                                                                                                                  0.5                                                                  0.7
                               Minimum interclass distance
                                                                                                                                                                                                     0.4                                                         0.6
                                                                                                                                                                                                        0.3                                               0.5
                                                              0.2                                                                                                                                                                                   0.4
                                                                                                                                                                                                           0.2                                0.3
                                                                                                                                                                                                              0.1                       0.2
                                                                                                                                                                                                                                  0.1
                                                                                                                                                                                   Decay parameter γ2                                                Decay parameter γ1
                                                             0.15
                                                                                                                                                                                                       Center embedding (inseparable configurations plotted)
                                                              0.1
                                                             0.05
                                                                                                                                                                                              1
                                                               0                                                                                                                            0.9
                                                               1
                                                                                                                                                                       Decay parameter γ3
                                                                                                                                                                                            0.8
                                                                                                                                1                                                           0.7
                                                                    0.5                                                                                                                     0.6
                                                                                                               0.5                                                                          0.5
                                                                          0                                                                                                                 0.4
                              Decay parameter γ1                               0
                                                                                                            Decay parameter γ2                                                              0.3
                                                                                                                                                                                            0.2
                                                                                                                                                                                            0.1
                                                                                                                                                                                              0.1
Figure 6: Top: linear separability as a function of decay pa-                                                                                                                                    0.2
                                                                                                                                                                                                    0.3
                                                                                                                                                                                                       0.4
                                                                                                                                                                                                          0.5                                                               0.3
                                                                                                                                                                                                                                                                                  0.2
                                                                                                                                                                                                                                                                                        0.1
                                                                                                                                                                                                             0.6                                                      0.4
rameters (white: separable). Bottom: the minimum interclass                                                                                                                                                     0.7
                                                                                                                                                                                                                   0.8
                                                                                                                                                                                                                      0.9                     0.8
                                                                                                                                                                                                                                                    0.7
                                                                                                                                                                                                                                                          0.6
                                                                                                                                                                                                                                                                0.5
                                                                                                                                                                                                                            1
distance of cross serial and center embedding vectors in the                                                                                                                                      Decay parameter γ2
                                                                                                                                                                                                                                  1
                                                                                                                                                                                                                                        0.9
                                                                                                                                                                                                                                                           Decay parameter γ1
parameter region (shown as white) where both of them are
linearly separable. The top surface segments whose heights
show the minimum interclass distance are those of cross se-                                                                                           Figure 7: The inseparable configurations with three banks
rial and the bottom ones are those of center embedding.                                                                                               of inputs for two levels of recursion plotted as blue points.
                                                                                                                                                      Top: cross serial patterns; Bottom: center embedding pat-
                                                                                                                                                      terns. Down-sampling of the whole set of configurations is
that center embedding is the hardest form of recursive struc-                                                                                         used for better visualization.
ture among the three being studied.
   The results on two levels of recursion are plotted in Fig-
                                                                                                                                                      symbols denote the location at which a context vector is gen-
ure 6, which shows the relative hardness of cross serial and
                                                                                                                                                      erated):
center embedding both from linear separability and from the
minimum interclass distance when both of them are linearly                                                                                                   a = N p Ns N p VpVsVp , b = Ns N p Ns VsVpVs .
separable. In the figure, the separable parameter regions are
colored white, and the inseparable ones black. We find that in                                                                                        The removal of the above vectors renders the set of context
the whole parameter space (0.01 ≤ γ1 ≤ 1, 0.01 ≤ γ2 ≤ 1), the                                                                                         vectors separable, for the two decay parameters ranging from
minimum interclass distance of cross serial vectors is always                                                                                         0.01 to 1, except when the two parameters are identical.
larger than or equal to that of center embedding vectors given                                                                                          For two-level center embedding, the breaker vectors are:
the same (γ1 , γ2 ) pair. On the other hand, right branching
vectors are always separable. Again, we can see that within                                                                                                  a = Ns N p NsVs VpVs , b = N p Ns N pVp VsVp ,
this parameter region, center embedding is the hardest form                                                                                                  c = Ns Ns N p VpVsVs , d = N p N p Ns VsVpVp .
of structure among the three being examined.
                                                                                                                                                      Similarly, the removal of the above vectors renders the set
                                                                                                                                                      separable, for the same parameter region as above.
Patterns that Break the Separability. Using the afore-
mentioned SVMs, we are able to identify the patterns that                                                                                             Three Banks of Inputs
break the separability for two-level cross serial and center em-                                                                                      We also study the linear separability of patterns with three
bedding structures. We define the breakers for linear separa-                                                                                         banks of inputs. Similar to Eqn. (5), the input vector here is
bility to be the minimal set of vectors whose removal renders                                                                                         the concatenation of three individual context vectors with γ1 ,
the previously inseparable set linearly separable, which are                                                                                          γ2 and γ3 as the decay parameters respectively.
the same as those misclassified vectors reported by the SVM                                                                                              Our results on two levels of recursion, as shown in Figure
running at the decay parameters right next to the separable                                                                                           7, suggest that the inseparable configurations are all caused
region (see Figure 6).                                                                                                                                by the degenerate cases, where at least two of the decay pa-
   For two-level cross serial, the breakers are (where bold face                                                                                      rameters are the same. Other than those cases, three banks
                                                                                                                                                465

of inputs are systematically able to separate the cross serial       Chomsky, N. (1957). Syntactic structures. Mouton: The
and center embedding patterns with two levels of recursion.            Hague.
Two-level right branching patterns are always separable with         Christiansen, M. H., & Chater, N. (1999). Towards a connec-
three banks.                                                           tionist model of recursion in human lingusitic performance.
                                                                       Cognitive Science, 23, 157–206.
                           Conclusions                               Cortes, C., & Vapnik, V. (1995). Support-vector networks.
We have presented a single layer network architecture to ac-           Machine Learning, 20(3), 273–297.
count for people’s ability to understand recursive structures        Dantzig, G. B. (1963). Linear programming and extensions.
in language. Rather than posit separate slots or employing a           Princeton, NJ: Princeton University Press.
recurrent network, we propose that sequence information is           Elizondo, D. A. (2006). The linear separability problem:
retained using a simple decay mechanism (c.f. ordinal mod-             Some testing methods. IEEE Transactions on Neural Net-
els of serial order such as the primacy model (Page & Norris,          works, 17(2), 330–344.
1998)). One might imagine that such a mechanism would                Elman, J. L. (1991). Distributed representations, simple
ensue as the firing rates of populations of neurons decreased          recurrent networks and grammatical structure. Machine
as a function of the time since the corresponding word was             Learning, 7, 195–225.
presented.                                                           Hastie, T., Tibshirani, R., & Friedman, J. (2001). The ele-
   By restricting ourselves to the single layer case, we have          ments of statistical learning. New York: Springer.
been able to provide a more precise analysis than would oth-         Page, M., & Norris, D. (1998). The primacy model: A new
erwise be possible. The main separability results are summa-           model of immediate serial recall. Psychological Review,
rized in Table 2.                                                      105(4), 761–781.
                                                                     Pollack, J. B. (1988). Recursive auto-associative memory:
         # of levels (c.s.)  1-bank    2-bank     3-bank               Devising compositional distributed representations. In Pro-
                 1            never     always    always               ceedings of annual meeting of the cognitive science soci-
                 2            never    depends    always               ety.
                                                                     Reich, P. (1969). The finiteness of natural language. Lan-
         # of levels (c.e.)  1-bank     2-bank    3-bank               guage, 45, 831–843.
                 1           always     always    always             Wiles, J., & Elman, J. L. (1995). Learning to count without a
                 2            never    depends    always               counter: A case study of synamics and activation landcapes
                                                                       in recurrent networks. In Proceedings of annual meeting of
Table 2: Separability of cross serial (top) and center embed-          the cognitive science society.
ding (bottom) patterns. “Always” means true for all the non-
degenerative parameters tested, “never” means false for all
the parameters tested, and “depends” means depending on the
decay parameters used.
   The decaying representation has a number of useful proper-
ties for accounting for human performance with recursive pat-
terns. Specifically, with two banks of inputs, for both one and
two levels of recursion, our model can rank the three kinds of
recursion in an increasing order of hardness as: right branch-
ing, cross serial and center embedding, which is consistent
with the results established by Bach et al. (1986).
                            References
Bach, E., Brown, C., & Marslen-Wilson, W. (1986). Crossed
   and nested dependencies in German and Dutch: A psy-
   cholinguistic study. Language and Cognitive Processes,
   1(4), 249–262.
Bridle, J. S. (1990). Probabilistic interpretation of feedfor-
   ward classification network outputs with relationships to
   statistical pattern recognition. In F. F. Soulie & J. Herault
   (Eds.), Neurocomputing: Algorithms, architectures and ap-
   plications (p. 227-236). Springer-Verlag.
Chalmers, D. J. (1990). Syntactic transformations on dis-
   tributed representations. Connection Science, 2, 53–62.
                                                                 466

