UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Multimodal Nature of Embodied Conversational Agents
Permalink
https://escholarship.org/uc/item/6078d8kn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Benesh, Nick
Jeauniaux, Patrick
Louwerse, Max
et al.
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                     The Multimodal Nature of Embodied Conversational Agents
                                           Max Louwerse (mlouwerse@memphis.edu)
                                              Nick Benesh (nbenesh@memphis.edu)
                                         Shinobu Watanabe (swatanab@memphis.edu)
                                                Bin Zhang (bzhang@memphis.edu)
                                           Patrick Jeuniaux (pjeuniau@memphis.edu)
                                           Divya Vargheese (dvarghes@memphis.edu)
                                       Department of Psychology / Institute for Intelligent Systems
                                                          University of Memphis
                                                         Memphis, TN 38152 USA
                              Abstract                                  using a set of acoustic resonators and vibrating reeds, and
                                                                        Von Kemplen developed the first speaking machine that
  Embodied conversational agents (ECA’s) have become
  ubiquitous in human-computer interaction applications.                produced sound combinations. In the first part of the 20th
  Implementing humanlike multimodal behavior in these agents            century Jacques Vaucanson developed mechanical animated
  is difficult, because so little is known about the alignment of       objects like a flute-playing boy and a duck that could flap its
  facial expression, eye gaze, gesture, speech and dialogue act.        wings, eat, and digest grain.
  The current study used the data from an extensive study of              Obviously, lots of progress has been made, with today’s
  human face-to-face multimodal communication for the                   ECA’s being far more human-like than the older systems.
  development of a multimodal ECA, and tested to what extent
  multimodal behavior influenced the human-computer                     At the same time, today’s advanced embodied interfaces,
  interaction. Results from a persona assessment questionnaire          like their predecessors, have a limited use of the multimodal
  showed the presence of facial expressions, gesture and                aspects of communication. Even though some of today’s
  intonation had a positive effect on five assessment scales. Eye       systems have excellent speech interfaces (Pellom, Ward &
  tracking results showed facial expressions played a primarily         Pradhan, 2000), conversational skills (Graesser et al., 2004),
  pragmatic role, whereas intonation played a primarily                 gestural movements (Cassell, Kopp, Tepper, Ferriman, &
  semantic role. Gestures played a pragmatic or semantic role,
                                                                        Striegnitz, 2007), or mouth movements (Massaro, 2006),
  dependent on their level of specificity. These findings shed
  light on multimodal behavior within and between human and             they typically excel on just one aspect of multimodal
  digital dialogue partners.                                            communication. And even when human-like facial
                                                                        expressions and gestures are integrated in ECA’s, they are
   Keywords: embodied conversational agents, multimodal                 carefully guided by literature but otherwise intuitive (Baylor
   communication, avatars.
                                                                        & Kim, 2005), or come from actors acting out different
                                                                        modalities which are then transmitted to the agent, for
                         Introduction                                   instance by using body suits. The reason a full
Embodied conversational agents (ECA’s) are animated                     implementation of linguistic and paralinguistic channels of
characters that emulate human multimodal communication.                 communication naturally used by humans has not been
Such communication involves both linguistic (e.g., speech               realized so far is that relatively little is known about how
intonation, discourse structure) and paralinguistic (e.g.,              these channels combine within and across speakers in
facial expression, hand gestures, eye gaze) signals. There              human-human communication. Although evidence has been
has been a baby boom of these agents both in the virtual                collected on the alignment of pairs of modalities (e.g.
world as well as in the literature. Offspring are produced in           Gullberg & Homqvist, 2006; Thompson & Massaro, 1996),
departments of psychology, artificial intelligence, computer            few studies have investigated the associations between more
science and education: human-like (Graesser, et al., 2004)              than two modalities at a time.
and cartoon-like (Cassell et al., 2008); with anticipated                 In addition, it is questionable whether the development of,
careers in the military (Johnson et al., 2004), education               and research in, human-like ECA’s is valuable in the first
(Graesser et al., 2004; McNamara, Levinstein, & Boonthum,               place. After all, the argument can be made that humans
2004) or speech pathology (Massaro, 2006). In all these                 project human characteristics on objects that do not even
domains, the role of the agent is to improve the                        slightly resemble humans (Reeves & Nass, 2003).
communication of a given message.                                       Moreover, aiming for humanlike ECA’s increases the
  The genealogy of these agents goes back many centuries.               chances of entering the uncanny valley (Mori, 2005), with
One of the first proposals for embodied interfaces came                 users liking the humanlike avatar less than cartoonish
from Heron of Alexandria’s (62 AD) who described an                     avatars. At the same time, there is evidence that humanlike
‘automatic’ puppet-theatre operated by weights. In the 18th             characteristics like stereotypes are applied to humanlike but
century Friedrich von Knaus developed the first talking                 not to cartoonlike agents (Louwerse, Graesser, Lu, &
heads, while C. G. Kratzenstein synthesized vowel sounds                Mitchell, 2005).
                                                                   1459

   Beyond the applied goal of improving the communication            Conventional statistical techniques like correlations and
of a message to human users, ECA’s can be employed for             classical regression models are unsuccessful in determining
testing scientific hypotheses.                                     the alignment of these communicative channels, because
   In a nutshell, it is important to uncover how multimodal        their use would assume that two variables are either fully
channels are aligned in humans, and what the effect of             synchronized on a time line or not at all. Moreover, the non-
alignment has on participants. That effect can ideally be          independence of observations would undermine the analysis
tested using ECA’s since they allow for careful                    based on these statistics. Instead, cross-recurrence analyses
manipulation of the linguistic and paralinguistic channels.        are useful because they can reveal the temporal dynamics of
   The aim of the current study is to use the data from a          a data set and are meant to be used to model non-
human-human multimodal communication experiment,                   independent observations. Cross-recurrence plots quantify
implement these human facial movements, gestures and               the recurrences of values in two times series. This nonlinear
intonation in an ECA, and test the presence of each of the         data analysis allows for comparisons between
multimodal channels in a persona assessment, as well as in         communicative channels as they unfold over time. This
participants’ attention to the agent.                              technique has been used successfully in illustrating the
                                                                   coupling of eye movements in dialog (Richardson, Dale, &
            Human-human communication                              Kirkham, 2007).
When language users communicate, they are involved in a              All modalities were at least polled at 250-millisecond
rich complex of activities, involving discourse acts               intervals and a cross-recurrence analysis was run on this
associated to the appropriate intonation and accompanied by        data. In addition, to identify whether the cross-recurrence
facial expressions, hand gestures, and eye gaze. In a recent       pattern significantly differed from the baseline, a shuffled-
project on multimodal communication in humans and agents           time series baseline was computed. Only those multimodal
(Guhe & Bard, 2008; Louwerse et al., 2007) we collected 34         channels were considered to be aligned if at least five points
hours of multimodal dialogues from 64 students from the            in the time series of the cross-recurrence analysis yielded a
University of Memphis. Facial expressions and gestures             significant difference with the baseline as measured by a
were recorded by five camcorders, eye gaze was recorded            paired-sample t-test. Table 1 presents on overview of the
by a remote eye tracker, and speech from both participants         alignment of the multimodal channels facial expressions,
was recorded on separate audio channels.                           gestures and eye gaze to the dialog acts and should be
  To control base conditions, genre, topic, and goals of           interpreted as follows. For instance, when IGs use an
unscripted dialogs, we used the Map Task scenario                  Acknowledgment dialog act, they will keep their eyes on the
(Anderson, et al., 1991). An Instruction Giver (IG) coached        map, and not on the IF. On the other hand, when IGs use an
the Instruction Follower (IF) through a route on the map. By       Explain dialogue act they look at the IF.
way of instructions, participants were told that they and
their interlocutors had maps of the same location but drawn                             Implementation
by different explorers and so potentially different in detail.       Because of the applications embodied conversational
They were not told where or how the maps differed, in order        agents are used in (e.g., intelligent tutoring systems), the
to increase the likelihood of observing diverse linguistic and     ECA was developed to play the role of the IG, while the
paralinguistic signals.                                            human user would play the role of the IF. This meant that
  Participants were seated in front of each other but were         the agent was developed to communicate the path on the
separated by a divider to ensure that they focused on the          map to the IF. The agent program was developed using
monitor. They communicated through microphones and                 Visual C# and Visual C++ in Visual Studio 2005. It
headphones, and could see the upper torso of their dialogue        consisted of two main components, the interface program
partner and the map on a computer monitor in front of them         and a speech recognition system, which communicated
through a webcam. This computer-mediated session, using            through a TCP socket.
webcams, was necessary for eye tracking calibration, as              The interface program had a full screen dialog window
well as to reduce torso movement. The IG was presented             divided into two halves. On the left half, a Haptek avatar
with a colored map with a route (see Louwerse, 2007) and           was situated. On the right half, the IF map. A dialog
was asked to communicate the route to the IF as accurately         manager decided what the avatar said and how the avatar
as possible. The IF’s task was to accurately draw the path on      behaved. The dialog manager simulated a state machine.
the screen using the mouse.                                        Within each map location, the dialog manager created
  All dialogues were transcribed and each utterance was            states. Each state served a dialogue function: 1) confirm
classified in one of 12 dialogue acts that are typically used      current location, 2) give instruction, or 3) back up to
for Map Task coding (Carletta et al., 1997; Louwerse &             previous location. For every state change the dialog
Crossley, 2006). Facial expressions were coded in a subset         manager first took input from the LumenVox Speech
of the Action Units (Ekman, Friesen, Wallace, & Hager,             Engine, processed this information and produced a response
2002) and gestures were classified using McNeil’s (1992)           using Speechify speech synthesis and Haptek facial
taxonomy.                                                          expressions, eye gaze and gestures.
                                                               1460

                                       eye on map                       eye brows up   asymm. brows   eye brow down           lip tightener   mouth open                                                                           deictic gesture   iconic gesture   route gesture
                                                                                                                                                                                                                     stroke face
                           eye on IF                blinking   squint                                                 frown                                pucker   biting lip                   nodding   shaking                                                                    multi beat   single beat
                                                                                                                                                                                 smile   laugh                       chinrest
acknowledgment             -             +            +          -                                       -                                      -                     +           +        +     +         -          +                -                 -               -              -             -
align                      +
check                                                                   -                                                                                                         -
clarify                                                                                                                                                               -                          +         +          -                +
explain                    +                          +          +      +                                               +      -                -           -         -           +              +         +          -                +                 +                              +             +
instruct                   -             +            +                 +                 +                                    -                -           -         -           -        -     +         +          -       -        +                 +               +              +             +
query-w                                               +          +                                                      +      -                -                     -                                                                                  -
query-yn                   +                                     +      +                                +                     -                -           -         -           -        -     -                    -                +                                                              +
ready                      -                                     -                        -              -                     -                                      -                          -         -                           -                 -
reply-n                                                                 +                                                                                             -                                    +          +
reply-w                                                                 +                                                                       -           -         -           -              +                                     +                                 -                            -
reply-y                    -                          +                 +                 -                                    -                -                                                +                                     -                 -                              -
                               eyes           eye brows                mouth             head                hands
  Table 2. Overview of cross-recurrence patterns between dialogue acts and other modalities. A positive cross-recurrence
(+) indicates a higher, and a negative cross-recurrence (-) indicates a lower frequency of events, compared to the baseline.
  To create the facial movements for the Haptek agent                                                                                                                                             Experiment
(Figure 1), the Action Units (AUs) linked to the selected                                                                                        An experiment tested whether the modalities implemented
facial expressions were taken as templates. Activation of the                                                                                 in the agent had a positive effect on the perceived usefulness
AU was based on IG cross-recurrence behavior. Facial                                                                                          of the agent and the performance at the task. In order to test
movements and gesture movements worked on a pre-set                                                                                           the impact of the naturalness of the agent conditions were
muscle and a joint point system. This allowed for natural                                                                                     created whereby facial expressions were (or were not)
multimodal behavior to be implemented. The intensity of                                                                                       activated, gestures were (or were not) activated, and
behavior was modified when considered too expressive (or                                                                                      intonation was (or was not) activated. The intonation
unnatural) based on trial and error testing to achieve desired                                                                                condition used the Speechify intonation or removed any
effect.                                                                                                                                       intonation that the Speechify synthesized speech uses. This
                                                                                                                                              resulted in 2 (face) x 2 (gesture) x 2 (intonation) = 8 within-
                                                                                                                                              subject conditions. In addition, two intonation specific
                                                                                                                                              conditions were added without face or gesture movements,
                                                                                                                                              but with enhanced intonation that either matched or
                                                                                                                                              mismatched Steedman’s (2000) theory of contrast (correct
                                                                                                                                              vs. incorrect stress).
                                                                                                                                                 In an eye tracking experiment using rather static agents
                                                                                                                                              Louwerse et al. (in press) found ECA’s to attract attention to
                                                                                                                                              the nose bridge. Their findings were very similar to
                                                                                                                                              Gullberg and Holmqvist (2006) who reported eye tracking
                                                                                                                                              evidence that the face of the dialogue partner dominates as a
                                                                                                                                              target of visual attention, whereby fixations would primarily
                                                                                                                                              center on the nose bridge of the speaker’s face capturing the
                                                                                                                                              eyes and mouth of the speaker simultaneously. This
                                                                                                                                              suggests that the face fulfills a pragmatic role. We therefore
                                                                                                                                              predicted that the same pragmatic effect would emerge in
                                                                                                                                              the face condition.
        Figure 1. Interface with Haptek agent and IG map.                                                                                        In an eye tracking experiment on pointing gestures and
                                                                                                                                              linguistic expressions Louwerse and Bangerter (2005) found
                                                                                                                              1461

gestures fulfilling very much a semantic function. When             Results
information from linguistic expressions did not suffice,            Questionnaire
attention moved to gestures. However, when linguistic               Internal consistency of the questionnaire as measured by
expressions were sufficient, gestures did not receive the           Cronbach’s α was computed on all 24 participants. Overall
same amount of attention. We therefore predicted that the           reliability was .86. High internal consistency was found for
same semantic effect would emerge in the gestures and               all five categories, facilitation of learning (α = .80),
intonation condition.                                               credibility (α = .92), human-likeness (α = .82), engagement
                                                                    (α = .83), and quality (α = .90).
Participants                                                          We conducted 2 (presence/absence of facial expressions) x
Twenty-four students at the University of Memphis                   2 (presence/absence of gestures) x 2 (presence/absence of
interacted with the ECA and received course credit for their        intonation) mixed-model analysis on the participants ratings
participation.                                                      with participants and items as random factors (Baayen,
                                                                    Davidson, & Bates, 2008). The model was fitted using the
Materials                                                           restricted maximum likelihood estimation (REML) with a
Fourteen maps were used, 10 experimental maps and 4 filler          Kenward-Rogers adjustment for degrees of freedom.
maps. The order of the 8 (2 x 2 x 2) multimodal maps was              The presence of facial expressions had a positive effect on
fixed but the order of the conditions was counterbalanced.          answers in all five categories, the presence of gesture on
The order of the two intonation specific maps was                   answers in all five categories except credibility. Intonation
counterbalanced. In the multimodal maps, all modalities             positively affected all five categories except the categories
(face, gesture, and intonation) were varied per condition. In       humanlike and credibility. Results are presented in Table 3.
the intonation specific maps, only stress was varied between        None of the enhanced correct or incorrect stress conditions
correct and incorrect. Maps were of equivalent difficulty,          yielded significant differences. These findings first and
and similar to those in the human-human experiment                  foremost suggest participants value multimodal behavior in
discussed before. As in the human-human experiments,                ECA’s, whereby the role of facial expressions is most
there were slight differences between the IG and IF maps to         important. Gestures and intonation play a slightly lesser role
elicit conversation.                                                particularly when it comes to assessment of credibility
                                                                    (gesture and intonation) and human-likeness (intonation).
Apparatus                                                           These findings might support the specifically pragmatic role
Participants’ communication was recorded by camcorders              for facial expressions and the specifically semantic role for
and a speech recorder, similar to the set up in the human-          gestures and intonation. However, testing semantic factors
human experiment. We will focus here on eye gaze only,              requires a measurement other than a persona assessment.
recorded for the IF using an SMI iView RED remote eye               We therefore looked at the role of eye fixations in the
tracker with a sampling frequency of 60Hz.                          interaction.
Procedure                                                           Eye gaze
For all 14 maps, participants were seated in front of the             Areas of interest (AOI) were defined as areas on the face
computer presenting the ECA. They communicated through              of the ECA, the start and end locations on the map, and
a microphone and headphones with the ECA. In between                items important for disambiguation of location based on
maps, the eye tracker was recalibrated to ensure precision.         shape or color. Total fixation time on areas of interest on the
  After completing each map, participants filled out a              ECA and the map were computed. Outliers were defined as
questionnaire based upon Ryu and Baylor’s (2005) Agent              3 SD above the mean within a condition, subjects and area
Persona Instrument to evaluate the ECA in the relevant              of interest, and were removed from the analysis. This
condition. This instrument is the result of factor analyses on      affected less than 3% of the data.
data from a number of human-agent interaction studies and             As before, a mixed-effects model was used with the total
consists of questions related to four categories (facilitation      fixation time as the dependent variable and with participants
of learning, credibility, human-likeness and engagement). In        and items as random factors and presence and absence of
addition, we added questions related to the extent                  face, gesture and intonation as fixed factors.
participants liked the quality of the interaction (e.g., I liked      The presence of facial expressions increased the fixation
the agent’s voice; I liked the agent’s appearance). All             time on the face of the agent and more specifically on the
questions were answered on a 1-6 scale, 1 being totally             nose bridge of the agent. This finding is in line with eye
disagree and 6 being totally agree.                                 tracking studies in human-human communicative settings
                                                                    discussed earlier, and confirms the hypothesis that facial
                                                                    expressions play a pragmatic role in interactions.
                                                                      The increased fixations on the ECA’s face cannot be
                                                                    explained by the fact that motion attracted the attention, as
                                                                    the presence of gestures also directed attention to the face of
                                                                    the ECA, even in the absence of facial expressions.
                                                                1462

                                       Face                              gesture                             Intonation
                          absence          presence         Absence           presence          absence          Presence
facilitation learning     3.01 (1.59)      3.91 (1.54)**    3.11 (1.62)       3.78 (1.56)**     3.17 (1.61)      3.76 (1.60)**
credibility               3.17 (1.79)      3.81 (1.58)**    3.26 (1.80)       3.69 (1.61)       3.33 (1.76)      3.69 (1.64)
human-likeness            3.31 (1.54)      3.79 (1.44)**    3.35 (1.53)       3.73 (1.47)**     3.40 (1.53)      3.70 (1.48)
engagement                3.03 (1.57)      3.96 (1.48)**    3.14 (1.60)       3.81 (1.51)**     3.18 (1.58)      3.80 (1.54)**
quality                   3.10 (1.59)      3.63 (1.56)**    3.15 (1.59)       3.56 (1.58)**     3.22 (1.59)      3.55 (1.59)*
          Table 3. Means and standard deviations of ratings in persona assessment questionnaire. ** p < .01, * p < .05
AOI                       face                                     gesture                                intonation
             absence             presence              absence              presence              absence            presence
start    199.66 (186.66)    185.67 (176.79)        181.99 (180.23)     212.93 (185.75)*      180.88 (157.60)     187.66 (176.64)
end      147.47 (128.23)    105.31 (102.30)**      136.23 (126.43)     123.70 (111.22)       127.83 (122.02)     138.56 (118.88)
face     1452.71 (936.55)   1824.66 (1352.61)**    1519.46 (1176.12)   1729.30 (1068.15)*    1636.32 (1294.45)   1570.76 (1034.81)
nose     269.48 (264.66)    379.88 (472.63)**      343.79 (427.79)     280.96 (278.30)       311.68 (430.40)     293.38 (325.63)
eyes     358.71 (362.11)    518.03 (721.24)*       439.91 (633.26)     407.04 (409.00)       447.18 (684.30)     410.05 (437.31)
color    467.09 (465.27)    560.1 (528.78)         460.87 (445.91)     574.68 (554.04)       602.50 (599.60)     478.92 (408.87)*
shape    614.77 (563.49)    502.84 (456.43)*       587.00 (545.36)     543.39 (493.93)       544.39 (454.72)     592.75 (626.11)
         Table 3. Means and standard deviations of total fixation times on areas of interest (AOI). ** p < .01, * p < .05
This finding does not confirm the hypothesis that gestures             The role of intonation in disambiguation was also found
play a semantic role, but perhaps the semantic content was           when incorrect and correct stress was compared. In the case
too general. The finding, however, is in line with the               of incorrect stress, fixation time was three times higher on
human-human communication literature, that shows that                the color landmarks than when correct stress was given (M
addressees often do not attend to gestures but instead fixate        = 482.14, SD = 358.89 vs. M = 190.56, SD = 213.08,
on the face of the dialogue partner (Gullberg & Holmqvist,           F(1,23) = 12.29, p < .01). Recall that in the conditions of
2006).                                                               incorrect and correct stress no facial expressions or gestures
  Facial expressions and gestures also played a role at the          were present. Nevertheless, a difference approaching
start and the end of the experiment. Fixations on the                significance was found between intonation conditions in
opening landmark on a map received more fixation time                fixation time on the area of the gestures, with twice as much
when gestures were present, while closing landmarks                  fixation time on the gesture area in the incorrect stress
received more fixation time when facial expressions were             condition than in the correct stress condition (M = 605.88,
present. This might suggest that at the start of the map             SD = 681.83 vs. M = 319.68, SD = 366.96, F(1, 46) = 3.69,
navigation addressees need hand gestures to get oriented             p = .06), as if incorrect stress made the need for gestural
(semantic factors), whereas at the end of a map these                cues larger (Louwerse & Bangerter, 2005). These findings
gestures are not needed but eye contact to close the dialogue        confirm the semantic role of intonation, and that of specific
is (pragmatic factors).                                              gestural movements, in communication.
  The area of interest shape referred to one specific group of
three landmarks on each map for which shape was a                                             Conclusion
disambiguating factor. For instance, in a situation involving        The current study investigated the multimodal behavior in
two blue fish, two red cars and two red trees in each other’s        ECA’s. Two questions played a central role, the first being
vicinity, the use of the referential expression two red trees        how multimodal behavior can be implemented in ECA’s if
the disambiguating word referred to the shape. Similarly,            so little information is available on the alignment of
the area of interest color referred to one specific group of         communicative channels in humans; the second what the
three landmarks on each map for which color was the                  effect of humanlike multimodal behavior is on interactions
disambiguating factor. Because linguistically color always           with ECA’s. Using a large multimodal corpus of face-to-
preceded shape, color provided slightly more ambiguity. In           face conversations, we were able to implement natural
the absence of any intonation, fixation times indeed                 humanlike multimodal behavior in ECA’s. That this
increased on these color landmarks, when the participant             implementation was perceived as being efficacious was
had to compare the correct landmark of two similar ones.             confirmed in the assessment of the persona. Moreover,
This confirms the semantic role for intonation. No                   interactions with the ECA showed that facial expressions,
differences in fixations to shape were found.                        gestures and intonation all had a positive effect on the
                                                                1463

communication, with some evidence that facial expressions          Louwerse, M.M. & Bangerter, A. (2005). Focusing attention
played a pragmatic role, whereas intonation played a                 with deictic gestures and linguistic expressions. In B.
semantic role. Gestures had a pragmatic factor when they             Bara, L. Barsalou, & M. Bucciarelli (Eds.), Proceedings
were general, a semantic factor when they were specific.             of the Cognitive Science Society (pp. 1331-1336).
These findings shed light on multimodal behavior within              Mahwah, NJ: Lawrence Erlbaum.
and between human and digital dialogue partners.                   Louwerse, M. M. & Crossley, S. A. (2006). Dialog act
                                                                     classification using N-Gram algorithms. In G. Sutcliffe &
                     Acknowledgments                                 R. Goebel (Eds.), Proceedings of the International
This research was supported by grant NSF-IIS-0416128. We             Florida Artificial Intelligence Research Society (pp. 758-
thank Ellen Bard, Rick Dale, Markus Guhe, and Mark                   763). Menlo Park, California: AAAI Press.
Steedman for their help in the experiment design and data          Louwerse, M. M., Benesh, N., Hoque, M.E., Jeuniaux, P.,
analysis. We also express our thanks to Haptek and                   Lewis, G., Wu, J., & Zirnstein, M. (2007). Multimodal
LumenVox for their support. The usual exculpations apply.            communication in face-to-face conversations. In D. S.
                                                                     McNamara & J. G. Trafton (Eds.), Proceedings of the
                         References                                  29th Annual Cognitive Science Society (pp. 1235-1240).
                                                                     Austin, TX: Cognitive Science Society.
Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G.          Louwerse, M.M., Graesser, A.C., Lu, S., & Mitchell, H.H.
  M., Garrod, S., Isard, S., Kowtko, J., McAllister, J.,             (2005). Social cues in animated conversational agents.
  Miller, J., Sotillo, C., Thompson, H. S., & Weinert, R.            Applied Cognitive Psychology, 19, 1-12.
  (1991). The HCRC Map Task Corpus. Language and                   Louwerse, M.M., Graesser, A.C., McNamara, D.S. & Lu, S.
  Speech, 34, 351-366.                                               (in press). Embodied conversational agents as
Baayen, R.H., Davidson, D.J. and Bates, D.M. (2008).                 conversational partners. Applied Cognitive Psychology.
  Mixed-effects modeling with crossed random effects for           Massaro, D.W. (2006). A computer-animated tutor for
  subjects and items. Journal of Memory and Language, 59,            language learning: Research and applications. In P.E.
  390-412.                                                           Spencer & M. Marshark (Eds.), Advances in the spoken
Baylor, A. L. & Kim, Y. (2005). Simulating instructional             language development of deaf and hard-of-hearing
  roles through pedagogical agents. International Journal of         children (pp. 212-243). New York, NY: Oxford
  Artificial Intelligence in Education, 15, 95-115.                  University Press.
Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-           McNamara, D. S., Levinstein, I. B., & Boonthum, C.
  Sneddon, G., & Anderson, A. (1997). The reliability of a           (2004). iSTART: Interactive strategy trainer for active
  dialogue structure coding scheme. Computational                    reading and thinking. Behavioral Research Methods,
  Linguistics, 23, 13-31.                                            Instruments, and Computers, 36, 222-233.
Cassell, J., Kopp, S., Tepper, P., Ferriman, K., & Striegnitz,     McNeill, D. (1992). Hand and Mind: What Gestures Reveal
  K . (2007) Trading spaces: How humans and humanoids                about Thought. Chicago, IL/London, UK, University of
  use speech and gesture to give directions. In T. Nishida           Chicago Press.
  (ed.) Conversational Informatics (pp. 133-160). New              Mori, M. (1970). The uncanny valley. Energy, 7, 33–35.
  York: John Wiley & Sons.                                         Pellom, B., Ward, W., & Pradhan, S. (2000). The CU
Ekman, P., Friesen, Wallace V., & Hager, J.C. (2002).                communicator: An architecture for dialogue systems.
  Facial Action Coding System (FACS). CD-ROM.                        International Conference on Spoken Language
Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H. H.,            Processing (ICSLP). Beijing, China.
  Ventura, M., Olney, A., & Louwerse, M. M. (2004).                Reeves, B., & Nass, C. (1996). The media equation: How
  AutoTutor: a Tutor with Dialogue in Natural Language.              people treat computers, television, and new media like
  Behavior Research Methods, Instruments, and                        real people and places. Cambridge, MA: Cambridge
  Computers, 36, 180–193.                                            University Press.
Guhe, M. & Bard, E. G. (2008) Adapting the use of                  Richardson, D.C., Dale, R. & Kirkham, N.Z. (2007). The art
  attributes to the task environment in joint action: Results        of conversation is coordination: Common ground and the
  and a model. Proceedings of Londial – The 12th                     coupling of eye movements during dialogue.
  Workshop on the Semantics and Pragmatics of Dialogue,              Psychological Science, 18, 407-413.
  91–98.                                                           Ryu, J. & Baylor, A. L. (2005). The psychometric structure
Gullberg, M. & Holmqvist, K. (2006). What speakers do                of pedagogical agent persona. Technology, Instruction,
  and what listeners look at. Visual attention to gestures in        Cognition & Learning, 2, 291-319.
  human interaction live and on video. Pragmatics and              Steedman, M. (2000). Information structure and the syntax-
  Cognition, 14, 53-82.                                              phonology interface. Linguistic Inquiry, 34, 649-689.
Johnson, W. L., Choi, S., Marsella, S., Mote, N.,                  Thompson, L. & Massaro, D. (1986). Evaluation and
  Narayanan, S., & Vilhjálmsson, H. (2004). Tactical                 integration of speech and pointing gestures during
  language training system: Supporting the rapid acquisition         referential understanding. Journal of Experimental Child
  of foreign language and cultural skills. Proceedings of            Psychology 42, 144- 168.
  InSTIL/ICALL.
                                                               1464

