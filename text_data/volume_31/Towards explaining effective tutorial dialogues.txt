UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards explaining effective tutorial dialogues
Permalink
https://escholarship.org/uc/item/4kr6s7wb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Cosejo, David
Di Eugenio, Barbara
Fossati, Davide
et al.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Towards explaining effective tutorial dialogues
                                      Barbara Di Eugenio and Davide Fossati
                                                  {bdieugen, dfossa1}@uic.edu
                         Department of Computer Science, University of Illinois at Chicago, USA
                                          Stellan Ohlsson and David Cosejo
                                                    {stellan, dcosej1}@uic.edu
                             Department of Psychology, University of Illinois at Chicago, USA
                           Abstract                                 come by. In the course of our own tutorial dialogues
   We present a study of human tutorial dialogues in a
                                                                    collection and analysis (Di Eugenio et al., 2006, 2008),
   core Computer Science domain that: focuses on indi-              we have come to believe that even many experienced tu-
   vidual tutoring sessions, rather than on contrasting dif-        tors do not behave socratically most of the times. Still,
   ferent types of tutors; uses multiple regression analysis        students learn with them, sometimes but not necessarily
   to correlate features of those sessions with learning out-
   comes; and highlights the effects of two types of tutor          more than what they learn with inexperienced tutors.
   moves that have not been studied in depth so far, direct         As we discuss in Ohlsson et al. (2007), this brought
   instruction and positive feedback.                               us to advocate a different approach to tutorial dialogue
   Keywords: Artificial Intelligence; Education; Dis-               analysis. We eschew defining who an expert tutor is a
   course; Interactive Behavior; Human Experimentation;
   Intelligent Tutoring Systems.                                    priori, or casting one type of tutor against another, and
                                                                    analyzing differences between them. Rather, we pool the
                      Introduction                                  data from all the tutors together, and we focus on the
One-on-one tutoring has been shown to be a very ef-                 effectiveness of the individual tutoring session, by run-
fective form of instruction compared to other educa-                ning regression analyses that correlate features of those
tional settings. For more than twenty years, researchers            sessions with learning outcomes.
have worked on uncovering features of tutorial inter-                  The next point concerns which features of the tutor-
action that engender learning in students (Fox, 1993;               ing session should be entered in the regression analysis.
Graesser, Person, & Magliano, 1995; Lepper, Drake, &                The body of work we described above has proposed a
O’Donnell-Johnson, 1997; Chi, 2001; Moore, Porayska-                variety of coding schemes to annotate tutorial dialogues.
Pomsta, Varges, & Zinn, 2004; Evens & Michael, 2006;                In Ohlsson et al. (2007), we noted that most often such
Litman et al., 2006; Di Eugenio, Fossati, Haller, Yu, &             coding schemes are motivated by pedagogical tenets, by
Glass, 2008). This research is partly motivated by the              linguistic theories of speech acts, and often by what hap-
search for computational models1 of effective tutoring              pens in the tutoring dialogues themselves. What many
strategies, which are an essential component of dialogue            schemes are missing is being informed by cognitive in-
interfaces to Intelligent Tutoring Systems, or ITSs.                sights into how people learn.
   From such an extensive body of work, many find-                     In this paper, we present a corpus analysis which em-
ings have arisen, but a unifying explanation has yet to             bodies those earlier proposals of ours: it pools together
emerge, partly because tutorial interactions are so rich            tutoring sessions collected in a study that had originally
that it is hard to characterize them fully. One line of at-         started as a contrast between a more experienced and a
tack, pursued by us as well as by others in the past, has           less experienced tutor; because it is motivated by what is
tried to understand what expert tutors do (Lepper et al.,           cognitively plausible for learning, it focuses on two fea-
1997; Evens & Michael, 2006; Di Eugenio, Kershaw, Lu,               tures that have not been studied in depth so far, direct
Corrigan-Halpern, & Ohlsson, 2006; Cade, Copeland,                  instruction and positive feedback; and uses regression
Person, & D’Mello, 2008). However, it is not clear what             analysis. In particular, we show that positive feedback
expert tutors are to start with; expertise does not nec-            correlates with learning, and that our methodology al-
essarily mean, ’with extensive experience’. Another ap-             lows us to further annotate the data in a controlled, prin-
proach is to imbue the notion of expertise with what                cipled way. We also very briefly discuss the ITS we have
we know from pedagogy, and equate expert tutors with                built, partly based on these results.
idealized, Socratic human tutors. However, as Graesser
and collaborators pointed out in a series of papers (Per-              Human tutoring in Computer Science
son, Graesser, Magliano, & Kreuz, 1994; Graesser et al.,            Our domain of interest is Computer Science (CS), specif-
1995), the idealized Socratic human tutor is difficult to           ically, introductory data structures such as linked lists,
    1                                                               stacks and binary search trees, and the algorithms that
      We mean computational in the algorithmic sense coming
from computer science, rather than in the sense of mental           manipulate them. This domain is partly motivated by
computation coming from cognitive psychology.                       the interests of some of us, as educators in CS. Basic
                                                                1430

data structures and algorithms are in the core of the CS
                                                                          Table 1: Learning gains and t-test statistics
undergraduate curricula, and have been identified as dif-
ficult concepts for students to master (Katz, Allbritton,            Topic        Tutor       N     Mean    Std      t       p
Aronis, Wilson, & Soffa, 2003). At the same time, basic               List       Novice      24      .09    .22   -2.00    .057
CS has received little attention from the educational and                      Experienced   30      .18    .26   -3.85    < .01
ITS research communities. The few ITSs that concern                             Combined     54      .14    .25   -4.24    < .01
CS cover a smattering of topics, including computer lit-                         Control     53      .01    .15   -0.56     ns
eracy (Graesser, Person, Lu, Jeon, & McDaniel, 2005),                Stack       Novice      24      .35    .25   -6.90    < .01
programming languages such as Lisp (Corbett & Ander-                           Experienced   24      .27    .22   -6.15    < .01
son, 1990), general programming and design skills (Lane                         Combined     48      .31    .24   -9.20    < .01
& VanLehn, 2003), and databases (Mitrović, Suraweera,                           Control     53      .05    .17   -2.15    < .05
Martin, & Weerasinghe, 2004). We believe iList, the sys-              Tree       Novice      24      .33    .26   -6.13    < .01
tem we have developed partly on the basis of our corpus                        Experienced   30      .29    .23   -6.84    < .01
study, and which we will briefly discuss at the end of the                      Combined     54      .30    .24   -9.23    < .01
paper, fulfills an important need.                                               Control     53      .04    .16   -1.78     ns
Methods
We collected a corpus of 54 one-on-one tutoring ses-               it is well established that human tutors are more effec-
sions on linked lists, stacks, and binary search trees (for        tive than those other conditions (Evens & Michael, 2006;
brevity, we will refer to the first topic as lists, and to the     VanLehn et al., 2007).
third, as trees). Each individual student participated                In both conditions, the vast majority of students
in only one tutoring session, with a tutor randomly as-            were recruited from our undergraduate introductory CS
signed from a pool of two tutors. One of the tutors is             classes, before they studied the data structures of inter-
an experienced CS professor, with more than 30 years of            est; there were also few graduate students from other
teaching experience, including one-on-one tutoring. The            engineering departments, but none of them was familiar
other tutor is a senior undergraduate student in CS, with          with the data structures in the pre-test, as the learning
only one semester of previous tutoring experience.                 results show.
   Students took a pre-test right before the tutoring ses-
sion, and an identical post-test immediately after. The            Learning outcomes If there were no learning among
test had two problems on lists, two problems on stacks,            our tutored subjects, it would obviously not make sense
and four problems on trees. Each problem was graded                to annotate the corpus to discover how students learn.
out of 5 points, for a possible maximum score of 10 points         Hence, the first finding we report is that the tutored stu-
each for lists and stacks, and 20 points for trees. Pre-           dents did learn, whereas those in the control group did
and post-test scores for each topic were later normalized          not. Paired samples t-tests revealed that post-test scores
to the [0..1] interval.                                            are significantly higher than pre-test scores in the two tu-
   The tutors did not know the problems on the pre-test,           tored conditions for all the topics, except for lists with
since we did not want them to tutor to the test, but they          the less experienced tutor, where the difference is only
were presented with a high level summary of the pre-test           marginally significant. If the two tutored groups are ag-
results. Additionally, they had a predefined set of prob-          gregated, there is significant difference for all the topics.
lems they could use during tutoring, and were alerted              Students in the control group show significant learning
when the tutoring session had lasted 45 minutes. Even              only for stacks, but not for lists and trees. Means, stan-
so, tutors had considerable latitude in how long they tu-          dard deviations, and t-test statistic values are reported
tored students: in some cases they decided to continue             in Table 1. N represents the number of subjects in that
beyond 45 minutes, while in other cases they stopped               specific group (for stacks, N is lower for the Experienced
tutoring around 35 minutes, hence the variability in the           tutor, because the tests administered to 6 subjects did
length of the tutoring sessions (see below).                       not include problems on stacks).
   An additional group of 53 students (control group)                 The learning gain, expressed as the difference between
took the pre- and post-tests, but instead of participating         post-score and pre-score, of students that received tutor-
in a tutoring session they attended a 40 minute lecture            ing is significantly higher than the learning gain of the
about an unrelated CS topic. The rationale for such a              students in the control group, for all the topics. This
control condition was to assess whether by simply taking           is showed by ANOVA between the aggregated group
the pre-test students would learn about data-structures,           of tutored students and the control group. For lists,
and hence, to tease out whether any learning we would              F (1, 106) = 11.0, p < 0.01. For stacks, F (1, 100) = 41.4,
see in the tutored conditions would be indeed due to tu-           p < 0.01. For trees, F (1, 106) = 43.9, p < 0.01.
toring. We did not run more usual control conditions,                 There is no significant difference between the two tu-
such as reading relevant material in a textbook, since             tored conditions in terms of learning gain. The fact that
                                                               1431

                                                               ances were also longer than students’ utterances, with
   Table 2: Turns, utterances and words: mean (std)            8.2 words per sentence, as opposed to 5.8 words from
       Unit                  Tutors       Students             students. Person’s expert tutors (Cade et al., 2008) also
       Turns             46.6 (37.4)    45.5 (37.3)            talk more than their students, producing 77% of total
       Utterances       186 (107.6)       48.6 (40)            words (p.c.).
       Words          1971.8 (1072) 155.7 (151.3)                 Another observation from Table 2 is that standard de-
                                                               viations are large. The variability in the length of dia-
                                                               logues is another reason why a regression analysis that
students did not learn more with the experienced tutor
                                                               can take into account the length of individual sessions,
was an important finding that led us to question the ap-
                                                               is appropriate.
proach of comparing and contrasting more and less expe-
rienced tutors. Whereas these specific learning outcomes       Coding categories As we noted above, whereas a
may be idiosyncratic to our two tutors, and should be          number of different coding schemes for tutorial dialogues
confirmed by further experiments, the research direction       exist, we tried to go back to (some of) the basics: we
this finding led us to is independently fruitful.              want the system for categorizing tutoring behavior to be
                                                               informed by what we know about learning (Ohlsson et
Corpus analysis                                                al., 2007). We postulated that, given current theories of
The 54 tutoring sessions were videotaped, and amount to        skill acquisition (Anderson, 1986; Sun, Slusarz, & Terry,
a total of 33 hours and 52 minutes. The videotaped ma-         2005; Ohlsson, 2008), the following types of tutorial in-
terial was subsequently transcribed, following a subset        tervention can be explained in terms of why and how
of the conventions described in the transcription manual       they might support learning:
of the CHILDES project (MacWhinney, 2000). An ut-
terance is a natural unit of speech bounded by breaths        1. A tutor can tell the student how to perform the task
or pauses.     Figure 1 shows excerpts from two of our            (direct procedural instruction).
dialogues that include some of the transcription conven-
                                                              2. A tutor can provide feedback :
tions. For example, ’+...’ marks trailing; angle brack-
ets mark abandoned speech, or overlap when they oc-             (a) positive, to confirm that a correct but tentative stu-
cur in two adjacent utterances by two different speakers;            dent step is in fact correct;
’xxx’ marks unintelligible speech.                              (b) negative, to help a student detect and correct an
Descriptive statistics Table 2 shows the average                     error.
number of turns, utterances, and words, per tutor             3. A tutor can state declarative information about the
(across the two tutors), and per student. Tutors are              domain.
shown as an aggregate since there are no significant dif-
ferences between these two tutors along those dimen-              This should not be taken as a full inventory of what a
sions. These numbers show that tutors talk much more           tutor may do, but as a core set of moves that are theoret-
than students, producing more than 10 times as many            ically motivated (the reader may be surprised not to see
words. Tutors produce longer turns, consisting of 4 ut-        prompts; please see below for further comments). This
terances on average, while students’ turns contain 1 ut-       inventory may be augmented as necessary, if the regres-
terance on average; and tutors’ utterances are longer,         sion reveals that these moves do not account for enough
consisting of 10.6 words on average, while students’ ut-       of the variance. Hence, we set out to code our dialogues
terances contain 3.2 words on average. Note that the           for categories corresponding to these conceptualizations.
numbers of tutor and student turns are equivalent by           So far, we have coded for what we call direct procedu-
construction, since a turn is an uninterrupted sequence        ral instruction (DPI), namely, telling the student what
of utterances by one of the speakers.                          to do, and for positive and negative feedback. We will
   The verbosity of our tutors may surprise the reader.        come back to the specifics of our coding for DPI and for
They talk a lot, to the tune of producing 93.5% of the to-     feedback below.
tal words! Certainly, they do not resemble the idealized
Socratic tutor, who is supposed to prompt the student                                    Results
to construct knowledge by themselves (Chi, 1994): in           We present the results of our analysis, performed via
that scenario, students should do most of the talking. In      linear regression models. Table 3 summarizes significant
our view, this is an idealized view of the tutoring pro-       correlations, separately by topic and cumulatively. We
cess. Whereas our tutors may be extreme in their ver-          found three models to be significant: Model 1 accounts
bosity, even tutors who appear to come closer to the ideal     for prior knowledge via the pre-test, Model 2 accounts
do talk more and with longer sentences than students.          for session length as well, Model 3 includes both positive
In the CIRCSIM face-to-face studies, tutors produced           and negative feedback (since the variables were entered
63% of the words (Evens & Michael, 2006); their utter-         one at a time, there exists a significant Model 2.5 that
                                                           1432

                  T:   do you see a problem?
                  T:   I have found the node a@l, see here I found the node b@l, and then I put g@l in
                       after it.
      Begin +     T:   here I have found the node a@l and now the link I have to change is +...
                  S:   ++ you have to link e@l <over xxx.> [>]
      End +       T:   [<] <yeah> I have to go back to this one.
                  S:   *mmhm
                  T:   so I *uh once I’m here, this key is here, I can’t go backwards.
      Begin -     S:   <so you> [>] <you won’t get the same> [//] would you get the same point out of
                       writing t@l close to c@l at the top?
                  T:   oh, t@l equals c@l.
                  T:   no because you would have a type mismatch.
      End -       T:   t@l <is a pointer> [//] is an address, and this is contents.
                           Figure 1: Positive and negative feedback (T = tutor, S = student)
adds only positive feedback to Model 2. We don’t show              The number of positive feedback episodes and the
it in Table 3 because of space constraints). We note that       number of negative feedback episodes have been intro-
a measure of student activity we devised did not result         duced in the regression model (Model 3, Table 3). The
in significant correlations.                                    model showed a significant effect of feedback for lists and
                                                                stacks, but no significant effect on trees. Interestingly,
Prior knowledge (Model 1)                                       the effect of positive feedback is positive, but the effect
First of all, we need to factor out the effect of prior         of negative feedback is negative, as can be seen by the
knowledge, measured by the pre-test score. A linear re-         sign of the β values.
gression model reveals a strong effect of pre-test scores
on learning gains (Table 3). However, the R2 values
show that there is a lot of variance left to be explained,             Table 3: Linear regression – human tutoring
especially for lists and stacks, although not so much for          Topic   Model     Predictor          β   R2        p
trees. Note that the β weights are negative, namely,               List       1      Pre-test        -.45   .18  < .05
students with higher pre-test scores learn less than stu-                     2      Pre-test        -.40   .28  < .05
dents with lower pre-test scores. This is an example of                              Session length   .35        < .05
the well-known ceiling effect: students with more previ-                      3      Pre-test        -.35   .36  < .05
ous knowledge have less learning opportunity.                                        Session length   .33           .05
                                                                                     + feedback       .46           .05
Time on task (Model 2)                                                               - feedback      -.53        < .05
Another variable that is recognized as important by the            Stack      1      Pre-test        -.53   .26  < .01
educational research community is time on task, and we                        2      Pre-test        -.52   .24  < .01
can approximate it with the length of the tutoring ses-                              Session length   .05            ns
sion. Surprisingly, session length has a significant effect                   3      Pre-test        -.58   .33  < .01
only on lists (Table 3).                                                             Session length   .01            ns
                                                                                     + feedback       .61        < .05
Tutor moves (Model 3)                                                                - feedback      -.55        < .05
Feedback has been extensively studied in one form or               Tree       1      Pre-test        -.79   .61  < .01
the other. The focus has often been on negative feedback                      2      Pre-test        -.78   .60  < .01
(even if indirect, because human tutors avoid too much                               Session length   .03            ns
negativity (Fox, 1993; Lepper et al., 1997)). The role of                     3      Pre-test        -.77   .59  < .01
positive feedback, i.e., feedback provided in response to                            Session length   .04            ns
something correct the student did or said, has not been                              + feedback       .06            ns
studied as much. However, it turns out that positive                                 - feedback      -.12            ns
feedback is much more abundant than negative feedback:             All        1      Pre-test        -.52   .26  < .01
in our CS domain, positive feedback occurs eight times                        2      Pre-test        -.54   .29  < .01
as often as negative, and in a previous study of ours, in                            Session length   .20        < .05
the domain of solving sequence patterns, it occurred four                     3      Pre-test        -.57   .32  < .01
times as often (Lu, 2007).                                                           Session length   .16           .06
   The dataset was manually annotated for feedback                                   + feedback       .30        < .05
episodes, where either positive or negative feedback is                              - feedback      -.23           .05
delivered, with acceptable intercoder agreement (κ =
0.67).     Examples of feedback episodes are shown in              We additionally annotated the episodes of positive and
Figure 1.                                                       negative feedback for initiative. An episode can be ini-
                                                            1433

                                                               on task some, although not as much. This is not surpris-
         Table 4: Feedback initiative: mean (std)              ing. More interesting is that several aspects of the tutor’s
                       Student initiative Tutor initiative     behavior impact learning gains, as evidenced by variables
   Positive feedback       3.9 (3.8)         10.2 (9.1)        accounting for variance in the learning outcomes, over
  Negative feedback        1.7 (1.2)          2.0 (1.2)        and above the variance accounted for by prior knowledge
                                                               and time on task. To our surprise, the evidence is mount-
                                                               ing that tutors engage in direct instruction, and that this
tiated either by the student or by the tutor. In the first     may be effective. Exactly why direct instruction may be
case, the student volunteers some information without          effective in the one-on-one context, when it is generally
being asked or prompted by the tutor, and the tutor            believed not to be effective in a classroom setting re-
replies with some feedback (as in the bottom part of           mains to be explained. The data also indicate that tuto-
Figure 1). In the second case, the tutor first asks or         rial feedback is important, but the evidence for positive
prompts the student (not necessarily verbally), then the       feedback is as strong if not stronger than the evidence for
student replies, and finally the tutor provides feedback       negative feedback. This is interesting, given that nega-
on the student’s answer (as in the top part of Figure 1).      tive feedback directly informs the student about what he
The distribution of initiative labels is reported in Ta-       or she needs to change, while positive feedback requires
ble 4. The numbers in the table are aggregated across          that the student already got the answer or the problem
the three topics, but splitting the three topics apart re-     solving step correct. Much work in ITS research, in-
vealed similar patterns.                                       cluding our own past work, has assumed that negative
   ANOVA revealed overall significant differences among        feedback is pedagogically more powerful than positive
the four groups (F (3, 325) = 43.27, p < 0.01).                feedback; the latter has been interpreted mostly in moti-
Tukey post-hoc test revealed significant differences           vational terms. Our results indicate that this view might
(p < 0.01) between positive-tutor and positive-student;        need to be reconsidered. Methodologically, the multiple
positive-tutor and negative-tutor; and positive-tutor and      regression analysis allows us and other ITS researchers
negative-student. Namely, there are significantly more         to apply a stringent criterion for empirical support for a
feedback episodes, both positive and negative, initiated       particular aspect of tutor behavior: that it accounts for
by the tutor, rather than by the student; and the tu-          unique variance in learning outcomes.
tors themselves initiate significantly more positive than         The next steps are to understand whether two addi-
negative feeback episodes. Note that tutor initiative as       tional tutoring modes, providing declarative knowledge
coded here is a subset of a more general prompt category,      and prompting, contribute to learning – prompting con-
that includes prompts that did not result in subsequent        sists of both the tutor initiative in feedback episodes we
feedback; the latter are rare in our data.                     already coded for, and other cases we have recently coded
Direct procedural instruction (DPI) In our cod-                for. With the multiple regression analysis, the field can
ing manual, we define DPI as the tutor directly telling        gradually sort out what works and what does not, and
the student what to do. This includes: correct steps           so assemble an effective toolkit of tutoring modes that
that lead to the solution of a problem (e.g., “and there       allows routine design and implementation of effective in-
is nothing there, so we put six right there”); high-level      telligent tutoring systems.
steps or subgoals (e.g., “it wants us to put the new node         Finally, from the point of view of CS pedagogy, our
that contains G in it, after the node that contains B”);       models are stronger and more interesting for lists, in
and tactics and strategies (e.g., “so with these kind of       that prior knowledge has the least explanatory power.
problems, the first thing I have to say is always draw         This agrees with our beliefs as CS educators that lists
pictures”). We initially tried to distinguish goals and        are among the most difficult concepts that new students
subgoals from the rest, but failed; when we collapsed all      need to master. Hence, partly on the basis of the re-
these categories, coders managed to reach an acceptable        sults of our corpus analysis, we have built various ver-
level of intercoder agreement, κ = 0.71.                       sions of iList, a tutoring system that focuses on tutor-
Linear regression showed a significant positive correla-       ing lists. iList has been already successfully deployed in
tion between DPI and learning gain for lists (β = 0.0038,      classrooms, both at our institution and at the US Naval
t(49) = 2.69, R2 = 0.11, p < 0.01) and trees (β = 0.0024,      Academy in Annapolis (Fossati, Di Eugenio, Brown, &
t(50) = 3.07, R2 = 0.14, p < 0.01). Currently, we are          Ohlsson, 2008). We are currently endowing the latest
incorporating DPI in the multiple regression models.           version with the ability of providing both positive and
                                                               negative feedback and direct instruction.
           Discussion and Conclusions
                                                                                 Acknowledgments
The results of our analyses are complicated in their de-
tails, but relatively straightforward in principle: Prior      This work is supported by ONR (Award N00014–07–
knowledge has a strong effect on learning outcomes, time       1–0040), and by the UIC Graduate College (2008/2009
                                                           1434

Dean’s Scholar Award).                                              ogy and Society, Special Issue on Women and Mi-
                                                                    norities in Information Technology, 22 (3), 20–27.
                      References                               Lane, H. C., & VanLehn, K. (2003). Coached program
                                                                    planning: Dialogue-based support for novice pro-
Anderson, J. R. (1986). Knowledge compilation: The
                                                                    gram design. In Proceedings of the Thirty-Fourth
     general learning mechanism. In R. S. Michalski,
                                                                    Technical Symposium on Computer Science Edu-
     J. G. Carbonell, & T. M. Mitchell (Eds.), Machine
                                                                    cation (SIGCSE ’03) (pp. 148–152). ACM Press.
     learning (Vol. 5, pp. 289–310). Los Altos, CA:
                                                               Lepper, M. R., Drake, M. F., & O’Donnell-Johnson, T.
     Kaufmann.
                                                                    (1997). Scaffolding techniques of expert human tu-
Cade, W. L., Copeland, J. L., Person, N. K., & D’Mello,
                                                                    tors. In K. Hogan & M. Pressley (Eds.), Scaffold-
     S. K. (2008). Dialogue modes in expert tutoring.
                                                                    ing student learning: Instructional approaches and
     In Intelligent tutoring systems (Vol. 5091, p. 470-
                                                                    issues. Cambridge, MA: Brookline.
     479). Springer Berlin / Heidelberg.
                                                               Litman, D. J., Rosé, C. P., Forbes-Riley, K., VanLehn,
Chi, M. T. H., de Leeuw, N., Chiu, M.-H., & LaVancher,
                                                                    K., Bhembe, D., & Silliman, S. (2006). Spoken
     C. (1994). Eliciting self-explanations improves un-
                                                                    versus typed human and computer dialogue tutor-
     derstanding. Cognitive Science, 18 (3), 439-477.
                                                                    ing. International Journal of Artificial Intelligence
Chi, M. T. H., Siler, S. A., Yamauchi, T., & Hausmann,
                                                                    in Education, 16, 145–170.
     R. G. (2001). Learning from human tutoring. Cog-
                                                               Lu, X. (2007). Expert tutoring and natural language
     nitive Science, 25, 471-533.
                                                                    feedback in intelligent tutoring systems. Unpub-
Corbett, A. T., & Anderson, J. R. (1990). The effect of             lished doctoral dissertation, University of Illinois -
     feedback control on learning to program with the               Chicago.
     Lisp tutor. In Proceedings of the Twelfth Annual          MacWhinney, B. (2000). The CHILDES project. tools
     Conference of the Cognitive Science Society (pp.               for analyzing talk: Transcription format and pro-
     796–803). Cambridge, Ma.                                       grams (Third ed., Vol. 1). Lawrence Erlbaum,
Di Eugenio, B., Fossati, D., Haller, S., Yu, D., & Glass,           Mahwah, NJ.
     M. (2008). Be brief, and they shall learn: Gen-           Mitrović, A., Suraweera, P., Martin, B., & Weerasinghe,
     erating concise language feedback for a computer               A. (2004). DB-suite: Experiences with three in-
     tutor. International Journal of AI in Education,               telligent, web-based database tutors. Journal of
     18 (4), 317-345.                                               Interactive Learning Research, 15 (4), 409–432.
Di Eugenio, B., Kershaw, T. C., Lu, X., Corrigan-              Moore, J. D., Porayska-Pomsta, K., Varges, S., & Zinn,
     Halpern, A., & Ohlsson, S. (2006). Toward a com-               C. (2004). Generating Tutorial Feedback with
     putational model of expert tutoring: a first report.           Affect. In FLAIRS04, Proceedings of the Seven-
     In FLAIRS06, the 19th International Florida AI                 teenth International Florida Artificial Intelligence
     Research Symposium. Melbourne Beach, FL.                       Research Society Conference.
Evens, M. W., & Michael, J. A. (2006). One-on-one              Ohlsson, S. (2008). Computational models of skill acqui-
     tutoring by humans and machines. Mahwah, NJ:                   sition. In R. Sun (Ed.), The Cambridge Handbook
     Lawrence Erlbaum Associates.                                   of Computational Psychology (p. 359-395). Cam-
Fossati, D., Di Eugenio, B., Brown, C., & Ohlsson, S.               bridge, UK: Cambridge University Press.
     (2008). Learning Linked Lists: Experiments with           Ohlsson, S., Di Eugenio, B., Chow, B., Fossati, D., Lu,
     the iList System. In ITS 2008, the 9th Interna-                X., & Kershaw, T. (2007). Beyond the code-
     tional Conference on Intelligent Tutoring Systems.             and-count analysis of tutoring dialogues. In AIED
Fox, B. A. (1993). The human tutorial dialogue project:             2007, the 13th International Conference on Artifi-
     Issues in the design of instructional systems. Hills-          cial Intelligence in Education.
     dale, NJ: Lawrence Erlbaum Associates.                    Person, N., Graesser, A., Magliano, J., & Kreuz, R.
Graesser, A., Person, N., Lu, Z., Jeon, M., & McDaniel,             (1994). Inferring what the student knows in one-to-
     B. (2005). Learning while holding a conversation               one tutoring: The role of student questions and an-
     with a computer. In L. PytlikZillig, M. Bodvars-               swers. Learning and Individual Differences, 6 (2),
     son, & R. Brunin (Eds.), Technology-based edu-                 205–229.
     cation: Bringing researchers and practitioners to-        Sun, R., Slusarz, P., & Terry, C. (2005). The interaction
     gether. Information Age Publishing.                            of the explicit and the implicit in skill learning: A
Graesser, A., Person, N., & Magliano, J. (1995). Collab-            dual-process approach. Psychological Review, 112.
     orative dialogue patterns in naturalistic one-to-one      VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan,
     tutoring. Applied Cognitive Psychology, 9, 495-522.            P. W., Olney, A., & Rosé, C. P. (2007). When
Katz, S., Allbritton, D., Aronis, J. M., Wilson, C., &              are tutorial dialogues more effective than reading?
     Soffa, M. L. (2003). Gender and race in predicting             Cognitive Science, 31 (1), 3–62.
     achievement in computer science. IEEE Technol-
                                                           1435

