UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Impact of Visual Information on Reference Assignment in Sentence Production
Permalink
https://escholarship.org/uc/item/33w9t002
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Coco, Moreno
Keller, Frank
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 The Impact of Visual Information on Reference
                                           Assignment in Sentence Production
                                             Moreno I. Coco (M.I.Coco@sms.ed.ac.uk) and
                                                     Frank Keller (keller@inf.ed.ac.uk)
                                               School of Informatics, University of Edinburgh
                                                10 Crichton Street, Edinburgh EH8 9AB, UK
                                Abstract                                 (depicted alternatively as Agent or Patient of the event). The
   Reference is the cognitive mechanism that binds real-world en-
                                                                         goal of the study was to investigate whether there is a se-
   tities to their conceptual counterparts. Recent psycholinguistic      quential relation between the visual entities fixated their lin-
   studies using eye-tracking have shed light on the mechanisms          guistic naming. A key observation is that the fixation of a
   used to establish shared referentiality across linguistic and vi-     visual entity and the production of a linguistic referent are
   sual modalities. It is unclear, however, whether vision plays an
   active role during linguistic processing. Here, we present a lan-     closely timelocked; the latency between the two modalities is
   guage production experiment that investigates how cued sen-           referred to as the eye-voice span.
   tence encoding is influenced by visual properties in naturalistic
   scenes, such as the amount of clutter and the number of poten-           A similar effect can be observed in VWP studies of lan-
   tial actors, as well as the animacy of the cue. The results show      guage comprehension (e.g., Altmann & Kamide 1999). Here,
   that clutter and number of actors correlate with longer response
   latencies in production, and with the generation of more com-         participants listen to a speech stimulus while viewing a scene,
   plex structures. Cue animacy interacts with both clutter and          and typically they fixate a visual entity shortly after a cor-
   number of actors, demonstrating a close coupling of linguistic        responding linguistic referent has been encountered in the
   and visual processing in reference assignment.
                                                                         speech. The VWP has mainly been used in psycholinguistic
   Keywords: language production, sentence encoding, picture             research, with a focus on how specific aspects of the linguis-
   description, visual information, clutter, accessibility, animacy.
                                                                         tic stimuli cause certain visual objects to be selected as refer-
                            Introduction                                 ents (Knoeferle et al., 2006; Snedeker & Trueswell, 2003). In
                                                                         these studies, visual information is merely used to provide a
When humans comprehend or produce language, they rarely
                                                                         context for language processing, without taking into account
do so in isolation. Linguistic information often occurs syn-
                                                                         mechanism of scene comprehension studied extensively in
chronously with visual information, e.g., in everyday activi-
                                                                         the visual cognition literature (Henderson et al., 2007). This is
ties such as attending a lecture or following directions on a
                                                                         compounded by the fact that the visual stimuli used in VWP
map. The visual context constrains the interpretation of the
                                                                         studies typically include clip-art objects arranged in arrays or
linguistic material, and vice versa, making processing more
                                                                         pseudo-scenes. The resulting visual processing is of reduced
efficient and less ambiguous. Linguistic and visual process-
                                                                         complexity, perhaps consisting merely of responses to the lin-
ing have been investigated extensively in isolation, but there
                                                                         guistic input. Moreover, this visual simplicity often results in
is little work that explicitly relates the two modalities to each
                                                                         a one-to-one mapping between visual and linguistic referents.
other.
                                                                         This is unrealistic compared to naturalistic scenes where the
   In this paper, we focus on a particular aspect of syn-
                                                                         same linguistic label can often correspond to multiple visual
chronous linguistic and visual processing, viz., the formation
                                                                         objects.
and maintenance of shared reference between the two modal-
ities. Essentially, this is the problem of determining that a vi-           A realistic theory of the formation and maintenance of ref-
sually perceived entity such as CUP is referentially linked to a         erence across modalities has to treat visual information on a
sequence of words such as the cup. On the linguistic level, the          par with linguistic information. Such a theory must explain
complexity of this task increases if referring expressions are           how mechanisms known to operate independently in both
embedded into larger linguistic structures (e.g., the cup on the         modalities cooperate in referent assignment. The present pa-
table, the spoon in the cup). On the visual level, complexity            per aims to contribute to such a theory. On an abstract level,
increases if the CUP is embedded into a scene, for example an            the hypothesis we test is that the visual stimulus properties
interior such as a kitchen, which can contain a large number             exert an influence on linguistic processing. Previous work has
of objects.                                                              investigated the influence of low-level visual properties such
   Insights into the mechanisms underlying the interaction be-           as saliency (a composite of color, intensity, and orientation,
tween visual and linguistic processing can be obtained using             Itti & Koch 2000). In a VWP study, Coco & Keller (2008)
the Visual World Paradigm (VWP, Tanenhaus et al. 1995).                  found that saliency influences the resolution of prepositional
In VWP studies, participants are engaged in a synchronous                phrase (PP) attachment ambiguities in language comprehen-
visual and linguistic task while their eye-movements are                 sion. They found that saliency has referential effects; it is used
recorded. For example, in a language production study con-               to predict which visual objects can be encoded as post-verbal
ducted by Griffin & Bock (2000), participants were eye-                  arguments in a given sentence.
tracked while they described pictures containing two actors                 However, it is important to note that low-level visual fea-
                                                                     274

tures such as saliency are not referential per se; they are          Clutter A way to define reference in vision is to look for
properties of image regions, not of objects (Henderson et al.,       a global measure of visual information. Measures of visual
2007). It is therefore necessary to focus on higher-level visual     information can be defined in various ways; a common ap-
properties, which are clearly object-driven and likely to affect     proach uses the notion of set size. The bigger the set of objects
the mechanisms of referent assignment in sentence encoding           in a visual search task, the slower the response time. Hence,
directly. In this paper, we present a language production ex-        the number of countable visual objects has been assumed to
periment that investigates how scene descriptions are influ-         give a direct measure of visual information (Wolfe, 1998).
enced by high-level features such as visual clutter (the density     However, this notion of visual information has recently been
of objects in a scene) and of the number of animate referents        criticized (Rosenholtz et al., 2007), especially in the con-
available. We use naturalistic scenes as visual stimuli to avoid     text of naturalistic scenes, where it can be come difficult, if
the limitations of visual arrays and clip-art images, tradition-     not impossible, to define and count each single object in the
ally used in the VWP literature.                                     scene.
                                                                        An alternative way of quantifying visual information is
                         Experiment                                  clutter (Rosenholtz et al., 2007) (see Figure 1 for an exam-
                                                                     ple). Clutter is defined as the state (organization, represen-
Design                                                               tation) of visual information in which visual search perfor-
                                                                     mance starts to degrade. Clutter can be modeled statistically
In this experiment, participants had to describe a naturalistic
                                                                     and quantified using the feature congestion method (for de-
scene, after being prompted by a single word (the description
                                                                     tails see Rosenholtz et al. 2005). In our study, we use this
cue). As dependent variables we recorded Looking Time, i.e.,
                                                                     Clutter measure to investigate the effect of the amount of vi-
the time that elapsed before the onset of the response, De-
                                                                     sual information on sentence encoding.
scription Time, i.e., the time taken to complete the response,
and we also investigated the syntactic structure of the re-
sponse produced. The design of the experiment manipulated            Actors and Animacy A crucial feature that distinguishes
both visual and linguistic referential information. We varied        different types of real-world entities is animacy. Animacy
the total amount of visual information present in the scene in       is known to play a role in language production; in partic-
the factor Clutter (Minimal vs. Cluttered). We also manipu-          ular, it can influence the assignment of grammatical func-
lated the number of animate objects present in the scene in          tions and word order (Branigan et al., 2008). Animate entities
the factor Actors (One vs. Two). On the linguistic side, we          are conceptually more accessible than inanimate ones (Levelt
varied the prompt given to participants for their description        et al., 1999) and therefore privileged during syntactic encod-
in the factor Cue, which could refer either to an animate or an      ing. This is reflected by the fact that animate entities are more
inanimate object in the scene (Animate vs. Inanimate). The           likely to be encoded with the grammatical function subject,
scenes were designed such that they always contained at least        while inanimate entities occur mostly with the function ob-
one animate object and two identical inanimate objects, so           ject.
as to introduce systematic visual referential ambiguity. As an          In this study we took a broader view of the feature ani-
example, see Figure 1, where the clipboard is the ambiguous          macy; animacy is not only a linguistic notion, but it is also
inanimate object. Note that the animate objects are referen-         visually encoded. We therefore manipulated animacy in both
tially unambiguous, even in the two actor condition (man and         the linguistic and the visual modality. Visually, we introduce
woman in the example stimulus).                                      different degrees of animacy by changing the number of ac-
   The null hypothesis for this experiment is that visual and        tors depicted in the scene. Linguistically, we either gave an
linguistic factors do not interact in language processing. This      animate or an inanimate noun as the cue for sentence produc-
would mean that Clutter and Actors should only influence             tion.
Looking Time in a way that is compatible with behavior
in standard visual search tasks: we expect longer Looking            Method
Time in the Cluttered condition, as more objects have to be          The experimental design crossed three factors, each with two
searched, and longer Looking Time also in the Two Actors             levels. The two visual factors were number of Actors in the
condition, which contains an additional object. Our experi-          scene (One or Two) and the degree of visual Clutter (Minimal
mental hypothesis is that visual information has an impact on        or Cluttered). The linguistic factor was the Cue given to the
language production, which means that we expect an inter-            participants to prompt their sentence production (Animate or
action between the visual factors Clutter and Actor and the          Inanimate).
linguistic factor Cue (in addition to the main effects of the           As stimuli, we created a set of 24 photo-realistic scenes
visual factors that may be caused by standard visual search          using Photoshop by cutting and pasting visual objects from
processes).                                                          a set of preexisting photographs. Differences in luminosity,
   In the following we will give a more detailed motivation          contrast and color balance between the different photographs
for the factors included in the design and describe how they         were adjusted through an accurate use of layers, luminosity
were operationalized.                                                masks and color balancing. In order to (1) control for the se-
                                                                 275

                                                                      Results and Discussion
                                                                      We analyze two response time measures. The first one is
                                                                      Looking Time, i.e., the time participants spent scanning the
                                                                      image before starting to type. It is calculated from the onset
                                                                      of the trial until participants pressed the first key on the key-
                                                                      board. The second response time measure, Description Time,
                                                                      is the time participants took to type their response. It is calcu-
                                                                      lated from the first key press until Enter was hit to move on
                                                                      to the next trial.
                                                                         We also analyzed the syntactic patterns in the responses
                                                                      produced by participants. For this, we tagged each sentence
                                                                      produced using a an automatic part-of-speech tagger, viz.,
                                                                      Ratnaparkhi’s (1996) maximum entropy tagger, which per-
                                                                      forms with an accuracy of 96.6%. The tagger uses the Penn
                                                                      Treebank tagset to assign syntactic categories to words. We
                                                                      collapsed the various tags for nouns in the tagset (e.g., NNS,
                                                                      NNP) and verbs (e.g., VBD, VBN) to two general categories
Figure 1: An example of an experimental stimulus, with the            (NN, VB). For each sentence, we recorded the frequency of
four visual variants which occur in the experiment and the            these two categories, as well as the occurrence of existen-
two linguistic cues presented.                                        tial there and clause coordinator and. We also identified and
                                                                      counted the number of passive constructions (for this the full
                                                                      tag set was used, which marks passive verb morphology).
                                                                         The statistical analyses were carried out using linear
mantic variability across visual scenes and (2) ground lan-           mixed-effect models (Jaeger, 2008) to determine the effect
guage production in a restricted semantic domain, all pictures        the categorical predictor variables on both reaction times and
were created using six different interior environments: bath-         syntactic frequency. We chose mixed models for their ability
room, bedroom, dining room, entrance, kitchen, and office.            to capture both fixed and random effects (Baayen et al., 2008).
Each interior was represented by four different scenes. For           We included the following predictors in our analysis: Actors
each scene, we created four variants by manipulating Clut-            (One or Two), Clutter (Minimal, Cluttered), Cue (Animate,
ter and Actors, as illustrated in Figure 1. The scenes were           Inanimate) and Language (Native, NonNative). The baseline
designed such that the inanimate object was referentially am-         on which the Intercept was calculated was given by the condi-
biguous, i.e., each picture contained two visual instances of         tion Cue-Inanimate, Actor-One, Clutter-Cluttered, Language-
it, while the animate one was unambiguous, even in the Two            Native. The mixed models were built and evaluated following
Actors condition.                                                     the model selection procedure suggested by Crawley (2007).
                                                                      We started with a fully specified model containing all the pre-
    In the experiment, participants were first presented with a
                                                                      dictors and all possible interactions and then we reduced the
set of instructions explaining the task and giving examples.
                                                                      model iteratively by removing the highest order interaction
After a practice phase, they saw one visual stimulus at a time,
                                                                      with the highest p-value. The estimates were recomputed at
together with the linguistic cue. They were instructed to pro-
                                                                      each iteration.
vide a written description of the stimulus using the cue. The
total of 192 different items were distributed over four lists us-
ing a Latin square design. Each subject saw one of the lists,         Reaction Times Table 1 presents the coefficients and p-
i.e., 48 stimuli in total (each of the 24 scenes was presented        values of the mixed model for Looking Time (only significant
twice, once with animate and one with inanimate cue). The             predictors and interactions are included). The model intercept
stimuli were randomized for each participant, and presented           represents the response time in the baseline condition in mil-
without fillers. The experiment took about 15 minutes in total.       liseconds, and coefficients indicate the effect a given predic-
                                                                      tor has on Looking Time (again in milliseconds). We find that
    The experiment was realized using the WebExp software
                                                                      participants were significantly faster to scan the pictures in
package for conducting psychological experiments over the
                                                                      the condition Clutter-Minimal compared to Clutter-Cluttered.
web. WebExp is able to measure reaction times with accu-
                                                                      This finding is likely to be an effect of visual search, as the
racy comparable to that of lab-based experiments, as shown
                                                                      scene needs to be searched for the cued object; in the Clut-
by Keller et al. (2009) for self-paced reading data.
                                                                      tered condition more objects are present, leading to longer
    Participation was open to both native and non-native speak-       search time.
ers of English (this was included as a factor in the analy-              Significantly shorter Looking Time was also observed in
sis). The sample included 32 participants, including 16 native        the Cue-Animate condition. Again, this can be explained in
speakers and 16 non-native speakers.                                  terms of visual search behavior, as our stimuli contain more
                                                                  276

                                       Looking Time                                                              Noun
              Predictor           Coefficient        p                            Predictor            Coefficient         p
      Intercept                    3774.6        0.0006                    Intercept                     2.2520        0.0002
      Clutter-Min                    -503         0.01                     Cue-Anim                      -0.3333       0.0001
      Cue-Anim                     -1097.4        0.01                     Actors-Two:Cue-Anim           0.2252          0.01
      Language-NonNative           1120.3         0.04                                                            Verb
      Actors-Two:Cue-Anim           -468.4        0.05                            Predictor            Coefficient         p
      Clutter-Min:Cue-Anim           596.2        0.01                     Intercept                     1.6129        0.0006
                                     Description Time                      Clutter-Min                   0.1968         0.004
              Predictor           Coefficient        p                     Cue-Anim                      0.2307        0.0001
      Intercept                     12053        0.0024                    Actors-Two:Cue-Anim           0.1624          0.03
      Actors-Two                     987.2        0.04                     Clutter-Min:Cue-Anim          -0.2267        0.002
      Language-NonNative           2139.3         0.01
      Cue-Anim                     -1801.1       0.0001              Table 2: Mixed effects models of noun frequency and verb
                                                                     frequency
Table 1: Mixed effects models of Looking Time and Descrip-
tion Time
                                                                     is thus shortened by the larger set of possibilities. Moreover,
                                                                     the unambiguous visual reference of Cue-Animate may boost
inanimate than animate cues, thus making it easier to discrim-       the selection of those conceptual structures that are related to
inate animate objects, leading to reduced search time. In addi-      the actor cued, contributing on the decrease of looking time.
tion, the cue animate object was always unambiguous, while           This interpretation is supported also by our syntactic analysis
the cued inanimate object was always present twice in the            (see next section) in which Actor and Cue-Animate positively
scene, creating referential ambiguity, and thus increasing vi-       correlate with the use of nouns and verbs. Participants pro-
sual search time. There may also be an explanation in linguis-       duce longer sentence structures, often encoding both Actors.
tic terms: As mentioned above, animate entities are conceptu-           Table 1 also presents the mixed model for Description
ally more accessible than inanimate ones, which gives them a         Time (again only significant predictors and interactions are
privileged status during syntactic encoding.                         included). The results overlap with those for Looking Time.
   We also found that in the condition Language-NonNative,           For condition Cue-Animate, participants were faster to gen-
participants take longer to scan the picture. This can be ex-        erate a sentence compared to Cue-Inanimate. As for Looking
plained by the fact that non-native speaker presumably take          Time, this result can be explained by the fact that animate
longer to decode the cue and to plan their utterance.                entities are more accessible in language production, and that
   Turning to the interactions, we found that Clutter-Minimal        visual search is faster, as there is only at most one other ani-
significantly interacts with Cue-Animate: participants took          mate object in the scene. We also find significantly increased
longer to respond to animate prompts in the minimal clut-            Description Time for the Actor-Two condition. An inspection
ter condition. This interaction cannot be explained purely in        of the responses (see below) shows that participants tend to
terms of visual search. The Clutter-Minimal, Cue-Animate             encode both actors in their descriptions of the scene, which
condition is the one with the fewest competing objects (mini-        explains why encoding takes longer in this condition, com-
mal clutter) and only one or two animate objects to consider;        pared to the Actor-One condition, in which only one actor
visual search should therefore be particularly fast, and the in-     is encoded. Again, non-native participants show a longer re-
teraction should be absent or have a negative coefficient. The       sponse time than native ones, presumably because sentence
fact that we find a positive interaction indicates that a lin-       production is slower in non-native speakers.
guistic process is at work. In a visual scene with few objects
it is more difficult to retrieve enough information regarding        Syntactic Categories Table 2 presents the results for the
actions that a potential actor can perform. Thus, participants       syntactic analysis of the picture descriptions generated by the
spend more time scanning the scene and planning their utter-         participants. We fitted separate mixed models to predict the
ance before sentence encoding starts.                                number of nouns and the number of verbs included in the
   There is also a significant negative interaction of Actors-       responses. Again, only significant predictors and interactions
Two and Cue-Animate; Looking Time is reduced in this con-            are listed in the table; the intercept represents the noun or
dition. Again, this cannot be explained purely in visual terms;      verb frequency in the baseline condition, and the coefficients
the presence of two actors cued by the animate cue should            indicate how this frequency increases or decreases under the
lead to longer search times, as two objects need to be con-          influence of the relevant predictor.
sidered instead of one. Instead, we find a negative coefficient         The results indicate that significantly fewer nouns are pro-
for this interaction. Presumably, the more animate entities the      duced in Cue-Animate condition. This condition was visually
scene contains, the more conceptual structures are activated.        unambiguous, and thus required less elaborate descriptions
The time spent on planning a conceptual structure to encode          compared to the Cue-Inanimate condition, for which partici-
                                                                 277

pants generated longer sentences in order to unambiguously            Syntactic Constructions We also selectively analyzed a
pick out one of the two visual referents available in this con-       number of syntactic constructions contained in the responses
dition. Moreover, the competition between the two visual ob-          generated by the participants. Such construction provide in-
jects for the inanimate cue was often resolved by encoding            formation about the sentence structures employed to describe
both visual referents within the same sentence structure. An          the pictures. We counted how often participants employed the
example of a sentence produced in this condition is The mug           existential there construction. The results show that this con-
is beside the man, another is on top of the files, both mugs          struction occurred less frequently in the Cue-Animate con-
have pencils in them. Except of the referring expression itself,      dition (coefficient −0.2153, p < 0.0001). This indicates that
all nouns are used in combination with spatial prepositions to        participants were less likely to give static spatial descriptions
unambiguously differentiate each visual referent.                     of animate visual referents, compared to inanimate ones.
                                                                         We also find that and is used less frequently in the Cue-
   However, when Cue-Animate was in interaction with                  Animate condition (coefficient −0.0868, p < 0.0001). This
Actor-Two, participants produced significantly more nouns.            result can be attributed to the ambiguous visual reference of
This correlates with the shorter Looking Times found for              Cue-Inanimate. The use of and marks a strategy of ambiguity
the same interaction. Participants often referentially encoded        resolution when both visual referents for Cue-Inanimate are
both visual actors within the same sentence structure. An ex-         linguistically encoded. The connection between referents is
ample is A man stands behind a counter in a hotel while a             established by combining clauses through coordination.
customer writes on a piece of paper. Even though the cue
                                                                         When we analyzed the number of passive constructions,
given (here, the man) refers only to one actor and was vi-
                                                                      we again found a significant negative effect of Cue-Animate
sually unambiguous, the participant encoded also the second
                                                                      (coefficient −0.0436, p < 0.0001). This is in line with stan-
actor.
                                                                      dard findings in the sentence production literature: animate
   Turning now to the analysis of the number of verbs pro-            entities are likely to be realized as subjects of active construc-
duced, we again find a significant effect of Cue-Animate, but         tions, while inanimate tend to be realized subjects of passive
with a positive coefficient, which means that participants gen-       constructions (assuming that the cued entity is typically real-
erated more verbs than in the Cue-Inanimate condition. This           ized as a subject). An example of a production that contains
underlines the connection between the feature Animacy and             the use of both coordination and passive is A teddy is being
the semantics of verbs. As verbs encode actions, they are less        hugged by the girl sitting on the bed and another teddy is
likely to occur in descriptions of inanimate entities. The latter     sitting on the floor at the corner of the bed.
tend to include verbs describing static, mostly spatial, rela-
tions like lie or place, whereas animate entities can be related                           General Discussion
to a broader range of events, both static and dynamic, result-        The overall goal of this study was to investigate how visual
ing in more verbs being generated.                                    factors influence sentence encoding. The analysis focused on
   An interaction between Actor-Two and Cue-Animate is                shared reference between the two modalities, and the mech-
also present, which is consistent with the main effect of Cue-        anisms through which reference is established in language
Animate. The more animate entities are presented in the vi-           production. We assumed an interactive account of visual and
sual scene, the more verbs are used to relate them with the           linguistic processing, where informational changes in one
event that is being encoded. An example description is A              modality are reflected in the processing of the other one. In
woman drinks from a cup while a man prepares a chicken                our experimental design, we manipulated different aspects of
to be cooked.                                                         visual reference such as visual clutter and the number of po-
                                                                      tential actors, and the animacy of the cue used for sentence
   The factor Clutter-Minimal also has a significantly posi-          production. Moreover, we systematically introduced visual
tive coefficient, which means that more verbs are generated if        referential ambiguity for the inanimate cue in order to inves-
the scene is uncluttered. However, there is also a significant        tigate the strategies of ambiguity resolution adopted.
negative interaction of Clutter-Minimal with Cue-Animate.                The analysis of Looking Time shows significant effects of
The minimal amount of visual information available in the             the visual factors such as Clutter and Actors: the more clut-
Clutter-Minimal scenes makes it more difficult to select and          ter or actors, the more time the participants spend before
encode the actions performed by the actor, resulting in fewer         starting to type the sentence. The Animacy of the cue was
verbs being generated. This result is in line with the longer         also significant: an inanimate cue resulted in longer Looking
Looking Time for the same interaction. We can assume that             Time, mainly because of visual referential ambiguity. How-
the greater number of verbs found in Clutter-Minimal can be           ever, more interesting were the interactions between the vi-
attributed to Cue-Inanimate, in which the ambiguous visual            sual factors and Animacy. If we assumed independence be-
reference leads to more elaborate descriptions. An example            tween visual and linguistic processing, we would expect re-
description that illustrates this interpretation is An open book      sponse latencies typical of standard visual search tasks, based
is sitting on the counter and there is another one sitting on         on the referential properties of the cue and influenced only
the table.                                                            by the visual properties of the stimulus. Instead, we found a
                                                                  278

clear interaction of visual information and Animacy. A visual                                 References
scene with minimal clutter means that the set of actions that        Altmann, G., & Kamide, Y. (1999). Incremental interpreta-
can be used to relate animate actors is impoverished. Thus,             tion at verbs: restricting the domain of subsequent refer-
longer visual search is required to integrate the animate cues          ence. Cognition, 73, 247–264.
with information of the scene, the opposite of what is pre-          Baayen, R., Davidson, D., & Bates, D. (2008). Mixed-effects
dicted under an explanation in terms of visual search alone.            modeling with crossed random effects for subjects and
On the other hand, two actors in a scene mean a larger set              items. Journal of memory and language, 59, 390–412.
of conceptual structures is available to relate to the animate
                                                                     Branigan, H., Pickering, M., & Tanaka, M. (2008). Contri-
cue. Also, an animate actor is easier to relate to another an-
                                                                        bution of animacy to grammatical function assignment and
imate actor in the same sentence with an action description,
                                                                        word order during production. Lingua, 2, 172–189.
compare to an animate and an inanimate entity. This inter-
pretation meshes with the results we obtained for the syn-           Coco, M. I., & Keller, F. (2008). Competition between visual
tactic analysis of the responses produced by participants. For          and linguistic resources. Presented as poster at AMLAP,14,
the Actors-Two and Cue-Animate conditions, we found that                Cambridge, UK.
longer sentences were produced (containing more nouns and            Crawley, M. (2007). The R book. John Wiley and Sons, Ltd.
verbs), often encoding both actors. Such results can only be         Griffin, Z., & Bock, K. (2000). What the eyes say about
explained in an account in which linguistic and visual pro-             speaking. Psychological science, 11, 274–279.
cessing interact closely.                                            Henderson, J., Brockmole, J., Castelhano, M., & Mack,
   We also analyzed Description Time and the syntactic struc-           M. (2007). Visual saliency does not account for eye-
ture of the responses and found that these are mainly influ-            movements during visual search in real-world scenes. Eye
enced by the animacy of the cue and the presence of visual              movement research: insights into mind and brain.
referential ambiguity. When the cue was inanimate, partic-           Itti, L., & Koch, C. (2000). A saliency-based search mecha-
ipants spent more time resolving the visual ambiguity. The              nism for overt and covert shifts of visual attention. Vision
sentences produced in this condition contained more nouns,              Research, 40, 1489–1506.
which were used to spatially disambiguate between the two            Jaeger, T. (2008). Categorical data analysis: Away from
competing visual objects. Moreover, disambiguation often                anovas (transformation or not) and towards logit mixed
occurred together with the use of conjunction and. In line              models. Journal of memory and language, 59, 433–446.
with previous research on language production, the use of            Keller, F., Gunasekharan, S., Mayo, N., & Corley, M. (2009).
passives and existential there was correlated with cue ani-             Timing accuracy of web experiments: A case study using
macy. An inanimate cue is more likely to be a subject of a              the WebExp software package. Behavior Research Meth-
passive and correlated with static spatial descriptions.                ods, 41, 1–12.
   Our results are limited by the fact that we only had two re-      Knoeferle, P., Crocker, M., Scheepers, C., & Pickering, M.
sponse time measures available, Looking Time and Descrip-               (2006). The influence of the immediate visual context on
tion Time. A more fine-grained way of investigating the time            incremental thematic role-assignment:evidence from eye-
course of sentence production is desirable, e.g., using eye-            movements in depicted events. Cognition, (pp. 481–529).
tracking. In particular we have to be careful not to presup-         Levelt, W., Roelofs, A., & Meyer, A. (1999). A theory of
pose that Looking Time corresponds to the time spend doing              lexical access in speech production. Behavioral and brain
visual search and planning the resulting utterance, while De-           sciences, (pp. 1–75).
scription Time corresponds to the time spend generating the          Ratnaparkhi, A. (1996). A maximum entropy model for part-
utterance. In reality, these two processes are likely to happen         of-speech tagging. In In Proceedings of the conference
in an interleaved manner, rather than in sequence.                      on empirical methods in natural language processing, (pp.
   In future work we will further investigate integrated ref-           133–142).
erence from a lexical point of view. In particular, we are in-       Rosenholtz, R., Li, Y., & Nakano, L. (2007). Measuring vi-
terested in the relationship between contextual scene infor-            sual clutter. Journal of Vision, 7, 1–22.
mation (bedroom, kitchen, etc.) and the lexical items gen-           Rosenholtz, R., Mansfield, J., & Jin, Z. (2005). Feature con-
erated. This could be investigated using additional statisti-           gestion, a measure of display clutter. SIGCHI, (pp. 761–
cal techniques, e.g., cluster analysis. Furthermore, we are             770).
planning to conduct an eye-tracking experiment to measure            Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid
the impact of visual factors on sentence encoding more di-              ambiguity: Effects of speaker awareness and referential
rectly. Such a study is likely to shed light on (1) the changes         context. Journal of Memory and Language, (pp. 103–130).
in eye-movement patters triggered by visual factors such as          Tanenhaus, M. S.-K., M.J., Eberhard, K., & Sedivy, J. (1995).
clutter, (2) the effects of cueing a linguistic referent which          Integration of visual and linguistic information in spoken
may, e.g., result in more localized visual search behavior, and         language comprehension. Science, (pp. 632–634).
(3) whether visual referential ambiguity and its linguistic res-     Wolfe, J. (1998). Visual search. Attention, (pp. 13–73).
olution are correlated.
                                                                 279

