UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
From Social Networks To Distributional Properties: A Comparative Study On Computing
Semantic Relatedness
Permalink
https://escholarship.org/uc/item/49n6b52b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Cramer, Irene
Waltinger, Ulli
Wandmacher, Tonio
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                           From Social Networks To Distributional Properties:
                    A Comparative Study On Computing Semantic Relatedness
                                    Ulli Waltinger (ulli marc.waltinger@uni-bielefeld.de)
                                                  Text Technology, Bielefeld University
                                              Irene Cramer (irene.cramer@udo.edu)
                                         Faculty of Cultural Studies, TU Dortmund University
                                Tonio Wandmacher (tonio.wandmacher@uni-osnabrueck.de)
                                        Institute of Cognitive Science, University of Osnabrück
                              Abstract                                 behaving similarly. For example they can show a strong asso-
                                                                       ciative relationship (e.g. ball - goal), and they can be related
   In recent years a variety of approaches in computing seman-
   tic relatedness have been proposed. However, the algorithms         across different linguistic categories (e.g. milk - white, dog -
   and resources employed differ strongly, as well as the results      bark). With respect to the automatic computation of SR, how-
   obtained under different experimental conditions. This article      ever, many research questions remain unanswered. As stated
   investigates the quality of various semantic relatedness mea-
   sures in a comparative study. We conducted an extensive ex-         above, many algorithms were presented in the past decade,
   periment using a broad variety of measures operating on so-         but thorough evaluations and comparisons of their ability to
   cial networks, lexical-semantic nets and co-occurrence in text      capture SR in a human-like manner are still rare. In this work
   corpora. For two sample data sets we obtained human relat-
   edness judgements which were compared to the estimates of           we therefore present a study comparing various semantic re-
   the automated measures. We also analyzed the algorithms im-         latedness measures. We evaluate sixteen different algorithms
   plemented and resources employed from a theoretical point of        involving four different resources based on a human judge-
   view, and we examined several practical issues, such as run
   time and coverage. While the performance of all measures is         ment experiment, and we analyze the algorithms from a the-
   still mediocre, we could observe that in terms of of coverage       oretical and practical point of view. The paper is organized
   and correlation distributional measures operating on controlled     as follows: the subsequent section describes two works rep-
   corpora perform best.
                                                                       resenting the methodological basis for our study. The various
   Keywords: Semantic Relatedness; Semantic Similarity; Hu-            semantic relatedness measures employed in our experiment
   man Judgement; Social Networks; WordNet; LSA;
                                                                       are described in Section Semantic Relatedness Measures. The
                                                                       experimental setup as well as the results obtained are pre-
                          Introduction                                 sented in Section Evaluation.
The computation of semantic relatedness (SR) has become
an important task in many NLP applications such as spelling                                   Related Work
error detection, automatic summarization, word sense dis-              The task of estimating SR between two given lexical items
ambiguation, and information extraction. In recent years a             can be performed by humans in an effortless and intuitive
large variety of approaches in computing SR has been pro-              manner. However, this notion is very difficult to formalize
posed. However, algorithms and results differ depending on             from a psycholinguistic or computational point of view. In
resources and experimental setup. It is obvious that SR plays          terms of an evaluation of SR algorithms, most commonly
a crucial role in the lexical retrieval of humans. In various          human judgement experiments are conducted. The perfor-
priming experiments it could be shown that semantically re-            mance of an SR measure is determined by directly compar-
lated terms influence the semantic processing of one another.          ing the automatic computed results with those gained from
For example, if ”bread” is primed by ”butter” it is recog-             the human judgements via correlation. As a most prominent
nized more quickly. Moreover, many theories of memory are              example Budanitsky and Hirst (Budanitsky & Hirst, 2006)
based on the notion of SR. The spreading activation theory             presented a comparison of five semantic relatedness mea-
of (Collins & Loftus, 1975) for example groups lexical items           sures for the English language. They recommended a three-
according to their SR in a conceptual graph. Similar ideas can         level evaluation including theoretical examination, compari-
be found in the ACT theory of Anderson (Anderson, 1983).               son with human judgements and evaluation with respect to a
The question that arises for us is, how this kind of relatedness       given NLP-application. The measures were evaluated on two
can be determined by automatic means. In the literature the            different data sets: The first data set was compiled by Ruben-
notion of SR is often confounded with semantic similarity;             stein and Goodenough (Rubenstein & Goodenough, 1965);
there is however a clear distinction between these notions.            it contained 65 word-pairs. The second set, containing 30
Two terms are semantically similar if they behave similarly            word pairs, was compiled by Miller and Charles (Miller &
in a given context and if they share some aspects of meaning           Charles, 1991). For each of the five measures, Budanitsky
(e.g. in the case of synonyms or hypernyms). On the other              and Hirst reported correlation coefficients between 0.78 and
hand two terms can be semantically strongly related without            0.83. Boyd-Graber et al. (Boyd-Graber, Fellbaum, Osher-
                                                                   3016

son, & Schapire, 2006) presented a list of 120,000 concept            hyponym-tree.
pairs, which were rated by 20 subjects with respect to their
evocation - how much one concept brings to mind the other.                                                        2 · sp(s1 , s2 )
                                                                                       relLC (s1 , s2 ) = − log                        (1)
Volunteers were given manual instruction before the experi-                                                          2 · DTree
ment and were trained on a sample of 1000 randomly selected
                                                                      s1 and s2 : the two synonym sets examined; sp(s1 , s2 ):
pairs. However, it has to be pointed out that this approach
                                                                      length of shortest path between s1 and s2 in the hyponym-
focuses on constructing new relations within the lexical re-
                                                                      tree; DTree : depth of the hyponym-tree
source WordNet (Fellbaum, 1998) rather than assessing se-
mantic relatedness. Still, four different semantic relatedness      • Wu-Palmer (Wu & Palmer, 1994): The Wu-Palmer mea-
measures were compared. Results (correlation coefficients)            sure utilizes the least common subsumer in order to
ranged only between 0.008 and 0.131 Nevertheless, the ap-             compute the similarity between two synonym sets in a
proach of Boyd-Graber et al. (2006) is based to an important          hyponym-tree.
extent on human involvement. We argue that this is a cru-
cial condition for all approaches that aim to model aspects                                               2 · depth(lcs(s1 , s2 ))
                                                                                  relWP (s1 , s2 ) =                                   (2)
of human lexical processing (such as computing SR), there-                                               depth(s1 ) + depth(s2 )
fore we also make use of this kind of evaluation. Moreover,
many works presenting new SR algorithms prove their accu-             depth(s): length of the shortest path form root to vertex s;
racy with respect to one or two similar approaches. A large           lcs(s): least common subsumer of s
and standardized evaluation campaign is however still miss-         • Resnik (Resnik, 1995): Given a hyponym-tree and a fre-
ing. For this reason we consider in our study a large variety         quency list, the Resnik measure utilizes the information
of SR measures, and evaluate them with respect to a human             content in order to compute the similarity between two syn-
judgement experiment (on German data), first presented by             onym sets.
Cramer&Finthammer (Cramer, 2008).                                                                       ∑w∈W (s) freq(w)
                                                                                            p(s) :=                                    (3)
                                                                                                           TotalFreq
           Semantic Relatedness Measures
                                                                                                IC(s) := − log p(s)                    (4)
In general, we split all implemented algorithms on the basis
of their resources into three different groups. Net-based mea-                           relRes (s1 , s2 ) = IC(lcs(s1 , s2 ))         (5)
sures make use of a lexical-semantic net like the already men-
                                                                      freq(w): frequency of a word within a corpus; W (s): set of
tioned WordNet, which has been developed for many differ-
                                                                      the synonym set s and all its direct/indirect hyponym syn-
ent languages (e.g. GermaNet (Lemnitzer & Kunze, 2002)).
                                                                      onym sets; TotalFreq: sum of the frequencies of all words
Most of the implemented algorithms use a hyponym-tree in-
                                                                      in GermaNet; IC(s): information content of the synonym
duced from the given lexical-semantic net. Since such a re-
                                                                      set s
source models only systematic semantic relations such as hy-
ponymy or meronymy, unsystematic connections (i.e. associ-          • Jiang-Conrath (Jiang & Conrath, 1997): Given a
ations) can not be directly computed. Distributional measures         hyponym-tree and a frequency list, the Jiang-Conrath mea-
consider semantics on the basis of similar distributional prop-       sure computes the distance of two synonym sets.
erties of words in large text corpora. Such approaches deduce
SR on the basis of co-occurences of features from text or web            distJC (s1 , s2 ) = IC(s1 ) + IC(s2 ) − 2 · IC(lcs(s1 , s2 )) (6)
data. As a third group we regard social networks such as the
online encyclopedia Wikipedia. Wikipedia driven approaches          • Lin (Lin, 1998): Given a hyponym-tree and a frequency
are able not only to comprise statistics from the entire text         list, the Lin measure computes the semantic relatedness of
collection, but are also able to induce the category taxonomy         two synonym sets.
using classical graph algorithms. The following three sec-
tions outline the sixteen different algorithms that we have im-                                             2 · IC(lcs(s1 , s2 ))
                                                                                       relLin (s1 , s2 ) =                             (7)
plemented for our evaluation.                                                                                IC(s1 ) + IC(s2 )
Net-based Measures                                                  • Hirst-StOnge (Hirst & St-Onge, 1998): This measure
With the development of lexical-semantic nets (such as the            computes the semantic relatedness on the basis of the en-
Princeton WordNet) in the mid-1990’s various measures for             tire GermaNet graph structure. It classifies the relations
computing SR have been proposed. The eight most prominent             considered into 4 classes: extra strongly related, strongly
algorithms were implemented using GermaNet (Lemnitzer &               related, medium strongly related, and not related.
Kunze, 2002) as a resource.
                                                                    • Tree-Path: The tree-path measure computes the length of
• Leacock-Chodorow (Leacock & Chodorow, 1998): This                   a shortest path between two synonym sets in a hyponym-
   measure computes the length of the shortest path between           tree.
   two synonym sets and scales it by the depth of the complete                             distTree (s1 , s2 ) = sp(s1 , s2 )          (8)
                                                                3017

• Graph-Path: The graph-path measure calculates the                        • Normalised Wiki Distance (NSD Wiki): We adapted the
   length of a shortest path between two synonym sets in the                  approach of (Cilibrasi & Vitanyi, 2007), but restricted the
   whole graph.                                                               corpus index to a social network by means of Wikpedia.
                                                                              That is, the normalised distance is derived on basis of the
                   distGraph (s1 , s2 ) = spGraph (s1 , s2 )      (9)         Apache Lucene index of Wikpedia articles only
   spGraph (s1 , s2 ): Length of a shortest path between s1 and s2
                                                                           Among the 2nd order approaches one model has obtained
   in the GermaNet graph
                                                                           particular attention, due to its success in a large variety of
   Using GermaNet as a lexical resource in computing seman-                tasks involving semantic processing: Latent Semantic Analy-
tic relatedness, lexical-semantic relations such as hyponymy               sis (LSA).
are considered only. However, it seems that for determining
SR humans do not distinguish between systematic and un-                    • Latent Semantic Analyis (LSA) (Deerwester, 1990):
systematic relations. Since GermaNet does not incorporate                     LSA is based on a term×context matrix A, displaying the
unsystematic relations, we expect all of the above measures                   occurrences of each word in each context. The decisive
to produce many false negatives; i.e. word pairs with low                     step in the LSA process is then a singular value decom-
relation values in GermaNet, but strongly related ranked by                   position (SVD) of the matrix, which enhances the contrast
humans.                                                                       between reliable and unreliable relations. To measure the
                                                                              distance of the word vectors, the cosine measure is most
Distributional Measures                                                       often used, since it normalizes for length.
Based on the assumption that words with similar dis-
tributional properties have similar meaning, distributional                • Semantic Vectors (Sem.Vec.) (Widdows & Ferraro,
approaches infer semantic relatedness considering co-                         2008): The open source Semantic-Vectors package creates
occurrences of words in text corpora. Distributional simi-                    a word space model from a term-document matrix using
larity can be determined in two major ways: One group of                      positional indexing. Word similarity is performed by pro-
measures establishes relatedness on direct co-occurrence in                   ducing a query vector and calculate its distance to the term
text (1st order relatedness). The other group aims to compare                 vectors (using the cosine).
the similarity of contexts in which two terms occur (2nd or-
                                                                              The important advantage of 2nd order approaches, is that
der relatedness). In 1st order approaches, the co-occurrence
                                                                           they are usually better able to capture paradigmatic relations
probability of two terms is set in relation to the probability of
                                                                           such as synonymy or hyponymy, since paradigmatically sim-
the singular terms. In recent times a number of co-occurrence
                                                                           ilar words tend to occur in similar contexts.
measures were proposed that use hit counts from large search
engines (Google/Yahoo). We have implemented four differ-                   Wikipedia-based Measures
ent SR measures using hit counts:
                                                                           In terms of Wikipedia based semantic interpretation some ap-
• Pointwise Mutual Information (PMI): The point wise                       proaches have been proposed which mainly focus either on
   mutual information (PMI) measure on hit counts for ex-                  the hyperlink structure (Milne, 2007), the vector space model
   ample can be defined as follows:                                        (VSM) or on category concepts for graph-related measures
                                                                           (Ponzetto & Strube, 2006; Zesch, Gurevych, & Mühlhäuser,
                                                 hc(wi , w j )
       relGPMI (wi , w j ) = log M + log                         (10)      2007). We have implemented three different algorithms using
                                              hc(wi ) × hc(w j )           Wikipedia as a resource in computing semantic relatedness:
   where hc(wi ), hc(wi , w j ) are the hit counts of a key word           • Explicit Semantic Analysis (ESA) (Gabrilovich &
   wi or a word pair wi , w j and M is the total number of pages              Markovitch, 2007): This method represents term similarity
   indexed by the search engine.                                              by an inverted term-document index in a high-dimensional
• Google Quotient (Cramer, 2008): It is defined as follows:                   space of concepts derived from Wikipedia. In this case,
                                                                              concepts are represented by Wikipedia articles. Each con-
                                          2 · hc(wi , w j )                   cept corresponds to an attribute vector of terms occurring
                  relGQ (wi , w j ) =                            (11)
                                        hc(wi ) + hc(w j )                    in the respective article (weighted by a TFIDF scheme
                                                                              (Salton & McGill, 1983)). Semantic relatedness of a pair
   Again, hc(wi ), hc(wi , w j ) are the hit counts of a key word             of terms is computed by comparing the concept vector
   wi or a word pair wi , w j .                                               A with B using the cosine metric. Since Gabrilovich &
                                                                              Markovitch reported their results for experiments on En-
• Normalised Google Distance (NSD) (Cilibrasi & Vitanyi,
                                                                              glish, we adopted their approach to the lemmatized Ger-
   2007):
                                                                              man Wikipedia data set. We also removed small and overly
                         max[log hc(wi ) log hc(w j )] − log hc(wi , w j )    specific concepts (articles having fewer than 100 words
   relNGD (wi , w j ) =
                             log M − min[log hc(wi ), log hc(w j )]           and fewer than 5 hyperlinks), leaving 126,475 articles for
                                                                 (12)         building the inverted index.
                                                                      3018

• Wikipedia Graph-Path: Given the entire Wikipedia hy-               Distributional measures The three web-based (1st order)
   perlink graph Gw = (V, E), where Wikipedia articles de-           measures obtained their hit counts via the Google API; all
   note a set of vertices V and hyperlinks between articles, and     counts were calculated beforehand and stored in a repos-
   categories denote a set of edges E ⊆ V 2 . The Wikipedia          itory. The LSA word space was calculated using the In-
   Graph-Path distance calculates the length of the shortest         fomap toolkit3 v. 0.8.6 on a newspaper corpus (Süddeutsche
   path (sp) between two articles in Gw .                            Zeitung) of 145 million words, which had been lemmatised
                                                                     by the lemmatizer presented in (Waltinger & Mehler, 2009).
                  distWGw (v1, v2) = spGw (v1, v2)          (13)     The co-occurrence matrix (window size: ±75 words) com-
                                                                     prised 80.000×3.000 terms and was reduced by SVD to 300
• Category Concept Analysis (CCA):                                   dimensions. For the vector comparisons the cosine measure
   Given a lemmatized Wikipedia dump an inverted concept-            was applied. Table 2 shows the results (correlation, coverage
   term matrix is constructed. Different to Gabrilovich &            and processing time) for all distributional measures tested.
   Markovitch (2007), concepts are defined as Wikipedia cat-
   egories, i.e. we assigned each article to its categories in       Wikipedia-based measures The calculation of the
   Wikipedia. For term weighting the TFIDF scheme was                Wikipedia measures is based upon the German version of
   used. Small concepts have been removed using a thresh-            Wikipedia (october 2007). The Semantic Vector package4 uti-
   old value for a minimum length of the term vector (more           lizes the Apache Lucene library. Explicit Semantic, Category
   than 400 lemmata). The relatedness computation was per-           Concept Analysis and Wikpedia Graph Path are implemented
   formed using the cosine metric, the dice coefficient and the      in C++ using Trolltech Qt. For both Category Concept
   jaccard similarity coefficient, utilizing a maximum length        Analysis and Explicit Semantic Analysis we had to reduce the
   of 20,224 as the category concepts vectors (A and B).             matrices on the lemma-dimension for computational reasons,
                                                                     i.e. when building the matrix we excluded those lemmata
                          Evaluation                                 whose corpus frequency did not exceed a threshold of 300.
Method                                                               Building the Normalized Search Distance measures, we have
In order to assess the above mentioned algorithms, we evalu-         directly connected to the special page search of Wikipedia
ated their performance on the basis of a human judgement ex-         (http://de.wikipedia.org/wiki/Spezial:Suche). Furthermore,
periment. For the German language there are – to our knowl-          we also calculated an LSA word space on Wikipedia;
edge – three human judgement data sets available: (1) a trans-       however, due to computational limitations we had to utilize a
lation of the Rubenstein list (Gurevych, 2005), (2) a semi-          subcorpus only, by taking the first 800 words of each article
automatically generated list compiled by Zesch & Gurevych            (148 mill. words in total). Table 3 lists the results for all
(2006) and (3) two lists assembled by Cramer and Fintham-            Wikipedia-based measures.
mer (2008) , comprising a total of 600 word pairs. Since the         Results
data sets of Cramer and Finthammer (2008) cover not only
a wide range of relation types (random connections, associ-          Comparing the correlation results shown in Tables 1, 2 and
ations, synonyms etc.), but also various degrees of relation         3 it can be observed that the net-based measures have the
strengths, we decided to use their lists to evaluate the algo-       lowest scores (r= 0.11 - 0.48); interestingly they score quite
rithms implemented.1                                                 similar within one test set, despite their rather different calcu-
   The first test set (A) contains 100 word pairs (nouns of di-      lation. For the distributional measures a clear difference can
verse semantic classes comprising abstract and concrete con-         be seen between the three web-based techniques (0.27 - 0.37)
cepts). The test set B contains 500 randomized word pairs            and the LSA results (scoring up to 0.64); this may either be
with not more than 20 % of collocations and associations.            due to the fact that LSA (being a 2nd order approach) is able
Set A was rated by 35 subjects and set B was rated by 75             to establish more paradigmatic relations such as synonymy
subjects. Volunteers rated the word-pairs on a 5-level scale         or hyponymy, or the hit counts, obtained from Google are not
and received no further instruction (apart from estimating the       sufficiently precise indicators of co-occurrence. Among the
semantic relatedness of the given terms).                            Wikipedia measures the WikiSearch Distance scores signifi-
                                                                     cantly better than the other measures (up to 0.69). A second
                                                                     observation of the results concerns the differences between
Net-based measures The net-based measures were calcu-                the correlations of the test sets A and B. Especially the net-
lated on GermaNet v. 5.0 using GermaNet Pathfinder v. 0.83.          based measures, but also most of the Wikipedia-based show
Table 1 lists the correlations (Pearson) for test sets A and B,      significantly worse correlations for set B. Recalling that set B
as well as the coverage and the average processing time per          contains a large part of random word pairs (80%), a probable
word pair2 .                                                         explanation is that such measures tend to overestimate relat-
    1 See (Cramer & Finthammer, 2008) for detailed information       edness, i.e. they cannot well discriminate between related
about the experiment and the data sets.
    2 The computation was performed on an AMD Athlon XP 2400+,           3 http://infomap-nlp.sourceforge.net/
2,0 GHz and 1GB of RAM.                                                  4 http://code.google.com/p/semanticvectors/
                                                                 3019

                                                     WordNet-based measures
                      Test     Leacock &      Wu &                 Jiang &                Hirst &      Tree     Graph
                       set     Chodorow       Palmer     Resnik Conrath           Lin     St-Onge       path     path
                    r Set A       0.48         0.36       0.44       0.46        0.48       0.47        0.41     0.42
                    r Set B       0.17         0.21       0.24       0.25        0.27       0.32        0.11     0.31
                   Coverage      86.9%        86.9%      86.9%      86.9%       86.9%      86.9%      86.9%     86.9%
                  t/pair (ms)     <10          <10        <10        <10         <10        1110       <10       3649
             Table 1: Correlations (Pearson), coverage and processing time per pair of the net-based measures tested
                                        Test        PMI       Google        NSD           LSA
                                         set      Google     Quotient      Google     (newspaper)
                                      r Set A       0.37        0.27         0.37         0.64
                                      r Set B       0.34        0.31         0.36         0.63
                                     Coverage      100%        100%         100%         87.0%
                                    t/pair (ms)     <10         <10          <10          <10
           Table 2: Correlations (Pearson), coverage and processing time per pair of the distributional measures tested
and unrelated word pairs. The differences between the ap-             ering all the results above it can be stated that the calculation
proaches tested clearly show how important the influence of           of semantic relatedness is far from being solved. Each of the
the resource is. One conclusion that may be drawn from our            resources that we used certainly captures an important part
results is that for determining SR a small, hand-crafted and          of lexical meaning; however, it seems that this is not yet suf-
structured resource such as a lexical-semantic net is clearly         ficient for describing the complex nature of SR between any
inferior to a large and semi-structured (Wikipedia) or even           two terms. Secondly, a factor that we disregarded in our study
completely unstructured resource (plain text). With respect to        is the influence of context. It is quite obvious that SR is not a
coverage, the web-based measures (including the WikiSearch            static and independent size. On the contrary, it is dynamically
Distance clearly outperform all other approaches. This is not         interrelated with the current lexical, syntactic and semantic
astonishing, given the fact that they operate on the largest          context, and a proper theory of (or algorithm computing) SR
vocabulary available. The off-line approaches on the other            will have to take it into account.
hand are not as sparse as one might have imagined, the lowest
scores are still over 75%, and the net-based as well as the LSA                    Conclusions and Future Work
approach achieve a coverage of approximately 87%. The pro-            We presented a study comparing sixteen different SR mea-
cessing time (per word pair) however differs quite strongly. It       sures on various lexical resources. The measures made use of
is also to be taken with a grain of salt, since it depends on         information from lexical-semantic nets, co-occurrence distri-
the implementation chosen. Most of the approaches show al-            bution and the structure as well as the content of a large social
most negligible processing times (<10 ms), however if com-            network (Wikipedia). We conducted an extensive evaluation
plex tree or graph traversals are involved (e.g. GermaNet or          on the basis of a human judgement experiment. Morever, the
Wiki graph path), the processing times can reach up to sev-           implemented algorithms and employed resources were ana-
eral seconds. Comparing all these different measures and re-          lyzed with respect to practical issues such as run time and
sources, we observed that the distributional measures, espe-          resource coverage. In terms of coverage and correlation we
cially those based on a 2nd order approach (such as LSA),             could observe that distributional measures perform best, how-
perform significantly better than the net-based measures and          ever, results show that even the best performing algorithms
those using explicit categorial information (ESA, CCA). We            leave a lot of room for improvement. For the future, we want
therefore conclude that the use of explicit structural informa-       to propose the definition of a shared task which might bring
tion, in the form of semantic links, categories or of hyperlink       us considerably closer to results of high performance but also
graphs, establishes semantic relatedness not as well as distri-       to better understand the complex characteristics of SR.
butional information. Secondly we could clearly see that the
choice of the resource plays an important role. Interestingly,                             Acknowledgement
those measures using the web as a corpus were inferior to             Financial support of the German Research Foundation (DFG)
those operating on smaller but better controlled training cor-        through the Excellence Cluster 277 Cognitive Interaction
pora (cf. the important difference between the web-based and          Technology, the Research Group 437 Text Technological In-
the wikipedia-based NSD). With respect to corpus choice we            formation Modeling and the LIS-Project P2P-Agents for The-
can conclude that quality is more important than quantity, an         matic Structuring and Search Optimisation in Digital Li-
observation which is in line with Kilgarriff (2007). Consid-          braries at Bielefeld University is gratefully acknowledged.
                                                                 3020

                                Test      NSD             Sem. Vec.               Wiki Graph       LSA
                                 set     (Wiki)    CCA      (Wiki)        ESA         Path       (Wiki)
                              r Set A     0.69     0.57      0.51         0.52        0.49         0.65
                              r Set B     0.61     0.36      0.28         0.44        0.37         0.57
                             Coverage    100%     79.8%     99.1%       75.9%        92.0%        83.8%
                            t/pair (ms)   850      <10       1299         240         2301         <10
         Table 3: Correlations (Pearson), coverage and processing time per pair of the Wikipedia-based measures tested
                          References                                 context and wordnet similarity for word sense identifica-
                                                                     tion. In C. Fellbaum (Ed.), Wordnet: An Electronic Lexical
Anderson, J. R. (1983). A spreading activation theory of
                                                                     Database (pp. 265–284). The MIT Press.
   memory. Journal of Verbal Leaning and Verbal Behaviour,
                                                                   Lemnitzer, L., & Kunze, C. (2002). Germanet - representa-
   22, 261–295.
                                                                     tion, visualization, application. In Proceedings of the 4th
Boyd-Graber, J., Fellbaum, C., Osherson, D., & Schapire, R.
                                                                     language resources and evaluation conference (pp. 1485–
   (2006). Adding dense, weighted, connections to wordnet.
                                                                     1491).
   In Proceedings of the 3rd global wordnet meeting (pp. 29–
                                                                   Lin, D. (1998). An information-theoretic definition of simi-
   35).
                                                                     larity. In Proceedings of the 15th international conference
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-
                                                                     on machine learning (pp. 296–304).
   based measures of semantic relatedness. Computational
                                                                   Miller, G. A., & Charles, W. G. (1991). Contextual correlates
   Linguistics, 32 (1), 13-47.
                                                                     of semantic similiarity. Language and Cognitive Processes,
Cilibrasi, R., & Vitanyi, P. M. B. (2007). The google similar-
                                                                     6(1), 1–28.
   ity distance. IEEE Transactions on Knowledge and Data
                                                                   Milne, D. (2007). Computing semantic relatedness using
   Engineering, 19(3), 370–383.
                                                                     wikipedia link structure. In Proc. of nzcsrsc07.
Collins, A., & Loftus, E. (1975). A spreading activation           Ponzetto, S., & Strube, M. (2006, July). Wikirelate! comput-
   theory of semantic processing. Psychological Review, 82,          ing semantic relatedness using wikipedia.
   407-428.                                                        Resnik, P. (1995). Using information content to evaluate
Cramer, I. (2008). How well do semantic relatedness mea-             semantic similarity in a taxonomy. In Proceedings of the
   sures perform? a meta-study. In Proceedings of the sympo-         ijcai 1995 (pp. 448–453).
   sium on semantics in systems for text processing.               Rubenstein, H., & Goodenough, J. B. (1965). Contextual cor-
Cramer, I., & Finthammer, M. (2008). An evaluation pro-              relates of synonymy. Communications of the ACM, 8(10),
   cedure for word net based lexical chaining: Methods and           627–633.
   issues. In Proceedings of the 4th global wordnet meeting        Salton, G., & McGill, M. J. (1983). Introduction to modern
   (pp. 120–147).                                                    information retrieval. New York: McGraw Hill.
Deerwester, S. (1990). Indexing by latent semantic analysis.       Waltinger, U., & Mehler, A. (2009). Web as preprocessed
   J. Ameri. Soci. Inf. Sci, 41(6), 391407.                          corpus: Building large annotated corpora from heteroge-
Fellbaum, C. (Ed.). (1998). Wordnet. an electronic lexical           neous web document data. In preparation.
   database. The MIT Press.                                        Widdows, D., & Ferraro, K. (2008, May). Semantic vec-
Gabrilovich, E., & Markovitch, S. (2007). Computing                  tors: a scalable open source package and online technol-
   Semantic Relatedness using Wikipedia-based Explicit Se-           ogy management application. In E. L. R. A. (ELRA) (Ed.),
   mantic Analysis. Proceedings of the 20th International            Proceedings of the sixth international language resources
   Joint Conference on Artificial Intelligence, 6–12.                and evaluation (lrec’08). Marrakech, Morocco.
Gurevych, I. (2005). Using the structure of a conceptual net-      Wu, Z., & Palmer, M. (1994). Verb semantics and lexical
   work in computing semantic relatedness. In Proceedings of         selection. In Proceedings of the 32nd annual meeting of the
   the ijcnlp 2005 (pp. 767–778).                                    association for computational linguistics (pp. 133–138).
Hirst, G., & St-Onge, D. (1998). Lexical chains as rep-            Zesch, T., & Gurevych, I. (2006). Automatically creating
   resentation of context for the detection and correction           datasets for measures of semantic relatedness. In Proceed-
   malapropisms. In C. Fellbaum (Ed.), Wordnet: An Elec-             ings of the workshop on linguistic distances at coling/acl
   tronic Lexical Database (pp. 305–332). The MIT Press.             2006 (pp. 16–24).
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity         Zesch, T., Gurevych, I., & Mühlhäuser, M. (2007). Compar-
   based on corpus statistics and lexical taxonomy. In Pro-          ing wikipedia and german wordnet by evaluating semantic
   ceedings of rocling x (pp. 19–33).                                relatedness on multiple datasets. In In proc. of naacl-hlt.
Kilgarriff, A. (2007). Googleology is bad science. Computa-
   tional Linguistics, 33(1), 147–151.
Leacock, C., & Chodorow, M. (1998). Combining local
                                                               3021

