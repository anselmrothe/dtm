UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Joint acquisition of word order and word reference
Permalink
https://escholarship.org/uc/item/0gs0p3c7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Maurits, Luke
Navarro, Daniel
Prefors, Amy
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                            Joint acquisition of word order and word reference
                                           Luke Maurits (luke.maurits@adelaide.edu.au)
                                           Amy F. Perfors (amy.perfors@adelaide.edu.au)
                                       Daniel J. Navarro (daniel.navarro@adelaide.edu.au)
                                              School of Psychology, University of Adelaide
                                                       Adelaide SA 5005, Australia
                               Abstract                                 words have been shown to make use of word order knowledge
                                                                        in a comprehension task (Hirsh-Pasek & Golinkoff, 1996).
   Inferring the mappings between words and their referents is          This suggests that word order knowledge is acquired very
   a difficult problem that all language learners face. Similarly,      early, but it is not clear how it is done. Although learn-
   learning which word orders are permitted in one’s language is
   one of the first grammatical learning tasks these same learn-        ing word order is a more limited problem than learning the
   ers must solve. We present a modeling framework which ad-            referents of words, since there are fewer possible solutions
   dresses simple versions of both of these problems by using the       to the problem, it is still a puzzle how it can be done so
   joint information in each to bootstrap the other. We discover        quickly. Some have suggested that prosodic bootstrapping
   that these two distinct learning tasks may be easier to solve        may explain a related problem, the acquisition of head direc-
   jointly because of the way in which the inferences in one prob-
   lem constrain the inferences in the other.                           tion (Christophe, Nespor, Guasti, & Ooyen, 2003). Although
                                                                        this requires the assumption of innate knowledge of the map-
   Keywords: word learning; word order; Bayesian models; mu-
   tual constraint; reference; linguistics                              ping principles between prosodic cues and head direction, and
                                                                        does not address the acquisition of word order itself, prosodic
                                                                        bootstrapping may play an important role.
                           Introduction
The language-learning child is faced with two simultaneous                 In this paper we propose that both of these acquisition
acquisition problems: acquiring the (semantic) rules that map           problems can be made more tractable by addressing them
the words she hears onto the objects and actions she per-               jointly. On the one hand, if the learner believes that word or-
ceives, and acquiring the (syntactic) rules that govern how             derings tend to be consistent, constraints are imposed on the
those words should be combined to make grammatical sen-                 manner in which words may be mapped onto entities in the
tences. Both are difficult learning problems in their own right,        world. On the other hand, even knowing a few word mean-
and have been the topic of considerable research.                       ings is enough to provide a great deal of evidence about word
   Determining the meaning of words on the basis of real-life           order. These intuitions suggest that viewing the problem as
observational evidence is quite difficult (Gillette, Gleitman,          a joint acquisition problem can make both individual prob-
Gleitman, & Lederer, 1991), in part because of the inher-               lems easier. While in one sense the proposition may appear
ent ambiguity of words, in part because the number of po-               counterintuitive – after all, there is in some sense ‘more’ to
tential meanings is logically underconstrained (Quine, 1960).           learn in the joint problem – to the extent that each problem
While it may be that the identification of a word’s referent is         mutually constrains the other, the acquisition problem should
made easier by pre-existing biases (Markman, 1990), recent              be made less difficult, rather than more. This basic idea is
research has also suggested several methods by which chil-              not a new one: for instance, earlier work noted its poten-
dren could explicitly learn which objects or actions a particu-         tial (Siskind, 1990, 1991). However, performing inferences
lar word refers to. For instance, social cues such as pointing          about both syntactic and semantic information was beyond
or gaze (Frank, Goodman, & Tenenbaum, 2007) can assist the              the computational capabilities of the time, and in practice,
learner, as can a sensitivity to the statistics of cross-situational    that work simply demonstrated that hardwired syntactic in-
word learning (Frank et al., 2007; Yu & Smith, 2008) and the            formation could make the learning of semantics easier. Our
ability to form theories about the abstract rules that govern           research goes beyond this work in two ways: first, because
the mapping of words onto categories (e.g., Kemp, Perfors,              we demonstrate that truly joint inference, in which both as-
& Tenenbaum, 2007). Experiments and computational mod-                  pects of the problem mutually bootstrap each other, can make
eling suggest that the difficulties and ambiguities inherent in         the learning problem easier; and second, because the syntac-
cross-situational word learning can be at least partially alle-         tic information is simpler and sparser (word order rather than
viated by these techniques.                                             X-bar theory or richer grammatical knowledge). Our study
   Acquiring the rules of syntax is also a famously difficult           presents two models that seek to establish word-referent map-
problem. Even if we restrict ourselves to more tractable sub-           pings on the basis of cross-situational learning statistics: one
problems – for instance, the acquisition of word order – the            model also seeks to acquire word order, and uses this to as-
empirical data present some difficult issues. Children make             sist word-referent mapping learning, and one does not. We
few mistakes in word order when they start combining words              demonstrate that solving the joint acquisition problem results
(e.g., Brown, 1973), and even children who do not combine               in more rapid learning of word reference.
                                                                    1728

Figure 1: A simple example world, consisting of 7 objects corre-
sponding to common animals and 6 relations. The leftmost portion
of the figure shows representations of the relations. The top left re-
lation, which corresponds to the concept of EATS, is enlarged in the       Figure 2: Naming distributions for two objects in our toy world,
rightmost portion of the figure for clarity. The object labels in this     demonstrating synonymy (both objects have more than one word
portion are for the reader’s convenience: they are not inherent prop-      with non-zero naming probability) and polysemy (the words “feline”
erties of the objects and are not visible to our model.                    and “mammal” have non-zero naming probability for both objects).
            A Simple Language & World Setup                                “peanut” or “indigo”. To make this scheme more explicit,
                                                                           Figure 2 shows two examples of a naming distribution; it il-
Our models consider a learner who exists in a physical world
                                                                           lustrates how this scheme permits both synonymy and poly-
of objects and inter-object relations. The learner is attempt-
                                                                           semy, thus reproducing some of the factors which complicate
ing to acquire a language (consisting of word order knowl-
                                                                           word learning in the real world.
edge and a lexicon of word-world mappings) through expo-
                                                                              In addition to the probabilistic lexicon, our simplified lan-
sure to concurrent observations of the world and linguistic in-
                                                                           guage model includes the concept of word order. We con-
put. Though heavily simplified, it is intended as a first-order
                                                                           sider a set of six word orders corresponding to the six possi-
approximation to the acquisition problem facing children.
                                                                           ble ways of ordering subjects, verbs and objects.1 The word
The World                                                                  order in our language is specified by a probability distribution
                                                                           Θ over the set of these six possible word orders. As an illus-
Formally, our world is specified in terms of a set of m objects,           tration, we might think of the English language as assigning
O = {o1 , . . . , om }, and a set of n relations that exist between        80% probability to the SVO ordering, 20% to the OVS order,
those objects, R = {r1 , . . . , rn }. Each relation is a function         and 0% to all other word orders. This distribution encodes a
defined for pairs of objects (i.e., ri ⊆ O × O ); if the relation          strong preference for active voice, allows the occasional use
holds for two objects r(o1 , o2 ) is true. Not all true things             of passive voice, and indicates that the other four word order-
are equally likely to be observed: if BITES(o1 , o2 ) indicates            ings are ungrammatical.2
that object 1 is able to bite object 2, then the specific obser-
vation BITES(dog, man) will be made much more frequently                   The Nature of the Input
than BITES(man, dog). We formalize this notion by equipping
the world with a probability distribution Φ(·) over observed               In our simulations we generate a collection of observations
relationships. The learner’s physical observations are gener-              from the world and corresponding data from the language,
ated from Φ(·), and consist only of true statements about the              and the learner’s task is to use this input to infer the cor-
world, but some things are seen much more often than others.               rect underlying naming distribution λx for each object and
Figure 1 shows a diagrammatic representation of a simple ex-               relation – and perhaps, jointly, to infer the correct word or-
ample world involving 7 objects and 6 relations. The struc-                der Θ for the language. Formally, the input available to the
ture of one of the relations, corresponding to the concept of              learner, D , consists of observations of relations and objects,
EATS (·,·) is magnified.                                                   z = r(o1 , o2 ), which are drawn from Φ(·), each of which is
                                                                           paired with a three-word linguistic utterance, w = w1 w2 w3 ,
The Language                                                               which is generated by randomly selecting a word for each
The language component of the modeling involves a prob-                    of r, o1 and o2 from the appropriate naming distributions and
abilistic lexicon with a vocabulary of v words, V =                        combining them to form w using a word order θ drawn from
{w1 , . . . , wv }. For every object or relation x in the world,           the language’s word order distribution Θ. For instance, if
there is a naming distribution λx over the vocabulary (i.e.,               the selected word order is θ = SVO then w1 ∼ λo1 , w2 ∼ λr
λx : V → [0, 1]). We denote the set of all naming distribu-                and w3 ∼ λo2 . Each data point in D corresponds to a cou-
tions by Λ. A naming distribution is essentially the map be-               pled observation-sentence pairing generated in this way, i.e.
tween items in the world and the words for those items; it                 D = {d1 = (z1 , w1 ), d2 = (z2 , w2 ), . . .}. Each di implicitly has
assigns higher probability to those words more likely to be                a word order variable θi associated with it, which is not ob-
used as names for the relevant object. For instance, if the ob-                1 This includes SVO, SOV, VSO, VOS, OSV, and OVS.
ject x corresponds to the entity cat, the distribution λx should               2 Note  that we do not, in fact, encode a preference for any partic-
assign the most probability to the word “cat”, a substantial               ular word order – whether found in English or not – into the model.
amount to words such as “kitty” or “pet”, a small but non-zero             We merely allow the model to postulate that some orders will turn
amount to “feline” and no probability to the words “monkey”,               out to be more common than others in the target language.
                                                                       1729

Table 1: Example input data D . Each row represents a single datum,
coupling a relational observation z with a linguistic one w.            the objects o1 or o2 , and the same is true of w2 and w3 . This
                                                                        forces the model to rely only upon the concurrence of rela-
     Relational observation (z)      Linguistic utterance (w)           tions or objects and words in attempting to estimate the set of
          EAT (cat, mouse)                “cat eat rodent”              naming distributions Λ.
       CHASE (lion, antelope)            “lion chase prey”                 The estimates of the naming distributions, which we de-
          EAT (cow, grass)             “cow consume grass”              note by Λ̂, are calculated via Bayesian inference over the
        EAT (antelope, grass)           “antelope eat grass”            space of possible naming distributions; a symmetric Dirich-
                                                                        let distribution with parameter α serves as our prior for each
                                                                        of the λ̂x . We perform the inference numerically using Gibbs
                                                                        sampling, a common and convenient form of Markov Chain
servable by the learner.                                                Monte Carlo3 (Gilks, Richardson, & Spiegelhalter, 1996).
   Note that our linguistic input differs from ‘real’ input in          This involves iteratively assigning a word order variable θi
that we give no regard to functional words such as “a”, “the”           to each data point di in D . Each of these assignments is made
or “this”. Filtering complete sentences in this way seems rea-          randomly using a probability distribution conditioned on all
sonable given that young infants are capable of making the              the other assignments: this full conditional distribution and
distinction between function and content words on the basis             other technical details are available in the appendix. For now
of frequency and prosody (e.g., Jusczyk, 1997). We are also             it will suffice to say that the probability of assigning a partic-
assuming that a language learner is able to unambiguously as-           ular word order θ to a given data point is proportional only
sociate each linguistic utterance with a relational observation,        to its consistency with other assignments; in other words, the
which may rely on the use of cues like gaze. (We discuss this           model prefers words to have few meanings and meanings to
oversimplification later in the paper).                                 be associated with few words. Note that although the model
   A brief example of the data is given in Table 1. Note that           does learn word order assignments θi , it does not learn any
the learner does not have direct knowledge of the relational            general rules about word order that hold across utterances.
structure shown in Figure 1, the correct naming distributions           The θi values that it learns correspond only to the mapping
in Figure 2, or knowledge of which elements in the observa-             from the particular words in the utterance wi to the entities in
tion map onto which words in the linguistic utterance: every-           the observation zi .
thing must be inferred from the data in D .                                The word-order learning model MWO is identical to the
                                                                        baseline model except that is assumes that word order tends
                         Methodology                                    to be consistent across all utterances. The learner thus aims
Models                                                                  to estimate some explicit, non-uniform word order distribu-
The main motivation for our research was to explore to what             tion Θ̂. Once again, we model this using Bayesian inference,
extent two difficult acquisition problems – establishing word           assuming that the learner places a symmetric Dirichlet prior
reference, and learning word order – could each be made                 distribution with parameter β over the possible word order
easier by attempting to solve them jointly. To that end, we             distributions. In this model, the probability of assigning a
compare two word learning models that differ in their ability           particular word order θ to a given data point is dependent on
to acquire word order information. Both models seek to in-              the consistency of its word-world assignments (as in MB ), as
fer the correct naming distributions Λ and are presented with           well as the consistency of word orderings across data points.
data D . Each individual datapoint d consists of coupled ob-            Technical details for both models, including the full condi-
servations and three-word utterances (z, w). The difference             tional distribution, are available in the appendix.
between the models is that the baseline model, which we call            Data sets
MB , assumes that there is no consistent word order in the lan-
guage; the word-order learning model, which we call MWO ,               Simulated data sets are created based on the generative pro-
assumes that the language has a consistent distribution over            cess detailed earlier. To explore how performance changes as
word orders, Θ, and seeks to learn that as well as the naming           a function of the quantity of data, we create a series of data
distribution. While it may seem cognitively implausible that            sets D with varying numbers of observation-sentence pairs.
real language learners maintain some mental representation              Data sets with more data points are generated by adding addi-
of a complete probability distribution over possible labels for         tional points to the smaller data sets. All results are averaged
each object or concept they encounter, as both our models               over 10 different data sets at each size; each set was generated
do, this idea has received some empirical credibility from re-          using different random values of Φ and Λ.
cent experimental work (Vouloumanos, 2008); additionally, it
may not be necessary to have a precisely accurate probability           Results
distribution in order to receive substantial benefit from joint         The task of our learner was to make reasonable inferences
learning (although that is a topic for further research).               about the likely referents of each of the words in the language,
   To elaborate on the difference between models, the base-             as well as, in the case of MWO , to determine the probable word
line model MB implicitly assumes that the distribution Θ over           order in the language. Figure 3 depicts the rate of acquisition
word orderings is perfectly uniform. That is, given the cou-                3 We do not suggest that child language learners literally imple-
pled z = r(o1 , o2 ) and w = w1 w2 w3 , it does assume that each
                                                                        ment Gibbs sampling or Bayesian inference. We use these tools as
word refers to precisely one of the three relations or objects,         models of “ideal learning” in order to explore whether mutual con-
but does not try to learn any consistent mappings – a priori,           straint in this task is sufficient to make the joint learning problem
w1 is just as likely to refer to the relation r as it is to one of      substantially easier, and what could be learned in principle.
                                                                    1730

 Figure 3: Inferred word order probabilities by model MW for various          Figure 4: Accuracy of models MB and MWO , approximated by pro-
sized data sets. The world has 20 objects, 10 relations and 50 words.         portion of inferred naming distributions with correct means, based
                                                                              on data set size. The world has 20 objects, 10 relations and 50 words.
                                                                              MWO is shown with gray diamonds and MB with black circles.
of word order by MWO as the quantity of data increases. The
 correct word order distribution Θ for this data assigned prob-
 ability 0.8 to the word order SVO and 0.2 to OVS, with all                      and “fiberglass” 40% of the time would count as correct,
 other orderings receiving zero probability.4 It is evident that                 and one that predicted the reverse would not.
 only a small amount of data is necessary before the model ac-
 curately infers the correct word order – Figure 3 shows that                 These two measures were chosen to address the conflicting
 the inferred probabilities are essentially perfect with a data               criteria of intuitive interpretability (which is satisfied by cal-
 set size of 30 or above, and are approximately correct with as               culating the proportion of learned distributions with correct
 small a data set size as 15. In a sense this is not surprising,              modes) and accuracy (which is better satisfied by calculating
 given that there are only six possibilities to choose from, but              KL divergence). Since we found no qualitative difference be-
 it is noteworthy in light of children’s early acquisition of word            tween the results depending on which measure was used, we
 order. We note that for the simulations which produced this                  present all results here in the second, more intuitive format.
 data, we used a Dirichlet distribution parameter of β = 1 for                   Figure 4 shows the accuracy of MB and MWO , as measured
 the prior estimate of Θ. Such a value provides no bias in the                by the proportion of naming distributions with correct modes,
direction of sparsity or non-sparsity. The fact that word order               for data set sizes ranging from 1 to 50. These datasets were
can be acquired quickly from so few ‘coupled’ data despite                    generated using a simple world consisting of 20 objects, 10
the lack of bias may suggest no need to hypothesize that chil-                relations and 50 words. For both models, accuracy increases
dren are born with strong innate constraints on word ordering                 as the quantity of data increases, and accuracy is overall quite
to explain their rapid acquisition.5                                          high: after observing only 20 utterance-observation pairs, the
     How well does the model acquire the correct word-world                   word-order learning model MWO has found the correct refer-
mappings? We assess this by calculating the accuracy of the                   ent for over 50% (i.e., over 15 of the 30) of the relations and
inferred naming distributions for each object in the world.                   objects. Even the baseline model MB has acquired around
Because the learner induces entire naming distributions λx for                40%, which provides further evidence for the observation,
each object x, rather than mappings to a single lexical item,                 suggested by other researchers, that learning of reference can
 calculating this is not completely straightforward. We mea-                  be greatly facilitated by the use of cross-situational statistical
 sured accuracy in two ways:                                                  information (Frank et al., 2007; Yu & Smith, 2008).
                                                                                 More interestingly, we also observe that MWO outperforms
1. By calculating the average Kullback-Leibler divergence6                    the baseline MB ; this is shown more clearly in Figure 5, which
     between actual naming distributions and their correspond-                shows the difference in accuracy between the two models. It
     ing inferred naming distributions.                                       is clear that jointly learning word order offers a significant
                                                                              advantage, especially when the amount of data is small. This
2. By calculating the proportion of learned naming distribu-                  advantage decreases as the data set increases in size, which
     tions that have the correct modal mapping: a distribution                is to be expected: in the limit, the high quantity of correlated
     that predicted that CAT mapped onto “cat” 60% of the time                cross-situational information should suffice to overcome any
      4 Each of our simulations were performed with two correct word          ambiguities in reference. Importantly, smaller data sets are
 order distributions, one which placed all probability on a single word       of special interest to us, since they more closely approximate
 order and one which split the probability between two orderings with         the inference problem facing the child, who receives quite
 probabilities 0.8 and 0.2. No qualitative differences in our results         sparse data relative to the amount to be learned in the world,
 were observed. All figures presented in this paper correspond to             and shows rapid learning in that situation. Our result suggests
 data generated with the bimodal distribution.
      5 We also tested the β = 0.01 case, which encodes a strong bias         that children may be able to use inferences about word order –
 toward sparsity. This made little qualitative difference to the results.     which are supported quite early – to bootstrap their inferences
      6 The KL divergence between two distributions P and Q defined           about word reference.
 on the set X is given by DKL (P||Q) = ∑x∈X P(x) ln(P(x)/Q(x)).                  To what extent are these results due to the fact that our toy
                                                                          1731

Figure 5: Accuracy benefit to joint learning in a small world. Com-     Figure 6: Accuracy benefit to joint learning in a large world. Com-
parison of the baseline model (MB ) with the word-order learning        parison of (MB ) with (MWO ) in terms of accuracy in a world with
model (MWO ) in terms of accuracy of acquiring the correct word-        80 objects, 40 relations, and 200 words. Once again the joint model
world mappings in a small world with 20 objects, 10 relations, and      MWO clearly outperforms the baseline MB . In the larger world the
50 words. The y axis shows the increase in accuracy that comes from     duration of the effect appears to be greater.
jointly learning word order as well as reference alone. Model MWO
clearly outperforms MB , particularly when there is little data.
                                                                        learning of two complicated tasks can make both tasks eas-
                                                                        ier. We suggest that many types of inference – which classic
world is relatively small, with few objects, words, and rela-           learnability analysis would suggest are too difficult for chil-
tions? While constructing a world of the same complexity                dren to acquire as rapidly as they do – may be significantly
that the child faces is beyond our purview, we address the              easier when conceptualized as a joint problem in language
issue of scalability by presenting the same models with data            and higher-order cognition. Moreover, by constructing mod-
from a substantially larger world (80 objects, 40 relations, and        els that explicitly handle the joint inference problem as well
200 words). Figure 6 depicts the same accuracy advantage of             as models for each of the individual ones, we can begin to
MWO over MB as for smaller amounts of data, but that advan-             quantify both the qualitative and quantitative features of the
tage is retained for longer. This is sensible because in a larger       speedup effect.
world, significantly more data is required before the infor-               More broadly, this modeling framework can be expanded
mation conveyed by cross-situational correlation information            in interesting ways to explore problems of more complexity
alone is sufficiently saturated to negate the advantage of also         and, thus, greater applicability to the situations faced by child
being able to use word order. This suggests that the extreme            learners. The model currently assumes that all data consists of
simplicity of our small world compared to the real world has            joint utterances and observations of the world – yet often chil-
not exaggerated the strength of the advantage of joint learn-           dren are in situations where they observe objects and events
ing; in fact, it may have underestimated it. In a world as large        happening but receive no linguistic input, or where they hear
and complicated as the real world, being able to rely on in-            sentences that have no apparent connection to the events in
ferences about word order to figure out the meaning of the              the world. What happens if the model is presented with data
words in the sentence may be of significant benefit.                    sets consisting of all three kinds of data? Preliminary indica-
                                                                        tions suggest that the advantage of joint learning still exists –
                          Discussion                                    indeed, the learner is still able to leverage some information
This work demonstrates that two distinct language acquisi-              out of the singleton data: for instance, observations of events
tion problems – learning word reference and inducing word               without language still provide evidence about what kinds of
order – can be made easier by addressing them jointly. While            events are more or less likely. Future work will explore this
in some sense this is counter-intuitive, since in the joint prob-       issue in more detail.
lem there is ‘more’ to be learned, we suggest that the joint               Another shortcoming of the current modeling framework is
problem is in fact easier because each problem constrains               that it makes certain implicit assumptions about the nature of
the other. Knowing that verbs tend to be first can enable a             the knowledge the learner starts with. Our word-order learn-
learner to map the word “glim” in the sentence “glim torg               ing component assumes that the learner already has concepts
nim” onto the action in the world; conversely, knowing that             for subjects, objects and verbs, and that languages may dif-
“glim” refers to a kind of biting action can enable a learner           fer in how those are ordered. While there is some evidence
to infer, upon hearing the same sentence, that words denoting           that notions of agency and objecthood form a core part of
actions may come first. This is sensible, but has not until now         cognition from infancy (e.g., Spelke & Kinzler, 2007), an in-
been supported by quantitative analysis.                                teresting extension to this analysis would be to present in-
   Our world and the learning situation are in many ways                put consisting of items with features, and explore whether the
vastly oversimplified versions of the task facing the child             model could induce the notions of subject, object and verb,
learning language. Our goal here is not to argue that children          based on a presumption that word order is consistent and that
approach the situation in precisely the way our models do, but          words map onto things in the world. This framework is also
rather to lend some empirical support to the notion that joint          easily extendible to address the acquisition of more compli-
                                                                    1732

cated syntactic knowledge: for instance, the realization that       Spelke, E., & Kinzler, K. (2007). Core knowledge. Develop-
in some languages it is permissible to optionally drop sub-            mental Science, 10(1), 89–96.
ject pronouns. In other languages, word order plays a much          Vouloumanos, A. (2008). Fine-grained sensitivity to statis-
less important role than it does in English: this information          tical information in adult word learning. Cognition, 107,
is conveyed by other means, such as morphological inflec-              729–749.
tion. This, too, could be added to our model, in addition to        Yu, C., & Smith, L. (2008). Infants rapidly learn word-
a word-order learning component. One would expect that an
effective learner would learn to make use of whichever kind            referent mappings via cross-situational statistics. Cogni-
of information was most informative, although further work             tion, 106, 1558-1568.
is necessary to explore whether this expectation is correct,
and how much different types of information help with the                                              Appendix
overall learning problem.                                           For all Gibbs samplers used in our models, we employ an initial
   In general, the analysis here provides a framework for           ‘burn in’ period of 1000 iterations and then generate our estimate
                                                                    histograms using 500 samples, with an inter-sample lag of 100 iter-
investigating how the joint acquisition of distinct pieces of       ations.
knowledge can make the acquisition of each individual piece
easier. Our results suggest that classic learnability problems,     Model MB
which often presume that information is acquired in isolation,      The full conditional distribution for the word order θi (assigned to
may not always apply to the situation facing the child learner.     the ith component of D , di ), is given below, where we denote the
                                                                    relational component of di by zi = r(s, o), the linguistic component
                                                                    by wi = w1 w2 w3 , and by θ −i the set of all other word order assign-
                    Acknowledgements                                ments:
DJN was supported by an Australian Research Fellowship
(ARC grant DP-0773794). We thank three anonymous re-                      P(θi | θ −i , D ) ∝ P(θi | θ −i )P(wi |zi , θi )
viewers for their comments.                                                                 ∝ λˆr (RELθ (wi ))λˆr (SUBθ (wi ))λ̂(OBJθ (wi ))
                                                                                                          i                i             i
                         References                                 Note that the term P(θi | θ−i ) has been absorbed by the proportion-
                                                                    ality, by virtue of the assumption that it is a constant (i.e., 1/6).
Brown, R. (1973). A first language. Cambridge, MA: Havard           The functions RELθ , SUBθ and OBJθ are defined for each possible
   University Press.                                                value of θ so that, given the input w = w1 w2 w3 , they return the word
                                                                    which corresponds to the relation, subject and object, respectively,
Christophe, A., Nespor, M., Guasti, M., & Ooyen, B. (2003).         given the particular word order θ. For instance, if θ = SVO, then
   Prosodic structure and syntactic acquisition: The case of        RELθ (w) = w2 , SUBθ (w) = w1 and OBJθ (w) = w3 .
   the head-direction parameter. Developmental Science, 6,              Here λˆx , for x = s, r, o are our inferred approximations to the rele-
   211–220.                                                         vant naming distributions. At any iteration, these approximations are
                                                                    given by the following expression, which is arrived at by applying
Frank, M., Goodman, N., & Tenenbaum, J. (2007). A                   Bayes’ law and the use of the same symmetric Dirichlet distribution
   Bayesian framework for cross-situational word learning.          prior, with parameter α, for all naming distributions:
   Advances in Neural Information Processing Systems, 20.
                                                                                                            nxw (x, w) + α
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996).                                           λ̂x (w) =
                                                                                                              nx (x) + vα
   Markov chain Monte Carlo in practice. Chapman & Hall.
Gillette, J., Gleitman, H., Gleitman, L., & Lederer, A. (1991).     Here nxw (x, w) counts the number of data points in which the re-
   Human simulations of vocabulary learning. Cognition, 73,         lational component contains the relation or object x, the linguistic
   153–176.                                                         component contains the word w, and the word order assigned to the
                                                                    utterance is such that w is understood to be a name for x. The term
Hirsh-Pasek, K., & Golinkoff, R. (1996). The origins of             nx (x) counts the number of observations z which involve x. We have
   grammar: Evidence from early language comprehension.             used α = 0.01 in our simulations, which represents a strong prior
   Cambridge, MA: MIT Press.                                        bias toward sparsity of the naming distributions.
Jusczyk, P. (1997). The discovery of spoken language. Cam-          Model MW O
   bridge, MA: MIT Press.                                           Reusing our notation from model MB , the full conditional distribu-
Kemp, C., Perfors, A., & Tenenbaum, J. B. (2007). Learning          tion for word order assignments in MWO is:
   overhypotheses with hierarchical Bayesian models. Devel-
                                                                                        P(θi | θ −i , D ) ∝ P(θ | θ −i )P(wi | zi , θi )
   opmental Science, 10(3), 307–321.
Markman, E. (1990). Constraints children place on word                                                    ∝ Θ̂(θi )P(wi | zi , θi )
   meanings. Cognitive Science, 14, 57–77.                          Here the rightmost term, representing the likelihood of wi being gen-
Quine, W. (1960). Word and object. Cambridge, MA: MIT               erated as a description of zi given the word order θi , is exactly as be-
   Press.                                                           fore. The leftmost term Θ̂ is our inferred approximation to the word
Siskind, J. (1990). Acquiring core meanings of words, repre-        order distribution, which at any iteration is given by:
   sented as Jackendoff-style conceptual structures, from cor-                                                nθ (θ) + β
   related streams of linguistic and non-linguistic input. ACL.                                      Θ̂(θ) =
                                                                                                              |D | + 6β
Siskind, J. (1991). Dispelling myths about language boot-
   strapping. AAAI Workshop on Machine Learning of Natu-            Here nθ (θ) counts the number of linguistic components of D which
   ral Language and Ontology, 157–64.                               have been assigned the word order θ.
                                                                1733

