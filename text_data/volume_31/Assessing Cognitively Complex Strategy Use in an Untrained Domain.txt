UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Assessing Cognitively Complex Strategy Use in an Untrained Domain
Permalink
https://escholarship.org/uc/item/12w9n2w1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Guess, Rebekah H.
Jackson, G. Tanner
McNamara, Danielle S.
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

              Assessing Cognitively Complex Strategy Use in an Untrained Domain
                                          G. Tanner Jackson (gtjacksn@memphis.edu)
                                        Institute for Intelligent Systems, University of Memphis
                                                         Memphis, TN 38152 USA
                                            Rebekah H. Guess (rguess@memphis.edu)
                                             Department of English, University of Memphis
                                                         Memphis, TN 38152 USA
                              Danielle S. McNamara (d.mcnamara@mail.psyc.memphis.edu)
                                           Department of Psychology, University of Memphis
                                                         Memphis, TN 38152 USA
                             Abstract
   Researchers of advanced technologies are constantly seeking                                    iSTART
   new ways of measuring and adapting to user performance.                 Interactive Strategy Training for Active Reading and
   Appropriately adapting system feedback requires accurate             Thinking (iSTART) is a web-based tutoring system
   assessments of user performance. Unfortunately, many                 designed to improve students reading comprehension by
   assessment algorithms must be trained on and use pre-
   prepared data sets or corpora in order to provide a sufficiently
                                                                        teaching self-explanation strategies. The iSTART system
   accurate portrayal of user knowledge and behavior. However,          was originally modeled after a human-based intervention
   if the targeted content of the tutoring system changes               called Self-Explanation Reading Training, or SERT
   depending on the situation, the assessment algorithms must be        (McNamara, 2004; McNamara & Scott, 1999; O’Reilly,
   sufficiently independent to apply to untrained content. Such is      Best, & McNamara, 2004). The automated iSTART system
   the case for iSTART, an intelligent tutoring system that             has consistently produced gains equivalent to the human-
   assesses the cognitive complexity of strategy use while a            based SERT program (Magliano et al., 2004; O’Reilly,
   reader self-explains a text. iSTART is designed so that
   teachers and researchers may add their own (new) texts into
                                                                        Sinclair, & McNamara, 2004; O’Reilly, Best, & McNamara,
   the system. The current paper explores student self-                 2004). Unlike SERT, iSTART is web-based, and can
   explanations from newly added texts (which iSTART hadn’t             potentially provide training to any school or individual with
   been trained on) and focuses on evaluating the iSTART                internet access. Furthermore, because it is automated, it can
   assessment algorithm by comparing it to human ratings of the         work with students on an individual level and provide self-
   students’ self-explanations.                                         paced instruction. iSTART also maintains a record of
   Keywords: Automatic assessment; reading strategies.                  student performance and can use this information to adapt
                                                                        its feedback and instruction for each student. Lastly, the
                          Introduction                                  iSTART system combines pedagogical agents and
                                                                        automated linguistic analysis to engage the student in an
Modeling student performance has become a somewhat
                                                                        interactive dialog and create an active learning environment
ubiquitous aspect of computer systems, particularly
                                                                        (e.g., Bransford, Brown, & Cocking, 2000; Graesser, Hu, &
intellignet tutoring systems. Student models range from
                                                                        Person, 2001; Graesser, Hu, & McNamara, in press;
simple (age, gender) to very complex (series of actions
                                                                        Louwerse, Graesser, & Olney, 2002).
triggering dynamic representations of knowledge
structures). The simpler models can be easily transferred
                                                                        iSTART Modules
across systems and into new domains; however they lack the
detail and sophistication to personalize system behavior.               iSTART incorporates pedagogical agents that engage users
More complex models can provide flexibility and                         with the system and tutor them on how to correctly apply
robustness within the system; however they usually require              various reading strategies. The agents were designed to
specific task attributes or training that may limit their               introduce students to the concept of self-explanation and to
adaptability to new domains.                                            demonstrate specific strategies that could potentially
   Situated in this struggle is any number of systems that              enhance their reading comprehension. The iSTART
model cognitive processes, drive interactive android                    program consists of three system modules that implement
behaviors, or simulate human tutors. This study focuses on              the pedagogical principle of modeling-scaffolding-fading:
an evaluation of an algorithm designed to assess the                    introduction, demonstration, and practice.
cognitive complexity of student generated self-explanations                The introduction module uses a classroom-like discussion
within an intelligent tutoring system, iSTART.                          format between three animated agents (a teacher and two
                                                                        student agents) to present the relevant reading strategies
                                                                        within iSTART. These agents interact with each other,
                                                                    2164

providing students with information, posing questions to             where the classroom or a student has committed to using the
each other, and giving example explanations to illustrate            system over time, such as over the course of a year. During
appropriate strategy use (including counterexamples). These          this practice phase, the student is assigned to read texts that
interactions exemplify the active processing that students           are usually chosen by the teacher. These are texts that may
should use when providing their own self-explanations.               be entered into the system with little notice. Because of the
After each strategy is introduced and the agents have                need to provide texts on the fly, the iSTART feedback
concluded their interaction, the students are asked to               algorithms must provide appropriate feedback, not only for
complete a set of multiple-choice questions that gauge their         the texts during initial practice (for which the iSTART
understanding of the recently covered concepts.                      algorithms are highly tuned), but also for new texts. For this
   After all strategies are introduced, students progress to the     reason, the iSTART evaluation algorithms must be highly
demonstration module. In the demonstration module, new               flexible and must be able to generalize to virtually any text.
animated characters interact (Merlin & Genie) and guide the
students as they attempt to analyze example explanations             iSTART Evaluation Algorithm
provided by the Genie agent. In this capacity, Genie acts as         Determining the appropriate feedback for each explanation
another example student, reads text aloud, and provides a            is dependent on the evaluation algorithm implemented
self-explanation for each sentence. Meanwhile, Merlin                within iSTART. Obviously the feedback has the potential to
instructs the learner to identify the strategies used within         be more appropriate when the evaluation algorithm more
each of Genie’s explanations. Merlin provides feedback to            accurately depicts explanation quality and related
Genie on his explanations and to the students on the                 characteristics. In order to accomplish this task and interact
accuracy of their strategy identifications. For example,             with students in a meaningful way, the system must be able
Merlin will tell Genie that his explanation is too short and         to adequately interpret natural language text explanations.
ask him to add information, or he will applaud when the                 Several versions of the iSTART evaluation algorithm
student makes a correct identification. The feedback                 have been tested and validated with human performance
provided to Genie is similar to the feedback that Merlin will        (McNamara et al., 2007). The resulting algorithm utilizes a
give to the students when they finish that section and move          combination of both word-based approaches and latent
on to the practice module.                                           semantic analysis (LSA; Landauer et al., 2007). The word-
   Once the students are in the practice module, Merlin              based approaches provide a more accurate picture of the
serves as their self-explanation coach. He provides feedback         lower level explanations (ones that are irrelevant, or simply
on their explanations and prompts them to generate new               repeat the target sentence). They are able to provide a finer
explanations using their newly acquired repertoire of                distinction between these groups than LSA. In contrast,
strategies. The main focus of this module is to provide              LSA provides a more informative measure for the higher
students with an opportunity to apply the reading strategies         level and more complex explanations. Therefore, a
to new texts and to integrate their knowledge from different         combination of these approaches is used to calculate the
sources in order to understand a challenging text. Their             final system evaluation.
explanation may include knowledge from prior text, or                   The word based approach originally required a significant
come from world and domain knowledge. Merlin provides                amount of hand-coded data, but now uses automatic
feedback for each explanation generated by the student. For          methods when new texts are added. The original measure
example, he may prompt them to expand the explanation,               required experts to create a list of “important” words for
ask the students to incorporate more information, or suggest         each text and then also list of associated words for each
that they link the explanation back to other parts of the text.      “important” word. This methodology was replaced, and now
Merlin sometimes takes the practice one step further and has         the word-based component relies on a list of “content”
students identify which strategies they used and where they          words (nouns, verbs, adjectives, adverbs) that are
were used. Throughout this interaction, Merlin’s responses           automatically pulled from the text via Coh-Metrix
are adapted to the quality of each student’s explanation. For        (Graesser, McNamara, Louwerse, & Cai, 2004; McCarthy,
example, longer and more relevant explanations are given             Guess, McNamara, in press; McNamara, Louwerse, &
more enthusiastic expressions, while short and irrelevant            Graesser, 2002). The word-based assessment also includes a
explanations prompt Merlin to provide more scaffolding and           length criterion where the student’s explanation must
support.                                                             exceed a certain number of words (calculated by
   There are two types of practice modules. The first practice       multiplying the number of words in the target sentence by a
module is situated within the context of the initial 2 hour          prespecified coefficient for each assessment category).
training. That is, initially, the student goes through the              The LSA based approach uses a set of benchmarks to
introduction, demonstration, and practice in about 2 hours.          compare student explanations to various text features. These
The initial practice includes practice with two texts, on            LSA benchmarks include 1) the title of the passage, 2) the
which the iSTART algorithms were trained (McNamara,                  words in the target sentence, and 3) the words in the
Boonthum, Levinstein, & Millis, 2007).                               previous two sentences. The third benchmark originally
   The second phase of practice, extended practice, begins           involved only words from causally related sentences, but
subsequently. Extended practice can be used in situations            this required more hand-coding, and thus was replaced by
                                                                 2165

the words from recent sentences. Within the science genre,        the student’s explanation incorporated some aspect of the
this replacement was expected to do well, because of the          text beyond the target sentence (text-based). If an
linear argumentation most often employed in science               explanation earns a “3” from the iSTART evaluation then
textbooks. However, it is unclear how well these assessment       the explanation incorporates information from a global
metrics will apply to new texts or domains.                       level, and may include outside information or refer to an
   The uncertainty of generalization raises an additional         overall theme across the whole text (global-based).
constraint for the current system. Namely, the strategies
within iSTART, though taught via science texts, actually                  Table 1: Examples of Self-Explanation Categories
apply to a wide range of text genres and the algorithm
should therefore be able to accommodate a comparable                   iSTART                             Example
range of explanations. This evaluation system was designed             Category
to be automated so that the text repository could be                   Target Sentence       “Energy-storing molecules are
expanded without consequence to system performance (i.e.,                                      produced on the inner folds.
so that the evaluation algorithm should perform equally well
across newly encountered texts). This need motivated the               Irrelevant                  “Hello, I am a taco.”
current study.
                                                                       Sentence-based       “the molecules holding on to the
                        Experiment                                                           energy are created on the inner
Participants included 549 students recruited from science                                                  folds.”
classes in Mid-Southern High Schools. Throughout the
course of a semester, students spent time each week                    Text-based             “These sentences say that the
interacting with iSTART. Their teachers added and assigned                                  mitochondria’s inner membrane
new science texts for the students to use within iSTART.                                         produces energy storing
The long-term interaction between students and iSTART                                                   molecules.”
provided a large repertoire of student explanations to
analyze (around 40,000). The current analyses represent a              Global-based        “The inner folds develop energy-
subset of these that were coded by human raters, consisting                                 storing molecules that help store
of 5,400 student generated self-explanations.                                                 more energy for the plant and
                                                                                                help it grow, survive, and
Procedure                                                                                               reproduce.”
All students interacted with the same version of iSTART (as       Human Ratings After all the student self-explanations were
described above). After training was complete (introduction       collected from extended practice (approximately 5400
module, demonstration module, and practice module),               explanations), three human experts were asked to provide
students used an iSTART extended practice module to self-         independent ratings. These human experts have little or no
explain texts assigned by their teachers. This extended           knowledge in how LSA works and they were extensively
practice module functioned just like the practice module          trained on self-explanation strategies, and the ratings were
within iSTART (including feedback). The only difference           provided independently of the iSTART algorithm scores
between the practice module and the extended practice             (i.e., raters never saw the output from iSTART). The human
module was the text being used. The practice module uses          raters provided scores for each self-explanation on seven
the same set of texts for every student, whereas the texts in     categories: garbage, vague/irrelevant, repetition, paraphrase,
the extended practice module were added by the teachers           local bridging, elaboration, and global bridging. An
themselves and were not included in any training or               explanation contained garbage if any words or entries were
validation by the iSTART algorithm.                               not coherent. Vague and irrelevant explanations consisted of
                                                                  information out of context that does not pertain to and
Assessments                                                       would not contribute to comprehension of the text. The
As students interacted with the texts in extended practice        student explanations that simply repeated the words from
their self-explanations were assessed by iSTART, feedback         the target sentence were classified as repetition. When
was provided, and the explanations were logged for future         students used their own words to restate the ideas from the
use.                                                              target sentence an explanation was categorized as a
                                                                  paraphrase. Local bridging occurred when students self-
iSTART Algorithm The iSTART assessment algorithm                  explained by using information from previous sentences
evaluated every student self-explanation. This assessment         within the text. An elaboration required that students expand
was coded as a 0, 1, 2, or 3. An assessment of “0” relates to     on the information from the text and incorporate some of
explanations that are either too short or contain mostly          their personal knowledge. Lastly, the raters considered
irrelevant information. An iSTART score of “1” is                 global bridging to be present when a student explanation
associated with an explanation that primarily relates only to     attempted to draw together a main idea or theme from the
the target sentence itself (sentence-based). A “2” means that
                                                              2166

text (this could possibly be a combination of both local          new texts on which the iSTART algorithm was never
bridging and elaboration together).                               trained.
  Ratings were provided on all seven categories for each
student self-explanation. The ratings ranged from 1                     Table 2: Correlations between iSTART and human
(definitely not present) to 6 (definitely present). Raters were                             ratings.
told to consider the difference between each rating level to
be equal, such that the distance between 1 and 2 was equal            Human Composite Ratings                          iSTART algorithm
to the distance between 2 and 3, and so forth. Interrater             Nonsense/Irrelevant Factor                           -.362**
reliability on a training set of data (including all 3 raters)        Sentence-Based Factor                                -.127**
resulted in an average correlation of .70 across all seven            Text-Based Factor                                     .637**
categories. The ratings for some of these categories were             Global-Based Factor                                  .593**
combined to form composite scores that represent scores
analogous to those provided by the iSTART algorithm. In               Human Algorithm Analog                                  .661**
essence, the human ratings were combined to provide a                ** indicates p<.001
similar score of 0, 1, 2, or 3 that represented
nonsense/irrelevant         explanations,       sentence-based       The correlations presented in Table 2 demonstrate a
explanations, text-based explanations, and global-based           reliable relation between the human ratings and those
explanations, respectively. The original seven categories         provided by the iSTART algorithm. The first four
provide a fine grained account of the student explanations,       correlations illustrate the relation between the iSTART
but these composite scores provide a more continuous              algorithm and each of the individual explanation levels. We
measure of the cognitive processing that is most likely to        would expect a significant negative correlation between the
contribute to each self-explanation.                              nonsense factor and the iSTART algorithm. This negative
  The composite ratings provide a general indicator for the       relation indicates that iSTART successfully attributed lower
amount of cognitive complexity involved in generating each        scores to explanations when they contained a higher amount
self-explanation. The nonsense/irrelevant explanations            of nonsense and irrelevant information. Thus iSTART was
obviously lack appropriate effort and/or focus and require        able to assess when students were off track or lost focus
no processing of the text. The nonsense/irrelevant                during their self-explanations. In a similar line of reasoning,
explanations receive a score of “0” because they represent        we would expect a positive correlation between the global
the least amount of cognitive effort (which is none at all).      factor and the iSTART evaluation. This positive correlation
Generating a sentence-based explanation requires only             illustrates that when students tended to include a more
minimal processing of the target text, and does not               global focus in their self-explanation the iSTART algorithm
demonstrate any inference making or knowledge activation.         evaluated this information appropriately. The last significant
These explanations are a step up from the previous category       correlation between the human algorithm analog and the
and involve minimal processing, so they receive a score of        iSTART algorithm reveals that the iSTART algorithm is
“1”. Creating text-based explanations requires integration        indeed extendable to new texts and is significantly similar to
between the target sentence and prior sentences, at least to      human performance.
the extent of connecting two ideas explicitly present in
different parts of the text. Text-based explanations receive a                              2500
                                                                                                                         iSTART scores:
score of “2” because drawing a connection between the
                                                                                                                          0     1      2   3
current sentence and a sentence/idea previously covered is                                  2000
more complex than addressing a single sentence alone.
                                                                        Number of ratings
Going beyond the text-based local connections, a global-
based explanation requires the activation of outside                                        1500
knowledge and/or making generalizations across multiple
points within the text. The global-based explanations                                       1000
include the activation and integration of knowledge, as well
as using it appropriately (i.e., not including irrelevant
                                                                                             500
information), and therefore represent the highest category of
explanation here (a score of “3”). Together these scores
provide a direct analog between the human scores and the                                       0
iSTART scores.                                                                                     Human 0   Human 1   Human 2      Human 3
                                                                                                             Human Scores (0-3)
                Results and Discussion                            Figure 1: Comparing ratings between iSTART and Humans.
Analyses were conducted to assess the relation between
iSTART’s evaluation algorithm and the assessments made              Figure 1 provides a general illustration of how well the
by expert human raters. Note that the current analyses all        iSTART algorithm ratings coincide with human ratings. It is
involve iSTART assessments on student explanations for            evident that humans and iSTART mostly agree on
                                                              2167

explanations that are nonsensical or irrelevant (both rate as a    screen, which does not require activation of prior knowledge
score of 0), sentence-based (both rate as a score of 1), and       or any inference generation. A text-based explanation may
global-based (both rate as a score of 3). It appears that the      include some paraphrasing, but it also requires the student to
text-based explanations are more difficult to determine.           make some connection within the text. Creating a successful
However, both humans and iSTART did agree that the                 text-based explanation means that the students must actively
incidence of text-based explanations (nhuman=167,                  connect current information, with information previously
niSTART=237) was much lower than either sentence-based             covered. While this process involves a small level of local
(nhuman=2872, niSTART=2754) or global explanations                 inference generation, it does not require any integration of
(nhuman=1228, niSTART=1393). The relative difficulty of            existing knowledge from the text or outside information. A
classifying text-based explanations has been demonstrated          global-based explanation, on the other hand, requires
in previous iSTART analyses (McNamara et al., 2007).               integration of knowledge, either from the text itself or from
   Further analyses were conducted to examine the specific         outside information. This global focus requires the student
agreement between the iSTART algorithm and the human               to go beyond the text itself and to actively process the
algorithm analog. A linear-weighted kappa yielded a                material and how it relates to various sources. The fact that
significant level of agreement between the human and               both iSTART and humans can agree on these classifications
iSTART scores, kappa = .646, p<.001. This result means             demonstrates that the algorithm can reliably distinguish
that the iSTART evaluation algorithm and human experts             between various levels of cognitive complexity required to
significantly agree on their assessments for student               generate a self-explanation.
explanations. The result is encouraging, considering that            Although these data are encouraging with respect to the
these assessments were made on untrained texts and across          iSTART algorithm, a couple of aspects may limit its’
an entire semester.                                                overall generalizability. The first limiting aspect is the fact
                                                                   that the iSTART system was originally designed to cover
                                                                   science texts, and during this study all newly added texts
                       Conclusions                                 were also within the science genre. While these new texts
The results from this analysis suggest that the iSTART             were within the same genre, they could cover vastly
algorithm has the ability to adapt to new texts and                different topics and share almost no explicit information
information in an appropriate and informative manner.              with the original texts.
Though similar analyses have been performed with the                 The second limiting aspect regards the identification of
iSTART algorithm, none of previous evaluations have                text-based explanations. As indicated by Figure 1, the text-
included extraneous texts. The significant results indicate        based explanations are difficult to identify and seem to
that iSTART’s evaluations are sufficiently accurate for            occur less often (both human experts and iSTART showed a
learning purposes (as additional studies have demonstrated         low density of text-based explanations). The difficulty of
consistent learning), and can reliably predict the amount of       identifying text-based explanations could be due to the lack
cognitive processing required to generate self-explanations.       of sufficient numbers, but more likely this can be attributed
   The agreement between iSTART and human raters means             to the nature of the explanations themselves. It seems that
that the system can accurately represent the students’             students tend to one of two things: 1) do very little
explanations and therefore provides the system an                  processing (nothing more than paraphrasing), or 2) they
opportunity to give accurate and meaningful feedback. This         become engaged and actively process the information.
increases the validity of the training, and provides greater       Obtaining a more homogenous distribution of strategies
assurance of a consistent application of appropriate               would help investigate the current claim that each category
pedagogy.                                                          requires an incremental step in cognitive processing.
   The results here are particularly interesting for the             The data presented here support the effort to improve
cognitive science community because they validate a tool on        automatic assessment techniques that relate to human
a completely untrained dataset as it accurately assesses the       cognitive processing. Though the algorithm structure itself
amount of cognitive processing required to produce natural         doesn’t correlate to any specific processes, it can use natural
language explanations. As mentioned earlier, each self-            language to reliably predict the amount of processing
explanation category (nonsense, sentence-based, text-based,        required for a student contribution. This computational
or global-based) was designed to assume an incremental             challenge has taken considerable effort, and now allows for
step up in the level of cognitive processing. The garbage          the system to be extended into areas not previously
and irrelevant explanations would require no amount of             considered. Future research will continue to evolve the
processing on the part of the student. These explanations          current algorithm and will incorporate new features into the
could include typing in the alphabet, inputting a single           system.
character, or making disparaging remarks. The sentence-
based explanations obviously require the student to at least                           Acknowledgments
read the target sentence. This category involves information       This research was supported in part by the Institute for
only related to one specific sentence. So the student              Educational        Sciences      (IES       R305G020018-02;
essentially paraphrases content already present on the             R305G040046, R305A080589) and National Science
                                                               2168

Foundation (NSF REC0241144; IIS-0735682). Any                       scores to predict text readability and facilitate
opinions, findings, and conclusions or recommendations              comprehension. Technical report, Institute for Intelligent
expressed in this material are those of the authors and do not      Systems, University of Memphis, Memphis, TN.
necessarily reflect the views of the IES or NSF. Thanks also      McNamara, D. S., & Scott, J. L. (1999). Training reading
to Dr. Phil McCarthy for his contribution to the paper.             strategies. In M. Hahn & S. C. Stoness (Eds.),
                                                                    Proceedings of the Twenty-first Annual Meeting of the
                                                                    Cognitive Science Society (pp. 387-392). Hillsdale, NJ:
                         References                                 Erlbaum.
Bransford, J., Brown, A., & Cocking, R., Eds. (2000). How         O’Reilly, T., Best, R., & McNamara, D. S. (2004). Self-
   people learn: Brain, mind, experience, and school.               Explanation reading training: Effects for low-knowledge
   Washington, D.C.: National Academy Press. Online at:             readers. In K. Forbus, D. Gentner, T. Regier (Eds.),
   http://www.nap.edu/html/howpeople1/                              Proceedings of the Twenty-sixth Annual Meeting of the
Graesser, A. C., Hu, X., & McNamara, D. S. (2005).                  Cognitive Science Society (pp. 1053-1058). Mahwah, NJ:
   Computerized learning environments that incorporate              Erlbaum.
   research in discourse psychology, cognitive science, and       O’Reilly, T., Sinclair, G. P., & McNamara, D. S. (2004).
   computational      linguistics.   In     A.    F.    Healy       Reading strategy training: Automated verses live. In K.
   (Ed.), Experimental cognitive psychology and its                 Forbus, D. Gentner, T. Regier (Eds.), Proceedings of the
   applications: Festschrift in honor of Lyle Bourne, Walter        Twenty-sixth Annual Meeting of the Cognitive Science
   Kintsch, and Thomas Landauer (pp. 183-194).                      Society (pp. 1059-1064). Mahwah, NJ: Erlbaum.
   Washington, D.C.: American Psychological Association
Graesser, A. C., Hu, X., & Person, N. (2001). Teaching with
   the help of talking heads. In T. Okamoto, R. Hartley,
   Kinshuk, J. P. Klus (Eds.), Proceedings IEEE
   International Conference on Advanced Learning
   Technology: Issues, Achievements and Challenges (460-
   461).
Graesser, A. C., McNamara, D. S., Louwerse, M. M., &
   Cai, Z. (2004). Coh-Metrix: Analysis of text on cohesion
   and language. Behavior Research Methods, Instruments,
   and Computers, 36, 193-202.
Landauer, T., McNamara, D. S., Dennis, S., & Kintsch, W.
   (Eds.). (2007). Handbook of Latent Semantic Analysis.
   Mahwah, NJ: Erlbaum.
Louwerse, M. M., Graesser, A. C., Olney, A., & the
   Tutoring Research Group. (2002). Good computational
   manners: Mixed-initiative dialog in conversational agents.
   In C. Miller (Ed.), Etiquette for Human-Computer Work,
   Papers from the 2002 Fall Symposium, Technical Report
   FS-02-02, 71-76.
Magliano, J. P., Todaro, S., Millis, K. K., Wiemer-Hastings,
   K., Kim, H. J., & McNamara, D. S. (2004). Changes in
   reading strategies as a function of reading training: A
   comparison of live and computerized training. Submitted
   for publication.
McCarthy, P. M., Guess, R.H, McNamara, D. S. (in press).
   The components of paraphrase. To appear in Behavior
   Research Methods.
McNamara, D. S. (2004). SERT: Self-explanation reading
   training. Discourse Processes, 38, 1-30.
McNamara, D.S., Boonthum, C., Levinstein, I.B., & Millis,
   K. (2007). Evaluating self-explanations in iSTART:
   comparing word-based and LSA algorithms. In T.
   Landauer, D.S. McNamara, S. Dennis, & W. Kintsch
   (Eds.),Handbook of Latent Semantic Analysis (pp. 227-
   241). Mahwah, NJ: Erlbaum.
McNamara, D. S., Louwerse, M. M., & Graesser, A. C.
   (2002). Coh-Metrix: Automated cohesion and coherence
                                                              2169

