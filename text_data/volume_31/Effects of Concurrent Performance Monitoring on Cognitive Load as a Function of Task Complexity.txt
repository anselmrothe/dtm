UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Effects of Concurrent Performance Monitoring on Cognitive Load as a Function of Task
Complexity

Permalink
https://escholarship.org/uc/item/7hs860fh

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Paas, Fred
Van Gog, Tamara

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Effects of Concurrent Performance Monitoring on Cognitive Load as a Function of
Task Complexity
Tamara van Gog (tamara.vangog@ou.nl)
Centre for Learning Sciences and Technologies & Netherlands Laboratory for Lifelong Learning,
Open University of The Netherlands
P.O. Box 2960, 6401 DL Heerlen, The Netherlands
1

Fred Paas (fred.paas@ou.nl)
Centre for Learning Sciences and Technologies & Netherlands Laboratory for Lifelong Learning,
Open University of The Netherlands
P.O. Box 2960, 6401 DL Heerlen, The Netherlands
2
Psychology Department, Erasmus University Rotterdam, The Netherlands

Abstract
For self-regulated learning to be effective, students need to be
able to accurately monitor their performance while they are
working on a task, use this as input for self-assessment of that
performance after the task, and select an appropriate new
learning task in response to that assessment. From a cognitive
load perspective, monitoring can be seen as a secondary task
that may become hard to maintain and may hamper
performance on the primary task (i.e., learning) under high
load conditions. Therefore, this study investigated the effects
of concurrent performance monitoring on cognitive load and
performance as a function of task complexity. Task
complexity was varied as between-subjects factor and
monitoring as within-subjects factor. It was hypothesized that
monitoring would significantly increase cognitive load and
decrease performance on complex, but not on simple tasks.
Results from a pilot study based on data from 31 participants
seem to confirm this hypothesis.
Keywords: education; cognitive load; monitoring; task
complexity; self-regulated learning.

Cognitive Demands of Self-regulated Learning
A major aim of many contemporary educational programs is
to foster students self-regulation skills. It is often assumed
that this aim can be achieved in a ‘learning by doing’
manner (i.e., by providing learners with a high amount of
control over their learning process). Unfortunately,
however, studies that compared the effects of learner
controlled vs. system controlled instruction, often show
detrimental effects on learning outcomes of providing
learners with control over what tasks they work on, in what
order, and for how long (e.g., Niemic, Sikorski, & Walberg,
1996). So even if learners would acquire self-regulation
skills this way (which can also be questioned, considering
the findings on learning by doing in acquiring problem
solving skills; cf. Kirschner, Sweller, & Clark, 2006;
Sweller, Van Merriënboer, & Paas, 1998), giving learners a
high degree of control may have unwanted effects when it
comes to learning outcomes. These effects, however, are not
entirely surprising if we look at the cognitive demands
imposed by self-regulated learning.

For self-regulated learning to be effective, students need
to be able to accurately monitor their performance while
they are working on a task, use this as input for selfassessment of that performance after the task, and select an
appropriate new learning task (one that allows them to train
the task aspects they do not yet master sufficiently) in
response to that assessment (cf., Ertmer & Newby, 1996;
Zimmerman, 1990). Research has shown, however, that
accurate self-assessment is very difficult for learners. Not
only are humans prone to several biases that make accurate
self-assessment difficult (see Bjork, 1999), but accuracy of
self-assessment also seems to be related to the amount of
experience in a domain (Dunning, Johnson, Erlinger, &
Kruger, 2003). Presumably, advanced learners are more
accurate self-assessors because their experience not only
provides them with more task knowledge, but also with
more knowledge of the criteria and standards that good
performance should meet (Dunning et al., 2003).
Interestingly, it is also the case that when positive effects on
learning outcomes are reported in studies on learner control,
this tends to be for high prior knowledge learners (Lawless
& Brown, 1997; Scheiter & Gerjets, 2007; Steinberg, 1989).
This suggests that the accuracy of self-assessment indeed
plays a very crucial role in the effectiveness of selfregulated learning.
However, as mentioned before, self-assessment of
performance after task completion, also relies on accurate
performance monitoring while working on the task. If
learners do not have a good recollection of their
performance, for example, of what actions they took and
what the results of those actions were, they cannot
accurately assess it. So, another possible explanation (which
is not mutually exclusive with the other ones mentioned
here) for why learners, and especially novice learners, are
not accurate self-assessors, is that difficulties may already
arise in the performance monitoring stage.

Monitoring and Cognitive Load
Many learning tasks are complex, that is, they impose a
high intrinsic cognitive load (Sweller, 1988; Sweller et al.,
1998). Intrinsic cognitive load depends on task complexity,

1605

because it is determined by the number of interacting
information elements that have to be related, controlled, and
kept active in working memory during task performance. It
also depends on the expertise of the task performer: As a
result of learning, elements are combined into cognitive
schemata stored in long-term memory that can be retrieved
and handled as a single element in working memory,
thereby decreasing the intrinsic load of the task (Sweller et
al., 1998).
The need to monitor performance during self-regulated
learning can be seen as a secondary task. Under dual-task
conditions, accurate performance of the secondary task or of
both the primary and the secondary task becomes hard to
maintain under high load conditions (see e.g., Brünken,
Plass, & Leutner, 2003). That is, under conditions of high
task complexity, or high intrinsic load, little resources are
available for processes that impose additional cognitive
demands, such as concurrently monitoring performance. As
a result, monitoring, task performance, or both, may be
hampered. Under conditions of low task complexity, or low
intrinsic cognitive load, on the other hand, additional
cognitive demands can be easily accommodated as ample
resources are available. In other words, under conditions of
high intrinsic load, which is the case with many learning
tasks, the need to monitor performance may: a) lead to low
quality monitoring (secondary task), and therefore, a poor
recollection of performance on which to base selfassessment, and/or b) hamper performance of the learning
task (primary task). This cognitive load perspective on the
cognitive demands that are imposed by learner control may
(at least partially) explain why self-regulated learning is
often ineffective for learning compared to teacher- or
system-controlled instruction. Moreover, given that intrinsic
load is also determined by expertise, this cognitive load
perspective would also (again, at least partially) explain
why novices might be less able to accurately monitor their
performance than individuals who have prior knowledge of
the task.
This pilot study seeks to establish the effects of
concurrent performance monitoring on cognitive load as a
function of task complexity, using Sudoku puzzles of
different complexity levels. It is hypothesized that the
instruction to monitor performance while working on the
task will lead to a significant increase in cognitive load and
a significant decrease in performance of the primary task on
complex, but not on simple tasks.

Method
Participants and Design
From a larger sample of 85 Dutch secondary education
students who volunteered to participate in this study, those
participants were selected who knew the rules of Sudoku
and sometimes played Sudoku (once a week on average),
resulting in 31 participants (11 males; age M = 15.42, SD =
.56). Task complexity was used as between-subjects factor

(Simple, n = 12; Complex, n = 19), whereas monitoring was
applied as within-subjects factor.

Materials
Demographic questionnaire. This short questionnaire
asked participants to indicate their age and gender as well as
there familiarity with Sudoku rules and their experience
with playing Sudoku.
Tasks. The tasks consisted of two Sudoku puzzles at each
level of complexity (Simple, Complex). Sudoku puzzles
consist of a grid with several regions that has to be filled
with numbers so that every row, column, and region contain
only one instance of each number. The two simple Sudoku
puzzles (low in intrinsic load) consisted of a 4x4 grid with
four 2x2 regions (mini-grids). Four cells were already filled
in. The complex Sudoku puzzles (high in intrinsic load)
consisted of a 9x9 grid with nine 3x3 regions (mini-grids).
In the first puzzle, 30 cells were already filled in, in the
second puzzle 28 cells. Both were at a medium level of
difficulty according to the source from which they were
obtained.
Mental effort rating scale. Invested mental effort was
measured using the 9-point subjective rating scale
developed by Paas (1992). The scale ranged from (1) very,
very low mental effort, to (9) very, very high mental effort.
This scale is a reliable measure of actual cognitive load (i.e.,
the cognitive capacity that is actually allocated to
accommodate the demands imposed by the task) and is
sensitive to variations in task complexity between and
within tasks (for details see Paas, Tuovinen, Tabbers, & Van
Gerven, 2003; Van Gog & Paas, 2008).

Procedure
In both conditions, participants first filled out the
demographic questionnaire. Then, they worked on the two
Sudoku puzzles, for maximally 2 minutes per puzzle. This
was not enough time to solve the Complex puzzles, but
participants were instructed to try and complete as much of
the puzzle as possible. Time on task was held constant to
cancel out potential interaction of this factor with mental
effort measures. Start and stop times were indicated by the
experimenter. Immediately after completing each puzzle,
participants rated their invested mental effort on the 9-point
rating scale. Monitoring was applied as a within-subjects
factor, with participants first working on the task without
monitoring (“Please try to complete as much of the puzzle
as you can within the next two minutes”), then with the
instruction to concurrently monitor their performance
(“Please try to complete as much of the puzzle as you can
within the next two minutes while simultaneously keeping
track of what you are doing, in what order, and why”). The
order of task performance with and without monitoring was
not counterbalanced, as the instruction to monitor on the
first task might influence later task performance even when
this instruction would not be given with the second task.

1606

Moreover, this order could not affect cognitive load to the
advantage of the hypothesis: as mentioned before,
experienced cognitive load tends to decline with increasing
practice of a task (cf. Van Gog, Paas, & Van Merriënboer,
2005; for a discussion of the relationship between
performance and cognitive demands as a result of practice
see e.g., Kanfer & Ackerman, 1989; Yeo & Neal, 2004).
Here, it is expected to increase on the second task (at least
on the complex tasks) as a result of the monitoring
instruction.

Data Analysis
Participants’ performance was rated by counting the
number of cells they correctly filled in. This resulted in a
maximum score of 12 on both Simple Sudoku puzzles, and
a maximum score of 51 and 53, respectively, on the
Complex puzzles.

Results
The manipulation of intrinsic cognitive load (caused by the
number of interacting information elements a task contains)
was successful: mean mental effort invested in the Simple
condition (n = 12) was 1.75 (SD = .81), whereas in the
Complex condition (n = 19) this was 6.03 (SD = 1.75). This
difference is highly significant t(29) = -7.90, p < .001.
GLM repeated measures analysis showed that in line
with our hypothesis, the instruction to monitor did not affect
mental effort ratings in the Simple tasks condition (Mwithout =
1.83, SD = 1.03; Mwith = 1.67, SD = .89), F(1,11) < 1, ns.
Nor did it affect performance: All participants in the Simple
task condition managed to correctly complete the entire
puzzle both times (i.e., maximum score of 12 on both tasks).
GLM repeated measures analysis showed that on
Complex tasks, instruction to monitor resulted in a trend
towards higher mental effort ratings (Mwithout = 5.68, SD =
2.11; Mwith = 6.37, SD = 1.71), F(1,18) = 3.63, MSE = 1.23,
p = .073, ηp2 = .168, and had a significant negative effect on
performance (Mwithout = 4.89, SD = 2.64; Mwith = 2.79, SD =
1.51), F(1,18) = 10.84, MSE = 3.88, p = .004, ηp2 = .376.

Discussion
Some caution is required in interpreting these data, because
of several reasons. First of all, this pilot study had a small
number of participants. Secondly, future studies need some
adaptations to the design, such as counterbalancing the
order of the tasks (even though they are at the same level of
difficulty, the possibility that the findings are due to
potential differences between the tasks should be ruled out).
Follow-up experiments are planned from which data should
be available at the time of the conference.
Although definite conclusions cannot yet be drawn, these
preliminary results seem to be in line with our hypothesis
that monitoring can be seen as a secondary task that
increased the total experienced cognitive load and decreased
performance on complex tasks. On simple tasks, these
effects did not arise, presumably because the task was so

simple that any additional cognitive demands could be quite
easily accommodated.
These results may provide at least a partial explanation
for why high prior knowledge learners seem better able to
assess their own performance (cf. Dunning et al., 2003) and
seem to do better than novices in learner-controlled
instruction (cf. (Lawless & Brown, 1997) when they are
working on the same tasks. Because these tasks are lower in
intrinsic load for the advanced learners, they may have
enough cognitive capacity available for performing the
learning task and monitoring their task performance
simultaneously.
However, it is important to note that this advantage for
high prior knowledge learners probably only arises when
they indeed work on the same tasks as novices. In effective
learning trajectories (whether self-regulated or regulated by
a teacher or system), students ideally work on tasks that are
at an appropriate level of difficulty for them, that is, on
tasks that are appropriately challenging (cf. Vygotsky’s,
1978, concept of ‘zone of proximal development’), not ones
that they can already perform (unless the goal is to automate
and/or speed up performance). In this sense, students at all
levels of expertise ideally always work on tasks that are
relatively high in intrinsic load for them.
Next to the methodological issues mentioned earlier that
need to be addressed in future studies, there are some other
interesting questions to be investigated. First of all, in this
study, effects on performance on the secondary task, that is,
on the quality of monitoring, were not investigated. The
data from this study suggests that for complex tasks,
monitoring has a negative effect on performance on the
primary task, but it would be interesting to investigate
whether this negative effect is due to more resources being
allocated to monitoring in order to perform that secondary
task well, or whether performance on both tasks suffers.
Therefore, future studies should also investigate the quality
of students’ monitoring in relation to cognitive load and task
complexity.
In addition, this study used tasks that were either very
simple or very complex; another interesting question we
intend to address is what would happen with tasks of
medium complexity (e.g., 6x6 Sudoku puzzles). It can be
hypothesized that these would lead to an increase in
cognitive load as evidenced by mental effort ratings, but
without the detrimental effect on performance because the
additional load might still be manageable.
Finally, in the future, studies such as this one should be
replicated with other types of tasks that rely more on
conceptual knowledge rather than procedural knowledge
and should take into account not only direct effects on task
performance, but also effects on learning.

Acknowledgments
This work is part of a research project funded by the
Netherlands Organization for Scientific Research (Veni
Grant 451-08-003).

1607

References
Bjork, R. A. (1999). Assessing our own competence:
Heuristics and illusions. In D.Gopher and A. Koriat
(Eds.), Attention and performance XVII. Cognitive
regulation of performance: Interaction of theory and
application (pp. 435-459). Cambridge, MA: MITPress.
Brünken, R., Plass, J. L., & Leutner, D. (2003). Direct
measurement of cognitive load in multimedia learning.
Educational Psychologist, 38, 53-61.
Dunning, D., Johnson, K., Erlinger, J., & Kruger, J. (2003).
Why people fail to recognize their own incompetence.
Current Directions in Psychological Science, 12, 83–87.
Ertmer, P. A. & Newby, T. J. (1996). The expert learner:
Strategic, self-regulated, and reflective. Instructional
Science, 24, 1-24.
Kanfer, R., & Ackerman, P. (1989). Motivation and
cognitive abilities: An integrative/aptitude-treatment
interaction approach to skill acquisition. Journal of
Applied Psychology, 74, 657-690.
Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why
minimal guidance during instruction does not work: An
analysis of the failure of constructivist, discovery,
problem-based experiential and inquiry-based teaching.
Educational Psychologist, 41, 75-86.
Lawless, K. A., & Brown, S. W. (1997). Multimedia
learning environments: Issues of learner control and
navigation. Instructional Science, 25, 117-131.
Niemiec, R. P., Sikorski, C., & Walberg, H. J. (1996).
Learner-control effects: A review of reviews and a metaanalysis. Journal of Educational Computing Research,
15, 157-174.
Paas, F. (1992). Training strategies for attaining transfer of
problem-solving skill in statistics: A cognitive load
approach. Journal of Educational Psychology, 84, 429434.
Paas, F., Tuovinen, J. E., Tabbers, H., & Van Gerven, P. W.
M. (2003). Cognitive load measurement as a means to
advance cognitive load theory. Educational Psychologist,
38, 63-71.
Scheiter, K., & Gerjets, P. (2007). Making your own order:
Order effects in system- and user-controlled settings for
learning and problem solving. In T. O’Shea, E. Lehtinen,
F. E. Ritter, & P. Langley (Eds.), In order to learn: How
ordering effects in machine learning illuminate human
learning and vice versa (pp. 195-212). Oxford: Oxford
University Press.
Steinberg, E. R. (1989). Cognition and learner control: A
literature review, 1977-88. Journal of Computer-Based
Instruction, 16, 117-124.
Sweller, J. (1988). Cognitive load during problem solving:
Effects on learning. Cognitive Science, 12, 257-285.
Sweller, J., Van Merriënboer, J. J. G., & Paas, F. (1998).
Cognitive architecture and instructional design.
Educational Psychology Review, 10, 251-296.
Van Gog, T., & Paas, F. (2008). Instructional efficiency:
Revisiting the original construct in educational research.
Educational Psychologist, 43, 16-26.

Van Gog, T., Paas, F., & Van Merriënboer, J. J. G. (2005).
Uncovering
expertise-related
differences
in
troubleshooting performance: Combining eye movement
and concurrent verbal protocol data. Applied Cognitive
Psychology, 19, 205-221.
Vygotsky, L. S. (1978). Mind in society. Cambridge, MA:
Harvard University Press.
Yeo, G. B., & Neal, A. (2004). A multilevel analysis of
effort, practice and performance: effects of ability,
conscientiousness, and goal orientation. Journal of
Applied Psychology, 89, 231–247.
Zimmerman, B. J. (1990). Self-regulated learning and
academic achievement: An overview. Educational
Psychologist, 25, 3-17.

1608

