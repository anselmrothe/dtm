UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
MDLChunker: a MDL-based Model of Word Segmentation
Permalink
https://escholarship.org/uc/item/2np5f8gq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Lemaire, Benoit
Robinet, Vivien
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                      MDLChunker: a MDL-based Model of Word Segmentation
                                           Vivien Robinet (vivien.robinet@imag.fr)
                                TIMC-IMAG laboratory, University of Grenoble, Domaine de la Merci
                                                      38706 La Tronche, FRANCE
                                           Benoît Lemaire (benoit.lemaire@imag.fr)
                                TIMC-IMAG laboratory, University of Grenoble, Domaine de la Merci
                                                      38706 La Tronche, FRANCE
                              Abstract                               computational models: those following a top-down
                                                                     approach, by inserting boundaries into continuous speech
  This paper applies a MDL-based computational model of              (bracketing strategy) and those using a bottom-up approach,
  inductive learning to the problem of word segmentation. The        creating new units by grouping frequent ones (clustering
  main idea is that syllables are grouped into words as soon as
  this operation decreases the size of the overall representation
                                                                     strategy)
  of the data, that is the codelength of information. When              Parser (Perruchet & Vinter, 1998) uses the clustering
  exposed to a stream of artificial words, our model                 strategy. It maintains a list of weighted candidate words,
  (MDLChunker) is able to reproduce Giroud & Rey (in press)          which can be viewed as a mental lexicon. At each time step,
  effect: humans learn sub-words as well as real words at the        Parser randomly selects between 1 and 3 units in the input
  beginning, but after a while they learn real words better than     to form a new candidate word. A unit is initially a syllable,
  sub-words. In order to better mimic human learning, a              but can become a longer group of syllables through an
  limited-size short-term memory was added to the model and
  estimates of its size are given.                                   aggregative chunking process, provided that their weights
                                                                     are high enough. This bottom-up mechanism is intended to
   Keywords: inductive learning; word segmentation;                  model the perception shaping phenomenon: what we
   minimum description length; computational model;                  already learned affects our perception. We do not view a
   distributional cues; simplicity principle
                                                                     new item as composed of elementary items if it has already
                                                                     been learned, we view it as a whole. For instance, “bkjbk” is
                          Introduction                               viewed as a sequence of 5 letters whereas “obama” is
In a seminal paper, Saffran et al. (1996) showed that, when          viewed as one item. Parser contains a reinforcement
exposed to a stream of concatenated artificial words,                parameter increasing the weight of the current percept. A
humans are able to segment correctly and learn the words             forgetting mechanism is also implemented by a constant
from the only transitional probabilities between syllables.          diminution of each weight at each time step. Finally, an
This result suggested that infants, although sensitive to            interference mechanism slightly decreases the weight of
acoustical factors such as phrasal prosody or lexical stress         each candidate word which shares a syllable with the new
(Swingley, 2005), could be influenced by distributional cues         percept. When applied to a long sequence of concatenated
to segment the stream of speech they are exposed to.                 artificial words, the best-weighted candidate words are those
  This paper presents a cognitive computational model                of the language (Perruchet & Vinter, 1998). BootLex
(MDLChunker) to account for that phenomenon. It is based             (Batchelder, 2002) follows a bottom-up approach similar to
on the general idea that humans tend to make decisions               PARSER.
leading to the simplest representation, i.e. minimizing the             Top-down word segmentation can be simulated by
codelength of information in memory. Compared to existing            connectionist models. Christiansen, Allen & Seidenberg
models, it does not rely on any adjustable parameters.               (1998) used a simple recurrent network to extract word
  Originally, MDLChunker was build to predict human                  boundaries from a corpus of child directed speech.
performances on artificial grammar learning tasks, which is             Brent & Cartwright (1996) model also follows a top-down
a classical paradigm in the implicit learning field (Servan-         approach. It does not contain a mechanism to aggregate the
Schreiber & Anderson, 1990). Our purpose is to use the               syllables, but only assess the relevance of a given
same model to account for word segmentation, which is the            segmentation. Therefore, it exhaustively generates all
task traditionally studied in statistical learning. In this way,     possible segmentations and uses the minimum description
we follow Perruchet & Pacton (2006) who propose that                 length principle (MDL) to select the most relevant one.
implicit learning and statistical learning are two approaches        Similarly, Argamon et al. (2004) uses the MDL principle in
of the same phenomenon.                                              a segmentation task consisting in finding prefixes and
                                                                     suffixes inside words. Since our model also uses that
                        Existing models                              minimum description length principle, we now present this
Two strategies can be used to model the way infants may              idea.
solve the word segmentation problem (Swingley, 2005),
which corresponds to two kinds of distributional-based
                                                                   2866

                 A MDL-based model                               Case #1:
Following Chater (1999), our hypothesis is that simplicity       Lexicon=a, b, c, ab, cc ; Data|Lexicon=ab c ab c ab c ab cc b
can account for many cognitive tasks. We already built a         c b ab cc b ab c ab c ab c ab c ab.
model (Robinet et al., 2008), that implement the general         For example, codelength for c is -log2(11/32) because c
notion of simplicity using the formalism provided by the         occurs 11 times out of 32. Codelength(cc) is -log2(3/32), etc.
MDL principle. This model was designed to predict the time       The size for the entire case #1 is therefore 65.5 bits.
course of concept creation in a task where participants are
learning an artificial grammar. We believe this model is         Case #2:
general enough to apply to other tasks, such as word             Lexicon=a, b, c, ab, cc, abc. Since a new word “abc” is
segmentation. When applied to the word segmentation task,        added to the lexicon, the data are segmented accordingly:
MDLChunker could be seen as an online version of Brent &         Data|Lexicon=abc abc abc abc c b c b abc c b abc abc abc
Cartwright (1996) model, with an explicit representation of      abc ab
chunks progressively updated over time.                          The total size for case #2 is 63.4 bits. Therefore, case #2 is a
   From this point of view of simplicity, a good                 better segmentation because its total size is smaller.
segmentation would minimize the amount of information
which has to be stored. Using a lexicon would compress
information by limiting redundancy. For instance, since the
sequence of letters “o b a m a” is frequent, adding the word
“w1: o b a m a” to the lexicon would compress the input.
Adding “w2: b k j b k” would not compress anything, only
consuming the memory size necessary to define it. A good
segmentation can therefore be viewed as a trade-off between
the conciseness of the lexicon and the expressiveness of the
input data with respect to that lexicon. For instance, a very
small lexicon, although saving resources, would lead to a
high number of unrecognized words in the input. On the
contrary, a very detailed lexicon corresponding to numerous
combinations of syllables would take a large place in
memory, although being good at processing new inputs.
   Information theory offers a formal way to implement that
idea, namely the minimum description length (MDL)
principle (Rissanen, 1978). This method consists in
computing the lengths of the codes for representing the
lexicon (hereafter represented as Lexicon) and the lengths of
the codes for representing the input data knowing the
lexicon (hereafter represented as Data|Lexicon), and
minimize their sum. Codelengths are estimated by means of
Shannon's formula, saying that a symbol s, occurring with
probability p, can be ideally compressed with a binary code
whose length is I(s)=-log2(p). In our case, p is estimated by
the frequency of s.
  Let us give an example. Suppose a language composed of
the two words “abc” and “cb”. Input is therefore a long
concatenation of these words. A good lexicon would contain
“abc” and “cb” after enough data has been processed.
Suppose that, so far, we have already processed
“abcabcabcabccbcbabccbabcabcabc”. Because we have
already learned the words “a”, “b”, “c”, “ab” and “cc”, the
data was segmented as such: “ab c ab c ab c ab cc b c b ab
cc b ab c ab c ab c”. If we need to process “abcab”, there are       Figure 1: Sizes of lexicon and data for case #1 and case
several ways to segment the new input. Let us consider two                   #2 after processing the input “abcab”
of them: “ab c ab” and “abc ab”. Case #1 only uses existing
words whereas case #2 suggests “abc” as a new word.              This way of selecting the most probable segmentation
Figure 1 presents codelengths for the two cases.                 humans have chosen can be viewed as a form of Occam's
                                                                 razor: given several segmentation hypotheses, humans
                                                                 would select the simplest one. This idea that simplicity
                                                               2867

          Figure 2 : Time course of codelengths obtained with our model during the processing of the first 750 syllables.
                    Codelengths are presented for all the words, sub-words and non-words of the experiment.
could be a unifying principle in cognitive science has been        random concatenation of these words, uttered at the rate of
discussed by Chater & Vitanyi (2003).                              3.3 syllables/s by a speech synthesizer without any prosodic
As we mentioned earlier, Brent & Cartwright (1996) used            information.
the MDL principle for selecting the best segmentation, but            After a 10 min training, recognition performances on
they had to generate all possible combinations. In order to        dissyllabic words (such as "GH") was significantly higher
be more cognitively plausible, we designed a model to              than those obtained on dissyllabic sub-words (such as
account for the process by which words are progressively           "BC"), while no difference occurs after a 2 min training.
aggregated through the process of the input.                          Authors successfully reproduced this vanishing sub-word
  In its first version, MDLChunker considers a fixed-size          effect with PARSER, using the same Perruchet & Vinter
part of the input at each step. It then operates in 3 sub-steps:   (1998) parameter values. This result credits the bottom-up
      the new piece of input is encoded using the existing        hypothesis by suggesting that sub-word recognition is a
          words in the lexicon;                                    necessary step in the word learning process.
      the model creates new words by grouping two                    Our model was run on the same artificial language to
          existing words, provided that they decreases the         check whether we could reproduce this vanishing sub-word
          overall size (MDL principle);                            effect. The input was split in blocks of 5 syllables1. After
      data is re-expressed using the existing words.              each block was processed, codelengths were computed for
  With this architecture, our model is able to correctly           the 2 tri-syllabic words (ABC and DEF), the 4 dissyllabic
segment a stream of artificial words, even if words share          words (GH, IJ, KL and MN), the 4 sub-words (AB, BC, DE
some syllables. It successfully learned the artificial words of    and EF) and the 8 non-words used by Giroud & Rey (CK,
the Perruchet & Vinter (1998) experiment.                          FM, CG, FI, HI, JK, LM and NG). 1000 simulations were
                                                                   performed with random input sequences and codelengths
            The vanishing sub-word effect                          were averaged.
This model was also tested on data from Giroux & Rey (in           Figure 2 presents all codelengths as a function of the
press). They designed a new experiment to compare                  number of syllables processed. The lower the codelength,
recognition performances of adults hearing either 2 or 10
min of an artificial spoken language. The language is                 1
                                                                              Because input was split in blocs of 5 syllables,
composed of 6 words, formally represented here as ABC,             trissyllabic words are statistically broken more often than
DEF, GH, IJ, KL and MN. Participants just listened to a            dissyllabic ones. Thus, isolated syllables from trissyllabic words
                                                                   are more frequent and have a lower codelength
                                                                 2868

the more frequent the word. At the beginning, there is no
difference between all dissyllabic words and sub-words, but
tri-syllabic ones are trivially coded using 50% more space.
After about 30 syllables have been processed, things begin
to change: non-words2 costs more and more to be
represented, whereas real words, both dissyllabic3 and
trissyllabic, are efficiently coded, even less than sub-words.
We used the test designed by Giroux & Rey, in order to
compare our model to human data. The words GH, IJ, KL,
MN are tested against the non-words CK, FM, CG, FI and
the sub-words AB, DE, BC, EF are tested against the non-
words HI, JK, LM, NG. In this test, the model has to choose
the dissyllabic unit that best matches with the language on
which it was trained (in our case, that with the lowest
codelength). The two tests (words vs non-words and sub-
words vs non-words) are performed eight times for each
training phase (virtual participant). Averaged performances
are presented Figure 3 along with those obtained by Giroux
& Rey.
   No significant difference between performance on words
over sub-words was obtained with our model at 15 syllables
(F(1, 999)=0.69 ; p=0.41), whereas a significant difference
(F(1, 999)=465 ; p<10-16) is observed at 75 syllables.
Fed with the same artificial grammar, our model could thus
reproduce this vanishing sub-word effect, but the rate of
learning appears too fast compared to the human
counterpart. This is due to the fact that our model never
forgets anything. In order to be more cognitively plausible,
we improved our model by restricting its memory i.e. the
data from which the model could make associations to
create new words. We also changed the way the input is
processed in order not to split the input into fixed-size parts
but instead to process it as a stream of syllables.
    Improvement: adding of a memory module
Instead of taking into account all the data already processed,
this new version of the model used a memory buffer to only
keep a given amount of data. This buffer plays the role of a
short-term memory (STM) whereas the set of existing words
(the lexicon) is more like a long-term memory. The model
works in the following way (Figure 4):
       The beginning of the current input is segmented in
          order to minimize its codelength and the first two
          units are candidates for forming a new word
          (perception shaping);
   2
          A difference between HI, HK, LM, NG and CK, FM,                  Figure 3 : Percentages of correct results for humans,
CG, FI could be observed in Figure 2, because of the lower              PARSER and our model, on the two tests: words vs non-
codelength of syllables C and F which are parts of trissyllabic          word and sub-words vs non-words. A 2 minutes training
words (see footnote 1).                                                 corresponds to 400 syllables and a 10 minutes training to
   3
          A difference between BC, EF and AB, DE could be                                   2000 syllables.
observed in Figure 2. While our model systematically creates the
first among two possible new words, AB and DE are created more
often than BC and EF. The former are only created if the input split
breaks ABC in A and BC, which is less frequent than having either
ABC or AB and C.
                                                                     2869

         This new word is created only if its creation would     is then candidate for being a new word in the lexicon
          decrease the overall size of the system (STM            (Figure 4, step 2).
          rewritten + lexicon with the new word).                    With this new word, the state of the system would be
         The new percept (the first input unit) is added to      represented using 63.4 bits, which is better than the 65.5 bits
          STM;                                                    if “abc” were not created. Therefore, the new word is
         Old percepts exceeding memory size are removed          created and “ab” is added to STM (Figure 4, step 3).
          from STM. This step depends both on the memory             There are several ways to avoid STM overflow when a
          size and on the codelength of the oldest percepts.      new set of syllables has to be added. We could have kept a
                                                                  fixed number of “words”. In order to be more coherent with
                                                                  the rest of our model, we kept a fixed quantity of
                                                                  information i.e., a maximum number of bits. After a new
                                                                  percept has been added to memory, old percepts are
                                                                  therefore removed to keep memory size under its limit.
                                                                  Then the process continues with the next input.
                                                                           Estimation of a good memory size
                                                                  By supplementing our model with a finite memory, we
                                                                  added a parameter (the memory size) that needs to be
                                                                  adjusted. With a huge memory (1000 bits), the model learns
                                                                  at a very high rate, like in its previous version. This is not
                                                                  cognitively plausible and does not correspond to human
                                                                  data. With a too small memory (100 bits), no learning
                                                                  occurs at all, because there is not enough data available at
                                                                  the same time in memory to find regularities. With a size of
                                                                  150 bits, we found a good similarity between the model and
                                                                  the human learning rate. With this 150 bits memory size, we
                                                                  reproduced both the vanishing-words effect, and the time
                                                                  course of learning. At 2 minutes (400 syllables) no
                                                                  significant difference (F(1, 999)=-0.21; p=0.64) was observed
                                                                  on performances between words and sub-words. This
                                                                  difference became significant (F(1, 999)=48.4; p<10-12) after
                                                                  a 10 minute training.
                                                                                Comparison with PARSER
                                                                  Our MDL based model obtains results that are very close to
                                                                  those obtained with PARSER. These two chunking models
                                                                  use two different approaches to implement the same
                                                                  functions: perception shaping, reinforcement, forgetting and
                                                                  interference.
                                                                  Perception shaping
                                                                  The underlying idea is that perception depends on what was
                                                                  already learned.
                                                                     PARSER: Shape the input by selecting the longest
                                                                  existing unit above a fixed threshold (shaping threshold).
                                                                     MDLChunker: When looking for the shortest encoding
    Figure 4: Architecture of our system after adding of the      of the input, the model tends to prefer units with shorter
                        memory module                             codelengths, since codelength depends on earlier
                                                                  perceptions.
Let us go back to the previous example in order to highlight
the changes in the new version of the model. Existing words       Reinforcement
are “a”, “b”, “c”, “ab” and “cc”. Previous STM state is the       The idea is to increase the weight of recently perceived unit
following: “ab c ab c ab c ab c ab cc b c b ab cc b ab c ab c     in order to give frequent units a higher weight.
ab c”. New input is “abcabccb ...” shaped as “ab c ab cc b”          PARSER: Add a fixed weight to the perceived unit. The
(Figure 4, step 1). The first two units are selected in the new   weight differs depending on the position relative to the
input. In our case, this is “ab” and “c”. The new word “abc”      shaping threshold.
                                                                2870

  MDLChunker: Perception of a new unit increases its                                     References
frequency, thus naturally decreasing its codelength.
                                                                 Argamon, S., Akiva, N., Amir, A., & Kapah, O. (2004).
                                                                   Efficient unsupervised recursive word segmentation using
Forgetting
                                                                   minimum description length. In Proc. 20th International
The idea is to decrease the weight of the units that no longer     Conference on Computational Linguistics (Coling-04).
occurs.                                                          Batchelder, E. O. (2002). Bootstrapping the lexicon: A
  PARSER: At each iteration it decreases all the unit              computational model of infant speech segmentation.
weights by a fixed quantity.                                       Cognition, 83(2), 167-206.
  MDLChunker: Frequency of unperceived units slightly            Brady, T. F., Konkle, T., & Alvarez, G. A. (2008). Efficient
decreases naturally after each new perceived unit.                 Coding in Visual Short-Term Memory: Evidence for an
                                                                   Information-Limited Capacity. In B. C. Love, K. McRae,
Interference                                                       & V. M. Sloutsky (Eds.), Proceedings of the 30th Annual
The idea is that when shaping the perceived unit by using          Conference of the Cognitive Science Society (pp. 887-
some units, other units that could apply have their weight         892). Austin, TX: Cognitive Science Society.
decreased.                                                       Brent, M. R., & Cartwright, T. A. (1996). Distributional
  PARSER: Decrease the weight of all the units                     regularity and phonotactic constraints are useful for
overlapping the perceived unit.                                    segmentation. Cognition, 61(1-2), 93-125.
  MDLChunker: Frequency of units not used to encode              Chater, N. (1999). The Search for Simplicity: A
the perceived unit slightly decreases naturally, which in turn     Fundamental Cognitive Principle? The Quarterly Journal
increases its codelength, thus making it less interesting for      of Experimental Psychology A, 52, 273-302.
future perceptions.                                              Chater, N., & Vitanyi, P. (2003). Simplicity: a unifying
                                                                   principle in cognitive science? Trends in Cognitive
                        Conclusion                                 Sciences, 7(1), 19-22.
In this paper we show that, our original chunking model          Christiansen, M. H., Allen, J., & Seidenberg, M. S. (1998).
designed for an implicit learning paradigm, can easily             Learning to segment speech using multiple cues: A
account for word segmentation, which is the classical              connectionist model. Language and Cognitive Processes,
paradigm used in statistical learning (Saffran et al., 1996).      13(2/3), 221-268.
  Without any improvement in our parameter-free model,           Giroux, I., & Rey, A. (in press). Lexical and sub-lexical
we successfully reproduced the Perruchet & Vinter (1998)           units in speech perception. Cognitive Science.
experiment as well as the vanishing sub-word effect              Miller, G. A. (1956). The magical number seven, plus or
presented by Giroux & Rey (in press). We observed no               minus two: Some limits on our capacity to process
significant differences between words and sub-words at the         information. Psychological Review, 63(2), 81-97.
beginning of the training, following by a significant            Perruchet, P., & Pacton, S. (2006). Implicit learning and
superiority of words over sub-words during the training.           statistical learning: one phenomenon, two approaches.
While the memory size is infinite in this first version of our     Trends in Cognitive Sciences, 10(5), 233-238.
model, the learning rate is much higher than humans one.         Perruchet, P., & Vinter, A. (1998). PARSER: A Model for
We limited this memory size to account for forgetting              Word Segmentation. Journal of Memory and Language,
effects. We found that for a size of 150 bits, model and           39(2), 246-263.
humans learning rates are equivalent according to the            Rissanen, J. (1978). Modeling by shortest data description.
conducted tests.                                                   Automatica, 14(5), 465–471.
  Both MDLChunker and PARSER can extract words from              Robinet, V., Bisson, G., Gordon, M., & Lemaire, B. (2008).
a stream of syllables. They are also able to reproduce the         Modèle cognitif de l'apprentissage inductif de concepts.
vanishing sub-word effect, suggesting that chunking is an          In Actes du colloque annuel de l'association pour la
efficient bottom-up process to model word segmentation.            recherche cognitive. France.
  We plan several kinds of improvements in the future.           Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
First, this general MDL-based model of inductive learning          Statistical Learning by 8-Month-Old Infants. Science,
needs to be applied to other word segmentation tasks as well       274(5294), 1926.
as other problems. We also aim at improving its model of         Saffran, J. R., Newport, E. L., & Aslin, R. N. (1996). Word
memory and we will probably attempt to unify the                   Segmentation: The Role of Distributional Cues. Journal
representation of the lexicon (long-term memory) and the           of Memory and Language, 35(4), 606-621.
data already processed (short-term memory). The idea that        Servan-Schreiber, E., & Anderson, J. R. (1990). Learning
memory could be modeled in term of quantity of                     Artificial Grammars With Competitive Chunking. Journal
information (Brady et al., 2008) appear challenging to us. It      of Experimental Psychology: Learning, Memory, and
would be interesting to see how this point of view compares        Cognition, 16(4), 592-608.
with the classical view of expressing memory limit in terms      Swingley, D. (2005). Statistical clustering and the contents
of a number of chunks (Miller, 1956).                              of the infant vocabulary. Cognitive Psychology, 50(1), 86-
                                                                   132.
                                                               2871

