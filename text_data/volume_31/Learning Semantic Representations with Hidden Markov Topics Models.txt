UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Semantic Representations with Hidden Markov Topics Models
Permalink
https://escholarship.org/uc/item/17s6g6dm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Andrews, Mark
Vigliocco, Gabriella
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Learning Semantic Representations with Hidden Markov Topics
                                                          Models
                                       Mark Andrews (m.andrews@ucl.ac.uk)
                                   Gabriella Vigliocco (g.vigliocco@ucl.ac.uk)
                                          Cognitive, Perceptual and Brain Sciences
                                                  University College London,
                                                        26 Bedford Way
                                                     London, WC1H 0AP
                                                       United Kingdom
                          Abstract                                information is unavailable in bag-of-words models and
                                                                  consequently the extent to which they can extract se-
   In this paper, we describe a model that learns seman-
   tic representations from the distributional statistics of      mantic information from text, or adequately model hu-
   language. This model, however, goes beyond the com-            man semantic learning, is limited.
   mon bag-of-words paradigm, and infers semantic repre-
   sentations by taking into account the inherent sequential         In this paper, we describe a distributional model that
   nature of linguistic data. The model we describe, and          goes beyond the bag-of-words paradigm. This model
   which we refer to as a Hidden Markov Topics model is           is a natural extension to the current state of the art
   a natural extension of the current state of the art in
   Bayesian bag-of-words models, i.e. the Topics model of         in probabilistic bag-of-words models, namely the Topics
   Griffiths, Steyvers, and Tenenbaum (2007), preserving          model described in Griffiths et al. (2007) and elsewhere.
   its strengths while extending its scope to incorporate         The model we propose is a seamless continuation of the
   more fine-grained linguistic information.
                                                                  Topics model, preserving its strengths — its thoroughly
                                                                  unsupervised learning, its hierarchical Bayesian nature
                      Introduction
                                                                  — while extending its scope to incorporate more fine-
How word meanings are learned is a foundational prob-             grained sequential and syntactic data.
lem in the study of human language use. Within cogni-
tive science, a promising recent approach to this problem                               The Topics Model
has been the study of how the meanings of words can
be learned from their statistical distribution across the         The standard Topics model as described in Griffiths and
language. This approach is motivated by the so-called             Steyvers (2002, 2003); Griffiths et al. (2007) is a prob-
distributional hypothesis, originally due to Harris (1954)        abilistic generative model for texts, and is based on the
and Firth (1957), which proposes that the meaning of              Latent Dirichlet Allocation (LDA) model of Blei, Ng,
a word is given by the linguistic contexts in which it            and Jordan (2003). It stipulates that each word in a cor-
occurs. Numerous large-scale computational implemen-              pus of texts is drawn from one of K latent distributions
tations of this approach — including, for example, the            φ1 . . . φk . . . φK $ φ, with each φk being a probability
work of Schütze (1992), the HAL model (Lund, Burgess,            distribution over the V word-types in a fixed vocabu-
& Atchley, 1995), the LSA model (Landauer & Dumais,               lary. These distributions are the so-called topics that
1997) and, most recently, the Topics model (Griffiths             give the model its name. Some examples, learned by a
et al., 2007) — have succesfully demonstrated that the            Topics model described in Andrews, Vigliocco, and Vin-
meanings of words can, at least in part, be derived from          son (In Press), are given in the table below (each column
their statistical distribution in language.                       gives the 7 most probable word types in each topic).
   Important as these computational models have been,
                                                                       theatre        music   league    prison      air
one of their widely shared practices has been to treat the
                                                                         stage         band     cup     years    aircraft
linguistic contexts in which a word occurs as unordered
                                                                           arts        rock   season  sentence    flying
sets of words. In other words, the linguistic context of
                                                                          play         song    team      jail     flight
any given word is defined by which words co-occur with
                                                                         dance        record   game     home      plane
it and with what frequency, but it disregards all fine-
                                                                         opera         pop    match   prisoner   airport
grained sequential and syntactic information. By disre-
                                                                           cast       dance  division  serving     pilot
garding these types of data, these so-called bag-of-words
models drastically restrict the information from which            As is evident from this table, each topic is a cluster of
word meanings can be learned. All languages have strong           related terms that corresponds to a coherent semantic
syntactic-semantic correlations. The sequential order in          theme, or subject-matter. As such, the topic distribu-
which words occur, the argument structure and general             tions correspond to the semantic knowledge learned by
syntactic relationships within sentences, all provide vital       the model, and the semantic representation of each word
information about the possible meaning of words. This             in the vocabulary is given by a distribution over them.
                                                              154

                                                                    generated according to an elementary language model
                                                                    — specifically a mixture of unigram distributions —
                                 wji                                which are then hierarchically coupled with one another.
                                                                    From this, it is evident that the standard model can be
                      φ                                             extended by simply changing the elementary language
                                                                    model on which it is based. There are multiple possi-
                                 xji                                ble language models that could be used in this respect.
                                       1 ≤ i ≤ nj
                                                                    One possibility is to use a bigram language model as
                      β                                             described in Wallach (2006). Another possibility is to
                                                                    use a language model based on a full phrase-structure
                                  πj
                                           1≤j≤J
                                                                    grammar as described in Boyd-Graber and Blei (2009).
                                                                    However, in that work, the syntactic structure underly-
                                                                    ing the sentences in the texts is assumed to be known
                                  α                                 in advance, and is provided by syntactically tagged cor-
                                                                    pus. In what follows, we describe a Topics models whose
                                                                    elementary language model is a Hidden Markov model
Figure 1: A Bayesian network diagram of the standard                (HMM). We refer to this as the Hidden Markov Topics
Topics model described in Griffiths et al. (2007) and else-         model (HMTM)1 .
where. Details are provided in the main text. Note that
β denotes the parameters of a V -dimensional Dirichlet                         Hidden Markov Topics model
distribution, from which each of K topic distributions              In the HMTM, just as in the standard Topics model, each
are sampled.                                                        wji is drawn from one of K topics, the identity of which
                                                                    is determined by the latent variable xji . However, rather
   To describe the Topics model more precisely, let us              than sampling each xji independently from a probability
assume we have a corpus of J texts w1 . . . wj . . . wJ ,           distribution πj , as in the standard model, these latent
where the jth text wj is a sequence of nj words, i.e.               variabls are generated by a Markov chain that is specific
wj1 . . . wji . . . wjnj . Each word, in each text, is assumed      to text j. By direct analogy with the standard model,
to be sampled from one of the model’s K topics. Which               across the texts, the parameters of these Markov chains
one of these topics is chosen is determined by the value            are drawn from a common set of Dirichlet distributions.
of a latent variable xji that corresponds to each word              As such, the HMTM is a hierarchical coupling of HMMs,
wji . This variable takes on one of K discrete values,              defining a continuum of Hidden Markov models, all shar-
and is determined by sampling from a probability distri-            ing the same state to output mapping.
bution πj , which is specific to text j. As such, we can               In the HMTM, the likelihood of the corpus is
see that each text wj is assumed to be a set of nj in-                                       YJ Z
dependent samples from a mixture model. This mixture                 P(w1:J |φ, α, γ) =             dπj dθj P(πj , θj |α, γ)
model is specific to text j, as the mixing distribution                                     j=1
is determined by πj . However, across texts, all mixing               
                                                                               nj                  nj
                                                                                                                                      
distributions are drawn from a common Dirichlet distri-               
                                                                        XY
                                                                                  P(wji |xji , φ)
                                                                                                  Y
                                                                                                      P(xji |xji−1 , θ j )P(xj1 |π j ) .
bution, with parameters α. Given known values for φ
                                                                        {xj } i=1                 i=2
and α, the likelihood of the entire corpus is given by
                                                                                                                                      (2)
                                     J  Z
                                                                       Here, θj and πj are the parameters of the Markov
                                    Y
              P(w1:J |φ, α) =              dπj P(πj |α)
                                   j=1
                                                                    chain of latent-states in text j. The θj is the K × K
                                                          (1)     state transition matrix (i.e. the kth row of θj gives the
                   nj
                  Y X                                               probability of transitioning to each of the K states, given
                            P(wji |xji , φ)P(xji |πj ) ,
                                                                    that the current state is k), and πj is the initial distribu-
                  i=1 {xji }
                                                                    tion for the K states. The distribution over the πj and
In this, we see that the Dirichlet distribution P(πj |α)                1
                                                                          The HMTM bears some resemblance to a model de-
introduces a hierarchical coupling of all the mixing dis-           scribed in Griffiths et al. (2007) that couples a Hidden
tributions. As such, the standard Topics model is a hier-           Markov model with a standard Topics model. There are,
                                                                    however, substantial differences between these models. In
archical coupling of mixture models, effectively defining           particular, the HMTM is designed to learn semantic repre-
a continuum of mixture models, all sharing the same K               sentations by directly availing of the sequential information
component topics.                                                   in text. By contrast, the HMM based model described in
                                                                    Griffiths et al. (2007) learns semantic representations using
   The standard Topics model is thus an example of a                a standard topics model, while the HMM is used to learn
hierarchical language model. It assumes that each text is           syntactic categories.
                                                                155

θj , i.e. P(πj , θj |α, γ), is a set of independent Dirich-               On convergence of the Gibbs sampler, we will
let distributions, where α are the parameters for the                  obtain samples from the joint posterior distribution
Dirichlet distibution over πj , and γ 1 . . . γ K $ γ are the          P(x1:J , α, β, γ|w1:J ). From this, other variables of in-
parameters for the distribution over each of the K rows                terest can be obtained. For example, it is desirable
of θj .                                                                to know the likely values of the topic distributions
                                                                       φ1 . . . φk . . . φK . Given known values for x1:J and β,
                                β                                      the posterior distribution over φ is simply a product
                                                                       of Dirichlet distribution, from which samples are eas-
                                φ
                                                                       ily drawn and averaged. Further details of this Gibbs
                                                                       sampler are provided in the Appendix.
                                                                       Demonstration
          wj1        wj2              wji            wjnj              Here, we demonstrate the operation of the HMTM on
                                                                       a toy problem. In this problem, we generate a data-set
                                                                       from a HMTM with known values for φ, α and γ. We
          xj1         xj2             xji            xjnj              can then train another HMTM, whose parameters are
                                                  1 ≤ i ≤ nj           unknown, with this training data-set. Using the Gibbs
                                                                       sampler, we can draw samples from the posterior over
          πj                    θj
                                               1≤j≤J
                                                                       φ, α, β and γ, and then compare these samples to the
                                                                       true parameter values that generated the training data.
          α                     γ                                         In the example we present here, we use a “vocabulary”
                                                                       of V = 25 symbols, and a set of K = 10 “topics”. As
                                                                       is common practice in demonstrations of related models,
Figure 2: A Bayesian network diagram for the Hidden                    the “topics” we use in this example can be visualized
Markov Topic model. Details are provided in the main                   using the grids shown below.
text.
Learning and Inference
From this description of the HMTM, as well as from
its Bayesian network diagram given in Figure 2, we can
see that the HMTM has four sets of parameters whose
values must be inferred from a training corpus of texts.
These are the K topic distributions, i.e. φ, the Dirichlet
parameters for the latent-variable Markov chains, i.e. α               Each grid, although shown as a 5 × 5 square, is just a
and γ, and the Dirichlet parameters from which the topic               25 dimensional array, with 5 high values (darker) and 20
distributions are drawn, i.e. β 2 . The posterior over φ,              low values (lighter). Each of these arrays corresponds to
α, β and γ given a set of J texts w1:J is                              one of the topics distributions in our toy problem, i.e.
                                                                       each one places the majority of its probability mass on
 P(φ, α, β, γ|w1:J ) ∝ P(w1:J |φ, α, γ)P(φ|β)P(α, β, γ),               a different set of 5 symbols.
                                                              (3)         The γ 1 . . . γ K $ γ parameters in the HMTM are a
where the likelihood term P(w1:J |φ, α, γ) is given by                 set of K K-dimensional Dirichlet parameters. Each γ k
Equation 2. This distribution is intractable (as is even               is an array of K non-negative real numbers. A common
the likelihood term itself), and as such it is necessary               reparameterization of Dirichlet parameters, such as γ k
to use Markov Chain Monte Carlo (MCMC) methods to                      is as smk , where s is the sum of γ k and mk is the γ k
sample from it.                                                        divided by s. For each set of K Dirichlet parameters, we
   There are different options available for MCMC sam-                 used an s = 3.0. The K arrays m1 . . . mK are depicted
pling. The method we employ is a Gibbs sampler that                    below on the left.
draws samples from the posterior over α, β, γ and x1:J .
Here, x1:J are the sequences of latent-variables for each
of the J texts, i.e. x1 , x2 . . . xJ , with xj = xj1 . . . xjnj .
This Gibbs sampler has the useful property of integrating
over φ, π 1:J and θ 1:J , which entails both computational
efficiency and faster convergence.
    2
      The β parameters can be seen as a prior over φ, but a
prior that will be inferred from the data rather than simply           From these Dirichlet parameters, we may generate arbi-
assumed.                                                               trarily many sets of state-transition parameters for a 10
                                                                   156

Figure 3: Averages of samples from the posterior distribution over the topic distributions (left) and locations of the
γ parameters (right). Shown on the top row are averages, over 10 draws, drawn after 20 iterations. Shown on the
lower row are averages, again over 10 draws, taken after 100 iterations. Compare with the true parameters shown
on the previous page.
state HMM. As an example, we show two such sets on              samples drawn from the posterior over φ and m1 . . . mk ,
the right. As is evident, these parameters retain char-         i.e the location parameters of γ 1 . . . γ k . On the top row
acteristics of the patterns found in the orginal Dirichlet      of Figure 3, we show averages of samples drawn after 20
parameters. We can see that, on average, the state tran-        iterations of the sampler. On the lower row, we show av-
sition dynamics leads a given state to map to either the        erages of drawn after 100 iterations. In both cases, these
state before it, or the state after it. For example, we can     averages are over 10 samples taken two iterations apart
see that, on average, state 2 maps to state 1 or state 3,       in time. To the left in each case, are the inferred topics.
state 3 maps to state 2 or state 4, and so on. While this       To the right are the inferred locations of the Dirichlet pa-
average dynamical behavior is simple, it is not trivial,        rameters. These inferred parameters can be compared to
and does not lead to fixed point or periodic trajectories.      the true parameters on the previous page. By doing so,
Note also that the small differences in the transition dy-      it is clear that even after 20 iterations of the sampler,
namics can lead to quite distinct dynamical behaviors in        patterns in the topic distributions have been discovered.
their respective HMMs.                                          After 100 iterations, the inferred parameters are almost
   On the basis of these φ and θ, and using an array            identical to the originals.
of K ones as the parameters α, we generated J = 50                 Although not shown, the Gibbs sampler also succes-
training “texts” as follows. For each text j, we drew a         fully draws samples from the posterior over α, β 3 and
set of initial conditions and transition parameters for a       the scale parameter s for γ. In addition, we may use
HMM from α and γ, respectively. We then iterated the            draws from the posterior to estimate, using the harmonic
HMM for nj = 100 steps, to obtain a state trajectory            mean method, the marginal likelihood of the model un-
xj1 . . . xjnj . On the basis of the value of each xji , we     der a range of different numbers of topic distributions.
chose the appropriate topic (i.e. if xji = k we chose φk )      Although, the harmonic mean method is not highly rec-
and drew an observation wji from it. The training thus          ommended, we have found that in practice it consistently
comprised a set of J symbol sequences, with each symbol         leads to an estimate of the correct number of topics.
taken a value from the set {1 . . . 25}.
   Using this as training data, we trained another HMTM
                                                                            Learning Topics from Text
whose parameters were unknown. As described earlier,            In this final section, we present some topics learned by
the Gibbs sampler draws samples from the posterior dis-         a HMTM trained on a corpus of natural language. The
tributon over x1:J , α, β and γ. From these samples,            corpus used was a sub-sample from the British National
we may also draw samples from the posterior over the
topics. In Figure 3, we graphically present some results            3
                                                                      For the case of β, we used a symmetric Dirichlet distri-
of these simulations. Show in this figure are averages of       bution.
                                                            157

                               beer        sheep         sugar      aircraft          film         say             ship
                             guinness      cattle         fruit       plane         movie         know            boat
                              alcohol      meat         butter          jet         series         talk           ferry
           HMTM Topics
                                ale      livestock       bread       airline           tv         think          vessel
                              whisky       dairy       chocolate   squadron          story         feel           ships
                              spirits       beef          milk     helicopter     television   understand         navy
                               wine         pigs         cream       fighter         soap        believe       shipping
                               cider      animal         water      hercules       movies         speak         lifeboat
                                pint        cow          lemon       airbus         drama          ask             fleet
                               lager        pig            egg       falcon        episode       explain      coastguard
                                pub         farm         fruit         air            film        want           boat
                               drink    agriculture       add       aircraft          star        think         island
           Standard Topics
                             guinness       food         fresh       flight       hollywood        like            sea
                               beer       farming       butter       plane          movie        people           ship
                             drinking      sheep       cooking      airport         screen       moment          crew
                               wine     agricultural   minutes       flying          stars       happen          ferry
                                bar        cattle         hot         pilot        director      wanted         sailing
                              alcohol    ministry        food          fly         actress       worried         yacht
                             brewery        crop        bread          jet           actor       believe       shipping
                              whisky        pigs       chicken      airline           role       exactly        board
Table 1: Examples of topics learned by a HMTM (top row) that was trained on a set of documents taken from the
BNC. On the lower row, we show topics from a standard topics model also trained on a set of documents from the
BNC.
Corpus (BNC)4 . The BNC is annotated with the struc-                      terior over the topics, which are then averaged, as de-
tural properties of a text such as sectioning and sub-                    scribed earlier. In the upper part of Table 1, we present
sectioning information. The latter type of annotation                     seven averaged topics from the HMTM simulation. For
facilitates the processing of the corpus. To extract texts                the purposes of comparison, in the lower part Table 1
from the BNC we extracted contiguous blocks that were                     we present seven averaged topics taken from a standard
labeled as high-level sections, roughly corresponding to                  topics model. This standard topics model was trained
individual articles, letters or chapters. These sections                  on a larger corpus, and is described in detail in Andrews
varied in size from tens to thousands of words, and from                  et al. (In Press). The topics in the standard model were
these we chose only those texts that were approximately                   chosen by finding the topics that are the closest matching
150-250 words in length. This length is typical of, for                   to the HMTM topics we chose.
example, a short newspaper article. Following these cri-                     The side-by-side comparison provides an appreciation
teria, we then sampled 2500 individual texts to be used                   for how the topics in a HMTM differ from the standard
for training. Of all the word types that occurred within                  model. In the HMTM, the topics are more refined in the
this subset of texts, we excluded words that occurred                     semantics, referring to more specific categories of things
less 5 times overall, and replaced their occurrence with                  or events. For example, we see that the first topic to the
a marker symbol. This restriction resulted in a total of                  left in the HMTM refers to alcholic beverages, specifi-
5182 unique words.                                                        cally those associated with a (British) pub. By contrast,
   We trained a HMTM with K = 120 topics using this                       the corresponding topic from the standard model is less
corpus. After a burn-in period of 1000 iterations, we                     specifically about beverages and refers more generally
drew 50 samples from the posterior over the latent tra-                   to things of, or relating to, pubs. In the next example,
jectories and β, with each sample being 10 iterations                     we see that the topic from the HMTM refers to farm
apart. We used these to draw samples from the pos-                        animals. By contrast, the corresponding topic from the
                                                                          standard model is less specifically about farm animals
   4
     The BNC is a 100 million word corpus of contemporary                 and more related to agriculture in general. In all of the
written and spoken English in the British Isles. According                examples shown, a similar pattern of results holds.
to its publishers, the texts that make up the written com-
ponent include “extracts from regional and national news-
papers, specialist periodicals and journals for all ages and                                   Conclusion
interests, academic books and popular fiction, published and
unpublished letters and memoranda, school and university                  The aim of this paper is to demonstrate how to extend
essays”.                                                                  the standard Topics model so as to learn more fine-
                                                                    158

grained semantic representations from the statistics of                variables, the conditional posterior over the share of the
langauge. This is done by using the sequential statisti-               probability mass between any two elements of mk is also
cal information of language. As mentioned, the sequen-                 log-concave and can be sampled by ARS. As such, the
tial order in which words occur provide vital informa-                 Gibbs sequentially samples from each latent variable xji ,
tion about the possible meaning of words. This infor-                  each scale parameter of the Dirichlet parameters given
mation is not available in the standard Topics model,                  by α, β, γ, and also from the share of probability mass
nor in most of its counterparts. In the examples shown,                between every pair of elements of each location parame-
we have seen that more refined semantic representations                ters of the Dirichlet parameters.
can be learned when sequential information is taken into
account.                                                                                      References
   This general usefulness of sequential information can               Andrews, M., Vigliocco, G., & Vinson, D. P. (In Press).
be understood by way of a, albeit contrived, simple ex-                       Integrating experiential and distributional data to
ample. Words like horse, cow, mule are likely to occur                        learn semantic representations. Psychological Re-
as subjects of verbs like eat, chew, while words like grass,                  view.
hay, grain are likely to occur as their objects. A model               Blei, D., Ng, A., & Jordan, M. (2003). Latent dirichlet
that learns topics by taking into account this sequential                     allocation. Journal of Machine Learning Research,
information may learn that words like horse, cow, and                         3, 993-1022.
mule etc., form a coherent topic. Likewise, such a model               Boyd-Graber, J., & Blei, D. (2009). Syntactic topic
may infer other topics based on words like eat, chew,                         models. In Neural information processing systems.
etc., or words like grass, hay, grain, etc. By contrast,               Firth, J. R. (1957). A synopsis of linguistic theory 1930-
the standard Topics model, based on the assumptions                           1955. In Studies in Linguistic Analysis (special vol-
that the sequential information in a text is irrelevant,                      ume of the Philological Society, Oxford) (p. 1-32).
is likely to conflate these separate topics into one single                   Oxford: Blackwell.
topic referring to, for example, farms or farming.                     Gilks, W. R., & Wild, P. (1992). Adaptive rejection
                                                                              sampling for gibbs sampling. Applied Statistics,
                           Appendix                                           41 (2), 337-348.
The Gibbs sampler for the HMTM model draws samples                     Griffiths, T., & Steyvers, M. (2002). A probabilistic
from P(x1:J , α, β, γ|w1:J ). It does so iteratively sam-                     approach to semantic representation. In Proceed-
pling from a given latent variable xji , assuming values                      ings of the 24th annual conference of the cognitive
from all other latent variables, and for α, β, γ. The                         science society.
conditional distribution over xji is given by                          Griffiths, T., & Steyvers, M. (2003). Prediction and se-
                                                                              mantic association. In S. T. S. Becker & K. Ober-
  P(xji |x−[ji] , w1:J , α, β, γ)                                             mayer (Eds.), Advances in neural information pro-
                                                             (A.1)            cessing systems 15 (pp. 11–18). Cambridge, MA:
   ∝ P(wji |xji , x−[ji] , w−[ji] , β)P(xji |x−[ji] , α, γ),
                                                                              MIT Press.
where we denote the set of latent variables excluding xji              Griffiths, T., Steyvers, M., & Tenenbaum, J. (2007).
by x−[ji] , and denote the set of observables excluding                       Topics in semantic representation. Psychological
wji by w−[ji] . Superficially, this conditional distribu-                     Review, 114, 211-244.
tion appears identical to the conditional distributions in             Harris, Z. (1954). Distributional structure. Word, 10 (2-
the Gibbs sampler for the standard Topics model, as de-                       3), 775-793.
scibed in Griffiths and Steyvers (2002, 2003); Griffiths               Landauer, T., & Dumais, S. (1997). A solutions to
et al. (2007). However, due to the non-independence in                        Plato’s problem: The Latent Semantic Analyis
the latent trajectory that results from the Markov dy-                        theory of acquistion, induction and representation
namics, the term P(xji |x−[ji] , α, γ) must be calculated                     of knowledge. Psychological Review, 104, 211-240.
as a ratio of Polya distributions, i.e.                                Lund, K., Burgess, C., & Atchley, R. A. (1995). Seman-
                                                                              tic and associative priming in high-dimensional se-
                                     P(x1:J |α, γ)                            mantic space. In Proceedings of the seventeeth an-
         P(xji |x−[ji] , α, γ) =                     .       (A.2)
                                    P(x−[ji] , α, γ)                          nual conference of the cognitive science society.
                                                                       Schütze, H. (1992). Dimensions of meaning. IEEE Com-
The Dirichlet parameters α, β, γ may also be sampled                          puter Society Press.
by Gibbs sampling. For example, each γ k is reparame-                  Wallach, H. (2006). Topic modeling: Beyond bag-of-
terized as smk (as described in the main text). Assuming                      words. In Proceedings of the 23rd international
known values for all variables, the conditional posterior                     conference on machine learning (p. 977-984).
distribution of s is log-concave and can be sampled using
Adaptive Rejection sampling (ARS), see Gilks and Wild
(1992). Likewise, assuming known values for all other
                                                                   159

