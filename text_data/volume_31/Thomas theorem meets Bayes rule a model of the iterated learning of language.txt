UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Thomas' theorem meets Bayes' rule: a model of the iterated learning of language
Permalink
https://escholarship.org/uc/item/89k9s2br
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Ferdinand, Vanessa
Zuidema, Willem
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Thomas’ theorem meets Bayes’ rule: a model of the iterated learning of language
                                              Vanessa Ferdinand (v.a.ferdinand@uva.nl)
                                                  Willem Zuidema (zuidema@uva.nl)
                       Cognitive Science Center Amsterdam & Institute for Logic, Language and Computation,
                    University of Amsterdam, Plantage Muidergracht 24, 1018 TV, Amsterdam, The Netherlands
                               Abstract                                  biases. One way to disentangle the role of the learning biases
                                                                         and the role of cultural transmission in iterated learning is to
   We develop a Bayesian Iterated Learning Model (BILM) that             implement ILMs with Bayesian rational agents, whose induc-
   models the cultural evolution of language as it is transmitted
   over generations of learners. We study the outcome of iter-           tive biases are explicitly coded. The first main BILM result
   ated learning in relation to the behavior of individual agents        of this kind was that the outcome of iterated learning (i.e. the
   (their biases) and the social structure through which they trans-     stationary distribution of the transition matrix describing tran-
   mit their behavior. BILM makes individual learning biases
   explicit and offers a direct comparison of how individual bi-         sition probabilities between all possible languages) exactly
   ases relate to the outcome of iterated learning. Most earlier         mirrors the agents’ distribution over priors. This result im-
   BILMs use simple one parent to one child (monadic) chains of          plied that the learners’ biases entirely determine the outcome
   homogeneous learners to study the outcome of iterated learn-
   ing in terms of bias manipulations. Here, we develop a BILM           of iterated learning (Griffiths and Kalish, 2005). However,
   to study two novel manipulations in social parameters: pop-           when agents with a different hypothesis selection strategy are
   ulation size and population heterogeneity, to determine more          used (i.e. they maximize as opposed to sample from the pos-
   precisely what the transmission process itself can add to the
   outcome of iterated learning. Our monadic model replicates            terior probabilities over hypotheses), the outcome of iterated
   the existing BILM results, however our manipulations show             learning amplifies the biases and does not mirror the prior
   that the outcome of iterated learning is sensitive to more fac-       exactly (Kirby et al, 2007). This is taken as evidence that
   tors than are explicitly encoded in the prior. This calls into
   question the appropriateness of assuming strong Bayesian in-          cultural transmission is adding something to the outcome of
   ference in the iterated learning framework and has important          iterated learning.
   implications for the study of language evolution in general.
                                                                            Given that ILMs are complex, dynamical systems we
                                                                         should look at the outcome of iterated learning as a product
                           Introduction                                  of the behavior of individual agents (agent properties) and the
The language that you speak is not a product of your mind                ways in which they interact (social properties). In this way,
alone. Of course, in acquiring language, you did bring to task           we can make a clear distinction between which aspects of the
a sophisticated learning apparatus able to discover intricate            outcome can be attributed to agent biases and which aspects
patterns and track frequency distributions at all levels of lin-         can be attributed to the transmission process itself. Normally,
guistic analysis. However, you learned your language from                only manipulations to the agent itself are made (in terms of
data that had been shaped by countless generations of lan-               bias values and hypothesis selection strategies) and any out-
guage learners and users before you. Without this history,               comes that do not strictly mirror the bias are attributed to un-
your personal language would have looked quite different.                specified additions of the cultural transmission process.
   In the field of language evolution, the iterated learning                In the present research, we have chosen to manipulate two
model (ILM) has emerged as a popular formalization of this               social parameters: population size and bias heterogeneity. In
cultural transmission of language over successive genera-                doing so, we find that the outcome of iterated learning no
tions. Through a series of models, it has become clear that              longer mirrors the prior and is sensitive to more biases in the
the iteration of learning may have profound influences on                Bayesian agent than are explicitly encoded in the prior. If
the resulting languages. Many of the defining features of                the prior can no longer be seen as encoding the whole of an
natural language have been shown to be possible results of               agent’s bias, this calls into question the appropriateness of the
iteration, including compositional semantics and recursive-              current BILM framework as a remedy for explicitly linking
ness (Kirby, 2002) and the bidirectionality of the sign (Smith,          specific properties of the agents to the outcome of iterated
2002). Moreover, these models have shown that the learnabil-             learning.
ity of language in an important sense comes for free: seem-
ingly arbitrary choices of current learners are correct pre-                                  Model description
cisely because they are the same choices as those made by the            In this section we outline the components of a Bayesian
previous generations (Deacon, 1997; Zuidema 2003). This is               ILM and describe how they are implemented in our model.
an instance of Thomas’ theorem: "If men define situations as             The components are the standard ones from any Bayesian
real, they are real in their consequences" (Thomas & Thomas,             inference model (hypothesis space, data, prior, posterior)
1928).                                                                   combined with those from iterated learning (the learners or
   However, learning in the ILM tradition is operationalized             ’agents’, data production, population structure). The imple-
with a variety of adhoc learning algorithms and many ques-               mentation and analysis were all carried out in Matlab (for de-
tions remain about the role and appropriateness of the built-in          tails, see Ferdinand & Zuidema, 2008).
                                                                     1786

                                                   prior   t    prior    t+1      prior   t+2
                                                    h      d     h         d       h        d
                                                    h            h                 h
                                                   prior        prior             prior
                                                                                                   Figure 3: Hypothesis structure and priors
                                                                                                   affect stationary distribution of monadic
Figure 1: Graph of hypotheses [.6 .3          Figure 2: Bayesian iterated learning,                maximizers. b=1; hypotheses [α (1 −
.1; .2 .6 .2; .1 .3 .6] and example           in the monadic (black) and polyadic                  α)/2) (1 − α)/2); (1 − x)/2 x (1 − x)/2;
prior vector [.7 .2 .1]. Each hypoth-         (+green/grey) condition. When all pri-               (1 − α)/2) (1 − α)/2) α], with x on hori-
esis’ shape is entirely determined by         ors are equal, the chain is called homo-             zontal axis and α = .33 in curve (a), .4 in
the likelihoods it assigns to the data.       geneous, otherwise it is heterogeneous.              (b), .6 in (c) and .8 in (d).
Hypotheses: agents are assumed to consider a fixed set of                      Agents perform inference by combining their prior beliefs
   hypotheses about the state of the world. Each of these hy-               and observations of data using Bayes’ rule:
   potheses assigns different likelihoods to each of a fixed set
   of possible observations (they are thus probability distri-                                                  P(d|h)P(h)
                                                                                                    P(h|d) =                                   (1)
   butions over data, P(d|h)). These hypotheses could repre-                                                       P(d)
   sent, for instance, different languages that generate a set of           where P(h|d) denotes the posterior probability that a hypoth-
   utterances, or different functions that describe a set of data           esis could have generated the data in question and P(d) =
   points. The exact nature of the hypotheses is left unspec-
                                                                            ∑h∈H P(h)P(d|h) is the probability of the data averaged over
   ified, but the basic properties of the model should be gen-              all hypotheses.
   eralizable to a variety of systems where information is cul-
   turally transmitted (including not only language, but also,              Hypothesis choice: Once the posterior probabilities are cal-
   e.g., music and bird song). In this paper, the hypotheses are               culated, we still need a strategy to choose one particu-
   set at the beginning of each simulation and are the same for                lar hypothesis h, from which to generate the data. We
   all agents. In our examples, we will use a set of hypotheses                consider the same two strategies as in much earlier litera-
   of size three: h ∈ H = {h1 , h2 , h3 }.                                     ture: (i) the maximizer simply chooses the hypothesis with
                                                                               the maximum posterior probability (MAP) or, if there are
Data: The observations that an agent can make about the                        multiple maximums, a random one from the set of MAP-
   state of the world will be referred to as data points. Data                 hypotheses; (ii) the sampler chooses one hypothesis ran-
   points are from a fixed set of possible values and in                       domly, but weighted by the posterior probabilities.
   our model we restrict the size of this set to three: D =
   {d1 , d2 , d3 }. In linguistic terms, a data point can be inter-         Data Production: Once a hypothesis is chosen, the next
   preted as any piece of evidence (a “trigger”) that the target               step is to generate new data using that hypothesis. For
   language has a particular property. Different hypotheses                    instance, assuming the agent has chosen h1 , each data
   assign different likelihoods to different data points. Mul-                 point in the output string will be randomly generated, but
   tiple data points may be organized into a string, where the                 weighted according to the likelihood of each data point un-
   likelihood of the string is the product of the likelihoods of               der h1 . The number of data points in this string defines
   the points. Data in our model is any such string of length                  the “transmission bottleneck” b. A characteristic string
   b ≥ 1: d ∈ Db .                                                             of size b = 10 under hypothesis h1 of figure 1 would be
                                                                               < d1 d1 d1 d2 d1 d2 d2 d1 d3 d1 >.
Prior probability: The prior is a probability distribution
   over hypotheses, defining the probability P(h) assigned to               Population structure: Agents are organized into discrete
   each hypothesis before any data is seen. Thus, the prior                    generations of one or more individuals (figure 2). If
   models the inductive bias of the learner. An example short-                 the number of agents per generation is exactly 1, the
   hand we will use to note the prior is [.7 .2 .1], where the                 model resembles previous ILMs; we will call this condi-
   prior probabilities for h1 , h2 and h3 are .7, .2 and .1, re-               tion monadic. We will also consider larger populations, a
   spectively1 .                                                               condition labeled polyadic. Because each generation only
                                                                               learns from the previous, the model can – regardless of
    1 Note that in Bayesian modelling the choice of priors is typi-
cally constrained by a set of principles that assure the outcome is         very relevant for some of our results, as one reviewer notes, but it is
not trivially steered by the modeller (e.g., priors must be uninfor-        consistent with our view that it is the reality of human biology that
mative) and that calculations remain feasible (e.g., priors must be         ultimately determines which priors and hypotheses are appropriate
conjugate to the likelihood function). In our model, we allow any           in linguistic modelling, and not the mathematical convenience for
choice of prior over any set of hypotheses; this is ‘unorthodox’ yet        the modeller.
                                                                      1787

   the population size – be characterized as a Markov chain2 ,              theory of language universals and variation: it gives the rela-
   where the population at time t (after learning) is charac-               tive frequencies with which we should see particular language
   terized as a vector of hypotheses (~ht ), and for each pair              variants, when very many (N → ∞) independent cultural evo-
   of hypotheses < ht , ht+1 > the probability of transitioning             lution chains have run for a very long time (t → ∞).
   from one to the other is given by ∑d P(d|ht ) × P(ht+1 |d).                  In an experimental run, the relative frequency of all chosen
   Cultural transmission (iteration) is modeled by using each               hypotheses are also entirely determined by the transition ma-
generation’s output data string as the next generation’s input              trix, but because a run contains a finite number of transitions,
data string. All agents in one generation produce the same                  it represents one actual trajectory of transitions, from a larger
number of data samples, which are all concatenated into the                 set of probable trajectories under that matrix. However, when
output data string for that generation. The likelihood of a                 a large number of transitions can be recorded in a simulation
data string is invariant to the order of the data samples it                (by setting the number of generations sufficiently high), then
contains. Each agent has no way of knowing the number of                    a tally of the actual hypotheses chosen by the agents over the
agents which produced the data string or which data came                    course of the simulation closely approximates the analytical
from which agent. Additionally, each generation has an iden-                stationary distribution. This is the normalized hypothesis his-
tical composition of agents as the generation before it. If a               tory (HH); it is a reliable, experimental approximation of the
population is heterogeneous, then the same set of heteroge-                 stationary distribution and is reported in most of our results.
neous agents occur in each generation.
                                                                            Monadic iterated learning
                               Results
We will study this model numerically while varying the prior,               Griffiths & Kalish (2005) showed, both analytically and nu-
hypothesis structure, bottleneck size, hypothesis choice strat-             merically, that the stationary distribution of iterated learning
egy, population size, and population heterogeneity in terms of              of monadic (i.e., a single agent per generation) samplers mir-
different priors per agent. In this section we will first briefly           rors the prior, independently from hypothesis structure or bot-
describe how we monitor the behavior of the model. We then                  tleneck size. In our numerical model we easily replicate this
show our model can replicate aspects of previous Bayesian                   result; for all choices of priors, hypotheses and bottlenecks
ILMs. We will then address new findings for multi-agent pop-                we find that the (approximation) of the stationary distribution
ulations with heterogeneous and homogeneous biases.                         (approximately) matches the prior (table 1).
Monitoring model behaviour
                                                                                                analytical            numerical
We are looking for features that are invariant as well as
                                                                                             h1    h2      h3   h1        h2        h3
changes in the dynamics of the model that can be causally
                                                                                     S1      .7     .2     .1 .6946     .2040     .1014
attributed to specific changes in parameter settings. A con-
                                                                                     S2      .7     .2     .1 .7095     .1939     .0966
crete representation of a “dynamical fingerprint” can be ob-
                                                                                     M1     .33 .33 .33       .3310     .3327     .3363
tained by constructing a transition matrix Q for each model.
                                                                                     M2     .83 .15 .03       .8167     .1553     .0280
This matrix gives the probabilities that any of the hypotheses
will lead to itself or any other hypothesis in the next genera-             Table 1: Analytically and numerically determined stationary
tion. In linguistic terms, the transition matrix is the idealized           distributions for 4 conditions of the monadic BILM. S marks
equivalent of a theory of (generational) language change: it                sampler, M maximizer, and the 1 and 2 give bottleneck sizes.
gives the probability that a language in a particular state at              Prior = [.7 .2 .1], hypotheses as in figure 1; 10000 iterations.
generation t will change into any of a set of possible states at
generation t + 1 (cf. Niyogi, 2002).
   If an ILM simulation could be run for an infinite amount                     One interpretation of this result is that, in the end, the fea-
of time, the relative frequency of each chosen hypothesis                   tures of the languages of the world are entirely determined
would settle into a particular distribution that is determined              by the biases of the learner – but we will see that this inter-
entirely by these transition probabilities. This distribution is            pretation is incorrect. (Kirby, Dowman, & Griffiths, 2007)
known as the stationary distribution and serves as an ideal-                emphasized that in the maximizer condition, iterated learn-
ized shorthand for the “outcome of iterated learning”. The                  ing will not converge to the prior. They describe the maxi-
stationary distribution is proportional to the first eigenvector            mizer stationary distributions as amplifying the structure of
of the transition matrix (Griffiths & Kalish, 2005). In some                the prior, although they are careful to note that the exact dis-
cases, we can therefore easily determine the stationary dis-                tribution will depend on the hypothesis structure. We find that
tribution analytically. In linguistic terms, the stationary dis-            it is in fact easy to choose parameters such that the stationary
tribution is the idealized equivalent of linguistic typology, a             distribution is entirely determined by the hypothesis structure
                                                                            and prior play no role, as shown in the third line of table 1.
    2 (Note that for large populations and large sets of hypotheses the
                                                                            In figure 3 we show an example of the complex interaction
transition matrix is of course too big to calculate exhaustively, but
identifying a system as a Markov chain can help us decide on the            between priors and hypothesis structure under the maximizer
right approximations to study such systems in simulations.                  condition.
                                                                        1788

Polyadic iterated learning                                             hypotheses, these probabilities do not conform to the likeli-
                                                                       hoods as defined by any of their hypotheses. The result is, for
In our model, each agent in a polyadic run sees the same data
                                                                       a multi-agent population of samplers, the stationary distribu-
string, separately calculates its posterior values, chooses its
                                                                       tion no longer mirrors the prior (figure 4).
own hypothesis, and generates its own data. The data from
all agents of the same generation are then concatenated into
one unified data string, which is given to the next generation
as their input. When the population size is set to any number
n and the number of data samples is set to m, the bottleneck
size is b = n × m.
   The polyadic and monadic configurations thus differ in
one respect: the data string that is passed between genera-
tions is not stochastically generated from one single hypoth-
esis, but from multiple. This has different consequences for
the maximizer and the sampler models.We find that polyadic             Figure 4: The maximizer model’s stationary distribution is in-
maximizers behave almost exactly as monadic maximizers.                variant to population size. For samplers, population size does
Polyadic samplers, however, show markedly different behav-             affect the dynamics and the stationary distribution no longer
ior. In particular, samplers in this condition do not converge         mirrors the prior. Stationary distributions for populations 1
to the prior. This seems to be in contradiction with the math-         and 2, for maximizer and sampler models with: prior [.7 .2
ematical result in Kalish et al. (2007), who show that their           .1], hypotheses [.8 .1 .1; .1 .8 .1; .1 .1 .8], 10,000 generations.
single-agent results can be generalized to multi-agent popula-
tions, where the stationary distribution will continue to mirror          Hence, the mathematical proof of (Griffiths & Kalish,
the prior.                                                             2005) requires, in practice, that each sampler is given a new
   It is worthwhile working out the reasons for this difference.       set of hypotheses, for each corresponding population size,
First consider a homogeneous, polyadic maximizer model.                where each hypothesis represents the combined likelihood set
The behavior of all agents in the population is identical: be-         for each possible combination of hypotheses that the agents of
cause all agents receive the same data string and have identi-         the population may have when outputting into the data string.
cal priors and hypotheses, the posterior of all agents will be         This set is different when learners learn from 1, 2, 3, . . ., 100
the same. Maximizers will all choose the same hypothesis,              individuals, but the proof requires a perfect hypothesis space.
because this choice is based on the maximum value of their                The stationary distribution of polyadic samplers are a func-
identical posteriors. The only exception is when there are             tion of priors, hypotheses and population size. Figure 5 (left)
multiple maximum values in their posterior. In this case, they         shows that the stationary distribution mirrors the prior less
each choose one of the maximum value hypotheses randomly,              and less as the hypothesis structure becomes strongly peaked
with equal weight. This situation generally only arises when           and the prior more biased. However, for a combination of
there is no bias in the prior values (to help diversify the poste-     relatively flat hypotheses and weakly biased priors, the sta-
rior values). Aside from this exception, multiple maximizers           tionary distribution still mirrors the prior. Furthermore, in-
producing m samples, is equivalent to a single maximizer pro-          creasing the population size systematically amplifies the ef-
ducing n × m samples. Therefore, maximizer dynamics due                fect of the likelihoods on the sampler’s stationary distribution
to population size are identical to the dynamics due to a larger       (figure 5, right). The figure also shows that the stationary
bottleneck.                                                            distribution reflects hypotheses structure in the absence of a
   For a homogeneous, polyadic sampler model, the dynam-               prior bias.
ics are very different. Because samplers choose their hy-
potheses weighted by their posteriors, a homogeneous pop-              Heterogeneous iterated learning
ulation will not choose the same hypotheses. Therefore, the            Finally, we implemented a heterogeneous ILM by taking a
actual data the next generation receives, may come from indi-          polyadic model and assigning different prior vectors to each
viduals with different hypotheses. This implies that the data          of the agents (figure 2). This model, therefore, is the most
is a sample from the product of the distributions defined by           complex of all models constructed. The main result is that
the hypotheses of those individuals. The actual probability            heterogeneous agents’ hypotheses choices converge as they
distribution over possible data strings is therefore not among         are allowed to share more and more data, despite having fixed
the set of hypotheses entertained by our learners!                     and different priors from each other. In the case of hetero-
   This has interesting implications concerning the Bayesian           geneous samplers, we find that the stationary distribution is
rationality of the agents. In general, when a string of data is        simply the average of the stationary distributions of the cor-
generated from a set of likelihoods which learners are not ex-         responding homogeneous sampler runs. In the case of maxi-
plicitly given, then they are not longer perfect Bayesian rea-         mizers, however, the heterogeneous stationary distribution is
soners. This is exactly the case with a multi-population of            not a simple average, and depends on all hypothesis structures
samplers. When a data string is generated from 2 different             and priors (details in Ferdinand & Zuidema, 2008).
                                                                   1789

                                                                              imizer parameter sets to use for iterated learning simulations.
                                                                                 Our main result, however, was obtained from a manipu-
                                                                              lation in population size. By just increasing the population
                                                                              size to 2, the sampler model’s stationary distribution does not
                                                                              strictly mirror the prior. One response to this finding could
                                                                              be to claim that we have made a mistake by not giving our
                                                                              agents the right set of hypotheses. Indeed, in a Bayesian ra-
                                                                              tional analysis it is the task of the modeller to choose hypoth-
                                                                              esis structure and prior carefully such that it objectively rep-
                                                                              resents the uncertainty the learner faces. The learner doesn’t
                                                                              have to actually use any of the Bayesian apparatus; it might
Figure 5: Population size amplifies sampler sensitivity to hy-                rely on completely different heuristics that have somehow
potheses structure. Hypotheses: a [.8 .1 .1; .1 .8 .1; .1 .1 .8],             (e.g., by natural selection) been optimized to approximate
b [.4 .3 .3; .1 .8 .1; .3 .3 .4], c [.8 .1 .1; .3 .4 .3; .1 .1 .8]; prior     the Bayesian rational solution (i.e., it is a normative model).
= unbiased. Population sizes n=1 to 5.                                        However, if this line of attack is chosen, it should apply to all
                                                                              cases where iterated learning does not converge to the prior
                                                                              (and we listed many). In the stationary distribution, the objec-
                            Discussion                                        tive prior probability for learners is the stationary distribution.
                                                                              A Bayesian rational analysis of iterated learning that predicts
This Bayesian ILM both replicated the general properties of
                                                                              non-convergence to the prior is thus inconsistent.
previous existing Bayesian ILMs and provided new results re-
garding multi-agent populations and bias heterogeneity. The                      The alternative response is to acknowledge that the prior
replications are that a monadic sampler’s stationary distri-                  probabilities and the hypothesis structure (i.e., likelihoods)
bution always mirrors the prior bias of the agent (Griffiths                  together instantiate the subjective biases of the individual
& Kalish, 2005), and a maximizer’s stationary distribution                    agents. The Bayesian inference in that case should be in-
is determined both by the prior and the hypothesis structure                  terpreted as a descriptive model, and the components of the
(Kalish et al., 2007). Also, for a range of parameters, the prior             Bayesian apparatus must somehow correspond to cognitively
has no effect on the maximizer stationary distribution (Smith                 real processes. We have seen that convergence to the prior
& Kirby, 2008), but above this threshold, iterated learning                   only occurs in the exceptional case that (i) learners have per-
amplifies this bias (Kirby et al., 2007). Additionally, a strong              fect knowledge of the possible sources of the data they are
bottleneck effect was observed, with the general effect of in-                learning from, (ii) they carry out the Bayesian probability
creasing transmission fidelity of both the sampler and maxi-                  calculation perfectly and (iii) they then sample from the pos-
mizer models (Kalish et al., 2007). All of these replications                 terior. In all other cases – which include human language
attest to this particular implementation as a valid Bayesian                  learning, we suggest – predicting stationary distributions is
iterated learning model.                                                      in fact a difficult task that depends on many variables. The
                                                                              results in this paper form a further contribution to developing
   Throughout the replication work, new insights into the role
                                                                              the computational tools for that task.
of data likelihoods for both the maximizer and sampler were
obtained. The focus of all previous research with Bayesian                       Hence, Bayesian Iterated Learning does not actually give
ILMs is the on the prior and how its manipulations affect the                 us what it was intended to: an explicit and complete quantifi-
stationary distribution. Kalish et al. (2007) manipulated the                 cation of an agent’s bias. When we look at the cultural trans-
degree of hypotheses overlap, as well as noise level, but it ap-              mission system as a sum of the agents’ behavior and the way
pears their hypothesis structures were all symmetrical. Smith                 they interact (the social structure) we see the prior does not
& Kirby (2008) demonstrate that the maximizer strategy of                     quantify the whole agent’s bias. We see that not only does the
hypothesis choice is evolutionarily stable over that of sam-                  hypothesis choice strategy affects agent behavior and there-
plers. However, the maximizer parameters for which this re-                   fore the outcome of iterated learning, but hypothesis structure
sult was proven, it seems, were again derived from a symmet-                  does the same (since we show an effect of different hypoth-
rical hypothesis structure and for the range of priors which are              esis structures on the stationary distribution). If we attribute
unaffected by bias strength. The result is consistent behavior                these components to a cognitive agent, they should all fall un-
of the Smith & Kirby’s maximizer model over the bias values                   der the agent’s bias (innate or learned) if the bias is everything
they selected. Our results show that this is a subset of max-                 that the learner brings to the task - so everything besides the
imizer behavior and that unstable behavior is easily obtained                 data itself.
for the right relationship between hypotheses and priors. So,                    Moreover, the particular dynamics which previously
perhaps maximizer would not be the evolutionary stable strat-                 thought to differentiate the sampler and maximizer models,
egy in all cases, and a certain range of maximizer parameters                 may not be as clear cut as they seemed and non-convergence
is evolutionary stable over other maximizer parameter sets.                   to the prior cannot be taken as direct evidence against the
Knowledge of this kind would help guide the choice of max-                    sampling strategy or support for the maximizer strategy.
                                                                          1790

These results are bad news when trying to interpet iterated           models, these caveats are often overlooked. In particular, a
learning experiments with human and animal subjects, aimed            Bayesian rational analysis is not applicable here. In such an
at connecting their learning biases to the outcomes of iteration      analysis, we (i) take the learning problem as fixed, and (ii)
(Kalish, Griffiths, & Lewandowsky, 2007; Kirby, Cornish, &            calculate the ’rational’ strategy that would allow children to
Smith, 2008)                                                          solve it in an optimal way, and (iii) assume that the actual
                                                                      learning strategy approximates the rational one. The latter
                            monadic     polyadic    heterogen.        (iii) is a very strong assumption in general, but particularily
  manipulation              S    M      S     M     S      M          problematic in iterated learning where (i) is violated.
  prior (b is small)        +     -     -      -     -       -           A consistent interpretation of Bayesian Iterated Learning
  prior (b is large)        +     +     +     +     +       +         must therefore take the BI as an accurate description of in-
  hypothesis structure      -     +     +     +     +       +         dividual learning. The ILM takes (i) the learning strategy as
  population size                       +      -    +       +         fixed, and studies (ii) how the learning problem – the lan-
  homo/heterogeneity                                 -      +         guage – changes over generations, and, while it becomes bet-
                                                                      ter fitted to the learning strategies, (iii) gives rise to language
Table 2: Summary of manipulations and effects. S and M                universals and variation. However, under this interpretation,
mark samplers and maximizers. A ‘+’ indicates that the (av-           Bayesian rationality cannot be assumed and convergence to
erage) stationary distribution will change when the given ma-         the prior is rather the exception than the rule. This leaves the
nipulation is applied.                                                original linguistic challenge – how are language universals
                                                                      and learner biases connected? – still unsolved, although we
   However, on a more positive note, we find that the station-        have progressed yet another step in developing the theoretical
ary distributions in the many different conditions – monadic          and experimental paradigms for addressing that question and
or polyadic, samplers or maximizers, homogeneous or het-              established that cultural transmission really adds something
erogeneous – are selectively responsive to particular manip-          to the explanation.
ulations of the iterated learning regime. In table 2 we give a                                     References
summary of our results; the table indicates, for instance, that
                                                                       Deacon, T. (1997). Symbolic species, the co-evolution of lan-
if a population is assumed to be homogeneous, then change
                                                                          guage and the human brain. The Penguin Press.
in population size is the relevant manipulation to distinguish         Ferdinand, V., & Zuidema, W. (2008). Language adapting to the
between maximizers and samplers. Similarly, in monadic it-                brain: a study of a Bayesian iterated learning model. (Tech.
erated learning, a change in hypothesis structure should only             Rep. No. PP-2008-54). University of Amsterdam.
result in a different stationary distribution when learners are        Griffiths, T. L., & Kalish, M. L. (2005). A Bayesian view of
maximizers.                                                               language evolution by iterated learning. In Proc. of the 27th
                                                                          annual conference of the cognitive science society.
                         Conclusions                                   Kalish, M., Griffiths, T., & Lewandowsky, S. (2007). Iterated
Children learn their native language efficiently and accurately           learning: Intergenerational knowledge transmission reveals in-
from very noisy data provided by earlier generations. Under-              ductive biases. Psych. Bulletin and Review, 14(2), 288.
standing how they accomplish this task, and why it is so dif-          Kirby, S. (2002). Learning, bottlenecks and the evolution of
                                                                          recursive syntax. In T. Briscoe (Ed.), Linguistic evolution
ficult to mimick their learning in computer models, is a key
                                                                          through language acquisition: formal and computational mod-
challenge for the cognitive sciences. In recent years, two ap-
                                                                          els. Cambridge University Press.
proaches for solving bits of the puzzle – Bayesian inference           Kirby, S., Cornish, H., & Smith, K. (2008). Cumulative cul-
and iterated learning – have yielded many new insights. In a              tural evolution in the laboratory: An experimental approach to
series of papers, Griffiths, Kalish and co-workers have com-              the origins of structure in human language. PNAS, 105(31),
bined the strengths of Bayesian modelling and iterated learn-             10681-10686.
ing (Kalish et al., 2007; Kirby et al., 2007), and developed a         Kirby, S., Dowman, M., & Griffiths, T. L.(2007). Innateness and
mathematical framework for understanding the dynamics of                  culture in the evolution of language. PNAS, 104(12), 5241-
cultural transmission and the roles of learning and innateness            5245.
in shaping the languages we see today. The best known result           Niyogi, P. (2002). Theories of cultural evolution and their ap-
from this work so far, is that “iterated learning converges to            plications to language change. In T. Briscoe (Ed.), Linguistic
the prior” (Griffiths & Kalish, 2005).                                    evolution through language acquisition: formal and computa-
   In this paper, we have extended this work by investigat-               tional models. Cambridge University Press.
ing the outcomes of iterated learning under a number of ma-            Smith, K. (2002). The cultural evolution of communication in a
nipulations, including an increase in population size and het-            population of neural networks. Connection Sci., 14(1), 65-84.
                                                                       Thomas, W., & Thomas, D. (1928). The child in america: Be-
erogeneity of biases. However, by detailing several apparent
                                                                          havior problems and programs. New York: Knopf.
counterexamples to the “convergence to the prior” results, our
                                                                       Zuidema, W. (2003). How the poverty of the stimulus solves the
results also point to some caveats in the interpretation of those         poverty of the stimulus. In S. Becker, S. Thrun, & K. Ober-
earlier results and of Bayesian models of cognition more gen-             mayer (Eds.), Advances in neural information processing sys-
erally. Perhaps due to the mathematical sophistication of such            tems 15 (proceedings of nips’02) (p. 51-58). MIT Press.
                                                                  1791

