UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Extending Trust: Coupled Systems, Trust and the Extended Mind.
Permalink
https://escholarship.org/uc/item/7tm63055
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Author
Leblanc, Neal
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Extending Trust: Coupled Systems, Trust and the Extended Mind
                                          Neal R.T. Leblanc (nleblanc@silverblaze.net)
                              Institute of Cognitive Science, Carleton University, 1125 Colonel By Drive
                                                       Ottawa, ON, K1S 5B6 Canada
                               Abstract                                  complex and simple, across the spectra of nature. Many
   In this paper, I attempt to to examine the concept of reliability
                                                                         insect species (including ants and termites) employ
   in Extended Cognition, using frameworks and data from                 pheromone trails to allow them to easily return to a food
   social and evolutionary psychology to examine two of the              source or to warn against danger, such as predators
   criteria: transparency and endorsement. Using this framework,         (Camazine et al., 2001). Higher order animals frequently
   I will argue that the seemingly contradictory experimental            use scents or visual aids to mark trails within their
   results in Extended Cognition research are the result of              territories, as well as to lead them back to caches of food
   ignoring the differences between types of cognitive artefacts
                                                                         they have made (Sterelny, 2004). While these examples
   (active vs. passive) and the higher levels of trust required for
   active artefacts to be considered reliable as a result of our         many not be as elegant or representationally rich as
   ascribing them agency.                                                humanity’s written words, they serve the same purpose.
                                                                         They allow for the offloading information from the
   Keywords: Extended Cognition; Epistemic Structures; Trust;
                                                                         organism into the environment. Thus, as Dennett (1996)
   Distributed Cognition; Agency.
                                                                         says: “This widespread practice of off-loading releases us
                                                                         from the limitations of our animal brains.”
                          Introduction
                                                                            It is important to note, however, that despite the
In their seminal paper “The Extended Mind,” Clark and                    complexities of language and numerals, even the epistemic
Chalmers (1998) put forward what appears to be a                         structures used by humans are often quite simple. In fact,
somewhat radical claim: that cognition is not bound within               these simpler epistemic structures are “everywhere” in
the confines of the skin and skull. They argue that making               human life (Kirsh, 2006). Many of these structures simply
use of cognitive technologies as part of the cognitive                   serve to ease our memory burden, as when we keep our keys
process produces a powerful, two-way interaction between                 near the door or put something that needs to be mailed
the human and the artefact. This interaction results in a                under our keys. However, we also alter our environment in
coupled system, such that “all components play an active                 order to convert complex tasks into simpler ones. Examples
and causal role, and they jointly govern behaviour” (1998).              of this are legion, ranging from the simple act of marking a
As a result of the complex and non-linear interaction, the               trail to simplify later navigation, to the organization of
performance and ability of the system as a whole is greater              important notes into a filing system, to the complex
than and cannot simply be explained as the simple sum of                 behaviour of skilled bartenders who use both the sequence
the capabilities of its components. Further, removing any                and shape of bar glasses in order to optimize their
component of the coupled system (be it the tool or a neural              performance (Clark, 2001a). Kirsh & Magilo (1994) call
cluster) will cause an overall reduction in the system’s                 such environment-altering behaviour epistemic actions.
competence. Thus, rather than arbitrarily using the skin as a            Using a simple Tetris-player task, they demonstrate not only
barrier to determine what is part of the cognitive system,               that people perform such actions (despite being literally
they argue that reliability should be the salient                        counter-productive in terms of purely pragmatic game-play
discriminatory characteristic for what is part of the cognitive          efficiency), but that the number of such actions taken was
system and what is not. They argue that reliability consists             strongly predictive of task performance. In a follow-up
of three criteria: availability, transparency (automaticity of           longitudinal study (Magilo & Kirsh, 1996), it was shown
use) and endorsement of the artefact and its content (trust).            that the number of such epistemic actions (and associated
   The purpose of this talk is to examine the concept of                 backtracking) increased with the skill level of individual
reliability in Extended Cognition, using frameworks and                  participants, indicating that it was an effective learned
data from social and evolutionary psychology to examine                  strategy. Thus, as Clark (1997) put it, “We use intelligence
the individual criteria (save availability, which is                     to structure our environment so we can succeed with less
remarkably straightforward). Building on this discussion, I              intelligence. Our brains make the world smart so we can be
will attempt to reconcile seemingly contradictory                        dumb in peace!” (p. 180).
experimental results in Extended Cognition research.                        After more than a decade of study, Kirsh (2006) notes that
                                                                         such structures and actions are generated so simply and
                 Simple Cognitive Artefacts                              automatically that they often go unnoticed by both
The simplest and most common type of cognitive artefact is               researchers and the people making use of them. As a result,
the epistemic structure (or artefact): a construct made in the           he suggests that perhaps the only way to study them is to
environment which serves to hold information. The ability                record a person’s behaviour, and then perform an
to create such structures has evolved in many species, both              ethnographic analysis after the fact.
                                                                     703

                                                                   could, in fact, drive the generation of additional structures,
Cognitive Artefacts in a Shared Environment                        essentially leading to bootstrapping.
Sterelny (2004) argues that Clark (and, by extension, other
proponents of extended cognition) has made a critical error        Transparency and the Costs of Coupling
in his picture of the extended mind and of epistemic               Despite the fact that Sterelny’s hypothesized expensive
artefacts. Specifically, Clark focusses only on tools being        social guards do not appear to be present, the use of
used by a single agent, whereas offloaded epistemic                cognitive artefacts is not completely without cost. The
structures exist in the shared environment and are often           communication link between agent and artefact is itself an
themselves shared, and are thus subject to interference.           information processing task which involves the encoding
Sterelny provides a detailed evolutionary account of the use       and decoding of information and the activation of the
of tools and epistemic artefacts, which stresses the               perceptual system, at the very least. The act of activating the
importance of the evolution and use of social guards (tricks       coupling link, however, appears to be nearly automatic:
which we employ in order to protect and validate the data in       “Biological brains ... are by nature open-ended controllers.
the environment and to detect cheating by members of our           To deal fluently with bodily change and growth, they have
social group), especially in light of evolutionary pressure to     developed ways of computing, pretty much on a moment-to-
get a free ride by making use of the epistemic structures of       moment basis, what resources are readily available and
others (or, for that matter, manipulating the structures of        under direct control” (Clark, 2005).
competitors or prey). He contrasts these with purely                  The decision to couple or not is determined by a quick
internal resources that are not exposed to outside                 cost-benefit analysis of the perceived utility of the artefact
manipulation, and thus do not need to be vetted. Sterelny          against the cost of its use, evaluated on a case-by-case basis
believes that cheater detection is “a problem whose                (Lee & Moray, 1992; 1994). This analysis, however, seems
informational load is both heavy and unpredictable” (2004),        to be unbiassed in its selection of which resources to apply
and therefore argues that, as a result, we have a tension          to a given problem, be they external or internal. Gray et al.
between two of the criteria of reliability: transparency and       (2004; 2006) demonstrated this experimentally by having
endorsement. The deployment of social guards when                  subjects perform a task with the option of using an
dealing with external resources generates high demands on          automated assistant during a simple cognitive task:
our cognitive economy, increasing attention and processing,        programming a simulated VCR. They conclude that the
thus endangering the automatic endorsement which is                “control system is indifferent to the information source”
required for an external resource to count as part of the          (2006). What is important is the cost of using the aid, which
mind. Thus, in order to endorse the content of something,          they conclude is simply a function of reaction time, at least
its use is no longer automatic. Sterelny takes this even           for this non-critical task. These data seem intuitive, if one
further, arguing that the cognitive costs of coupling are          considers the task of adding two single-digit numbers. In
higher than the benefits that would be gained. Sterelny            such a case, the perceived utility of the calculator is so small
(2005) does, however, allow that some social guards may            that even if one is close at hand, it is only rarely used -
themselves be offloaded into the environment (such as our          whereas people will expend large amounts of effort and
ability to recognize our own handwriting).                         energy to find a calculator when faced with more
   In response, Parsell (2006) argues that Sterelny is likely      complicated mathematical tasks.
overestimating the cost of the use of social guards. First,           While activating a coupling link appears to be an
Parsell demonstrates that a simple connectionist network           automatic task, building that link initially is itself a learned
can be created which performs cheater-detection without            behaviour. There is a cost in time and cognitive resources
requiring any additional modules, thus showing that the            that must be paid in order to integrate a new and novel
processing costs of some types of cheater-detection may be         artefact into our cognitive systems (Karwowski, 2000).
trivial. Furthermore, following Sterelny’s admission that          Furthermore, the cost of integration is not fixed, but
the social guard task may itself be partially offloaded,           depends on the complexity of the artefact. This is the basis
Parsell discusses the use of passwords in modern                   of Karwowski's Complexity-Incompatibility Principle: “As
technology, and perhaps more importantly, makes a case             the artifact-human [sic] system complexity increases, the
that the perception of continued possession of an artefact         compatibility between system elements, expressed through
creates a (possibly misplaced) strong endorsement of its           their ergonomic interactions at all levels, decreases, leading
contents, seemingly bypassing or negating the need for             to greater ergonomic entropy of the system.” (2000) Thus,
social guards.                                                     he argues that special care must be taken in the design of
   Chandrasekharan & Stewart (2007) use an evolutionary            artefacts in order to assure compatibility with humans.
computer model to demonstrate that strategies for use of              Sutton (2006) presents a similar view, arguing that much
epistemic structures can occur as a result of evolutionary         of modern human cognition is a result of what he refers to
pressures, at least in synthetic agents. Further, they             as the “soft assembly” of transient and repeatable systems
demonstrate that the use of epistemic structures not only          involving both internal and external representations and
lowers cognitive load (countering Sterelny's concerns about        resources. As a result, our neural resources come to be
the cost being too high), but postulate that this lowering         “expressly tailored to accommodate and exploit the
                                                               704

additional representational and computations potentials           Surprisingly, even when told explicitly that the automated
introduced” (2006) as we integrate those devices which we         system made less than half as many errors as they did,
find to be useful. This is reinforced by research which           81.25% of subjects chose to use a selection of their own
shows that our plastic minds incorporate tools into the body      responses rather than those of the automated responses. By
map, and become accustomed to and anticipate the feedback         comparison, when told that the automated responses were
these tools provide (Hawkins, 2004). Thus, true coupling          actually those of a human expert, 50% of subjects chose to
occurs when we go beyond the “soft assembly” by                   use the judgement of the aid. Thus, they conclude that
integrating an artefact which we have found to be highly          people interact with machines somewhat differently than
reliable and either highly durable or frequently available,       they do with humans.
such that the “new capacities are sufficiently robust and
enduring as to contribute to the persisting cognitive profile     Artefacts and Agency
of a specific individual” (Sutton, 2006).                         These results stand in contrast with those of Reeves and
                                                                  Nass (1996), who demonstrate that humans exhibit
             Trust and Complex Artefacts                          behaviours with computers similar to their behaviours with
Perceived utility of an artefact is an especially strong          other humans. Such examples include attraction to agents
concept amongst researchers in human-computer interaction         whose characteristics are most like their own, a greater
and the psychology of trust in automation, where it serves as     willingness to accept flattery than criticism from the
the core component (if not the very definition) of trust in       computer, and a less critical approach to the computer
technology and automation in many frameworks (e.g. Lee &          directly, rather than “behind its back.” Based on these
Moray, 1992; 1994; Fogg & Tseng, 1999; Dzindolet, Pierce,         results, Reeves and Nass conclude that human-computer
Beck & Dawe, 2002; Riegelsberger, Sasse & McCarthy,               interaction is natural and social in nature.
2005; Kaasinen, 2005). In this literature, the perceived             Miller (2004) uses these results to argue that computers
utility is defined as the comparison of the user’s assessment     have bypassed what he refers to as the “agentification
of the system’s performance versus the user’s assessment          barrier,” a point where an artefact reaches a sufficient level
of their own performance, an analysis which is highly             of complexity and autonomy that we ascribe qualities such
subject to bias.                                                  as intent and awareness to it. Miller demonstrates that this
   According to the trust framework put forward by                difference is so pronounced that we even use different
Dzindolet, Pierce, Beck & Dawe (2002), disuse of a                language when referring to computers rather than other
cognitive artefact (which is to say, choosing not to use the      tools: “Even my language, as I write this, is illustrative: I hit
artefact even when it would be appropriate) is a result of        myself with the hammer, while my computer does things to
mistrust of the artefact.       In their experiments, they        me” (2004). As a result, Miller claims that humans readily
demonstrate that users have an initial expectation of             generalize their expectations from human-human interaction
computer superiority (which they call the automation bias),       to human-computer interaction regardless of whether or not
however, errors made by the systems are extremely salient.        that is the intent of system designers.
Specifically, people rated performance of automated
systems as lower than their own, even when the system             Framing Trust
made less than half as many errors and non-cumulative             Riegelsberger, Sasse & McCarthy (2005) follow a similar
feedback was provided at each trial. By contrast, subjects        track as they lay out what they believe to be a general
were more lenient and trusting of “human experts” with the        framework for trust, encompassing both human-human
same or worse performance profiles as the automated               interaction and human-computer interaction. They claim
system. They assert that the automated system is betraying        that the largest difference between trust in technology and
our initial trust by making its errors (thus violating our        trust in humans is that, when dealing with automation, the
expectations), and thus is quickly judged to be                   primary issue is the trustor’s perception of the trustee’s
untrustworthy. One simple example that they present is the        ability, since computers do not have motivation. However,
case of automated alarm systems, and what has come to be          they contend that most technological “agents” are in fact
called the cry wolf effect. Essentially, only a few false         part of a larger socio-technological system and should thus
alarms signals are required to greatly degrade trust in the       be analysed using the entire framework. This is especially
system, and, accordingly, response to the alarm. Dzindolet        true, they argue, due to the fact that some users are likely to
et. al also demonstrate the rapid-distrust effect                 ascribe motivation to the automated agent (as per Miller's
experimentally. In this study, subjects were asked to             agentification). They assert that simple social guards are in
perform a task, and after each of their responses, they were      a constant state of evolution as the guidelines for what
shown the response of an “aide,” which was either described       should make an agent trustworthy are co-opted by
as a human expert or a computer program. At the end of the        untrustworthy actors, thus reducing or eliminating their
study, subjects were offered a reward based on the accuracy       value, as can be shown with the increased complexity of
of a randomly selected sample of the answers from the             internet phishing scams. Simply increasing the number of
previous trial, and allowed to have the reward calculated         social guards is also not a viable option, because then the
based on their own answers or that of the aide.                   burden of trust-testing takes over the entire transaction,
                                                              705

causing Sterelny’s argument that the cost of use outweighs          messages were considered anthropomorphic in one study
the usefulness to materialize. Thus, they argue, while              but not the other), or if it is a result of differences in the test
trustworthiness “markers” do contribute to perceived                subjects and their levels of exposure to technology.
trustworthiness, they are not by themselves sufficient to              On the opposite end of the spectrum, however, is
generate trust.                                                     evidence that agents which are intrusive or annoying can
   Thus, beyond simple markers, a trustor must rely on cues         generate an affective distrust, leading to disuse of the agent.
from the trustee and the environment in order to assess both        As an example, Schaumburg (2000) performed a study of
the ability and the motivation of the trustee. From this, five      Microsoft Office users, showing that not only did subjects
factors of trust are posited, and are split into external and       dislike the Office Assistant (or, as it was more commonly
intrinsic groups (Riegelsberger, Sasse & McCarthy, 2005).           known, “Clippy”), but that they actually expressed strong
External factors are pressures which act to coerce the trustee      negative feelings towards it. As a result, Clippy was ranked
into compliance. These include temporal embeddedness, or            as the least efficient way to solve a problem, rejected in the
the prospect of later retaliation; social embeddedness, or the      context of learning a new application or feature (fewer than
prospect of the trustee’s reputation; and institutional             33% said they would do so, 46% said they would never use
embeddedness, which is a combination of the trust in the            him), and was only “liked” by 22% of subjects. Most
brand associated with the trustee and the trust in the society      subjects reported that they did not trust Clippy to correctly
which creates regulations to which they must conform or             identify their goal or to provide useful assistance.
risk punishment. The intrinsic factors are: ability, which is
the belief that the trustee is able to perform the task             Trust vs. Risk
(perceived utility); and internalized norms, which, in the          One additional point raised by Riegelsberger, Sasse and
case of technology means dependability – that the system            McCarthy (2005) in setting out their framework is that
will continue to work in the same way over time. Since the          some researchers have shown that trust is only required in
external measures of trust are used to measure the                  situations in which there is risk, although they claim that
motivation of a trustee, they are less salient when evaluating      risk is hard to define. Generally, risk is measured
the trustworthiness of an automated agent. In fact, it is           economically, as the product of probability of success and
unclear that an automated system is embedded either                 gain (or, in cases where losses are likely, inverted cost)
temporally or socially.          Institutional embeddedness,        (Demaree, DeDonno, Burns & Everhart, 2008); however,
however, does appear to be a factor; sociologists are               this definition of risk is best applied to systems which are
showing that everyday interactions are increasingly based           deterministic in nature (such as simple gambling tasks).
on trust in a brand rather than the individual (Riegelsberger,      Attempting to apply it as a metric in a trust framework
Sasse & McCarthy, 2005), which creates an obvious                   results in a circular definition, in that it is the trust in the
extension to computer-based agents, especially when used            system which allows for the estimation of the probability of
for commerce.                                                       success. Social psychological measures of risk make use of
                                                                    game theory, resulting in a similar circularity. It does seem
Affective Trust                                                     to follow, however, that risk is a function of the potential
Riegelsberger, Sasse & McCarthy (2005) state that the lack          gains and potential losses of a given action or system,
of trust in technology can be partially attributed to a lack of     regardless of the actual form of that function. Thus,
interpersonal cues. Citing research by Rickenburg &                 Riegelsberger et al.’s (2005) binary view of “risk” or “no
Reeves (2000), they show that some cues lead to an                  risk” can be extended, meaning that the degree of
affective trust even if there is no rational reason for this        trustworthiness required in any given interaction is
trust. For example, the use of a synthetic voice or a               proportional to the amount of risk the trustor must
synthetic animated character with only very basic                   undertake.
interpersonal cues was found to increase trust.                        In cases of distributed cognition, the trustor is not only
   Schaumburg (2001), on the other hand, argues that                making herself vulnerable (and thus, at risk) by not
trustworthiness does not come as a consequence of painting          performing the entire task herself and with her own
a face onto an agent’s interface. In fact, he makes the claim       resources, and thus risking the outcome of this task, but she
that in some cases, such an interface may increase the user’s       is also potentially wasting valuable cognitive resources and
anxiety rather than decreasing it, depending on the nature of       time as she learns to integrate the potentially untrustworthy
the social interaction and whether or not the user initially        artefact into her cognitive system.
overestimates the agent’s usefulness. His claim is based in
part on a study by Van Mulken, André and Müller (1999) in                                Bridging the Gap
which users did not follow the recommendations of an                The current research about artefact use and coupling is
anthropomorphic agent (such as a cartoon character) more            highly contradictory. On the one hand, people appear to
readily than a non-anthropomorphic one (such as a text or           rapidly and automatically couple with artefacts (e.g. Kirsh,
audio message), and did not rate it as any more trustworthy.        2006; Kirsh & Magilo, 1994; Magilo & Kirsh, 1996; Clark,
It would be interesting to determine if these data differ due       1998; 2001a; Sutton 2006), and even to generate epistemic
to purely methodological reasons (since, for instance, audio        artefacts without being aware of doing so. On the other
                                                                706

hand, artefacts appear to be often misused or disused, even         like agents, ascribing motivations, awareness and intent to
when the artefact is known to be more accurate (Lee &               these artefacts. These agent-like artefacts are sufficiently
Moray, 1992; 1994; Dzindolet, Pierce, Beck & Dawe, 2002;            different from passive artefacts in that they do not induce
Honeybourne, Sutton & Ward, 2006). The one point of                 automatic endorsement. Thus, Clark and Chalmers’ (1998)
agreement appears to be that some form of trust is required         concerns about the difficulty of meeting the reliability
in order to create a coupled system; however, as I have             conditions in agent-agent interactions manifest themselves
shown previously, the ease with which that trust can occur          when interacting with active artefacts. As a result, we
is debated. One important distinction appears to have been          simply cannot create a coupled system with such an artefact
missed in these debates, however: the difference in the very        until it has earned our trust. However, this process is made
nature of the artefacts to which the coupling occurs.               difficult by the fact that these artefacts lack many of the
                                                                    factors of trust which are employed in agent-agent
Passive Artefacts                                                   interactions, such as temporal and social embeddedness and
Passive cognitive artefacts, such as epistemic artefacts, are       are markedly dissimilar from ourselves. And, of course, the
ancient and have evolved over time with humanity (Clark,            amount of trust required in any given interaction or
2001b), in a sort of evolutionary bootstrapping; tools              transaction is a function of the amount of risk undertaken by
allowed our forebears to be smarter, which allowed them to          the trustor.
make better tools, in the same manner suggested by                     Thus, it is the offered reward (or, more accurately, the
Chandrasekharan & Stewart (2007). This co-evolutionary              risk of getting less than the full reward), which explains the
process has, naturally, also left its mark on us; specifically,     difference in results between the experiment Dzindolet,
the availability of tools in our environment to perform the         Pierce, Beck & Dawe (2002), in which people used their
hard tasks necessary for success has made adapting to their         own judgement over that of an artefact they knew made
discovery and use a better evolutionary strategy than               fewer errors, and that of Gray et al. (Gray & Fu, 2004; Gray,
attempting to overcome problems with our own limited                Sims, Fu & Schoelles, 2006) in which reaction time was the
resources. Passive artefacts are the tools that Dennett             only factor in the decision to use the artefact or not. In the
(1996) and Clark (2001a; 2001b) describe when they speak            latter case, the overall level of task performance was
of offloading into the environment, both to free up cognitive       unimportant to the participants, and thus there was almost
resources and to allow us to reshape problems.                      no risk in employing the artefact.
   Thus, when dealing with such passive artefacts, Sterelny’s
hypothesized expensive social guards (Sterelny 2004, 2005)          Multi-Function and Hybrid Artefacts
do not manifest themselves. It is unclear, however, if this is      It is important to note that in the modern technological age,
a result of Parsell’s (2006) claims about the triviality of the     increasingly when discussing an artefact, we refer not to the
cost of cheater detection or, if Sterelny was, in fact, correct     physical device itself, but rather its software. For example,
about the high costs of employing social guards. From an            to a practiced user there is almost no functional difference
evolutionary standpoint, the benefit of acquired behaviours         between a physical or electronic address book. As such,
needs to outweigh their costs, and so, it may the case that         both can be considered to be passive artefacts. The same
the expensive social guards were too complex to have                can be said of many other software packages: electronic
evolved. It may simply be that the benefits of automatically        notepads, rolodexes and the like are all clearly passive
endorsing the content of our epistemic artefacts far                devices. However, the same physical hardware which
outweighed the costs of being deceived in a non-obvious             serves as an address book can also employ “active”
and thus non-trivially detected way.                                software.
   As a result of this automatic endorsement, the reliability          Some software artefacts, however, such as word
criteria for coupling are met almost trivially, and thus            processors, have begun to bridge the gap and act both as
people exhibit the sort of behaviour described by Sutton            active and passive. Whereas older versions of such software
(2006) (and Clark, 1997, 2001a; 2001b; Kirsh & Magilo,              simply allowed for the suspension of thoughts in linguistic
1994; et cetera), easily extending themselves to passive            form thus freeing us from our working memory limitations,
cognitive artefacts. The ubiquitousness of such artefacts in        newer ones alter the text we type by automatically
modern culture (notebooks, address books, paper, filing             correcting spelling and grammar, for instance. In general, I
cabinets, palm pilots, et cetera) lends credence to this view.      would suggest that such artefacts are true hybrids, and
                                                                    treated as such – being automatically trusted in their ability
Active Artefacts                                                    to hold our thoughts without being subject to alteration or
Much more recently, however, there has been the creation            error, while at the same time needing to earn our trust to be
of active cognitive tools: automated and semi-autonomous            able to alter (or correct) them.
systems which are capable of manipulating representations.
These are the systems which perform analyses and                                              References
inferences, that make suggestions, that automate activities,        Camazine, S., Deneubourg, J.-L., Franks, N., Sneyd, J.,
and so on. As per Miller (2004), active tools have crossed             Theraulaz, G., & Bonabeau, E. (2001). Self-Organization
the “agentification barrier,”, and therefore, we treat them
                                                                707

  in Biological Systems. Princeton: Princeton University         Lee, J. & Moray, N. (1994). Trust, self-confidence, and
  Press.                                                           operators’ adaptation to automation.        International
Chandrasekharan, S. & Stewart, T. (2007). The origin of            Journal of Human-Computer Studies, 40, 153-184.
  epistemic structures and proto-representations. Adaptive       Maglio, P. & Kirsh, D. (1996). Epistemic action increases
  Behaviour, 15, 329-353..                                         with skill. In Proceedings of the Eighteenth Annual
Clark, A. (1997). Being There: Putting Brain, Body, and            Conference of the Cognitive Science Society. Mahwah,
  World Together Again. Cambridge: MIT Press.                      NJ: Lawrence Erlbaum.
Clark, A. (2001a). Mindware: An Introduction to the              Miller, C. (2004). Human-Computer etiquette: Managing
  Philosophy of Cognitive Science. New York: Oxford                expectations with intentional agents. Communications of
  University Press.                                                the ACM, 47, 31-34.
Clark, A. (2001b). Reasons, robots and the extended mind.        Parsell, M. (2006). The cognitive cost of extending an
  Mind & Language, 16, 121-145.                                    evolutionary mind into the environment. Cognitive
Clark, A. (2005). Intrinsic content, active memory and the         Processing, 7, 3-10.
  extended mind. Analysis, 65, 1-11.                             Reeves, B. & Nass. C. (1996). The Media Equation: How
Clark, A. & Chalmers, D. (1998). The extended mind.                People Treat Computers, Television, and New Media Like
  Analysis, 58, 7-19.                                              Real People and Places.          New York: Cambridge
Demaree, H., DeDonno, M., Burns, J., & Everhart, D. E.             University Press.
  (2008). You bet: How personality differences affect risk-      Riegelsberger, J., Sasse, M. & McCarthy, J. (2005). The
  taking preferences.         Personality and Individual           mechanics of trust: A framework for research and design.
  Differences, 44, 1484-1494.                                      International Journal of Human-Computer Studies, 62,
Dennett, D. (1996).        Kinds of Minds: Toward an               381-422.
  Understanding of Consciousness. New York: Basic                Schaumburg, H. (2001). Computers as tools or as social
  Books.                                                           actors? – The users’ perspective on anthropomorphic
Dzindolet, M., Pierce, L., Beck, H. & Dawe, L. (2002). The         agents. International Journal of Cooperative Information
  perceived utility of human and automated aids in a visual        Systems, 10, 217-234.
  detection task. Human Factors, 44, 79-94.                      Sterelny, K. (2004). Externalism, epistemic artefacts and
Dzindolet, M., Peterson, S. Pomranky, R, Pierce, L. &              the extended mind. In R. Schantz (ed), The Externalist
  Beck, H. (2003). The role of trust in automation reliance.       Challenge: New Studies on Cognition and Intentionality.
  International Journal of Human Computer Studies, 58,             New York: Walter de Gruyter.
  697-718.                                                       Sterelny, K. (2005). Made by each other: Organisms and
Gray, W. & Fu, W-T. (2004). Soft constraints on interactive        their environment. Biology and Philosophy, 20, 21-36.
  behavior: The case of ignoring perfect knowledge in-the-       Sutton, J. (2006). Distributed cognition: Domains and
  world for imperfect knowledge in-the-head. Cognitive             dimensions. Pragmatics & Cognition, 14, 235-247.
  Science, 28, 359-382.
Gray, W., Sims, C., Fu, W-T., & Schoelles, M. (2006) The
  soft constraints hypothesis: A rational approach to
  resource     allocation    for    interactive   behaviour.
  Psychological Review, 113, 461-482.
Hawkins, J. (2004). On Intelligence: How a New
  Understanding of the Brain will Lead to the Creation of
  Truly Intelligent Machines. New York: Times Books.
Honeybourne, C., Sutton, S. & Ward, L. (2006).
  Knowledge in the Palm of your hands: PDAs in the
  clinical setting.     Health Information and Libraries
  Journal, 23, 51-59.
Karowski, W. (2000). Symvatology: the science of an
  artifact-human compatibility.       Theoretical Issues in
  Ergonomics Science, 1, 76-91.
Kirsh, D. (2006). Distributed cognition: A methodological
  note. Pragmatics & Cognition, 14, 249-262.
Kirsh, D. & Maglio, P. (1994). On distinguishing epistemic
  from pragmatic action. Cognitive Science, 18, 513-549.
Lee, J. & Moray, N. (1992). Trust, control strategies and
  allocation of function in human-machine systems.
  Ergonomics, 35, 1243-1270.
                                                             708

