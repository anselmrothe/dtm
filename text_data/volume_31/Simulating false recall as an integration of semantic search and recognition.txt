UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Simulating false recall as an integration of semantic search and recognition
Permalink
https://escholarship.org/uc/item/2h05q3pz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Johns, Brendan
Jones, Michael
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

      Simulating False Recall as an Integration of Semantic Search and Recognition
                                               Brendan T. Johns (johns4@indiana.edu)
                                               Michael N. Jones (jonesmn@indiana.edu)
                              Department of Psychological and Brain Sciences, Indiana University
                                            1101 E. Tenth St., Bloomington, In 47405 USA
                              Abstract                                    framework for understanding false recall, the model focuses
                                                                          on an account of process rather than structure. The model
   We present a computational model of false recall phenomena.
                                                                          represents the associative connections between words in
   The model is based on an integrated architecture including
   modules that have been successful at accounting for other              memory by using association norms, or by hand-fitting
   types of memory tasks. Words presented in a list are                   representations. While the process of the fSAM model may
   represented by semantic vectors from a co-occurrence model,            be a correct one, an explanation of a semantic behavior such
   and are encoded into a composite store. At test, the model             as false recall also requires an account of the representation
   generates a list of candidate words from the lexicon and               that the process operates upon. We believe that integrating
   decides which of these words to recall using a recognition             models that create a realistic structural representation with
   process. We show that the model is able to account for a wide
   range of effects in false recall, including levels of false recall
                                                                          such process models can yield great benefits. If we do not
   in DRM studies, item-level effects, number of associates, and          have the correct account of structure when we build the
   categorical vs. associative list structure.                            process model, we may have to posit a more complex
                                                                          processing mechanism than humans actually use in order to
   Keywords: False recall; co-occurrence representations;                 produce the complex behavior seen in humans. In reality,
   memory models; free recall; generate-recognition models                much of the requisite complexity for a behavior may be
                                                                          coded in the structure of the representation, and a much
                          Introduction                                    simpler processing mechanism will suffice to produce the
The Deese/Roediger-McDermott (DRM) paradigm (Deese,                       behavior (cf. Jones & Mewhort, 2007).
1959; Roediger & McDermott, 1995) has provided                               A promising class of models that can be used to explain
fundamental evidence about how humans can remember                        the structure of semantic memory are co-occurrence
events that were not stored. In this paradigm, a subject is               learning models. Examples include Latent Semantic
typically given a list of items to encode. The list contains              Analysis (LSA; Landauer & Dumais, 1997), the Topics
clusters of items related to a critical lure that is not                  model (Griffths, Steyvers, & Tenenbaum, 2007), and
presented on the list; during subsequent recognition or                   BEAGLE (Jones & Mewhort, 2007). These models build
recall, the critical lure is remembered at similar levels as the          semantic representations for words by observing co-
encoded items. For example, given pillow, snore, bed, tired,              occurrence statistics in a large text base. They have seen
etc. to encode, subjects are likely to falsely recall or                  considerable success at accounting for a variety of semantic
recognize sleep. Exactly what type of information and                     behaviors directly from their representations, such as
overlap is necessary between the targets and the critical lure            synonymy test performance, semantic similarity ratings,
still remains the topic of considerable debate. The DRM                   association norms, and identification times (Johns & Jones,
paradigm has received much recent attention as a task to                  2008). Due to their success, it seems natural to use co-
understand false memory in applied fields, such as                        occurrence representations as structural representations in a
eyewitness testimony.                                                     process model of false recall.
   From a cognitive modeling perspective, the DRM                            However, co-occurrence models only provide an account
paradigm is particularly challenging because a full                       of the structure of memory. To produce sophisticated
understanding of the illusion requires an account of both the             behavior as in false recall, we require a suitable processing
structural organization of semantic memory and the process                model to interface with this structure. In this paper, we
of memory retrieval. As Estes (1975) originally noted at the              present an account of the process of false recall that operates
outset of cognitive modeling, one cannot study structure or               on a realistic semantic representation learned by a co-
function independently; observed human behavior is an                     occurrence model. We build semantic representations for
interaction of the two, and to fully understand a cognitive               words using the recent Semantic Distinctiveness Memory
operation, we need a model that explains both structure and               (SDM) model (Johns & Jones, 2008). Words presented on a
function, and how the two interact to produce behavior.                   DRM list are retrieved from the SDM mental lexicon and
   Accounts of false recall have tended to focus on general               are stored in a composite memory store. At test, the model
verbal conceptual frameworks. Recently, the first                         retrieves a list of candidate words from the lexicon that are
computational model of false recall was presented (the                    similar enough to the composite store, and this candidate list
fSAM model of Kimball, Smith, & Kahana, 2007). While                      is then used by a recognition module which decides whether
the fSAM model is an excellent step towards a formal                      or not to recall the word. Our account is an integrated
                                                                      2511

architecture fusing together models of semantic                    modification that the SDM model makes, in comparison to
representation, recognition, and memory search that have           other co-occurrence models, is the type of information
proven successful at accounting for other types of memory          added into the word-by-document matrix: instead of raw
data. It is the integration of these components that produces      frequency, it uses a semantic distinctiveness (SD) value
our free recall behavior. The integrated architecture of these     representing how distinct the current context is compared
models is displayed in Figure 1. By integrating different          with the previous contexts that the word has occurred in.
models it is possible to explain more data than any single         The first step in computing this SD value is to create a
model can explain by itself. The different aspects of the          „context‟ or „document‟ vector, which we call a composite
false recall model will now be described in turn.                  context vector (CCV). This vector represents the meaning of
                                                                   the current context. For each word that occurs in a document
                                                                   (W1,...,WN), the word‟s vector is added into the composite
                                                                   vector. Formally, this is:
                                                                                                                             (1)
                                                                   where N is the set of words in the document, and Ti is the
                                                                   memory trace corresponding to word i. The next step is to
                                                                   compute a similarity value (given by a vector cosine)
                                                                   between each word that occurs in the context and the CCV.
                                                                   This similarity value is then transferred through an
                                                                   exponential probability density function to give an SD
   Figure 1. The architecture of the full memory system.           value:
                                                                                                                             (2)
  False Recall through Search and Recognition
                                                                   where λ is a fixed parameter with a small positive value; as
Our false-recall model is based on classic generate-               λ is increased the difference in the value of high vs. low
recognition models (e.g. Kintsch, 1970), in which items are        similarity contexts is accentuated. This SD metric is the
internally generated and are then tested with a recognition        value added into the memory slot for that word and context.
check. These simple models will be extended with a                 A context with a high SD value means that it is more
mechanism that searches through a fully quantified mental          distinct compared with the other contexts that a word has
lexicon built by a co-occurrence learning model. Once a            appeared in. This practice gives greater salience to more
search set is created, these words are then tested with a          unique contexts, in terms of the word‟s magnitude, than
recognition model that has been shown to be susceptible to         redundant contexts.
false recognition. Hence, there are four different modules of         These SDM representations will be used as the lexical
the model that we will describe: 1) the lexical                    structure that drives recall. SDM representations are sparse
representations, 2) the encoding process, 3) the search            vectors, in which non-zero values contain a number between
process, and 4) the recognition process.                           0 and 1 that represents how important that particular context
                                                                   was to forming the semantic representation for that word.
1. Lexical Representations from SDM                                Even though the typical practice is to reduce the
To simulate the contents of semantic memory, we use                dimensionality of these vectors using some type of vector
representations constructed by the SDM model (Johns &              reduction technique, as is done in LSA (Landauer &
Jones, 2008), a recent co-occurrence learning model. The           Dumais, 1997) and the Topics model (Griffiths, et al.,
SDM model produces semantic representations similar to             2007), we use the raw episodic traces. In a test of the raw
those created by other models such as LSA, but it also             vectors against LSA, we find that the model attains a better
simulates the effect of semantic distinctiveness on a word‟s       fit to semantic similarity ratings and backward association
strength in memory. We have demonstrated, using both a             strength, and the model only does slightly worse on forward
corpus analysis (Johns & Jones, 2008) and an artificial            association strength. In the following simulations, the SDM
language learning experiment (Recchia, Johns, & Jones,             lexicon was built by training on the TASA corpus, and the
2008), that words that occur in more semantically distinct         vectors will have a resulting dimensionality of 36,700.
contexts are more strongly represented within memory.
Johns & Jones (2008) showed that this SDM model                    2. Encoding Process
produces better fits to both lexical decision and naming           We use a single composite vector to represent a study list.
times, and produces superior semantic organization                 The representations for each word seen in a study list are
compared to other models. In addition, the model can               retrieved from the SDM mental lexicon, and are summed
account for semantic isolation effects, semantic similarity        into a composite vector, which represents of the “gist” of all
ratings, and word association norms.                               words on the list. Word vectors are first normalized so that
   As in other co-occurrence learning models, SDM builds a         each word adds approximately the same amount of
term-by-document matrix from a text corpus. The
                                                               2512

  information, and each is weighted by a uniform random              determine if the retrieved word‟s semantic representation is
  number between 0 and 1 prior to addition to simulate               coherent with the study list‟s representation.
  encoding failure.                                                     In Johns & Jones (2009) we describe a recognition model
     The use of a composite vector is one of the reasons that        that is designed to do exactly this. This model also uses the
  this model will be able to account for a variety of false          semantic representation that the SDM model creates and
  memory results. It is based on the assumption that humans          encodes a study list in the same manner as described above.
  encode the meaning of items in the context of the other            There are two main aspects to this recognition model,
  items encoded. From this perspective, the task of recall may       amplification and decision.
  involve a process that determines if a particular word‟s
  semantic representation is coherent with the gist                  a) Amplification The recognition model is based on an
  representation that was seen in the study list. This approach      analogy to amplification. For each item in the search set, the
  has some similarities with the claims of Fuzzy Trace Theory        recognition attempts to amplify the word‟s representation in
  (FTT; Brainerd & Reyna, 2002), which proposes that                 the composite memory representation. This is accomplished
  (among other things) memory stores „gist‟ traces of events,        in two ways - by adding probe information into the
  and these are traces that capture the meaning of an episode,       composite and by removing contradictory information. How
  without specific perceptual features. In a similar way, our        efficiently a candidate word‟s semantic information is
  model encodes a composite vector of all the words that             amplified within the composite may be viewed as
  occur in a specific study list, and this vector represents the     confirmatory information (signal), and how much
  gist of the study list.                                            mismatching information is amplified (noise) may be
                                                                     viewed as contradictory information. Contradictory
  3. Search Process                                                  information is taken out by simply multiplying the memory
                                                                     vector by a uniform random number between 0 and 1 at
  To internally generate words to test with recognition, a
                                                                     each location where the probe word contains no information
  searching mechanism within the mental lexicon is
                                                                     (i.e. where the probe vector is 0). This process causes the
  employed. The SDM lexicon contains approximately 70,000
                                                                     probe word‟s representation to increase in the memory store.
  words. Hence, the searching process is a difficult one: it
                                                                     This process is intimately tied to the decision process
  requires extraction of the words that occurred on the list
                                                                     described below because how efficiently the word is
  (and are encoded within the composite vector), whilst
                                                                     amplified within memory determines the decision that is
  ignoring all other words. Due to the immensity of this
                                                                     made.
  searching task, the word representation must be of sufficient
  resolution, and the SDM model contains this necessary
                                                                     b) Decision Two different types of information are used by
  structure.
                                                                     the RSA model to decide whether the candidate word was
     Firstly, we define the similarity between a word in the
                                                                     encoded or not: 1) similarity information and 2)
  lexicon and the study list‟s composite trace as the cosine
                                                                     contradictory information. Similarity information is simply
  between their respective vectors. This value is then
                                                                     assessed with a cosine between the probe vector and the
  converted to one minus the magnitude of the word under
                                                                     composite vector. If this cosine exceeds a certain criterion
  consideration divided by the maximum magnitude in the
                                                                     (set at 0.991 in the following simulations) then an „old‟
  lexicon (approximately 1000). Formally, this is
                                                                     decision is made.
                                     len(word) ,                Contradictory information is the amount of information
           Sim  cos(word, probe) * 1               (3)
                                      max                    the model has that the word did not occur in the study list.
                                                                     This information is used to make „new‟ decisions, and is
  where len returns the magnitude of the word in memory.             done by taking the absolute difference between the defining
     This similarity value is then used to drive the searching       portions of the probe and the corresponding locations within
process. This process is simply based on classic signal            the memory vector, and dividing this summation by the
  detection: If the similarity between a word in the lexicon         magnitude of the probe. The resulting value is between 0
  and the composite is greater than a criterion, then the word       and 1, where it will be 0 if all of the probe information is
  is added into the search set. This criterion is fixed at 0.1       contained in memory, and it will be 1 if none of the probe
  across all our simulations. The criterion seems intuitively        information is contained within memory. Because
  low because the SDM vectors are sparse vectors, hence, the         contradictory information decreases across iterations, this
  cosines that are taken with this model tend to be low.             value is a running count. If this count exceeds a criterion
                                                                     (set at 3.9 in the following simulations), then a „new‟
  4. Using RSA for the Recognition Process                           decision is made. A detailed formal treatment of both the
  Once the search set is compiled, we need a decision                amplification and decision process can be found in Johns &
  mechanism to determine if the retrieved words actually             Jones (2009).
  occurred. This is necessary because the model does not store
  any item-level information, so it is not possible to conduct
  item-to-item comparisons. Instead, a process is necessary to
                                                                 2513

Discussion                                                          intruded, and in the Gallo & Roediger (2002) 3.5 words
                                                                    intruded. These predictions are slightly high compared to
This model is based off of classic generate-to-recognition
                                                                    the empirical studies, but considering the massive search
models (e.g. Kintsch, 1970) and utilizes a mental lexicon
                                                                    task that the model must undertake (searching through
built by a co-occurrence learning model. At study, the
                                                                    70,000 words), the pattern is nonetheless impressive.
presented list is encoded into a composite memory vector. A
                                                                       However, this comparison only provides qualitative
searching process then utilizes this composite vector to
                                                                    evidence that the model is attaining false recall levels
search through the mental lexicon and pull out the most
                                                                    equivalent to those observed in experimental data. As
similar words to the composite, in order to create a search
                                                                    Stadler, et al. (1999) and Gallo & Roediger (2002) have
set. Once this search set has been created, the words in this
                                                                    shown, there is considerable variability in the amount of
set are given to a recognition model, which decides whether
                                                                    false recall across lists within an experiment. Because our
or not to recall a word.
                                                                    model possesses individual representations for each word, it
   The parameter space for this processing model is very
                                                                    is possible to measure the different levels of false recall that
simple – there are only three fixed parameters. These
                                                                    are seen for particular critical words in the model and
parameters are not manipulated across the different
                                                                    compare these quantitatively with empirical results.
simulations, so there is very little complexity actually built
into the processing model. Instead the emphasis in this
model is based on the contents of memory for different
experiments, and not processing differences across tasks.
                         Simulations
The methodology that we use in simulating false recall
results is very simple: we take the words used in a specific
experiment, retrieve the vector representations from the
SDM model, encode the words in a study list to a composite
representation, and feed this composite to the search and
recognition processes.
Simulation #1: Levels of False Recall
Different levels of false recall are observed in different
DRM lists. Here we simulate three different sets of DRM
lists: 1) the DRM lists from Roediger & McDermott‟s
(1995) classic study, 2) the extended DRM list set from             Figure 2. The simulated levels of veridical and false recall
Stadler, Roediger, & McDermott (1999), and 3) the more              and the corresponding empirical results.
variable lists from Gallo & Roediger (2002). The levels of
veridical recall were also tested to ensure that the model is       Simulation #2: Item-Level Analysis
attaining true recall levels as well as false recall levels.        Stadler, et al. (1999) and Gallo & Roediger (2002) have
                                                                    both published the levels of false recall observed with
Method The DRM lists for the above described studies                different DRM lists. As both of these studies show, there is
were attained from the specified papers. One list (that for         considerable variability in the levels of false recall elicited
man) was excluded because it was in the stop list that the          by different DRM lists. To test the model‟s quantitative
SDM model was trained with. For a single trial, four DRM            predictions, we correlated the levels of false recall for the
lists were randomly selected and added into the composite.          model and data using the same words from each experiment.
Then the proportion of studied items recalled was recorded,
as well as the number of critical lures falsely recalled on         Method 54 lists from Stadler, et al. (1999) and Gallo &
each trial. In total, 250 trials were run for each set of DRM       Roediger (2002) were obtained from these studies. Again,
lists. This limited number of trials was conducted because of       four DRM lists were added into a single composite vector
the large amount of computation that this model requires.           and the levels of false recall for the different critical words
                                                                    were attained. In total, 250 trials were simulated for both the
Results The levels of false and veridical recall across the         lists from the Stadler, et al. and the Gallo & Roediger study.
different DRM lists are displayed in Figure 2. This figure
shows that the model attains a very good approximation to           Results Across the 55 lists (with repeats removed), a
the levels of recall across the different list sets. Also, the      significant correlation of r = 0.496, p < 0.001 was obtained
level of non-critical word intrusions across the different list     between the model‟s predictions and the behavioral data.
sets was also recorded. For the lists from Roediger &               Hence, it appears that the model is producing relative levels
McDermott (1995) 2.2 non-critical words intruded on                 of false recall across different critical items that shows
average. In the Stadler, et al. (1999) lists, 1.8 words             strong correspondence to the false recall levels in humans.
                                                                2514

If the five lists that the model does worst on (king, rough,        As the number of associates to a critical word is increased,
needle, smell, and health) are removed, then the correlation        the probability of the model falsely recalling the critical
increases to an r = 0.675, p < 0.001. There is no principled        word increases as well. In Johns & Jones (2009) we show
reason to remove these items, but it does show that for the         that the recognition component of our model (RSA) is able
majority of lists the model is giving a good approximation.         to simulate the results of Robinson & Roediger (1997) when
   However, this simulation does not rule out the possibility       this same experimental setup is tested through a recognition
that these critical words are being recalled not due to             experiment, rather than a recall test. Hence, there are two
semantic similarity, but to some other factor in their              reasons the model is able to account for this result: the
representation (e.g. frequency). To further demonstrate that        search set is more likely to include the critical word as the
the amount of semantic information about a specific word            number of associates to a critical word is increased, and the
contained in memory is driving false recall, we conducted a         recognition model is more likely to accept it.
simulation manipulating the number of associates to a
critical word.                                                      Simulation #4: Categorical vs. Associative Recall
                                                                    Park, Shobe, & Kihlstrom (2005) examined the levels of
                                                                    false recall that are seen with critical lures for associative
                                                                    lists and categorical lists. Specifically, the categorical study
                                                                    lists were composed of subordinate (vertical) category
                                                                    instances, while the association lists contain horizontal free
                                                                    associates (the typical DRM lists). Park, et al. (2005) found
                                                                    significantly lower levels of false recall for the category
                                                                    labels than for the DRM critical words.
                                                                    Method The category labels were taken from Park, et al.
                                                                    (2005). The corresponding category lists were attained from
                                                                    the Battigue & Montague (1969) norms and the DRM lists
                                                                    were the six used in the Park, et al. study. Study lists were
                                                                    created by adding four lists from both of these categories.
                                                                    The probability of accepting the critical lure for the DRM
                                                                    lists and the category lists was recorded. In addition, the
Figure 3. Simulation of Robinson & Roediger (1997).                 probability of accepting the studied items was also recorded.
Simulation #3: Effect of Number of Associates                                   Table 1. Simulation of Park, et al. (2005)
Robinson and Roediger (1997) have demonstrated that as
the number of associates to a critical word contained within                            Category Lists              DRM Lists
a study list is increased, the probability of falsely recalling                        Old        Critical       Old        Critical
the critical word increases substantially. Their result                   Data         0.79         0.0          0.74        0.33
strongly suggests that a causal factor underlying false recall           Model         0.59         0.0         0.675        0.23
is the amount of semantic information about a critical word
that is contained in memory. The same pattern should be             Results The results of this simulation are displayed in Table
predicted by our model: as the number of associates to a            1. The levels of false recall to DRM lists and category lists
word is increased, the similarity of the probe to the               for the model are very similar to those found by Park, et al.
composite will also increase. This causes the critical lure to      (2005). That is, the level of false recall to the DRM list is
have a higher probability of being included in the search set       much higher than that for category labels, mirroring what
and also to be accepted by the recognition model.                   was found in the empirical study using the same materials.
Method We used the same lists as did Robinson and                   Simulation #5: Relationship between False
Roediger. (1997). On each repetition, five different DRM            Recognition and False Recall
lists were selected and 3, 6, 9, 12, or 15 items in the list        Both Stadler, et al. (1999) and Gallo & Roediger (2002)
were randomly selected and added into the study list.               have reported a significant correlation between levels of
Probability of the model recalling a critical word was              false recognition and levels of false recall. Stadler, et al.
recorded.                                                           report r = 0 .77, p < 0.001 across lists, while Gallo &
                                                                    Roediger (2002) report r = 0.78, p < 0.001 between recall
Results The results of this simulation are displayed in             and recognition across their lists. These correlations are
Figure 3. This figure shows convincingly that the amount of         likely artificially inflated because in these studies
semantic information contained within the memory vector             recognition occurred after a recall period, but it does show
about a particular word is driving the levels of false recall.      that there is a relationship between levels of false recall and
                                                                2515

recognition. Due to the fact that in our recall model a               of the Connecticut category norms. Journal                of
recognition component is used to make a decision about                Experimental Psychology Monograph, 80(3, Part 2).
whether a specific word occurred or not, we expect the              Brainerd, C. J., & Reyna, V. F. (2002). Fuzzy-trace theory
model to attain a similar relationship between the predicted
                                                                      and false memory. Current Directions in Psychological
levels of false recall and false recognition.
                                                                      Science, 11, 164-169.
Method Levels of false recall were simulated for the lists          Deese, J. (1959). On the prediction of occurrence of
contained in Stadler, et al. (1999) and Gallo & Roediger              particular verbal intrusions in immediate recall. Journal of
(2002). Levels of false recognition were computed with the            Experimental Psychology, 58, 17-22.
RSA model, described in Johns & Jones (2009), for these             Estes, W. K. (1975). Some targets for mathematical
same lists. 250 trials were done for both models.                     psychology. Journal of Mathematical Psychology, 12,
                                                                      263-282.
Results For the lists from Stadler, et al. (1999) a correlation     Gallo, D.A.,& Roediger, H.L. (2002). Variability among
of R = 0.578, p < 0.001 was obtained, and for the lists from          word lists in eliciting memory illusions: evidence for
Gallo & Roediger (2002) a correlation of r = 0.574, p <               associative activation and monitoring. Journal of Memory
0.001 was found between the levels of false recognition and           and Language, 47, 469-497.
false recall. These correlations are smaller than those             Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B.
reported in the behavioral results because, as described              (2007).Topics in Semantic Representation. Psychological
above, recall preceded recognition in those studies. This             Review, 114, 211-244.
relationship is not surprising considering that the RSA             Johns, B. T., & Jones, M. N. (2009). False recognition
recognition model plays an intricate role in our recall model.        through semantic amplification. Proceedings of the 31st
However, it does demonstrate that the searching mechanism             Annual Cognitive Science Society.
employed by the recall model plays a key role in the levels         Johns, B. T., & Jones, M. N. (2008). Predicting lexical
of false recall. This simulation shows that both the searching        decision and naming times from a semantic space model.
and the recognition processes contained in the recall model           Proceedings of the 30th Annual Cognitive Science Society
are creating the level of false recall that the model attains.        (pp. 279-284). Austin, TX: Cognitive Science Society.
                                                                    Jones, M. N., & Mewhort, D. J. K. (2007). Representing
                         Conclusion                                   word meaning and order information in a composite
                                                                      holographic lexicon. Psychological Review, 114, 1-37.
This model demonstrates the power of using cognitively              Kimball, D. R., Smith, T. A., & Kahana, M. J. (2007). The
plausible representations of words. By incorporating                  fSAM model of false recall. Psychological Review, 114,
semantic representations based on a co-occurrence learning            954-993.
model, we require only a very simple processing model to            Kintsch, W. (1970). Models for free recall and recognition.
explain a wide range of false recall data. In addition, the           In D. Norman (Ed.), Models of Human Memory. New
model is more constrained than models using hand-coded                York: Academic Press, 333-374.
semantic representations because it provides an account of          Landauer, T. K., & Dumais, S. T. (1997). A solution to
both memory structure and process, and how the two                    Plato‟s problem: The latent semantic analysis theory of
interact to produce false recall.                                     the acquisition, induction, and representation of
  Further, this approach not only allows us to make                   knowledge. Psychological Review, 104, 211-240.
quantitative predictions about levels of false recall expected      Sederberg, P. B., Howard, M. W., & Kahana, M. J. (2008).
in DRM lists, but it allows for the integration of models that        A context-based theory of recency and contiguity in free
are used to explain different aspects of memory. Integrated           recall. Psychological Review, 115, 893-912.
together with the SDM model (Johns & Jones, 2008) and               Recchia, G., Johns, B. T., & Jones, M. N.(2008). Context
the RSA model (Johns & Jones, 2009) these models explain              repetition benefits are dependent on context redundancy.
a significant number of effects across many different                 Proceedings of the 30th Cognitive Science Society
paradigms. This integration also allows for a greater                 Meeting, 267-272.
simplicity across all of the models, as well as the possibility     Robinson, K., & Roediger, H. L. (1997). Associative
to be combined with other models, such as the TCM                     processes in false recall and false recognition.
(Sederberg, Howard, & Kahana, 2008). Across all three                 Psychological Science, 8, 389-393.
models there are a total of four parameters, which is less          Roediger, H. L., & McDermott, K. B. (1995). Creating false
than many models designed to explain a single paradigm.               memories: Remembering words not presented in lists.
There are obviously many results that these models cannot             Journal of Experimental Psychology: Learning, Memory,
explain, but due to the simplicity of these different models it       & Cognition, 21, 803-814.
is an appealing architecture to investigate with other              Stadler, M.A., Roediger, H.L., & McDermott, K.B. (1999).
memory phenomena.                                                     Norms for word lists that create false memories. Memory
                         References                                   & Cognition, 29, 424-432.
Battig,W.F., & Montague,W.E. (1969). Category norms for
  verbal items in 56 categories: A replication and extension
                                                                2516

