UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The effect of robot gaze on processing robot utterances
Permalink
https://escholarship.org/uc/item/5ct9w65z
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Crocker, Matthew W.
Staudte, Maria
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                          The effect of robot gaze on processing robot utterances
                                               Maria Staudte & Matthew W. Crocker
                                                 Department of Computational Linguistics
                                                              Saarland University
                                                            Saarbrücken, Germany
                                                  {masta, crocker}@coli.uni-saarland.de
                              Abstract                                    that this is not true interaction, it has been shown that a tele-
                                                                          present robot has similar effects on the subjects’ perception
   Gaze during situated language production and comprehension
   is tightly coupled with the unfolding speech stream in a man-          and opinion as a physically present robot (Kiesler, Powers,
   ner that provides interlocutors with relevant on-line informa-         Fussell, & Torrey, 2008; Woods, Walters, Koay, & Dauten-
   tion about both what we intend to say and what we have under-          hahn, 2006).
   stood. This paper investigates whether people similarly exploit
   such gaze when listening to a robot make statements about the             We hypothesize that if people integrate cognitively moti-
   shared visual environment. On the basis of two eye-tracking            vated robot gaze with the actual robot utterance, they combine
   experiments exploiting different tasks, we report evidence (a)         both, gaze and utterance, into one reference resolution mech-
   that people’s visual attention is influenced on-line by both the
   robot’s gaze and speech, (b) that congruent gaze (to mentioned         anism. That is, we predict that people will follow robot gaze
   objects) facilitates comprehension, and (c) that robot gaze does       to objects in the scene (Prediction 1) and that robot gaze leads
   indeed influence what listeners think the robot intended. This         people to make assumptions about what the intended refer-
   supports the view that spoken interaction with artificial agents
   such as robots benefits when those agents exhibit cognitively-         ents are (Prediction 2). We present results from two exper-
   derived real-time speech-mediated attention behaviour.                 iments supporting the hypothesis that people exploit cogni-
                                                                          tively motivated robot gaze during utterance comprehension.
                           Introduction                                   Specifically, we show both, that people follow robot gaze to
Situated spoken language processing has been widely and                   an object when its gaze is available and that this affects the
carefully studied, mainly within the Visual World Paradigm.               resolution of the linguistic reference.
On one hand, listeners fixate potential referents in visual
scenes as soon as there is enough linguistic information                   Experiment 1: Integration of concurrent visual
to delimit a set of potential referents (Tanenhaus, Spivey-                               and linguistic information
Knowlton, Eberhard, & Sedivy, 1995). Among others, Alt-                   Experiment 1 investigates the on-line influence of robot gaze
mann and Kamide (2004) have shown that people look at                     on situated utterance comprehension. The presence of the
the referent about 200-300ms after the onset of the referen-              robot as speaker and its gaze direction provide a visual refer-
tial noun. On the other hand, speakers look at what they plan             ence: In addition to verbally referring to an object the robot
to talk about: Referential gaze in speech production typically            can also use gaze as a visual pointer to an object. To sep-
precedes the onset of the corresponding linguistic reference              arate the influence of robot gaze and speech we manipulate
by about 800ms-1sec (Griffin, 2001; Meyer, Sleiderink, &                  two factors in a 2×3 within-subjects design: Statement valid-
Levelt, 1998).                                                            ity (true/false) and gaze congruency. Congruency denotes the
   While these findings robustly characterise speech-                     match of the visual reference established by the robot’s gaze
mediated gaze to objects and events in visual scenes,                     and the linguistic reference in the robot’s statement, and com-
listeners’ visual attention may also be influenced by speak-              prises three levels (congruent, incongruent, no robot gaze).
ers’ gaze. When people communicate about a common                         We consider gaze to be congruent (and informative) when it
scene they establish mutual gaze as well as joint attention to            is directed towards the same object that is going to be men-
support fast and robust understanding (see Moore (1995)).                 tioned shortly afterwards (reference match) while it is con-
Hanna & Brennan (2007), for instance, have shown that                     sidered as incongruent when gaze is directed to an object dif-
listeners use speakers’ gaze to identify a visual referent                ferent from the mentioned referent (mismatch). In the third
even before the linguistic point of disambiguation uniquely               congruency level robot gaze is absent, providing a baseline
identifies the mentioned referent. While there exist some sig-            condition: Participants’ visual attention is driven purely by
nificant results on robot gaze and its role for engagement in             the utterance. The robot’s statements were produced by the
human-robot interaction (HRI; see Staudte & Crocker (2009)                Mary TTS system (Schroeder & Trouvain, 2001) and are of
for a literature review and initial results), we investigate by           the form given below.
means of objective measures whether HRI would benefit
                                                                               Example:
from robot speech and gaze production that is as closely
                                                                               ”Der Zylinder ist groesser als die Pyramide, die pink ist.”
coupled as in humans.
                                                                               (”The cylinder is bigger than the pyramid that is pink.”)
   In order to better control experimental conditions and to
obtain statistically relevant data, we chose a video-based set-           The scene provides two potential referents for the final noun
ting with a tele-present robot. Although it might be argued               phrase (e.g. two pyramids of different sizes and colours) one
                                                                      431

                                                                                                                      IP1          IP2           IP3
                                Gaze Reference to:                          IPs:
    Spoken          Target            Competitor      none
    Reference to:   (small pyramid)   (big pyramid)                              (German)       “ Der Zylinder ist größer als die Pyramide ,die pink ist.”
                                                                        SPEECH:
                                                                                 (English)      “ The cylinder is bigger than the pyramid that's pink.”
    Target          true              true            true
    (small pyr.)    congruent         incongruent     no gaze
                                                                          TIME:
    Competitor      false             false           false               (sec)   1        2       3         4          5            6        7
    (big pyr.)      incongruent       congruent       no gaze
                                                                        GAZE: <partner> <cylinder>            <small pyramid> <partner>
             Table 1: 2x3 design of both experiments.
                                                                    Figure 2: The approximate timing of utterance-driven robot gaze,
                                                                    in condition true-congruent.
of which the robot mentions. While the small pink pyramid
(the target) matches the example description of the scene de-
picted in Figure 1, the big brown pyramid (the competitor)          monitored participants’ eye movements at a sampling rate of
does not. This way the mentioned colour of the target deter-        500 Hz. The video clips were presented on a 24-inch colour
mines the statement truth. The manipulation of both factors -       monitor. Viewing was binocular, although only the dominant
statement validity and congruency - results in six conditions       eye was tracked, and participants’ head movements were un-
per item. An sample of all conditions given the example sen-        restricted. For each trial, a video was played until the partici-
tence and scene is provided in Table 1.                             pant pressed a button and the next video started. Participants
    The comparison of the two conditions (gaze/no-gaze) re-         always had to use their dominant hand to press the ”correct”
veals when and how precisely robot gaze changes participant         button. The experiment lasted 30 minutes.
behaviour, while the congruency factor allows us to observe         Analysis The presented videos are segmented into Interest
whether people use robot gaze as an early cue to mentioned          Areas (IA) labelled, for instance, target or competitor, with
referents and whether this can influence comprehension.             eye-tracker output mapped onto these IAs to yield the number
                                                                    of participant fixations on an IA. The spoken utterance (see
Methods                                                             Example sentence) describes the relation between the central
Participants Forty-eight native speakers of German,                 object, the anchor, and the target or competitor. The noun
mainly students enrolled at Saarland University, took part in       ”pyramid” from the example sentence is a partial linguistic
this study (14 males, 34 females). Most of them had no expe-        reference that has two referents in the scene: the small, pink
rience with robots. They were told that the eye-tracker cam-        pyramid as target or the large, brown pyramid as competitor.
era was monitoring their eye movements and pupil size to               We segmented the video/speech stream into three Interest
measure the cognitive load of the task on them.                     Periods (IP) as depicted in Figure 2. IP1 is defined as the
                                                                    1000ms period ending at the onset of the target noun (IP2). It
Materials A set of 24 items was used so each participant
                                                                    contains the robot’s gaze towards the target object as well as
saw four different items in each of the six conditions shown in
                                                                    verbal content preceding the target noun phrase (e.g. ”bigger
Table 1. Each item consists of three different videos crossed
                                                                    than the”). IP2 stretches from the target noun onset to offset
with two different sentences. Additionally we counterbalance
                                                                    (mean duration of 674ms). IP3 is defined as the 700ms pe-
each item by reversing the comparative adjective, i.e., from
                                                                    riod beginning at the onset of the disambiguating colour ad-
”bigger” to ”smaller” such that targets become competitors
                                                                    jective. Consecutive fixations within one IA were pooled as
and vice versa. We obtain a total of twelve video/utterance
                                                                    one inspection. We compute proportions of inspections per
pairs per item while ensuring that target size, location and
                                                                    IA within an IP in a condition (summed for each IA across
colour were balanced. All versions show the same scene and
                                                                    trials and divided by the total number of inspections in this
only differ with respect to where the robot looks and which
                                                                    IP). For each IP, we compare the inspection proportions on
object it refers to. The objects are all plain geometric shapes
                                                                    the target and on the competitor IA across conditions.
that were pre-tested to make sure that their size and colour
                                                                       The adjective denoting the colour of the referent completes
differences were easily recognisable. We used 48 fillers for
                                                                    the linguistic reference and identifies the actual target. Only
24 item videos such that participants saw a total of 72 videos.
                                                                    at that point in time is it possible to judge the statement va-
The robot’s gaze and the spoken sentence are timed such that
                                                                    lidity, which is why it is called the linguistic point of disam-
it looks towards an object approximately one second prior to
                                                                    biguation (LPoD).1 The elapsed time between this adjective
the onset of the referring noun.
                                                                    onset and the moment of the button press is therefore consid-
Task & Procedure Participants were instructed to attend             ered as response time (RT).
to the scene and quickly decide whether the robot’s statement          Our sentences include three different comparisons (size,
was right or wrong with respect to the scene. To make the task
                                                                        1A   similar design, also featuring late linguistic disambiguation
appear more natural, participants were further told that their
                                                                    with early visual disambiguation by means of gaze-following, was
results were used as feedback in a machine learning proce-          already successfully tested in a study on human-human interaction
dure for the robot. An EyeLink II head-mounted eye-tracker          by Hanna and Brennan (2007) .
                                                                432

            (a) Robot looks at partner,                  (b) then at the anchor,                      (c) and at TARGET object.
                                            Figure 1: Sample scene from the experiments.
height and width) and two complementary versions of each
comparative, e.g. ”bigger than” and ”smaller than”, in order
to counterbalance target and competitor objects within one
scene. We coded the comparative type (bigger, smaller) and
the distractor IAs (small object for ’bigger than’ and big ob-
ject for ’smaller than’) accordingly. When the partial sentence
”The cylinder is bigger than” has been uttered, i.e. before
the target noun is mentioned, at least two objects in the scene
match this partial description: the target and a small distractor
next to the anchor. Inspections that occur in this time region
(IP1) therefore reveal whether people anticipate one, both or
no potential referents at this stage. To conduct this extended          Figure 3: Inspection proportions in three gaze conditions and IP1
analysis, we also coded the distractor objects as IAs. In the           and IP2.
next time region (IP2), the target noun is uttered which ex-
tends the utterance to ”The cylinder is bigger than the pyra-           interestingly, we expect a main effect of gaze congruency:
mid”. While ”pyramid” identifies two possible referents, only           If participants exploit robot gaze, they can anticipate the va-
the target object is compatible with the comparative.                   lidity of statements in those stimuli when gaze is congruent
Predictions IP1 (”The cylinder is BIGGER THAN THE”):                    with the statement. In contrast, when gaze is incongruent, we
In the no-gaze conditions, we expect the tendency to fixate             expect that participants anticipate a proposition that eventu-
objects that are smaller than the cylinder, i.e., the pink target       ally does not match the actual statement. Hence, we assume
pyramid and the small distractor. When gaze is present, we              slower RT for incongruent robot behaviour.
expect people to look more at the object fixated by the robot.
                                                                        Results
   IP2 (”The cylinder is bigger than the PYRAMID”): In the
absence of gaze there are two pyramids of which only the                The initial results of Experiment 1, including target and com-
pink target pyramid is a suitable referent for the incomplete           petitor inspections as well as RT, have been presented and
linguistic description. Therefore, if people process the robot          explained in more detail in Staudte and Crocker (2009) and
utterance similarly to human utterances, we expect people to            are reported below for the sake of convenience.
clearly prefer fixating the target pyramid over the competitor.            Since sentence truth does not play a role in IP1 and IP2
However, when robot gaze is present and it is considered rel-           (because the LPoD only occurs in IP3), we collapsed each
evant, incongruent gaze is expected to interrupt this fixation          two conditions where trials are identical up to IP2. That
behaviour.                                                              is, conditions true-congruent and false-incongruent are col-
   IP3 (”The cylinder is bigger than the pyramid that’s                 lapsed into the condition ”gaze to target”, true-incongruent
PINK.”): While robot gaze occurs earlier in the sentence, in            and false-congruent are collapsed into ”gaze to competitor”
IP1, the factor determining sentence truth (LPoD) occurs at             and the two no-gaze conditions are merged into ”no gaze”.
the end of the sentence, in IP3. That is, if gaze has a lasting            In IP1, when the robot looks towards either the target or
effect on reference resolution, we expect to observe that peo-          the competitor, we observe that people clearly follow this
ple continuously fixate the visually salient referent when the          gaze and inspect the according IA.2 We also observe a ten-
linguistic reference is consistent with it. We expect incongru-         dency for participants to look at the target more often than at
ency to elicit fixations on both potential referents.                   the competitor in the no-gaze condition although this is not
                                                                        statistically significant. In IP2, when the robot mentions the
   With respect to RT, we predict that participants respond
                                                                        target noun, the main effect of robot gaze and the interaction
faster to true statements than to false ones, mainly because
the ”correct” button is pressed by their strong hand and be-                2 Main effect of robot gaze: F(2, 94) = 21.96, p < 0.001 and sig-
cause verification is typically faster than falsification. More         nificant interaction between gaze direction and IA.
                                                                  433

Figure 4: Inspection proportions on distractor in two gaze condi-
tions (towards or away from distr.) for both comparative versions.        Figure 5: Average response times (RT) for all six conditions.
effect remain. Moreover, when the target noun is uttered and           tioned colour. Moreover, we observe a main effect of gaze
no robot gaze is available, people inspect the target IA signif-       congruency (F targ (2, 94) = 3.42, p > 0.05 and F comp (2, 94) =
icantly more often than the competitor IA (F(1, 46) = 15.53            03.01, p > 0.06) and a robust interaction effect of the two
with < 0.001 ) despite the fact that the uttered sentence is still     factors (F targ (2, 94) = 4.05, p > 0.05 and F comp (2, 94) =
ambiguous at that point (see Figure 3).                                4.94, p > 0.1). The interaction suggests that in congruent
   We also look at the effect of robot gaze on a more fine-            conditions people continuously inspect the object fixated and
grained level of utterance processing: Based on the compar-            mentioned by the robot (which is the target in true statements
ative in the sentence, people prefer the target over the com-          and the competitor in false statements) whereas in incongru-
petitor since it is the most probable referent. The presence           ent conditions, subjects make a visual attention shift from the
of robot gaze, however, introduces additional (and potentially         object fixated by the robot to the object actually mentioned
conflicting) information since it draws attention to either tar-       by the robot.
get or competitor. Thus, when gaze is available, preference               The RT results confirm what the fixation data suggests. We
for the object that fulfills the partial utterance is no longer        observe main effects of both statement validity (F(1, 47) =
observable, instead participants follow the robot’s gaze.              24.83, p < 0.001) and gaze congruency (F(2, 94) = 17.2, p <
   We conducted a similar analysis (as for target and com-             0.001) (see Figure 5). Obviously, participants’ visual atten-
petitor) for both distractor objects next to the anchor. For           tion is influenced by where the robot looks and consequently
the no-gaze conditions, the analyses clearly show that people          people respond faster when they already fixated an object
inspect the distractor IA that matches the uttered compara-            that the robot mentions (congruent). When no gaze or, even
tive significantly more often, which confirms previous find-           worse, incongruent gaze is available, the required attention
ings from human-human studies. Since the size difference               shift results in slower responses.
of the two distractors does not seem to have an effect on the             These RT results suggest that robot gaze as visual attention
predictive power of the comparative, we collapsed the two              guidance influences (the speed of) utterance comprehension.
IAs for the analysis of the gaze-conditions and obtain the fol-        Specifically, it was shown that linguistic reference is pro-
lowing factors: Sentence match (distractor either matches or           cessed faster when people’s visual attention is already on the
mismatches the comparative) and gaze direction (is or is not           according linguistic referent (Prediction 1). Whether robot
in the general direction of robot gaze, i.e., when the robot           gaze (as visual reference) further influences reference reso-
looks at the target/competitor located further away, its gaze          lution and therefore the complete utterance representation is
passes one of the distractors). The results shown in Figure            explored in Experiment 2.
4, suggest that the direction of the robot’s gaze is indeed a
very dominant cue that mainly determines where people look.
                                                                        Experiment 2: The effect of visual reference on
Nevertheless, we find main effects for both robot gaze direc-                       linguistic reference resolution
tion (F(1, 46) = 71.0, p > 0.001) and the comparative match            The results of Experiment 1 suggest that robot gaze is a strong
(F(1, 46) = 9.77, p > 0.001).                                          cue which guides visual attention in an automated way and
   In IP3, we analyse the IAs, target and competitor, sep-             that this influences comprehension. However, there are two
arately, each with respect to the original factors statement           possible explanations for the observed RT effects. Either
validity and gaze congruency.We observe a main effect                  robot gaze is a purely visual cue that influences visual atten-
of statement validity (F targ (1, 47) = 14.81, p > 0.001 and           tion but does not convey referential meaning or it is believed
F comp (1, 47) = 38.7, p > 0.001) suggesting that people look          to express some kind of intentionality which elicits predic-
significantly more often at the object with the actually men-          tions about the intended referent of the speaker. If, indeed,
                                                                   434

                                           Linguistic cue to:          categories assigned were Target, Competitor, No correction
  Condition               Gaze to:    Comparative      Colour          given (accepted as correct), Else (participants described a dif-
  false - no gaze:        —           Target           Competitor      ferent object). We then computed proportions of corrections
  false - congruent:      Target      Target           Competitor      across trials (corrections, e.g., for the target, in one condi-
  false - incongruent:    Competitor  Target           Competitor      tion is divided by the total of trials in this condition). We use
                                                                       repeated-measures analyses of variance (ANOVA) with two
Table 2: Linguistic and visual cues to objects in three congruency     factors (described object, gaze condition) for the statistical
conditions for a false sentence                                        analysis of correction proportions.3
                                                                       Predictions As previous research has shown, people prefer
the effect of robot gaze on people’s visual attention is due to        to use absolute (shape and colour) to relative features (size,
its referential meaning, we predict that robot gaze not only           location) for the production of referring expression (Beun &
affects how fast references are resolved but also which object         Cremers, 1998). We therefore expect to observe a general re-
is believed to be the intended referent of the utterance.              pair preference for the competitor, that is, for the object that is
   Experiment 2 investigates how precisely robot gaze affects          identified by colour. If visual attention is directed towards an
the resolution of the robot’s intended referent when people            object but no expectations with respect to the linguistic ref-
have to correct the robot utterance. Specifically, we observe          erence are elicited, we expect that people’s repair pattern in
participants in response to a false robot utterance that is ac-        the gaze-conditions will not differ significantly from the no-
companied by either incongruent, congruent or no robot gaze.           gaze condition. However, if robot gaze elicits expectations
                                                                       about the intended referent, we expect to observe that people
Methods                                                                describe the target more often in the false-incongruent con-
Participants Thirty-six native speakers of German, again               dition (when the robot looks at the target) than in the false-
mainly students enrolled at Saarland University, took part in          congruent or false-no gaze conditions.
this study (12 males, 24 females).
                                                                       Results
Task & Procedure Participants were instructed to give a
corrected sentence of the robot’s utterance when they thought          Results described below are plotted in Figure 6. We observe
that the robot had made a mistake. They were further told              a main effect of the factor ’described object’ (F(1, 35) =
to start the sentence with the same object reference that the          29.31, p < 0.001) suggesting that people generally prefer to
robot started with, making it easier for the system to learn           correct a sentence with respect to the competitor which has
from the corrected sentences. The experiment was self-paced,           been identified by colour. Moreover, we find an interaction
i.e., participants decided when to continue to the next video          effect of the two factors ’described object’ and ’gaze condi-
by pressing a button. The procedure of Experiment 2 is oth-            tion’ (F(2, 70) = 16.04, p < 0.001) indicating that gaze does
erwise identical to Experiment 1.                                      indeed has an impact on the choice of referents. More pre-
                                                                       cisely, we observe that people correct an utterance with re-
Materials There are two cues in the robot utterance iden-              spect to the target significantly more often when the robot
tifying the correct referent. The first cue is the comparative         looks towards the target (false-incongruent) than when it
(bigger than or smaller than) and the second cue is the ob-            looks at the competitor or nowhere at all.
ject colour. False statements are false when these two cues do            Another fact indicating that gaze is affecting reference res-
not identify the same referent, e.g., when the cylinder is not         olution becomes apparent when analysing corrections in re-
bigger than the brown pyramid. Thus, people can repair this            sponse to true robot utterances. Although we did not expect
utterance by changing either the comparative or the colour             participants to correct true statements, we observed that in
adjective. Details on referential variation for each condition         15% of true-incongruent trials people corrected the robot with
are shown in Table 2.                                                  a sentence about the competitor. This suggests that people be-
   A sentence produced by a participant consequently de-               lieved that the robot was indeed talking about the competitor
scribes either the visually more salient object (based on robot        that it looked at even though both the comparative and the
gaze) or the alternative object when it seems linguistically           colour referred to the target object.
more salient to them. Note that two factors constitute linguis-           The results from the correction analysis supports our hy-
tic saliency, the comparative and the colour, and that their           pothesis that people believe robot gaze to reflect its intention
preference is measurable in the no-gaze condition: Any bias            to talk about an object it looks at (Prediction 2). The inspec-
towards the comparative or towards the colour as an identify-          tion patterns in this experiment, though not reported, are very
ing feature is picked up and can be used as a baseline for the         similar to the ones observed in Experiment 1 and strengthen
analysis of the two gaze conditions.                                   the hypothesis that people integrate visual and linguistic ref-
Analysis Produced corrections were recorded from the start             erences online, similar to human-human interaction.
of a video until the participant pressed a button to continue.             3 We are aware that ANOVA is not the optimal test for originally
For the analysis of the corrections, we annotated the sen-             categorical data but to our knowledge this remains the standard anal-
tences with respect to which object was described. The four            ysis in most production studies.
                                                                   435

                                                                          linguistic cues about intended referents. This highlights the
                                                                          general importance of visual information (both the scene and
                                                                          speaker gaze) during situated language processing, and sup-
                                                                          ports our more specific claim that cognitive models of real-
                                                                          time language-mediated gaze benefit situated communication
                                                                          with artificial agents such as robots.
                                                                                              Acknowledgments
                                                                          The research reported of in this paper was supported by IRTG
                                                                          715 ”Language Technology and Cognitive Systems” funded
                                                                          by the German Research Foundation (DFG).
                                                                                                   References
                                                                          Altmann, G., & Kamide, Y. (2004). Now you see it, now
                                                                                 you don’t: Mediating the mapping between language
Figure 6: Proportion of objects described in response to false utter-            and the visual world. In J. Henderson & F. Ferreira
ances, e.g. ”The cylinder is bigger than the pyramid that is brown”.
                                                                                 (Eds.), The interface of language, vision, and action:
                                                                                 Eye movements and the visual world (p. 347-386). NY:
                         Conclusions                                             Psychology Press.
                                                                          Beun, R., & Cremers, A. (1998). Object Reference in a
Neither task in the presented experiments required partic-                       Shared Domain of Conversation. Pragmatics and Cog-
ipants to exploit robot gaze, they rather required them to                       nition, 6, 111-142.
listen to the utterance while observing the object arrange-               Chris Moore, P. D., Philip J. Dunham (Ed.). (1995). Joint
ments. Nonetheless, we see clear on-line evidence of gaze-                       Attention Its Origins and Role in Development. LEA.
following, despite a relatively high proportion of incongru-              Griffin, Z. M. (2001). Gaze durations during speech reflect
ent trials which might lead subjects to lose confidence in the                   word selection and phonological encoding. Cognition,
robot’s performance. We further observed that robot gaze as                      82, B1-B14.
a visual cue can even override some linguistic cues. These re-            Hanna, J., & Brennan, S. (2007). Speakers’ eye gaze disam-
sults suggest that robot gaze even in this minimal form, being                   biguates referring expressions early during face-to-face
merely simulated by a moving stereo camera head, provides                        conversation. JML, 57, 596-615.
a visual cue that people respond to in the same automatic way             Kiesler, S., Powers, A., Fussell, S., & Torrey, C. (2008). An-
that they respond to human gaze (Prediction 1).                                  thropomorphic interactions with a robot and robotlike
   We further present support for the hypothesis that cogni-                     agent. Social Cognition, 26, 169-181.
tively plausible robot gaze is beneficial for comprehension               Knoeferle, P., & Crocker, M. W. (2007). The influence of re-
(and thus for communication) by showing that people faster                       cent scene events on spoken comprehension: evidence
respond to congruent robot gaze and speech behaviour. To                         from eye-movements. JML (Special issue: Language-
distinguish the purely visual component of robot gaze from its                   Vision Interaction), 57, 519-543.
potentially referential meaning, we changed the task from a               Meyer, A., Sleiderink, A., & Levelt, W. (1998). Viewing and
response time task in Experiment 1 to a production task in Ex-                   naming objects: Eye movements during noun phrase
periment 2, in which people had to verbally correct the robot’s                  production. Cognition, 66, B25-B33.
statement. With no time pressure on people’s responses, the               Schroeder, M., & Trouvain, J. (2001). The German Text-to-
shift of visual attention cannot be the crucial factor anymore.                  Speech Synthesis System MARY: A Tool for Research,
Instead, the produced correction statements reveal which ref-                    Development and Teaching. In 4th isca workshop on
erent listeners thought was ’intended’ by the robot and con-                     speech synthesis. Blair Atholl, Scotland.
firm that this was influenced by robot gaze (Prediction 2).               Staudte, M., & Crocker, M. W. (2009). Visual Attention in
   The presented findings suggest that people integrate visual                   Spoken Human-Robot Interaction. In Proc. of the 4th
reference information derived from the robot’s gaze with their                   ACM/IEEE Conference on HRI. San Diego, USA.
on-line interpretation of robot speech. This finding is broadly           Tanenhaus, M. K., Spivey-Knowlton, M., Eberhard, K., &
consistent with Knoeferle and Crocker’s (2007) ”Coordinated                      Sedivy, J. (1995). Integration of visual and linguistic
Interplay Account” (CIA) of situated comprehension, but re-                      information in spoken language comprehension. Sci-
quires an extension to that model in which not only speech,                      ence, 268, 1632-1634.
but also speaker gaze, is used to both direct attention and vi-           Woods, S., Walters, M., Koay, K. L., & Dautenhahn, K.
sually ground utterance meaning during comprehension. In-                        (2006). Comparing Human Robot Interaction Scenar-
terestingly, just as Knoeferle and Crocker show that scene                       ios Using Live and Video Based Methods: Towards a
events can have priority over linguistic expectations, we sim-                   Novel Methodological Approach. In Proc. AMC’06,
ilarly find evidence that speaker (robot) gaze can override                      The 9th Int. Workshop on Advanced Motion Control.
                                                                      436

