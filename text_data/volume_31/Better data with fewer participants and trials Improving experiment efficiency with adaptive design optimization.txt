                          Predictions for narrow               Predictions for wide             updated at each stage. At each stage, the information gained
                           range of parameter                  range of parameters              from all prior stages is used to improve the design at the cur-
               100                                   100
                                                                                                rent stage. Thus, the problem to be solved in adaptive design
 Correct responses
                     80                               80
                                                                                                optimization (ADO) is to identify the most informative de-
                     60                               60
                                                                                                sign at each stage of the experiment, taking into account the
                                                                                                results of all previous stages, so that one can infer the un-
      (POW)
                     40                               40
                     20                               20                                        derlying model and its parameter values in as few steps as
                      0                                0                                        possible.
                          0   5     10    15    20         0      5     10    15      20
               100                                   100
                                                                                                   Finding optimal adaptive designs in a general setting re-
 Correct responses
                     80                               80                                        quires simultaneous high-dimensional optimization and inte-
                     60                               60                                        gration. This can be an exteremely difficult task given that
                     40                               40                                        the integration can be intractable for the complex, nonlinear
       (EXP)         20                               20                                        models.
                      0                                0
                          0   5     10     15   20         0      5     10     15     20           Despite these computational challenges, the flexibility and
                                  Lag time                            Lag time
                                                                                                efficiency of adaptive designs have made them popular in var-
                                                                                                ious fields. For example, in astrophysics, ADO has been used
Figure 1: Scatter plot of the probabilities of each number of                                   in the design of experiments to distinguish between dark en-
correct responses, out of 100 Bernoulli trials, predicted by                                    ergy models (Heavens et al., 2007). ADO has also been used
power model (top row) and exponential model (bottom row)                                        in designing phase I and phase II clinical trials to ascertain
for a narrow range of parameters (left column) and a wide                                       the dose-response relationship of experimental drugs (Haines
range of parameters (right column). Darker colors represent                                     et al., 2003; Ding et al., 2008), as well as in estimating psy-
higher probabilities.                                                                           chometric functions (Kujala & Lukka, 2006; Lesmes et al.,
                                                                                                2006). However, because identification of optimal adaptive
                                                                                                designs in general settings was not possible until recently,
                                                                                                most of this prior work focused on problems that were suf-
inspection is meant to illustrate the more complicated prob-
                                                                                                ficiently simple (e.g. linear models with normal error) so that
lems that arises in practice. In actual retention experiments, it
                                                                                                analytical solutions could be found.
would be highly unusual to have enough prior information to
restrict the model parameters to such a small range. When the
                                                                                                   We draw on a recent breakthrough in stochastic Bayesian
parameters are allowed to vary across a more realistic range,
                                                                                                optimization (Müller et al., 2004) in offering a method for
the picture looks more like the second column of Figure 1. In
                                                                                                finding optimal adaptive designs in a general setting, with-
that case, an optimal design for discriminating the two mod-
                                                                                                out relying on approximations nor simplifying assumptions.
els can not be found by visual inspection alone.
                                                                                                The basic idea is to recast the problem as a probability den-
   Given how closely the power and exponential models can                                       sity simulation in which the optimal design corresponds to the
mimic one another when their parameters are allowed to                                          mode of the distribution. This allows one to find the optimal
vary realistically, finding an optimal design for distinguish-                                  design without having to evaluate the integration and opti-
ing them is no easy task. A principled approach to this                                         mization directly. The density is simulated by Markov Chain
challenging problem can be found in Bayesian decision the-                                      Monte-Carlo (Gilks et al., 1996), and the mode is sought by
ory (Chaloner & Verdinelli, 1995). In the Bayesian frame-                                       gradually ”sharpening” the distribution with a simulated an-
work, each possible experimental design is treated as a gam-                                    nealing procedure (Kirkpatrick et al., 1983).
ble whose payoff is determined by the outcome of an exper-
iment carried out with that design. The idea is to estimate                                        In the remainder of the paper, we first give a formal account
the informativeness of hypothetical experiments carried out                                     of the Bayesian framework for ADO, including the compu-
with a given design, so that an ”expected utility” (i.e., infor-                                tational algorithm for finding optimal designs via probabil-
mativeness) of that design can be computed. This is done by                                     ity density simulation, and then we demonstrate the imple-
considering every possible observation that could be obtained                                   mentation of ADO in a computer-simulated retention experi-
from an experiment with that design and then evaluating the                                     ment. Readers interested in only an example of its application
relative likelihoods and utilities of those observations. The                                   should jump to the section ”ADO for discriminating retention
design with the highest expected utility is then chosen as the                                  models.” However, we wish to emphasize that this example is
optimal design.                                                                                 just one of many potential applications of ADO. The compu-
   The Bayesian approach to experimental design optimiza-                                       tational algorithm is very general and works just as well with
tion extends easily to the case of adaptively designed exper-                                   other types of design variables. Readers interested in addi-
iments, in which testing proceeds over the course of several                                    tional applications, such as optimization of categorization ex-
stages (e.g., trials), and the design of the experiment can be                                  periments, should see Myung & Pitt (in press).
                                                                                           94

       Bayesian Adaptive Design Optimization                               stage. The above updating scheme is applied successively
Adaptive design optimization within a Bayesian framework                   on each stage of experimentation, after an initialization with
has been considered at length in the statistics community                  equal model priors p(s=0) (m) = 1/K and a non-informative
(Atkinson & Donev, 1992) as well as in other science and en-               parameter prior p(s=0) (θm ).
gineering disciplines (e.g. El-Gamal & Palfrey, 1996; Bard-
sley et al., 1996; Allen et al., 2003). The issue is essen-
                                                                             Computational Methods: Finding an optimal
tially a Bayesian decision problem where, at each stage s =                           design via probability simulation
(1, 2, . . .) of experimentation, the most informative design at           Given the mutliple computational challenges involved in find-
stage s (i.e., that design with the highest expected utility) is           ing the design d ∗ , standard optimization methods such as
chosen based on the outcomes of the previous experiments                   Newton-Raphson and Revelverg-Marquardt are insufficient.
{y1 , . . . , ys−1 }. The criterion for the informativeness of a de-       However, a promising new approach to this problem has
sign often depends on the goals of the experimenter. The ex-               been proposed in statistics (Müller et al., 2004). It is a
periment which yields the most precise parameter estimates                 simulation-based approach that includes an ingenious com-
may not be the most effective at discriminating among com-                 putational trick that allows one to find the optimal design
peting models, for example (see Nelson, 2005, for a compari-               without having to evaluate the integration and optimization
son of several utility functions that have been used for optimal           directly in Equation (1). The basic idea is to recast the prob-
design of experiments in cognitive science).                               lem as a simulation from a sequence of augmented probability
   Whatever the goals of the experiment may be, solving for                models.
the optimal design is a highly nontrivial problem. Formally,                  To illustrate how it works, let us consider the design opti-
Bayesian ADO entails finding an optimal design ds∗ , at each               mization problem to be solved at any given stage s of experi-
stage s of the experiment, which maximizes a utility function              mentation, and, for simplicity, we will suppress the subscript
Us (d) defined as                                                          s in the remainder of this section. According to the ingenious
                         Z Z                                               trick of Müller et al. (2004), we treat the design d as a ran-
                 K
                                                                           dom variable and define an auxiliary distribution h(d, ·) that
  U(d) =        ∑ p(m)         u(d, θm , y) p(y|θm , d) p(θm ) dy dθm ,
                                                                           admits U(d) as its marginal density. Specifically, assuming
                m=1
                                                                    (1)    that u(d, θm , y) is non-negative and bounded, we define
where m = {1, 2, . . . , K} is one of a set of K models being
                                                                               h(d,y1 , θ1 , . . . , yK , θK ) =
considered, d is a design, θm is a parameterization of model                         "                              #
m, and y is the outcome of an experiment with design d un-                               K
der model m with parameter θm . We refer to the function                           α     ∑ p(m) u(d, θm , ym )        p(y1 , θ1 , . . . , yK , θK |d) (4)
                                                                                        m=1
u(d, θm , y) in Equation (1) as the local utility of the design
d. It measures the utility of a hypothetical experiment carried            where α(> 0) is the normalizing constant of the marginal dis-
out with design d, when the true model is m with parameter                 tribution, and
θm , and the sample y is obtained. Thus, Us (d) represents the                                                        K
expected value of the local utility function, where the expec-                     p(y1 , θ1 , . . . , yK , θK |d) = ∏ p(ym |θm , d)p(θm )
tation is taken over all models under consideration, the full                                                        m=1
parameter space of each model, and all possible outcomes,                  is the joint prior distribution of observations y and parame-
with respect to the model prior probability ps (m), the prior              ters θ. Note that the subscript m in the above equation refers
parameter probability ps (θm ), and the sampling distribution              to model m, not the stage of experimentation. For instance,
function p(y|θm ), respectively.                                           ym denotes an experimental outcome generated from model
   The model and parameter priors are updated at each stage                m with design d and parameter θm . It can be shown that
s = {1, 2, . . .} of experimentation. Specifically, upon the spe-          marginalizing h(d, ·) over {y1 , θ1 , . . . , yK , θK } in Equation (4)
cific outcome zs observed at stage s of an actual experiment               yields h(d) = αU(d). This means that the probability model
carried out with design ds , the model and parameter priors to             h(d) generates designs with probability proportional U(d).
be used to find an optimal design at the next stage are updated            Consequently, the design with the highest utility according to
via Bayes rule and Bayes factor calculation                                U(d) can be found by taking the mode of a sufficiently large
                                                                           sample of designs drawn from the marginal distribution h(d).
                                    p(zs |θm , ds ) ps (θm )
               ps+1 (θm ) =     R                                   (2)       However, finding the global optimum could potentially re-
                                  p(zs |θm , ds ) ps (θm ) dθm             quire a very large number of samples from h(d), especially
                                            p0 (m)                         if there are many local optima or if the design space is very
                ps+1 (m) =        K
                                                                    (3)
                                ∑k=1 p0 (k) BF(k,m) (zs ) ps (θ)           irregular or high-dimensional. To overcome this problem, we
                                                                           augment the auxiliary distribution with independent samples
where BF(k,m) (zs ) denotes the Bayes factor defined as the ra-            of y’s and θ’s given design d as follows:
tio of the marginal likelihood of model k to that of model m
                                                                                                            J
given the realized outcome zs , where the marginal likelihoods                       hJ (d, ·) = αJ ∏ h(d, y1, j , θ1, j , . . . , yK, j , θK, j )    (5)
are computed using the updated priors from the preceding                                                   j=1
                                                                        95

for a positive integer J and αJ (> 0). The marginal distribu-
tion of hJ (·) obtained after integrating out model parameters                                                   1
and outcome variables will then be equal to αJ U(d)J . Hence,                                                   0.9
as J increases, the distribution hJ will become more highly
                                                                                                                0.8
                                                                              Probability of True Model (EXP)
peaked around its (global) mode corresponding to the opti-
mal design d ∗ , thereby making it easier to identify the mode.                                                 0.7
   Following Amzal et al. (2006), we implement a sequential                                                     0.6
Monte Carlo particle filter algorithm that begins by simulat-
ing hJ (d, ·) in Equation (5) for J = 1, and then increases J                                                   0.5
incrementally on subsequent iterations on an appropriate sim-                                                   0.4
ulated annealing schedule (Kirkpatrick et al., 1983; Doucet et
                                                                                                                0.3
al., 2001).
                                                                                                                0.2           Optimal Adaptive Design
    ADO for discriminating retention models                                                                     0.1
                                                                                                                              Random Sequential Design
                                                                                                                              Fixed 10pt Design
We conducted a computer simulation to illustrate the ADO
                                                                                                                 0
procedure, the purpose of which was to design an experi-                                                              0   2            4            6    8
ment to discriminate between the retention models in Table 1.                                                                 Stage of Experiment
The point of the simulated experiment is to demonstrate how
quickly an optimal adaptive experiment can correctly identify               Figure 2: Posterior model probabilities in the simulated ex-
the true model, which in this case was EXP with parameters                  periments.
a = 0.7103 and b = 0.0833.
   The first step in conducting the simulation was to choose
a utility funtion (i.e., measure of model discriminability) that            the correct model with over 0.95 probability after just three
was appropriate for the goal of the experiment. Since the goal              stages or 90 Bernoulli observations.
was to discriminate between models POW and EXP, we chose                       To help illustrate the logic of the ADO procedure in the
to use a utility function based on the entropy of the posterior             simulated experiment, the predictions of the two models at
model probabilities. Specifically, we set the utility of a design           each of the first five stages (trials) are shown in Figure 3. Re-
d to be the mutual information (Cover & Thomas, 1991) be-                   call that our goal in ADO is to choose, at each stage, the lag
tween M and Yd , where M is a random variable representing                  time at which the predictions of the models differ the most,
uncertainty about the true model, and Yd is a random variable               and then update model probabilities based on the relative like-
denoting the result of an experiment carried out with design                lihoods of the observed outcome. For example, in stage 2 of
d.                                                                          the simulated experiment, depicted by the second column of
   The next step was to solve for the optimal design (i.e.,                 graphs in Figure 3, 96.4 seconds was found to be the opti-
the time lags) to use in the initial stage of the experiment,               mal design. It can be seen from the stage-2 scatter plots that
based on the initial (i.e., prior) model and parameter proba-               EXP predicts fewer correct responses than POW at lag times
bilities. We considered designs with one test phase (i.e., recall           around 96.4 seconds. When zero correct responses were ob-
of the study items) after a single time lag, and a fixed bino-              served (an outome that is much more likely under EXP than
mial sample size of n = 30 at each stage of experimentation.                under POW, as indicated by the colors at the tips of the ar-
Using equal prior probabilities on POW and EXP, and inde-                   rows in Figure 3) the odds of EXP are increased from 0.35 to
pendent beta priors on a and b in each model1 , we used the                 0.7 in stage 3 and the odds of POW are decreased from 0.65
just-described algorithm to find the time lag, d1 , that maxi-              to 0.3. As this process continues, the predictions of the mod-
mized U1 (d). We then generated data from the true model at                 els narrow around the observed outcomes and the posterior
t = d1 , and updated the prior probabilities accordingly. These             probability of the true model (EXP) approaches 1.
updated distributions were then used as the priors for finding                 For comparison of ADO against a different sequential de-
the optimal design at the next stage, so that the process could             sign scheme, we also conducted several simulated experi-
be repeated. We continued this process for eight stages of the              ments with randomly generated designs. These randomly de-
experiment. A typical profile of the posterior model proba-                 signed experiments proceeded in the manner described above,
bility ps (EXP) as a function of stage s is shown by the solid              except that the lag time at each stage was chosen randomly
black line in Figure 2. The optimal adaptive design identifies              between zero and 100 seconds. The solid gray line in Figure
                                                                            2 shows a typical posterior model probability curve obtained
    1 We used priors a ∼ Beta(2, 1) and b ∼ Beta(1, 4) for the power        in these experiments. The line shows that the true model
model and a ∼ Beta(2, 1) and b ∼ Beta(1, 80) for the exponential            could be identified with 0.90 probability after 6 stages or 180
model over each model’s parameter space, 0 < a, b < 1. These pri-
ors reflect conventional wisdom about these retention models based          Bernoulli trials; that’s nearly twice as many as were required
on many years of investigation. The choice of priors does indeed            when ADO was used.2
change the optimal solution, but the importance of this example is
the process of finding a solution, not the actual solution itself.                      2 We repeated the simulations several times in order to verify that
                                                                       96

                  Stage 1                   Stage 2                   Stage 3                   Stage 4                 Stage 5
           Optimal design: 16.1s Optimal design: 96.4s Optimal design: 96.8s Optimal design: 4.3s Optimal Design: 59.9s
            Correct responses: 7 Correct responses: 0 Correct responses: 0 Correct responses: 12 Correct responses: 2
          30                       30                        30                        30                      30
                    prPOW)=0.5            pr(POW)=0.65               pr(POW)=0.30             pr(POW)=0.02           pr(POW)<0.001
          20                       20                        20                        20                      20
          10                       10                        10                        10                      10
            0                       0                         0                          0                      0
              0      50      100      0       50       100      0        50        100     0      50      100     0       50     100
          30                       30                        30                        30                      30
                    pr(EXP)=0.5            pr(EXP)=0.35               pr(EXP)=0.70             pr(EXP)=0.98           pr(EXP)>0.999
          20                       20                        20                        20                      20
          10                       10                        10                        10                      10
            0                       0                         0                          0                      0
              0      50      100      0       50       100      0        50        100     0      50      100     0       50     100
                 Lag Time                  Lag Time                  Lag Time                  Lag Time                Lag Time
Figure 3: Predictions of the power and exponential models in the first five stages (i.e., trials) of the simulated experiment. The
predictions are based on the prior parameter estimates at each stage. For example, the stage 2 column of scatter plots depicts
outcome probabilities at each lag time, and for each model, according to parameter estimates that were updated based on the
results observed in stage 1. The text above and inside the graphs provide information about the prior probabilites of each model,
the optimal designs for discriminating the models, and the observed outcomes (correct responses) at each stage of the simulated
experiment. Arrows denote the number of correct responses at the optimal lag time
   To compare ADO against a more principled approach than                      The results of these simulations clearly demonstrate the ad-
randomly selecting lag times at each stage, we conducted ad-                vantage of the optimal adaptive design. In the experiment
ditional simulations using a typical design from the retention              designed with ADO, the true model could be identifed with
literature. While there is no established standard for the set              over 0.95 probability after just three stages or 90 Bernoulli
of lag times to test in actual retention experiments, a few con-            observations. In contrast, the experiments that did not use
ventions have emerged. For one, previous experiments utilize                ADO required twice as many observations (6 stages or 180
what we call ‘fixed designs,’ in which the set of lag times at              Bernoulli trials) to produce a similar level of evidence in fa-
which to assess retention are specified before experimenta-                 vor of the true model. This marked improvement in efficiency
tion begins, and held fixed for the duration of the experiment.             is even more striking in light of the fact that the ADO exper-
Thus, there is no Bayesian updating between stages as there                 iment was optimized over just one of many possible design
would be in a sequential design. The lag times are typically                variables. Even better designs could potentially be obtained
concentrated near zero and spaced roughly geometrically. For                by optimizing over additional design variables, such as the
example, Rubin et al. (1999) used a design consisting of 10                 number of lag times to test at each stage, and the number of
lag times: (0s, 1s, 2s, 4s, 7s, 12s, 21s, 35s, 59s, 99s). To get a          Bernoulli trials to allocate to teach lag time at each stage.
meaningful comparison between this fixed design and the se-                    It is worth noting that the preceding simulations are in-
quential designs that we just tested, we generated data at each             tended only as a proof-of-concept. Obviously, in this simple
stage from the same model as in the previous simulations,                   case, an optimal design could be found via comprehensive
but with just 3 Bernoulli trials at each of the 10 lag times in             grid searches. However, the approach that we have demon-
the Rubin et al. design. That way, each stage of the experi-                strated here generalizes easily to much more complex prob-
ment included the same number of total observations as in the               lems in which brute-force searchers would be impractical or
experiments with just one lag time at each stage. Posterior                 impossible.
model probabilities and parameter estimates were computed
after each stage but not carried over as priors for subsequent                             Other Applications of ADO
stages. The obtained posterior model probabilities from a typ-              The design optimization scheme that we have presented here
ical simulation are shown by the dashed line in Figure 2.                   can be applied in a variety of settings besides the simple re-
the results were not due to random chance. Occasionally the ran-            tention experiment described above. Simply put, any exper-
dom approach stumbled upon a good design and performed very                 iment in which observations can be made sequentially, and
well, and sometimes even an optimal design was hindered by un-
likely experimental outcomes, but the reported results reflect typical      for which the goal is to distinguish among closed form math-
performances of the two designs.                                            ematical models, could potentially benefit from the applica-
                                                                        97

tion of ADO. For a different example, consider the following          ing of design points for model discrimination. Computers
application to an experiment in the field of cognitive develop-       & Chemistry, 20, 145-157.
ment.                                                               Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental
                                                                      design: A review. Statistical Science, 10(3), 273-304.
ADO for Developmental Psychology
                                                                    Cover, T., & Thomas, J. (1991). Elements of information
It is widely believed that the abstract representation of num-        theory. John Wiley & Sons, Inc.
bers by children transitions from a logarithmic scale to a lin-     Ding, M., Rosner, G., & Müller, P. (2008). Bayesian optimal
ear scale as they grow(Opfer & Siegler, 2007). Verifying this         design for phase ii screening trials. Biometrics, 64, 886-
hypothesis leads to the problem of discriminating between             894.
linear and logarithmic models in experiments. Typical exper-
                                                                    Doucet, A., Freitas, N. de, & Gordon, N. (2001). Sequential
iments designed to elicit such numerical representations use
                                                                      monte carlo methods in practice. Springer.
a “number-to-position” task, in which children are shown a
                                                                    El-Gamal, M., & Palfrey, T.(1996). Economical experiments:
number between zero and 1000 and asked to estimate its po-
                                                                      Bayesian efficient experimental design. International Jour-
sition on a line. The choice of numbers to be shown at each
                                                                      nal of Game Theory, 25, 495-517.
stage of the experiment constitutes a design variable that can
be controlled by the experimenter, hence choosing appropri-         Gilks, W. R., Richardson, S., & Spiegelhalter, D. (1996).
ate numbers to show (i.e. designs) in order to quickly and            Markov chain monte carlo in practice. Chapman & Hall.
reliably discriminate linear and log models could be seen as a      Haines, L., Perevozskaya, I., & Rosenberer, W. (2003).
design optimization problem. Simulation results have shown            Bayesian optimal designs for phase i clinical trials. Bio-
that using ADO to find the optimal set of numbers to show             metrics, 59, 591-600.
in each trial greatly reduces the number of trials required to      Heavens, A., Kitching, T., & Verde, L. (2007). On model
reveal the data-generating model (Tang et al., 2009).                 selection forecasting, dark energy and modified grav-
                                                                      ity. Monthly Notices of the Royal Astronomical Society,
                         Conclusion                                   380(3), 1029–1035.
Computational models are precise in their predictions about         Kirkpatrick, S., Gelatt, C., & Vecchi, M.(1983). Optimization
human behavior. Our experimental methods for probing cog-             by simulated annealing. Science, 220, 671–680.
nition to assess the accuracy of their predictions are much less    Kujala, J., & Lukka, T.(2006). Bayesian adaptive estimation:
so. Adaptive design optimization is a tool that can not only          The next dimension. Journal of Mathematical Psychology,
improve the informativeness of an experiment in discriminat-          50(4), 369-389.
ing models, but also increase the efficiency of data collection.    Lesmes, L., Jeon, S.-T., Lu, Z.-L., & Dosher, B. (2006).
We are currently applying the method in experimentation and           Bayesian adaptive estimation of threshold versus contrast
expanding its application to more complex experimental de-            external noise functions: The quick tvc method. Vision Re-
signs and models.                                                     search, 46, 3160-3176.
                                                                    Müller, P., Sanso, B., & De Iorio, M. (2004). Optimal
                    Acknowledgements                                  bayesian design by inhomogeneous markov chain simu-
This research is supported by National Institute of Health            lation. Journal of the American Statistical Association,
Grant R01-MH57472 to JIM and MAP. We wish to thank                    99(467), 788–798.
Hendrik Küeck and Nando de Freitas for valuable feedback           Myung, J. I., & Pitt, M. (in press). Optimal experimental
and technical help provided for the project, and Michael Ros-         design for model discrimination. Psychological Review.
ner for the implementation of the design optimization algo-         Nelson, J. (2005). Finding useful questions: On bayesian di-
rithm in C++.                                                         agnosticity, probability, impact, and information gain. Psy-
                                                                      chological Review, 112(4), 979–999.
                         References                                 Opfer, J., & Siegler, R. (2007). Representational change and
Allen, T., Yu, L., & Schmitz, J. (2003). An experimental de-          children’s numerical estimation. Cognitive Psychology, 55,
   sign criterion for minimizing meta-model prediction errors         165-195.
   applied to die casting process design. Applied Statistics,       Rubin, D., Hinton, S., & Wenzel, A.(1999). The precise time
   52, 103-117.                                                       course of retention. Journal of Experimental Psychology,
Amzal, B., Bois, F., Parent, E., & Robert, C. (2006).                 25(5), 1161–1176.
   Bayesian-optimal design via interacting particle systems.        Rubin, D., & Wenzel, A.(1996). One hundred years of forget-
   Journal of the American Statistical Association, 101(474),         ting: A quantitative description of retention. Psychological
   773–785.                                                           Review, 103(4), 734–760.
Atkinson, A., & Donev, A. (1992). Optimum experimental              Tang, Y., Cavagnaro, D. R., Myung, J. I., & Pitt, M. A.(2009).
   designs. Oxford University Press.                                  Adaptive design optimization for developmental psychol-
Bardsley, W., Wood, R., & Melikhova, E.(1996). Optimal de-            ogy experiments. (Presented at The 42nd Annual Meeting
   sign: A computer program to study the best possible spac-          of the Society for Mathematical Psychology)
                                                                 98

