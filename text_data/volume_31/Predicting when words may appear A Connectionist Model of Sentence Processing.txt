UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Predicting when words may appear: A Connectionist Model of Sentence Processing
Permalink
https://escholarship.org/uc/item/5rh1x6pm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Dennis, Simon
Mehay, Dennis
Yekollu, Srikar
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Predicting when words may appear: A Connectionist Model of Sentence
                                                                  Processing
                                              Simon Dennis (simon.dennis@gmail.com)
                                                 225 Psychology Building, 1835 Neil Avenue
                                                           Columbus, OH 43210 USA
                                             Dennis Mehay (mehay@ling.ohio-state.edu)
                                                       222 Oxley Hall, 1712 Neil Avenue
                                                           Columbus, OH 43210 USA
                                                  Srikar Yekollu (srikary@gmail.com)
                                                 395 Dreese Laboratories, 2015 Neil Avenue
                                                           Columbus, OH 43210 USA
                              Abstract                                    with which to investigate the properties of the general theo-
   We introduce a connectionist version of the Syntagmatic                retical framework, it has proven inadequate in some respects.
   Paradigmatic model (Dennis, 2005) and train it on subcorpora           In particular, it does not scale well to large corpora, primarily
   drawn from the gigaword corpus. Decaying syntagmatic repre-            because it proposes the sentence as the main unit of analysis.
   sentations of the words to the left and right are used to estimate
   the paradigmatic associates of a word. The pattern of paradig-         In this paper, we will present a connectionist version of the
   matic associates is then combined with an order independent            SP model that operates at the word level and demonstrate that
   associative representation of the surrounding words to predict         it is able to to predict when words will be used with some
   the word that will appear in a given slot. The best performing
   version of the model produced a perplexity of 28.3 on a vocab-         precision.
   ulary of 5006 words, significantly lower than Good Turing and
   Kneser Ney ngram models trained under the same conditions.                                          The Model
   Furthermore, we changed parameters and lesioned compo-
   nents to isolate which properties of the model are critical to its
   performance. Online update of the weights - a kind of priming
   - allows the model to track short term contingencies and signif-
   icantly improves performance. When we removed the paradig-
   matic and associative layers performance dropped, when we                                     Output Units
   removed just the associative layer performance dropped and
   when we removed the right context from the syntagmatic and
   associative layers performance also dropped - suggesting that
   all of the hypothesized components of the model are crucial to                Associative Units          Paradigmatic Units
   its performance.
   Keywords: syntagmatic, paradigmatic, sentence processing,
   perplexity, connectionist
                                                                                                Syntagmatic Units       Syntagmatic Units
                          Introduction                                                               (before)                 (after)
Dennis (2005) introduced the Syntagmatic Paradigmatic
model as a general account of verbal cognition. It is based
on the distinction between syntagmatic associations that oc-
cur between words that appear together in utterances (e.g.                        Figure 1: The syntagmatic paradigmatic model
run fast) and paradigmatic associations that occur between
words that appear in similar contexts, but not necessarily in                Our connectionist implementation of the SP model has five
the same utterances (e.g. deep and shallow, cf. Ervin-Tripp,              main sets of units arranged into a two layer architecture. The
1970). The model has been used to explain a number of                     initial syntagmatic inputs consist of two sets of units each of
phenomena including long term grammatical dependencies                    which contain a unit for each word in the vocabulary of the
and systematicity (Dennis, 2005), the extraction of statisti-             system. The syntagmatic units (before) represent the words
cal lexical information (syntactic, semantic and associative)             that appear before a given word within the sentence, while
from corpora (Dennis, 2003a), sentence priming (Harrington                the syntagmatic units (after) represent those words that ap-
& Dennis, 2003), verbal categorization and property judg-                 pear after the word. To capture the temporal order of the
ment tasks (Dennis, 2005), serial recall (Dennis, 2003b), and             words, the activations of these units decay in an exponential
relational extraction and inference (Dennis, 2005, 2004).                 fashion (cf. ordinal models of serial order such as the primacy
   Previous instantiations of the model have used string edit             model, Page & Norris, 1998). If a word is repeated then its
theory (SET) as a way of capturing the notion of paradig-                 new activation is simply added to any existing activation as
matic association. While SET has been a fruitful mechanism                follows:
                                                                      779

                                                                     gary loves bob
                     st,wi ,be f ore = ∑ λe−λ(t−i)           (1)     adam loves eve
                                         i<t                         todd loves sarah
                      st,wi ,a f ter = ∑ λe−λ(i−t)           (2)     barak loves michelle
                                        t<i                          who loves sue ? mike
                                                                     who loves ellen ? bert
where wi is the word that appears in the ith timestep, t is the
                                                                     who loves bob ? gary
current timestep, and λ is the rate of the exponential decay.
                                                                     who loves eve ? adam
This form of representation is efficient, only requiring 2V
                                                                     who loves sarah ? todd
units where V is the size of the vocabulary, generalizes nat-
                                                                     who loves michelle ? barak
urally as the length of dependencies increases and as Ding,
Dennis, and Mehay (2009) argue provides implicit constraints             and then present it with the question “who loves mary ?
that may explain the relative ease of processing of different        xxx”. Any model that relies exclusively on sequential infor-
kinds of embeddings.                                                 mation will be unable to correctly predict that “john” should
   To produce the activations of the paradigmatic units a            fill the “xxx” slot, because “john” only appears at the start of
multinomial logistic regression model is used. The syntag-           a sentence. With the SP model, however, we get the following
matic representations are multiplied by a weight matrix and          paradigmatic patterns for each of the words in the sentence:
the softmax nonlinearity is applied as follows:                      who:       who 92 loves 03 john 01
                                                                     loves: loves 93 who 02
                                          eWtw st
                            pt,w =                           (3)     mary: loves 13 who 13 michelle 10 sarah 10
                                        ∑ j eWt j st                            eve 10 bob 10 ellen 10 sue 10 ? 07
Weights are then updated with a simple gradient descent rule:                   mary 01 barak 01 todd 01 adam 01
                                                                     ?:         ? 37 michelle 07 sarah 07 eve 07
                                                                                bob 07 ellen 07 sue 07 barak 03
            Wt+1, j    = Wt, j + γ(1 − pt, j )st , j = wt                       todd 03 adam 03 gary 03 bert 03
                       = Wt, j − γpt, j st , j 6= wt         (4)     xxx:       barak 09 todd 09 adam 09 gary 09
                                                                                bert 09 mike 09 ? 08 michelle 05
where γ is the learning rate and wt is the word that appears in                 sarah 05 eve 05 bob 05 ellen 05
the corpus at time t.                                                    Note that in the “xxx” slot we get a strong representation
   The paradigmatic layer estimates the probability that a           of the {barak, todd, adam, gary, bert, mike} pattern. This
word could have fit into the slot defined by the surrounding         pattern represents the lover role and a similar paradigmatic
words, that is, it is an estimate of the paradigmatic associates     pattern occurs in the “john” slot when the model is processing
of the current word.                                                 the sentence “john loves mary”:
   The associative layer codes adjacent words in an order in-
dependent fashion and allows the model to capture relational         john:      who 29 john 19 loves 06 barak 05
regularities (e.g. the active to passive transform). The asso-                  todd 05 adam 05 gary 05 bert 05 mike 05
ciative inputs are coded as follows:                                 loves: loves 42 john 10 mary 10
                                                                     mary: mary 17 ? 11 michelle 08 sarah 08
                         at,wi = ∑ λe−λ|i−t|                 (5)                eve 08 bob 08 ellen 08 sue 08
                                     t6=i
                                                                         As a consequence, paradigmatic associations form be-
   Finally, the output units are again calculated using the          tween the {barak, todd, adam, gary, bert, mike} pattern
multinomial logistic regression model (see Equation 3) and           and “john”. The paradigmatic mechanism in itself, how-
the same gradient descent training rule is applied (see Equa-        ever, would not suffice to predict “john”, as “barak”, “todd”,
tion 4). Although the architecture has two layers these are          “adam”, “gary”, “bert” and “mike” are also associated with
trained independently. There is no backpropagation of error.         the lover pattern. Only “john” has an associative connection
Furthermore, in all of the simulations reported in this paper a      to “mary”, however, and the additional support afforded by
single iteration through the training set was performed.             this connection favors “john”. At the output layer, we get the
                                                                     following patterns:
                     A Simple Example
                                                                     who:       who 97 john 03
To understand the logic of the model, consider the following
                                                                     loves: loves 100
simple example. Suppose we trained the model (λ = 0.1, γ =
                                                                     mary: michelle 11 sarah 11 eve 11 bob 11
0.1, 2000 iterations) on the following corpus:
                                                                                ellen 11 sue 11 barak 05 todd 05
john loves mary                                                                 adam 05 gary 05 bert 05 mike 05
mike loves sue                                                       ?:         ? 92 john 04
bert loves ellen                                                     xxx:       john 28 barak 06 todd 06 adam 06
                                                                 780

         gary 06 bert 06 mike 06 sue 05
         ellen 05 bob 05 eve 05 sarah 05
                                                                                       600
   The model approximates a propositional representation in
                                                                                                                                                              with OOV
an associative form that does not rely on the creation of inde-                        500                                                                    without OOV
pendent propositional units of representation. Dennis (2004)
shows that these representations can be used to answer sim-                            400
ple questions about tennis matches. Taking naturally occur-
                                                                          Perplexity
                                                                                       300
ring text from the Association of Tennis Professionals web-
site, the model was able to complete questions of the form
                                                                                       200
“Who won the match between Sampras and Agassi? xxx”.
Particularly interesting was the fact that the model took ad-
                                                                                       100
vantage of the systematic occurrence of tokens through the
corpus as a consequence of the causal relationships between                            0
events. It implements a kind of “inference by coincidence”                                   Syntagmatic
                                                                                             Paradigmatic
                                                                                                            Good Turing
                                                                                                             Trigram
                                                                                                                          Good Turing
                                                                                                                            Bigram
                                                                                                                                        Good Turing
                                                                                                                                         Unigram
                                                                                                                                                      Kneser Ney
                                                                                                                                                       Trigram
                                                                                                                                                                   Kneser Ney
                                                                                                                                                                     Bigram
                                                                                                                                                                                Kneser Ney
                                                                                                                                                                                 Unigram
to determine results even when they are not explicitly stated
(Dennis, 2004).
   While such results may be of theoretical interest, the ques-
tion remains as to whether the model can capture naturally                Figure 2: Performance of the SP model as compared against
occurring data. In the following sections, we test the model              unigram, bigram and trigram models using both Good Turing
and investigate which components are crucial to its perfor-               and Kneser Ney smoothing.
mance.
                            Results                                       field ngram models have been shown to produce robust per-
In the current paper, we focus on the models ability to predict           formance and so we will compare the SP model against ngram
which word will appear in a given sentential context. We                  models using two commonly used and successful smoothing
will use perplexity to quantify the performance of the model.             techniques - Good Turing and Kneser Ney (Chen & Good-
Perplexity is an information theoretic measure of the degree              man, 1998). Our ngram modelling results were generated us-
of uncertainty of a given choice. Formally, it is two to the              ing the SRILM toolkit (Stolcke, 2002).
power of the cross entropy of the model with respect to the                  Unless otherwise noted, the training set consisted of
empirical distribution of the data, which can be estimated as             100,000 sentences of the English gigaword corpus sections
follows:                                                                  NYT200001, NYT200002 and NYT200003 (2.4 million
                                                                          words, Graff, Kong, Chen, & Maeda, 2007) and the test set
                              1   N
                     P = 2− N ∑k=0 log2 p(wk )               (6)          of 1000 sentences (23611 words). In our first set of simula-
                                                                          tions, we trained the model using a learning rate γ of 1 and
   where p(wk ) is the probability according to the model of              a decay rate λ of 1. The paradigmatic layer was restricted to
the kth word and N is the size of the sample. To provide an               the 200 most frequent words and all weights were restricted
intuition, a perplexity of x is the degree of uncertainty that            to lie between 10 and -10. The vocabulary of the model (and
exists when faced with x equally probable and independent                 hence the size of the two syntagmatic banks of units, the as-
alternatives.                                                             sociative bank and the output bank) was restricted to the 5006
   In typical uses of the perplexity measure one is interested in         most frequent tokens. Any tokens that occurred that were not
calculating the mean information content of an entire corpus.             within the 5006 most frequent, were represented as an out
That is, one is interested in the probability of the sequence             of vocabulary (OOV) item (xxx). Perplexity can be affected
w1 , w2 , ...wn . In that case, it is common to decompose this            by the inclusion or exclusion of these tokens, so we report
probability using the chain rule. When one does this only left            performance in both conditions. When referring to perplex-
or right context can be used, but not both. In our case, we               ity values in text, we will adopt the convention of reporting
are interested in the information content of individual word              the perplexity with OOV tokens followed by the perplexity
choices given both left and right context. The same formula               without OOV items in brackets.
applies, but the perplexity values that we report are not di-                Figure 2 shows the perplexity of the SP model as com-
rectly comparable to per word corpus perplexity values that               pared against unigram, bigram and trigram; Good Turing and
appear in the literature.                                                 Kneser Ney models. The SP model performs significantly
                                                                          better in all cases.
Comparison with Ngram Models                                                 The size of the training corpus has a significant affect
Driven by the need to create language models for speech                   on performance. Figure 3 shows the performance of the
recognition and other tasks, much of the work on predict-                 model and the trigram Good Turing and Kneser Ney models
ing words occurs in computational linguistics. Within this                when the training set contained 500,000 sentences (12 mil-
                                                                    781

             100
                                with OOV                                                          70                                                SP with OOV
                                without OOV                                                                                                         SP without OOV
                                                                                                                                                    Ngram with OOV
                                                                                                  60
             80                                                                                                                                     Ngram without OOV
                                                                                                  50
             60
Perplexity                                                                           Perplexity
                                                                                                  40
                                                                                                  30
             40
                                                                                                  20
             20
                                                                                                  10
                                                                                                  0
             0
                      SP           SP         Good Turing   Kneser Ney
                   On the Fly   Train/Test     Trigram       Trigram                                    0e+00    1e+05         2e+05       3e+05      4e+05       5e+05
                                                                                                                              Number of Sentences
Figure 3: Performance of the models given a 500,000 sen-                       Figure 4: Performance of the SP model compared against that
tence (12 million word) training corpus.                                       of the Ngram Features model.
lion words, on the fly values will be explained in the next sec-               formance of the models, we examined the tokens that were
tion). The best performance of the SP model was a perplexity                   strongly predicted by one model, but not the other. Table
of 28.31 (35.61) with a training set of 27 million words.                      shows the most frequent tokens that were well predicted by
   To the best of our knowledge this is state of the art for this              the SP model and by the ngram model, respectively. The SP
task, and suggests that the model is capturing important reg-                  model tends to do a better job of predicting high frequency to-
ularities to which ngram models are insensitive. However, it                   kens, while the ngram model is doing better on low frequency
may be argued that left context only ngram models are un-                      tokens.
fairly compromised in this setting because they do not have                    The Importance of Priming
access to the right context. Typically, these models would be
used to assess the probability of a string of words. An incon-
sistent right context would then manifest in poor prediction
of subsequent words.                                                                              100
   To provide a fairer test, we constructed a multinomial lo-
                                                                                                                             with OOV
gistic regression model that employed bigram and trigram                                          80                         without OOV
features generated from the left and right context. So,
for instance, when the model was exposed to the sequence
                                                                                                  60
w1 , w2 , w3 , w4 , w5 and we were trying to predict w3 then
we would use the features w2 −, −w4 , w1 w2 −, −w4 w5 and                            Perplexity
w2 − w4 .                                                                                         40
   The number of features generated by the ngram model was
very large. Consequently, we restricted the vocabulary to                                         20
1000 tokens and trained on the 500,000 sentence corpus used
above. Under these conditions, the ngram model had a total                                        0
of 457549 features and was approximately 285 times as large                                                     On the Fly                           Train/Test
as the SP model. Nevertheless, the SP model performed bet-
ter although the difference was not large. The SP model pro-
duced a perplexity of 15.75 (29.93), while the ngram model
produced a perplexity of 17.75 (31.91).                                        Figure 5: Performance measured as training progressed ver-
   Figure 4 shows the performance of the SP and ngram mod-                     sus in train and test mode. The superior performance of on-
els as a function of training examples. The performance of the                 line training suggests that priming plays an important role.
ngram model improves quickly, but is eventually matched and
then surpassed by the SP model. Also, it is quite noticeable                      As training progresses through the corpus, the weights cap-
that the variance of the ngram model is significantly higher                   ture long term regularities, but also track short term contin-
than that of the SP model. To understand the relative per-                     gencies that one might think of as a form of priming. To
                                                                         782

             SP Model           Ngram Model
           Count Token        Count Token
             308 ,               33 )
                                                                                            250
             166 the             24 bush                                                                  ●   On the Fly with OOV
                                                                                                          ●   On the Fly without OOV
             142 .               19 “                                                                         Train/Test with OOV
                                                                                            200               Train/Test without OOV
              74 of              17 .
              41 to              16 a
                                                                                            150
                                                                               Perplexity
              31 ”               14 angeles
              29 a               11 :
                                                                                            100
              28 said            11 ,
              12 years           10 spokesman                                               50
                                                                                                  ●
                                                                                                      ●                 ●
                                                                                                                                               ●
                                                                                                  ●                                            ●
              12 n’t              9 use                                                               ●                 ●
              10 ’s               9 and                                                     0
               9 new              9 (
                                                                                                  1   2         3       4       5      6   7   8
               9 i                8 secretary
                                                                                                                      Learning Rate
               8 who              8 p.m.
               8 in               7 year
               6 more             7 very
               5 his              7 texas                                 Figure 6: Performance as the learning rate is manipulated.
Table 1: Tokens well predicted by one of the models and not
the other. Counts are derived by sorting the tokens by the dif-          iteration. Performance does not change substantially, but it
ferences between the probability predicted by the SP model               appears that between 100 and 200 paradigmatic units is opti-
and Ngram models. The tokens corresponding to the top 1000               mal. We suspect that these most frequent words fulfil a role
of these differences were then tabulated.                                somewhat like parts of speech.
                                                                         Adding a Second Bank of Syntagmatic Units
determine the potential significance of priming in the model,            Ding et al. (2009) demonstrate that the syntagmatic (before)
perplexity was calculated as training progressed. Note that              input representations as defined above have a number of in-
because training consisted of a single iteration through the             teresting properties with respect to the kinds of embedding
training set, each prediction was of an unseen word. Figure              they can support. In particular, they found that with a sin-
5 shows the results on the final 50,000 tokens of the training           gle bank of units a single level of center embedding can be
set compared against the train/test results from the previous            correctly predicted, but a single level of cross serial embed-
section in which weights were fixed during testing. Clearly,             ding cannot - that is the patterns are not linearly separable.
priming of this kind significantly improves the performance              The single bank model is not able to account for two levels
of the model.                                                            of embedding of either the center or cross serial kinds. With
   The trade off between retaining long term regularities                two banks of units employing different decay rates, however,
while tracking short term contingencies can be seen in the               cross serial and center embedding of one and two levels is
performance of the model as the learning rate is manipulated.            possible. Based on these results, they argued that a two bank
With high learning rates recent information is emphasized,               model best approximates human capabilities.
while with lower learning rates long term information pre-                  Figure 7 shows the results as the decay rate of a second
dominates. Figure 6 shows that there appears to be an optimal            bank of syntagmatic units varies from 0 (no second bank) to
tradeoff at around γ = 4 for this corpus. Because the train/test         0.1. The ability of the model to predict which word can fill a
methodology can only take advantage of long term regulari-               given slot is affected very little by the inclusion of the addi-
ties, performance gets worse as the learning rate increases.             tional bank.
Size of the Paradigmatic Layer                                           Lesioning the Model
In the simple example outlined above, all words in the vocab-            While the paradigmatic and associative banks of units play a
ulary appeared in the paradigmatic representation. In order              central role in the logic of the model, to what extent do they
to make the model more computationally efficient, we inves-              really aid in prediction? To determine this, we created a ver-
tigated the impact of reducing the size of this layer signifi-           sion of the model which used the syntagmatic banks (before
cantly. Instead of taking the entire vocabulary of 5006 words,           and after) to predict words directly. Perplexity rises substan-
we kept just the most frequent words. If a word did not ap-              tially, from 45.46 (60.90) to 70.69 (99.46). Next, we lesioned
pear in this set no learning on the first layer of weights was           just the associative layer, leaving the paradigmatic layer in-
conducted. Note the number of output units remained the                  tact. Again performance was significantly impacted rising to
same and the second layer of weights was trained on every                90.60 (139.23). Finally, we lesioned the right context both
                                                                   783

                                                                                        decay rate had little impact as did changing the size of the
                                                                                        paradigmatic layer - at least over the range that we manipu-
                   100                                                                  lated it.
                                                                                           We believe that these results provide prima facie evidence
                   80                                                                   for the model and suggest that there may be significant ad-
                                                                                        vantage in applied domains to considering the cognitive con-
                   60     ●     ●                                                       straints on the sentence processing mechanism.
      Perplexity
                                                                            ●
                   40
                          ●     ●
                                                                            ●                             Acknowledgments
                                                                                        We would like to acknowledge the support of Defense Re-
                                           On the Fly with OOV
                   20
                                       ●
                                       ●   On the Fly without OOV
                                                                                        search and Development Canada contract number W7711-
                                           Train/Test with OOV
                                           Train/Test without OOV
                                                                                        067985. We would also like to thank Chris Brew and Eric
                   0
                                                                                        Fosler-Lussier for their feedback on this work.
                         0.00       0.02       0.04       0.06      0.08   0.10
                                                                                                               References
                                           Second Bank Decay Rate
                                                                                        Chen, S. F., & Goodman, J. (1998). An empirical study of
                                                                                          smoothing techniques for language modeling (Tech. Rep.
                                                                                          No. TR-10-98). Cambridge, MA: Harvard.
Figure 7: Performance when two banks of syntagmatic units
                                                                                        Dennis, S. (2003a). An alignment-based account of serial
with different decay rates are used. In each case, the decay
                                                                                          recall. In R. Alterman & D. Kirsh (Eds.), Twenty fifth Con-
rate of the first bank was 1.0.
                                                                                          ference of the Cognitive Science Society (Vol. 25). Boston,
                                                                                          MA: Lawrence Erlbaum Associates.
at the syntagmatic layer and the associative layer. Perplexity                          Dennis, S. (2003b). A comparison of statistical models for
rose to 107.58 (160.67), demonstrating that the right context                             the extraction of lexical information from text corpora. In
makes a significant contribution to the ability of the model to                           R. Alterman & D. Kirsh (Eds.), Twenty fifth Conference
predict correctly.                                                                        of the Cognitive Science Society (Vol. 25). Boston, MA:
                                                                                          Lawrence Erlbaum Associates.
                         Discussion and Conclusions                                     Dennis, S. (2004). An unsupervised method for the extraction
                                                                                          of propositional information from text. Proceedings of the
The SP model was initially developed as framework for un-                                 National Academy of Sciences, 101, 5206-5213.
derstanding verbal cognition (Dennis, 2004, 2005). By cre-                              Dennis, S. (2005). A memory-based theory of verbal cogni-
ating a connectionist implementation, we have been able to                                tion. Cognitive Science, 29(2), 145-193.
scale the model to large corpora, 27 million words of natu-                             Ding, L., Dennis, S., & Mehay, D. (2009). A single layer net-
rally occurring text from the gigaword corpus, with a reason-                             work model of sentential recursive patterns. In Proceedings
ably large vocabulary of 5006 words. With the model trained                               of the Cognitive Science Society.
on this corpora we were able to achieve a perplexity of 28.3.                           Ervin-Tripp, S. M. (1970). Substitution, context and associ-
This result is a significant improvement on Good Turing and                               ation. In L. Postman & G. Keppel (Eds.), Norms of word
Kneser Ney ngram models on the same task and to the best of                               association (p. 383-467). New York: Academic Press.
our knowledge represents the state of the art. When an ngram                            Graff, D., Kong, J., Chen, K., & Maeda, K. (2007). English
features model was trained using left and right context, the                              gigaword third edition. Linguistic Data Consortium.
advantage for the SP model was retained, although it was not                            Harrington, M., & Dennis, S. (2003). Structural priming
as large.                                                                                 in sentence comprehension. In R. Alterman & D. Kirsh
   Furthermore, by observing as we changed parameters and                                 (Eds.), Twenty fifth Conference of the Cognitive Science
lesioned components, we have been able to isolate which                                   Society (Vol. 25). Boston, MA: Lawrence Erlbaum Asso-
properties of the model are critical to its performance. The                              ciates.
first lesson is that priming is important. Online update of the                         Page, M., & Norris, D. (1998). The primacy model: A new
weights - a kind of priming - allows the model to track short                             model of immediate serial recall. Psychological Review,
term contingencies. When we allowed weights to update as                                  105, 761-781.
we calculated the perplexity performance improved signifi-                              Stolcke, A. (2002, September). SRILM - an Extensible Lan-
cantly. Secondly, it appears that all of the component layers                             guage Modeling Toolkit. In Proceedings of the Interna-
make a substantial contribution to performance. When we                                   tional Conference on Spoken Language Processing. Den-
removed the top layer performance dropped. When we re-                                    ver, Colorado.
moved the associative layer performance dropped and when
we removed the right context from the syntagmatic layer
and associative layer performance also dropped. By contrast
adding a second bank of syntagmatic inputs with a different
                                                                                  784

