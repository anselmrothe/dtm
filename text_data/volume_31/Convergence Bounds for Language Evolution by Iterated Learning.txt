UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Convergence Bounds for Language Evolution by Iterated Learning
Permalink
https://escholarship.org/uc/item/692725p3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Griffiths, Thomas
Klein, Dan
Rafferty, Anna
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Convergence Bounds for Language Evolution by Iterated Learning
                                            Anna N. Rafferty (rafferty@cs.berkeley.edu)
                            Computer Science Division, University of California, Berkeley, CA 94720 USA
                                         Thomas L. Griffiths (tom griffiths@berkeley.edu)
                            Department of Psychology, University of California, Berkeley, CA 94720 USA
                                                  Dan Klein (klein@cs.berkeley.edu)
                            Computer Science Division, University of California, Berkeley, CA 94720 USA
                               Abstract                                  the sole factor influencing the languages learners acquire.
   Similarities between human languages are often taken as ev-           Our key contribution is providing bounds on the number of
   idence of constraints on language learning. However, such             generations required for convergence, known as the conver-
   similarities could also be the result of descent from a com-          gence time, which we obtain by analyzing Markov chains as-
   mon ancestor. In the framework of iterated learning, language
   evolution converges to an equilibrium that is independent of its      sociated with iterated learning. Bounding the convergence
   starting point, with the effect of shared ancestry decaying over      time is a step towards understanding the source of linguistic
   time. Therefore, the central question is the rate of this conver-     universals: If convergence occurs in relatively few genera-
   gence, which we formally analyze here. We show that conver-
   gence occurs in a number of generations that is O(n log n) for        tions, it suggests constraints on learning are more likely than
   Bayesian learning of the ranking of n constraints or the values       common descent to be responsible for linguistic universals.
   of n binary parameters. We also present simulations confirm-             To bound the number of generations required for iterated
   ing this result and indicating how convergence is affected by
   the entropy of the prior distribution over languages.                 learning to converge, we need to make some assumptions
                                                                         about the algorithms and representations used by learners.
                           Introduction                                  Following previous analyses (Griffiths & Kalish, 2007), we
Human languages share a surprising number of properties,                 assume that learners update their beliefs about the plausibil-
ranging from high level characteristics like compositional               ity of a set of linguistic hypotheses using Bayesian inference.
mapping between sound and meaning to relatively low-                     We outline how this approach can be applied using two kinds
level syntactic regularities (Comrie, 1981; Greenberg, 1963;             of hypothesis spaces that appear in prominent formal linguis-
Hawkins, 1988). One explanation for these universal proper-              tic theories: constraint rankings, as used in Optimality Theory
ties is that they reflect constraints on human language learn-           (Prince & Smolensky, 2004), and vectors of binary parame-
ing, with the mechanisms by which we acquire language be-                ter values, consistent with a simple Principles and Parameters
ing restricted to languages with these properties (e.g., Chom-           model (Chomsky & Lasnik, 1993). In each case, we show
sky, 1965). However, if all modern languages are descended               that iterated learning with a uniform prior reaches equilib-
from a common ancestor, these similarities could just reflect            rium after O(n log n) generations, where n is the number of
the properties of that ancestor. Evaluating these different pos-         constraints or parameters.
sibilities requires establishing how constraints on learning in-
fluence the properties of languages, and how long it takes for
                                                                                       Analyzing Iterated Learning
this process to remove the influence of a common ancestor. In            Iterated learning has been used to model a variety of aspects
this paper, we explore these questions using a simple model              of language evolution, providing a simple way to explore the
of language evolution.                                                   effects of cultural transmission on the structure of languages
   We model language evolution as a process of iterated                  (Kirby, 2001; Smith, Kirby, & Brighton, 2003). The basic
learning (Kirby, 2001). This model assumes that each gen-                assumption behind the model – that each learner learns from
eration of people learns language from utterances generated              somebody who was themselves a learner – captures a phe-
by the previous generation. While this model makes certain               nomenon we see in nature: Parents pass on language to their
simplifying assumptions, such as a lack of interaction be-               children, and these children in turn pass on language to their
tween learners in the same generation, it has the advantage              own children. The sounds that the children hear are the in-
that it can be analyzed mathematically. Previous research has            put, and the child produces language (creates output) based
shown that after some number of generations, the distribution            on this input, as well as prior constraints on the form of the
over languages produced by learners converges to an equilib-             language.
rium that reflects the constraints that guide learning (Griffiths           Formally, we conceptualize iterated learning as follows
& Kalish, 2007). After convergence, the behavior of learners             (see Figure 1). A first learner receives data, forms a hypothe-
is independent of the language spoken by the first generation.           sis about the process that generated these data, and then pro-
   These results provide a way to relate constraints on learn-           duces output based on this hypothesis. A second learner re-
ing to linguistic universals. However, convergence to the                ceives the output of the first learner as data and produces a
equilibrium has to occur in order for these constraints to be            new output that is in turn provided as data to a third learner.
                                                                     2451

This process may continue indefinitely, with the t th learner re-          (a)               hypothesis                 hypothesis
ceiving the output of the (t − 1)th learner. The iterated learn-
ing models we analyze make the simplifying assumptions                            data                       data                       data       ...
that language evolution occurs in only one direction (previ-
ous generations do not change their hypotheses based on the
data produced by future generations) and that each learner re-
ceives input from only one previous learner. We first charac-
                                                                           (b)    d0
                                                                                       p(h|d)
                                                                                               h1
                                                                                                    p(d|h)
                                                                                                              d1
                                                                                                                  p(h|d)
                                                                                                                          h2
                                                                                                                               p(d|h)
                                                                                                                                         d2        ...
terize how learning occurs, independent of specific represen-                                           ∑d p(h|d)p(d|h)            ∑d p(h|d)p(d|h)
tation, and then give a more detailed description of the form
                                                                           (c)                 h1                         h2                       ...
of these hypotheses and data.                                           Figure 1: Language evolution by iterated learning. (a) Each
   Our models assume that learners represent (or act as if they         learner sees data, forms a hypothesis, and generates the data
represent) the degree to which constraints predispose them to           provided to the next learner. (b) The underlying stochastic
certain hypotheses about language through a probability dis-            process, with dt and ht being the data generated by the tth
tribution over hypotheses, and that they combine these pre-             learner and the hypothesis selected by that learner respec-
dispositions with information from the data using Bayesian              tively. (c) We consider the Markov chain over hypotheses
inference. Starting with a prior distribution over hypotheses           formed by summing over the data variables. All learners
p(h) for all hypotheses h in a hypothesis space H, the pos-             share the same prior p(h), and each learner assumes the input
terior distribution over hypotheses given data d is given by            data were created using the same p(d|h).
Bayes’ rule,
                                       p(d|h)p(h)                       Markov chains. In particular, Markov chains can converge to
                       p(h|d) =                                  (1)
                                  ∑h0 ∈H p(d|h0 )p(h0 )                 a stationary distribution, meaning that after some number of
                                                                        generations t, the marginal probability that a variable Xt takes
where the likelihood p(d|h) indicates the probability of see-
                                                                        value xt becomes fixed and independent of the value of the
ing d under hypothesis h. The learners thus shape the lan-
                                                                        first variable in the chain (Norris, 1997). Intuitively, the sta-
guage they are learning through their own biases in the form
                                                                        tionary distribution is a distribution over states in which the
of the prior probabilities: the prior p(h) incorporates the hu-
                                                                        probability of each state is not affected by further iterations
man learning constraints. These probabilities might, for ex-
                                                                        of the Markov chain; in our case, the probability that a learner
ample, tend to favor lword forms with alternating consonant-
                                                                        learns a specific grammar at time t is equal to the probability
vowel phonemes. We assume that learners’ expectations
                                                                        of any future learner learning that grammar. The stationary
about the distribution of the data given the hypothesis are
                                                                        distribution is thus an equilibrium state that iterated learn-
consistent with the actual distribution (i.e. that the probabil-
                                                                        ing will eventually reach, regardless of the hypothesis of the
ity of the previous learner generating data d from hypothesis
                                                                        first ancestral learner, provided simple technical conditions
h matches the likelihood function p(d|h)). Finally, we as-
                                                                        are satisfied (see Griffiths & Kalish, 2007, for details).
sume that learners choose a hypothesis by sampling from the
posterior distribution (although we consider other ways of se-             Previous work has shown that the stationary distribution
lecting hypotheses in the Discussion section).1                         of the Markov chain defined by Bayesian learners sampling
   The analyses we present in this paper are based on the ob-           from the posterior is the learners’ prior distribution over hy-
servation that iterated learning defines a Markov chain. A              potheses, p(h) (Griffiths & Kalish, 2007). These results illus-
Markov chain is a sequence of random variables Xt such that             trate how constraints on learning can influence the languages
each Xt is independent of all preceding variables when condi-           that people come to speak, indicating that it is possible for
tioned on the immediately preceding variable, Xt−1 . Thus,              iterated learning to converge to an equilibrium that is deter-
p(xt |x1 , . . . , xt−1 ) = p(xt |xt−1 ). There are several ways of     mined by these constraints and independent of the language
reducing iterated learning to a Markov chain (Griffiths &               spoken by the first learner in the chain.
Kalish, 2007). We will focus on the Markov chain on hy-                    However, characterizing the stationary distribution of iter-
potheses, where transitions from one state to another occur             ated learning still leaves open the question of whether enough
each generation: the t th learner assumes the data were gen-            generations of learning have occurred for convergence to this
erated by ht , where these data are dependent only on the               distribution to have taken place in human languages. To un-
hypothesis ht−1 chosen by the previous learner. The transi-             derstand the degree to which linguistic universals reflect con-
tion probabilities for this Markov chain are obtained by sum-           straints on learning rather than descent from a common ances-
ming over the data from the previous time step di−1 , with              tor, it is necessary to establish bounds on convergence time.
p(ht |ht−1 ) = ∑di−1 p(ht |di−1 )p(di−1 |ht−1 ) (see Figure 1).         Previous work has identified factors influencing the rate of
   Identifying iterated learning as a Markov chain allows us to         convergence in very simple settings (e.g., Griffiths & Kalish,
draw on mathematical results concerning the convergence of              2007). Our contribution is to provide analytic upper bounds
    1 Note that these various probabilities form our model of the       on the convergence time of iterated learning with relatively
learners. Learners need not actually hold them explicitly, nor per-     complex representations of the structure of a language that
form the exact computations, provided that they act as if they do.      are consistent with linguistic theories.
                                                                    2452

             Bayesian Language Learning                                      (a)                             (b)
                                                                                 ht-1   dt-1      ht             ht-1   dt-1             ht
Defining a Bayesian model of language learning requires
                                                                                                  0                                      A
choosing a representation of the structure of a language. In                                                                             B
                                                                                                  0
this section, we outline Bayesian models of language learning                     0      0        0               A                B     C
                                                                                                                         A
compatible with two formal theories of linguistic representa-                     0      0                        B                A
                                                                                                                         C
                                                                                  0      ?        0               C                C     A
tion: Principles and Parameters (Chomsky & Lasnik, 1993),
                                                                                                  0                                      C
and Optimality Theory (Prince & Smolensky, 2004).                                                 1                                      B
Principles and Parameters
                                                                          Figure 2: Bayesian language learning. (a) Representation of
The Principles and Parameters framework postulates that all               a hypothesis and data item for Principles and Parameters. On
languages obey a finite set of principles, with specific lan-             the left is a possible hypothesis for n = 3; the center shows
guages defined by setting the values of a finite set of parame-           a possible data output derived from this hypothesis (with
ters (Chomsky & Lasnik, 1993). For example, one parameter                 m = 1), and the right shows all hypotheses consistent with
might encode the head directionality of the language (with                this data output. (b) A similar representation for OT. The rel-
the values indicating left- or right-headedness), while another           ative ordering of A and C is preserved in the data, but not the
might encode whether covert subjects are permitted. For sim-              ordering of B.
plicity, we will assume that parameters are binary. Learning
a language is learning the settings for these parameters. In              and denominator and the posterior is the prior renormalized
reality, learning is not an instantaneous process. Learners are           over the set of consistent hypotheses. For a uniform prior,
presented with a series of examples from the target language              the posterior probability of a consistent hypothesis is simply
and may change their parameters after each example. The ex-               the reciprocal of the number of consistent hypotheses. In the
act model of learning varies based on assumptions about the               uniform case, 2m of our hypotheses are valid, so p(h|d) = 21m .
learners’ behavior (e.g., Gibson & Wexler, 1994). However,
in this work, we do not model this fine-grained process, but              Optimality Theory
rather lump acquisition into a single computation, wherein a              In Optimality Theory (OT), learning a language is learn-
single hypothesis h is selected on the basis of a single data             ing the rankings of various constraints (Prince & Smolen-
representation d.                                                         sky, 2004; McCarthy, 2004). These constraints are univer-
   To specify a Bayesian learner for this setting, we define a            sal across languages and encode linguistic properties. For
hypothesis space H, a data representation space D, a prior dis-           example, one constraint might encode that high vowels fol-
tribution p(h), and a likelihood p(d|h). Our hypothesis space             low high vowels. Whether a construction is well-formed in
is composed of all binary vectors of length n: H = {0, 1}n .              a language is based on the ranking of the constraints that the
We represent the data space as strings in {0, 1, ?}n in which             construction violates. Specifically, well-formed constructions
0 and 1 indicate the values of parameters that are fully deter-           are those that violate the lowest-ranked constraints. Produc-
mined by the evidence and question marks indicate underde-                ing well-formed constructions thus requires determining how
termined parameters. For now, we assume a uniform prior,                  constraints are ranked in the target language.
with p(h) = 1/2n for all h ∈ H. To define the likelihood, we                  To specify a Bayesian learner, we again need to identify the
assume the data given to each generation fully specify all but            hypothesis space H, data space D, prior p(h), and likelihood
m of the n parameters, with the m unknown parameters cho-                 p(d|h). In the OT case, each hypothesis is an ordered list of
sen uniformly at random without replacement. Then, p(d|h)                 n constraints, with the order of constraints representing rank.
is zero for all strings d with a 0 or 1 not matching the binary           The hypothesis space H is thus the symmetric group of per-
vector h or that do not have exactly m question marks (i.e.               mutations of rank n, Sn , and is of size n!. We assume learners
those consistent with h). Moreover, we assume that       p(d|h) is      see sufficient data to specify the relative ordering of all but m
                                                         n                constraints. The data space is then strings over {1, 2, . . . n} of
equal for all strings consistent with h. There are           strings
                                                         m                length n − m, with no repeated elements, ordered from left-
consistent with any hypothesis, so p(d|h) = m!(n−m)!  n!    for all d     to-right in order of precedence (see Figure 2). The relative
consistent with h (see Figure 2).                                         ordering of the n − m specified constraints is maintained ex-
   Applying Bayes’ rule (Equation 1) with this hypothesis                 actly from the generating hypothesis. We again see that the
space and likelihood, the posterior distribution is                       likelihood, p(d|h), is 0 for all orderings not consistent with
                        (                                                 our hypothesis and equal for all consistent orderings. Analo-
                                 p(h)                                     gously to the previous case, we select
             p(h|d) =      ∑h0 :h0 `d p(h0 )
                                               h`d
                                                                  (2)                                            mconstraints to remove
                                   0         otherwise                                                            n
                                                                          from the ranking randomly, giving           possible data strings
                                                                                                                  m
where h ` d indicates that h is consistent with d. This follows           for each hypothesis. This gives p(d|h) = m!(n−m)!
                                                                                                                          n!   . Thus, the
from the fact that p(d|h) is constant for all h such that h `             posterior is the same as that in Equation 2. Since we can
                                                                                                                              n!
d, meaning that the likelihood cancels from the numerator                 freely permute m of our parameters, we have (n−m)!        consis-
                                                                      2453

                                                                       ment. A sufficient condition for convergence using the previ-
   (a)                                  (b)                  BC
                                                           A           ously defined prior is that all parameters have been sampled
           000         010
                                                                       at least once. This is true because each sample changes the
                   001      011
                                         Remove card       A
                                                              C        value of the parameter in a way that is insensitive to its cur-
          100          110                                             rent value, making the result equivalent to drawing a vector of
                                                                       values uniformly at random. The time to convergence is thus
               101          111                              AC
                                             Reinsert      B           upper-bounded by the time required to sample all parameters
                                                                       at least once. This is a version of the coupon-collector prob-
Figure 3: (a) The Principles and Parameters case is analogous          lem, being equivalent to asking how many boxes of cereal
to a walk on the hypercube when m = 1. Above, the corners              one must purchase to collect n distinct coupons, assuming
(hypotheses) that could be reached after one step (iteration)          coupons are distributed uniformly over boxes. The first box
beginning at 000 are shown. (b) The OT case is analogous to            provides one coupon, but then the chance of getting a new
a shuffle in which a random card (in this case, the grey card)         coupon in the next box is (n − 1)/n. In general, the chance of
is removed and reinserted into a random spot.                          getting a new coupon in the next box after obtaining i coupons
                                                                       is (n − i)/n. The expected time to find the next coupon is
tent hypotheses for any data string d. If our prior is uniform,
                                                                       thus n/(n − i), and the expected time to find all coupons is
then p(h|d) = (n−m)!n!  for all consistent h and 0 otherwise.          n ∑ni=1 1i , or n times the nth harmonic number. The bound of
                                                                       n log n results from an asymptotic analysis of the harmonic
           Convergence of Iterated Learning                            numbers, showing that the largest term in the asymptotic ap-
We now seek to bound the time to convergence of the Markov             proximation grows as n log n as n becomes large.
chain formed by IL. Bounds on the time to convergence are
often expressed based on total variation distance. This is a              Now, we incorporate the fact that each learner does not
distance measure between two probability distributions µ and           know m parameters, and can thus change up to m parameters
ν on some space Ω that is defined as kµ − νk ≡ 21 ∑ |µ(x) −            at each iteration. We assume this constant m is fixed and con-
                                                       x∈Ω
                                                                       stant across learners. Changing m parameters at each step is
ν(x)| = max|µ(A) − ν(A)| (Diaconis & Saloff-Coste, 1996).
           A⊆Ω                                                         equivalent to redefining an iteration as a collection of m suc-
We seek to bound the rate of convergence of the marginal               cessive steps, each of which changes one parameter. Consider
distributions of the hi to the stationary distribution, expressed      choosing which parameter to change at each step indepen-
via the number of iterations for the total variation distance to       dently; this means that we might change a single parameter
fall below a small number ε. This allows us to analytically            multiple times in one iteration. This process must converge
determine how many iterations the Markov chain must be run             in O( mn log n) iterations, since each iteration in which we can
to conclude that the current distribution is within ε of the sta-      change up to m parameters is equivalent to m steps in our orig-
tionary distribution.                                                  inal Markov chain. In our situation, however, we choose the
    To establish these bounds on the convergence rate, we              m parameters without replacement, so no parameter changes
show that the Markov chains associated with iterated learning          more than once per iteration. Since the net effect of chang-
are analogous to Markov chains for which there are known               ing the same parameter twice in one iteration is equivalent to
bounds. First, we consider the Principles and Parameters ap-           changing it once (from the original value to the final value),
proach. As described above, we assume each learner has re-             changing m different parameters brings us at least as close
ceived sufficient data to set all but m of the n parameters in the     to convergence as changing fewer than m different parame-
hypothesis. We first consider the case where there is only one         ters. Thus, the Markov chain corresponding to the Principles
unknown parameter (m = 1). Then at each generation of iter-            and Parameters approach to grammar learning is O( mn log n)
ated learning, one parameter may be changed at a time. This            in time to convergence.
is equivalent to a random walk on a hypercube, where the hy-
percube has vertices with binary labels and each vertex is con-           We next consider bounding the convergence rate under the
nected by an edge to only those vertices that differ in exactly        assumption that learners learn a ranking over constraints as in
one digit (see Figure 3). In this Markov chain, vertices are           OT. We now assume that, at each generation, the learner has
states and edges indicate the possible transitions out of each         sufficient data to rank all but m of n constraints. Again, we
state. We also assume that there is a transition from each state       first consider the case where m = 1. The process of changing
to itself; this accounts for the case where a learner chooses the      the ordering of one item in a permutation while leaving the
same parameter values as the previous generation. Previous             relative ordering of the other items unchanged has been stud-
analyses show that this Markov chain converges at the rate             ied previously in the context of a random-to-random shuffle
of O(n log n) (i.e. at a rate upper-bounded by some constant           (see Figure 3). The best bound for the random-to-random
multiplied by n log n) (Diaconis & Saloff-Coste, 1996). The            shuffle is O(n log n) (Diaconis & Saloff-Coste, 1993), with
multiplicative constant absorbs the value of ε indicating the          the intuitive argument being similar to that given above. As
desired distance to convergence.                                       before, we view each iteration as m successive steps, making
    An intuition for this result comes from the following argu-        time to convergence O( mn log n).
                                                                   2454

                                                                                                             Convergence Time
                    120                                     50                                                                  40
                           m=1                                    m=1
 Convergence Time
                    100    m=2                                    m=2
                                                            40    m=3                                                           35
                           m=3
                     80
                                                            30                                                                  30
                     60
                                                            20                                                                  25
                     40
                                                            10                                                                  20
                     20                                                                                                           0   1      2         3      4   5
                                                                                                                                           Entropy of Prior
                      0                                      0
                       2         4     6      8        10     3          4          5          6
                                                                                                             Convergence Time
                                                                                                                                20
                            Number of Parameters (n)               Number of Constraints (n)
                                                                                                                                15
Figure 4: Rate of convergence using a uniform prior with
the Principles and Parameters model (left) and the OT model                                                                     10
(right). In both cases, time to convergence is proportional to
 n                                                                                                                               5
m log n.
                                                                                                                                  0   1      2         3      4   5
                                                                                                                                           Entropy of Prior
                                     Empirical Simulations                                            Figure 5: Relationship of the entropy of the prior and conver-
                                                                                                      gence behavior. For both Principles and Parameters (top) and
The preceding sections provide a mathematical analysis of
                                                                                                      OT (bottom), entropy and time to convergence are positively
convergence rates for iterated learning; we now turn to sev-
                                                                                                      correlated.
eral simulations to show convergence behavior and to confirm
and extend our mathematical results. We first demonstrate                                             for Principles and Parameters; n = 5, m = 2 for OT) and var-
convergence time for various choices of the size of the hy-                                           ied β to adjust entropy. Our results showed that entropy and
pothesis space and the amount of information contained in                                             expected time to convergence were positively correlated for
the data, fixing a uniform prior, and then examine whether                                            both representations: as entropy increased, expected time to
the entropy of the prior has an effect on convergence time.                                           convergence also increased (Figure 5). This suggests that the
   To demonstrate the dependence of convergence time on                                               constraints on learning provided by a non-uniform prior be-
n and m, we show in Figure 4 the effect of varying these                                              have similarly to a reduction in the size of the hypothesis
quantities for the uniform prior. For all simulations, we                                             space in their effects on convergence time.
show expected iterations to convergence (number of iterations
for kp(ht ) − p(h)k < 0.0001) given that the observed start-                                                                              Discussion
ing point is distributed according to the stationary distribu-
tion. As expected, as n increases, convergence time increases                                         We began with the question of whether sufficiently many gen-
slightly more than linearly, and as m increases, convergence                                          erations of language learning have occurred for similarities
time decreases proportionally.                                                                        across languages to be the result of biases in human learn-
   One variable that could affect convergence time that was                                           ing rather than a common origin. Using iterated learning to
not previously considered is the entropy of the prior distri-                                         explore this question, we showed that the convergence time
bution: i.e., whether the prior made all hypotheses equally                                           of Markov chains associated with iterated learning can be
likely or put almost all weight on only a few hypotheses. Our                                         bounded, and our simulations confirm the relationship be-
previous analyses show how convergence time varies with the                                           tween the complexity of the hypothesis space (n), the degree
size of the hypothesis space, but non-uniform priors provide                                          to which incoming data (language) limits the choice of lan-
another way to model constraints on learning that might in-                                           guage (m), and number of generations to convergence. The
fluence convergence. The uniform prior is the unique maxi-                                            key result is that the time to convergence for two plausible
mum entropy distribution for any hypothesis space. However,                                           linguistic representations is O( mn log n).
there is no unique solution for achieving a given entropy for                                             This result has two interesting implications. First, the fi-
a distribution with k values. Thus, we altered entropy in the                                         delity with which languages are transmitted between gener-
following, non-unique way. We define one hypothesis h p as                                            ations – reflected in the value of m – has a direct effect on
the prototypical hypothesis. Then, we calculate the distance                                          the rate of convergence. While we have only considered in-
between h p and h for each hypothesis h using an appropri-                                            teger values of m, our results also apply to fractional values.
ate distance measure ∆. For Principles and Parameters, we                                             For example, if a parameter changes value on average every
used the Hamming distance. For OT, we used Kendall’s tau,                                             ten generations, then the convergence time is bounded by tak-
a distance measure for permutations (Diaconis, 1998). Then,                                           ing m to be 0.1; m is thus the expected number of parameters
for all h, p(h) ∝ exp(−β∆(h, h p )). Changing β changes the                                           changed per generation. Consequently, it may be possible
entropy of the distribution. Changing entropy in this man-                                            to estimate m from historical records for different languages.
ner gives our priors a characteristic shape: h p has maximum                                          Second, when we vary n, the time to convergence increases a
probability, and the probability of other hypotheses decreases                                        little more than linearly but the size of the hypothesis space
with distance from h p .                                                                              increases exponentially. Thus, relatively rapid convergence
   In the simulations involving variable entropy, we fixed n                                          should occur even with very large hypothesis spaces.
and m for the two linguistic representations (n = 7, m = 2                                                These results provide constraints on the size of the hypoth-
                                                                                                   2455

                                  −1
                                 10                                                                                   ing that where all n parameters differ from the prototype.
   Param. Changed per Gen. (m)
                                            convergence plausible
                                                                                                                         Our key result is thus that language evolution by iterated
                                  −2
                                 10                                                                                   learning can converge remarkably quickly to the prior – in
                                                                                                                      time that increases linearly as the hypothesis space increases
                                  −3
                                 10                                                                                   exponentially in size. This result is suggestive about the na-
                                                                                 convergence not plausible            ture of linguistic universals, although we are hesitant to draw
                                  −4
                                 10                                                                                   strong conclusions yet. Several restrictive assumptions went
                                                                                                                      into our analysis that could affect convergence time. For ex-
                                  −5
                                 10
                                      10
                                        1
                                                                        10
                                                                             2
                                                                                                             10
                                                                                                               3      ample, the lack of interaction between learners means that
                                                              Number of Parameters (n)                                there is no pressure to adopt a shared communication scheme,
                                                                                                                      and learning from just one other learner removes the opportu-
Figure 6: Values of m and n for which human language could
                                                                                                                      nity for errors in transmission to be corrected. We also make
have converged for the Principles and Parameters model, un-
                                                                                                                      no assertions about the source or nature of the constraints that
der our simplifying assumptions.
                                                                                                                      limit the size or entropy of the prior over hypotheses. How-
                                                                                                                      ever, our analyses are a step towards a more complete un-
esis space and the fidelity of learning necessary for the dis-                                                        derstanding of the origin of universals, with future models
tribution of human languages to have reached an equilibrium.                                                          exploring the impact of these assumptions and developing a
In the case of the Principles and Parameters model, the exact                                                         more complete account of how languages change over time.
(rather than asymptotic) bound is known, and the bound it-
                                                                                                                      Acknowledgements We thank the anonymous reviewers for their
self very tightly tracks the point at which convergence occurs.
                                                                                                                      helpful comments. This work is supported by an NSF Graduate Fel-
Consequently, we can identify exactly which values of m and
                                                                                                                      lowship (ANR) and by NSF grant BCS-0704034 (TLG).
n would make convergence plausible, given some assump-
tions about the number of generations over which languages                                                                                      References
have been evolving. Figure 6 shows the plausible values of                                                            Chomsky, N. (1965). Aspects of the theory of syntax. Cambridge,
m and n given the approximately 100,000 years that anatomi-                                                             MA: MIT Press.
cally modern humans have existed and assuming 25 years per                                                            Chomsky, N., & Lasnik, H. (1993). The theory of principles and pa-
                                                                                                                        rameters. In J. Jacobs, A. von Stechow, W. Sternefeld, & T. Van-
generation. Several authors have estimated the number of pa-                                                            nemann (Eds.), Syntax: An international handbook of contempo-
rameters that might be required for a Principles and Parame-                                                            rary research (pp. 506–569). Berlin: Walter de Gruyter.
ters model; these estimates range from as low as 20 to 50-100                                                         Comrie, B. (1981). Language universals and linguistic typology.
                                                                                                                        Chicago: University of Chicago Press.
parameters (Kayne, 2000; Lightfoot, 1999). Thus, given this                                                           Diaconis, P. (1998). Group representations in probability and statis-
range of values for n, we can see that convergence is plausible                                                         tics. Institute of Mathematical Statistics.
for a variety of values of m. While this graph is only a rough                                                        Diaconis, P., & Saloff-Coste, L. (1993). Comparison techniques for
                                                                                                                        random walk on finite groups. The Annals of Probability, 21(4),
guide given the strong simplifying assumptions of our model,                                                            2131–2156.
it allows some understanding of how our analysis applies to                                                           Diaconis, P., & Saloff-Coste, L. (1996). Random walks on finite
actual human language.                                                                                                  groups: A survey of analytic techniques. Probability measures on
                                                                                                                        groups and related structures, XI.
   In conducting our analyses, we assumed that learners sam-                                                          Greenberg, J. (Ed.). (1963). Universals of language. Cambridge,
ple from the posterior distribution over hypotheses. Alterna-                                                           MA: MIT Press.
                                                                                                                      Griffiths, T. L., & Kalish, M. L. (2007). A Bayesian view of lan-
tive methods of selecting a hypothesis, such as selecting the                                                           guage evolution by iterated learning. Cognitive Science, 31, 441-
hypothesis with the maximum posterior probability (MAP)                                                                 480.
and exponentiated sampling, have been considered in previ-                                                            Hawkins, J. (Ed.). (1988). Explaining language universals. Oxford:
                                                                                                                        Blackwell.
ous work (Griffiths & Kalish, 2007; Kirby, Dowman, & Grif-                                                            Kayne, R. S. (2000). Parameters and universals. New York: Oxford
fiths, 2007). In the case of a uniform prior, both methods are                                                          University Press.
equivalent to the sampling method we considered since all                                                             Kirby, S. (2001). Spontaneous evolution of linguistic structure:
                                                                                                                        An iterated learning model of the emergence of regularity and
hypotheses with non-zero probability have the same proba-                                                               irregularity. IEEE Journal of Evolutionary Computation, 5, 102-
bility in the uniform case; thus, our analyses of convergence                                                           110.
time hold. In the non-uniform case, raising the posterior to                                                          Kirby, S., Dowman, M., & Griffiths, T. L. (2007). Innateness and
                                                                                                                        culture in the evolution of language. Proceedings of the National
the power of γ before sampling is equivalent to multiplying                                                             Academy of Sciences, 104, 5241-5245.
the β parameter in the model we used to construct our non-                                                            Lightfoot, D. (1999). The development of language: Acquisition,
uniform priors by γ. Thus, predictions concerning conver-                                                               change and evolution. Blackwell: Oxford.
                                                                                                                      McCarthy, J. J. (2004). Optimality theory in phonology: A reader.
gence time can also be made for exponentiated sampling in                                                               Malden: Wiley-Blackwell.
the non-uniform case. For MAP in the non-uniform case,                                                                Norris, J. R. (1997). Markov chains. Cambridge, UK: Cambridge
convergence to the prototype hypothesis (that with the highest                                                          University Press.
                                                                                                                      Prince, A., & Smolensky, P. (2004). Optimality theory: Constraint
probability in the non-uniform prior) will occur. The time in                                                           interaction in generative grammar. Blackwell Publishing.
which this occurs is still O( mn log n): at every step, the learner                                                   Smith, K., Kirby, S., & Brighton, H. (2003). Iterated learning: a
changes unknown parameters to match the prototype, produc-                                                              framework for the emergence of language. Artificial Life, 9, 371–
                                                                                                                        386.
ing another coupon collector problem with the worst case be-
                                                                                                                   2456

