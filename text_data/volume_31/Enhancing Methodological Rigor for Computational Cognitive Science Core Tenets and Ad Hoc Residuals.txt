UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Enhancing Methodological Rigor for Computational Cognitive Science: Core Tenets and Ad
Hoc Residuals
Permalink
https://escholarship.org/uc/item/1812c3tr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Beal, Jacob
Roberts, Jennifer
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Enhancing Methodological Rigor for Computational Cognitive Science:
                                          Core Tenets and Ad Hoc Residuals
                                              Jennifer Roberts (jenmarie@mit.edu)
                                                      MIT CSAIL, 32 Vassar Street
                                                       Cambridge, MA 02139 USA
                                                  Jacob Beal (jakebeal@bbn.com)
                                                 BBN Technologies, 10 Moulton Street
                                                       Cambridge, MA 02138 USA
                               Abstract                               modeling formalisms to instantiate the same theory, and es-
                                                                      tablish whether a computational model’s performance is truly
   Computational models are notoriously difficult to compare and
   interpret, resulting in a community segmented around model-        independent of arbitrary design decisions.
   ing paradigms. In this paper, we seek to develop community
   standards and methodology that will make it easier to compare                        Cognitive Plausibility
   work across computational paradigms, discern what types of
   empirical predictions can be drawn from computational work,        Computational models must be defined at a finer-grained
   and test the validity of computational models. Using an estab-     level of detail than other types of psychological models, be-
   lished Bayesian model, we illustrate how our proposed meth-        cause computational models must lead to software implemen-
   ods will achieve these goals.
                                                                      tations or sets of mathematical computations. Defining such
   Keywords: Methodology; Marr Levels; Core Tenets                    a model almost always entails specifying details for which
                                                                      the modeler has little or no psychological or neurological ev-
                           Introduction                               idence (Newell, 1990; Anderson, 1983; R. Cooper, Fox, Far-
As cognitive scientists, we seek to develop a unified theory          ringdon, & Shallice, 1996; McClelland, 2009).
of the human mind. Our current modeling efforts, however,                Because details for which we have no evidence must be
use a diverse set of tools and formalisms that span from neu-         specified in order for the model to be implemented, computa-
ral networks to cognitive architectures to Bayesian reasoning         tional models contain both cognitively plausible elements and
to first order predicate logic, and each model only addresses         ad hoc implementation details. We call the parts of a compu-
aspects of the larger problem. In this paper, we seek to de-          tational model that the modeler considers cognitively plausi-
velop community standards that will make it easier to com-            ble and wishes to communicate to the modeling community
pare work across paradigms, discern what types of empirical           the core tenets of the model, and we refer to the incidental
predictions can be drawn from computational work, and en-             design decisions as the ad hoc residuals.
hance our ability to translate models into statements about the          We further refine this idea using Lakatos’ (1970) obser-
computational nature of intelligence.                                 vation that theoretical claims can be broken into two groups
   In the first section, we introduce two new terms: core tenet,      based on the strength of a researcher’s commitment to the
for a part of a computational model argued to be cognitively          claims. Core tenets can thus be subdivided into central as-
plausible, and ad hoc residual, for an arbitrary implementa-          sumptions and peripheral hypotheses (following the example
tion detail. We then describe two types of core tenets: central       of Cooper and Shallice (2000)). Tenets that the researcher
assumptions that an author believes to be paramount to the            is strongly committed to preserving, because he or she be-
cognitive theory behind their computational model and pe-             lieves those aspects of the model generate crucial behavior,
ripheral hypotheses that the author believes are cognitively          are called central assumptions. Tenets that the researcher re-
plausible but feels less committed to preserving in their cur-        gards as open to modification are called peripheral hypothe-
rent form. We advocate the explicit delineation of a model’s          ses. While an author writing about a model may have a clear
core tenets and ad hoc residuals as well as the explicit divi-        idea of which parts they intend as central or peripheral core
sion of core tenets into central assumptions and peripheral hy-       tenets and which parts they intend as ad hoc residuals, that
potheses when publishing computational papers, because we             intent often is not communicated clearly.
believe this practice will enable computational researchers to           Cooper (2006, 2007) highlights the difficulty of identify-
communicate more effectively and to learn more from one               ing core tenets in another person’s work. Cooper analyzes the
another’s work. Identification of the intended core tenets and        progression of the cognitive theories behind Soar and ACT-R,
ad hoc residuals will reduce the chance that other researchers        labeling architectural aspects that remained stable over time
will take issue with an aspect of a model that the authors con-       as central assumptions and aspects that were added to accom-
sider tangential to their work. We argue that this practice will      modate empirical findings as peripheral hypotheses. While
allow researchers to clearly communicate the correspondence           this is a reasonable way to proceed when the original author
between their models and theories of human cognition, estab-          has not explicitly delineated core tenets, Cooper’s criteria for
lish the validity of one another’s theories by using multiple         identifying central assumptions confounds the ease of mod-
                                                                  377

                                Core Tenets                                                              Ad Hoc Residuals
  Cognitively plausible parts of the computational model, which specify           Implementation details that are based on minimal cognitive or
  a testable cognitive theory. The core tenets should be verified both            neurological evidence and are not intended to be part of a cog-
  empirically and computationally. As a cognitive theory, core tenets             nitive theory. The success of the computational model should be
  can be divided into central assumptions and peripheral hypotheses.              shown to be independent of the ad hoc residuals.
  Central Assumptions                    Peripheral Hypotheses
  Aspects of the cognitive the-          Aspects of the cognitive the-
  ory that the scientist will only       ory that the scientist will update
  change as a last resort                based on empirical findings
Figure 1: Framework through which any computational model of cognition may be understood. This framework distinguishes
the parts of a computational model that the modeler considers cognitively plausible from the parts he or she deems cognitively-
irrelevant. Because core tenets specify a theory of cognition, the core tenets can be divided into central assumptions and
peripheral hypotheses based on a scientist’s level of commitment to preserving the tenets in their current form (Lakatos, 1970).
ifying computational models with the question of cognitive                 plementation, which includes the physical realizations of the
plausibility.                                                              representations and algorithms (Marr, 1982). He argued that
   Aspects of a computational model might remain stable for                some modelers work at the hardware level while others work
engineering reasons, so the stability of a particular aspect is            at the algorithmic or computational level. This implies that
not a reliable metric for ascertaining whether the modeler                 a modeler working at the computational theory level would
considers the aspect cognitively plausible. For example, pre-              consider the goal of their algorithm to be a core tenet and
2004 versions of Soar relied exclusively on production rules.              would consider the algorithmic and hardware details to be ad
Does this imply that production rules were viewed as the only              hoc residuals. In contrast, a modeler working at the algo-
representation used by the brain, a reasonable approximation               rithmic level would have core tenets that involve representa-
for the brain’s symbolic representations, or an engineering                tions and algorithmic details and ad hoc residuals related to
tool that could be modified or replaced if necessary? Citing               the hardware implementation.
the continual use of production rules as evidence for its sta-                While a useful construct, the Marr levels still leave room
tus as a core tenet ignores the possibility that first order logic         for ambiguity. Every researcher can have a slightly different
was initially adopted and continues to be used for engineering             interpretation of where the separations between levels belong.
simplicity. Cooper’s efforts to identify the central assump-               For example, Broadbent (1985) and Rumelhart and McClel-
tions and peripheral hypotheses of Soar highlight the need for             land (1985) debated about which Marr levels existing cogni-
researchers to unambiguously identify their core tenets, be-               tive theories and connectionist models really addressed.
cause despite Cooper’s objective approach, identifying which                  Further complicating the issue, complex models contain
parts of a computational model are intended to be cognitively              processes with multiple subgoals, so the overall process and
plausible still involves guess work.                                       each of the subprocesses all have Marr computational level
   Throughout the remainder of this paper, we focus on the                 descriptions. For example, the Companion cognitive archi-
distinction between core tenets and ad hoc residuals, that is,             tecture has been used to perform transfer learning (Forbus
the divide between the cognitively plausible parts of a model              & Hinrichs, 2006). At the computational level, the overall
and the details added for implementation purposes. We agree                system attempts to solve problems by analogically compar-
with Cooper that further dividing core tenets into central as-             ing problems to previously-encountered examples. The sys-
sumptions and peripheral hypotheses is extremely important                 tem contains an analogical component with its own computa-
and advocate using this terminology when defining models.                  tional level, which focuses on comparing structural elements
In later sections, we will focus on determining the extent to              to determine the similarity between two problems. Thus, the
which either type of core tenet affects a model’s performance              Companion architecture might suggest a computational level
and methods for comparing either type of core tenet across                 theory, but are all the computational level details relevant to
modeling paradigms.                                                        the cognitive theory, or only a subset of them?
   Marr partially addressed the divide between core tenets and                Claiming that two models pertain to distinct Marr levels
ad hoc residuals when he proposed that computational mod-                  implies that the models have different core tenets and ad hoc
els can be analyzed using the following three levels: (1) the              residuals, but the Marr level descriptions do not unambigu-
computational theory, which includes the goal of the compu-                ously specify what is different (a point also noted in McClel-
tation and the general strategy for performing the computa-                land (2009)). Currently, the phrase “modeling at the com-
tion, (2) the representation and algorithm, which includes the             putational level” merely indicates that some details are con-
input and output representations as well as the algorithm for              sidered ad hoc residuals without explicitly explaining which
manipulating those representations, and (3) the hardware im-               details fall into this category.
                                                                       378

           An Illustrative Cognitive Model                            language for describing the model, but any other framework
In the remaining sections, we will show how core tenets and           in which we could describe a similar trade-off between sim-
ad hoc residuals can (1) clarify assertions about cognitive           plicity and goodness of fit would work equally well. Readers
plausibility, (2) provide mechanisms for testing how much of          assessing the cognitive plausibility of the work should then
a computational model’s success depends on its cognitively            evaluate the extent to which they believe a trade-off between
plausible parts and how much of its success depends on inci-          simplicity and goodness of fit adequately captures a crucial
dental design decisions, and (3) specify empirically-testable         component of how people play the number game, without al-
cognitive theories. To illustrate our points, we use a relatively     lowing the Bayesian framework or particular probability dis-
straight-forward computational model of how people play a             tributions to play a crucial role in their evaluation process.
simple number game (Tenenbaum, 2000). This analysis ap-                  Realistically, most computational models involve a set of
plies equally well to any computational model of cognition,           core tenets. Based on Tenenbaum’s description of his model,
regardless of its paradigm or degree of complexity.                   we can identify a few principles that seem like compelling
   Tenenbaum (2000) explores how people play a game in                core tenet candidates, such as the trade-off between simplicity
which they receive sets of numbers between 1 and 100, like            and goodness of fit (described in the paper as the size princi-
{4, 8, 24, 12}, and guess what rule generated the set. Pos-           ple) and the ability to process both similarity-based and rule-
sible rules include “multiples of 3,” “squares,” and “numbers         based hypotheses using the same apparatus. Whether or not
between 10 and 20.” In this case, the numbers satisfy several         other aspects of the model or Bayesian framework should be
rules, including “multiples of two,” “multiples of four,” and         considered core tenets remains a source of speculation, but
“numbers less than 30.” To ascertain how people think the             explicitly specifying a list of core tenets and ad hoc residuals
set has been generated, each person receives a series of test         and indicating a level of commitment to the items in that list
numbers, like {3}, {22}, and {16}, and they must rate the             by dividing the core tenets into central assumptions and pe-
likelihood that each test number belongs to the set.                  ripheral hypotheses would clarify Tenenbaum’s perspective.
   Tenenbaum proposed a computational model in which peo-                A clear division between core tenets and ad hoc residu-
ple start with a set of hypotheses about how sets have been           als would also elucidate the simplifying assumptions neces-
generated. Hypotheses come in two forms, those that depend            sary to make a modeling effort tractable. McClelland (2009)
on a mathematical rule and those that involve an interval. The        writes eloquently about the need for simplifications in order
prior probability assigned to the rule-based set is uniformly         to develop a model that can test the consequences of a set of
distributed across all rule-based hypotheses, while the weight        core ideas in an comprehensible manner. Labeling simplifica-
assigned to the interval-based hypotheses is distributed so that      tions as ad hoc residuals will make it clear to the reader what
intermediate-sized intervals receive the bulk of the prior prob-      falls within and outside of the scope of the model and what
ability.                                                              should be considered when evaluating the model.
   The probability of observing a set of numbers, X, given               Anyone can effortlessly spring to a controversial conclu-
that a particular hypothesis generates the set, depends on how        sion about a computational model. For example, do connec-
many numbers between 1 and 100 the hypothesis can gener-              tionist models imply that the brain uses a back propagation
ate. For example, the hypothesis “multiples of 2” can gen-            algorithm? Do Bayesian models imply that the brain con-
erate 50 numbers between 1 and 100, while the hypothesis              tains a probabilistic engine? Do symbolic models imply that
“numbers from 1 to 10” generates 10. High likelihoods are             the brain stores information using first order logic predicates?
assigned to hypotheses that fit the observed data without gen-        Even though these conclusions may not reflect a modeler’s
erating too many extra numbers. Thus, {10, 30, 70} would              beliefs, these types of reactions can lead other researchers
be more likely to be generated by “multiples of 10” than              to doubt the premise of a particular piece of work, hassle a
by “multiples of 2.” This model builds on what Tenenbaum              modeler with questions that are tangential to the intent of the
refers to as the size principle, a trade-off between simplicity       work, or even reject an entire modeling paradigm.
and goodness of fit, which essentially states that people prefer         Clearly specifying the core tenets of a model can focus the
the simplest hypothesis that explains the data.                       ensuing conversation on the parts of a model that the authors
                                                                      deem most relevant and most cognitively plausible. Debates
         Using Core Tenets to Indicate Intent                         over controversial claims may be the hallmark of science, but
                                                                      such debates should focus on claims that the modelers de-
Just by examining his computational model, we do not know
                                                                      cisively make, not claims that readers infer from the ad hoc
which parts of the model Tenenbaum considers cognitively
                                                                      residuals of a model or modeling paradigm.
plausible. By explicitly stating a set of core tenets, however,
we can indicate which parts of a model readers should focus
on when evaluating a piece of computational work.
                                                                             Using Core Tenets to Validate Models
   For example, if we take the trade-off between simplicity           Ideally, we would like to know that a model’s success relies
and goodness of fit as the only core tenet, all other aspects         only on the parts we believe to be important and not on ar-
of the model would be considered ad hoc residuals. Under              bitrary design decisions, but we as a community currently
this interpretation, the Bayesian framework would be a useful         lack a systematic process for making this distinction. Cooper
                                                                  379

  Shared Core Tenet(s)                                  Trade-off between Simplicity and Goodness of Fit
  Framework                                 Bayesian                                          Library of Stored Examples
                                                                                           with a Memory Retrieval System
  Instantiation Tenenbaum's Likelihood Function:               Alternate likelihood                     Memory indexed by small sets
                                                                   functions like                     of observed numbers paired with
                                                              the square root of the                  hypothesis lists that are weighted
                                                                    original one                         based on the core trade-off;
                                                                                                       Algorithm processes large sets
                                                                                                             by retaining trade-off
  Implementation          Details like         Alternate details like
                       - analytic solvers              - MCMC
                    - hypothesis space of    - additional hypotheses
                      mathematical rules      like "all numbers with 2
                          and intervals           in the one's digit"
Figure 2: A shared-core-tenet analysis can be used to explore how the behavior of any computational model, regardless of its
paradigm or complexity, depends on a set of core tenets. This example depicts a comparison between different formulations
of the Tenenbaum (2000) number game model, where the leftmost branch describes the original formulation of the model and
the remaining branches specify hypothetical alternative formulations. Other alternate formulations, like those described in Shi,
Feldman, and Griffiths (2008), would be depicted using additional branches.
et al (1996) suggests using a criticality/sensitivity ratio that            Creating Core-Tenet-Consistent Models
measures the degree to which a computational model’s be-
havior depends on core tenets versus ad hoc residuals, but                  We can develop an array of core-tenet-consistent models by
we do not see a systematic way of defining such a ratio be-                 first specifying a set of core tenets that will be shared by the
cause a model’s dependence on underlying parameters and                     models. As a simple example, we consider the core tenet
constraints is too complex to be summarized by a single ra-                 trade-off between simplicity and goodness of fit in Tenen-
tio. In addition, Cooper’s work only seeks to compare theo-                 baum’s number game model.
ries within a single research program, but we are interested                   We can explore the space of models consistent with the
in what we can learn by comparing research programs that                    shared core tenets by using the following levels of abstraction.
share one or more core tenets but rely on different model-                  At the highest level, we start with a framework capable of
ing paradigms. For instance, if ACT-R and Soar both share                   instantiating the shared core tenets. Tenenbaum’s model uses
a core tenet, can that provide evidence that the shared tenet               a Bayesian framework, but another formulation of the model
yields some desirable behavior? Similarly, if Bayesian and                  might use a cognitive architecture or logical model to express
connectionist models for the same phenomenon share a core                   the principles of simplicity and goodness of fit.
tenet, what does that tell us about the tenet?                                 The instantiation level specifies how the shared core tenets
                                                                            are expressed within the formalism selected at the framework
                                                                            level. At this level of detail, Tenenbaum’s model depends on
   In subsequent sections, we introduce a framework for sys-                the specific hypothesis space, prior probabilities, and likeli-
tematically comparing core-tenet-consistent models, models                  hood distributions described in the previous section. Alterna-
that share a single core tenet or a set of core tenets, regard-             tive models might use an expanded hypothesis space or differ-
less of whether the models use the same paradigm. This in-                  ent set of probability distributions that still preserve the core
volves developing alternate formulations of the original com-               tenet trade-off.
putational model that preserve the core tenets but change the                  The bottom level specifies the remainder of the implemen-
ad hoc residuals. If the core tenets are valid, these core-tenet-           tation details necessary to turn the specific instantiation of
consistent versions of the model should produce results that                the shared core tenets into a working computational model.
are qualitatively similar to both empirical findings and the re-            Tenenbaum’s model appears to compute analytical solutions
sults obtained using the original model, while providing evi-               for the posterior distributions, but an alternative formulation
dence that the results do not depend on ad hoc residuals.                   might rely on a non-deterministic algorithm like MCMC.
                                                                       380

Also at this level, Tenenbaum’s model calculates a posterior             To verify the bottom set of core tenets, one must demon-
probability for all hypotheses when determining the probabil-         strate that a Bayesian model can produce results that match
ity that a novel number belongs to a particular set, whereas an       the performance of each individual participant of the number
alternative algorithm might use heuristics to focus on a subset       game experiment. Tenenbaum’s original formulation com-
of hypotheses.                                                        pared results obtained using his Bayesian model to results
   The number of shared core tenets combined with their               obtained by averaging over a population. This suggests that
specificity will determine how significantly a core-tenet-            his model captures a population average, but it may or may
consistent formulation of a model may diverge from the orig-          not adequately capture what any one person is doing. To es-
inal version. For example, if the core tenets mirror the con-         tablish that these core tenets describe an individual’s cogni-
straints imposed by the Bayesian framework, then all core-            tive process, one would have to establish that a particular for-
tenet-consistent models will either use the Bayesian frame-           mulation consistent with the core tenets can be used to ade-
work or a pseudo-Bayesian one. In contrast, when core tenets          quately model each individual’s performance. The conglom-
provide looser constraints, alternative formulations will in-         erate set of formulations, each tuned to a specific person’s
volve a wider variety of frameworks and paradigms.                    performance, should then provide a computational account
                                                                      for the group performance. Thus, discerning whether the bot-
Testing the Validity of Core Tenets and the                           tom set of core tenets provides a good characterization of in-
Insignificance of Ad Hoc Residuals                                    dividual behavior or simply a description of aggregate group
Figure 2 shows a sampling of the models consistent with a             behavior would probably require additional empirical tests.
core tenet trade-off between simplicity and goodness of fit. If
this core tenet is valid, these core-tenet-consistent versions of     Testing Computational Plausibility Claims
the model should produce results that are qualitatively simi-         When designing a complex computational model, one often
lar to both empirical findings and the results obtained using         starts with a set of implicit assumptions, perhaps philosophi-
Tenenbaum’s original model.                                           cally based, that constrain the design of the system. When a
   Developing multiple formulations explores whether the              system fails to perform as anticipated, these implicit assump-
model’s performance relies on any of the ad hoc residuals.            tions may change or become more constrained. Computa-
If an ad hoc residual contributes to the model’s success, we          tional work of this type seeks to answer the question: Can we
would expect the residual to play a crucial part in every im-         accomplish task X under the set of constraints Y ?
plementation. For example, when we presume that the trade-               Depending on task complexity and the initial sets of con-
off between simplicity and goodness of fit is the only core           straints, surprising failures may yield insights into the com-
tenet, this implies that everything else, including the specifi-      putational nature of intelligence, but publishing negative re-
cation of a fixed hypothesis space, is an ad hoc residual. If the     sults remains difficult. Core tenets provide a systematic lan-
specification of a fixed hypothesis space is in fact crucial to       guage for describing this type of work. By contrasting the
Tenenbaum’s model, every core-tenet-consistent model, de-             core tenet constraints necessary for building a working sys-
fined with respect to simplicity and goodness of fit, should          tem with the sets of constraints for which no working sys-
contain some type of fixed hypothesis space. In this case,            tem can be found, we can begin to systematically explore the
even if a fixed hypothesis space had initially been considered        space of constraints required for functional cognitive models.
an ad hoc residual, its presence in every core-tenet-consistent
model might imply that it should be considered a core tenet.                                 Contributions
However, if an ad hoc residual that does not seem cognitively         We have introduced the terms core tenets and ad hoc resid-
plausible still plays a crucial part in every instantiation, we       uals to distinguish cognitively plausible parts of a compu-
would need to search for cognitively plausible alternatives.          tational model from incidental implementation details, and
                                                                      we have demonstrated how these concepts can augment and
      Core Tenets Specify Cognitive Theories                          validate work on a well-known computational model (Tenen-
Ultimately, we would like to develop cognitive theories that          baum, 2000).
describe an individual’s cognitive processes and make useful             We argue that computational cognitive scientists should ex-
empirical predictions, but what types of predictions can legit-       plicitly identify core tenets and ad hoc residuals and distin-
imately be drawn from a detailed, fully-implemented compu-            guish between central and peripheral core tenets when de-
tational model? How can we systematically reconcile com-              scribing their models. This practice will elucidate an au-
putational work with non-computational work?                          thor’s intent, provide mechanisms for systematically test-
   Core tenets identify which parts of a computational model          ing whether the core parts of a computational model play
describe an individual’s cognitive processes. For example,            an instrumental role in the model’s success, and help en-
Tenenbaum’s paper does not directly indicate how his model            sure that a model’s performance remains independent of ar-
relates to individual cognition (Tenenbaum, 2000), but Fig-           bitrary implementation details. Using core tenets and ad
ure 3 shows two possible interpretations, specified by two ex-        hoc residuals can (1) bridge between modeling paradigms
panded sets of core tenets, both of which translate into cogni-       by helping researchers to create core-tenet-consistent instan-
tive theories of how individuals play the number game.                tiations of a model, (2) bridge between computational and
                                                                  381

  Sample Set of Core Tenets                                              Cognitive Theory Implied by the Tenets
                                                                         Each person has a method for defining a personal hypothesis
  1. Specification of a hypothesis space for each set of numbers         space, and people may use different hypothesis spaces depend-
     encountered during the game                                         ing on context. Given a set of observed numbers, people weight
                                                                         the likelihood of each hypothesis using a trade-off between sim-
  2. Methods for weighting hypotheses based on a trade-off               plicity and goodness of fit, as well as a tendency to favor inter-
     between simplicity and goodness of fit and for favoring             mediately sized intervals over small and large intervals. For a
     intermediately-sized intervals over small or large intervals        particular set of numbers, people may only consider a subset of
                                                                         the hypotheses they use throughout the course of the game.
  1. A unique pre-defined hypothesis space for each individual that
     remains fixed throughout the game                                   Each individual has a personal hypothesis space, and people
                                                                         reuse the same space every time they play the number game.
  2. A set of prior and likelihood functions defined over the hy-        Each individual uses a personal prior and likelihood function
     pothesis space in a manner that favors simplicity, goodness of      to weight the space, and all individuals use the same posterior
     fit, and intermediate interval size                                 probability equation for comparing hypotheses. For every set of
                                                                         numbers, each person considers every hypothesis in his or her
  3. An algorithm for considering the posterior probability of all       hypothesis space, regardless of how large the space is.
     hypotheses
Figure 3: Core tenets specify theories about an individual’s cognitive processes. The sets of core tenets in this figure describe
two possible interpretations of the Tenenbaum (2000) model, which translate into two theories of how an individual plays the
number game. Because Tenenbaum’s model instantiates both sets of core tenets, his model is consistent with both theories.
non-computational work by clearly indicating how a compu-               Cooper, R. P. (2007). The role of falsification in the develop-
tational model translates into a theory of individual cognition           ment of cognitive architectures: Insights from a Lakatosian
from which we can draw empirical predictions, (3) avoid mis-              analysis. Cognitive Science, 31, 509-533.
conceptions by focusing scientific debate on claims we inten-           Cooper, R. P., & Shallice, T. (2000). Contention scheduling
tionally make instead of claims that appear to be implied by              and the control of routine activities. Cognitive Neuropsy-
our models, and (4) provide a theoretical framework for test-             chology, 17, 297338.
ing claims about computational plausibility with respect to a           Forbus, K. D., & Hinrichs, T. R. (2006). Companion cogni-
set of constraints.                                                       tive systems: A step toward human-level AI. AI Magazine,
   We expect that providing clear statements about the core               27(2).
tenets and ad hoc residuals of our models will greatly enhance          Lakatos, I. (1970). Falsification and the methodology of sci-
the ability of cognitive scientists to communicate and to com-            entific research programmes. In I. Lakatos & A. Musgrave
pare work across paradigms, and we strongly encourage the                 (Eds.), Criticism and the growth of knowledge (p. 91-196).
community to adopt and enforce this standard.                             Cambridge, UK: Cambridge University Press.
                                                                        Marr, D. (1982). Vision: A computational investigation into
                     Acknowledgements                                     the human representation and processing of visual infor-
                                                                          mation. New York, NY, USA: Henry Holt and Co., Inc.
The authors thank Richard Cooper, Josh Tenenbaum, and
                                                                        McClelland, J. L. (2009). The place of modeling in cognitive
three anonymous reviewers for their helpful comments.
                                                                          science. Topics in Cognitive Science, 1, 11-38.
J.M.R thanks the Fannie and John Hertz Foundation and both
                                                                        Newell, A. (1990). Unified theories of cognition. Cambridge,
authors thank the National Science Foundation for their fi-
                                                                          MA: Harvard University Press.
nancial support.
                                                                        Rumelhart, D. E., & McClelland, J. L. (1985). Levels in-
                                                                          deed! A response to Broadbent. Journal of Experimental
                            References
                                                                          Psychology: General, 114(2), 193-197.
Anderson, J. R. (1983). The architecture of cognition. Cam-             Shi, L., Feldman, N. H., & Griffiths, T. L. (2008). Performing
   bridge, MA: Harvard University Press.                                  Bayesian inference with exemplar models. In 30th Annual
Broadbent, D. (1985). A question of levels: Comment on                    Conference of the Cognitive Science Society.
   McClelland and Rumelhart. Journal of Experimental Psy-               Tenenbaum, J. B. (2000). Rules and similarity in concept
   chology: General, 114(2), 189-192.                                     learning. In S. A. Solla, T. K. Leen, & K. R. Muller (Eds.),
Cooper, R., Fox, J., Farringdon, J., & Shallice, T. (1996). A             Advances in Neural Information Processing Systems 12
   systematic methodology for cognitive modelling. Artificial             (pp. 59–65). Cambridge, MA: MIT Press.
   Intelligence, 85, 3-44.
Cooper, R. P. (2006). Cognitive architectures as Lakatosian
   research programs: Two case studies. Philosophical Psy-
   chology, 19, 199-220.
                                                                    382

