UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Finding a Better k: A psychophysical investigation of clustering
Permalink
https://escholarship.org/uc/item/3t85q06v
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Author
Lewis, Joshua M.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                  Finding a Better k: A psychophysical investigation of clustering
                                                              Joshua M. Lewis
                                                    Department of Cognitive Science
                                                   University of California, San Diego
                                                           San Diego, CA 92093
                                                      josh@cogsci.ucsd.edu
                               Abstract
   Finding the number of groups in a data set, k, is an important
   problem in the field of unsupervised machine learning with ap-
   plications across many scientific domains. The problem is dif-
   ficult however, because it is ambiguous and hierarchical, and
   current techniques for finding k often produce unsatisfying re-
   sults. Humans are adept at navigating ambiguous and hierar-
   chical situations, and this paper measures human performance
   on the problem of finding k across a wide variety of data sets.
   We find that humans employ multiple strategies for choosing
   k, often simultaneously, and the number of possible interpreta-
   tions of even simple data sets with very few (N < 20) samples
   can be quite high. In addition, two leading machine learning
   algorithms are compared to the human results and methods for
   improving these techniques are discussed.
   Keywords: machine learning; clustering; psychophysics.
                          Introduction
Within the field of unsupervised machine learning, clustering                         Figure 1: Are there 8, 7 or 2 groups?
is a technique used to separate an arbitrary collection of data
points into groups (commonly called clusters). Clustering is
usually comprised of two steps. First, one must choose the              sent the effectiveness of a certain k by calculating an assign-
number of groups, represented by the variable k, for which              ment based on that k and then measuring the sum squared
to look. Second, one must assign each data point to one or              distance between each data point and the centroid of the clus-
more groups while ensuring that there are no empty groups.              ter that it is assigned to. This seems like a reasonable strat-
This second step has received the majority of attention from            egy, but imagine choosing a k equal to the number of points
researchers, with techniques such as the venerable k-means              in one’s data. In this situation, each cluster will have exactly
and spectral clustering [1] focusing exclusively on assign-             one data point, which will be located at the cluster centroid.
ment and leaving the task of choosing k up to other algo-               Obviously the sum squared distance in this case will be zero,
rithms.                                                                 indicating a good fit but providing an answer completely use-
   There are a few reasons why choosing k is a less attractive          less in terms of data analysis. In general, k + 1 will always fit
problem for researchers as compared to the assignment prob-             data better than k based on this simple measure. A solution to
lem. In some application domains for clustering algorithms,             this limitation is to design a more sophisticated statistical test
researchers may approach their data with a particular value             to determine when to stop increasing the number of clusters
for k already in mind. In this case, they can simply enter              in order to better fit the data. Unfortunately, statistical tests
the desired number of clusters into an algorithm like k-means           are based on assumptions about the underlying distribution
and have a solution without bothering to find which values              of the data and if these assumptions are incorrect the test will
for k have statistical support. Beyond this practical matter,           fail to provide a reasonable result.
choosing k has all the hallmarks of a difficult computer sci-              The challenges presented in choosing k might lead one to
ence problem. For most data there is no one right answer                wonder whether humans are accomplished at the task. Hu-
for what k should be. In fact there may be many answers,                mans are adept at navigating ambiguous and hierarchical sit-
some more likely than others. Thus k is an inherently am-               uations, and we generally cringe at the thought of labori-
biguous quantity, causing much algorithmic difficulty. Some             ously counting large numbers of objects, so perhaps we are
of this ambiguity comes from multiple possible hierarchical             k-choosing experts. There is a distinct (and often implicit)
interpretations of the data. For the data in Fig. 1, for example,       trend in the clustering literature to use the human visual sys-
the value of k depends on whether one wants to focus on the             tem as a standard against which the performance of cluster-
details (that there are seven or eight small groups) versus the         ing algorithms should be judged. In one prominent spectral
broad trend (that there are two clear larger groups).                   clustering paper, the authors state, “The results are surpris-
   Beyond challenges with ambiguity and hierarchy, there is             ingly good... the algorithm reliably finds clusterings consis-
also the issue of profligacy. Naı̈vely, one might want to repre-        tent with what a human would have chosen [1].” Given that
                                                                    315

our visual system is an adept and powerful data processing             While there are likely many interesting phenomena to in-
system (surprisingly resistant to myriad forms of algorithmic       vestigate in the human data, such as consistency, reaction
mimicry) it is reasonable to solicit its judgments on a thorny      time, the relationship between reaction time and consistency,
problem for which it seems particularly well-suited.                the relationship between reaction time and k, etc., this pa-
   This paper takes inspiration from a computer vision project      per is mostly concerned with the overall gist of the human
undertaken at UC Berkeley [2]. Faced with the challenging           responses, their relationship to state-of-the-art k-choosing al-
task of determining how to segment images such that ob-             gorithms, and the new k-choosing methods they inspire. To
jects are separated from one another by outlines, researchers       that end, the human data were analyzed and will be presented
enlisted human subjects to manually outline the objects in          collapsed across subjects, scales and rotations. The results are
several hundred images of real-world scenes. The problem            presented in normalized bar plots meant to represent a prob-
of image segmentation is very similar to choosing k in that         ability distribution over k, based on the number of responses
ambiguity and hierarchy play a major role in determining            at each particular k. For each display there are at least 72
reasonable answers. Detail oriented subjects might outline          responses represented, assuming one answer per subject per
the leaves on a tree whereas others might just outline the          trial. The actual number of responses might be larger if sub-
branches. Through this effort researchers collected what is         jects were inclined to give multiple answers.
known as the Berkeley Segmentation Dataset, a large collec-            The 50 point light displays used in this experiment were
tion of human image segmentation data. These data have mo-          chosen to provide a mixture of depth and breadth within the
tivated and assisted several research projects and continue to      extremely large space of possible point light displays. Six-
be a valuable resource in the computer vision field. Studies        teen of the displays consisted of various riffs on mixtures of
explicitly measuring human clustering judgements are rare,          Gaussians, while another three were mixtures of Gaussians
but at least one study exists that focuses on the developmental     overlaid with uniformly distributed random noise. Nine dis-
changes in human visual grouping of synthetic data sets [3].        plays consisted solely of uniformly distributed random noise
   This paper presents human judgements on a diverse set            (with differing number of samples between eight and 10,000).
of clustering stimuli. The motivation for this undertaking is       Three displays depicted two-dimensional embeddings of real
twofold. First and foremost, we hope to gain intuitions about       data. Eight displays contained lines, circles or a combination
the methods humans use to choose k and use those intuitions         of the two. The final 11 displays consisted of other synthetic
to develop better k-choosing algorithms. The results of this        data transformed by a variety of nonlinear distortions. See
endeavor will be discussed later on. Second, we hope to cre-        Fig. 2 for thumbnails of all the displays used. Subjects al-
ate a comprehensive and detailed data set representing hu-          ways saw the displays as white points on a black background,
man clustering behavior that can be used as a standard against      but for the sake of presentation the displays in this paper are
which to measure algorithmic performance, and to fuel inno-         black on white and the points have been increased in size.
vation in this branch of machine learning.                             We focused heavily on mixture of Gaussian data sets due
                                                                    to the prominence of the Gaussianity assumption in the ma-
                        Human Data                                  chine learning literature [4][5][6]. We also used several data
Eighteen undergraduate human subjects were recruited for            sets with uniform noise in order to investigate how subject
this project, 11 female and 7 male, to determine the num-           responses varied with sample size and to what extent sub-
ber of groups present in 50 distinct point light displays. Each     jects saw patterns where none were justified by the underly-
point light display was presented at two different scales and       ing distribution. Our shape-based and distorted displays were
two different rotations, for a total of four presentations per      included for breadth and represent a case where the data are
display and 200 trials per subject. Subjects were asked to          drawn from no standard underlying distribution.
determine the number of groups in each display and were en-            Though all of the data sets are two-dimensional, we antic-
couraged to give more than one answer if appropriate. There         ipate that insights gained from this study will lead to algo-
was no time limit for response. Subjects were told to ignore        rithmic improvements even in high-dimensional spaces. Cer-
answers above 20 and to focus on “the bigger picture” to find       tain algorithms (such as the Eigengap algorithm discussed
a reasonable answer less than 20. In addition to k judgments,       below) operate over affinity matrices that are insensitive to
response times and sequence information were recorded. The          the underlying dimensionality of a data set. Thus, improve-
sequence of trial presentations was structured into four blocks     ments in these algorithms as measured by similarity to hu-
of 50 randomly ordered trials each, with each block consist-        man performance in two dimensions will likely scale to high-
ing of a unique permutation of every point light display. After     dimensional data.
the subjects completed all 200 trials, they were interviewed in
order to gain insight into their techniques. The interview con-                                Results
sisted of two questions:
                                                                    Several interesting trends emerge in the human responses. In
• What strategies did you use for this task?                        the interview section of the study, subjects predominantly re-
                                                                    port two central strategies: looking for areas of greatest den-
• Were any of the displays harder than the others?                  sity, perhaps separated by empty space (N = 13), and count-
                                                                316

                                                                  collection (mixture) of objects (e.g. arcs or Gaussians) and
                                                                  then explicitly count the number of those objects regardless
                                                                  of overlapping density. Rarer strategies include grouping by
                                                                  shape orientation (N = 1), and grouping by shape type (if
                                                                  there are both circles and lines in a display, there are two
                                                                  groups, N = 1). Finally, one subject explicitly mentions a
                                                                  hierarchical strategy, where he or she searches for small clus-
                                                                  ters first, and then groups them into larger clusters.
                                                                     Subjects cite two main sources of difficulty: displays con-
                                                                  taining very few data points (N = 9) and displays with lots
                                                                  of (often overlapping) shapes (N = 12). A few subjects con-
                                                                  sider displays with random noise to be difficult (N = 3).
                                                                     We find echoes of these subjective measures in the choices
                                                                  of k that humans make. Insofar as the distribution over k is
                                                                  less peaked (has higher entropy) for a particular data set, one
                                                                  might interpret that data set as more difficult. Conforming
                                                                  with interview responses indicating that small sample sizes
                                                                  cause difficulty, we can see in Fig. 3 that entropy decreases as
                                                                  sample size increases for displays of uniform noise.
                                                                     In concordance with interview responses indicating two
                                                                  primary strategies, we find several examples of bimodal re-
                                                                  sponses for displays where these two strategies would di-
                                                                  verge. Some examples are shown in Fig. 4.
                                                                     In all of the mixture of Gaussian cases, humans perform
                                                                  very consistently. In cases where the Gaussians have low
                                                                  variance and well separated means, almost all subjects indi-
                                                                  cate the correct number of Gaussians. Where the Gaussians
                                                                  have high variance and close means, humans generally agree
                                                                  on a tight range of values for k that corresponds to the num-
                                                                  ber of “blobs” in the display. See Fig 5 for some examples of
                                                                  these results.
                                                                                            Strategies
                                                                  Based on the observations discussed above, humans follow at
                                                                  least two broad strategies when choosing k, density strategies
                                                                  and model fitting strategies. In this section, two algorithms
                                                                  from recent work in the field that represent these two strate-
                                                                  gies will be briefly described and their performance compared
                                                                  to the human data.
                                                                  Density Strategies
                                                                  Density strategies discover clusters by looking for regions
                                                                  of low density between groups of points, following density
                                                                  within groups to find all the points that belong to them, and
                                                                  attempting to ignore low density noise. Several algorithms
                                                                  have endeavored to formalize these strategies, notably [7].
                                                                     A more recent algorithm [8], which this paper will refer to
                                                                  as the Eigengap algorithm, brings similar strategies for find-
                                                                  ing k under the spectral clustering umbrella. The Eigengap
                   Figure 2: The stimuli.                         algorithm treats each data point as a node on a graph, and
                                                                  then performs a random walk between the nodes, with the
                                                                  probability of transitioning between any two nodes weighted
                                                                  by the distance between them. If two nodes are close together
ing shapes or blobs (N = 11). Many of those subjects report       then the probability of transitioning from one to the other will
using both strategies (N = 9). The latter strategy can be         be high and if two nodes are far apart then the probability of
interpreted as a model fitting strategy, where subjects see a     transitioning from one to the other will be low. Thus, if a
                                                              317

                                                                       Figure 4: A sample of displays that elicited bimodal re-
                                                                       sponses from subjects.
Figure 3: Responses to uniform noise. Displays with few
samples present significant ambiguity. As sample size in-
creases, entropy decreases.
group of points is separated by a large distance from the rest
of the data, a random walk will be unlikely to transition across
that gap. In this case, all the points within the group will have
a high probability of ending up on other points in the group
and little probability of ending up outside the group.
   A matrix, P , representing the probability of any point end-        Figure 5: Human responses are generally consistent for mix-
ing up at any other point in the data set will therefore be block      ture of Gaussians data sets.
diagonal if there are distinct groups within the data set that are
separated by sufficient distance. This block diagonal structure
is represented by the n largest eigenvalues of P , and eigen-          between neighboring eigenvalues sorted in descending order,
values greater than the nth will generally be much smaller             one can find a useful estimate of the number of groups in the
than the first n eigenvalues. By finding the largest difference        data. For example, if the difference between the third and
                                                                   318

fourth eigenvalues is 0.4 and that distance is greater than the
distance between all other adjacent eigenvalues, then there
are likely to be three groups in the data.
   As the random walk progresses the Eigengap algorithm
naturally finds groups of coarser and coarser structure. Over
an increasing number of steps, a random walk will become
more and more likely to cross over low density sections of the
data set, and thus two groups that initially might be separated
will over time merge and lower values for k will be discov-
ered. In this way the Eigengap algorithm can respond well to
hierarchical data given a sufficiently long random walk.
   The implementation of the Eigengap algorithm in this pa-
per uses a small tweak as compared to [8]. Given a data set
with N points, the authors of [8] suggest searching over N
possible values for σ, a parameter used in generating the tran-
sition probability matrix, between the minimum and maxi-
mum pairwise distances in the data set. The algorithm used
in this paper searched over 10 possible values for σ in order
to drastically reduce computation time while still investigat-
ing a reasonable range of values. Also, given the large (over
10,000) number of points in some of the data sets, a sparse
implementation of the Eigengap algorithm was used, with
pairwise distances only calculated between nearest neighbors
(and the number of nearest neighbors equal to one percent of
the total number of points in the data set).
Model Fitting Strategies
Several model fitting strategies based on an assumption of
mixture of Gaussian distributed data have been proposed in
the past [4] [5]. This section describes a recent variant called
                                                                     Figure 6: Sample human (left) versus combined Eigengap
PG-means [6]. PG-means searches for Gaussian clusters in
                                                                     and PG-means (right) probability distributions over k.
a data set using an iterative process. The algorithm is ini-
tialized with k = 1 and it attempts to find an appropriate
centroid and covariance matrix for a single Gaussian cluster         considered the model distributions for purposes of calculat-
given the data using the Expectation-Maximization (EM) al-           ing KL.
gorithm. PG-means then randomly projects the data set and               Unsurprisingly, given its ability to return multiple values
the Gaussian model down to one dimension n times (we used            of k and discover hierarchical organization, Eigengap outper-
n = 10). The Kolmogorov-Smirnov (KS) test is applied to              forms PG-means with a sum KL divergence of 269.1 com-
each projection and if every KS test indicates a sufficiently        pared to 316.2 for PG-means. A simple unweighted combi-
good fit (as measured by a parameter α that was set to 0.001)        nation of the two, however, performs better than either algo-
then the current value for k is accepted. Otherwise, k is in-        rithm on its own with a sum KL divergence of 245.8 (an im-
cremented by one and the entire process is repeated.                 provement of 8.7 percent over the Eigengap algorithm). See
   If PG-means did not find an answer less than k = 20, the          Fig. 6 for some sample comparisons of this combined result
algorithm was halted and its response considered to be k = 1.        to human responses.
Note that unlike the Eigengap algorithm, PG-means will only
give one possible value for k.                                                        New Density Strategies
                                                                     While the Eigengap algorithm performs its function of fol-
Comparison with Human Data
                                                                     lowing density well, this paper proposes two novel strategies
To broadly compare Eigengap and PG-means performance                 that use density in other ways. Both of the techniques pro-
with human performance, both the human results and the al-           posed are based on leveraging higher order density informa-
gorithmic results are interpreted as probability distributions       tion than traditional pairwise distances or affinities.
over k. The sum Kullback-Leibler (KL) divergence is then                First, we are developing an algorithm that intelligently
calculated between the human results and both Eigengap and           culls uninformative samples from a dataset in order to in-
PG-means over all 50 data sets. The human results are con-           crease the accuracy and decrease the computational com-
sidered the true distribution and the algorithmic results are        plexity of k-choosing algorithms. These uninformative sam-
                                                                 319

                                                                     pared to 269.1 for the standard Eigengap algorithm). Inter-
                                                                     estingly, combining this modified version of Eigengap with
                                                                     PG-means nets only a 2.1 percent improvement over the mod-
                                                                     ified Eigengap alone (compared to the 8.7 percent improve-
                                                                     ment when combining PG-means with standard Eigengap),
                                                                     indicating that sensitivity to density changes might be part of
                                                                     what drives model fitting strategies in humans. See Fig. 7 for
                                                                     a comparison across all algorithms.
                                                                                                Conclusion
                                                                     Finding reasonable values for k is an important and difficult
                                                                     problem in unsupervised machine learning. As one can see
                                                                     from the samples in Fig. 6, current algorithms do well in cer-
Figure 7: The KL divergence scores for PG-means, Eigen-              tain situations and very poorly in others. By further inves-
gap, their combination, and the enhanced version of Eigen-           tigating human performance and attempting to apply the in-
gap. (Note that the Y-axis is scaled to make the distinctions        sights garnered from such investigation, substantial progress
more visible.)                                                       can be made in developing new algorithms to tackle this
                                                                     thorny problem.
ples should correspond to the less dense or “bright” sam-                                  Acknowledgments
ples that humans tend to ignore. Consider a sample, x~i , and        The author would like to thank Virginia de Sa, Gedeon Deák
the set of its κ (kappa) nearest neighbors,      νi . Define the     and Marta Kutas for valuable feedback on this project. This
neighborhood variance of x~i as σi2 = n∈νi ||x~i − x~n ||2 /κ
                                         P
                                                                     work was supported by NSF IGERT Grant #DGE-0333451 to
and define the normalized neighborhood variance of x~i as            GW Cottrell/VR de Sa.
norm(σi2 ) = σi2 / minn∈νi (σn2 ). Remove all samples from
the dataset whose norm(σi2 ) measure is above threshold.                                        References
Both this threshold and κ can be reasonably set based on             [1] A. Y. Ng, M. Jordan, and Y. Weiss. On spectral clustering:
the number of samples in the dataset and the potential range              analysis and an algorithm. In T. G. Dietterich, S. Becker, and
                                                                          Z. Ghahramani, editors, Advances in Neural Information Pro-
of k. In a mixture of Gaussians setting, samples with high                cessing Systems 14, pages 849–856. MIT Press, Cambridge,
norm(σi2 ) will be far from the mean of their underlying dis-             MA, 2002.
tribution, and thus less prototypical than points with lower         [2] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of hu-
norm(σi2 ).                                                               man segmented natural images and its application to evaluating
   For an intuition on how this might help a density based                segmentation algorithms and measuring ecological statistics. In
                                                                          Proc. 8th Int’l Conf. Computer Vision, volume 2, pages 416–
approach to choosing k, consider a simple mixture of two                  423, July 2001.
Gaussians whose means are well separated but whose sam-
                                                                     [3] J.M. Santos and J. Marques de Sá. Human clustering on bi-
ples overlap in some part of the space. An Eigengap-style al-             dimensional data: An assessment. Technical Report 1, INEB
gorithm will be able to traverse points across both Gaussians             Instituto de Engenharia Biomédica, Porto, Portugal, 2005.
quite easily due to this overlap, leaving little indication that     [4] Dan Pelleg and Andrew Moore. X-means: Extending K-means
there are two clear clusters. By culling all but a small number           with efficient estimation of the number of clusters. In Proc.
of points with the lowest normalized neighborhood variance                17th International Conf. on Machine Learning, pages 727–734.
                                                                          Morgan Kaufmann, San Francisco, CA, 2000.
this “bridge” between the two Gaussians is removed.
   Second, we propose an extension to spectral clustering            [5] Greg Hamerly and Charles Elkan. Learning the k in k-means.
                                                                          In Advances in Neural Information Processing Systems, vol-
based on the observation that human subjects consider sam-                ume 17, 2003.
ples with large differences in neighborhood variance to be
                                                                     [6] Yu Feng and Greg Hamerly. Pg-means: learning the number
likely drawn from different clusters (though this is not the              of clusters in data. In B. Schölkopf, J. Platt, and T. Hoffman,
case when the variance changes smoothly across space).                    editors, Advances in Neural Information Processing Systems 19,
σi2 can be interpreted as the projection of x~i into a one-               pages 393–400. MIT Press, Cambridge, MA, 2007.
dimensional space, and a new pairwise affinity matrix be-            [7] Martin Ester, Hans-Peter Kriegel, Jorg Sander, and Xiaowei Xu.
tween samples can be created based on distances in this space.            A density-based algorithm for discovering clusters in large spa-
By adding this new affinity matrix as a second view to spec-              tial databases with noise. In Evangelos Simoudis, Jiawei Han,
                                                                          and Usama Fayyad, editors, Second International Conference
tral clustering one might expect to obtain results more simi-             on Knowledge Discovery and Data Mining, pages 226–231,
lar to human judgments. Early data from applying this tech-               Portland, Oregon, 1996. AAAI Press.
nique to the algorithm in [8] are promising and support this         [8] A. Azran and Z. Ghahramani. Spectral methods for automatic
expectation. A version of this technique improves Eigengap                multiscale data clustering. Computer Vision and Pattern Recog-
performance as compared to human performance by approx-                   nition, 2006 IEEE Computer Society Conference on, 1:190–197,
                                                                          17-22 June 2006.
imately 11 percent with a sum KL divergence of 239.7 (com-
                                                                 320

