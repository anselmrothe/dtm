UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning a Theory of Causality
Permalink
https://escholarship.org/uc/item/7tn8v20n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Goodman, Noah
Tenenbaum, Joshua
Ullman, Tomer
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                            Learning a Theory of Causality
                                Noah D. Goodman, Tomer D. Ullman, Joshua B. Tenenbaum
                                                        {ndg, tomeru, jbt}@mit.edu
                                               MIT, Dept. of Brain and Cognitive Sciences
                              Abstract                                   pects of probability and logic (Kemp, Goodman, & Tenen-
                                                                         baum, 2008; Tenenbaum, Griffiths, & Niyogi, 2007; Good-
   We consider causality as a domain-general intuitive theory
   and ask whether this intuitive theory can be learned from co-         man, Tenenbaum, Griffiths, & Feldman, 2007). Because the
   occurrence of events. We begin by phrasing the causal Bayes           assumptions of CBN are formalizable via probability and
   nets theory of causality, and a range of alternatives, in a logi-     logic, they are potentially expressible in such a language for
   cal language for relational theories. This allows us to explore
   simultaneous inductive learning of an abstract theory of causal-      intuitive theories. This suggests the hypothesis that CBN
   ity and a causal model for each of several causal systems. We         is not an innate resource, but is itself an intuitive theory of
   find that the correct theory of causality can be learned rela-        causality, learned inductively from evidence and represented
   tively quickly, often becoming available before specific causal
   theories have been learned—an effect we term the “blessing of         in a more basic language of theories.
   abstraction”. We then explore the effect of providing a vari-            A theory of causality would have several properties un-
   ety of auxiliary evidence, and find that a collection of simple       usual for an intuitive theory. First, it would be domain-
   “input analyzers” can help to bootstrap abstract knowledge.
   Together these results suggest that the most efficient route to       general knowledge. Intuitive theories are typically thought of
   causal knowledge may be to build in not an abstract notion of         as domain-specific knowledge systems, organizing our rea-
   causality, but a powerful inductive learning mechanism and a          soning about domains such as physics or psychology, but
   variety of perceptual supports. While these results are purely
   computational, they have implications for cognitive develop-          there is no a priori reason to rule out domain-general knowl-
   ment, which we explore in the conclusion.                             edge. Second, a theory of causality would have to be acquired
   Keywords: Causality, hierarchical Bayesian model, innate-             remarkably early in development. There are intriguing hints
   ness.                                                                 that aspects of causal knowledge are present from early in-
                                                                         fancy, but little evidence that a full notion of cause is innate
                          Introduction                                   (Saxe & Carey, 2006). Yet if a theory of causality is to un-
What allows us to extract stable causal relations from the               derly the acquisition of specific causal knowledge it must be
stream of experience? Hume believed that it was the princi-              available within the first year of life. Could such an abstract
ple of association: constant conjunction of events follow from           theory be learned from evidence so rapidly, even in principle?
an underlying association; from this principle, and observed             To investigate this question we turn to hierarchical Bayesian
events, one may infer a causal association. Recent research              modeling.
in psychology (Pearl, 2000; Gopnik et al., 2004) and philos-                The formalism of hierarchical Bayesian modeling makes
ophy (Woodward, 2003) has established the interventionist                it possible to express the assumptions relating knowledge at
or causal Bayes nets (henceforth CBN) account of causality               multiple levels of abstraction (Gelman, Carlin, Stern, & Ru-
as a description of the principles by which causal reasoning             bin, 1995), and Bayesian inference over such a model de-
proceeds. These principles include a directed, probabilistic             scribes an ideal learner of abstract knowledge (Tenenbaum,
notion of causal dependence, and a privileged role for un-               Griffiths, & Kemp, 2006). Though real learning is undoubt-
caused manipulation—the interventions, which include ac-                 edly resource-constrained, the dynamics of an ideal learner
tions and experimental manipulations. The CBN framework                  can uncover unexpected properties of what it is possible to
leads to quicker, more reliable learning than weaker assump-             learn from a given set of evidence. For instance, it has
tions about the nature of causation, and has been successful             been reported (e.g. Kemp, Perfors, & Tenenbaum, 2007) that
at predicting human learning (e.g. Gopnik et al., 2004). In              learning at the abstract level of a hierarchical Bayesian model
recent discussions of the psychological basis of causality it is         is often surprisingly fast in relation to learning at the more
often assumed that the principles of CBN, or some variant,               specific levels. We term this effect the blessing of abstrac-
are an innate resource. In this paper we will argue there is             tion1 : abstract learning in an HBM is often achieved before
an alternative: that the principles guiding causal understand-           learning in the specific systems it relies upon, and, as a result,
ing in humans can be seen as an intuitive theory, learnable              a learner who is simultaneously learning abstract and specific
from evidence out of more primitive representations. Our ar-             knowledge is almost as efficient as a learner with an innate
gument, which will proceed via an ideal learner analysis, can            (i.e. fixed) and correct abstract theory. Hierarchical Bayesian
be seen as both an investigation into the psychological basis            models have been used before to study domain-specific ab-
of causality, and a case-study of abstract learning and the role         stract causal knowledge (Kemp, Goodman, & Tenenbaum,
of innate structure in Bayesian approaches to cognition.                 2007), and simple relational theories (Kemp et al., 2008).
   We have previously proposed that intuitive theories—                  Here we combine these approaches to study knowledge of
systems of abstract concepts and laws relating them—can be
represented in a “language of thought” which includes as-                    1 Cf. the “curse of dimensionality”.
                                                                     2188

              Law #1:              ∀x ∀y A(x) → ¬R(y, x)              Interventions are exogenous.
              Law #2:               ∀x A(x) → Fy R(x, y)              Interventions have at most one child.
              Law #3:                 ∀x F1 (x) → A(x)                Feature 1 is diagnostic for interventions.
              Law #4:                 ∀x F2 (x) → A(x)                Feature 2 is diagnostic for interventions.
              Law #5:           ∀x ∀y R(x, y) ∨ R(y, x) ∨ x=y         Dependence graph is fully connected.
              Law #6:                  ∀x ∀y ¬R(x, y)                 Dependence graph is unconnected.
              Law #7:                   ∀x Fy R(x, y)                 Variables have at most one child.
              Law #8:                   ∀x Fy R(y, x)                 Variables have at most one parent.
              Law #9:        ∀x ∀y ∀z R(x, y) ∧ R(y, z) → R(x, z)     Dependence graph is transitive.
              Law #10:             ∀x ∀y A(x) → ¬R(x, y)              Interventions have no children.
              Law #11:              ∀x Fy ¬A(y) ∧ R(y, x)             Variables have at most one non-intervention parent.
Figure 1: Eleven laws that can be expressed in the language for theories. The informal gloss on the right describes what each law might mean
within a theory of causality.
causality at the most abstract, domain general level.                     dependence. (CBN3) There is a preferred set of variables, the
   We will also explore the possibility that learning at the ab-          “interventions”, which are outside the system—they depend
stract level in an HBM, and the blessing of abstraction, can              on nothing. (CBN4) Interventions influence only one vari-
be substantially aided by providing appropriate low-level fea-            able. (CBN5) The intervention set is known for each causal
tures in the input. Our motivation for considering this possi-            system. In addition, assumptions are often made about the
bility is a suggestion by Carey (2009) that part of infants’              functional form of dependence (for instance, that interven-
core knowledge is in the form of perceptual input analyzers:              tions are “arrow breaking”). For simplicity we will address
modules that perform simple transformations of raw percep-                only the aspects of this theory that determine the structure of
tual input, making it suitable for conceptual cognition. We               the dependency relation and will assume (CBN1).
hypothesize that these perceptual input analyzers do not pro-
vide abstract conceptual knowledge directly, but instead serve            A language for theories of causal dependence
to make latent abstract concepts more salient and thus more               We wish to specify a hypothesis space of alternative theo-
learnable. For instance, the feeling of self-efficacy, advo-              ries of the dependency relation, R. This space should contain
cated by Maine de Biran as a foundation of causality (see                 CBN and a wide set of alternative theories, and should build
discussion in Saxe & Carey, 2006), could be an analyzer                   these theories by combining simple primitive units. Kemp et
which highlights events resulting from one’s own actions,                 al. (2008) proposed a very flexible language for expressing
making the latent concept of intervention more salient. Al-               relational theories, which is a small extension of first-order
together this suggests a novel take on nativism—a “minimal                logic, and used this language to predict the inductive gener-
nativism”—in which strong, but domain-general, inference                  alization of human learners in a novel domain. We propose
and representational resources are aided by weaker, domain-               that a version of this language can be used to capture domain-
specific perceptual input analyzers.                                      general knowledge, including (aspects of) a theory of causal-
   In the following sections we first formalize aspects of CBN            ity.
within a logical language for intuitive theories. We then study              The language we use contains logical operators: quanti-
the ideal learner of causal knowledge, investigating the speed            fiers over causal variables—“for all” (∀), “there exists” (∃),
of learning at different levels of abstraction, and the effect of         and “there exists at most one” (F)—and logical connectives—
perceptual input analyzers on learning speed.                             not (¬), and (∧), or (∨), if (←). In addition to the logical
                                                                          operators, and the causal dependence relation R(·, ·), the lan-
                    Theories of causality                                 guage contains invented predicates and observed predicates.
Causality governs the relationship between events. Formaliz-              Invented predicates are not observable, or pre-defined, but
ing this, the world consists of a collection of causal systems,           have a conceptual role in the theory. We restrict in this paper
in each causal system there is a set of observable causal vari-           to at most one invented predicate, A(·); this predicate need
ables. Causal systems are observed on a set of trials—on                  not a priori relate to causality in an interesting way, but it
each trial, each causal variable has a value. (We will call an            will play the role of defining intervention in the correct the-
observation of a causal variable on a particular trial an event.)         ory. Finally, the two predicates, Fi (·), are observable features
   The causal Bayes nets theory of causation (Pearl, 2000) de-            of variables. These can be thought of as perceptual input ana-
scribes the structure of dependence between events, isolating             lyzers extracting some feature of events2 , which may or may
a special role for a set of interventions. CBN can be seen as a           not be useful in a theory of causality.
collection of assumptions about causal dependence: (CBN1)                     2 It is most realistic to think of input analyzers operating at the
Dependence is directed, acyclic, and can be quantified as con-            level of specific events; we idealize them as features of causal vari-
ditional probability. (CBN2) There is independence / indirect             ables (i.e. types of events).
                                                                    2189

   Fig. 1 gives examples of laws that can be expressed in this
language. These laws include those needed to capture the                          Theory                  ∀x, y A(x) → ¬R(y, x)
CBN theory of causality, as well as a variety of plausible vari-                       T
ants describing alternative restrictions on dependency. Within
this set of laws (CBN3) corresponds to Law #1; (CBN4) cor-
                                                                                                             System1:        System2:
responds to Law #2; (CBN5) follows from Laws #3 and/or                             Causal
#4 when the features can be used to identify interventions;
(CBN2) is the lack of Laws #5 or #9.                                               model
                                                                                     Rs,As
A hierarchical Bayesian model                                                                                                Variables
                                                                                                             1 0 1 1         1 0 1 1
To ground this language for theories into observed events in
a set of causal systems, we construct a hierarchical Bayesian                      Events                    0 0 0 1         0 0 0 1     Trials...
model with theories of causality at the most abstract level and                       ds,t                   1 1 0 1         1 1 0 1
events at the most specific level (Fig. 2). We first describe the                                            1 0 0 1
generative process of this model, then we describe the ideal                                   Trials t
learner by inverting this process using Bayes’ rule.                                         Systems s
Generating a theory A causal theory—represented in the
                                                                       Figure 2: The hierarchical Bayesian model, and examples of the
theory language described in the previous section—is drawn
                                                                       information at each level. The causal dependence relation R(·, ·)
from the prior distribution over theories, P(T ). We take P(T )        is shown as directed edges between variables (circles), the latent
to be uniform over theories (of size less than some maxi-              predicate A(·) is shown as shading of the variables. Binary events
mum). While a representation-length prior (see Kemp et al.,            for each system, trial, and variable are shown as contingency tables.
2008) would naturally capture a bias for simpler theories, we
choose a uniform prior in order to focus on the dynamics of            Where the likelihood is given by:
learning driven entirely by the hierarchical setup.
                                                                                    P(D|T ) = ∏ P(Ds |T )
Generating causal models Next a causal model is gener-                                           s
ated for each causal system s. A causal model is an instantia-                                = ∏ ∑ P(Ds |A, R)P(A, R|T )
tion of each predicate in the theory—Rs and, if it is used, As .                                 s A,R
                                                                                                                                                 (3)
Following (Kemp et al., 2008), we will assume that the distri-
                                                                                              = ∏ ∑ P(Ds |R)P(A, R|T )
bution on causal models, P(As , Rs |T ), is uniform over those                                   s A,R
consistent with T —that is, the instantiations of Rs and As that
satisfy the logical laws of T .                                        System marginals The effect of an abstract theory on
                                                                       learning in a specific system, s, may be described by the pos-
Generating events Each causal model in turn generates ob-
                                                                       terior belief distribution over Rs . If we fix a theory, T , and
served events (a value for each variable) for a set of trials.
                                                                       use this to provide the prior over Rs , the posterior is given by:
The probability of generating a series of trials D = {dt } from
a system with causal relation R is given by:                                           P(Rs |T, Ds ) ∝ P(Ds |Rs )P(Rs |T )                       (4)
                       Z
                                                                       If the theory is not fixed, but is learned simultaneously with
            P(D|R) =       ∏ P(dt |R, Θ)P(Θ|α)dΘ             (1)
                                                                       the causal systems, we may still want to capture what has
                           t
                                                                       been learned about one specific system within the hierarchical
Where the conditional probability tables, Θ, list the probabil-        setup. This is given by the posterior marginal of Rs :
ity of each event given each set of values for its parents in R.
                                                                                      P(Rs |D) = ∑ P(Rs |T, D)P(T |D)
We make the weak assumption that each entry of Θ is drawn                                            T
independently from a symmetric-beta distribution with hyper-                                                                                     (5)
                                                                                                 = ∑ P(Rs |T, Ds )P(T |D)
parameter α. The integral in Eq. 1 is a product of standard                                          T
beta-binomial forms, which can be integrated analytically.
                                                                                       Ideal learner simulations
Theory induction                                                       To investigate the dynamics of learning in the theory induc-
                                                                       tion framework outlined above, we performed a series of sim-
The ideal Bayesian learner infers a posterior belief distribu-         ulation studies.
tion over theories from a set of observed trials across a range           The probability landscape of this model is complex, mak-
of causal systems. The posterior probability of a theory, T ,          ing it difficult to accurately characterize learning at all levels
given data, D = {Ds } is given by:                                     of abstraction. To ensure correct results, we chose to imple-
                                                                       ment the learning model by explicit enumeration over theo-
                    P(T |D) ∝ P(D|T )P(T )                   (2)       ries and causal structures. To make this enumeration possible
                                                                    2190

 (a)                                                                                     (b)                                                                                                                                       (c)
                            3                                                                                                      3
                          10                                                                                                     10                                                                                                              1
                                                                  Mean rank                                                                                                                               No theory                                    Theory learning
                                                                  10th percentile                                                                                                                         Learned theory                               System, no theory
                                                                                         Mean rank of correct causal structure
                                                                  50th percentile                                                                                                                         Correct theory                               System, correct theory
                                                                  90th percentile                                                                                                                                                                      System, learned theory
                                                                                                                                                                                                                                               0.75
 Rank of correct theory
                            2                                                                                                      2
                          10                                                                                                     10
                                                                                                                                                                                                                                   % Learned
                                                                                                                                                                                                                                                0.5
                            1                                                                                                      1
                          10                                                                                                     10
                                                                                                                                                                                                                                               0.25
                            0                                                                                                      0
                          10 2     3                          4                      5
                                                                                                                                 10 2         3                                                   4                            5
                                                                                                                                                                                                                                                 0 2                    3                       4     5
                            10   10                      10                     10                                                 10    10                                                     10                     10                        10                   10                      10    10
                                       Total # samples                                                                                            Total # samples                                                                                                           Total # samples
Figure 3: (a) Rank of the correct theory, mean and 10th/90th percentiles across 100 model runs. (b) Rank of the correct causal structure
(mean over systems and runs), given no theory, fixed correct theory, and simultaneously learned theory. Learning abstract knowledge always
helps relative to not having a theory, and is quickly as useful as an innate, correct theory. (c) The probability of correct learning: the fraction
of systems in which the correct structure has been learned (is at rank 1), and the fraction of runs in which the correct theory has been learned.
(In each run there were 50 systems, and one feature perfectly diagnostic of interventions. Hyperparameter α=0.5.)
we restricted to theories which can be formed as a conjunc-                                                                                       ture than having no theory. Comparing the learned-theory
tion of at most five of the laws shown in Fig. 1, and to sys-                                                                                     curve to the no-theory curve, we see that abstract knowl-
tems of only four variables. (Counting only theories with a                                                                                       edge helps at all stages of learning, despite having to learn
satisfying causal model, there are 691 theories in the set we                                                                                     it. Comparing the learned-theory curve with the innate-theory
considered. There are 543 possible causal structures R, and                                                                                       curve shows that by around 60 samples per system the theory
16 possible intervention sets A.)                                                                                                                 learner has matched the performance of a learner endowed
   For each run of the model we generated evidence for the                                                                                        with an innate, correct theory. Thus, the abstract layer of
learner by first choosing one variable in each system to be                                                                                       knowledge can serve a role as inductive bias even when the
an intervention, then generating a causal model for each sys-                                                                                     abstract knowledge itself must be learned—learning a theory
tem (consistent with the correct, CBN, theory of causality)                                                                                       of causality is as good (from the perspective of causal model
and data for each trial according to the generative process de-                                                                                   learning) as having an innate theory of causality.
scribed above. We initially fixed the number of systems to
50, and included one feature which correlates perfectly with                                                                                                                              0
                                                                                                                                                                                                      5 systems
intervention and another which is uncorrelated with interven-                                                                                                                                         10 systems
                                                                                                                                                                                                      20 systems
tion; we consider the effect of varying these conditions below.                                                                                                                          −1           50 systems
                                                                                                                                                                                                      150 systems
   We explore the dynamics of learning by varying the                                                                                                                                                 500 systems
                                                                                                                                                        Log−posterior of corect theory
amount of evidence given to the learner, as measured by the                                                                                                                              −2
total number of samples (i.e. trials) across all systems, with
each system given the same number of samples. The ideal                                                                                                                                  −3
Bayesian learner is able to learn the correct theory, given suf-
ficient evidence (Fig. 3a). This, by itself, is unsurprising—                                                                                                                            −4
indeed, Bayesian induction is guaranteed to converge to the
                                                                                                                                                                                         −5
correct hypothesis in the limit of an infinite amount of evi-
dence. It is more interesting to see that learning the correct
                                                                                                                                                                                         −6
theory appears relatively quick in this model (being achieved
with fewer than 30 samples per system in most runs).
                                                                                                                                                                                         −7 1                              2                                  3                         4
                                                                                                                                                                                          10                          10                                   10                         10
The blessing of abstraction                                                                                                                                                                                                           Total # samples
Abstract knowledge acts as an inductive bias, speeding the                                                                                        Figure 4: The posterior log-probability of the correct theory as a
                                                                                                                                                  function of total number of samples across systems, for different
learning of specific causal structure. Fig. 3b shows the mean                                                                                     numbers of systems. Each curve starts at 2 samples per system.
rank of the correct causal structure across systems with no                                                                                       Learning is best when evidence is gathered from many systems, even
abstract theory (i.e. a uniform prior over causal relations),                                                                                     when only a few samples are taken in each system.
with innate (i.e. fixed) correct theory, and with learned the-
ory (i.e. with the theory learned simultaneously with spe-                                                                                           How can abstract knowledge appropriately bias specific
cific causal models). We see, as expected, that the correct                                                                                       learning, when it must be learned itself? Comparing Fig. 3a
abstract theory results in quicker learning of causal struc-                                                                                      to Fig. 3b suggests that the correct theory is learned before
                                                                                                                                        2191

                                 1                                                                      served predicate F1 , is learned first, but is closely followed
                                0.9                                                                     by Law #1, which defines the main role of interventions in
                                                                                                        CBN. Slightly later, Law #2—specifying that interventions
                                0.8
                                                                                    Law #1              effect only one variable—is learned. All other laws slowly
  Marginal probability of law
                                0.7                                                 Law #2              drop off as the correct theory becomes entrenched. The grad-
                                                                                    Law #3
                                0.6                                                 Law #4              ual learning curves of Fig. 5, which are averaged over 100
                                                                                    Law #5              runs of the model, belie the fact that learning of the laws was
                                0.5                                                 Law #6              actually quite abrupt in most runs. Though the exact timing
                                                                                    Law #7
                                0.4                                                 Law #8              of these learning events was distributed widely between runs,
                                                                                    Law #9              the order of acquisition of the laws was quite consistent: in
                                0.3                                                 Law #10             two-thirds of runs Laws #1 and #3 were learned almost simul-
                                                                                    Law #11
                                0.2                                                                     taneously, followed later by Law #2. (To be precise, in 92%
                                0.1                                                                     of runs Law #2 was learned last, as measured by number of
                                                                                                        samples required to cross probability 0.75; of these runs, Law
                                 0 2
                                 10                 10
                                                      3                      4
                                                                            10              10
                                                                                                 5      #1 led Law #3 on 59% of runs, but the two laws were learned
                                                          Total # samples                               within one step of each other on 74% of runs.) This obser-
                                                                                                        vation may be significant given that cognitive development is
                                 Figure 5: Learning curves for the eleven laws of Fig. 1.               characterized by wide variation in timing of acquisition, but
                                                                                                        remarkable consistency in order of acquisition.
most of the correct causal structures. In Fig. 3c we have in-                                           A minimal nativism
vestigated this by plotting the probability of learning (defined                                        Thus far we have assumed that there is an observed feature
as the correct hypothesis being most probable), at the lev-                                             which can be used to tell when a variable is an interven-
els of both systems and theories. We see that learning at the                                           tion. We can imagine that this feature provides informa-
abstract theory level is much faster than at the system level.                                          tion extracted from perception of the observed events—that
Further, the time to correct learning at the system level is al-                                        is, it results from an input analyzer (Carey, 2009): an in-
most identical for innate-theory and learned-theory, which are                                          nate mechanism that performs simple transformations of per-
both faster than no-theory. This illustrates the fact that ab-                                          ceptual evidence. A number of relatively simple input ana-
stract learning is not bottom-up, waiting on specific learning;                                         lyzers could provide features useful for identifying interven-
instead, learning is being carried out at all levels simultane-                                         tions. For instance, the feeling of self-efficacy discussed by
ously, and here abstract knowledge is often learned before                                              Maine de Biran, or, more broadly, an innate agency-detector
specific knowledge. Note that this effect is not due to the                                             able to identify the actions of intentional agents (see Saxe &
relative size of hypothesis spaces (we consider 691 theories                                            Carey, 2006). Critically, none of these simple input analyzers
and 542 specific causal structures), nor a “helpful” choice of                                          is likely to identify all interventions (or even most), and they
prior (we use a maximum entropy—uniform—prior on the-                                                   are likely to be mixed together with features quite un-useful
ories, and for the no-theory case a similar prior on specific                                           for causal learning.
systems). Rather, this effect is driven by the ability of the                                              We simulated learning under several different “input ana-
higher level of the model to learn from a wide range of evi-                                            lyzer” conditions varying in: the number of useful features
dence drawn from multiple systems.                                                                      (the remaining feature(s) were distractors), what portion of
   To confirm that breadth of evidence is important, we con-                                            intervention variables could be identified from the useful fea-
sider the effect of distributing the same amount of evidence                                            tures, and the overlap between features. In Fig. 6 we have
among a different number of systems—is it better to spend ef-                                           plotted the marginal probability of the “intervention” portion
fort becoming an expert in a few systems, or to be a dilettante,                                        of the correct theory—Laws #1 and #2, which govern the role
learning only a small amount about many systems? Fig. 4                                                 of interventions in determining causal dependency, indepen-
shows the result of varying the number of systems, while                                                dent of the identification of interventions. We see that learn-
matching the total number of samples (resulting in differing                                            ing is extremely slow when no features are available to help
numbers of samples per system). Learning is fastest when                                                identify interventions. In contrast, learning is about equally
evidence is drawn from a broad array of causal systems, even                                            quick in all other conditions, depending slightly on the cov-
when only a few samples are observed in each system. In-                                                erage of features (the portion of interventions they identify)
deed, at one extreme learning is very slow when only five                                               but not on how this coverage is achieved (via one or multi-
systems are available. At the other extreme, learning from                                              ple features). Thus, even a patchwork collection of partial
500 systems is quick overall, and “catches up” to other con-                                            input analyzers, which pick out only a portion of intervention
ditions after only three samples per system.                                                            variables, is sufficient to bootstrap abstract causal knowledge;
   Turning to the dynamics of learning for individual laws,                                             learning can be relied on to pick the useful features from the
Fig. 5 shows the marginal probability of each of the eleven                                             distractors and to sort out the underlying truth that each par-
laws in Fig. 1. Law #3, relating interventions to the ob-                                               tially represents.
                                                                                                     2192

                                                1
                                                       F1 100%, F2 0%
                                                                                                                  for learning and representation are greatly aided by a col-
                                               0.9
                                                       F1 50%, F2 50%
                                                       F1 50%, F2 0%
                                                                                                                  lection of domain-specific “perceptual input analyzers”. It
                                                       F1 50%, F2 25% (overlapping)                               may be ontogenetically cheap to build innate structures that
    Marginal posterior of corect abstraction
                                                       F1 25%, F2 25%
                                               0.8     F1 and F2 both 0%                                          make some intervention events salient, but quite expensive
                                                                                                                  to build an innate abstract theory (or a comprehensive ana-
                                               0.7
                                                                                                                  lyzer). Since a powerful learning mechanism is present in
                                               0.6                                                                human cognition, the most efficient route to abstract knowl-
                                                                                                                  edge would then be by bootstrapping from these simple, non-
                                               0.5
                                                                                                                  conceptual mechanisms. Thus we are suggesting a kind of
                                               0.4                                                                minimal nativism: strong domain-general inference and rep-
                                                                                                                  resentational resources, aided by weak domain-specific per-
                                               0.3
                                                                                                                  ceptual input analyzers.
                                               0.2                                                                   Our results are purely computational, at the level of ideal
                                                                                                                  learning, but they provide a viewpoint that we believe will
                                               0.1 2
                                                 10                       10
                                                                            3
                                                                                                   10
                                                                                                     4
                                                                                                         10
                                                                                                           5
                                                                                                                  be useful for empirical research in cognitive development.
                                                                                 Total # samples
                                                                                                                  When young infants behave as if they have a piece of abstract
Figure 6: The marginal probability of the correct theory of interven-                                             knowledge, it is tempting to conclude that this knowledge is
tion (i.e. Laws #1 and #2) given different sets of “input analyzers”:                                             innate. This tendency may misguide—we have shown that
each condition has two features which are diagnostic of intervention
variables to the extent indicated (e.g. “F1 50%” indicates that the                                               abstract knowledge of causality, at least, can be learned so
first feature covers half of interventions). In the 50%/25% case the                                              quickly that it might seem to be innate. On the other side,
two features overlap, otherwise they are disjoint. Learning is diffi-                                             where innate structure is required to explain complex cogni-
cult when no diagnostic features are present, but quite rapid under
all other conditions.                                                                                             tion, it is often assumed to be abstract conceptual knowledge
                                                                                                                  (Carey, 2009). This should also be approached with care—
                                                                                                                  simpler innate structures, without conceptual content, may be
                                                          Discussion and conclusion                               sufficient when paired with a powerful learning mechanism.
                                                                                                                  Finally, the most obviously acquired systems of conceptual
We have studied an ideal Bayesian learner acquiring aspects                                                       knowledge are coherent explanations of a single domain, yet
of a domain-general intuitive theory of causality. This the-                                                      it may often be the broader domain-general intuitive theories
ory and a wide set of alternatives were represented in a “lan-                                                    which are acquired earliest and are most fundamental.
guage of thought” for relational theories, based upon first-
order logic. We found that the correct theory of causality can                                                                               References
be learned from little evidence, often becoming available be-                                                     Carey, S. (2009). The origin of concepts. Oxford University Press.
fore specific causal models have been learned. This enabled                                                       Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995).
the learned abstract knowledge to act as an inductive bias                                                          Bayesian data analysis. New York: Chapman & Hall.
                                                                                                                  Goodman, N. D., Tenenbaum, J. B., Griffiths, T. L., & Feldman, J.
on specific causal models nearly as efficiently as an innately                                                      (2007). Compositionality in rational analysis: Grammar-based in-
specified theory. However, this “blessing of abstraction” it-                                                       duction for concept learning. In M. Oaksford & N. Chater (Eds.),
self relied on a set of observed event features that served to                                                      The probabilistic mind: Prospects for bayesian cognitive science.
                                                                                                                    Oxford: Oxford University Press.
make the latent concept of intervention more salient.                                                             Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E., Kushnir, T.,
   The abstractness of a theory of causality proved not to hin-                                                     & Danks, D. (2004, Jan). A theory of causal learning in children:
                                                                                                                    causal maps and Bayes nets. Psychological Review, 111(1), 3–32.
der learning, given a rich language of thought and a pow-                                                         Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2007). Learn-
erful inductive learning mechanism. We found that abstract                                                          ing causal schemata. In Proceedings of the twenty-ninth annual
learning was fastest when evidence was drawn from a wide                                                            meeting of the cognitive science society.
                                                                                                                  Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2008). Theory ac-
variety of causal systems, even if only a small number of ob-                                                       quisition and the language of thought. In Proceedings of thirtieth
servations was available for each system. Because a domain-                                                         annual meeting of the cognitive science society.
general theory is able to draw evidence from the widest set of                                                    Kemp, C., Perfors, A., & Tenenbaum, J. (2007). Learning over-
                                                                                                                    hypotheses with hierarchical Bayesian models. Developmental
experiences, this suggests that domain-general intuitive theo-                                                      Science, 10(3), 307–321.
ries may, in some cases, be easier to learn than their domain-                                                    Pearl, J. (2000). Causality: models, reasoning, and inference. Cam-
specific counterparts. In future work we plan to investigate                                                        bridge University Press.
                                                                                                                  Saxe, R., & Carey, S. (2006). The perception of causality in infancy.
further the effects of distribution and variety of evidence.                                                        Acta Psychologica, 123(1-2), 144–165.
   Though we have argued that causality may be learnable,                                                         Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-based
our results should not be taken to support an entirely empiri-                                                      bayesian models of inductive learning and reasoning. Trends in
                                                                                                                    Cognitive Sciences, 10, 309–318.
cist viewpoint. We endow our learner with a rich language                                                         Tenenbaum, J. B., Griffiths, T. L., & Niyogi, S. (2007). Intuitive the-
for expressing theories and a strong inductive learning mech-                                                       ories as grammars for causal inference. In A. Gopnik & L. Schulz
anism. These are both significant innate structures, though                                                         (Eds.), Causal learning: Psychology, philosophy, and computa-
                                                                                                                    tion. Oxford: Oxford University Press.
ones that may be required for many learning tasks. In ad-                                                         Woodward, J. (2003). Making things happen : a theory of causal
dition, we have shown that the domain-general mechanisms                                                            explanation. New York: Oxford University Press.
                                                                                                               2193

