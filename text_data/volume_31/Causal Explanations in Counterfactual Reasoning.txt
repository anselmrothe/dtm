UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Causal Explanations in Counterfactual Reasoning
Permalink
https://escholarship.org/uc/item/8xp9g8r9
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Dehghani, Morteza
Iliev, Rumen
Kaufmann, Stefan
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                            Causal Explanations in Counterfactual Reasoning
                                        Morteza Dehghani (morteza@northwestern.edu)
                                                 Department of EECS, 2145 Sheridan Rd
                                                      Evanston, IL 60208-0834 USA
                                              Rumen Iliev (r-iliev@northwestern.edu)
                                              Department of Psychology, 2029 Sheridan Rd
                                                      Evanston, IL 60208-2710 USA
                                       Stefan Kaufmann (kaufmann@northwestern.edu)
                                              Department of Linguistics, 2016 Sheridan Rd
                                                      Evanston, IL 60208-4090 USA
                             Abstract                                   worlds relative to the “actual” world of evaluation. Simpli-
                                                                        fying somewhat, a counterfactual ‘If had been A, would have
   This paper explores the role of causal explanations in eval-         been C’ (A  C) is true if and only if C is true at all A-
   uating counterfactual conditionals. In reasoning about what
   would have been the case if A had been true, the localist in-        worlds that are maximally similar to the actual one. Stalnaker
   junction to hold constant all the variables that causally influ-     and Lewis account for various logical properties of counter-
   ence whether A is true or not, is sometimes unreasonably con-        factuals by imposing conditions on the underlying similarity
   straining. We hypothesize that speakers may resolve this ten-
   sion by including in their deliberations the question of what        relation, but neither attempts a detailed analysis of this no-
   would explain the hypothesized truth of A. To account for our        tion. Much of the subsequent work on modeling counterfac-
   recent psychological findings about counterfactuals, an alter-       tual reasoning can be viewed as attempts to make the notion
   native approach based on Causal Bayesian networks is pro-
   posed in which the intervention operator utilizes the agent’s        of similarity more precise.
   beliefs about the explanatory power of the antecedent of the            Recent years have seen an increased interest in the role of
   counterfactual. The results of three psychological experiments
   are reported in which the new method succeeds in predicting          causal (in)dependencies in determining speakers’ judgments
   subjects’ responses while the traditional method for evaluating      about counterfactuals, driven in large part by advances in the
   counterfactuals in Bayesian networks fails.                          formal representation and empirical verification of causal re-
   Keywords: Counterfactual Reasoning; Causal Explanations;             lations that had originated in statistics and artificial intelli-
   Causal Networks.
                                                                        gence and had since had a major impact in psychology and
                                                                        other disciplines (Spirtes et al., 1993; Pearl, 2000). The for-
                         Introduction                                   mal vehicle of choice in this area is that of Causal (Bayesian)
Counterfactual reasoning plays an important role in causal              Networks, directed acyclic graphs whose edges represent the
inference, diagnosis, prediction, planning and decision mak-            direction of causal influence and whose vertices are labeled
ing, as well as emotions like regret and relief, moral and legal        with variables. Each distribution of values over these vari-
judgments, and more. Consequently, it has been a focal point            ables corresponds to a possible state of the system represented
of attention for decades in a variety of disciplines including          by the model. Causal networks are partial descriptions of the
philosophy, psychology, artificial intelligence, and linguis-           world, thus in general each state corresponds to a class of
tics. The fundamental problem facing all attempts to model              possible worlds in the Stalnaker/Lewis sense. The standard
people’s intuitive judgments about what would or might have             analysis of counterfactual reasoning about what would have
been if some counterfactual premise A had been true, is to              been if some variable X had had value x relies on the notion
understand people’s implicit assumptions as to which actual             of an external intervention which forces X to have value x but
facts to “hold on to” in exploring the range of ways in which A         cuts all the causal links leading into X, ensuring that all those
might manifest itself. As Goodman (1955) stated it in his               variables remain undisturbed whose values are not (directly
classic example: In asking what would have been the case                or indirectly) caused by that of X. In effect, the counterfac-
if the match had been struck, people tend to infer from the             tual reasoning thus modeled is maximally “local” in the sense
presence of oxygen that it would have lit; but why not instead          that for all variables Y which do not lie “downstream” of X
infer that there would have been no oxygen from the fact that           in the direction of causal influence, the counterfactual ‘If X
it did not light? There is no principled logical difference be-         had had value x, then Y would still have its actual value’ is
hind asymmetries of this sort; something else seems to be at            invariably true.
work.                                                                      It is an empirical question whether and to what extent
   Many formal theories of counterfactual reasoning are in-             speakers’ judgments about particular counterfactuals actually
spired by the model-theoretic accounts of Stalnaker (1968)              reflect such highly localized reasoning. There is no doubt
and Lewis (1973). Minor differences aside, both crucially               that the method of blocking the flow of information from ef-
rely on a notion of comparative similarity between possible             fects to causes in modeling counterfactual inference captures
                                                                    2608

an intuitively real asymmetry. Yet psychological experiments         alone the “correct” one) can be challenging if not impossi-
have so far yielded only mixed support for the strong version        ble. Rarely it is the case that real-world events have clear,
of this idea (Sloman and Lagnado, 2005) and even systematic          equivocal causes and effects. For one thing, in many cases
violations in some cases (Dehghani et al., 2007). It appears         the casual links are probabilistic rather than deterministic.
that while causal locality is an important factor in counterfac-     Moreover, effects often have more than one relevant cause,
tual inference, it interacts with other tendencies which may         and distinguishing between a focal cause and mere enabling
outweigh or overrule it in some cases. We believe that a sys-        conditions can be difficult (McGill, 1993). Furthermore, the
tematic investigation of those other tendencies and of their         process of finding the focal cause may be heavily context-
interaction with causal locality is the key to further progress      dependent (Einhorn and Hogarth 1986; see also the papers in
towards a better understanding of the notion of similarity at        Collins et al. 2004).
work in counterfactual inference.                                       Nevertheless, we believe that causal explanations play a
   In this paper we explore the role of causal explanation in        crucial role in the interpretation of counterfactual condition-
evaluating counterfactuals. The basic idea is that in reason-        als. This is obviously the case in backward counterfactual
ing about what would have been the case if A had been true,          reasoning, i.e., reasoning from a hypothesized effect to its
the localist injunction to hold constant all the variables that      causes, in answering counterfactual questions like (1a).
causally influence whether A is true or not is sometimes un-
reasonably constraining, particularly when the hypothesized          (1)    a. If the Iraq war had not happened, would the 9/11
truth of A is a very unlikely outcome given their actual val-                   attacks have happened?
ues. We hypothesize that speakers may resolve this tension                  b. If the 9/11 attacks had not happened, would the
by including in their deliberations the question of what would                  Iraq war have happened?
explain the hypothesized truth of A – that is, whether an alter-
native state of the causes of A would make its hypothesized             But explanations are also likely to be implicitly involved
truth less surprising.                                               in our evaluation of forward counterfactuals like (1b): Even
   In the following, we first discuss causal explanation and         a speaker who does not believe that the Iraq war was a di-
Gärdenfors’ definition of the important notion of explana-          rect effect of the 9/11 attacks may answer the question quite
tory power. Next, we propose an alternative operator for the         differently depending on his or her beliefs about what actu-
Bayesian network framework which would allow this frame-             ally caused the attacks, what would have prevented them, and
work to make more precise prediction about counterfactual            how whatever may explain their non-occurrence would have
conditionals, taking into account both the causal structure          affected the events leading up to the war.
and the explanatory goodness of events. In the experiments              In order to make this a bit more precise, we turn to
section of the paper, we discuss three psychological experi-         Gärdenfors’s (1988) formal definition of causal explanation.
ments showing systematic violations of the Bayesian network          Gärdenfors defines an epistemic state as a triple K = hW, P, Bi,
framework by subjects who were asked to evaluate counter-            where W is a set of possible worlds with a common domain
factual statements, and how our new operator can account for         of individuals; P a function which, for each w ∈ W, defines a
these violations.                                                    probability measure Pw on sets of individuals in w; and B a
                                                                     probability measure on subsets of W, representing degrees of
                    Causal Explanation                               belief. The distinction between P and B allows Gärdenfors to
                                                                     represent beliefs about probabilities, defined as expectations
There is abundant evidence that humans have a deeply en-             of P relative to B, and thus to include statements about prob-
trenched inclination towards providing and acquiring expla-          abilities in his object language. We ignore this feature for
nations (Keil, 2006; Lombrozo and Carey, 2006). This need            simplicity.
to answer the “why” question is not limited to proposing                An agent who may need an explanation for an event E
naı̈ve theories about the relationships between objects or           most likely already holds E to be true. Whether and how
events. Rather, the tendency to search for missing links and to      urgently an explanation is needed depends on the degree of
understand properties has been shown to be linked to variety         belief given to E in a contracted state KE , an epistemic state
of cognitive processes, including predictions (Heider, 1958),        in which E is not known but which is otherwise as similar
diagnosis (Graesser and Olde, 2003), categorization (Murphy          to K as possible. There is no unique contraction in general,
and Medin, 1985), and attention allocation (Keil et al., 1998).      but Gärdenfors proposes as “typical” the case that KE is the
   There are different types of explanations, some of which          agent’s last epistemic state prior to learning E (p. 176).
(e.g., mathematical proofs) are not necessarily related to              If, according to KE , E is certain to happen, then the fact
causal links. Typically, however, causality plays a major role       that E did indeed happen does not require an explanation.
in explanations. When we explain a fire by the action of an          An explanation is only required to the extent that E is un-
arsonist, we rely on a construal of the situation in which the       expected according to KE . In Gärdenfors’s terms, an agent
arsonist’s action is a cause and the fire is its effect. In many     asking for an explanation for E expresses a cognitive disso-
cases, however, the causal analysis of a situation presents a        nance between E and the rest of her beliefs. This cognitive
complex picture and agreeing on the best explanation (let            dissonance is measured by the surprise value of E. Sintonen
                                                                 2609

(1984) argues that the role of the explanans is to reduce the        we point out two questions it raises, whose importance we
cognitive dissonance and provide cognitive relief, which he          acknowledge but whose investigation would go beyond the
measures as the reduction of surprise provided by the update         scope of the present study. First, what determines whether
of KE with the explanans.                                            or not the intervention is total or selective? Clearly one case
   Gärdenfors imposes two conditions on explanations rela-          in which some modification to the causes of X is called for
tive to a belief state K. First, if an explanation is needed         is when the actual values of those causes rule out X’s having
for E, it should increase the degree of belief in E accord-          value x. On a clear day, one cannot consistently entertain the
ing to KE . Thus C is considered a worthy explanation if             question of what would happen if there was a thunderstorm
BE (E|C) > BE (E). A measure of the explanatory power of             without imagining there being clouds in the sky. But we be-
C, defined as the difference BE (E|C) − BE (E), is used to pre-      lieve, and some of our experimental data below lend evidence
dict speakers’ choices between alternative explanations. The         to this view, that speakers manipulate the causes of X not only
second condition for a worthy explanation is that it should not      when X = x is impossible under their actual values, but also
already be known in state K, i.e., B(C) < 1.                         when it is merely unlikely. Such cases seem to require an
   In order to define a causal explanation, Gärdenfors first        appeal to a notion like the “cognitive dissonance” discussed
gives a definition for a cause. In his formalism, C is a cause       above, which presumably triggers a search for explanation
for E relative to the epistemic state only if it satisfies the       when it exceeds a certain threshold. What that threshold is
following two conditions: (i) P(E) = 1, P(C) < 1 and (ii)            and whether and how it is determined by the details of the
PC (E|C) > PC (E). In addition to the two conditions described       model and/or the nature of the system represented by the
for explanations, he restrict causal explanations by imposing        model is an empirical question which requires more inves-
the following additional condition on them: C is a causal ex-        tigation.
planation for event E, if C is a cause of E in relation to PC+ .        While the first question concerns the conditions under
   Note that Gärdenfors’ notions of goodness of explanation         which selective intervention is triggered, the second ques-
and explanatory power crucially refer to the conditional prob-       tion is the mirror image of the former and concerns cases
ability of the explanandum given the explanans, in relation          in which the variable X has multiple parents in the causal
to the unconditional probability of the explanans. Notably           structure: Once we allow for causal links into X to be left
missing is the prior probability of the explanans. Chajewska         intact, what determines whether any links will be cut at all?
and Halpern (1997) argue convincingly that this is a serious         Leaving all links intact amounts to conditionalizing the entire
omission, as this prior probability can play a role in choosing      network on the observation that X = x. But this is at odds
between different alternative explanations.                          with the intuition that in counterfactual reasoning the inter-
   Whatever the shortcomings of Gärdenfors’ account are, the        vention is, though perhaps not radical, still “minimal” in the
importance of the conditional probability of the explanandum         sense that as many facts are held constant as is reasonably
given the explanans is itself doubtless an important factor in       possible. Again, our experimental data below show evidence
causal explanation. In this paper, we focus on this conditional      that even when participants leave some links intact, they do
probability in both the theoretical and the experimental part.       cut others. To accommodate this observation, what seems to
We emphasize, however, that this limitation is not intended to       be required is an appeal to the notion of “cognitive relief”
deny that other factors should be considered, and we plan to         mentioned above: Speakers are content with manipulations
consider them in the next phase of this work.                        on the causes that provide sufficient relief, again with respect
                                                                     to some threshold which may be depend on facts about the
                  Selective Intervention                             model and/or the situation modeled.
In this section, we propose an alternative approach for eval-           This paper does not offer a precise answer to either of these
uating counterfactual conditionals in causal networks. In            questions. What our experiments show is that selective in-
essence, it is a weakened version of the operation associated        tervention does occur; exactly how it is triggered and con-
with Pearl’s (2000) ‘do’ operator. Recall that Pearl postulates      strained is a question left for future work.
a three-step procedure in reasoning counterfactually about the          With these caveats, we now give an informal outline of the
event that X = x: (i) Abduction: updating the exogenous vari-        idea behind selective intervention, illustrated with an exam-
ables according to the available evidence; (ii) Intervention:        ple. The goal is, first, to determine whether an explanation for
setting X := x and cutting all causal links into X; and (iii)        the hypothesized event is required, and second, if an explana-
Prediction. Our modification concerns the second step, Inter-        tion is required, which parents of the variable in question the
vention. As before, do(X = x) involves forcing the variable X        explanation should make reference to.
to have value x. However, rather than cutting all causal links       Chain and Fork Topologies. Consider first the simple case
into X and thus blocking any consequences of the interven-           of a causal chain, i.e., a graph in which the vertices are lin-
tion for X’s non-descendants, the links are cut selectively fol-     early ordered. Suppose some variable X with parent Y is the
lowing an analysis of the possible causal explanations for the       target of the counterfactual premise that X = x. The ques-
hypothesized event that X = x.                                       tion here is whether this premise would call for an explana-
   Before going on to describe the operation in more detail,         tion. Assuming that X is known to have some value x0 , x,
                                                                 2610

the first step is to undo this value setting and recalibrate the      we set up the search predicts that the agent will not even con-
network with X as an unobserved variable. This is reminis-            sider leaving two links intact if she can achieve sufficient cog-
cent of Pearl’s preparatory abduction step; however, here the         nitive relief by leaving only one. This may lead her to forego
goal is not to update the network with an observation, but to         considerable relief in case the best way to leave one link is
“downdate” it with the removal of the observation that X = x0 .       sufficient but inferior to options that involve leaving more
The next step is to assess in this new network the degree of          links. In other words, we predict a strong preference for cut-
surprise or cognitive dissonance that an observation of X = x         ting links, hence for keeping the intervention local. Whether
would cause in the actual state. This depends on the value            this prediction is borne out will have to be determined in fu-
of Y if observed, and on the probability distribution over Y’s        ture studies.
values otherwise. If the surprise is deemed tolerable, then no        Illustration. We use the second experiment of Dehghani et
explanation is required and the intervention proceeds as usual        al. (2007) to demonstrate how selective intervention is ap-
by cutting the link Y → X. Otherwise, the link Y → X is left          plied. Subjects were presented with a scenario in the collider
intact and the actual value of Y, if observed, is un-set, allow-      topology (A → C ← B) in which one cause is explicitly men-
ing changes in X to affect Y. In either case, the intervention        tioned to be stronger than the other (in the above sense of
concludes by setting X := x and updating the network (with            conditional probability):
or without the link into X).
                                                                         A lifeboat is overloaded with people saved from a sinking ship.
   Now, clearly the decision to leave the link Y → X intact
                                                                         The captain is aware that even a few additional pounds could
cannot be the end of the story. It raises the question of how to
                                                                         sink the boat. However, he decides to search for the last two
treat the link into Y, the link into Y’s parent, and so on. Cut-
                                                                         people: a missing child and a missing cook. Soon, they find
ting no links at all results in a loss of the useful distinction
                                                                         both people, but when they get onboard, the boat sinks.
between intervention and observation, hence of the ability
to distinguish between counterfactual and non-counterfactual             The subjects were then asked the following counterfactual
reasoning. We assume that the decision whether to cut the             questions:
link into Y is made by applying the above decision proce-
dure again – this time treating the set {Y, X} as unobserved in
                                                                      (2)     If the boat had not sunk, . . .
downdating the network, then asking how surprising an ob-
                                                                              a. . . . would they have found the child?         (C  B)
servation of X = x would be, and cutting the link into Y if the
                                                                              b. . . . would they have found the cook?          (C  A)
surprise would be tolerable. In general, the same procedure
is applied recursively to the ancestors of X until a link is cut.
                                                                         There was a significant tendency for subjects to reply ‘Yes’
   Selective intervention in the case of a causal fork, in            to the first question and ‘No’ to the second question. From the
which Y has multiple outgoing links, essentially follows the          perspective of the procedure outlined in this section, this re-
same logic as the causal chain. In a fork network, if an obser-       sult suggests that the link B → C was cut whereas the link
vation of X = x results in a degree of surprise, then an expla-       A → C was left intact. Assume that the boat’s not sinking
nation seems required to account for the change. In that case,        would have come as a considerable surprise to subjects given
the link Y → X will remain intact. However, if the surprise is        what they knew about the scenario, triggering the quest for a
deemed tolerable and no explanation seems required then the           causal explanation of the boat’s staying afloat. By consider-
intervention cuts the link Y → X.                                     ing each of A and B separately, subjects can easily check that
Collider Topology. The case in which X has multiple incom-            the boat is more likely to stay afloat if the cook is not found
ing links is not fundamentally different from the chain topol-        (but the child is) than if the child is not found (but the cook
ogy. Now, however, instead of merely asking whether or not            is). Therefore the link from the cook’s whereabouts (A) to
to leave the incoming link intact, the question is how many           the boat’s fate (C) is left intact. As a result, after the update
and which links to keep. Once again, if the values of X’s             of the network with C, the posterior probability that the cook
parents jointly make the event X = x unsurprising, the in-            was not found is high, prompting subjects to answer ‘Yes’
tervention proceeds as usual by cutting all links. Otherwise          to (2a) and ‘No’ to (2b).
the intervention is selective, but keeping the number of in-             In the next section, we compare the results of three
tact links into X to a minimum. Leaving a single link Y → X           new psychological experiments to the predictions of Causal
may be sufficient: In this case, Y is affected by the hypothe-        Bayesian Networks and the method discussed in this section.
sized X = x, but all other parents of X remain unaffected. As
before, whether it is “sufficient” in this sense to leave a sin-                                Experiments
gle link intact depends on the agent’s tolerance for cognitive        The following experiments involve scenarios which contain
dissonance. If no single link into X in itself provides enough        facts with different frequencies of occurrence and investigate
relief, then all pairs of links into X are considered, and so on,     how these different rates affect subjects’ responses to coun-
up to the entire set of incoming links.                               terfactual questions. We then compare these responses to the
   Some assumptions underlying the above description are              predictions of Causal Bayesian Networks with and without
still subject to empirical verification. For instance, the way        the selective invention modification. Note that the subjects
                                                                  2611

were randomly divided into two groups for each question.             As the results of this, the link between A and C is left in-
Therefore, Group A from Experiment 1 does not correspond             tact. After updating the network, the posterior probability of
to subjects in Group A in Experiment 2 or Group A in Exper-          A moving becomes low, resulting in the answer to the first
iment 3.                                                             question to be ‘No’. However, the posterior probability of
                                                                     B not moving becomes high resulting in the answer to the
Experiment 1                                                         second question to be ‘Yes’. Comparing these predictions to
In the first experiment, we examine how in a collider topology       subjects’ responses reveals that selective intervention seems
the likelihood of causes affect people’s evaluation of counter-      to be more consistent with subjects’ answers than the normal
factual statements. Sloman and Lagnado (2005) suggest that           ‘do’ operator.
people are more likely to keep the state of the consequent
intact when the effect is part of the antecedent of the coun-        Experiment 2
terfactual statement. Therefore, if the effect has been inter-       In this experiment we investigate how intervening on an effect
vened on, the status of the cause(s) should not change and           in a fork topology changes the status of the common cause.
hence the likelihood of occurrence of effects should not play        In a fork network, changing the value of the effect for which
a role when evaluating counterfactuals. We predict the link          the cause is the sole explanation, creates a degree of surprise
between the cause with the highest explanatory power and its         and hence, an explanation seems required to account for the
effect will be preserved, while the other links will be severed.     change. Therefore, we predict the link between the common
Therefore, people should more often undo the cause with the          cause and the child for which the cause is the best (only) ex-
highest explanatory power than the other cause.                      planation is preserved while the other link is dropped.
Method. 58 Northwestern undergraduate students were pre-             Method. The same participants were presented with a sce-
sented with a series of scenarios, and after each scenario they      nario in the fork topology. Group A was presented with the
were asked to evaluate the likelihood of a number of counter-        first question, while group B was presented with the second
factual statements. The questions were presented in form of a        question.
questionnaire, and subjects were asked to rate the likelihood        Scenario
of each question from 0 to 10, 0 being “definitely no” and 10        Ball A causes Ball B to move 5% of the time.
being “definitely yes”. A scenario that described a test situa-      Ball A causes Ball C to move 100% of the time.
tion was presented between subjects. Group A was presented           A, B and C definitely moved.
with the first question, while group B was presented with the
second question.                                                     (A)    If ball B had not moved, would ball C have moved?
Scenario                                                                    (B  C)
90% of the time ball A moves, ball C moves.
10% of the time ball B moves, ball C moves.                          (B)    If ball C had not moved, would ball B have moved?
Balls A, B and C definitely moved.                                          (C  B)
(A)     If ball C had not moved, would ball A have moved?            Results. The mean for B  C was 8.38 while the mean for
        (C  A)                                                      C  B was 4.00. The difference between the two questions
                                                                     was highly significant (t(27) = 4.34, p << 0.001).
(B)     If ball C had not moved, would ball B have moved?            Discussion. Causal Bayesian networks predict that an in-
        (C  B)                                                      tervention on the effect should not change the value of its
                                                                     cause(s). Therefore, in the case of the fork topology, inter-
Results. The mean for C  A was 3.36 while the mean for              vening on one of the child nodes should not effect the value
C  B was 6.13. The difference between the two questions             of other nodes in the graph. Hence, according to this theory,
was highly significant (t(42) = −2.95, p < 0.005).                   the answer to both questions should be ‘Yes’ (10). The results
Discussion. Causal Bayesian networks predict that the an-            suggest that the subjects however, selectively intervened on B
swers to both of the questions should be Yes (10), as inter-         dropping the A → B link and not on C, keeping the A → C link
vening on C will result in cutting the link from both of its         intact. Given that it has been explicitly mentioned that ball A
parents. However, the participants more often answered ‘No’          definitely moved and ball A is the only cause for C moving,
to the first question and ‘Yes’ to the second. This results sug-     Ball C not moving would call for an explanation. Hence, the
gests that the B → C link was cut, but the A → C link was left       link A → C would be preserved, as ball C is more likely not
intact. Ball C not moving results in a degree of surprise, given     to move if ball A does not move. However, A is not the only
that it has been explicitly mentioned in the scenario that ball      reason for B moving and therefore there could be other expla-
C definitely moved. By considering both A and B separately,          nations for B not moving. The intervention proceeds as usual
subjects can easily see that BC (C|A) > BC (C|B). That is, ball      by cutting the link A → B. Analyzing the above questions in
C is more likely not to move if ball A doesn’t move (and ball        the new network reveals that the answer to the first question
B does) than if A does move (and ball B doesn’t). There-             should be ‘Yes’ (10), while the answer to the second question
fore, A contains higher explanatory power for C not moving.          should be ‘No’ (0).
                                                                 2612

Experiment 3                                                         We believe that the notions of explanatory power and muta-
The same schema as the second experiment was used in this            bility can and should be combined in a more comprehensive
experiment. Only the probability of A causing B was in-              and plausible account of selective intervention.
creased to 95%, while the other probability was not changed.
                                                                                               References
Method. The same participants were presented with another
scenario. Group A was presented with the first question,             Chajewska, U. and Halpern, J. Y. (1997). Defining explana-
while group B was presented with the second question.                   tion in probabilistic systems. In Proceedings of the UAI-97,
                                                                        pages 62–71.
Scenario                                                             Collins, J., Hall, N., and Paul, L. A., editors (2004). Causa-
Ball A causes Ball B to move 95% of the time.                           tion and Counterfactuals. The MIT Press.
Ball A causes Ball C to move 100% of the time.                       Dehghani, M., Iliev, R., and Kaufmann, S. (2007). Effects of
A, B and C definitely moved.                                            fact mutability in the interpretation of counterfactuals. In
                                                                        Proceedings of the Twenty-Ninth Annual Conference of the
(A)    If ball B had not moved, would ball C have moved?                Cognitive Science Society, Nashville, TN.
       (B  C)                                                       Einhorn, H. J. and Hogarth, R. M. (1986). Judging probable
                                                                        cause. Psychological Bulletin, 99:3–19.
(B)    If ball C had not moved, would ball B have moved?             Gärdenfors, P. (1988). Knowledge in Flux: Modeling the Dy-
       (C  B)                                                          namics of Epistemic States. MIT Press, Cambridge, MA.
                                                                     Goodman, N. (1955). Fact, Fiction, and Forecast. Harvard
                                                                        University Press., Cambridge, Mass.
Results. The mean for B  C was 7.04 while the mean for
                                                                     Graesser, A. and Olde, B. (2003). How does one know
C  B was 3.68. The difference between the two questions
                                                                        whether a person understands a device? the quality of the
was highly significant (t(40) = 3.06, p < 0.005).
                                                                        questions the person asks when the device breaks down.
Discussion. The predictions of both methods remain the
                                                                        Journal of Educational Psychology, 95:52436.
same as the previous experiment. Even though,the difference
                                                                     Heider, F. (1958). The psychology of interpersonal relations.
between A causing B and A causing C is only 5%, subjects
                                                                        John Wiley & Sons., New York.
clearly distinguished between the two causal links. We be-
                                                                     Kahneman, D. and Miller, D. T. (1986). Norm theory: Com-
lieve this is due to the fact that A is the sole explanation for
                                                                        paring reality to its alternatives. Psychological Review,
C moving. However, B could have potentially been moved
                                                                        93:136–153.
by other causes and A not moving did not trigger a need for
                                                                     Keil, F. C. (2006). Explanation and understanding. Annual
an explanation. This experiment highlights the fact that inter-
                                                                        Reviews of Psychology, 57:227–254.
vention seems to have a clear qualitative effect, cut or no cut,
                                                                     Keil, F. C., Smith, C., Simons, D. J., and Levin, D. T. (1998).
rather than a gradient one.
                                                                        Two dogmas of conceptual empiricism: implications for
                         Conclusions                                    hybrid models of the structure of knowledge. Cognition,
                                                                        65:103–35.
We proposed an alternative approach for evaluating counter-          Lewis, D. (1973). Counterfactuals. Blackwell, Oxford.
factual conditionals based on the relationship between causal        Lombrozo, T. and Carey, S. (2006). Functional explanation
explanations and the causal topology of the graph. This ap-             and the function of explanation. Cognition, 99:167204.
proach consists of a modification to Pearl’s intervention op-        McGill, A. L. (1993). Selection of a causal background: Role
erator. As before, do(X = x) involves forcing the variable X            of expectation versus feature mutability. Journal of Person-
to have value x. However, rather than cutting all causal links          ality and Social Psychology, 64:701–707.
into X and thus blocking any consequences of the intervention        Murphy, G. L. and Medin, D. L. (1985). The role of theories
for X’s non-descendants, we proposed selective intervention             in conceptual coherence. Psychological Review, 92:289–
in which the links are cut selectively following an analysis of         316.
the possible causal explanations for the hypothesized event          Pearl, J. (2000). Causality: Models, Reasoning, and Infer-
that X = x.                                                             ence. Cambridge University Press, London.
   The result of our experiments show that selective interven-       Sintonen, M. (1984). The Pragmatics of Explanation. North-
tion does occur and causal explanations seem to play a role             Holland, Amsterdam.
in this selection. However, causal explanation may not be the        Sloman, S. A. and Lagnado, D. (2005). Do we ‘do’? Cogni-
only factor influencing this process. Previously the relation-          tive Science, 29:5–39.
ship between fact mutability, intervention and evaluation of         Spirtes, P., Glymour, C., and Scheines, R. (1993). Causation,
counterfactual have been explored by Dehghani et al. (2007).            Prediction, and Search. Springer-Verlag, New York.
Kahneman and Miller (1986) claim that there are certain facts        Stalnaker, R. (1968). A theory of conditionals. In Rescher,
that are easier to mentally undo, or mutate than others. We             N., editor, Studies in Logical Theory, pages 98–112. Black-
have previously shown that the more mutable a fact is the               well, Oxford.
more likely it is that the link from its cause would not be cut.
                                                                 2613

