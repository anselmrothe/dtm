UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
One-Shot Learning with Bayesian Networks
Permalink
https://escholarship.org/uc/item/2kw9t3vm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Kemp, Charles
Maas, Andrew L.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                         One-Shot Learning with Bayesian Networks
                                 Andrew L. Maas (amaas@andrew.cmu.edu)
                       Computer Science Department & Center for the Neural Basis of Cognition
                                                Carnegie Mellon University
                                        Charles Kemp (ckemp@cmu.edu)
                         Department of Psychology & Center for the Neural Basis of Cognition
                                                Carnegie Mellon University
                         Abstract                                (a)                                (b)
                                                                   Nationality  Language    Hair             Language
   Humans often make accurate inferences given a single            British      English     Red      100
   exposure to a novel situation.         Some of these            British      English     Brown     50
   inferences can be achieved by discovering and using
   near-deterministic relationships between attributes.            Brazilian    Portuguese  Blonde     0
   Approaches based on Bayesian networks are good at               Brazilian    Portuguese  Brown     Randerian     English
   discovering and using soft probabilistic relationships          Spanish      Spanish     Brown           Hair Color
   between attributes, but typically fail to identify and          Spanish      Basque      Black    100
   exploit near-deterministic relationships.     Here we           Randerian    Randerian   Blonde    50
   develop a Bayesian network approach that overcomes
   this limitation by learning a hyperparameter for each           Randerian    ?           ?          0
                                                                                                        Blonde      Brown
   distribution in the network that specifies whether it
   is non-deterministic or near-deterministic. We apply
   our approach to one-shot learning problems based on         Figure 1: Randeria one-shot learning problem. (a) Af-
   a real-world database of immigration records, and show
   that it outperforms a more standard Bayesian network        ter meeting people from several different countries, you
   approach.                                                   might discover that people from the same country tend
   Keywords: Machine Learning; One-Shot Learning;              to speak the same language. (b) Discovering the pattern
   Concepts and Categories; Bayesian Modeling                  in (a) supports one-shot learning about people from a
                                                               new country. After observing a single Randerian, you
                     Introduction                              might have strong expectations about the language spo-
Humans are able to discover and exploit relationships          ken by a subsequent Randerian, but weak expectations
between attributes (e.g. nationality and language) and         about her hair color.
between attribute values (e.g. Brazilian and Portuguese)
(Davies & Russell, 1987). Some relationships are near-         of one-shot learning (Fei-Fei, Fergus, & Perona, 2003).
deterministic, including the relationship between birth        Here we describe and evaluate a probabilistic model that
country and native language. We know, for example,             can handle one-shot learning problems similar to the
that two individuals born in the same country are very         Randeria problem.
likely to have the same mother tongue, and we know in             One-shot learning has been previously considered in
particular that individuals born in Brazil are very likely     the psychological literature. One prominent line of work
to speak Portuguese. Other relationships are probabilis-       has focused on “fast mapping” in word learning (Carey &
tic, including the relationship between hair color and eye     Bartlett, 1978; Smith, Jones, Landau, Gershkoff-Stowe,
color. We know that these attributes tend to be related,       & Samuelson, 2002). Empirical studies of word learn-
and we know about specific relationships between values        ing have documented that children are able to learn the
of these attributes (blondes often have blue eyes).            meaning of some new words given a single training exam-
   Suppose, for example, that after meeting several peo-       ple and researchers have developed formal models (Col-
ple from various countries, you meet a single person from      unga & Smith, 2005; Kemp, Perfors, & Tenenbaum,
Randeria, a country that is completely new to you. You         2007) that help to explain this ability. Our approach
observe that the person has blonde hair and speaks Ran-        grows out of this literature, and the work we describe
derian. Based on this single example, you may be very          builds on the hierarchical Bayesian model presented by
confident that the next Randerian you meet will speak          Kemp et al. (2007). Hierarchical Bayesian models (Gel-
the same language, but less confident that this second         man, Carlin, Stern, & Rubin, 2003) can include rep-
Randerian will also have blonde hair. Figure 1(a) shows        resentations at multiple levels of abstraction, and help
a schematic representation of the observed data, and           to explain how humans acquire abstract knowledge that
Figure 1(b) shows conditional distributions that capture       supports rapid or one-shot learning given exposure to a
our expectations about the language and hair color of          novel situation.
the second Randerian. The Randeria problem just in-               Our hierarchical Bayesian approach is built on top of
troduced is a special case of the more general problem         a standard method for learning Bayesian networks, also
                                                           142

           (a)                 (b)                               proach is to search through a hypothesis space of possible
                    E      L            E      L                 determinations and identify hypotheses that are consis-
             H                   H                               tent with the entries in the database.
                    N      B            N     B                     A probabilistic approach to learning determinations
                                                                 can improve on existing work in several respects. First,
                                                                 a probabilistic approach can handle near-deterministic
Figure 2: Models that capture relations among five at-
                                                                 relations that are subject to noise and exceptions. Some
tributes: birth country (B), language (L), nationality
                                                                 citizens of Randeria may be English speakers who were
(N), eye color (E) and hair color (H). (a) A standard
                                                                 born in the USA, and some countries (e.g. Spain) include
Bayes net can capture probabilistic relationships be-
                                                                 different linguistic communities (e.g. Spanish speakers
tween attributes, shown here as solid arrows. (b) Our
                                                                 and Basque speakers). Second, a probabilistic approach
model learns Bayes nets that capture two kinds of re-
                                                                 can incorporate soft probabilistic relations, including the
lationships: near-deterministic relationships (dashed ar-
                                                                 relationship between blonde hair and blue eyes. Russell
rows) and probabilistic relationships (solid arrows).
                                                                 (1989) allows for weighted determinations which can help
                                                                 to deal with uncertainty, but a probabilistic approach
known as Bayes nets. A Bayes net captures relation-
                                                                 provides a principled treatment of reasoning under un-
ships between attributes using probability distributions
                                                                 certainty. Finally, a probabilistic approach can provide
that specify how the value of a given attribute is gener-
                                                                 a unified account of learning and using determinations.
ated given the values of its parents. Our approach allows
                                                                 Logical approaches can rely on logical inference to ex-
for two kinds of relationships: relationships where an at-
                                                                 plain how determinations are used, but must typically
tribute value is a soft probabilistic function of the values
                                                                 invoke some other principle to explain how these deter-
of its parent attributes, and relationships where an at-
                                                                 minations are acquired.
tribute value is generated in a near-deterministic way
given the values of its parents (Figure 2b). By learn-              There has traditionally been some tension between
ing which relationships are probabilistic and which are          logical and probabilistic approaches to artificial intel-
near-deterministic, a Bayes net approach can account for         ligence, but several researchers have recently devel-
one-shot learning while preserving the ability to handle         oped general-purpose frameworks that combine logic and
probabilistic relationships.                                     probability (Milch et al., 2005; Richardson & Domingos,
   After reviewing related work and introducing our ap-          2006). Some of these frameworks may be able to ad-
proach, we apply it to an everyday problem that re-              dress the one-shot learning problems described earlier,
quires one-shot inferences—learning about people and             but here we take a different approach. General-purpose
their characteristics. Using demographic data for immi-          frameworks are impressive in their scope, but the flex-
grants who arrived at Ellis island in the early twentieth        ibility of these approaches often leads to very difficult
century, we introduce two one-shot learning scenarios            learning problems. Here we describe a relatively sim-
which correspond to real-world versions of the Randeria          ple probabilistic approach that relies on one of the best
problem. We show that our model makes more intuitive             known formalisms for capturing relationships between
inferences and predicts unobserved data better than a            attributes—Bayesian networks.
standard Bayesian network approach.
                                                                           Learning Bayesian networks
       Logical Approaches To One-Shot                            A Bayesian network includes a graph and a set of dis-
                        Learning                                 tributions that specify probabilistic relationships be-
One-shot learning has been previously considered by AI           tween attributes. This section introduces a standard
researchers, and the Randeria example introduced above           approach to learning and using these networks (Heck-
is directly inspired by the work of Davies and Russell           erman, Geiger, & Chickering, 1995).
(1987). These researchers explore the role of determi-              A Bayes net can be represented as a pair (G, θ), where
nations, or abstract logical statements that identify pat-       G is a directed acyclic graph over the attributes of in-
terns of dependency between attributes. For example,             terest and θi specifies the conditional probability distri-
the statement that “people of the same nationality speak         bution for attribute i, or the distribution over values of
the same language” is a determination that supports the          this attribute given the values of its parent attributes in
conclusion that all citizens of Randeria are likely to speak     graph G (Figure 3). Figure 2a shows a Bayes net graph
the same language. Because this rule is defined over at-         structure over some of the attributes in the Randeria
tributes, it is independent of any particular country and        problem.
can be used to perform one-shot learning when exposed               We assume here that all attributes are categorical, and
to a person from a new country. Russell (1989) discusses         represent θi as a conditional probability table (CPT)
how determinations can be learned given a database such          with one row for each setting of the parent attributes.
as the schematic example in Figure 1(a). The basic ap-           Each row in θi specifies a multinomial distribution over
                                                             143

                                 λ                              at the generalization that individuals from a given coun-
                                                                try tend to speak the same language. The next section
                         G       θ                              introduces a Bayesian network approach that overcomes
                                                                this limitation.
                                 D                                         The Type-Learning Model
Figure 3: Graphical model for Bayes net structure learn-        Our approach relies on the same basic machinery as the
ing. (G, θ) is a Bayes net, where G is a directed acyclic       standard approach, except that we no longer assume λ
graph, and θi is a table that specifies the conditional         is fixed to a single, known value for all attributes in the
probability distributions for node i in the graph. Each         graph. Instead we assume that attributes come in one
row in θi is drawn from a symmetric Dirichlet distribu-         of two types: non-deterministic attributes are generated
tion with parameter λi .                                        in a soft probabilistic way by their parents in the graph,
                                                                but near-deterministic attributes are generated accord-
values of attribute i, and we assume that these rows are        ing to a near-deterministic function of their parent at-
independently drawn from a symmetric Dirichlet distri-          tributes. To capture the difference between these types
bution with concentration parameter λi . A standard ap-         of attributes, we assume λi will be smaller for near-
proach to structure learning sets λi = 1 for all attributes     deterministic attributes than for non-deterministic at-
in the graph, which corresponds to a uniform prior over         tributes. A small value of λi means that each row in
possible multinomial distributions for the rows in each         CPT θi is expected have most of its probability mass
CPT.                                                            concentrated on a single value of attribute i. Setting
   Suppose that we observe a data matrix D, where the           λi = 1, which is a standard practice when learning Bayes
rows in D represent independent samples from a Bayes            nets, means that each row of θi is drawn from a uniform
net (G, θ). The posterior distribution over the compo-          prior over multinomial distributions.
nents of the Bayes net is                                          A type-based approach could be implemented by as-
                                                                suming that each λi is drawn from one of two distri-
          p(G, θ|D, λ) ∝ p(D|G, θ)p(θ|G, λ)p(G)         (1)     butions: a distribution with a small mean for the near-
                                                                deterministic attributes, and a distribution with mean
and we assume a uniform prior p(G) over graph struc-            1 for the non-deterministic attributes. Here we take
tures G. Since we use conjugate Dirichlet priors on the         a simpler approach, and assume that λi = 1 for non-
rows in each CPT, we can integrate out the parameters θ         deterministic attributes but that λi = 0.01 for near-
and work with the posterior distribution p(G|D, λ) over         deterministic attributes. Note, however, that the type
graphs (Heckerman et al., 1995). We can sample from             assignment for each attribute is not known in advance
this distribution using standard MCMC techniques for            and must be learned.
structure learning (Giudici & Castelo, 2003). If we as-
                                                                   A type-based approach can be contrasted with a
sume that any missing entries in D are missing at ran-
                                                                type-free approach that assumes that the λi are inde-
dom, a bag of samples from P (G|D) can be used to make
                                                                pendently generated from a continuous prior distribu-
predictions about these missing entries.
                                                                tion such as an exponential distribution. These two
   Bayesian networks have been widely used in the psy-
                                                                approaches incorporate different inductive biases and
chological literature to develop formal models of learning
                                                                should lead to slightly different predictions—for exam-
and reasoning (Glymour, 2001; Gopnik et al., 2004) The
                                                                ple, the type-based approach might be quicker to decide
standard approach to learning these networks, however,
                                                                whether a given attribute is near-deterministic (low λi )
cannot address one-shot learning problems like the Ran-
                                                                or non-deterministic (high λi ). Future work can con-
deria problem. This limitation depends critically on the
                                                                sider whether a type-based or a type-free approach ac-
difference between attributes (e.g. nationality) and at-
                                                                counts better for human inferences. Note, however, that
tribute values (e.g. Brazilian). Given enough data, the
                                                                both approaches are consistent with our core proposal,
standard approach will be sensitive to near-deterministic
                                                                which is that learning different values of λi for differ-
relationships between attribute values. After observ-
                                                                ent attributes can allow a Bayes net approach to handle
ing many Brazilian individuals, for example, the stan-
                                                                one-shot learning problems like the Randeria problem.
dard approach will learn parameters for the network in
                                                                   Since the type assignments that determine λ are not
Figure 2a that specify a near-deterministic relationship
                                                                known in advance, we work with a posterior distribution
between being Brazilian and speaking Portuguese. No
                                                                created by summing over all possible values of λ:
amount of experience, however, will allow the standard
approach to exploit near-deterministic relationships be-               p(G, θ|D) ∝ p(D|G, θ)p(θ|G)p(G)                  (2)
tween attributes. The standard approach can learn that                               X
Brazilians tend to speak Portuguese, and that Ameri-                              =     p(D|G, θ)p(θ|G, λ)p(G)p(λ)      (3)
cans tend to speak English, and so on, but cannot arrive                              λ
                                                            144

We use a uniform prior over type assignments, which                        Table 1: Passenger Data Attributes
amounts to a uniform prior over the two possible values                      Attribute       Example     # Values
of λi for any attribute i. Standard MCMC techniques                        Nationality       Spain       24
for structure learning can be extended to sample from                      Race              Spanish     16
                                                                           Language          Spanish     12
P (G, λ|D), but for the small data sets considered here we                 Birth Country     Spain       24
compute Equation 3 by enumerating all possible values                      Complexion        Dark        2
of λ. As for the standard approach in Equation 1, the                      Hair              Black       4
                                                                           Eyes              Brown       7
parameters θ can be integrated out for any given value
of λ, and we make inferences about missing values in           few edges, and as λ increases the number of edges in
D using a bag of samples from the learned distribution         the inferred graph will also tend to increase. This re-
P (G, λ|D).                                                    sult suggests that the value of λ matters, and supports
                                                               the idea that predictive accuracy may be improved by
Related Work
                                                               choosing different λi values for near-deterministic and
A special case of our general approach has previously          non-deterministic nodes.
been discussed in the psychological literature. Kemp et           Previous authors have explored the possibility of
al. (2007) describe a Bayesian model that can discover,        learning a single λ parameter for the entire network (Giu-
for example, that objects in the same category tend to         dici & Green, 1999), but there are few attempts to learn
have the same same shape—in other words, that the              different values of λi for different attributes. One pos-
relationship between category label and shape is near-         sible reason is that this approach is inconsistent with
deterministic. Their model, however, works with a re-          the assumption of likelihood equivalence, or the assump-
stricted class of Bayes nets where there is an arrow from      tion that networks in the same Markov equivalence class
the category label attribute to each other attribute, and      should receive the same prior probability (Heckerman et
where no other edges are allowed. The model developed          al., 1995). Although likelihood equivalence is often ap-
here can handle Bayes nets with arbitrary structure, in-       pealing, it will not always apply in settings where prior
cluding networks that specify relationships between at-        knowledge is available about network parameters. Our
tributes (e.g. hair color and eye color) that do not cor-      setting is one example, and the knowledge in this case
respond to category labels.                                    specifies that some relationships are near-deterministic
   Our emphasis on near-deterministic relationships is         but that others are probabilistic.
consistent with previous suggestions that humans as-
sume by default that causal relationships will be de-                                 Experiments
terministic (Schulz & Sommerville, 2006). Previous re-         We evaluate our approach in two ways using a real-world
searchers have developed probabilistic approaches that         data set. First, we directly model the Randeria problem
can exploit deterministic relationships when they are          to show the practical consequences of modeling near-
present. Closest to our own approach is the work of            deterministic relationships. Second, we use a larger test
Lucas and Griffiths (2007), who describe a hierarchi-          set to demonstrate the quantitative differences between
cal Bayesian model that can learn whether causal ob-           inferences made by our model and a standard Bayes net
servations are better explained by a deterministic rela-       approach.
tionship or a noisy-OR relationship between variables.
Note, however, that this model does not handle settings        Passenger Data
where a single network includes both near-deterministic        Our experiments used a real-world version of the data
and non-deterministic relationships, and cannot address        set shown schematically in Figure 1(a). The data spec-
one-shot learning problems like the Randeria problem           ify physical and cultural properties of immigrants who
considered here.                                               arrived at Ellis Island during the 1920s and 1930s,
   Our approach to one-shot learning relies critically         and were extracted from passenger manifests available
on the concentration parameters λi used to define the          at ellisisland.org. We took manifests for 4 ships
Dirichlet priors on the Bayes net parameters θ. We know        and created a data set with 85 people and 7 cate-
of no previous work that explores one-shot learning with       gorical attributes1 . Table 1 shows each attribute, its
Bayesian networks, but several previous researchers have       number of possible values, and example values for one
emphasized the role of the Dirichlet priors. One line of       person. The relationships between the attributes in-
work explores structure learning in the standard setting       clude both near-deterministic relationships (country de-
where there is a single value of λ for all nodes in the        termines language) and soft probabilistic relationships
network, and has demonstrated that the value of this           (hair color predicts eye color). Note, however, that the
parameter plays an important role in determining the           near-deterministic relationships are not perfectly clean
graph structure G that maximizes P (G|D) (Steck, 2008;         (e.g. not everyone from Spain speaks Spanish).
Silander, Kontkanen, & Myllymäki, 2007). When λ is
                                                                   1
very small, the best graph structure will often have very            The data set is available online at www.andrew-maas.net
                                                           145

               Language                     Hair Color         but had no other attributes observed. Figure 4 shows
75                                   75
50                   Type−Learning   50                        the marginal distributions over language and hair-color
25                   Standard        25                        for both models.
  0                                   0
Randerian English                      Blonde        Brown        Only the type-learning model was able to confidently
                                                               predict that a second Randerian would also speak Ran-
Figure 4: Conditional distributions on the language and        derian based on the single training instance provided.
hair color of a new person given only the information          When predicting hair color, both models produce similar
that she is Randerian. These marginals are analogous           distributions over the possible values. Despite allowing
to those in Figure 1(b), but are computed by models            for near-deterministic relationships, the type-learning
trained on real-world passenger data.                          model correctly realizes that hair color is not a near-
                                                               deterministic function of nationality.
    Our first experiment addresses the Randeria problem
                                                               One-Shot Learning Tests
schematically described in Figure 1. Our second ex-
periment explores prediction of missing attributes when        Figure 4 suggests that the type-learning model matches
these hidden attributes were specifically chosen to create     our intuitive notion about correct performance on the
one-shot learning problems similar to the Randeria ex-         Randeria problem, and our next analysis explores a set-
ample. Both experiments rely on learning the structure         ting where model success can be assessed more objec-
of a Bayesian network, and we first present structure-         tively. We took the passenger data and created a series of
learning results for the passenger data.                       one-shot learning problems for each attribute value. For
                                                               example, we create a one-shot learning problem for the
Learning Model Structure                                       case where Language=French by removing all French-
Structure learning for the standard model can be               speaking passengers except one from the training set.
achieved by drawing a MCMC sample from P (G|D, λ),             The test set contains all of the French speakers that were
where each λi is set to 1. For the type-learning model         removed, and the task is to predict the language of each
we drew an MCMC sample from P (G|D, λ) for each pos-           individual given all of their other attributes. In other
sible setting of λ. Given these samples, we constructed        words, we explore whether the models can confidently
an approximate posterior P (G, λ|D) by computing the           identify French speakers after observing a single exam-
relative posterior probabilities of each pair (G, λ) then      ple of this category. We repeated this process for each
normalizing.                                                   value of each attribute in the passenger data.
    Both models learned distributions on graph structures         To evaluate the models we measure both model ac-
which capture some of the intuitive relationships be-          curacy and model confidence. We expect that near-
tween the seven attributes. For example, both mod-             deterministic relations will allow confident predictions
els predict with high confidence that there is an edge         based on a single training instance, and use Kullback-
between the birth country and nationality attributes.          Leibler(KL) divergence as a metric of model confi-
The structures assigned high probability by the type-          dence. We considered the models’ inferred marginals
learning model tend to have more edges than the struc-         as approximating distributions to the true marginal,
tures preferred by the standard model. Adding more             KL(true||inferred). The true marginal is a point-mass
edges allows the model to explain certain attributes as        distribution which assigns all of its probability to the
near-deterministic functions of their parents.                 correct attribute value. In this case, the KL-divergence
    For any training set D, we use the above training          simplifies to −log[p(vt )] where p(vt ) is the probability a
technique to obtain structure distributions P (G|D, λ) for     model assigns to the true attribute value.
the standard model and P (G, λ|D) for the type-learning           Table 2 shows the results of the one-shot learning tests
model. These distributions serve as the basis for predic-      for both models. As expected, the type-learning model
tions about unobserved attributes.
Meeting a Randerian                                            Table 2: One-shot learning tests. Each model was shown
Our first test directly corresponds to the Randeria prob-      a single instance with a given attribute value (e.g. a sin-
lem mentioned in the introduction. We took the pas-            gle French-speaking passenger) and asked to make infer-
senger data already described and added a record for a         ences about all other instances with this attribute value.
single Randerian—an individual with blonde hair, a fair            Missing         KL Divergence        Accuracy (%)
complexion, and blue eyes, but a new nationality, race,            Attribute       TL Standard          TL Standard
                                                                   Nationality     1.46 2.72            73    58
language and birth country. Using the training technique           Race            1.74 2.16            63    36
described in the previous section, the models infer struc-         Language        1.38 2.16            60    60
ture distributions and network parameters. Both models             Country         1.23 2.32            82    45
                                                                   Complexion      1.99 1.96            13    18
were then asked to predict the language and hair color             Hair            3.22 3.28            0     0
of a second individual that was known to be Randerian,             Eyes            3.26 3.33            0     0
                                                           146

made more confident inferences for attributes with near-       Colunga, E., & Smith, L. B. (2005). From the lexi-
deterministic relations given only a single training exam-       con to expectations about kinds: a role for associative
ple. Given a single instance of a passenger from a new           learning. Psychological Review, 112 (2).
country, for example, the model achieves high accuracy         Davies, T. R., & Russell, S. J. (1987). A logical approach
and confidence (as measured by a low KL divergence)              to reasoning by analogy. In IJCAI 10 (pp. 264–270).
when predicting the country attribute for subsequent           Fei-Fei, L., Fergus, R., & Perona, P. (2003). A Bayesian
passengers from that country. In contrast, the standard          approach to unsupervised one-shot learning of object
model was often unable to make confident one-shot in-            categories. In ICCV 9.
ferences. Although this model made inferences from the         Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B.
single target instance at a rate better than chance, it          (2003). Bayesian data analysis (2nd ed.). New York:
had substantially lower confidence and accuracy for at-          Chapman & Hall.
tributes with near-deterministic relations. Both models        Giudici, P., & Castelo, R. (2003). Improving Markov
performed comparably for the three non-deterministic             Chain Monte Carlo model search for data mining. Ma-
attributes. We do not expect one-shot learning to be             chine Learning, 50, 127–158.
possible for these attributes, and accuracy was low in all     Giudici, P., & Green, P. (1999). Decomposable graphical
cases.                                                           Gaussian model determination. Biometrika, 86, 785-
                                                                 801.
                      Conclusion                               Glymour, C. (2001). The mind’s arrows: Bayes nets
Humans often make accurate inferences given a single             and graphical causal models in psychology. Cambridge,
example of a novel situation, and we presented a model           MA: MIT Press.
that attempts to match this ability. Our model uses            Gopnik, A., Glymour, C., Sobel, D., Schulz, L., Kushnir,
a Bayes net to capture relationships between attributes,         T., & Danks, D. (2004). A theory of causal learning in
and learns which of these relationships are soft and prob-       children: Causal maps and Bayes nets. Psychological
abilistic and which are near-deterministic. The ability          Review, 111, 1-31.
to exploit near-deterministic relationships gives our ap-      Heckerman, D., Geiger, D., & Chickering, D. M. (1995).
proach a different inductive bias than a standard Bayes          Learning Bayesian networks: The combination of
net approach, and we showed that this inductive bias             knowledge and statistical data. Machine Learning,
supports one-shot learning about novel situations.               20 (3), 197–243.
   Here we focused on a specific one-shot learning             Kemp, C., Perfors, A., & Tenenbaum, J. B. (2007).
problem—the Randeria problem—that is motivated by                Learning overhypotheses with hierarchical Bayesian
real-world inferences made by human learners. Future             models. Developmental Science, 10, 307–321.
studies can design behavioral experiments to test our ap-      Lucas, C., & Griffiths, T. (2007). Learning the functional
proach, and can explore, for example, how people make            form of causal relationships. In Proceedings of the 29th
inferences about unobserved entries in the passenger             annual conference of the cognitive science society (p.
data that we analyzed. Future experimental studies can           1810). Austin, TX: Cognitive Science Society.
also explore one-shot learning in other settings. Kemp         Milch, B., Marthi, B., Russell, S., Sontag, D., Ong, D. L.,
et al. (2007) describe a special case of our approach that       & Kolobov, A. (2005). BLOG: Probabilistic models
helps to explain word-learning data collected by Smith           with unknown objects. In IJCAI 19 (pp. 1352–1359).
et al. (2002), and our current approach should account         Richardson, M., & Domingos, P. (2006). Markov logic
for all of the findings captured by this previous model.         networks. Machine Learning, 62, 107–136.
This previous model, however, can only learn Bayesian          Russell, S. J. (1989). The use of knowledge in analogy
networks that belong to a very restricted class. Future          and induction. London: Pitman.
studies of one-shot learning can test our prediction that      Schulz, L. E., & Sommerville, J. (2006). God does not
people can learn and reason about a much broader class           play dice: causal determinism and children’s inferences
of relationships.                                                about unobserved causes. Child Development, 77 (2),
                                                                 427–442.
Acknowledgments We thank Daniel Navarro and
                                                               Silander, T., Kontkanen, P., & Myllymäki, P. (2007). On
two anonymous reviewers for comments on the
                                                                 sensitivity of the MAP Bayesian network structure to
manuscript. Andrew Maas was supported by an Interdis-
                                                                 the equivalent sample size parameter. In UAI 23.
ciplinary Training Grant in Computational Neuroscience
                                                               Smith, L. B., Jones, S. S., Landau, B., Gershkoff-Stowe,
(NIH/NIDA R09 DA023428).
                                                                 L., & Samuelson, L. (2002). Object name learning pro-
                                                                 vides on-the-job training for attention. Psychological
                      References                                 Science, 13 (1), 13–19.
Carey, S., & Bartlett, E. (1978). Acquiring a single new       Steck, H. (2008). Learning the Bayesian network struc-
  word. Papers and reports on child language develop-            ture: Dirichlet prior vs data. In UAI 24 (p. 511-518).
  ment, 15, 17–29.
                                                           147

