UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Applying Cognitive Architectures to Decision-Making: How Cognitive Theory and the
Equivalence Measure Triumphed in the Technion Prediction Tournament
Permalink
https://escholarship.org/uc/item/4622n7tt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Lebiere, Christian
Stewart, Terrence
West, Robert
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Applying Cognitive Architectures to Decision-Making: How Cognitive Theory and
      the Equivalence Measure Triumphed in the Technion Prediction Tournament
                                        Terrence C. Stewart (tcstewar@uwaterloo.ca)
                                     Centre for Theoretical Neuroscience, University of Waterloo
                                                        Waterloo, ON, N2L 3G1
                                            Robert West (robert_west@carleton.ca)
                                          Institute of Cognitive Science, Carleton University
                                                         Ottawa, ON, K1S 5B6
                                                 Christian Lebiere (cl@cmu.edu)
                                        Psychology Department, Carnegie Mellon University
                                                         Pittsburgh, PA, 15213
                             Abstract                                 to risk-aversion depending on the variance of the reward.
  For the Technion Prediction Tournament, we developed a
                                                                      However, many of these effects are subject to variations as a
  model of making repeated binary choices between a safe              function of learning, individual differences, and other
  option and a risky option. The model is based on the ACT-R          factors (Lebiere, Gonzalez & Martin, 2007).
  declarative memory system, with the use of the Blending
  mechanism and sequential dependencies.              By using        Technion Prediction Tournament
  established cognitive theory, rather than specialized machine
  learning techniques, our model was the most predictive when         To encourage the creation and evaluation of models of this
  generalizing to new conditions. However, we did not tweak           fundamental component of human decision-making, Ido
  parameters to minimize prediction error; instead we                 Erev organized a competitive modeling tournament called
  maximized the number of different conditions producing              the Technion Prediction Tournament. The tournament had
  statistically equivalent behavior. If we had not done this the      three divisions. The division of interest for us involved
  model would not have won the tournament. This leads to the          modeling human behavior in different versions of a repeated
  paradoxical result that by emphasizing cognitive explanation
                                                                      binary-choice game. Empirical data was gathered on 120
  over prediction, we achieve more accurate predictions.
                                                                      randomly chosen empirical conditions with different
   Keywords:        decision-making;     cognitive     modeling;      rewards. In each condition, one option always produced the
   equivalence; ACT-R; blending; sequential dependencies;             same deterministic reward M, while the other option would
                                                                      produce the reward H with probability pH and otherwise
          Repeated Binary Choice Decisions                            produce the reward L. Rewards and probabilities were
The effects of rewards on decision-making are highly                  chosen to make the expected value of each choice roughly
studied, and a wide variety of effects have been observed.            even, emphasizing attitudes toward risk rather than abilities
In the simplest paradigm, two choices are presented to a              to estimate reward. For each condition, 20 participants
participant, and once a decision has been made an explicit            made 100 decisions, receiving a numerical reward after each
numerical reward is provided. If this process is repeated             choice. This type of task is meant to capture the essential
many times, participants will start to favor one choice over          qualities of what most people would call a game or
the other if it is rewarded more.                                     competition (e.g., tennis, baseball, boxing, paper-rock-
  The standard empirical result is “probability matching”.            scissors, poker).
Here, if option A has a probability p of providing more                  The competition also included two other divisions where
reward than option B, then participants would choose option           only a single choice was made after subjects either learned
A with probability p. Interestingly, this is very different           or were told the reward structure. Intuitively, these
from the optimal strategy of choosing A if p>0.5 and                  conditions model informed human decision-making. Neither
otherwise choosing B. However, Friedman and Massaro                   of these divisions is considered here. For complete details
(1998) note that “probability matching in binary choice ... is        on the tournament, see Erev et al., (in press).
less robust than most psychologists seem to believe.”                    As part of the competition, empirical data on 60 of the
  Many more complex effects have since been identified.               120 conditions was publicly released. Researchers were
For example, in the Loss Rate Effect, “when the action that           free to use this data to produce predictive models that were
maximizes expected value increases the probability of                 then tested by examining their predictions on the remaining
losses, people tend to avoid it” (Erev & Barron, 2005, p.             60 conditions.
917). That is, a choice that has a higher expected value in              The model presented here won the tournament in the
the long run will be chosen less often if it is comprised of          repeated game division. That is, it produced more accurate
many small losses and few large gains. In the Payoff                  predictions (in terms of mean squared error) on the testing
Variability Effect, individuals will switch from risk-seeking         data set than any of the other models in the division. Due to
                                                                  561

limited space Erev et al., (in press) provides only a brief           option A gives a reward of 8 and option B gives a reward of
discussion of the model. However, the model had two                   10 half the time and otherwise a reward of 5, the expected
unique features that merit further attention. The first is that,      reward for B (i.e., the average reward over time) is 7.5.
unlike the other models in this competition, our model was            Therefore, A should be preferred to B (although, as noted
not a specialized model of game playing. Instead it was a             above, people will still choose B some of the time). The
cognitive model of decision-making based on the human                 ACT-R declarative memory system, by itself, does not
memory system. The second is that, unlike the other models            produce this effect. However, Lebiere (1999) created an
in the competition, we used Equivalence Testing (Stewart,             augmentation called the Blending Mechanism that allows
2007; Stewart & West, 2007) to set the model parameters.              ACT-R to do this. The Blending Mechanism, which is
Equivalence testing emphasizes the degree of statistical              described below, in combination with the ACT-R
equivalence between a set of observed empirical measures              declarative memory system, has been used successfully to
and the corresponding model outputs (i.e., as opposed to              model this type of game (e.g. Sanner et al., 2000; Lebiere et
using a regression to get the best possible fit to a data set).       al., 2003; Lebiere, Gonzalez & Martin, 2007).
                                                                         The second aspect of human game-playing that has been
              The Decision-Making Model                               modeled using ACT-R is the human ability to capitalize on
The basic idea behind our model of making decisions in a              sequential dependencies in their opponents’ outputs (e.g.,
repeated binary choice context was to treat it as a memory            West & Lebiere, 2001; West, Lebiere & Bothell, 2006).
task. This allowed us to leverage extensive previous                  Although the ACT-R declarative memory system was not
research in terms of the performance and accuracy of human            designed with this in mind, this ability falls out naturally
memory.                                                               from the way it works. It requires only that information
                                                                      about previous trials is stored in chunks along with the
ACT-R Declarative Memory                                              current outcome. This approach has been successfully used
                                                                      to model the human ability to detect and exploit sequential
The core of the model is the declarative memory system
                                                                      information, but has not previously been integrated with
from the ACT-R cognitive architecture (Anderson &
                                                                      Blending.
Lebiere, 1998), which has been used as the basis for a broad
                                                                         In terms of the competition the Blending model seems
range of explicit and implicit recall tasks. The general
                                                                      most relevant since the task is to learn the probabilities and
principle is that the odds of a memory being needed decay
                                                                      payoffs, and there are no sequential dependencies to detect.
as a power law over time, and that, if an item appears more
                                                                      However, we found that Blending combined with detecting
than once, these odds are summed, again as a power law,
                                                                      sequential dependencies worked better than Blending alone.
over all occurrences. This principle is a close match for
                                                                      That is, the search for sequential dependencies where there
realistic human cognitive environments (Anderson &
                                                                      were none made the model outputs more human-like,
Schooler, 1991).
                                                                      suggesting that humans do not turn off this ability when it is
   To implement this, each item i in memory is given an
                                                                      not needed. Essentially, the effect of this is to dampen the
activation level Ai, calculated using Equation 1, where tk is
                                                                      impact of the memory-based mechanism, especially for
the amount of time since the kth appearance of this item, d is
                                                                      recent results and rare results.
the decay rate, and ε(s) is a random value chosen from a
logistic distribution.                                                Sequential Dependencies in the Model
                                                                      In ACT-R each memory consists of a set of slot-value pairs
                                                                      and is referred to as a chunk. For this particular model, each
                                                                      chunk consists of which button was pressed, the numerical
   (1)                                                                reward that was received, and the history of button-pressing
                                                                      leading up to the current press. The number of previous
When a memory is to be retrieved, the activation level is             button presses was set to 2, as evidence indicates that this
calculated, and if it is above a retrieval threshold τ then it is     setting closely matches human performance (e.g. West et
successfully retrieved. The amount of time required for               al., 2005). This memory representation is a direct encoding
recall to occur is given by Equation 2, where F is a latency          of the relevant information available to the decision-maker
scaling factor.                                                       in the current context and does not require deliberate
                                                                      cognitive strategies. As such, it suggests a model of implicit
                                                                      decision-making that reflects the constraints of the human
   (2)                                                                architecture rather than design decisions made by the
                                                                      modeler.
The ACT-R model of declarative memory has been applied                   An example memory chunk is given in Table 1. This
to model two aspects of human game-playing abilities. The             configuration would occur if the participant pressed the
first is our somewhat limited ability to learn and exploit            right button twice, and then pressed the left button and got a
probabilities and payoffs. To do this it is necessary to              reward of 8.4.
compare expected rewards for each choice. For example, if
                                                                  562

                Table 1: Sample memory chunk                          exactly equal).        Once the reward is provided, a
                                                                      corresponding chunk is added into the declarative memory
                     Slot             Value                           system to reflect what actually happened. Then the history is
                   choice               left                          updated and the system is ready to make the next prediction.
                   reward               8.4                              For consistency with the use of the ACT-R declarative
                    lag_1              right                          memory system, this algorithm was implemented using the
                    lag_2              right                          ACT-R production system. This expresses each of the
                                                                      above steps using if-then rules, each of which requires 50
When performing a recall, the model only considers                    milliseconds to occur. Note that this, combined with
memories whose recent history match the current recent                Equation 2 for determining how long it takes to retrieve a
history. That is, the chunk shown in Table 1 will only be a           memory, allows the model to give predictions for the time
candidate for recall when the model has just finished                 taken to make its decision. This timing information also
selecting the right-hand button twice in a row.                       impacts the performance of the model, since Equation 1
                                                                      indicates how memories decay over time.
Blending in the model                                                    For the competition, the model needed to predict average
When the model attempts to recall a chunk that matches the            performance over 100 trials given a particular experimental
recent history, multiple chunks may be found. In the                  situation. To create this prediction, 1000 separate models
competition, when attempting to recall an expectation for             were generated and each one was run through 100 trials.
the button associated with the risky choice, there will be two        The final prediction was the average proportion of times the
chunks in memory: one from previous situations where the              risky choice was made.
Low reward was received and one where the High reward                    The source code for this model is available at
was received. The chunk with the higher activation will be            <http://ccmlab.ca>.
the one that has occurred the most in the past and/or most
recently (since the learned chunk activation reflects both                             Parameter Exploration
recency and frequency effects), and is therefore more likely          As with any computational model, there are a variety of
to contain the correct outcome for the current trial.                 numerical parameters that can be adjusted. However, since
However, because of the probabilistic nature of the payoff,           the model is based on the ACT-R cognitive architecture, we
it may not be the best choice. For instance, it could                 can turn to previous experiments to help constrain these
occasionally lead to very negative consequences that would            model parameters. For example, the parameter d in
offset the more common but limited gains. The blending                Equation 1 is consistently set to 0.5 in ACT-R models to
mechanism was developed for this type of situation and has            produce results that are predictive of human performance on
been used on other instance-based learning and decision-              many memory tasks. The model parameters are shown in
making tasks (e.g. Gonzalez et al., 2003).                            Table 2, along with the standard values used for each one.
   To blend the two chunks that match the current situation,
the numerical value for the rewards are combined using the                   Table 2: Canonical parameter values for the model
activation value (Equation 1) as a weighting factor. This
results in a blended reward value r, as shown in Equation 3.             Parameter            Canonical Value
This is then taken as the expected reward.                               decay                0.5           (d in Equation 1)
                                                                         noise                0.3           (s in Equation 1)
                                                                         latency              0.05          (F in Equation 2)
                                                                         retrieval threshold  0             (τ)
                                                                         lag                  2             (size of context)
   (3)                                                                   production time      0.05          (time to apply a rule)
Algorithm                                                             Of these parameters, only two are commonly changed in
                                                                      ACT-R models: noise (s), because it can be used to
Given this memory system, the underlying algorithm is                 represent both specific retrieval stochasticity a number of
straightforward. Two recalls are attempted to get an                  other sources of unpredictability, and retrieval threshold (τ),
expected reward for each of the two options. These occur              because it is used to compensate for constant variations in
sequentially and in random order. If either recall fails (i.e. if     other activation factors. However, both parameters were
there are no matching previous memories or if their                   searched over intervals close to their canonical values (0.25
activation is below the retrieval threshold τ), then that option      for the noise; 0 for the retrieval threshold). It is thus useful
is chosen. This was done to model exploratory behavior,               to explore the behavioral changes in the model as these
since the participant cannot remember or has never seen the           parameters are adjusted.
results of choosing that button in that context. If both
retrievals succeed, the one with the largest expected reward
(Equation 3) is chosen (choosing randomly if they are
                                                                  563

Equivalence Testing                                                      this range can be reduced (Tryon, 2001), but the work
   The Technion Prediction Tournament provided raw                       presented does not do this, and instead uses bootstrap
empirical data for 20 subjects performing 100 decisions in               confidence intervals (Davidson & Hinkley, 1997) so as to
each of 60 different experimental conditions. Each of these              make no assumptions about the distribution of the data.
conditions provides a separate measure for evaluating the                   To determine the relativized equivalence Er between a
model's performance.1                                                    particular parameter setting and the participants'
   The standard metric for model quality over a set of                   performance over the 60 measures provided, Equation 4 was
measures is the root-mean-squared error (RMSE), often                    used, where the model's confidence interval on situation i is
used to find the “best fit” parameter setting for a model.               Mi,L to Mi,U and the human participants' confidence interval
This measure is shown in Figure 1, indicating that the                   is Hi,L to Hi,U. This gives a result that is normalized so that a
smallest prediction error averaged over the 60 different                 value of 1.0 indicates that all model values are within the
conditions occurs with a very low noise value (s<0.01) and               corresponding confidence intervals (i.e. any model with
a retrieval threshold τ of -1.                                           Er<1 is not statistically distinguishable from the real
                                                                         participant performance on any particular situation).
                                                                            (4)
                                                                         This method was developed to more conservatively
                                                                         characterize the behavior of a model. By focusing on the
                                                                         worst-case scenario (rather than averaging over situations as
                                                                         in MSE approaches), it clarifies that the model is suitable
                                                                         for all of the situations being investigated. The results of
                                                                         this metric are shown in Figure 2.
     Figure 1: Root Mean Squared Error for the model on the
60 experimental conditions. Each point indicates the RMSE
for a different setting of the noise and threshold parameters.
However, the RMSE approach can be difficult to interpret.
Firstly, it averages over experimental conditions, meaning
that if there are a few conditions for which the model is
highly inaccurate, it can still have a small RMSE. Secondly,
and more fundamentally, this approach does not take into
account sampling error in the empirical data. Given that
only 20 participants were used for each condition, the
confidence intervals for each measure may be fairly large,                  Figure 2: Relativized Equivalence for varying parameter
making the RMSE approach prone to over-fitting.                                                       settings.
   To determine the overall quality of the model over all 60
conditions, we did not use the standard approach of                      When making use of this method, it is common to find there
minimizing the root-mean-squared error.                     Instead,     are particular conditions in the fitting set in which the model
relativized equivalence (Stewart & West, 2007) was used.                 produces a high Er value regardless of the parameter
Here, the key measure is the worst-case equivalence.                     settings. This is especially true with a large number of
Equivalence is defined as the maximum difference between                 conditions: with 60 conditions, even a perfect model will be
the 95% confidence intervals of the human participants and               expected to be outside a 95% confidence interval on three
the model. That is, it is the number for which there is 95%              conditions, just by chance. Imperfect models and/or
confidence that the human performance and the model                      imperfect data further increases the likelihood of this
performance differ by less than this amount. This approach               happening.
is derived from the equivalence test (Barker et. al, 2002)                  For this competition, 9 out of 60 conditions in the fitting
used in epidemiology. If a normal distribution is assumed,               set were identified as problematic. That is, no parameter
   1                                                                     settings were found that would give low Er values on these
     In the current work, individual differences were not modeled.
                                                                     564

conditions and also maintain low Er values on the majority                                Generalization
of the other conditions. Because this could be due to an
                                                                   To evaluate the various models in the tournament, their
unknown bias or outliers in these experimental conditions,
                                                                   ability to generalize to a testing set was measured. This was
they were excluded from our analysis. These conditions are
                                                                   done using the standard RMSE approach. Our model won
shown in Table 3. Re-running the analysis using a
                                                                   the tournament with a RMSE of 0.087, and the next closest
replication, or a conceptual replication, of the same
                                                                   model scored 0.092.
experimental conditions could help identify problematic
                                                                      Interestingly, if we had have used the standard best fit
conditions for the model, if they exist. However, at this
                                                                   approach rather than the Equivalence methodology, we
time the necessary replications have not been performed.
                                                                   would not have won the tournament. As can be seen in
                                                                   Figure 1, the best fitting model on the training data had a
     Table 3: Experimental conditions identified as outliers
                                                                   noise of 0.001 and a threshold of -1. However, Figure 4
                                                                   shows that this model performs considerably worse on the
         #           H         pH          L         M
                                                                   testing data, giving a RMSE of 0.096.
         7          -5.6       0.7       -20.2     -11.7
        13           -2       0.05       -10.4      -9.4
        20          -4.3       0.6       -16.1      -4.5
        21            2        0.1        -5.7      -4.6
        24           9.2      0.05        -9.5      -7.5
        30            3       0.91        -7.7       1.4
        36            5       0.08        -9.1      -7.9
        45           2.8       0.8         1         2.2
        49          13.4       0.5        3.8        9.9
                                                                     Figure 4: Root Mean Squared Error on the testing data for
                                                                                     varying noise and threshold.
                                                                                            Discussion
                                                                   By building a decision-making model using well-established
                                                                   cognitive models, we successfully predicted behavior in a
                                                                   novel domain. Our model beat a wide variety of machine-
                                                                   learning techniques; the next best models involved two-
                                                                   stage sampling and normalized reinforcement learning.
   Figure 3: Relativized Equivalence with outlier measures         None of the competing models other than ours made use of
                            removed.                               general knowledge about human cognition. Instead they
                                                                   relied on mathematical optimization techniques. As Lebiere
Once these outliers are remove (as shown in Figure 3), we          et al. (2003) demonstrated, modeling methods that rely on
can see that setting the noise parameter to values between         general assumptions about cognitive invariants, such as
0.2 and 0.4 and the threshold to values between -1 and -2          cognitive architectures, and can generalize models across a
produces models that are equivalent to the empirical data.         range of paradigms and conditions can be superior to
All of these models have Er values below 1.0. That is, there       machine learning techniques such as Bayesian networks or
is no statistically significant difference between the model's     Markov models on a number of counts: (a) they require less
behavior and the observed behavior over any the 51 non-            data to be parameterized because unlike machine learning
outlier measures considered. This indicates that the model         methods that attack each new problem tabula rasa,
is successfully capturing the human behavior at the level          constraints inherited from other models prune the parameter
that is statistically warranted.                                   space, (b) they require fewer domain-specific assumptions
   For the purposes of the Technion Prediction Tournament,         because cognitive constraints constrain the relevant problem
a single parameter value had to be chosen. We selected the         representation rather than leaving it entirely to the modeler,
center of the equivalent region, giving a threshold of -1.6        and (c) they allow a more complex representation of the
and a noise of 0.35, which is close to the canonical value for     problem-solving state, such as a combination of symbolic
noise 0f 0.25 used in previous models of this type.                structures and statistical parameters such as activation.
                                                               565

                         Conclusions                                Lebiere, C. (1999). Blending. In Proceedings of the Sixth
                                                                      ACT-R Workshop. George Mason University, Fairfax,
We have established a novel model of human decision-
                                                                      VA.
making in repeated binary choice conditions where one
                                                                    Lebiere, C., Gonzalez, C., & Martin, M. (2007). Instance-
option gives a fixed reward and the other option gives a
                                                                      based decision-making model of repeated binary choice.
reward that is randomly selected from two possible values.
                                                                      In Proceedings of the 8th International Conference on
While this model produces the smallest prediction errors
                                                                      Cognitive Modeling. Ann Arbor, MI.
among those entered into the Technion Prediction
                                                                    Lebiere, C., Gray, R., Salvucci, D., West, R. (2003). Choice
Tournament, we can also draw a stronger conclusion. In
                                                                      and learning under uncertainty: A case study in baseball
particular, this model produces behavior that is statistically
                                                                      batting. In Proceedings of the 25th Annual Conference of
indistinguishable from the human performance, given the
                                                                      the Cognitive Science Society, 704-709. Mahwah, NJ:
available empirical data.
                                                                      Lawrence Erlbaum Associates.
   While a few of the 120 experimental conditions did have
                                                                    Lebiere, C., & Wallach, D. (2001). Sequence learning in
to be removed in this analysis, this removal does not
                                                                      the ACT-R cognitive architecture: Empirical analysis of a
invalidate the model. Given the large number of conditions,
                                                                      hybrid model. In Sun, R. & Giles, L. (Eds.) Sequence
it is expected that even a perfect model would fail to match
                                                                      Learning: Paradigms, Algorithms, and Applications.
due to sampling error.          To establish whether these
                                                                      Springer LNCS/LNAI, Germany.
conditions do actually indicate problems with the model,
                                                                    Lebiere, C., & West, R. L. (1999). A dynamic ACT-R
more empirical measures are needed. If these measures are
                                                                      model of simple games. In Proceedings of the 21st Annual
consistent with the model, then this is a case of sampling
                                                                      Conference of the Cognitive Science Society, 296-301.
error. If these measures continue to be inconsistent then we
                                                                      Mahwah, NJ: Erlbaum.
will have sufficient evidence to adjust our model to take this
                                                                    Pylyshyn, Z. W. (1984). Computation and Cognition:
into account. However, without further empirical evidence
                                                                      Towards a Foundation for Cognitive Science. Cambridge:
there is no statistical justification for attempting to fit our
                                                                      MIT Press.
model more closely to the human performance. By
                                                                    Sanner, S., Anderson, J. R., Lebiere, C., & Lovett, M. C.
following the equivalence method for evaluation we
                                                                      (2000). Achieving efficient and cognitively plausible
successfully avoided this over-fitting.
                                                                      learning in Backgammon.            Proceedings of The
                                                                      Seventeenth International Conference on Machine
                         References                                   Learning. San Francisco: Morgan Kaufmann.
Anderson, J. R. & Lebiere, C. (1998). The Atomic                    Stewart, T. C. (2007). A Methodology for Computational
   Components of Thought. Mahwah, NJ: Erlbaum.                        Cognitive Modelling. PhD Thesis, Institute of Cognitive
Anderson, J. R. & Schooler, L. J. (1991). Reflections of the          Science, Carleton University.
   environment in memory. Psychological Science, 2, 396-            Stewart, T. C., West, R.L. (2007) Equivalence: A Novel
   408.                                                               Basis for Model Comparison. In Proceedings of the 29th
Barker L.E., Luman E.T., McCauley M.M., & Chu Y.R.                    Annual Meeting of the Cognitive Science Society.
   (2002) Assessing equivalence: An alternative to time use         Tryon, W. (2001). Evaluating statistical difference,
   of difference tests for measuring disparities in vaccination       equivalence, and indeterminacy using inferential
   coverage. American J. of Epidemiology, 156, 1056-1061.             confidence intervals: An integrated alternative method of
Davison, A.C. and Hinkley, D.V. (1997). Bootstrap                     conducting null hypothesis statistical tests. Psychological
   Methods and Their Application. Cambridge University.               Methods, 6(4), 371-386.
Erev, I. and Barron, G. (2005). On Adaptation,                      West, R. L., & Lebiere, C. (2001). Simple games as
   Maximization, and Reinforcement Learning Among                     dynamic, coupled systems: Randomness and other
   Cognitive Strategies. Psych. Review, 112(4), 913-931.              emergent properties. Journal of Cognitive Systems
Erev, I., Ert, E., Roth, A., Haruvy, E., Herzog, S., Hau, R.,         Research, 1(4), 221-239.
   Hertwig, R., Stewart, T., West, R., and Lebiere, C. (in          West, R. L., Lebiere, C. & Bothell, D. J. (2006). Cognitive
   press). A choice prediction competition for choices from           architectures, game playing and human evolution. In Sun,
   experience and from description. Journal of Behavioral             R. (Ed) Cognition and Multi-Agent Interaction: From
   Decision Making: Special edition on Decisions from                 Cognitive Modeling to Social Simulation. NY, NY:
   Experience.                                                        Cambridge University Press.
Friedman D., and Massaro D.W. (1998). Understanding                 West, R. L., Stewart, T. C., Lebiere, C., &
   variability in binary and continuous choice. Psychonomic           Chandrasekharan, S. (2005). Stochastic resonance in
   Bulletin & Review, 5, 370–389.                                     human cognition: ACT-R vs. game theory, associative
Gonzalez, C., Lerch, F. J., & Lebiere, C. (2003). Instance-           neural networks, recursive neural networks, Q-learning,
   based learning in real-time dynamic decision making.               and humans. In B. Bara, L. Barsalou & M. Bucciarelli
   Cognitive Science 27 (4), 591-635.                                 (Eds.), Proceedings of the 27th Annual Conference of the
                                                                      Cognitive Science Society. Mahwah, NJ: Lawrence
                                                                      Erlbaum Associates.
                                                                566

