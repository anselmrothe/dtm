UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Iterated learning in populations of Bayesian agents
Permalink
https://escholarship.org/uc/item/6wh9d141
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Author
Smith, Kenny
Publication Date
2009-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                             Iterated learning in populations of Bayesian agents
                                          Kenny Smith (kenny.smith@northumbria.ac.uk)
                 Cognition and Communication Research Centre, Division of Psychology, Northumbria University
                        Northumberland Building,Northumberland Road,Newcastle-upon-Tyne, NE1 8ST,UK
                              Abstract                                  languages during their transmission. The classic example of
   Previous analytic results (Griffiths & Kalish, 2007) show that       this second constraint is the mismatch between the infinite ex-
   repeated learning and transmission of languages in populations       pressivity of languages and the finite set of data from which
   of Bayesian learners results in distributions of languages which     such languages must be learned. This transmission bottleneck
   directly reflect the biases of learners. This result potentially
   has profound implications for our understanding of the link          favours languages which can be recreated from a subset via
   between the human language learning apparatus and the dis-           generalisation. Recursive compositionality is one such gen-
   tribution of languages in the world. It is shown here that a         eralisation (e.g. Kirby, 2002; Brighton, 2002), and therefore
   variation on these models (such that learners learn from the
   linguistic behaviour of multiple individuals, rather than a sin-     represents an adaptation by language in response to pressure
   gle individual) changes this transparent relationship between        arising from transmission factors external to the human mind.
   learning bias and typology. This suggests that inferring learn-      While this evolutionary process requires certain learner biases
   ing bias from typology (or population behaviour from labora-
   tory diffusion chains) is potentially unsafe.                        (e.g. ability to generalise), it does not arise as a consequence
                                                                        of these learning biases alone, but is modulated by the trans-
   Keywords: language learning; iterated learning; Bayesian
   learning; cultural evolution; language universals                    mission bottleneck (Brighton, Smith, & Kirby, 2005). This
                                                                        suggests that the biases of language learners can’t simply be
                          Introduction                                  read off from typological distributions.
What is the relationship between the biases of language learn-             However, this transmission-mediated view of the relation-
ers and the observed distribution of languages in the world?            ship between learning biases and typology has recently been
Under the standard generative account (e.g. Chomsky, 1965),             thrown into doubt by some modelling work in the Bayesian
a direct mapping is assumed between the mental apparatus                framework. As discussed below, Griffiths and Kalish (2007)
of language learners and language structure. In the strongest           show that iterated learning in populations of Bayesian learn-
possible form (e.g. Baker, 2001), the claim is that we can read         ers produces outcomes which are solely determined by the
off the structure of the language faculty from the typological          biases of language learners: in other words, in the linguis-
distribution of languages in the world.                                 tic case, the relationship between learning bias and language
   A second account which posits a similarly close match be-            typology might be a transparent one after all.
tween the biases of language learners and the structure of lan-            It is shown here that a variant of Griffiths & Kalish’s model
guage arises from considerations of cultural evolution (Chris-          (where each learner selects a single grammar after observing
tiansen & Chater, 2008). Rather than language structure                 data produced by multiple individuals, rather than a single in-
being strongly constrained by a highly restrictive domain-              dividual) leads to a blurring of the relationship between prior
specific learning apparatus, the idea is that languages have            biases of learners and outcomes of cultural evolution: popu-
adapted over repeated episodes of learning and production               lations of Bayesian agents converge on distributions of lan-
in response to much weaker (and possibly domain-general)                guages which are dependent on both the biases of language
constraints arising from the biases of language learners. This          learners and transmission factors (such as the diversity of
process is sometimes called iterated learning: the outcome              models a learner is exposed to).
of learning at one generation provides the input to learning
at the next. While typologically unattested languages might
                                                                              Summary of iterated learning results for
be both possible and even learnable, the languages we see in                                 Bayesian learners
the world will typically be selected from the restricted set of         Bayesian learners select a hypothesis h according to its pos-
highly learnable languages: languages which are hard to learn           terior probability in light of some data d:
will tend to change, and those which are easy to learn will be                                           P(d|h)P(h)
preserved, eventually yielding languages which are uniformly                                P(h|d) =                                 (1)
well-fitted to the biases of language learners. We have previ-                                         ∑h P(d|h)P(h)
ously termed this evolutionary pressure cultural selection for             P(d|h) gives the likelihood of data d being produced un-
learnability (Brighton, Kirby, & Smith, 2005).                          der hypothesis h, and P(h) gives the prior probability of each
   Are learner biases the only factor shaping the distribu-             hypothesis. For models of iterated learning of language, the
tion of languages in the world? It has been argued (see                 set of hypotheses are interpreted as the set of possible gram-
e.g. Kirby, 2002; Zuidema, 2003; Brighton, Kirby, & Smith,              mars, data are sets of utterances from which learners must
2005; Kirby, Dowman, & Griffiths, 2007) that, at a minimum,             induce a language, and the prior probability distribution over
language must be seen as a compromise between two fac-                  grammars arises from the bias (domain-specific or domain-
tors: the biases of learners, and other constraints acting on           general, innate or learned) of learners.
                                                                    697

    Previous analytic and numerical results in this framework               interest is the proportion of individuals in the population
 (primarily Griffiths & Kalish, 2007; Kirby et al., 2007) show              using each grammar at a particular generation, which con-
 that the relationship between the prior biases of learners and             verges to the prior over time.
 the outcomes of cultural evolution depends critically on how
                                                                            This equivalence between chains and populations is an in-
 learners select a grammar given the posterior probability dis-
                                                                         teresting and potentially important one, since it suggests that
 tribution over possible grammars.
                                                                         we can obtain useful insights into the behaviour of real-world
    When learners select a grammar with probability propor-
                                                                         populations by studying long thin diffusion chains (either for-
 tional to its posterior probability (known as sampling from
                                                                         mally or in the laboratory: Griffiths, Kalish, & Lewandowsky,
the posterior), the stable outcome of cultural evolution (the
                                                                         2008).
stationary distribution) is simply the prior distribution (Grif-
fiths & Kalish, 2007). This is true regardless of the initial                              A two-grammar model
distribution over grammars or transmission factors such as
                                                                         Given the potential implications of these results, it would be
the amount of data learners receive or the amount of noise
                                                                         interesting to know whether the equivalence between chains
on transmission: iterated learning in populations of samplers
                                                                         and populations holds in situations where each learner learns
results in convergence on the prior. As discussed above, this
                                                                         from more than a single model, potentially including models
suggests a transparent relationship between the prior bias of
                                                                         from the same generation. A simple two-grammar model can
learners and the observed distribution of languages in the
                                                                         be used to explore (at present, numerically) this issue.
world: the typological distribution exactly reflects the biases
of learners.                                                             Model details
    On the other hand, when learners select the hypothesis with          We assume that populations are infinitely large, and are or-
the maximum a posteriori probability (MAP selection), the                ganised into discrete generation. Learners observe a set of b
relationship between prior bias and the stationary distribu-             utterances, produced by (one or more) models selected from
tion is more complex (Griffiths & Kalish, 2007; Kirby et al.,            the immediately preceding generation of the population, and
2007). The distribution of languages produced by cultural                subsequently select a grammar with probability proportional
evolution will reflect the ordering of hypotheses in the prior,          to its posterior probability in light of that data (i.e. they sam-
but differences in prior probability are magnified, such that            ple from the posterior). Note that, importantly, learners are
the a priori most likely hypothesis is overrepresented in the            required to select a single grammar, despite potentially being
stationary distribution. Furthermore, different priors can lead          provided with data produced by multiple grammars, an issue
to the same stationary distribution, and changing transmission           we return to in the discussion.
factors (amount of data, noise, etc) can result in convergence              There are two grammars, h0 and h1 , and two utterances,
to a different stationary distribution. In MAP populations, the          d0 and d1 . Individuals produce single utterances as follows
relationship between learner biases and typological distribu-            (where ε gives the probability of noise on production):
tions is therefore somewhat opaque.
    These models suggest that the sampling / MAP opposition                                      P(dx |hx ) =     1−ε
is a critical one for understanding the relationship between                                  P(dy6=x |hx ) =     ε                      (2)
learner biases and the distribution of languages in the world.
While the true nature of the human hypothesis selection strat-              Given that the population is infinitely large and there are
egy is ultimately an empirical question, it is worth probing             only two grammars, we simply track pt , which is the propor-
the assumptions behind the formal results presented above.               tion of individuals at generation t who select hypothesis h0
Griffiths & Kalish’s sampling result holds in two cases:                 after learning (1 − pt gives the proportion selecting h1 ).
                                                                            If learners at generation t + 1 learn from a single model
1. Populations are treated as long thin chains, with a single            selected at random from generation t, the proportion of indi-
    individual per generation, and transmission occurring be-            viduals using h0 at time t + 1 will be
    tween adjacent generations in the chain (in the classic iter-                                                                        !
    ated learning model configuration). In this case, the tem-
    poral distribution of grammars over multiple generations              pt+1 = ∑ P(h0 |d). pt . ∏ P(x|h0 ) + (1 − pt ). ∏ P(x|h1 )
                                                                                   d                 x∈d                     x∈d
    converges to the prior: while any grammar may be in use                                                                              (3)
    in the chain at a particular generation, on average the usage        where the sum is over all possible data sets and the products
    of the various grammars reflects their prior probability.            are over the individual utterances in each data set. In other
2. Populations are infinitely large, organised into discrete             words, each learner learns from an h0 model with probability
    generations, and each individual learns from a single model          pt and an h1 model with probability 1 − pt , and subsequently
    at the previous generation.1 In this case the distribution of        selects h0 with probability determined by the data produced
                                                                         by that model.
     1 It is worth noting that a number of non-Bayesian, population-
                                                                            Alternatively, if a learner learns from multiple models, each
 biology inspired models of language evolution similarly focus on
 situations where learners learn from a single model (e.g. Nowak,        utterance in their data set may be produced by a different in-
 Komarova, & Niyogi, 2001).                                              dividual, possibly using a different grammar. We will assume
                                                                     698

that the model for each item of data is independently selected                                                           single             multiple
from the population at the preceding generation, which gives                 (a)                    1
the following expression for the proportion of individuals se-
lecting h0 at generation t + 1:                                                                    0.8
                                                                                   proportion h0
                                                                                                   0.6
  pt+1 = ∑ P(h0 |d). ∏ (pt .P(x|h0 ) + (1 − pt ).P(x|h1 )) . (4)
           d           x∈d                                                                         0.4
  Again, the sum is over all possible sets of data, and the
                                                                                                   0.2
product is over the items in each data set, where each utter-
ance is produced by an individual using either grammar h0 or
                                                                                                    0
h1 (according to the proportions of those two grammars in the                                            0               50           100              150        200
population).                                                                                                                           t
Results                                                                                                         p0 > 0.4465                p0 < 0.4465
The main result (see Figure 1a) is that, when learners learn                 (b)                    1
from multiple models, the proportion of individuals using
each grammar (after cultural evolution has run its course) is                                      0.8
no longer the same as the prior distribution. Rather, one lan-
                                                                                   proportion h0
guage predominates, with the winning language being deter-                                         0.6
mined by the starting proportions of the two grammars and
their prior probability.2                                                                          0.4
   Figure 1b shows this sensitivity to initial proportions of the
two languages in a little more detail. There is a critical value                                   0.2
of the initial proportion of h0 (at around 0.4465 for this com-
bination of parameters): for initial proportions below this, h1                                     0
                                                                                                         0          5         10      15         20          25   30
eventually dominates, otherwise h0 dominates. This sensi-
                                                                                                                                      t
tivity to initial conditions is not found in the single model
treatments discussed above.
                                                                             Figure 1: P(h0 ) = 0.6, b = 3, ε = 0.05. (a) When individ-
   When learners learn from multiple models, the insensitiv-
                                                                             uals learn from a single model, the population converges to
ity to transmission factors such as amount of data (b) nor-
                                                                             the prior. When learners learn from (potentially) multiple
mally seen in populations of samplers also disappears. This
                                                                             models, populations converge to one of two stable states, de-
is illustrated in Table 1. Notice that the effect of increased
                                                                             pending on initial conditions. (b) When learners learn from
amounts of data (higher b) runs in the opposite direction
                                                                             multiple models, the eventual distribution is sensitive to the
to that seen in MAP populations: whereas in the chains of
                                                                             starting proportions of the two grammars.
MAP learners described in Kirby et al. (2007) less data gives
greater exaggeration of the prior, here more data gives greater
exaggeration of the prior preference for h0 . Note also that, as
b increases, the impact of the strength of prior preference for              Table 1: Stable proportions of h0 for various values of p(h0 )
h0 on the final proportion of h0 in the population diminishes                and b. Populations initialised with equal proportions of h0
— in essence, when b ≥ 3, the population converges on h0                     and h1 , ε = 0.05.
regardless of strength of prior bias in favour of that grammar.                                                                       b
This is reminiscent of the MAP phenomenon of insensitivity                                    P(h0 )         1          2          3         4           5
to strength of prior bias, but is modulated by b.
                                                                                              0.51           0.51       0.548      0.978     0.989       0.997
   Why does b, the amount of data learners receive, lead to
                                                                                              0.6            0.6        0.822      0.983     0.992       0.998
this departure from the known sampler results? In a mixed
                                                                                              0.7            0.7        0.92       0.986     0.994       0.998
population, increasing b increases the diversity of learner’s
                                                                                              0.8            0.8        0.961      0.99      0.996       0.999
sample of the population’s linguistic behaviour (unlike in
                                                                                              0.9            0.9        0.983      0.993     0.998       0.999
the case where learners learn from a single model, when
they simply receive an increasingly accurate reflection of the
grammar of that model). Consequently, if one grammar pre-
dominates in the population, this is likely to be reflected in                  How do Bayesian learners respond to mixed samples? The
the data learners see.                                                       grammar which matches with the majority of the data has
   2 See Niyogi (2006) for a number of more general analytic results         higher posterior probability and is therefore likely to be se-
providing the dynamics of transmission in populations associated             lected. Importantly, under a wide range of conditions, the
with various non-Bayesian learning algorithms.                               grammar matching the more common data is disproportion-
                                                                       699

ately preferred. Given a data set consisting of i d0 items and        The model
 j d1 items, i ≥ j, the ratio of likelihoods P(d|h0 )/P(d|h1 ) is     In this more complex model, a language consists of a sys-
( 1−ε
   ε )
       i− j . This quantity is generally greater than the corre-
                                                                      tem for expressing m meanings, where each meaning can be
sponding ratio of data items (i/ j) for low noise rates. In other     expressed using one of k means of expression, called signal
words, learners exposed to a mixed sample and required to             classes. In a perfectly regular (or systematic) language the
select a single grammar are disproportionately likely to pick         same signal class will be used to express each meaning — for
the more frequently represented grammar, making Bayesian              example, the same compositional rules will be used to con-
learning in this context a type of conformist frequency-              struct an utterance for each meaning. Following Kirby et al.
dependent learning (Boyd & Richerson, 1985). The well-                (2007), we assume that learners have a preference for lan-
know consequence of conformist learning is the rich-get-              guages which use a consistent means of expression, such that
richer behaviour seen here, with the mismatch in frequencies          each meaning is expressed using the same signal class. This
of the two grammars increasing generation on generation.              prior is given by the expression
    Conformity bias is not, however, the whole story. Increas-
ing b has a second effect: as well as increasing the repre-                                         Γ(kα)        k
sentativeness of the sample of the population’s linguistic be-                      P(h) =                      ∏
                                                                                              Γ(α)k Γ(m + kα) j=1
                                                                                                                    Γ(n j + α)          (5)
haviour, it also increases the fidelity of transmission of the
majority grammar in a sample of a fixed diversity (holding            where Γ(x) = (x − 1)! when x is an integer,3 n j is the number
i/ j constant, increasing b increases the quantity i − j). Both       of meanings expressed using class j and α ≥ 1 determines the
these effects lead to an increase in the dominant grammar’s           strength of the preference for regularity: low α gives a strong
share of the population. The impact of the two effects can            preference for regular languages, higher α leads to a weaker
be probed by implementing a minor extension to the model              preference for such languages.
outlined above, where learners learn from a specified number             The probability of a particular meaning-form pair hx, yi
of models (c), with b/c data items from each parent. Table            (consisting of a meaning x and a signal class y) being pro-
2 shows the eventual proportion of h0 in converged popula-            duced by an individual with grammar h is:
tions for various c and b (for convenience we only consider
cases where b/c yields integer values). As can be seen from
                                                                                                 1 − ε if y is the class for x in h
                                                                                              
the table, increasing c or b independently increases the dom-                             1
                                                                         P(hx, yi |h) =     .      ε                                    (6)
inance of the winning grammar. Importantly, any diversity of                              m      k−1    otherwise
models (c ≥ 2) results in a single grammar winning out. For
b = 2, the grammar favoured by the prior wins out in situa-           where ε gives the noise probability on production and all
tions where learners received perfectly mixed input, and for          meanings are equiprobable (hence the scaling by 1/m).
b > 2 the conformity effect outlined above also comes into               We can then plug this production model into the two pop-
play.                                                                 ulation learning models outlined above. pi,t gives the pro-
                                                                      portion of individuals at generation t who select hi (again,
                                                                      learners are required to select a single grammar). If learners
Table 2: Stable proportion of h0 for various c and b. P(h0 ) =        at generation t + 1 learn from a single model:
0.6, ε = 0.05, both grammars initially equally frequent.
                                                                                     pi,t+1 = ∑ ∑ P(hi |d).p j,t . ∏ P(x|h j )          (7)
                                    b                                                          d   j               x∈d
   c    1      2       3         4        6       8         12
                                                                      where the sums are over all possible data sets and all possible
   1    0.6    0.6     0.6       0.6      0.6     0.6       0.6
                                                                      model grammars, and the products are over the b items in
   2    -      0.822   -         0.964    0.993   0.999     1
                                                                      each data set. If a learner learns from multiple models:
   3    -      -       0.983     -        0.999   -         1
   4    -      -       -         0.992    -       1         1                                                                 !
                                                                                 pi,t+1 = ∑ P(hi |d). ∏     ∑ p j,t .P(x|h j )  .       (8)
                                                                                            d          x∈d    j
                   A more complex model                               Results
                                                                      The main features of the two-grammar model are preserved
While the results for the two-grammar model are potentially           in the more complex model: sensitivity to initial conditions, a
interesting, one might reasonably worry that they are reliant         dependency on b, and an interaction between strength of prior
on some feature of the simplest possible two-grammar model.           and b.
Of particular interest are the models in the literature which            Figure 2 shows the final stable distribution over all gram-
allow multiple grammars with equal prior probability. With            mars for strong and weak prior preferences in favour of regu-
this in mind, the grammar model from Kirby et al. (2007)              larity, for various values of b. For b = 1 the standard sampling
is adopted here: similar results can be obtained for the 260-
grammar model of Griffiths and Kalish (2007).                             3 We will only consider the case where α takes integer values.
                                                                  700

result for learning from a single cultural parent is retrieved.
For high b, the majority of the population converges on one
of the a priori more likely grammars (with the identity of the
winning grammar depending on the initial frequencies). In-
deed, for b = 10 the strength of the prior preference in favour              0                        1       0                      1
of regularity makes little difference to the final distribution.         H                                H
   Finally, there appears to be a critical value of b required
for the population to converge on a single majority grammar.             M                                M
For b below this critical value, the would-be dominant gram-                           α=1, prior                     α=20, prior
mar suffers from a lack of transmission fidelity: learners tend
to receive data sets which underspecify the target language,             L                                L
and the posterior probabilities of the various languages are             H                                H
therefore heavily constrained by the prior. Note, however,
that the stable distribution is not identical to the prior: the
differences in prior probability are smoothed out somewhat.              M                                M
                                                                                        α=1, b=1                      α=20, b=1
Above the critical value of b (which is around b = 2m, but is
somewhat sensitive to α), transmission fidelity becomes suf-
                                                                         L                                L
ficiently high to allow one grammar to dominate through the
processes discussed for the two-grammar model. This con-                 H                                H
straint on b is analogous to the coherence threshold described
in Nowak et al. (2001).
                                                                         M                                M
                                                                                        α=1, b=3                      α=20, b=3
                        Discussion
The two models described above represent a first attempt to              L                                L
explore the impact of population structure on the outcomes
                                                                         H                                H
of iterated learning in populations of Bayesian agents. While
much remains to be done, they show that the analytic result
provided by Griffiths and Kalish (2007) can break down un-               M                                M
der some model configurations. Before considering the im-                               α=1, b=6                      α=20, b=6
plications of this finding, it is worth considering some of the
model’s more serious limitations.                                        L                                L
   Learners are required to select a single grammar based
                                                                         H                                H
on exposure to a potentially diverse sample (or equivalently,
learners use each grammar probabilistically, with probabil-
ities determined by their posterior probability). It may be              M                                M
                                                                                       α=1, b=10                      α=20, b=10
that there are more sophisticated treatments of the hypothe-
sis selection task for which the Griffiths and Kalish (2007)
result can be retrieved. One obvious possibility is to consider          L                                L
cases where learners have a structured model, such that they
appreciate that their data potentially comes from multiple in-
                                                                         Figure 2: m = 3, k = 3, ε = 0.05. Stable proportions of all
dividuals who may (or may not) use different grammars and
                                                                         27 grammars, grouped by prior probability (H are the highly
who may (or may not) exhibit consistent usage. Hierarchical
                                                                         regular grammars, L are the low regularity grammars, M are
Bayesian approaches can be straightforwardly used to model
                                                                         the grammars of intermediate regularity). The top row gives
this type of learner, and can therefore be used to explore the
                                                                         prior probability distributions for two values of α. The re-
evolutionary consequences of learning from multiple models
                                                                         maining rows give the stable proportions for various values
in a more satisfying (and cognitively plausible) fashion than
                                                                         of b. In all cases the population is initialised with one regu-
that described here.
                                                                         lar grammar having a slightly boosted frequency and all other
   The population model used here also offers only a minimal             grammars being equally frequent. All proportions have un-
increase in complexity over its predecessors. Although it adds           dergone a square root transformation to show the variability
the possibility of learners learning from multiple (equally-             among the less frequent grammars.
weighted) models, it entirely ignores horizontal (within-
generation) transmission. Populations also lack any interest-
ing internal structure. Transmission in real-world populations
takes place over complex social networks, with implications
for language structure (see e.g. Kerswill & Williams, 2000),
                                                                   701

a phenomenon little explored in the modelling literature.             pological distributions of languages in the world. The results
   The results presented here suggest that caution must               presented here show that this modelling result is dependent
(at least at present) be used when extrapolating from cul-            on learners learning from a single model. When this idealisa-
tural evolution in convenient one-individual iterated learning        tion is relaxed, the straightforward mapping from prior bias
chains to larger populations. While diffusion chain experi-           to typology breaks down.
ments provide a powerful tool for identifying the prior biases
of learners, in real populations those prior biases are fed in                           Acknowledgements
to a population dynamic whose consequences are largely not            Thanks to Dan Dediu, Mike Dowman, Tom Griffiths, Mike
understood. Exploring transmission in larger and more com-            Kalish, Tim O’Donnell and Jelle Zuidema for helpful com-
plex laboratory populations may prove necessary.                      ments.
   Secondly, the implication of this modelling work is that
                                                                                               References
assuming a straightforward or one-to-one mapping between
the biases of learners and the typological distributions of lan-      Baker, M. (2001). The atoms of language: The mind’s hidden
guages in the world may be unsafe. While the Griffiths and               rules of grammar. New York, NY: Basic Books.
Kalish (2007) result offers some hope in this direction (and          Boyd, R., & Richerson, P. J. (1985). Culture and the evolu-
may indeed still hold given a more sophisticated treatment               tionary process. Chicago, IL: University of Chicago Press.
of the multiple-model case, of the sort discussed above), the         Brighton, H. (2002). Compositional syntax from cultural
results presented here shows that, under certain assumptions             transmission. Artificial Life, 8, 25–54.
about the nature of the learning problem, changing the pop-           Brighton, H., Kirby, S., & Smith, K. (2005). Cultural se-
ulation dynamic can change the outcomes of cultural evolu-               lection for learnability: Three principles underlying the
tion: the typological distribution of languages in these models          view that language adapts to be learnable. In M. Taller-
is emphatically not the prior.                                           man (Ed.), Language origins: Perspectives on evolution
                                                                         (pp. 291–309). Oxford: Oxford University Press.
   Furthermore, in some situations the ranking of languages
                                                                      Brighton, H., Smith, K., & Kirby, S. (2005). Language as an
in the prior is not even preserved in the final distribution of
                                                                         evolutionary system. Physics of Life Reviews, 2, 177–226.
languages. Previous work in the Bayesian framework sug-
                                                                      Chomsky, N. (1965). Aspects of the theory of syntax. Cam-
gests that cultural evolution returns some distribution which
                                                                         bridge, MA: MIT Press.
is the same shape as the prior distribution — either the prior
                                                                      Christiansen, M., & Chater, N. (2008). Language as shaped
itself, or (in MAP populations) some stretched version of the
                                                                         by the brain. Behavioral and Brain Sciences, 31, 489–509.
prior where the magnitudes of the differences are changed but
                                                                      Griffiths, T. L., & Kalish, M. L. (2007). Language evolu-
the overall rank ordering of hypotheses in the stationary dis-
                                                                         tion by iterated learning with Bayesian agents. Cognitive
tribution is the same as in the prior. However, close inspec-
                                                                         Science, 31, 441–480.
tion of Figure 2 shows a different picture: some languages
                                                                      Griffiths, T. L., Kalish, M. L., & Lewandowsky, S. (2008).
with high or intermediate prior probability end up being less
                                                                         Theoretical and experimental evidence for the impact of in-
frequent than languages with the lowest prior probability. In
                                                                         ductive biases on cultural evolution. Philosophical Trans-
other words, attempting to read off even the ranking of lan-
                                                                         actions of the Royal Society, 363, 3503–3514.
guages in the prior from this typological distribution would
                                                                      Kerswill, P., & Williams, A. (2000). Creating a new town
lead to error. In the model this is largely due to competition in
                                                                         koine: Children and language change in Milton Keynes.
a homogeneous population between the languages with high
                                                                         Language in Society, 29, 65–115.
prior probability, combined with the coupling of each mid-
                                                                      Kirby, S. (2002). Learning, bottlenecks and the evolution of
ranking language to high-ranking languages with which they
                                                                         recursive syntax. In E. Briscoe (Ed.), Linguistic evolution
overlap. However, the point remains: the outcome of cultural
                                                                         through language acquisition: Formal and computational
transmission in populations where learners learn from multi-
                                                                         models (pp. 173–203). Cambridge: Cambridge University
ple models is not necessarily simply determined by the prior,
                                                                         Press.
but also by transmission factors such as the amount of data
                                                                      Kirby, S., Dowman, M., & Griffiths, T. L. (2007). Innateness
learners receive and the diversity of the set of models they re-
                                                                         and culture in the evolution of language. Proceedings of
ceive it from. The relationship between prior biases of learn-
                                                                         the National Academy of Sciences, USA, 104, 5241–5245.
ers and observed typological distributions is not transparent.
                                                                      Niyogi, P. (2006). The computational nature of language
                                                                         learning and evolution. Cambridge, MA: MIT Press.
                        Conclusions
                                                                      Nowak, M. A., Komarova, N. L., & Niyogi, P. (2001). Evo-
Some work on the outcomes of cultural transmission in pop-               lution of universal grammar. Science, 291, 114–117.
ulations of Bayesian learners suggests that cultural evolution        Zuidema, W. H. (2003). How the poverty of the stimulus
will deliver up the prior distribution. This implies that we can         solves the poverty of the stimulus. In S. Becker, S. Thrun,
gain insights into population behaviours by studying diffu-              & K. Obermayer (Eds.), Advances in Neural Information
sion chains (highly amenable to laboratory investigation), and           Processing Systems 15 (pp. 51–58). Cambridge, MA: MIT
that we can read off the prior biases of learners from the ty-           Press.
                                                                  702

