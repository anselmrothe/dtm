UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A First-Person Perspective on a Parent-Child Social Interaction During Object Play
Permalink
https://escholarship.org/uc/item/2qf8k7h3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Pereira, Alfredo F.
Shen, Hongwei
Smith, Linda B.
et al.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                      University of California

 A First-Person Perspective on a Parent-Child Social Interaction During Object Play
                Alfredo F. Pereira, Chen Yu, Linda B. Smith, Hongwei Shen (afpereir@indiana.edu)
                       Department of Psychological and Brain Sciences and Department of Cognitive Science
                                              1101 E 10th St, Bloomington, IN 47405 USA
                              Abstract                                    knowledge that constrains attention and learning (Bloom,
                                                                          2000).
   We studied parent-child social interactions in a naturalistic             We also view this situation as a context in which there is
   tabletop setting. Our approach was to analyze the real-time            considerable complexity, but we view the solution to that
   sensorimotor dynamics of the social partners as they engage in         complexity in terms of the coupled sensory-motor dynamics
   joint object play. From the child’s point of view, what she
                                                                          of the participants. This joint activity happens in the
   perceives critically depends on her own and her social partner’s
   actions, as well as on her actions on objects. These                   moment and is itself a source of constraints on cognitive
   interdependencies may scaffold learning if the perception-action       development (rather than mere noise to be overcome).
   loops, both within a child and between the child and his social        Accordingly, our research goal is to examine the natural
   partner, can simplify the environment by filtering irrelevant          dynamic structure of real time experience as it evolves in
   information, for example while learning about objects.                 developing children’s active engagement with physical
   In light of this general hypothesis, we report new findings.           objects and with social partners.
   These seek to describe the visual learning environment from a
   young child’s point of view and measure the visual information               Studying the Sensorimotor Activity of a
   a child perceives in a real-time interaction with a parent. The                           Parent-Child Dyad
   main results are (1) what the child perceives most often depends
   on her own actions and her social partner’s actions; (2) there are
                                                                          The experimental method we developed, allows researchers
   distinct interaction patterns in naturalistic child-parent             to measure, in a laboratory setting, the sensorimotor activity
   interactions; these can influence the child’s visual perception in     generated by two social partners (Smith, Yu and Pereira,
   different ways; (3) The actions generated by both social partners      sub.; Yu, Smith, Christensen and Pereira, 2007; Yu, Smith
   provide more constrained and clean input to the child,                 and Pereira, 2008). Our initial goal was to retain the
   presumably facilitating learning.                                      richness of a naturalistic social interaction but exploit the
   Keywords: embodied cognition; learning; computational                  controlled laboratory environment to make large-scale
   modeling; perception-action.                                           automatic data collection and analysis feasible. The task
                                                                          given to parents was straightforward, play with their child
                          Introduction                                    across a small table with toys; a common experience in the
   Consider a small moment in the everyday life of a toddler,             lives of children included in this study. Other than sitting at
one so common as to pass as uneventful: sitting in a living               a table and playing with a small set of objects, no more
room, playing with toys and with an adult. To the casual                  restrictions were placed in the interaction.
observer, watching from the outside as these social partners              Apparatus: Multimodal Sensing system
interact, cognitive development seems quite effortless. Even
very young children can engage smoothly in a social
exchange, take turns, show bouts of clear joint attention, and
switch attention between objects they manipulate and a
social partner. Developmental psychologists have long
documented that scenes such as this one are not cognitively
simple. For example, linking a word to a referent requires
the child to succeed at a multitude of tasks: parsing of
objects in a scene, motor plans to manipulate objects,
establish joint attention, shifts of attention between objects
and the social partner, correctly segment and identify the
object name from the adult’s speech, etc. More importantly,
the variability and noise in the scene can be substantial in
real world contexts. For example, there could be multiple
objects on the floor being seen and talked about, multiple
possibilities for action, and no synchrony between the
adult’s speech and the child’s object perception (Baldwin,                Figure 1: Experimental setup used. The child wears a
1993; Bloom, 2000). Properties of natural interactions such               headband with a small camera and a motion sensor. An
as these, have led many researchers to propose that the child             additional camera was placed right above the table and
brings into the moment, social, linguistic and conceptual                 provided an unblocked birds-eye view of the interaction.
                                                                      839

Room Setup A large 6.5m x 3.3m room was split in two.            an object that have light reflections on them as well (this
On one side we placed all the recording equipment and an         problem can be fixed in step three). The second step focuses
experimenter could sit at a computer to control the              on the remaining non-background pixels and breaks them up
experiment. On the other side we created an interaction          into several blobs using a fast and simple segmentation
environment for the parent and child. White, movable             algorithm (Comaniciu and P. Meer, 1997). The third step
curtains, from floor to ceiling, mounted on a ceiling track,     assigns each blob into an object category. In this object
surrounded a 3.3m  3.1m space. At the center of that area       detection task, we used Gaussian mixture models to pre-
we placed a small 61cm  91cm  64cm white table. Both           train a model for each individual object (Moghaddam and
participants wore white clothing. A consequence of this          Pentland, 1997). As a result of the above steps, we extract
setup was that, from the perspective of an head mounted          useful information from image sequences, such as what
cameras, any pixel not similar to white could only come          objects are in the visual field at each moment, what are the
from an object, a hand or a head of one of the participants.     sizes of those objects, and whether a hand is holding an
Head-Mounted Camera A small and lightweight camera               object (from the top-down view), which will be used in the
was mounted on a soft elastic sports headband. The               following data analyses. An object is labeled as held by a
recording rate for the head-mounted camera was 10 frames         participant if the object blob is overlapped with a hand blob
per second. The resolution of image frame is 320x240. The        for more than 10 frames. We asked two human coders to
lens focal length is f3.7mm and the CCD’s size is 6.35mm x       annotate a small proportion of the data (~1200 frames) and
6.35mm yielding a field of view of 81 degrees in the             compared the results with image processing results with
horizontal and in the vertical direction. The camera mount       91% of agreement.
piece allowed the camera to rotate on the horizontal axis
perpendicular to the camera’s line of sight. We used this to
adjust the angle between the camera line of sight and the
participant’s head orientation. Using a similar context of
tabletop play, Yoshida and Smith (2008) showed that the
head-mounted camera is a good approximation of the
contents of the child’s visual field. In fact, 90% of head
camera video frames in their study corresponded with
independently coded eye positions, which was largely due to
both the restricted geometry of the tabletop play and the
typical behavior of young children in this kind of
interaction.
Bird-Eye View Camera To provide for an unblocked,
static view of the activity on the table we placed a high-
resolution camera above it. The high quality of this data
stream also meant it was possible to improve the quality of
the visual segmentation for the video from a head-mounted
camera using information fusion. Also, to calculate what
objects were held by each participant.
Head motion tracking In addition to the camera mounted
in the headband we also mounted a Polhemus motion-               Figure 2: The overview of data processing using computer
tracking sensor to record the child’s head position and          vision techniques. Top: we first remove background pixels
orientation.                                                     from an image and then spot objects and hands in the image
Data Processing Pipeline: Image Segmentation and                 based on pre-trained object models. Bottom: the processing
Object Detection                                                 results from the bird-eye view camera.
The first goal of data processing pipeline was to
automatically extract visual information, such as the            Experimental Study: Object Play Embedded in
locations and sizes of objects, hands, and faces, from              a Social Interaction With a Mature Partner
sensory data in two cameras. These are based on computer         Participants Fifteen dyads of parent and child participated
vision techniques, and include three major steps (see Figure     in this study. Five children were boys. The target age period
2). Given raw images, the first step is to separate              for this study was 18 to 24 months, a period of large
background pixels and object pixels. Since we designed the       developmental changes in early word learning and visual
experimental setup (as described above) by covering the          object recognition (Smith and Pereira, in press). Children’s
walls, the floor and the tabletop with white fabrics and         mean age was 21.3 month, ranging from 19.1 to 23.4. All
asking participants to wear white cloth, we simply treat         families were from the Bloomington, IN area, white and
close-to-white pixels in an image as background.                 middle class. Six additional dyads were not included in the
Occasionally, this approach also removes small portions of
                                                             840

final analysis because the child refused to wear the                 is also of a minimum size. We chose a stringent rule: an
equipment.                                                           object is dominant if its ratio inside all object pixels is
Stimuli Parents were given four sets of toys (three toys for         greater than 0.7 and its total image size is greater than 6000
each set) in a free-play task. The toys were either rigid            pixels (7.8% of the image frame). This is a conservative
plastic objects or plush toys. Most of them had simple               choice of parameters and most likely represents a lower
shapes and either a single color or an overall main color.           bound on visual salience since this measure just takes in
Some combinations of objects were selected to elicit actions         consideration the raw visual size in the image frame.
that were especially evident to an adult asked to play with             Our hypothesis was that the child’s view might provide a
them.                                                                unique window onto the world by filtering irrelevant
Procedure The procedure needed three experimenters in                information (through movement of the body close to the
total. Two of them had specific roles when interacting with          object, or by bringing the object closer), enabling the child
parent and child: experimenter (A) focused on providing              to focus on one object (or one event) at a single moment. As
instructions to the parent and placing the equipment on the          shown in Figure 3, in almost 60% of frames, there is one
child, while another experimenter (B), kept the child                dominating object in the child’s view that is much larger
distracted continuously until parent and child were ready to         than other objects (even when using a conservative rule for
start the study. Parents were told in vague terms that the           what counts as dominating). This result suggests that the
study’s goal was to investigate how parent and children              child’s view may provide a constrained and cleaner input,
interact with the each other. The task was to take three toys        therefore facilitating learning processes by removing the
from a drawer next to them and play for a fixed period of            need to internally handle and filter irrelevant data. If one
time. After an indication from the experimenters, they               object (or event) dominates at time, then the focus of
should simply switch to the next set of toys.                        learning at the moment is externally decided by the child’s
                                                                     bodily selection.
   When the child seemed well distracted, experimenter (A)
placed the headband with the camera and motion sensor in
the child’s head. This was done quickly and in a single
movement that placed camera as close as possible of the
child’s eyes. Experimenter (B) kept playing and introduced
novel toys to keep the child’s interest away from the
headband.
Calibration of Head-Mounted Camera To calibrate the
horizontal camera position in the forehead and the angle of
the camera relative to the head, the experimenter asked the
child to look into one of the objects on the table, placed
close to the child. Experimenter (C) controlling the
recording in another room confirmed if the object was at the
center of the image and if not small adjustments were made
on the head-mounted camera gear.
Trial onset and offset At this point both experimenters left
                                                                        Figure 3: Average proportion of time inside each trial,
the room. Parents were told to listen to a specific sound and
                                                                     across all trials where: there is a dominating object; there
when alerted by it, start a new trial. This marked trial onset.
                                                                     are no object in the child’s view; there are 1,2 or 3 objects in
There were a total of four 90-second trials. The entire study,
                                                                     the child’s view (the last four categories sum to one). Note
including initial setup, lasted about 15 minutes.
                                                                     that although there are always three objects on the table, in
                                                                     only less than 20% of time, all of the three objects are in the
  Data Analysis and Results: Visual Perception                       child’s visual field while most often there are only 1 or 2
In this section, we report our results of the child’s visual         objects (more than 55% in total) in their visual field.
perception, exploring how the child’s own actions and the            Further, in more than 55% of time, there is always a
parent’s activities may change what the child perceives. We          dominating object in the child’s visual field at a moment.
used in total 56 trials from 15 dyads (with 10% of trials
from those subjects excluded, due to technical problems              Changes in Dominating Object
such as the quality of data recording). All of the following            The dominating object may change from moment to
measures and analyses are trial-based and correspond to              moment, and also the locations, appearances and the sizes of
averaging sensory data within a trial.                               other objects in the visual field change as well. Thus, we
                                                                     first calculated the number of times that the dominating
The Salience of the Visually Dominating Object                       object changed. From the child’s viewpoint, there are on
We defined the dominating object within a frame as the               average 15 such object switches in a single trial, suggesting
object that takes the largest proportion in the visual field and     that young children often move their head and body and in
                                                                 841

so doing switch attended objects, attending at each moment            We then calculated the proportion of time for each event
to just one object.                                                   type (c0, c1, c>1 and p0, p1, p>1) within each trial. By doing so
   In summary, the main result so far is that the child’s view        we represented each trial as a 6-dimensional vector, defined
is far more selective than the available information in the           by the proportion of time of number of objects held by the
environment (the third-person static view) with respect to            child and the parent. Next, we used a hierarchical
(1) the spatial distributions of objects; (2) the salience of         agglomerative clustering method (Jain and Dubes, 1988) to
individual objects; and (3) the temporal dynamics of the              group 56 interaction trials based on joint hand-state
dominating objects.                                                   representations described above.
                                                                         Four primary interaction patterns were found that cover
  Data Analysis and Results: Hand Actions and                         more than 85% (48/56) of all the trials (i.e. they are included
                     Visual Perception                                in one of the groupings found by the clustering step). These
Our next analyses considered factors organizing the child’s           four interaction patterns can be characterized as: 1) child-
view. At any moment, gaze and head movements may cause                lead interaction with high activity wherein the child’s hands
changes in the child’s visual field. We have directly                 are actively holding and manipulating objects most of the
measured this using a motion tracking system to record head           time; 2) bi-directional interaction wherein both the child and
movements in the same setup as the main experiment. The               the parent are holding objects; 3) parent-lead interaction in
results from that study show that young children rarely               which the parent is frequently holding an object; and 4)
move their head toward (closer to) the objects and therefore          child-lead interaction with low activity of the participants;
head movements wouldn’t cause the changes of object sizes;            both participants are not very active but the child’s hands
that is changes in the dominating object (Pereira, Smith and          are holding objects more than the parent does. For example,
Yu, 2008). In addition, gaze shifting in this current                 in the trials categorized as the child-lead interaction with
experiment setup may change the location of an object but             high activity, the child is holding only one object more than
not its size in the child’s visual field. These results suggest       64% of time (c1) while the parent does that only 36% of
that selectivity, in this setting, happens not through head
                                                                      time (p1). In the bi-directional interaction, both the child and
movements but through hand movements that may bring
                                                                      the parent spent a significant amount of time on holding one
objects into view. Both the child’s own hand movements
and the parent’s hand movements are potentially relevant              of the three objects on the table (c1 = 45% and p1 = 40%).
since we already know that both participants’ hands are                  Our following data analysis will focus on how both the
frequently appearing in the child’s visual field (Yu et al.,          child’s own actions and the parent’s actions may influence
2007). In light of this, we measure how hand actions by               the child’s visual perception in the context of these different
both the child and the parent select the objects in view for          interaction patterns.
the child.
Categorizing Parent-Child Interaction Based on
who is Holding Objects
In naturalistic interactions, there is variation in the nature of
the exchange from person to person across dyads, and from
moment to moment even with the same dyad. Some dyads
are more actively engaged with the object play task and with
potential to-be-learned objects, and others may be less
active. For example, how well toddlers and parents
coordinate their actions - the smoothness of their “body-
prosody” - is correlated with object name learning (Pereira,
Smith and Yu, 2008).
   Thus, we sought to quantify the quality of dynamic
interaction within and across dyads concentrating on how
they use their hands to reach, touch, move and manipulate
to-be-learned objects, and then used this to quantify the                Figure 4: Proportion of trials in each of the four primary
interaction.                                                          interaction patterns identified by the clustering step. A
   We first calculated six individual measures based on what          descriptive of each pattern is given below each bar by
the child and the parent were holding in each moment                  showing the mean value inside the group for each of the six
(sampled at 10Hz): 1) c0: the child is not holding any object.        individual measures (c0, c1, c>1 and p0, p1, p>1).
2) c1: the child is holding one single object. 3) c>1: the child
is holding more than one objects; 4) p0: the parent is not
                                                                      How Often Were the Participants Holding Objects?
holding any object; 5) p1: the parent is holding one single
object; 6) p>1: the parent is holding more than one objects.
                                                                  842

As shown in Figure 5 (left), in three out of the four defined        – the child’s and the parent’s – in visual selection, and also
interaction patterns, either the child’s hands or the parent’s       raise the question of whether actions by the two participants
hands are holding at least one object more than 70% of time.         play different roles for different children, or in different
Critically, hand actions (and other body parts, such as the          moments of learning.
orientation of the trunk) may signal social cues to the other
social partner indicating the object of interest in real time.
From this perspective, among these four patterns, the child-
lead interaction with high activity and the parent-lead
interaction potentially might be seen as creating a better
learning environment, wherein the percentage of time that
either the child or the parent (at least one of them) is holding
an object -- and thus clearly signaling interest in it -- is
higher than in the other two interaction patterns.
   A relevant result with respect to this idea is that the
average percentage of time an object is held in the
interaction pattern labeled “child-lead interaction with low
activity by both participants” is much lower than in the
other three interaction patterns.
   As shown in Figure 5 (right panel), the child and parent
are holding different objects 28% of time in the parent-lead
interaction; this difference in manual selection happens
much less frequently in the other three interaction patterns.
If hands signal the focus of the attention of the participant,
then the child and parent in the parent-lead interaction             Figure 5: Left: the percentage of time that either the child or
pattern are often not sharing attention. One possibility is that     the parent (at least one of them) is holding an object in each
when the parent attempts to lead the interaction by attracting       of the interaction pattern. Right: the percentage of time that
the child’s attention to the object held by the parent, the          both participants hold objects and that the objects
child does not follow the parent’s lead immediately but              individually held are different, in each of the interaction
instead explores other objects that the child himself is             patterns.
interested at the moment. Taken together, our results
suggest that hands play an important role in selecting the
attended information with the child’s own manual activities
perhaps being most important. In particular, the child-lead
interaction with high activity may provide the cleanest data
for the child.
How Does the Child Perceives Held Objects?
Since both participants’ hands are holding objects virtually
all the time, we examined how the child perceives an object
when the child versus the parent holds it. The first main
result is that objects that are held – by either participant –
are significantly larger in the child’s view (and thus more
likely to be dominating the view as defined above).
   As shown in Figure 6, objects in hands consistently take a
considerable proportion of the child’s visual field,
calculated as proportion of the field occupied by objects
(more than 50%). This is true for all of the four interaction
patterns. Even in the child-lead interaction with (relatively)       Figure 6: The average size of objects held by the child
low activity of both participants, objects in hands dominate         and/or the parent in the child’s visual field (as a proportion
the visual field in relative terms compared to other objects.        of the image frame). There are three conditions: 1) the
The second main result concerns the differences between              objects only held by the child; 2) the objects only held by
the child-lead interaction with high activity and the parent-        the parent; and 3) the objects held by either the child’s or
lead interaction: the child’s hands play a more important            the parent’s hands. In all the above cases, we measure the
role in influencing the child’s own visual field in the first        relative proportion of objects in hands compared with other
case while the parent’s hands are more important in the              objects in view.
second case. These results indicate the importance of action
                                                                 843

                   General Discussion                                                      Acknowledgements
   Viewed from a third-person perspective, the world is a                This work was supported in part by National Science
cluttered, noisy, and variable place with many overlapping            Foundation Grant BCS0544995, and by NIH grant R21
objects. From this point of view, the learning task seems             EY017843. A.F.P. was also supported by the Portuguese
severe – objects must be segregated one from another, and a           Fulbright Commission and the Calouste Gulbenkian
speaker’s intended referent will be ambiguous. However,               Foundation. The authors wish to thank Char Wozniak,
the third-person view is not the view that matters for                Amara Stuehling, Jillian Stansell, Saheun Kim, Heather
children; the view that matters is their own, the first person        Elson and Melissa Elson for data collection.
view. The visual data collected from a camera placed on a
child’s forehead suggest that the child’s visual field is
filtered and narrowed, principally by both the child’s own                                      References
hand actions but also in some cases by the caregiver’s                Baldwin, D. (1993). Early referential understanding:
actions. Young children solve the problem of noisy and                  Infant’s ability to recognize referential acts for what they
ambiguous data for learning about objects by using the                  are. Developmental psychology, 29, 832-843.
external actions of their own body to select information.             Bloom, P. (2000). How children learn the meanings of
This information reduction through their bodily actions                 words. Cambridge, MA: The MIT Press.
most certainly reduces ambiguity and by doing so provide              D. Comaniciu and P. Meer, “Robust analysis of feature
an adaptive strategy to bootstrap learning                              spaces: color image segmentation,”Proc. of IEEE Conf.
   The present results also make three points that have not             on Computer Vision and Pattern Recognition, pp 750-
received sufficient attention in the literature. The first              755, 1997.
                                                                      Jain, A.K., and Dubes, R.C.,1988, Algorithms for Clustering
concerns the activity of the child in selecting visual
                                                                        Data, Prentice Hall: New Jersey.
information. Considerable research has focused on the
                                                                      Moghaddam, B., & Pentland, A. (1997). Probabilistic visual
social context and how parents select information by                    learning for object recognition. IEEE Transactions on
guiding the child’s attention. But the present results make             Pattern Analysis and Machine Intelligence, 19(7), 696-
clear that the child’s own activity is also critical and any            710.
effect of social scaffolding from the mature partner/ parent          Pereira, A. F., Smith, L. B., Yu, C. (2008). Social
will depend on the child’s own interests and activities,                Coordination in Toddler's Word Learning: Interacting
which may or may not be well coupled to the parents.                    Systems of Perception and Action. Connection Science,
Second, the results point to manual activities as a major               20(2).
factor in selecting and reducing the visual information.              Smith, L. B., & Pereira, A. F. (in press). Shape, action,
Hands that grab objects and bring them closer to the eyes               symbolic play, and words: Overlapping loops of cause
                                                                        and consequence in developmental process. in S. Johnson
make those objects large in the visual field and also block
                                                                        (Ed.), A neo-constructivist approach to early
the view of other objects, consequences that may benefit                development. New York: Oxford University Press.
many aspects of object recognition (including segregating             Smith, L.B., Yu, C. & Pereira, A. F. (under review). Not
objects, integrating views, and binding properties) and                 Your Mother’s View: The Dynamics of Toddler Visual
object name learning. Finally, the results indicate important           Experience. Psychological Science.
issues for future work. There may be significant individual           Yoshida, H. & Smith, L.B. (2008). Hands in view: Using a
differences in how social partners’ activities support                  head camera to study active vision in toddlers. Infancy.
embodied attention and learning. It could be that some                Yu, C., Smith, L. B., & Pereira, A. (2008). Grounding Word
patterns of interaction lead to better learning (e.g., child lead       Learning in Multimodal Sensorimotor Interaction.
with high activity) than others. Alternatively, it may be that          Proceedings of the 30th Annual Meeting of the Cognitive
there are different learning styles, each potentially as                Science Society, Washington, DC, USA.
effective, that engage different mechanisms.                          Yu, C., Smith, L. B., Christensen, M., & Pereira, A. F.,
     Young children are fast learners and they are so through           (2007). Two Views of the World: Active Vision in Real-
                                                                        World Interaction. In McNamara & Trafton (Eds.),
their embodied interactions with people and objects in a
                                                                        Proceeding 29nd annual conference of cognitive science
cluttered world. In the present study, we suggest the                   society (p 731-736). Mahwah.NJ: ErIBaum
importance of embodied solutions – how the young child
and his social partner may use their bodily actions to create
and dramatically shape regularities in a learning
environment.
                                                                  844

