UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Incremental Model Construction: Eye-movements reflect mental representations and
operations – even if there is nothing to look at.

Permalink
https://escholarship.org/uc/item/6r79f14j

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)

Authors
Bittner, Andreas
Fangmeier, Thomas
Konieczny, Lars
et al.

Publication Date
2009-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Incremental Model Construction:
Eye-movements reflect mental representations and operations –
even if there is nothing to look at.
Marco Ragni, Thomas Fangmeier, Andreas Bittner, Lars Konieczny
({ragni, fangmeier, bittner, lars}@cognition.uni-freiburg.de)
Center for Cognitive Science
Friedrichstr. 50
79098 Freiburg, Germany

Abstract
In the following we present a cognitive model and a visual
world experiment to test a fundamental hypothesis derived
from the mental model approach: the assumption that special
relational reasoning relies on a mental model manipulation
device responsible for model construction, inspection and
variation. We will provide evidence for a direct linking
hypothesis of eye-movements, demonstrating that the eye
reflects the fundamental operations of model construction,
even if there are no objects to look at.
Keywords: Spatial reasoning; eye-movement study; preferred
mental models; visual world experiment; attention focus

Introduction
There are a number of different cognitive theories which
try to explain the human reasoning process. Proponents of
theories based on formal rules (Braine & O’Brien, 1998;
Rips, 1994) claim that people solve reasoning problems by
applying formal rules (e.g. transitivity rules) to abstract
representations of the premises. In contrast, proponents of
mental models claim that the main strategy employed in
reasoning is the successive construction of a so-called
mental model of the state of the affairs. This model contains
all the information given in the premises. New information,
such as a reasoning problem’s conclusion, is generated or
evaluated by inspecting possible models (Johnson-Laird &
Byrne, 1991). To provide an example:
(1) The plate is to the left of the knife.
The fork is to the left of the knife.
The glass is in front of the knife.
The spoon is in front of the plate.

This describes the following two possible models:

fork

spoon
plate

glass
knife

spoon
plate

fork

glass
knife

The structure of such tasks is typically identical in
everyday life and in psychological laboratories: there is
always some given spatial information (the so-called
premises) and the participants have to generate a (putative)
conclusion based on these premises. The premise
information can be discerned by small-scale spatial objects
(e.g. fruit or cutlery which have to be arranged) or large-

scale objects (e.g. prominent buildings such as church
towers). In the domain of reasoning with spatial relations,
the theory of mental models (MMT) has received substantial
empirical support (Byrne & Johnson, Laird, 1989; JohnsonLaird & Byrne, 1991; Jahn, Knauff, Johnson-Laird, 2007;
Ragni, 2008). MMT assumes a three stage process
consisting of a comprehension, a description, and a
validation phase. In the comprehension phase, reasoners
construct a mental model that reflects the information from
the premises. If new information is encountered during the
reading of the premises it is immediately used in the
construction of the model. During the description phase,
this model is inspected to find new information that is not
explicitly given in the premises. Finally, in the validation
phase the reasoner tries to construct alternative models that
refute this putative conclusion. However, some questions
remain open with regards to how people deal with multimodel problems. For example, which model is constructed
first, and does this model construction adhere to certain
principles? Why do reasoners neglect some models?
The preferred mental model theory (PMMT) claims that
humans generally tend to construct a preferred mental
model (PMM). The PMM is the starting point for deriving a
putative conclusion. In the model variation phase the
participants tend to make local and continuous
transformations starting from the PMM to search counterexamples (Rauh et al., 2005).
Several predictions of the PMMT about insertion
principles as well as transformation strategies in spatial
relational reasoning can be shown (Ragni et al., 2006). For
the second premise in example 1 “the fork is to the left of
the knife” there are two possible arrangements. Humans
typically tend to process these premises sequentially, i.e.
first, a mental model of “plate - knife” is generated and then
the new information, the fork, is inserted into the existing
model. The second premise allows the insertion of the fork
at two different places: inbetween the plate and the knife
(first fit principle), or to the left of the plate (first free fit
principle). It had been shown empirically that PMMs are
generally constructed by using the fff-principle (Ragni et al.,
2006). One very important point, however, is: is it possible
to decide the classic question about the way we reason
based on evidence from eye-movements? MMT has a
necessary assumption–that mental models are constructed

3046

incrementally and therefore use a mental model
manipulation device. Is it possible to demonstrate model
construction and manipulation operations in eye movement
data?
In this paper, we first work out the central questions
important for such an eye-movement study in spatial
relational reasoning. We will then present an experiment we
have conducted to investigate a possible linkage between
eye-movement and mental model operations. We will argue
that eye-movements reflect operations on mental models,
even in the absence of relevant visual information.

State-of-the-Art
Reasoning with spatial relations is certainly one of the most
thoroughly investigated parts in the field of deductive
reasoning (Byrne & Johnson-Laird, 1989; Johnson-Laird &
Byrne 1991; Goodwin & Johnson-Laird, 2005; Ragni,
2008). In recent years only a small number of researchers
have investigated human deductive reasoning by using eye
tracking studies. Abed (1991), showed that left-to-right
readers (Western subjects) had the highest number of leftto-right eye movements when they observed visual stimuli,
while right-to-left readers (Middle Eastern participants) had
the highest number of right-to-left movements. This set of
studies suggests that reading habits strongly influence
scanning direction.
Körner & Gilchrist (2004) analyzed how participants
process a question about the spatial relationship between
two letters while looking at a visualization of this scenario
containing the two letters and two others as distractor items.
The study shows that the format of the question influenced
the nature of the eye movements. Furthermore, a tendency
to use additional eye movements to generate a fixation
sequence corresponding to the order of the letters in the
question was identified.
The construction or retrieval of a spatial mental model
showed similar eye movements which were used to
coordinate elements of the internal model with elements of
the external world (Spivey & Geng, 2001). This supports
the idea that the scan path plays a role in structuring visual
information to facilitate reasoning.
While there is, to our knowledge, no pure relational
reasoning research covering indeterminate tasks using eyetracking techniques up to this day, there is some work about
conditional reasoning. One of the very first works
concerning reasoning investigated the Wason Selection
Task (Ball et al., 2006). This research revealed an imbalance
in inspection time between selected and rejected cards
observed with indicative selection tasks and a generalization
to deontic versions of the task. This inspection time has
been connected to the heuristic-analytic account (Evans,
1983) of the selection task, where implicit heuristic
processes direct attention to relevant aspects of the problem
and determine card selections.
Johansson et al. (2006) have investigated the question of
how eye movements correlate with and reflect the positions
of objects while participants listen to a spoken description.

Nevertheless, there is an ongoing debate as to whether eye
movements reflect the search of a spatial mental model as
an internal representation of a picture, imagery (Finke,
1989, Kosslyn, 1994), a mental model (Johnson-Laird,
1983), or if no internal image exists and the eye movements
only indicate that the external environment will be used as
pointers or indexes (Pylyshyn, 2002; Spivey & Geng, 2001)
which propose a rule-based approach.

The Hypothesis
In the SRM (Spatial Reasoning by Models, e.g. Ragni et al.,
2007) a focus was used to place, manipulate objects and
inspect a model to find spatial relations which are not
explicitly given in the premises.
Premise

Premise

Control process

Control process

y

Focus

y

Focus
3

3

2

2

1

1

pliers
0

0

x

x

-1

-1

-2

-2
-2

-1

0

1

2

-2

3

-1

0

1

2

3

Premise 1
Premise 1

Control process
Control process

y

Focus

y

Focus

3

3

2
2

1
1

pliers hammer

pliers

0

0

x

x
-1

-1

-2

-2
-2

-1

0

1

2

3

-2

-1

0

1

2

3

Figure 1: The SRM model (Ragni et al., 2005; Ragni &
Knauff, 2008) after processing the first object of a premise
of the form “the pliers is to the left of the hammer”.
We want to explore to what extend the focus
manipulation device of the SRM shares similarities with the
eye-movements of humans – even if there are no visual but
only auditory stimuli. Previous findings (Ragni et al., 2007)
on this focus manipulation device indicate that the number
of operations based on the movement operations of a socalled focus is a good predictor of cognitive complexity.
This SRM model allows for an identification and
specification of mental model operations in spatial
reasoning and consists of an input device, an array, a focus,
and is controlled by a control process (Fig. 1.). The focus
(manipulation device) is able to move right/left/front/ and
behind, can insert and delete objects, and write annotations
(indeterminacy). Certainly the main theoretical point is that
it allows the introduction of a formal complexity measure to
explain human reasoning difficulty. Our starting point is
that this theoretical assumption may have a counterpart in

3047

eye-movements. We explore this question by recording eye
movements during the processing of spatial relational
reasoning problems.

The Empirical Investigation
If the eyes follow mental operations, they may do so even if
limited visual information is provided. We conducted an eye
movement study where only the first mentioned object was
presented in the center of the screen, where it remained
throughout the whole trial. All other objects mentioned in
the spoken premises were not displayed.
Eye movements are linked mostly to automatic bottom-up
processes. The main question of this experiment is if eyemovements may reflect bottom-up mental operations if one
deals with relational reasoning tasks. In addition to the
object in the center, we presented a two-dimensional lowcontrast grid covering the entire display (Figure 1). If
participants move their eyes according to the mental
construction operations, they might use the grid for aligning
their fixations, however, we have not encouraged or
instructed the participants to do so. Otherwise, data analysis
would have to be much less accurate, since saccades and
fixations would probably be much too noisy and chaotic
between participants to be analyzed together.

Figure 2: The presented screen in our eye movement study.
Only the first mentioned object (in this case the green car)
of the premises was visible during the whole premise
presentation (i.e. no other visual stimulus appeared).
We used four premises with five objects (5-term series
problems). The premises have eight different construction
sequences for the insertion of the next object (combination
of insertion direction left or right) as well as determinate
and indeterminate problems. Objects were always
introduced in the order new object in relation to the known
object.
Table 1: Determinate problems.
EXAMPLE: Determinate Problem
Premises

Model

A is to the left of B

AB

C is to the right of B

ABC

D is to the right of C

ABCD

E is to the right of D

ABCDE

Question
Example: Which relation has B to E (A to D)?

Table 2: Indeterminate problems
EXAMPLE: Indeterminate problem
Premises

Preferred M.

Alternative Models

1

2

3

4

A is to the left of B

AB

AB

AB

AB

C is to the left of B

CAB

ACB

ACB

ACB

D is to the left of C

DCAB

DACB

ADCB

ADCB

E is to the left of D

EDCAB

EDACB

EADCB

AEDCB

Question
Example: Which relation has C to E (E to C)?

A question about the relation of two objects in the
model provides the information if the participants have built
a possible (indeterminate) or correct (determinate) model.
The four premises were presented one after the other
acoustically. Each premise will be presented in a determined
time (externally paced). At the end the participant were
asked to decide which relation holds between two objects
and to press the button for “left” or “right”, respectively.
During the entire trial, they see a grid with 11 x 8 squares on
the screen and only the first object was placed in the center
square as an anchor. Only eye-movement data from correct
answers were analyzed.
We calculated the proportion of fixations on each of the
nine interest areas (IAs) per time bin (500 msec) during the
presentation of the premises. The underlying grid (11 x 8
squares) provided the interest areas (IAs). However, since
only one-dimensional horizontal models were investigated,
we limited the analysis to the four left (IA1-4) and four right
squares (IA6-9) in one line, plus the center itself, where the
first object was presented (IA 5). Since half of the trials
were mirror images of the other half, we mapped those trials
onto the other by mirroring the IAs.
Hypotheses
1) Is there a correspondence of eye-movements and the
mental focus in the SRM for determinate tasks? As
there is only one possible model in determinate tasks,
there is only one possible movement of the mental
focus – from the given object, which was already
introduced in the premise before in the relational
direction to the insertion of the new object in the
present premise.
2) Do the eye-movements reveal a preferred mental model
insertion principle during indeterminate tasks? We
assume that the insertion principle “free first fit” (see
above) will also be observable during the indeterminate
problems. For this reason, we expected increased
fixation proportions on the first free place compared to
the first place that fits.
3) Do the eye-movements explain the figural effect? A
strong effect of reasoning complexity is the so-called
figural effect, i.e. the difference between continuous
insertions or discontinuous insertions (Knauff et al.,
1998). This can be explained by the direction change

3048

of the mental focus (cf. Fig. 1). If there is a
correspondence between mental focus and eyemovements there should be an analogous change of the
IA fixations. If changes of the direction are necessary
the IAs during the “way” to the opposite end of the
model should be more frequently fixated and there
should be a higher duration until the target IA is
reached.
Participants. Eighteen undergraduate students of the
University of Freiburg took part in this experiment. They
were paid for their participation.

correspond to the mental focus movements in the SRM. If
this is the case, then the eye-movements for tasks of the
form LLLL should go directly in one direction, while for
tasks with direction changes (e.g. LRRR) the fixation
proportions should be increased on the IAs corresponding to
the directions change. The results for the first case are
depicted in Figure 3, the result for the latter in Figure 5.
Here the eye-movements correspond strongly with a
successive model generation, i.e. each corresponding cell is
fixated exactly after the respective object is named.

Materials. The experimental stimuli contained 32 problems,
16 determinate and 16 indeterminate problems (see Tab. 1
& 2). Examples of deterministic tasks are the following:
(2)

A is to the left of B
C is to the right of B
D is to the right of C
E is to the right of D

(3)

A is to the left of B
C is to the left of A
D is to the left of C
E is to the left of D

Which relation holds between B and D?
While the determinate task (2) constructs the model only
in one direction (and is therefore encoded in the following
by RRRR, since each new object is inserted to the right of
the previous one) the determinate task (3) has a direction
change, after the first premise (and is therefore encoded as
RLLL). An example for an indeterminate task (RRRR) is
(4)

A is to the left of B
C is to the right of A
D is to the right of C
E is to the right of D

Which relation holds between C and E?

Procedure & Design
We conducted an eyetracking experiment (using SR
Research Experimental Builder and an EyeLink II system)
in order to measure reaction time and accuracy as well as
the eye-movements during the reasoning tasks. All 32
(determinate and indeterminate) tasks were presented in a
randomized order. Each of the four premises was presented
auditorily. Only the first object was presented during the
whole task (cp. Figure 1). After the last premise two objects
were offered and the relation between them had to be drawn
by pressing one of the answer keys (left or right).
Results
This experiment provided some intriguing results – due to
the space limitation we can present only a subset of the most
remarkable ones.
Determinate models. Our first hypothesis assumes that in
determinate cases the eye-movements of the participants

Figure 3. The determinate task consists of 4 premises (P1 to
P4) with the relations left (LLLL). The number in the
bracket denotes the IAs which should be fixated
successively according to the prediction. The vertical
partitions (e.g. O1, O2, etc.) represent the onset for the
acoustical presentation of the respective object. The
different lines represent the fixation probabilities of the
different interest areas (IA). The standard error of the mean
is represented by the transparent area above/below each line.
Indeterminate models. In contrast to the determinate case
the eye movement pattern changes in indeterminate cases
(cf. Figure 4): For instance, analyzing again the case LLLL,
the indeterminacy occurs during the presentation of the
second premise, where the position of the third object
allows for several positions.

Figure 4. The results for the determinate and indeterminate
problems with 3 IAs (LLLL). The continuous lines represent
the determinate case while the dashed lines represent the

3049

indeterminate case (cp. Fig. 3). In the indeterminate case
this is inserted later (cp. IA 3 in Figure 4) than in
determinate cases. Second, the interest area 5 (with the
visual stimulus) is longer, later and even (together with IA
4) more frequently inspected than in the determinate case.
Third, IA 3 is later fixated but reaches the full attention
during the third premise where Obj 4 (has to be inserted left
to Obj 3). This shows that Obj 3 is inserted in IA 3 (at the
next free position), i.e. the fff-principle is used and not
inbetween IA4 and IA5, which would correspond to the ffprinciple.
The figural effect. The third hypothesis is about the figural
effect – do fixations show a clearer pattern when premises
are presented in a continuous or discontinuous order? Here
we have found strong differences (especially with IA 4) in
the classical case between the direction changes and the
interest areas between LLLL and LLRR (Fig. 5) and LRRR
(Fig. 6).

Figure 5. The results for those determinate tasks with
exactly two direction changes (LLRR). (cp Figure 3).
As expected, the error rates differed significantly between
determinate and indeterminate tasks (Wilcoxon-Test, Z=2.291, p <0,022), but there were no differences between
verification times (Wilcoxon-Test, Z=-0,631, p =0,528).

Figure 6. The results for those determinate tasks with one
direction change in the second premise (LRRR or RLLL)
(cp. Figure 3).

General Discussion
The theoretical assumption of a mental model manipulation
device (the so-called focus) which explains a number of
empirical findings (Ragni, 2008) concerning cognitive
complexity (figural effect, premise order effect, relational
complexity (Goodwin & Johnson-Laird, 2008)) could be
supported by our eye tracking experiment.
Eye-movements of reasoners reflect the mental
representations and operations even if four of the five
objects were not visually presented on the screen and even if
the participants had not been encouraged or instructed to do
so. The data leads to the following five results: first, in
determinate cases without direction change the successive
construction of the model reflected by the corresponding
eye-movements could be identified. Second, if direction
changes were necessary the proportions of the intermediate
IAs would be raised. In comparison to one-way model
constructions more fixations in different IAs are necessary
which increase the time until the target IA would be fixated.
This corresponds to the figural effect (Knauff et al., 1998)
and offers an explanation for the different complexities.
Third, Fig. 4 depicts the latency during the introduction of
an indeterminate premise which could also be found in self
paced reasoning problems (Ragni et al., 2007). Fourth, by
means of the eyetracking experiment we were able to
identify preferred mental models. Although object insertion
principles like first fit are possible the results show a clear
preference for the first free fit principle (corresponding to
IA3, see Fig. 4) in which fixation probability increased
whereas the competing principle (corresponding to IA4, see
Fig. 4) decreased. Fifth, this is remarkable evidence for a
top-down mechanism of eye-movement control in the
absence of reliable bottom-up information (e.g. Spivey &
Geng, 2001).
There are some potential issues with our conclusions. The
visible grid-structure might have supported or even
triggered eye movements. Even if this was the case,
Fixations corresponded beautifully to the mental model
operations (especially the preferred mental model), and the
grid did not provide any information about currently
processed model. Furthermore, Johansson et al. (2006)
demonstrated that there can still be eye movements even
without a grid structure and also in complete darkness. One
might argue that the eye movements were directly triggered
by the named relations. As we cannot rule this out as a
possibility for all determinate cases where the relation right
is used (C is right of B) it does not hold for the first relation
(which is always of the form “A is left (or right) of B”).
During the first premise the eye movements are not in
congruence with the named relation. If reasoning problems
are processed in line with the mental model theory or the
rule-based approach cannot finally be explained with this
study, but the data from different laboratories suggests that
the mental model theory can better explain the results which
were found by means of behavioral, eye tracking and
imaging methods for spatial relational reasoning problems.

3050

Taken together, the data support fundamental assumptions
of MMT, namely that mental models are built
incrementally, successively used and refined for
representing spatial information.

Acknowledgments
This paper has been partially funded by the SFB/TR8
Spatial Cognition in the project iModelSpace. The authors
would like to thank Gerhard Strube, Felix Steffenhagen,
Tobias Sonntag, Andreas Klein. Additionally we thank the
anonymous reviewer of this paper for the helpful comments.

References
Abed, F. (1991). Cultural influences on visual scanning
patterns. Journal of Cross Cultural Psychology, 22, 525–
534.
Ball, L.J., Phillips, P., Wade, C.N., & Quayle, J.D. (2006).
Effects of belief and logic on syllogistic reasoning: Eyemovement evidence for selective processing models.
Experimental Psychology, 53, 77-86.
Braine, M. D. S., & O‘Brien, D. P. (1998). Mental logic.
Mahwah (NJ): Erlbaum.
Byrne, R. M., & Johnson-Laird, P. N. (1989). Spatial
reasoning. Journal of Memory & Language, 28, 564 575.
Carreiras, M., & Santamaria, C. (1997). Reasoning about
relations: Spatial and nonspatial problems. Thinking and
Reasoning, 3, 191-208.
Finke, R. A. (1989). Principles of mental imagery.
Cambridge, MA: MIT Press.
Goodwin, G., & Johnson-Laird, P.N. (2005). Reasoning
about relations. Psychological Review, 112, 468-493.
Hörnig, R., Oberauer, K., & Weidenfeld, A. (2005). Two
principles of premise integration in spatial reasoning.
Memory & Cognition, 33, 131-139.
Jahn, G., Knauff, M., & Johnson-Laird, P. N. (2007).
Preferred mental models in reasoning about spatial
relations. Mem Cognit, 35, 2075-87.
Johansson, R., Holsanova, J. & Holmqvist, K. (2006):
Pictures and spoken descriptions elicit similar eye
movements during mental imagery, both in light and in
complete darkness. Cognitive Science 30:6 (pp. 10531079). Lawrence Erlbaum.
Johnson-Laird, P. N. (2006). How we reason. New York:
Oxford Univ. Press.
Johnson-Laird, P. N. (2001). Mental models and deduction.
Trends Cognitive Science, 5, 434-442.
Johnson-Laird, P. N. & Byrne, R. M. J. (1991). Deduction.
Hove (UK): Erlbaum.
Johnson-Laird, P. N. (1983). Mental models: Towards a
cognitive science of language, inference, and
consciousness. Cambridge: Harvard University Press.
Körner, C., & Gilchrist, I. D. (2004). Eye movements in a
simple spatial reasoning task. Perception, 33, 485-494.
Kosslyn, S.M. (1994). Image and Brain, Cambridge, MA:
MIT Press.

Mani, K. & Johnson-Laird, P. N. (1982). The mental
representation of spatial descriptions. Mem Cognit, 10,
181-187.
Maybery, M. T., Bain, J. D., & Halford, G. S. (1986).
Information-Processing Demands of Transitive Inference.
Journal of Experimental Psychology: Learning, Memory,
and Cognition, 12, 600-13.
Oaksford, M., & Chater, N. (2007). Bayesian rationality:
The probabilistic approach to human reasoning. Oxford:
Oxford University Press.
Potts, G.R., & Scholz, K.W. (1975). The internal
representation of a three-term series problem. Journal of
Verbal Learning and Verbal Behavior, 14, 439–452.
Pylyshyn, Z. (2002). Mental imagery: In search of a theory.
Behavioural and Brain Sciences, 25, 157–238.
Rauh, R., Schlieder, C., & Knauff, M. (1997). Präferierte
mentale Modelle beim räumlich-relationalen Schließen:
Empirie und kognitive Modellierung. Kognitionswissenschaft, 6, 21-34.
Ragni M. (2008). Räumliche Repräsentation, Komplexität
und Deduktion: Eine kognitive Komplexitätstheorie.
Berlin: Akademische Verlagsgesellschaft.
Ragni, M., Fangmeier,T., Webber,L., & Knauff, M. (2007).
Preferred mental models: How and why they are so
important in human reasoning with spatial relations. In C.
Freksa, M. Knauff, B. Krieg-Brückner, B. Nebel, & T.
Barkowsky (Eds.), Spatial Cognition V: Reasoning,
Action, Interaction (pp. 175-190). Berlin: Springer.
Ragni, M. & Knauff, M. (2008). Deductive Spatial
Reasoning: A Computational and Cognitive Perspective.
Künstliche Intelligenz, 1, 13-17.
Ragni, M., Fangmeier,T., Webber, L., & Knauff, M. (2006).
Complexity in Spatial Reasoning. In R. Sun, & N. Miyake
(eds.), Proceedings of the 28th Annual Cognitive Science
Conference (pp. 1986-1991). Mahwah, NJ: Lawrence
Erlbaum Associates.
Ragni, M., Knauff, M., & Nebel, B. (2005). A
Computational Model for Spatial Reasoning with Mental
Models. In B. Bara, B. Barsalou, and M. Bucciarelli
(Eds.), Proceedings of the 27th Annual Cognitive Science
Conference (pp. 1064-70). Mahwah, NJ: Lawrence
Erlbaum Associates.
Rips, L. J. (1994). The psychology of proof: Deductive
reasoning in human thinking. Cambridge, MA, US: The
MIT Press.
Spivey, M. J. & Geng, J. J. (2001). Oculomotor mechanisms
activated by imagery and memory: eye movements to
absent objects. Psychol Res, 65 (4), 235-41.
Van der Henst, J. (2002). Mental model theory versus the
inference rule approach in relational reasoning. Thinking
& Reasoning, 8, 193-203.

3051

