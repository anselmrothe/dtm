UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
When Things Get Worse before they Get Better: Regulatory Fit and Average-Reward
Learning in a Dynamic Decision-Making Environment
Permalink
https://escholarship.org/uc/item/4969455q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Gureckis, Todd
Love, Bradley
Markman, Arthur
et al.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      When Things Get Worse before they Get Better: Regulatory Fit and Average-
                   Reward Learning in a Dynamic Decision-Making Environment
                                    A. Ross Otto, Arthur B. Markman, and Bradley C. Love
                                 Department of Psychology, University of Texas, Austin, TX 78712 USA
                                [rotto@mail.utexas.edu, markman@psy.utexas.edu, love@psy.utexas.edu]
                                                             Todd M. Gureckis
                   Department of Psychology, New York University, 6 Washington Place, New York, NY 10003
                                                            [todd.gureckis@nyu.edu]
                               Abstract                                   learns to maximize his or her payoffs by making choices
                                                                          and experiencing the consequences of those choices (e.g.
   This work explores the influence of motivation on choice               Daw et al., 2006; Bechara, A.R. Damasio, H. Damasio, &
   behavior in a dynamic decision-making environment, where               Anderson, 1994), optimal performance depends on
   the payoffs from each choice depend on one’s recent choice             balancing the demands of gathering and exploiting
   history. Previous research reveals increased levels of
   exploratory choice among participants in a regulatory fit. The
                                                                          information about choice payoffs. Worthy et al.’s (2007)
   present study placed promotion and prevention-focused                  study demonstrated that participants in a regulatory fit, for
   participants in a dynamic environment for which optimal                whom their situational regulatory focus matches the reward
   performance requires that participants sustain a single choice         structure of the task environment, exhibit more exploratory
   strategy in the face of temporary payoff decreases. These              choice strategies than do participants in a regulatory
   participants either gained or lost points with each choice. Our        mismatch, who exhibit more exploitative choice strategies.
   behavioral results and model-based analysis, using the                 Through continued exploration of choices—with the
   average-reward reinforcement learning framework, revealed
   differential levels of reactivity to local changes in payoffs—         consequence of occasionally taking decreases in payoffs—
   specifically, participants in a regulatory fit were less reactive      participants in a regulatory fit display behavior that is
   to local perturbations in payoffs than participants in a               adaptive for the overall long-term pursuit of rewards.
   regulatory mismatch and performed more optimally as a                  Further, their model-based analyses suggested that
   result.                                                                participants in a regulatory fit place less weight on outcomes
   Keywords: Decision making; motivation; reinforcement                   from recent choices than do participants in a regulatory
   learning                                                               mismatch. While this class of tasks is well suited for
                                                                          investigating exploratory versus exploitative choice
                           Introduction                                   behavior, we seek to understand how motivational factors
Motivation is essential to action (e.g., Carver & Scheier,                affect the degree to which recent changes in payoffs drive
1998; Yerkes & Dodson, 1908). Social psychology makes                     choice behavior.
the distinction between two general motivational                             In this report, we examine the effects of regulatory fit in a
orientations (or regulatory foci), a promotion focus and a                two-option, repeated-choice decision making task in which
prevention focus, which accentuate potential gains and                    payoff-maximizing, long-term optimal behavior requires
losses in the environment, respectively (Higgins, 1997).                  that participants persevere with one choice strategy,
                                                                          sustaining temporary decreases in payoffs in order to
Recent research reveals that an interaction occurs between
one’s regulatory focus and the reward structure of the task               maximize long-term gain. Our experiment placed
being performed, affecting peoples’ use of flexible                       participants in a version of the “rising optimum” task,
strategies in a number of tasks. In one study (Worthy,                    previously used to investigate temporal-difference accounts
Maddox, & Markman, 2007) utilizing a two-armed bandit                     of learning (Eagelman, Person, & Berns, 1998; Montague &
task for which optimal choice behavior required exploratory               Berns, 2002) and the problem of temporal credit assignment
choices as opposed to exploitative choices (c.f. Daw et al.,              in human sequential decision-making (Bogacz, McClure, Li,
2006), participants attempting to earn a prize (inducing a                Cohen, & Montague, 2007). Unlike other bandit tasks in
promotion focus) exhibited more optimal choice behavior                   which payoff contingencies remain invariant to participants’
when the task environment had a gains reward structure                    behavior (e.g., Bechara et al., 1994; Daw et al., 2006;
(i.e., participants were maximizing gains of points) than                 Worthy et al., 2007; Yechaim & Busemeyer; 2005), in this
                                                                          task, the state of the task environment changes as a function
when the task environment had a losses reward structure
(i.e. participants were minimizing loss of points). Likewise,             of a participant’s recent choices, which in turn governs the
participants attempting to avoid losing a prize (a prevention             payoffs associated with each action.
focus) performed more optimally when the task involved a                     Consider the two payoff curves depicted in Figure 1,
losses reward structure than when the task involved a gains               which correspond to the possible payoffs for two choices A
reward structure.                                                         and B in Egelman et al.’s (1998) “rising optimum” task. The
   In n-armed bandit tasks where which the decision-maker                 payoff received from a choice depends on the proportion of
                                                                          A choices made over the last 20 trials, represented by the
                                                                     1252

                                                                     standard TD-learning models.
                                                                        A number of studies have explored factors that shape
                                                                     participants’ choice allocations both in the rising optimum
                                                                     task and other similar dynamic environments. Bogacz et al.
                                                                     (2007) demonstrated how optimal choice performance
                                                                     depends on the amount of time that elapses between choices
                                                                     (i.e., inter-choice interval) using an eligibility trace model.
                                                                     A question then, comes to bear in light of prior research:
                                                                     how can motivational factors influence humans’ pursuit of
                                                                     overall long-term rewards in the face of local reward
                                                                     decreases, consequently driving them toward or away from
                                                                     payoff-maximizing choice in the rising optimum task?
                                                                        The present work extends previous research in two ways.
     Figure 1: Payoff functions for two choices as a function        First, we demonstrate that situational regulatory fit (and
         of response allocation. (See text for details).             mismatch) affect the degree to which participants are able to
                                                                     sustain temporary decreases in payoffs in order to maximize
horizontal axis. For example, if the participant makes only B        long-term payoffs. Second, in the framework of
choices for 20 trials in a row—effectively making their              reinforcement learning (RL), we provide a model-based
fractional allocation to the A choice 0—the payoffs from             analysis of choice behavior using a variant of the TD-
choices A and B would be 0.38 and 0.19 respectively. If the          learning algorithm (Sutton & Barto, 1998) known as
participant makes one A choice at this point, his or her             average-reward learning, elucidating our hypothesized
response allocation would change to 0.05, as only 1 out of           differences about participants’ reactivity to local changes in
20 of the last trials were A choices. Consequently, the              payoffs. In short, we hypothesize that the interaction
payoffs for choice A and B would be .36 and 0.2. Thus, the           between one’s motivational state and the reward structure of
payoffs associated with the choices fluctuate with the past          the environment will influence individuals’ ability to sustain
choice behavior of the participant. In this task, optimal long-      globally advantageous choices in the face of local
term choice behavior requires consistent A choices every             perturbations, such that decision-makers in a regulatory fit
trial, as the global optimum is located where the                    will exhibit more optimal, payoff-maximizing response
participant’s fractional choice allocation to choice A is 1          allocations than decision-makers in a regulatory mismatch.
(Montague & Berns, 2002).
   Prior research utilizing the rising optimum task reveals                                  Experiment 1
that participants easily become “stuck” in a local cycle
                                                                     We placed participants in a variant of the Rising Optimum
around the crossing point of the curves where the fractional
                                                                     task, whose payoff schedule (under the gains reward
allocation to choice A is approximately 0.3 (Bogacz et al.,
                                                                     structure) is depicted in Figure 1. Participants in the gains
2007; Montague & Berns, 2002). To illustrate, consider a
                                                                     condition started with 0 points and gained between 0 and 1
participant who makes repeated A choices until they find
                                                                     points with each choice, while participants in the losses
themselves at the crossing point of the two curves (Figure
                                                                     condition started with 0 points and lost between 0 and -1
1). As they continue to make A choices, exceeding an
                                                                     points with each choice. The bonus criteria was positioned
allocation of 0.3, the immediate payoff from choice A will
                                                                     such that participants would need to earn at least 75% of the
decrease, with greater immediate payoffs resulting from
                                                                     total possible points at the end of the experiment—which
choice B. Should they elect to make B choices at this point,
                                                                     required that participants persevere in the face of local
rewards for that option will decrease until the fractional
                                                                     reward decreases as they made repeated A choices.
allocation falls below 0.3 whereupon choice A will yield
                                                                     Consequently, participants whose choice allocations
higher immediate payoffs. This globally suboptimal
                                                                     remained near the “matching” equilibrium would not
response strategy—akin to matching behavior by humans
                                                                     achieve the bonus criterion.
described by Herrnstein (1990)—is predicted by simple
                                                                        Participants in a promotion focus were told that they
temporal-difference (TD) learning models of reinforcement
                                                                     would receive an entry into a drawing for a 1 in 10 chance at
learning (Montague & Berns, 2002; Sutton & Barto, 1998).
                                                                     winning $50 if they achieved the bonus criterion.
An optimal strategy of consistent A choice requires that the
                                                                     Participants in a prevention focus were given an entry into
participant persist in the face of the local decrease in payoffs
                                                                     the drawing and told that they had to achieve the bonus
as they depart the “matching” crossing point and move
                                                                     criterion to avoid losing the entry. As in previous research
towards the global optimum of the A payoff curve. In the
                                                                     (e.g. Shaw & Higgins, 1997), this manipulation was
absence of experimental manipulations, both Montague and
                                                                     designed so that participants in the promotion and
Berns (2002) and Egelman et al. (1998) grouped participants
                                                                     prevention focus conditions were effectively in the same
by their choice strategies (i.e., those who stayed near the
                                                                     objective situation.
crossing point, and those who were able reach near-optimal
                                                                        In light of previous work revealing heightened
allocations), finding that a substantial number of
                                                                     exploratory choice in regulatory fit (Worthy et al., 2007),
participants exhibited choice behavior not anticipated by
                                                                 1253

we hypothesized that participants in a regulatory fit (a                               response history was randomized such that the mean starting
promotion focus with a gains reward structure or a                                     allocation of A choices was 0.5 across all participants. Each
prevention focus with a losses reward structure) will sustain                          trial, participants were presented with two buttons labeled
globally advantageous choice strategies, exhibiting less                               “Choice A” and “Choice B”. The mapping of response
reactivity to local changes in payoffs. In contrast, we                                buttons to choices was counterbalanced across participants.
hypothesized that participants in a regulatory mismatch (a                             The task interface under the gains condition is shown in
prevention focus with a gains reward structure or a                                    Figure 2. Using the mouse, participants clicked one of the
promotion focus with a losses reward structure) would                                  buttons to indicate their choice, and white payoff bar grew
exhibit more reactivity to local changes in payoffs and thus                           (or fell, in the losses condition) vertically to indicate the
                                                                                       amount of points gained (or lost, in the losses condition) on
exhibit less globally optimal response allocations. Table 1
                                                                                       that trial. There was no time limit for making choices.
provides another description of our factorial design and
                                                                                          The payoff each trial was a function of the relative
hypotheses.
                                                                                       fraction of the number of A choices made by the participant
     Table 1: Overview of regulatory focus manipulation.                               over the last 20 trials. Specifically, the payoff for each
                                                                                       option, in the gains condition, with respect to relative
                                                            Reward Structure           fraction of A choices, is depicted in Figure 1. Gains payoffs
                                                    Gains            Losses            were all between 0 and 1. Payoffs in the losses condition
                                                                                       were calculated by subtracting 1 from the gains payoffs,
                             Prevention Promotion
                                                         Fit          Mismatch
          Regulatory Focus
                                                     (decreased       (increased       resulting in all negative payoff values. Cumulative gains (or
                                                     reactivity)      reactivity)      losses) were displayed on the side of the screen, as a bar that
                                                                                       grew (or shrunk, in the losses condition) in relation to the
                                                     Mismatch             Fit          bonus criterion. This bonus criterion was determined by
                                                     (increased       (decreased       calculating the average cumulative payoffs after 250 trials
                                                     reactivity)      reactivity)      with an “A” choice allocation of 0.75. This criterion was
                                                                                       equated across the gains and losses conditions.
                                                                                          After 250 trials, participants were given feedback on
Method                                                                                 whether they had met the bonus criterion or not. If they met
Participants Forty undergraduates from the University of                               the bonus criterion, participants in the promotion focus
Texas community participated in the experiment for course                              condition were given a ticket and told to enter it in the
credit. They were also given the opportunity to win an entry                           drawing, and participants in the prevention focus condition
into a drawing for $50 cash, and were told that no more than                           were informed that they could keep their ticket and enter it
10 participants would be included in each drawing. The two                             in the drawing.
between-subjects independent variables were the situational
regulatory focus (promotion and prevention) and the reward                                                       Results
structure of the task (gains and losses).
                                                                                       Performance Measures
Materials The experiment stimuli and instructions were
displayed on 17-inch LCD monitors. At the start of the                                    As a measure of response optimality, we analyzed the
experiment, participants were informed that they would                                 proportion of trials for which participants made optimal
either earn (promotion condition) or keep (prevention                                  “A” choices. A 2 (regulatory focus) x 2 (reward structure)
condition) a entry into the drawing if they met a bonus                                ANOVA conducted on overall proportion of A choices
criterion. Participants were instructed to make repeated                               collapsed over the course of the experiment revealed a
choices with the goal of maximizing overall, long-term                                 significant interaction (F(1,38)=32.48, p<.001) and no
gains of points (gains condition) or minimizing overall long-                          significant main effects. Among participants in the gains
term losses of points (losses condition).                                              reward structure, participants in a promotion focus
Procedure At the start of the experiment, each participant’s                           (M=0.591, SD= 0.05) made significantly more A responses
                                                                                       than participants in a prevention focus (M= 0.389, SD=
                                                                                       0.03) [t(18)=3.30, p<.01]. For participants in the losses
                                                                                       reward structure, participants in a prevention focus
                                                                                       (M=0.522, SD= 0.03) made significantly more A responses
                                                                                       than participants in a promotion focus (M= 0.330, SD=
                                                                                       0.01) [t(18)=5.97, p<.001].
          Figure 2: Example gains task interface.
                                                                                    1254

                                                                            Figure 3: Average distance, in points, from bonus
                                                                                          criterion by group.
                                                                   Model-Based Analysis
                                                                      Model Definition We implemented a variant of temporal
                                                                   difference (TD)-learning known as average reward learning
                                                                   (Schwartz, 1993; Sutton & Barto, 1998), as theoretical work
                                                                   suggests that average-reward may be a more realistic model
                                                                   of human behavior than discounted-reward models (Daw &
                                                                   Touretzky, 2000; Gureckis & Love, in press).               Our
                                                                   descriptive model affords a direct assessment of a given
                                                                   participant’s reactivity to local perturbations in payoffs in
                                                                   the rising optimum task.
                                                                      Unlike standard TD-learning RL models (e.g. Yechaim &
                                                                   Busemeyer, 2005), which rely only on estimated values of
                                                                   individual actions, average reward learning maintains an
                                                                   estimate of the average reward per time step, ρ, across both
   Figure 3: Proportion optimal (A) choices made, by group,        actions. The value of an action is defined by its estimated
     over the course of the 250 trials, in bins of 50 trials.      value relative to the average reward. Thus, actions that lead
                                                                   to better-than-average rewards (i.e., positive transient
   Figure 3 shows the proportion of “A” choices calculated         differences with respect to ρ) are selected more frequently
over blocks of 25 trials at a time averaged across                 under an exploitative policy. Under average reward
participants. We conducted a 2 (regulatory focus) x 2              learning, the TD error δ is defined as:
(reward structure) x 5 (trial block) ANOVA on number of A                                " = rt +1 # $ # Q(ai ) ,              (1)
choices made across the course of the 5 blocks, revealing a        where rt+1 is the actual experienced reward on that trial and
significant 2-way interaction between regulatory focus and         ρ is the model’s average reward per time step estimate.
reward structure (F(1,38) =17.32, p<.001), as well as a            Each trial, the update made to the estimated transient value,
significant main effect of reward structure (F(1,38) = 4.48,       Q(aj) of each
                                                                               ! action aj is informed by the current TD error δ:
p<.05) and a significant main effect of trial block (F(1,38) =                          Q(a j ) = Q(a j ) + " # e j # $ ,     (2)
7.86, p<.01). All other main effects and interactions failed
to reach significance.                                             where α is a learning rate parameter, 0 ≤ α ≤ 1 and ej is an
   As another measure of optimal performance, we                   eligibility trace for that action (described below). If the
calculated each participant’s final distance from the bonus        chosen option ai had the greater estimated value between the
                                                                               !
criterion, as depicted in Figure 4. A 2 (regulatory focus) x 2     two choices, the average reward estimate ρ is updated
(reward structure) ANOVA on this measure revealed a                according to the current TD error δ:
significant interaction (F(1,38)=20.05, p<.001) and no                                      " = " + (# $ % ) ,                (3)
significant main effects. Among participants in the gains          where β is an average-reward update size parameter, 0 ≤ β ≤
reward structure, participants in a promotion focus (M=            1, that determines how heavily the average-reward estimate
39.96, SD=8.67) came significantly closer to the bonus             weights recent rewards. When β is small, ρ relies on a large
                                                                                  !
criterion than did participants in a prevention focus (M=          historical window and updates very slowly, while if β = 1, ρ
66.53, SD= 4.91) [t(18)=2.66, p<.05]. For participants in the      depends only on rewards from the most recent trials and is
losses reward structure, participants in a prevention focus        updated quickly. Thus, a participant’s readiness to update
(M=52.68, SD= 4.40) ended significantly closer to the              their expectations of average, trial-to-trial payoffs could be
bonus criterion than participants in a promotion focus (M=         encapsulated by their average-reward update parameter.
75.06, SD= 0.72) [t(18)=5.022, p<.001].                               Finally, the model utilizes the “softmax” method of action
                                                                   selection whereby the probability of making choice ai each
                                                                   trial is:
                                                               1255

                                exp[" # Q(ai )]                (4)
                     P(ai ) =   2
                              $ j=1
                                    exp[" # Q(a j )]
where γ is an exploitation parameter (c.f., Daw et al., 2006;
Sutton & Barto, 1998) and Q(ai) is an estimate of the
transient reward
           !       associated with choice ai.
   In order to effectively manage temporal credit
assignment—that is, the rewarding or penalizing of past
choices which occur at variable times prior to the current
reward—the model utilizes accumulating eligibility traces in
a manner similar to Bogacz et al. (2007), as shown in
Equation 2. The eligibility trace ej for each action is
initialized to 0 at the start of the trials and after each action,          Figure 5: Average estimates of average-reward update
both eligibility traces are decayed by a constant term and the                          size parameter β by condition.
eligibility trace for the chosen action ei is incremented:
                               e j = "e j                      (5)     participants’ response dynamics, we also fit a single-
                                                               (6)     parameter baseline model to each participant’s data, which
                              ei = ei + 1
                                                                       assumed a constant probability of making A choices across
where λ is a decay parameter, 0 ≤ λ ≤ 1. Eligibility traces            all trials. The proportions of subjects in each condition for
improve the rate of learning by allowing prediction errors to          whom the average-reward model provided a better fit than
                  !
propagate backwards across multiple trials (Sutton & Barto,            the baseline model (by the Akaike Information Criterion,
1998).           !
                                                                       see Akaike, 1974) are reported in Table 2.
Model Fit Predictions and Results Consider a decision-                    Figure 5 depicts the average update size parameter values
maker, who passes the crossing point from left to right (see           for each condition. A 2 (regulatory focus) x 2 (reward
Figure 1) as they continually make A choices. If the
                                                                       structure) ANOVA conducted on estimated update size
decision-maker readily changes their average-reward
                                                                       parameters revealed a significant interaction (F(1,38)= 6.56,
estimate (i.e., large β) to reflect the dip in payoffs
                                                                       p<.05). On average, participants in regulatory fit had lower
encountered, they will seek the high positive transient
                                                                       estimated values of this parameter. The estimated values for
obtained from choosing B and move back towards the
                                                                       the four model parameters are also summarized in Table 2.
“matching” crossing point, maintaining a suboptimal choice             The estimated values of γ, α, and λ were not of interest in
allocation. However, if the decision-maker does not                    this analysis, and no significant interactions or main effects
significantly change their estimate (i.e., small β) as they            were found across the four conditions.
depart from the crossing point, their average-reward
estimate will remain anchored roughly at the crossing point,            Table 2: Proportion of Subjects for whom Average-Reward
meaning that choice B will not incur as large a transient                  Fit Best, and Average Estimated Parameter Values by
payoff as it would if the average-reward estimate followed                Experimental Condition. Standard Deviations for these
the dip. Consequently, choices A and B will have closer                          Parameter Values are Shown in Parentheses.
estimated transient values, and thus, will be more
equiprobable choices under softmax action selection. Thus,               Condition    Proportion      γ         α        β         λ
in a sense, slower average-reward updating makes                                        Best Fit
exploration tenable from the perspective of the local                   Promotion-                 17.839     0.061    0.010    0.559
                                                                                          0.70
decision-maker.                                                            Gains                   (6.797)   (0.094)  (0.011)  (0.878)
   The examination of group differences with respect to                 Promotion-                 18.478     0.034    0.134    0.540
                                                                                          0.70
                                                                          Losses                   (6.884)   (0.039)  (0.268)  (0.597)
average-reward update size parameter (β) values would
                                                                       Prevention-                 11.547     0.192    0.139    0.514
allow us to evaluate the degree to which participants’                                    0.80
                                                                           Gains                   (7.872)   (0.287)  (0.315)   (1.02)
expectancies of global payoffs fluctuate with changes in               Prevention-                 15.267     0.108    0.004    0.567
local payoffs. As our behavioral results suggested that                                   0.80
                                                                          Losses                   (9.660)   (0.194)  (0.005)  (0.803)
regulatory fit affected participants’ levels of reactivity to
local payoff changes, we hypothesized that participants in a                                     Discussion
regulatory fit would be slower to update their expectations
of average per-trial payoffs, and thus yield lower estimates              This report examines the effects of regulatory fit on
of the average-reward update size parameter than would                 optimal decision-making performance in a dynamic task
participants in a regulatory mismatch. We fit this model to            environment. While previous research has addressed the
the data using a parameter optimization procedure that                 neural correlates of “risky” choice behavior (Montague &
maximized the likelihood of the each individual                        Berns, 2002) and the effects of decaying memory for actions
participant’s estimated parameter values given their choice            between choices (Bogacz et al., 2007) in decision-making
behavior over 250 trials (see Yechaim & Busemeyer (2005)               environments where payoffs vary as a function of recent
for details). To ensure our average-reward model captured              behavior, little work has examined motivational factors that
                                                                       bear on performance in this class of tasks. We have shown
                                                                   1256

that regulatory fit strongly influences how human choice             Daw, N. D., O'Doherty, J. P., Dayan, P., Seymour, B., &
behavior adapts to changing payoff contingencies in the                Dolan, R. J. (2006). Cortical substrates for exploratory
environment. Specifically, we revealed that compatibility              decisions in humans. Nature, 441, 876-879.
between one’s situational regulatory focus and the reward            Daw, N. D., & Touretzky, D. S. (2000). Behavioral
structure of the environment diminishes one’s reactivity to            considerations suggest an average reward TD model of
local changes in payoffs—which, in the rising optimum                  the dopamine system. Neurocomputing, 32-33, 679-684.
task, is necessary for optimal, payoff-maximizing patterns           Eagelman, D. M., Person, C., & Montague, P. R. (1998). A
of choice. It should be noted, however, that optimal choice            Computational Role for Dopamine Delivery in Human
behavior did not depend solely on the reward structure of              Decision-Making. Journal of Cognitive Neuroscience,
the environment (e.g., gains and losses), but rather the               10(5), 623-630.
interaction between situational regulatory focus and task            Gureckis, T.M., & Love, B.C. (in press) Learning in noise:
reward structure.                                                      dynamic decision-making in a variable environment.
   A possible interpretation of differential levels of                 Journal of Mathematical Psychology.
sensitivity to local payoff changes is that continually              Herrnstein, R. J. (1990). Rational choice theory: Necessary
modifying one’s response policy on the basis of local payoff           but not sufficient. American Psychologist, 45(3), 356-367.
information impedes systematic exploration of the decision           Higgins, E. T. (1997). Beyond pleasure and pain. American
space. That is, reactivity to local changes in payoffs                 Psychologist, 52(12), 1280-300.
precludes full, systematic exploration of the decision space.        Maddox, W. T., Baldwin, G. C., & Markman, A. B. (2006).
The notion of systematic exploration is closely related to             A test of the regulatory fit hypothesis in perceptual
“temporal abstraction” in reinforcement-learning as                    classification learning. Memory & Cognition, 34(7),
described by Botvinick et al. (in press) by which agents can           1377-97.
reduce the effective size of the decision space through              Montague, P. R., & Berns, G. S. (2002). Neural economics
structured, multiple-action patterns of exploration. While             and the biological substrates of valuation. Neuron, 36(2),
previous accounts of motivational influences of choice in              265-84.
bandit tasks find that regulatory fit engenders more                 Otto, A.R., Gureckis, T.M., Markman, A.B., & Love, B.C.
stochastic decision-making on the independent, trial-to-trial          Navigating through Abstract Decision Spaces: Evaluating
level (Worthy et al., 2007), participants’ choice behavior in          the Role of State Generalization in a Dynamic Decision-
the present work suggests that regulatory fit also facilitates a       Making Task. Submitted.
more systematic form of exploration which persists over              Schwartz, A. (1993). A reinforcement learning method for
multiple choices.                                                      maximizing undiscounted rewards. In Proceedings of the
   We have shown in this report that motivational factors in           Tenth International Conference on Machine Learning,
the environment can influence individuals’ level of                    298-305. Amherst: Morgan Kaufmann.
reactivity to local payoff changes in a dynamic decision-            Shah, J. & Higgins, E.T. (1997). Expectancy * value effects:
making task, which can in turn impact their willingness to             Regulatory focus as determinant of magnitude and
explore globally optimal choice strategies. These results add          direction. Journal of Personality and Social Psychology,
to the body of findings from the decision-making and                   73 (3), 447–58.
classification literatures (Maddox, Baldwin, & Markman,              Sutton, R., & Barto, A. G. (1998). Reinforcement Learning.
2006; Worthy et al., 2007), which suggest motivation holds             Cambridge, MA: MIT Press.
strong effects for human cognition and behavior.                     Worthy, D. A., Maddox, W. T., & Markman, A. B. (2007).
                                                                       Regulatory fit effects in a choice task. Psychonomic
                         References                                    Bulletin & Review, 14(6), 1125-32.
Akaike, H. (1974). A new look at the statistical model               Yechiam, E., & Busemeyer, J. R. (2005). Comparison of
   identification. IEEE Transaction on Automatic Control               basic assumptions embedded in learning models for
   19(6), 716-723.                                                     experience-based decision making. Psychonomic Bulletin
Bechara, A., Damasio, A. R., Damasio, H., & Anderson, S.               & Review, 12(3), 387-402.
   W. (1994). Insensitivity to future consequences following         Yerkes, R. M., & Dodson, J. D. (1908) The relation of
   damage to human prefrontal cortex. Cognition, 50(1-3), 7-           strength of stimulus to rapidity of habit-formation.
   15.                                                                 Journal of Comparative Neurology and Psychology, 18,
Bogacz, R., McClure, S. M., Li, J., Cohen, J. D., &                    459-482.
   Montague, P. R. (2007). Short-term memory traces for
                                                                                        Acknowledgments
   action bias in human reinforcement learning. Brain
                                                                       This research was supported in part by AFOSR Grant
   Research, 1153, 111-21.
                                                                     FA9550-07-1-0178 and NSF CAREER Grant 349101 to
Botvinick, M. M., Niv, Y., & Barto, A. C. (in press).
                                                                     Bradley C. Love, and NIMH Grant MH077708 and AFOSR
   Hierarchically organized behavior and its neural
                                                                     Grant FA9550-06-1-0204 to W. Todd Maddox and Arthur
   foundations: A reinforcement learning perspective.
                                                                     B. Markman.
   Cognition.
Carver, C.S. & Scheier, M.F. (1998). On the Self-Regulation
   of Behavior. New York: Cambridge University Press.
                                                                 1257

