a different group of participants judged the applicability of              der to obtain enough contextual information about the target
each feature to each exemplar within its category. The rows                words, we crawled a large corpus from the web. For each of
of the matrices corresponded to the category exemplars (vary-              our target words, we collected at least 1,000 documents. The
ing in number from 20 to 33), and the columns of the matri-                resulting corpus contained 768 million word tokens and about
ces corresponded to the selected exemplar features for these               6 million types.
categories (varying in number from 156 for Fish to 382 for
                                                                           Semantic relations Our final knowledge source is an ontol-
Sports). The values in each cell indicate how many persons
                                                                           ogy. English WordNet (Fellbaum, 1998) and its sibling Eu-
(out of four) judged the feature (e.g., <lays eggs>) to be ap-
                                                                           roWordNets (Vossen, 1998) are lexical databases that bring
plicable to the exemplar (e.g., chicken). In addition, two huge
                                                                           together groups of synonyms (so-called synsets) in large net-
domain exemplar × feature matrices were constructed: one
                                                                           works, which show the semantic relationships between in-
for the Animal domain and one for the Artifact domain. For
                                                                           dividual words. Possible relationships are for instance hy-
both domains, all exemplars and all features of their mem-
                                                                           pernymy (motor vehicle – <car>) and hyponymy (taxi –
ber categories were aggregated. The Animal matrix contained
                                                                           <car>). Semantic distances obtained on the basis of Word-
129 rows corresponding to animal names that belonged to the
                                                                           Net have been shown to explain human similarity judgments
categories Birds, Fish, Insects, Mammals, and Reptiles. The
                                                                           independently of associative strength, lexical co-occurrence
columns corresponded to 765 exemplar features of Animals.
                                                                           or featural similarity (Maki, McKinley, & Thompson, 2004).
The Artifact matrix contained 166 rows corresponding to ob-
ject names that belong to the categories Clothing, Kitchen                 Vector models
Utensils, Musical Instruments, Tools, Vehicles, and Weapons.
                                                                           The first three types of semantic knowledge were subse-
The columns of this matrix represented 1,295 exemplar fea-
                                                                           quently transformed to a word-by-word matrix. The rows
tures of Artifacts.
                                                                           of this matrix represent the target words; the columns cor-
Associations Association measures have been successful in                  respond to the words that describe the targets. For the feature
the prediction of many semantic memory tasks such the dis-                 sets, these are the judged features for the Animal or Artifacts
tance effects in free recall and cued recall (Steyvers et al.,             matrices (used in Experiment 1) or the smaller category ma-
2004). We used a set of word associations collected between                trices (used in Experiment 2). For the set of associations, the
2003 and 2006 (De Deyne & Storms, 2008). The experiment                    columns are all associations given by the participants. For the
asked participants to give three different associations for each           lexical contexts, finally, they are the context words within a
cue. In this way, 381,909 responses were collected for a total             window of four words to the left and right of the target word.
of 1,424 cues. This amounts to at least 360 association re-                Because of its extremely large size, we reduced this last con-
sponses to a particular cue. In contrast to other word associa-            text matrix by removing rows with less than 10 elements, and
tion databases (e.g., Nelson, McEvoy, & Schreiber, 2004), we               columns with less than 2. Next, the frequencies of the as-
collected three associations from each participant in a contin-            sociations, feature and context words were weighted. A first
uous task, instead of just one. This has two advantages. First,            weighting function was performed by dividing the number of
we can collect weak(er) associations, which is especially im-              times the column word co-occurs with the target by its total
portant for cues with very dominant associations (e.g., blood              frequency in the matrix (Inverse Vector Frequency or IVF).
and <red>). Second, the resulting representations are denser               We also considered a second weighting function that is of-
and therefore more suited for a distributional approach of                 ten applied in linguistic studies for measuring the association
meaning.                                                                   between the co-occurrence of two words. This approach con-
                                                                           siders the fact that two words can co-occur by chance. We fol-
Lexical Context A third source of knowledge that contains
                                                                           lowed a proposal by Church, Gale, Hanks, and Hindle (1991)
information about the meaning of a word is the contexts in
                                                                           that uses t-scores to measure if two words are collocates (i.e.
which these words are used. This context can be defined as
                                                                           it captures the extent to which the occurrence of one word
the documents in which a word occurs (LSA) (Landauer &
                                                                           depends on the other).2 . In contrast to the vector models, the
Dumais, 1997), the words in a context window around the tar-
                                                                           EuroWordNet ontology contains representations in a graph
get word (Lund & Burgess, 1996), or the syntactic relations in
                                                                           and can be derived more directly. More details follow in the
which a word takes part (Pereira, Tishby, & Lee, 1993). Such
                                                                           next section.
contextual information has been used in the modelling of se-
mantic priming (Burgess et al., 1998; Landauer & Dumais,                   Hypotheses
1997) and semantic dyslexia (Buchanan, Burgess, & Lund,
1996), etc. Here we will use a so-called word-based approach               Given our knowledge about these four sources of informa-
where the context features are the four words to the left and              tion, we can make some predictions as to which ones should
right of the target in each of its contexts in a corpus1 . In or-          best be able to reflect human similarity judgments. We ex-
                                                                           pect the most valuable information about semantic similarity
    1 Pilot studies showed that the relatively small context windows
that we use (four words to either side of the target) result in better     rather than their mutual co-occurrence.
models of semantic similarity than approaches based on much larger             2 We also tried other weighting schemes, but found these func-
windows, or on the distribution of words in paragraphs or documents        tions to perform consistently better.
                                                                       1835

to come from the features as well as the expert ontology. Fea-       formed, resulting in a set A and B each consisting of 6 × 5
tures should give us most insight about the similarities and         Artifacts and 5 × 5 Animals. Each participant rated Animal
differences between concepts that belong to the same cat-            pairs or Artifact pairs of either set A or B in multiple sessions,
egory (due to the procedure), while the taxonomical struc-           with the only restriction that no replication sets were allowed
ture of EuroWordNet and the subclasses of a category should          to be rated the same day. All pairwise combinations (435 for
again correlate very well with similarities between more ab-         the Artifacts, 300 for Animals) were presented in a random
stract subsets of concepts (due to the type of relationships         order on a computer screen. The word order in the pairs was
commonly encoded in EuroWordNet).                                    randomized. Participants were asked to enter a number be-
   Word associations and lexical co-occurrence information           tween ‘1’ (for totally dissimilar) and ‘20’ (for totally similar).
collected from corpora will probably contain information             In case one or two words of an exemplar pair were unknown,
about the more general type of semantic relatedness than se-         they had to enter ‘-1’. They completed the task for a single
mantic similarity due to the large variety in semantic relation-     domain in less than 1 hour.
ships in these sources and the lack of syntax and category-          Experimental Results All exemplar pairs in set A and B
context compared to the concept features.                            were judged by at least 12 and by at most 18 different partici-
   Next, we evaluated these models in two experiments. The           pants and were known by the majority of the participants. We
first experiment considers semantic similarity between con-          removed the data from one participant in set B of the Animals
cepts spanning broad domains (Artifacts or Animals). This            as these data correlated less than .45 with the average ratings.
experiment corresponds closely to previous studies where the         The resulting ratings were all very reliable with Spearman
stimuli cover a wide range of semantic relationships. For ex-        Brown split-half correlations ranging from .94 to .97.
ample, LSA (Landauer & Dumais, 1997) correlated r = .72              Model Results Similarity coefficients were obtained for the
with similarity judgments collected by Rubenstein and Good-          Feature, Association and Context model by calculating the
enough (1965) and r = .64 with judgments reported by Miller          cosine between the vectors of each concept pair. Tradition-
and Charles (1991). These findings indicate we can expect            ally, graph-theoretic measures are used to derive similarity
LSA-like models such as our context model to perform rea-            from the EuroWordNet model. The measures we used for this
sonably well in this task                                            model was the Inverse Path Length measure, which simply
   In the second study, we investigate how well these mod-           takes the inverse of the number of steps between two words
els approach detailed semantic representations that measure          in the taxonomy (see Budanitsky & Hirst, 2006, for an exten-
judgments of similarity within a category such as Fruit, In-         sive discussion). Three items (swan and seagull, and black-
sects, Musical Instruments or Professions. While the simi-           bird) had to be removed from the Animals because they did
larity structure in these categories has received considerable       not occur literally in the EuroWordNet ontology.
attention in the categorization literature, hardly anything is
known about the ability of distributional approaches based on           First we tested vector spaced models where no weighting
context words or associations to capture these fine-grained          procedure was applied to the term frequencies in the word ×
representations.                                                     feature, word × association or word × context matrices. Av-
                                                                     eraged over both replications, the best results were found for
          Experiment 1: Domain Similarity                            the Feature model (r = .82) followed by the Context model
                                                                     (r = .65), the Association model (r = .60) and EuroWord-
Method
                                                                     Net (r = .51). However, we found that the unweighted term
Participants In total 30 persons, 26 females and 4 males             frequencies might present too harsh a test on these models
(average age 20 years) participated in the experiment. They          since the models differ in size and the way their features (as-
were mainly students at the University of Leuven, and were           sociations or context words) are distributed. In order to take
paid the equivalent of $10/h.                                        the relative importance of frequencies into account, we ap-
Materials and Procedure The stimuli consisted of mem-                plied the weighting schemes discussed above. Here we report
bers belonging to 6 different Artifact categories (Clothing,         the t-score weighted results for the Association and Context
Kitchen Utensils, Musical Instruments, Tools, Vehicles and           model and the IVF results for the Feature model3 .
Weapons) and 5 Animal categories (Birds, Fish, Insects,                 Table 1 shows the correlations of the weighted similar-
Mammals, and Reptiles) described in De Deyne et al. (2008).          ity coefficients from the models with the human similarity
Since it is not feasible to present all pairwise combinations        ratings for set A and B of the Artifact and Animal domain.
of all exemplars of these categories we selected 5 exemplars         The results are comparable for both replications. The Feature
from each of the Artifacts and Animals categories that cover a       model gives a near perfect account irrespective of the domain.
wide range of typicality. This way some members were cen-            The same holds for the Association model. While the Con-
tral to the category representation (e.g., sparrow is a typical
bird and thus a central member) while others were not (e.g.
                                                                         3 The choice of a weighting scheme did not change the ranking
bat is an atypical member in the periphery of Mammals, and
                                                                     of the various models in both Experiment 1 and 2, except for word
closely related to Birds). To increase the generalizability of       associations, where the unweighted term frequencies performed sys-
our results, two replications of the above procedure were per-       tematically worse than any other scheme.
                                                                 1836

text model gives a good account for the Artifacts, it is less      mance. Looking at the domain averages the best results were
accurate in predicting the judgments in the Animal domain.         found for Artifacts (.46) and Activities (.37), which were still
Finally, the EuroWordNet model (WN-D) performs inconsis-           below the values found for any of the other models.
tently for Artifacts and scores poorly in the Animal domain.          Table 2 shows that at a category level, the models are not
                                                                   always consistently ranked. Consider the category Insects for
                                                                   example. Here the Association model gives a better account
Table 1: Correlations of the human similarity judgments of         than the Feature model. In summary, it is easy to see that the
Experiment 1 and the different model similarity coefficients.      variability across categories indicates that different semantic
     Domain      Set    Feat.   Asso.    Context    WN-D           models capture different semantic content.
     Animals       A   .91**    .87**       .53**    .34**            The averages for the four domains also indicate that the
                   B   .87**    .82**       .55**    .16*          correlations for some domains differ systematically compared
     Artifacts     A   .92**    .76**       .81**    .63**         to others. For instance, in the ontology model all Artifact cat-
                   B   .85**    .75**       .80**    .39**         egories except Clothing receive relatively high correlations.
Note: * p < .05, ** p < .01 (two-tailed)                           The Natural Foods and Animals, by contrast, correspond less
                                                                   with human ratings. The reason for this result becomes clear
                                                                   when we look at the structure of the EuroWordNet categories
     Experiment 2: Intra-Category Similarity                       in more detail. Typically, Artifacts display a rather fine-
                                                                   grained structure. With the Natural Kinds (Food and Ani-
Method                                                             mals), this detail is rather exceptional: often, all exemplars of
Participants A total of 97 participants, 64 females and 33         a category are listed as immediate daughters of the category
males (average age 21 years), mainly students at the Univer-       name. On the basis of a tree structure, it is then impossible
sity of Leuven, participated in this task. Each was paid the       to arrive at reliable similarity figures. In general, the results
equivalent of $10/h.                                               for Natural Kinds are not as good as those from Artifacts and
                                                                   Activities for all models but the Feature model.
Materials and Procedure We collected within-category                  How can we explain these domain differences? One expla-
similarity ratings for 15 categories with 420 exemplars in         nation is that with concrete concepts, humans employ a holis-
total. The procedure was identical to Experiment 1. Each           tic – mostly perceptual – comparison strategy, which might
participant rated all pairwise combinations of the exemplars       be underestimated by our models. While the Feature model
of at least one and at most seven categories, with the only        contains many perceptual features, it is not always clear how
restriction that the exemplar pairs of the contrast categories     to weight them independent from the task at hand. For in-
Fruit and Vegetables were never rated by the same partici-         stance, in the case of Insects, the distinction between flying
pant. They completed the task in between 1 and 5 hours.            insects and non-flying insects has a profound influence on
They never participated more than 1 hour in a single session       the similarity ratings, even though this feature constitutes a
and always took a break of at least 2 hours before continuing.     characteristic rather than defining feature (Smith, Shoben, &
Experimental Results All exemplar pairs of the 15 cate-            Rips, 1974) and is just one of the 214 features of our Insects
gories were rated by at least 15 and by at most 22 different       matrix. According to this account, perceptual information is
participants. We removed 5% of the participants, as they cor-      important for Natural Kinds such as Fruit, Vegetables and An-
related less than .45 with the average ratings. The resulting      imals, while other types of information such as functional and
ratings were all very reliable with Spearman Brown split-half      thematic information is a larger determinant of Artifacts and
correlations ranging from .85 to .96. Prior to further anal-       Activities.
ysis, three items were removed because they were unknown
to most participants (Komodo dragon, iguanodon and span-                                     Discussion
ner). In addition we also removed three concepts that were         We have investigated whether several sources of semantic in-
compounds separated by a blank (e.g., red cabbage) and five        formation — features, word associations, co-occurrence in
words with ambiguous meanings (e.g., golf, which means             context and an ontology structure — are able to model hu-
both ’wave’ and ’sport’ in Dutch).                                 man similarity ratings. Our results suggest that they all have
Model Results The same weighting and similarity func-              the potential to do so, but that this potential is not always fully
tions were used as those described in Experiment 1. Table 2        realized. As expected, most models do well when similarity
gives the correlations of the similarity coefficients from the     is considered covering a wide conceptual space as was the
models with the human similarity ratings for each category.        case in Experiment 1.
In addition we also computed the correlations for the do-             However, this was not the case when only a small region
mains.                                                             of this concept space is considered. While a Feature model
   The Feature Model gives the best agreement with the             gave a good account for the fine-grained intra-category simi-
judged similarities. Next are the Association and the Con-         larity judgments, associations and context co-occurrences led
text model. The WN-D ontology model had the worst perfor-          to slightly less positive numbers. Furthermore, the ontology
                                                               1837

Table 2: Correlations of the model predicted and human similarity judgments of Experiment 2 for n pairs and 15 different
categories.
                        Domain            Category                  n    Feat.     Asso.    Context     WN-D
                        Natural Food      Fruit                   406    .75**     .67**    .22**       .07
                                          Vegetables              325    .72**     .47**    .31**       .29**
                                                                  731    .74**     .59**    .26**       .16**
                        Animals           Birds                   300    .77**     .53**    .49**       -.01
                                          Fish                    120    .79**     .77**    .42**       .44**
                                          Insects                 253    .55**     .71**    .28**       .08
                                          Mammals                 351    .77**     .53**    .35**       .11*
                                          Reptiles                 78    .89**     .72**    .26*        .49**
                                                                 1102    .73**     .61**    .37**       .13**
                        Artifacts         Clothing                378    .72**     .57**    .34**       .25**
                                          Kitchen Utensils        465    .78**     .53**    .50**       .46**
                                          Musical Instruments     276    .81**     .58**    .46**       .68**
                                          Tools                   325    .70**     .56**    .45**       .50**
                                          Vehicles                351    .83**     .76**    .63**       .49**
                                          Weapons                 153    .85**     .71**    .69**       .39**
                                                                 1948    .77**     .60**    .50**       .46**
                        Activities        Professions            3577    .80**     .63**    .63**       .32**
                                          Sports                  105    .86**     .82**    .63**       .53**
                                                                  455    .83**     .70**    .64**       .37**
Note: * p < .05, ** p < .01 (two-tailed)
model proved least appropriate. To gain a better insight in         visually very similar but can hardly be considered synonyms.
these differences, we looked at extreme cases in the scat-             All these models have their idiosyncrasies and limitations.
ter plots and influence statistics of the human and model-          The weakness of the ontology model is least consistent with
based similarities. This investigation revealed two interesting     our initial hypothesis, which claimed that the structure of the
patterns. First, the Association and Context models capture         taxonomy should basically reflect the similarities and dissim-
thematic relation semantics, which might be less important          ilarities between concepts. However, the main problem of this
in the similarity judgments. For example when plotting the          approach appears to be the low coverage of Dutch EuroWord-
association coefficients against human judgments, pairs like        Net, and the lack of detail in some of its categories. This is
judge-lawyer, car-bicycle, scarf-mittens or furnace-apron are       particularly apparent with the Natural Kinds categories. The
considered more similar according to the model than to hu-          Association and Context models suffer from different prob-
man judges. The influence of such thematic or situational           lems. On the one hand, their low performance as compared
properties is also present in the Context model, where it pre-      to the Features model in Experiment 2 underpins our initial
dicts higher similarities for pairs like bow-sword, lama-horse,     hypothesis that both of these approaches (and the Associa-
or bus-taxi. Importantly, such situational information is not       tion model in particular) generally capture a broader type of
as strongly present in the deviations for the feature similar-      semantic relatedness than just similarity. On the other hand,
ities. In this respect, the similarity rating task might have       the performance of the Context model fully depends on the
been somewhat artificial compared to everyday processing            corpus that was used to collect the co-occurrence frequencies.
of meaning where situational properties are useful cues for         While more data could lead to higher correlation figures, we
understanding our environment. It can be expected that a            also expect that some aspects of a word’s semantics (like its
different task such as judgments of semantic relatedness or         visual characteristics) are expressed in text only rarely.
priming studies would converge better with the Association
and Context models and less with the Feature account. Sec-             Despite some of these practical limitations, our results
ond, we already mentioned that perceptual similarity could          highlight the importance of the correct feature selection in
also present an important bias for the similarity judgments         modelling of semantic knowledge. The information con-
for Natural Kinds. This can be illustrated by the high simi-        tained in the Feature model pertains mostly to properties of
larity judgment for cucumber-zucchini. These vegetables are         the entities itself, such as their appearance or function. There
                                                                    are of course a number of fields where the success of vector-
                                                                1838

based models does not directly hinge on these features only.          Lund, K., & Burgess, C. (1996). Producing high-dimensional
In priming studies, for instance, the priming effect may be a           semantic spaces from lexical co-occurrence. Behavior Re-
result from a number of semantic relations, and could there-            search Methods, Instruments, and Computers, 28, 203-208.
fore be modelled with several types of semantic knowledge             Maki, W. S., & Buchanan, E. (2008). Latent structure in mea-
other than concept features. However, when the goal is to               sures of associative, semantic, and thematic knowledge.
model one specific type of semantic relationship, the choice            Psychonomic Bulletin & Review, 15(3), 598-603.
of features appears to be crucial. It is also here that we an-        Maki, W. S., McKinley, L. N., & Thompson, A. G. (2004).
ticipate the biggest improvements for our models in particu-            Semantic distance norms computed from an electronic dic-
lar. For example, we expect a context-based model that starts           tionary (WordNet). Behavior Research Methods, Instru-
from syntactic relations instead of lexical co-occurrence to            ments, and Computers, 36, 421-431.
give similarity judgments that correlate better with human            Markman, A. B., & Gentner, D. (1993). Structural alignment
similarity ratings. After all, the syntactic relations in which a       during similarity comparisons. Cognitive Psychology, 25,
word takes part are linked directly to the features of its con-         431-467.
cept, which in their turn influence the similarity between two        McRae, K., Cree, G. S., Seidenberg, M. S., & McNorgan, C.
concepts.                                                               (2005). Semantic feature production norms for a large set
                                                                        of living and nonliving things. Behavior Research Meth-
                     Acknowledgments                                    ods, 37, 547–559.
This work was supported by Grant 3H080198 of the Leuven               Miller, G. A., & Charles, W. G. (1991). Contextual correlates
University Council and Grant G.0513.08 of the Belgian Na-               of semantic similarity. Language and Cognitive Processes,
tional Science Foundation amended to the third author and a             6, 1-28.
research grant funded by the Research Foundation - Flanders           Nelson, D. L., McEvoy, C. L., & Dennis, S. (2000). What
(FWO) to the first and second author.                                   is free association and what does it measure? Memory &
                                                                        Cognition, 28, 887-899.
                          References                                  Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (2004).
                                                                        The University of South Florida free association, rhyme,
Buchanan, L., Burgess, C., & Lund, K. (1996). Overcrowd-                and word fragment norms. Behavior Research Methods,
   ing in semantic neighborhoods: Modeling deep dyslexia.               Instruments, and Computers, 36, 402–407.
   Brain and Cognition, 32, 111-114.                                  Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clus-
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-                 tering of English words. In 31st annual meeting of the acl
   based measures of lexical semantic relatedness. Computa-             (p. 183-190).
   tional Linguistics, 35, 13-47.                                     Rubenstein, H., & Goodenough, J. B. (1965). Contextual
Burgess, C., Livesay, K., & Lund, K. (1998). Explorations               correlates of synonymy. Communications of the ACM, 8,
   in context space: words, sentences, discourse. Discourse             627–633.
   Processes, 25, 211-257.                                            Sahlgren, M. (2006). The Word-Space Model. Using dis-
Church, K., Gale, W., Hanks, P., & Hindle, D. (1991). Using             tributional analysis to represent syntagmatic and paradig-
   statistics in lexical analysis. In U. Zernik (Ed.), Lexical          matic relations between words in high-dimensional vector
   acquisition: Exploiting on-line resources to build a lexicon         spaces. Unpublished doctoral dissertation, University of
   (p. 115-164). Hillsdale, NJ: Lawrence Erlbaum Associates.            Stockholm.
De Deyne, S., & Storms, G. (2008). Word associations:                 Sloman, S. A., Love, B. C., & Ahn, W. K. (1998). Feature
   Norms for 1,424 Dutch words in a continuous task. Be-                centrality and conceptual coherence. Cognitive Science, 22,
   havior Research Methods, 40, 198-205.                                189-228.
De Deyne, S., Verheyen, S., Ameel, E., Vanpaemel, W., Dry,            Smith, E. E., Shoben, E. J., & Rips, L. J. (1974). Structure
   M., Voorspoels, W., et al. (2008). Exemplar by feature               and process in semantic memory: A featural model for se-
   applicability matrices and other Dutch normative data for            mantic decisions. Psychological Review, 81, 214-241.
   semantic concepts. Behavior Research Methods, 40, 1030-            Steyvers, M., Shiffrin, R. M., & Nelson, D. L. (2004). Ex-
   1048.                                                                perimental Cognitive Psychology and its Applications. In
Fellbaum, C. (1998). WordNet: An electronic lexical                     A. Healy (Ed.), (chap. Word association Spaces for Pre-
   Database. Cambridge, MA: MIT Press. (Available at                    dicting Semantic Similarity Effects in Episodic Memory.).
   http://www.cogsci.princeton.edu/ wn)                                 Washington, DC: American Psychological Association.
Hampton, J. A. (1997). Associative and similarity-based pro-          Storms, G., De Boeck, P., & Ruts, W. (2000). Prototype and
   cesses in categorization decisions. Memory & Cognition,              exemplar based information in natural language categories.
   25, 625-640.                                                         Journal of Memory and Language, 42, 51-73.
Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s        Vossen, P. (1998). EuroWordNet: A multilingual database
   Problem: The latent semantic analysis theory of acquisi-             with lexical semantic networks for european languages.
   tion, induction and representation of knowledge. Psycho-             Dordrecht: Kluwer.
   logical Review, 104, 211-240.
                                                                  1839

