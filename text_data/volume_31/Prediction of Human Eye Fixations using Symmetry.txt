UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Prediction of Human Eye Fixations using Symmetry
Permalink
https://escholarship.org/uc/item/9tq6j18t
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Koostra, Gert
Schomaker, Lambert R.B.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            Prediction of Human Eye Fixations using Symmetry
                                               Gert Kootstra (G.Kootstra@ai.rug.nl)
                                     Lambert R.B. Schomaker (L.Schomaker@ai.rug.nl)
                                             Artificial Intelligence, University of Groningen
                                          Nijenborgh 9, 9747 AG Groningen, the Netherlands
                               Abstract
   Humans are very sensitive to symmetry in visual patterns.
   Reaction time experiments show that symmetry is detected
   and recognized very rapidly. This suggests that symmetry is a
   highly salient feature. Existing computational models of sali-
   ency, however, have mainly focused on contrast as a measure
   of saliency. In this paper, we discuss local symmetry as a
   measure of saliency. We propose a number of symmetry
   models and perform an eye-tracking study with human par-
   ticipants viewing photographic images to test the models. The
   performance of our symmetry models is compared with the
   contrast-saliency model of Itti, Koch and Niebur (1998). The
   results show that the symmetry models better match the hu-
   man data than the contrast model, which indicates that sym-
   metry can be regarded as a salient feature.
   Keywords: visual perception; overt visual attention; symme-               Figure 1: Examples of images containing symmetrical
   try; saliency; saliency models
                                                                          forms. The second column shows the human fixation den-
                                                                           sity maps, the third shows the contrast-saliency maps, and
                          Introduction                                   the last shows the symmetry-saliency maps. The preference
While viewing the world, humans constantly make eye                        of humans to fixate on the center of symmetry is correctly
movements to fixate on interesting parts of the visual field.             reproduced by the symmetry model. The bright regions are
In this way, the relevant information can be viewed with                    the parts of the maps above 50% of its maximum value.
high resolution, while irrelevant information can be ignored.
The process of focusing attention by making an eye move-                 formance of Itti and Koch’s model. Privitera and Stark
ment is called overt visual attention. The eye movements                 (2000) investigated a set of simpler saliency operators in-
are influenced both top-down, emerging for instance from                 cluding other features than contrast. The operators were also
past experiences, personal interests, and the task, as well as           found to predict human fixation points to some extent. It
bottom-up, purely from the stimulus. In the current research,            must be noted that Privitera and Stark also used a simple
we are interested in the stimulus-driven influences on eye               symmetry operator that weakly resembled the human data.
movements, without semantic control. What in the stimulus                   However, even though most existing models are based on
causes our eyes to move to a certain location? What causes               contrast, Figure 1 shows that humans have a clear prefer-
certain parts of the visual field to catch attention? More spe-          ence to fixate on the center of symmetry for some images
cifically, we investigate how well the locations of eye fixa-            (second column). This can not be explained using contrast.
tions can be predicted on the basis of local symmetry.                   Instead, the contrast model gives high response at the
   Most current models of visual attention base their predic-            boundaries, where the flowers contrast with the background
tions on contrast in the image. The model of Itti and Koch.              (third column). This apparent deficiency in current vision
(Itti & Koch, 2001; Itti, Koch, & Niebur, 1998), for in-                 models was the motivation for the present study.
stance, is based on contrasts in luminance, color and orien-                The human response, shown in Figure 1, suggests that
tation. Their saliency model is an implementation of the                 humans pay attention to symmetrical forms. In this paper we
feature integration theory of human visual search (Treisman              will investigate if eye fixations can be predicted on the basis
& Gelade, 1980). The saliency model of Itti and Koch has                 of local symmetry. As can be appreciated in the last column
been compared to human eye fixations by Parkhurst, Law                   in Figure 1, our symmetry saliency model does predict fixa-
and Niebur (2002). They tested the model on photographic                 tions in accordance with the human data. We will show that
images and showed that it matches the human fixation                     this is not only valid for images that contain explicit sym-
points significantly better than expected by chance. Ouer-               metrical forms like those in Figure 1, but more generally in
hani, von Wartburg, Hügli and Müri (2004) also found a                   complex photographic images with different types of con-
positive correlation between the model and human fixations.              tent.
   Also other saliency models, like the model of Le Meur,                   Symmetry is a prominent visual feature in our daily envi-
Le Callet, Barba and Thoreau (2006) are based on contrast                ronments. Many living organisms, for instance, have clear
calculations. They found a positive correlation between their            left-right symmetry in their bodies. This symmetry is even
model and human data that was slightly higher than the per-
                                                                      56

   Figure 2: The basis of our symmetry models. (a) gives three examples of pixel pairs whose gradients are compared by the
    symmetry operator. The geometry of the contribution of a pixel pair is shown in (b) and further explained in the text. (c)
                                 gives an overview of the multi scale setup of the symmetry models.
an indication of the fitness of the individual. For instance,        (Itti et al., 1998) is also compared with the human data. In
manipulated images of faces with enhanced symmetry are               this section, the developed symmetry-saliency models are
judged more attractive than the original faces (Grammer &            explained, followed by a description of the eye-tracking
Thornhill, 1994). Also in art, symmetry is usually preferred         studies and the method to compare the models with the hu-
over asymmetry (Tyler, 2000). According to Gestalt psy-              man data.
chologists symmetry improves the figural goodness
(Palmer, 1991). Because symmetry is so prominent, it is              Symmetry operators
likely that it plays a role in the visual system.                    Our models are based on the isotropic symmetry and radial
   It is also known that humans are sensitive to symmetry.           symmetry operator of Reisfeld, Wolfson and Yeshurun
Symmetrical patterns are detected very rapidly, especially           (1995), and on the color symmetry operator of Heidemann
when having multiple axes of symmetry (Palmer & He-                  (2004). We extended the operators to multi-scale symmetry-
menway, 1978). Also recognition performances increase                saliency models.
with symmetrical patterns (Royer, 1981). The increase in                The isotropic symmetry operator (Reisfeld et al., 1995)
performance might be explained by the intrinsic redundancy           calculates the amount of symmetry at a given position, x,
present in symmetrical forms, which gives rise to simpler            based upon gradients of the intensity in surrounding pixels.
representations (Barlow & Reeves, 1979). Humans further-             This is done by comparing pairs of pixels i and j at positions
more have the tendency to interpret symmetrical regions as           xi and xj, where x = ( xi + x j ) / 2 (see Figure 2a). Every pixel
figure, and asymmetrical regions as background (Driver,
Baylis, & Rafal, 1992).                                              pair contributes to the local symmetry by
   Symmetry also influences eye movements. Fixations on                                  c(i, j ) = d (i, j , σ ) ⋅ p(i, j ) ⋅ mi ⋅ m j       (1)
symmetrical forms are concentrated at the center (Richards           Where mi is the magnitude of the gradient at point i,
& Kaufman, 1969) of the form, or at the crossing points of            d (i, j , σ ) is a Gaussian weighting function on the distance
the symmetry axes (Kaufman & Richards, 1969). For im-                between the two pixels with standard deviation σ, and the
ages with a single symmetry axis, the fixations are concen-
                                                                     symmetry measurement p(i, j ) is calculated by
trated along this axis, whereas the fixations are more spread
out for non-symmetrical images (Locher & Nodine, 1987).                            p(i, j ) = (1 − cos(γ i + γ j ) ) ⋅ (1 − cos(γ i − γ j ) ) (2)
These studies, however, use simple stimuli with only one
                                                                     Where γ i = θi − α is the angle between the direction of the
symmetrical pattern presented.
   In this paper, we investigate the role of local symmetry on       gradient angle θi and the angle α of the line between pi
overt visual attention in complex photographic scenes, and           and pi (see Figure 2b). The first term in equation (2) has a
compare it to the role of contrast. We propose extended              maximum value when γ i + γ j = π , which is true for gradi-
symmetry-saliency models, and compare their performances             ents that are mirror symmetric with respect to p. Using only
with the contrast saliency model of Itti and Koch, hence-            this term would result in high values for points on a straight
forth referred to as the contrast model. We show that the            edge, which are not considered symmetrical. To avoid this
symmetry models correspondd significantly better with the            problem, the second term demotes pixel pairs with similar
human eye fixations.                                                 gradient orientation. In this way, the contributions of all
                                                                     pixel pairs, Γ( p) , within the radius, r, are summed up to
                            Methods
                                                                     give the isotropic symmetry value for p:
To investigate the role of symmetry in visual attention, we
                                                                                            Miso ( x, y ) = ∑ ( i , j )∈Γ ( p ) c(i, j )      (3)
developed a number of symmetry-saliency models and com-
pared them with human eye tracking data. To establish a                 To make the symmetry operator more sensitive to sym-
point of reference, the contrast-saliency model of Itti et al.       metrical patterns with multiple axes of symmetry, Reisfeld
                                                                  57

           Figure 3: Examples of images used, one for each category: flowers, animals, street scenes, buildings and nature.
et al. (1995) developed the radial symmetry operator as an                          lar symmetry values. Finally, the feature maps are combined
extension of the isotropic symmetry operator. First, the ori-                       into a symmetry saliency map by resizing all feature maps
entations of the contribution of the pixel pairs are calculated                     to the same size and summing them.
by ϕ (i, j ) = (θ i + θ j ) / 2 . Next, the symmetry orientation is                                               4
                                                                                                            S = ⊕ N (Ms )                       (7)
                                                                                                                s =0
determined as φ ( p) = ϕ (i, j ) for (i, j ) that give the highest
                                                                                    Where ⊕ is the summation operator that resizes all parts to
contribution c(i, j ) . This value is then used to promote the
                                                                                    the same size, and Ms is the symmetry feature map at scale
contributions of pixels pairs with dissimilar orientations:
                                                                                    s. This procedure results in three symmetry saliency maps:
            Mrad = ∑ (i , j )∈Γ ( p ) c(i, j ) ⋅ sin 2 (ϕ (i, j ) − φ ( p) ) (4)
                                                                                     S iso for isotropic symmetry, S rad for patterns with multiple
   The two symmetry operators mentioned above work on                               symmetry axes, and S col which uses color information.
intensity values only. Since some color transitions are not
detectable in gray-valued images, Heidemann (2004)                                  Eye-tracking experiment
adapted the isotropic symmetry operator to the color sym-                           We recorded human fixation data in an eye-tracking ex-
metry operator. This operator uses three color channels, red,                       periment using the Eyelink head-mounted eye-tracking sys-
green and blue. Equation (3) is adapted so that not only the                        tem (SR research). Fixation locations were extracted using
gradients of pixels in one channel, but also between differ-                        the accompanied software. The images were displayed full-
ent channels are compared:                                                          screen with a resolution of 1024 by 768 pixels on an 18’’
                    Mcol ( x, y ) = ∑ ∑ c(i, j , ki , k j )                  (5)    CRT monitor of 36 by 27 cm at a distance of 70 cm from
                                   ( i , j )∈Γ ( ki , k j )∈K
                                                                                    the participants. The visual angle was approximately 29º
where K contains all combinations of color channels, and                            horizontally by 22º vertically. Before the experiment, the
 c(i, j , ki , k j ) is the symmetry contribution calculated by                     eye tracker was calibrated using the Eyelink software. The
comparing pixel i in color channel ki with pixel j in color                         calibration was verified prior to each session, and recali-
channel kj. Furthermore, equation (2) is altered to:                                brated when needed.
                 p(i, j ) = cos 2 (γ i + γ j ) ⋅ cos 2 (γ i ) ⋅ cos(γ j )    (6)       Since we are interested in the bottom-up components of
                                                                                    visual attention, the participants were asked to freely view
so that the function gives the same result for gradients that                       the images. We did not give them a task, since that would
are rotated 180°. The second term keeps the same function-                          give a strong bias on the eye movements. Our approach is
ality as the second term in equation (2).                                           similar to (Kootstra, Nederveen, & De Boer, 2008; Le Meur
                                                                                    et al., 2006; Parkhurst & Niebur, 2003).
Symmetry-saliency models                                                               31 students (15 female) of the University of Groningen
The human visual system is thought to process information                           took part for course credits. The age of participants ranged
on multiple spatial scales. We therefore adapted the above                          from 17 to 32. All had normal or corrected-to-normal vi-
described operators to multi-scale saliency models, similarly                       sion. In the experiment, 99 images in five different catego-
to (Itti et al., 1998).                                                             ries were presented, 19 images of natural symmetrical ob-
   The process to calculate the symmetry maps is depicted in                        jects, 12 images of animals in a natural setting, 12 images of
Figure 2c. First, five spatial scales of the input image are                        street scenes, 16 images of buildings, and 40 images of
created by progressively applying a Gaussian filter followed                        natural environments (see Figure 3). Only the images in the
by a downscaling of the image by a factor of two. The dif-                          first category were selected for their symmetrical content.
ferent scales are then processed to symmetry feature maps                           The other categories represent a wide variety of images,
using the symmetry operators as discussed in the previous                           containing natural and cultural scenes, with organic and
section, where we use r = 24 and σ = 36 . Next, the five                            rectilinear shapes. All these images were taken from the
feature maps are normalized using the normalization opera-                          McGill calibrated colour image database (Olmos & King-
tor, N, used in (Itti et al., 1998). This normalization consists                    dom, 2004). The experiment was split up into sessions of
first of scaling the feature map values to the range [0..1],                        approximately 5 minutes. Between the sessions, the experi-
and then multiplying the feature map with (1 − m) 2 , where                         menter had a short relaxing conversation with the partici-
 m is the average value of all local maxima in the map. This                        pants, in order to get them motivated and focused for the
normalization promotes feature maps that contain a small                            next session. Before starting a new session, the calibration
number of symmetrical patterns that really stand out, as op-                        of the eye tracker was verified. After each presented images,
posed to feature maps that contain many patterns with simi-                         drift was measured and corrected if needed using the Eye-
                                                                                 58

 Figure 4: (a) The correlations results with the individual fixation-distance maps. The groups give the results for the different
image categories. The error bars are the 95% confidence intervals. The horizontal gray bars with the solid line show the mean
   and 95% confidence interval of the inter-participant correlation. The dashed lines show the correlation of the human data
 with random fixations (close to zero). (b) The correlation results with the combined fixation-distance maps, representing the
       consensus among the participants. The patterns in both plots are similar, but with higher correlations for the later.
link software. The participants could decide when to con-                   the values in these maps. The correlation coefficient has a
tinue and were allowed to take a short break.                               value between –1 and 1. A ρ of 0 means that there is no cor-
                                                                            relation between the two maps, which is true when correlat-
Analysis methods                                                            ing with random fixation-distance maps. Values for ρ close
To compare the saliency models with the human data, we                      to zero indicate that a model is a poor predictor of human
use two methods. A correlation method similar to that used                  fixation locations. Positive correlations show that there is
in (Le Meur et al., 2006; Ouerhani et al., 2004), and a fixa-               similar structure in the saliency map and the human fixation
tion-saliency method, similar to that used in (Parkhurst et                 map.
al., 2002).                                                                    This method calculates the correlation for individual par-
   The correlation method correlates the saliency maps with                 ticipants. However, the photographic images viewed by the
fixation-distance maps calculated from the human fixation                   participants are highly complex stimuli that generate many
data. For every single trial of every participant, the fixation-            fixations, with substantial variation among participants. Be-
distance map is calculated using the inverse distance trans-                cause of this variation, the correlations of individual fixa-
form. The distance transform of the human data gives the                    tion-distance maps with the saliency maps will be low.
distance to the nearest human fixation for all the pixels in                However, some of the fixations are shared by all partici-
the image. At the points of fixation, the values are thus zero,             pants. To see how well our models predict this consensus
and they increase linearly for pixels farther from the fixa-                among participants, we also calculate the correlation coeffi-
tions. We then take the inverse of the distance transforma-                 cient for the combined fixation-distance maps,
                                                                                       N
tion. This is done by subtracting all distance values from the               Ftot = ∑ i =1 F i , using equation (8).
maximum distance value. The inverse distance map, which
we call the fixation-distance map, therefore contains high                     The second comparison method, the fixation-saliency
values close to the fixation points, and lower values at                    method, measures the saliency value, according to the sali-
points far from the fixations, and can be seen as a probabil-               ency models, at the points of human fixation relative to the
ity distribution for fixations. This is slightly different from             average saliency value at a large number of randomly cho-
the approach in (Kootstra et al., 2008; Le Meur et al., 2006;               sen locations:
                                                                                                                   1000
Ouerhani et al., 2004), where a fixation density map is cal-                                       λi = s( f i ) ∑  j =1
                                                                                                                         s (rnd)        (9)
culated using Gaussian kernels. Our method puts emphasis
                                                                            Where λi is the fixation-saliency value for the ith fixation,
on the location of fixations, rather than on their density.
Moreover, our method is parameter free, i.e., there is no                   fi is the ith human fixation location and rnd is a randomly
width of the kernel to be set.                                              determined location in the image, excluding the borders.
   The value of the comparison between the saliency map                     s(p) is the average saliency value in a patch of the saliency
and the fixation-distance map is then given by the correla-                 map, S, centered at point p and with a radius of 28 pixels. If
tion coefficient, ρ:                                                        λi > 1, the saliency at human fixation points is higher than in
                ∑    ( ( F ( x, y ) − µ F ) ⋅ ( S ( x, y ) − µ S ) )        the rest of the image, meaning that the given saliency model
           ρ=
                x, y
                                                                     (8)    has predictive powers.
                                     σ F2 ⋅ σ S2
                                                                                                           Results
where F is the fixation-distance map, S is the saliency map
                                                                            In Figure 4a, the results of the correlation between the indi-
and µ and σ2 are respectively the mean and the variance of                  vidual fixation-distance maps and the saliency methods are
                                                                         59

   Figure 5: The plots give the saliency values at the fixation points relative to the average saliency in the image for the five
 image categories. Time, measured in fixation units is plotted on the horizontal axis. The fixation-saliency values for the three
   symmetry models and the contrast model of Itti and Koch are shown. The horizontal dotted line gives the expected value
                    when fixations are random (i.e., 1.0), and the error bars are the 95% confidence intervals.
shown. The five groups of bars contain the results for the          Figure 4b shows a very good match between the symmetry
different image categories. Within each group, the bars             models and the human data for all images, suggesting that
show the mean correlation coefficients for the saliency             the common fixations of the participants are captured. The
models. The error bars give the 95% confidence intervals on         difference with the contrast-saliency model is significant.
the mean. The scores of the saliency methods are plotted               Figure 5 shows the fixation-saliency values as a function
along with the inter-participant correlation, and the correla-      of the fixation number. For all image categories, the sym-
tion of the human data with random fixations. The first is          metry models have high values for the early fixations, and
depicted by the horizontal gray bar with a solid mid-line,          gradually dropping values for later fixations. This shows
giving the mean and 95% confidence interval. The correla-           that humans first fixate on highly symmetrical parts of the
tion with random fixations is depicted by the horizontal            images. The fixation-saliency scores for the contrast model,
dashed line, which is, as expected, virtually zero for all          on the other hand, are more or less constant over time, ex-
categories. All means and confidence intervals are calcu-           cept for the animal category. For the images containing
lated using multi-level bootstrapping. Significant differ-          natural symmetries, the difference with the contrast saliency
ences can be appreciated by looking at the 95% confidence           model is highly significant for all fixations. But also for
intervals.                                                          most other categories, the symmetry models score better,
   The inter-participant correlation is calculated for every        especially at the first few fixations. For the street-scene, the
image by correlating the fixation-distance maps of the par-         building, and the nature categories, symmetry at early fixa-
ticipants with those of all other participants, resulting in a      tions is significantly higher than contrast. For later fixations,
similarity measure among participants. The plot shows that          the difference is less apparent, but still in favor of the sym-
there is variability among the participants. The saliency           metry models, and significant for the nature category. For
methods are also faced with this variability, which pulls           the animal category, we see a different picture. There, the
down the correlation values. The inter-participant correla-         symmetry and contrast model do not significantly differ,
tion can therefore be used to put the scores of the saliency        with higher scores for the contrast model. Furthermore, the
methods into perspective. The correlation scores of the             contrast values are not constant for this category, but drop
models can be higher when the variation is high.                    over time. This difference can be explained by the fact that
   Looking at the bars and the 95% confidence intervals in          the animal images contain objects (the animals) that are
Figure 4a, it can be appreciated that the performance of the        highly distinguishable from their, often, more or less uni-
three symmetry models is significantly higher than that of          form and blurry background. In contrast, the fore- and back-
the contrast model. This is not only true for the natural sym-      ground in the other categories are less distinct, and more
metry images containing explicit symmetrical forms, also            cluttered.
for the other categories the symmetry models significantly
outperform the contrast model. For all categories, the per-                                   Discussion
formance of the symmetry models is in the same range as             We proposed three symmetry saliency models to investigate
the inter-participant score. Among the three symmetry mod-          the role of local symmetry in guiding eye fixations. To test
els, there is no significant difference in performance.             the performance of the models, we conducted an eye-
   The values in Figure 4a are relatively low, caused by the        tracking study, and evaluated the prediction of the models
variability among participants. Figure 4b shows the results         with the human data. We used the contrast saliency model
of the comparison of the human data with the combined               of Itti and Koch (Itti et al., 1998) to compare our results.
fixation-distance maps. This gives the similarity between              The results clearly show that human eye fixations can be
the saliency models and the consensus among participants.           significantly better predicted with our symmetry models
                                                                 60

than with the contrast model. Our models show a signifi-           Kaufman, L., & Richards, W. (1969). Spontaneous Fixation
cantly better correlation with the human data, comparable            Tendencies for Visual Forms. Perception & Psychophy-
with the inter-participant correlation, for the individual par-      sics, 5(2), 85-88.
ticipants. Particularly when we look at the combined fixa-         Kootstra, G., Nederveen, A., & De Boer, B. (2008, 1-4 Sep-
tion-distance maps, we see a very good match, showing that           tember 2008). Paying Attention to Symmetry. In proceed-
the proposed models predict the general consensus. The               ings of the British Machine Vision Conference, Leeds,
proposed models are general models in the sense that they            UK.
do not only perform well on the images containing natural          Le Meur, O., Le Callet, P., Barba, D., & Thoreau, D. (2006).
symmetrical objects, but also on images that are not selected        A Coherent Computational Approach to Model Bottom-
for their symmetrical content. Moreover, the fixation-               Up Visual Attention. IEEE Transactions on Pattern
saliency analysis shows that especially early fixations are on       Analysis and Machine Intelligence, 28(5), 802-817.
highly symmetrical content, suggesting that symmetry is a          Locher, P. J., & Nodine, C. F. (1987). Symmetry Catches
good predictor of involuntary eye fixations, which are pre-          the Eye. In J. K. O'Regan & A. Lévy-Schoen (Eds.), Eye
sumably more bottom-up controlled.                                   Movements: From Physiology to Cognition. North-
   A possible explanation that symmetry is a good predictor          Holland: Elsevier Science Publishers B.V.
of eye fixations is that eye movements are object oriented.        Olmos, A., & Kingdom, F. A. A. (2004). McGill Calibrated
Symmetry is known to play a role in figure-ground segrega-           Colour Image Database, http://tabby.vision.mcgill.ca.
tion (Driver et al., 1992). Local symmetry can therefore           Ouerhani, N., von Wartburg, R., Hügli, H., & Müri, R.
serve as a bottom-up cue for the presence of an object in the        (2004). Empirical Validation of the Saliency-based Model
image, which is then further inspected by making a fixation.         of Visual Attention. Electronic Letters on Computer Vi-
   We expected the radial symmetry model to outperform               sion and Image Analysis, 3(1), 13-14.
the isotropic symmetry model since psychophysical studies          Palmer, S. E. (1991). Goodness, Gestalt, Groups, and Gar-
showed that humans are more sensitive to forms with multi-           ner: Local Symmetry Subgroups as a Theory of Figural
ple symmetry axes (Palmer & Hemenway, 1978). However,                Goodness. In G. R. Lockhead & J. R. Pomerantz (Eds.),
this did not result in a significant increase in performance.        The Perception of Structure. Essays in Honor of Wendell
This can be explained by the fact that the isotropic model           R. Garner (pp. 23-40). Washington, DC: American Psy-
already gives a stronger response to patterns with multiple          chological Association.
symmetry axes. Also color does not improve the model.              Palmer, S. E., & Hemenway, K. (1978). Orientation and
   Contrast obviously also plays a role in bottom-up atten-          Symmetry: Effects of Multiple, Rotational, and Near
tion. In future research, we would like to study the combina-        Symmetries. Journal of Experimental Psychology: Hu-
tion of contrast and symmetry for the prediction of fixations.       man Perception and Performance, 4(4), 691-702.
   To conclude, the symmetry models that we developed are          Parkhurst, D. J., Law, K., & Niebur, E. (2002). Modeling
good predictors of visual attention, suggesting that humans          the Role of Salience in the Allocation of Overt Visual At-
pay attention to symmetry.                                           tention. Vision Research, 42, 107-123.
                                                                   Parkhurst, D. J., & Niebur, E. (2003). Scene Content Se-
                         References                                  lected by Active Vision. Spatial Vision, 16(2), 125-154.
                                                                   Privitera, C. M., & Stark, L. W. (2000). Algorithms for De-
Barlow, H. B., & Reeves, B. C. (1979). The Versatility and           fining Visual Regions-of-Interest: Comparison with Eye
   Absolute Efficiency of Detecting Mirror Symmetry in               Fixations. IEEE Transactions on Pattern Analysis and
   Random Dot Displays. Vision Research, 19, 783-793.                Machine Intelligence, 22(9), 970-982.
Driver, J., Baylis, G. C., & Rafal, R. D. (1992). Preserved        Reisfeld, D., Wolfson, H., & Yeshurun, Y. (1995). Context-
   figure-ground segregation and symmetry perception in              Free Attentional Operators: The Generalized Symmetry
   visual neglect. Nature, 360, 73 - 75.                             Transform. International Journal of Computer Vision, 14,
Grammer, K., & Thornhill, R. (1994). Human (Home                     119-130.
   sapiens) Facial Attractiveness and Sexual Selection: The        Richards, W., & Kaufman, L. (1969). "Center-of-Gravity"
   Role of Symmetry and Averageness. Journal of Com-                 Tendencies for Fixations and Flow Patterns. Perception &
   parative Psychology, 108(3), 233-242.                             Psychophysics, 5(2), 81-84.
Heidemann, G. (2004). Focus-of-Attention from Local                Royer, F. L. (1981). Detection of Symmetry. Journal of
   Color Symmetries. IEEE Transactions on Pattern Analy-             Experimental Psychology: Human Perception and Per-
   sis and Machine Intelligence, 26(7), 817-830.                     formance, 7(6), 1186-1210.
Itti, L., & Koch, C. (2001). Computational Modelling of            Treisman, A. M., & Gelade, G. (1980). A Feature-
   Visual Attention. Nature Reviews Neuroscience, 2(3),              Integration Theory of Attention. Cognitive Psychology,
   194-203.                                                          12(1), 97-136.
Itti, L., Koch, C., & Niebur, E. (1998). A Model of Sali-          Tyler, C. W. (2000). The Human Expression of Symmetry:
   ency-Based Visual Attention for Rapid Scene Analysis.             Art and Neuroscience. In proceedings of the ICUS Sym-
   IEEE Transactions on Pattern Analysis and Machine In-             metry Symposium, Seoul.
   telligence, 20(11), 1254-1259.
                                                                61

