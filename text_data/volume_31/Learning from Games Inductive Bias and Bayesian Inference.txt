UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning from Games: Inductive Bias and Bayesian Inference
Permalink
https://escholarship.org/uc/item/1g12g5ff
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Coen, Michael
Gao, Yue
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Learning from Games: Inductive Bias and Bayesian Inference
                                                Michael H. Coen1,2 and Yue Gao2
                                         Department of Biostatistics and Medical Informatics1
                                                  Department of Computer Sciences2
                                             University of Wisconsin, Madison, WI 53706
                                                      {mhcoen, gao}@cs.wisc.edu
                            Abstract                                  as observed by Hume (1739) and formalized by Wolpert
                                                                      (1996). While this is true in a theoretical sense, people
   A classic problem in understanding human intelligence is
   determining how people make inductive inferences when              surely do have preferences; in other words, it is quite
   presented with small amounts of data. We examine this              reasonable to suppose that we find some answers more
   question in the context of the guess-the-next-number game,         plausible than we do others, even in the absence of objective
   where players are presented with short series of numbers and       justifications.
   asked to guess the next one in the sequence. Our approach is         Although there is no doubt that humans generalize using
   unique in that we use a stochastic context free grammar to         an inductive bias, formally characterizing it can be
   model the mathematical operations that generate a given
   sequence. The individual probabilities in this grammar are
                                                                      challenging. For example, even in domains where one
   learned by observing people play this game, and thereby, they      believes that Occam’s Razor is the principle guiding human
   capture some of the mathematical inductive bias of our             induction (Myung and Pitt 1997), the numerous
   sample population. We then use this framework to solve             formulations of this classic notion of parsimony may lead to
   novel sequence guessing problems computationally, mirroring        diametrically opposed conclusions. (For an interesting
   human performance. Our goal is to better understand how            discussion of this issue in elucidating the innate
   people approach math problems by examining the space of            mathematical abilities of infants, see Carey 2002).
   mathematical functions they find easiest to both generate and
   recognize. We are also interested in tracking how this               In this paper, we explore a domain where we can formally
   changes over time as functions of education and age. Finally,      model inductive bias using probabilities in a stochastic
   we examine how our results confirm a large body of                 context free grammar, which appear to capture how people
   psychological observations about how people approach               do math “in their heads.” Because our notion of inductive
   mathematics problems.                                              bias is rigorously defined, we can also trace how it changes
                                                                      over time. Thus, we expect grammar school, high school,
   Keywords: Inductive Bias; Mathematical Modeling;                   and college students to have very different inductive biases,
   Stochastic Context Free Grammars; Bayesian inference.              which can of course be further characterized by their fields
                                                                      of study. We are also interested in tracking how these
                         Introduction                                 mathematical biases change as people age. More generally,
People regularly make inferences by induction, even when              this work provides a window in the types of mathematical
presented with very small amounts of information. The                 operations with which people are most comfortable as well
purpose of our work is to understand more about how this              as the types of operations that perhaps require more careful
occurs in mathematical reasoning, especially when these               instruction.
inferences are remarkably consistent and have little formal
justification. We explore this problem in the context of the
guess-the-next-number game. For example, suppose we
present someone with the sequence [1, 2, 4] and ask him to
guess the next number in the series. If he were to suggest 8,
stating this set corresponded to “powers of 2,” we would
presumably find this a plausible explanation. On the other
hand, were he to suggest the number is 6, explaining the
sequence seems to be “1 followed by the even numbers,” we
might find that answer less satisfactory.1
   Of course, this determination is highly arbitrary. Because
inductive inferences are not logically entailed, they must
rest upon some set of assumptions, known as an inductive
bias (Mitchell 1980). In the absence of such a bias, all
inferences consistent with a finite dataset are equally valid,
   1
     We will see, however, that appeals to minimum description        Figure 1. Examining the guess-the-next-number game.
length (e.g., Rissanen 1978) do not capture human preferences in      While the examples in (a) and (b) are fairly straightforward,
making these judgments.                                               the sequence in (c) is somewhat more ambiguous.
                                                                 2729

                The Next Number Game                               Expression → PrefixOp ( Expression ) p_1
                                                                   Expression → Expression InfixOp Expression p_2
   We examine our use of the next number game and some
                                                                   Expression → Previousi-1 p_3 | Previousi-2 p_4 | Previousi-3 p_5
of the ambiguities it presents to players. This game is
generally familiar to people around the world and is played        Expression → Number p_6
by both children and adults. In Figure 1, we examine               Expression → Index p_7
several instances of this game and begin to understand why         PrefixOp → exp p_8 | log p_9 | sin p_10 | cos p_11 | tan p_12
                                                                                                                                      p_17
specific solutions might be subject to debate.                     PrefixOp → floor p_13 | ceiling p_14 | mod p_15 | rem p_16 | prime
   When the sequences correspond to familiar concepts, such                     p_18    p_19       p_20     p_21     p_22
                                                                   InfixOp → +       |−       |×         |÷       |^
as the prime numbers or perfect cubes, players already             Number → SmallNum | LargeNum | SpecialNum
familiar with these concepts are likely to latch onto them.        SmallNum → [-9
                                                                                     p_26
                                                                                          ,…,9
                                                                                                  p_45
                                                                                                      ]
In that sense, this tells us far more about what                                                                 p_46
                                                                   LargeNum → [-50,…, -11, 11, …, 50]
predetermined sets of numbers a player is already familiar                               p_47         p_48      p_49    p_50
with than it does about his mathematical preferences. This         SpecialNum → -100           | -10       |¼        |½
                                                                                     p_51       p_52         p_53
was the approach pursued in Tenenbaum (1999), where he             SpecialNum → π          | 10       | 100
                                                                                     p_54
presented predetermined, unordered sets to players and             Index → [1,…,10]
asked them to rank how likely it was some other number
belonged to each set. He was able to predict their rankings        Figure 2. Our stochastic context free grammar for
using a clever variant of minimum description length.              generating mathematical functions. Non-terminals begin
However, his framework did not generalize to simple                with capital letters. Terminals symbols are indicated by
unknown sets, in other words, data it had not been trained         lower-case strings or numbers. Each production rule is
on, such as prime numbers – 1. We wanted to create a more          associated with some probability p_i, indicating its
generative framework that could predict arbitrary sequences        likelihood according to our training corpus of people
and simultaneously provide insight into the human                  playing this game. Players of the guess the next number
mathematical reasoning process, in a spirit similar to (Leslie     game provide both their answers and the function they think
et al. 2008). We also wanted to create a platform that             generated it. Our goal is to use this corpus of games to
enables tracking how these identified mathematical biases          derive the probabilities p_i in order to both discover and to
change over time.                                                  duplicate human inductive bias playing this game.
   The problem of examining inductive bias in fitting data to
closed form equations has been studied by Haverity and
Koedinger (2000). While they did not computationally               elements of a sequence. This function generating each
model the reasoning process behind their subjects’                 element may be based upon its position in the sequence
performance, our results are in strong agreement with their        (represented by “Index”), the immediately prior numbers in
observations and previous psychological studies, such as           the sequence (represented by “Previousi-k”), or perhaps even
Huesmann and Cheng (1973).                                         a constant (represented by “Number”).                     The generating
   We note that Sloane (2008) maintains a delightful, online       function is constructed out of both prefix and infix
encyclopedic database of integer sequences. While both             operations (“PrefixOp” and “InfixOp” respectively). A
highly esoteric and thorough, it is simply a lookup table of       prefix operator takes a single argument, such as is the case
series submitted by users. Many of its sequences are               with log, while an infix operator takes two arguments, such
generated by physical and mathematical processes that have         as when performing addition or subtraction. Constants are
no obvious closed form generative formulae. Thus, it is not        divided into three categories: (1) Small numbers, which we
relevant to our interests here, as it provides little, if any,     assume are easier to process cognitively; (2) Large number
information about human mathematical cognition.                    that are presumably more difficult to involve in mental
                                                                   arithmetic; and (3) Special numbers such as 10 or π, that
               A Generative Framework                              simplify many types of operations or have some other
                                                                   special significance. For example, trigonometric operations
We constructed a generative framework for mathematical             on simple functions of π will be very familiar to many
expressions using a stochastic context free grammar                university students. One might in fact formulate the
(SCFG). This allows us to capture the notions of “function         grammar to “confine” the use of π to trigonometric
recognition” and “function creation” in a well-defined             functions. However, as reasonable as this appears, it
framework. This approach for recognition has become                complicates the grammar and eliminates other functions of π
extremely popular for parsing in the Statistical Natural           that are not unreasonable, e.g., Index + π. Fortunately, this
Language Processing community (e.g., Collins 2003), and            type of simplification is unnecessary, as we do not expect
seemed extremely well-suited for expressing mathematical           that people are able to play this game with arbitrarily
operations. However, a basic distinction is that our               complex generating functions. Thus, the assumption that
grammars output mathematical expressions, which are then           people can play this game eliminates concerns about
evaluated to produce numerical values.                             pathologically complex functions that few (or no) people
   Our grammar is shown in Figure 2. It defines the notion         could ever recognize. We return to the notion of function
of an expression, which is a function that generates the
                                                               2730

complexity and cognitive plausibility below.                            Table 1. Examining the Bayesian probabilities of some
   Each production rule in this SCFG is associated with                 production rules in our generative mathematical grammar.
some probability, represented by the superscripted p_i                  The probabilities are determined for each non-terminal rule
following it. This probability represents the observed                  in our grammar separately.
likelihood our sample population has employed this
productive rule while playing a series of games. To reduce                                    Production Rule                     Probability
the amount of training data required, we “lumped together”                   Expression → PrefixOp ( Expression )                 0.00402
certain groups, such as large numbers, represented by the                    Expression → Expression InfixOp Expression           0.349
LargeNum production rule. While we assume any of these                       Expression → Previousi-1                             0.177
numbers may appear in the generating formula, gathering                      Expression → Previousi-2                             0.0321
data for each individual number would require that each                      Expression → Number                                  0.317
subject play a very large of games. Furthermore, given the                   Expression → Index                                   0.104
natural variability in determining the function generating a
given sequence, there is no way to guarantee subjects would                  InfixOp → +                                          0.388
employ a specific number. In other words, it can be difficult                InfixOp → −                                          0.143
to “force” subjects to employ particular production rules.                   InfixOp → ×                                          0.263
Ad hoc sequences, such as using a constant difference                        InfixOp → ÷                                          0.0388
between successive elements to “lead” players to each                        InfixOp → ^                                          0.163
number, e.g., for 11, we might use [1, 12, 33, 44, …], would
simply generate a uniform distribution among the members                     SmallNum → -1                                        0.04
of LargeNum, which is what we were seeking to avoid in                       SmallNum → 1                                         0.24
the first place. Thus, we avoid the problem entirely and                     SmallNum → 2                                         0.40
make certain classes of number probabilistically equivalent.                 SmallNum → 3                                         0.08
                                                                             SmallNum → 4                                         0.04
Finding the Probabilities                                                    …                                                    …
To determine the probabilities p_i for each production rule,
we first collected data from 20 university undergraduates
                                                                        (2) Consider the sequence [1,2,10]. That this sequence is
with nonmathematical backgrounds2, who played up to 22
                                                                               in some sense more difficult was apparent because
different rounds of the guess-the-next-number game over
                                                                               subjects spent more time studying it, often commenting
ascending and descending sequences. The students were
                                                                               it felt “difficult” or under constrained.
asked both to guess the next number in a displayed sequence
and to provide the formula generating it, in light of the                         All but one: f(index) = Previousindex-1 + (index – 1)3
presented sequence. There was no imposed time limit and a                         Yielding: [1, 2, 10, 37, 101, …]
student was free to skip a sequence if he could not solve it.
Our goal for this experiment was to learn the mathematical                        One guess: f(index) = Previousindex-1 + (Previousindex-1)3
inductive biases of college-aged non-mathematicians. We                           Yielding: [1, 2, 10, 1010, 1.0303×109,…]
examine three sample sequences from this experiment.
                                                                           Note that although these guesses lead to different
(1) Consider the sequence [1, 4, 9].            We found that all          predictions of subsequent sequence values, they are
      subjects predicted the next number would be 16, but                  structurally quite similar. From our Bayesian perspective,
      provided two syntactically different but numerically                 they will lead to very similar priors in the grammar.
      equivalent generating formulae, at least up to index 4.              There are a multitude of other functions that were not
                                                                           selected by our sample population, e.g., f(index) =
        40% guessed: f(index) = index2                                     Previousindex-1 + 8(index-1), presumably reflecting a distaste
                                                                           for this level of complexity. 3
        60% guessed: f(index) = Previousindex-1 + 2×index + 1
                                                                        (3) Finally, we examine the sequence [0,7,26], where all
   For this example, we see that most subjects preferred                       the subjects agreed on the next element (63) and on the
   conceptually simple arithmetic operations, even if the                      generating formula:
   resulting functional description was longer and bordered
   on being convoluted. We found this type of result quite                        f(index) = index3 – 1
   surprising, as it would not have occurred to us this
   sequence would be identified as anything other than
   perfect squares.
                                                                           3
                                                                              This formula and similar variants were provided by several
                                                                        graduate students in Computer Science, who were also asked to
   2
     Although one can debate our selection criterion, we eliminated     solve this problem. It comes as little surprise they have very
students majoring in mathematics, computer science, or physics.         different inductive biases for playing this game.
                                                                    2731

   Here, alternative explanations avoiding a difficult            sequences. This helps insure that new sequences do not
   operation, such as exponentiation, are so complex that         violate our learned inductive biases, at least for retesting this
   they are disregarded. We explore the measure of                population on larger sequence corpora or comparing their
   functional complexity in our system below.                     biases with that of another target population, e.g., math
                                                                  majors.
To derive individual production probabilities from all
presented sequences, we used the inside-outside algorithm         Processing the Sequences
(Baker 1979). Specifically, we employed a Gibbs sampler           After deriving the probabilities for our generative SCFG via
for SCFGs developed by (Johnson et al. 2007), which               Gibbs sampling, we encoded the grammar in Prism (Sato
derived Bayesian priors for the production rules based on         and Kameya 2008), a probabilistic version of Prolog that
the formulas generated by the subjects using Markov chain         requires parameterized probability distributions over its
Monte Carlo methods. Finally, we had to perform a                 production rules. Prism’s inference engine incorporates the
renormalization of the infix operators (InfixOp) probabilities    Viterbi algorithm (Forney 1973). Therefore, its resolution is
to account for commutative operations such as × and +.            guided by following the most likely series of rule
This is due to the fact that the Viterbi algorithm (see the       expansions to satisfy a given query. One can view Prism as
next section) has no way of realizing, for example, that a+b      an extension of Prolog that provides the most probable
is equal to b+a and thereby undercounts its likelihood. (It       solution to a given query. Queries here corresponded to the
computes a single path to the answer, without realizing there     question: given an input sequence, what is the most likely
are numerically equivalent ones that are syntactically            next number according to our SCFG? The process of
different in trivial ways.)                                       determining the next number generates the function
   We examine some of the more interesting results of this        responsible for doing so via Prolog’s resolution mechanism.
derivation in Table 1. Among the most significant but               Because our recursive grammar is computing
unsurprising findings is that people do not like performing       mathematical functions, as opposed to parsing a sentence, it
division in their heads; it represents 3.88% of infix             will never “run out” of input data in resolving this query.
operations. In contrast, they are ten times more likely to        Instead, it would continue down the most likely
prefer addition, which represents 38.8% of infix operations.      mathematical path forever, constantly generating ever more
Our subjects also preferred transforming non-terminal             complex expressions. Because the Viterbi algorithm needs
expressions into concrete numbers rather quickly, as              to hit a leaf to trigger backtracking, resolution would never
opposed to developing complex expressions. As might be            halt in our framework. We therefore add an explicit
expected, 1 and 2 were clearly the most popular numbers for       stopping criterion, using the probabilities in the SCFG to
mental arithmetic. (Note that 0 does not appear, as it has a      determine the overall probability of any expression
probability of zero; this is because no one used it to solve      examined in the course of resolution. By the definition of a
any sequence problems. This makes sense, as it contributes        context free grammar, the probabilities are independent, so
nothing given the mathematical operations presented here.         we can simply multiply all non-terminal probabilities to
In other words, there is no reason to employ it.)                 determine the value for a given expression. 4 If this
   How much does the selection of sequences itself bias           probability falls below a predefined threshold, our system
these results? For example, were we to only present               automatically triggers backtracking, essentially ruling the
ascending series, there would be an innate bias in favor of       current line of investigation as too complex to be plausible.
monotone functions such as addition and multiplication, at        This threshold can be computed directly from the gathered
least with the ontology presented here. Thus, we made an          corpus of human responses.
effort to balance the sequences to remove obvious sources           We view this as a probabilistic version of working
of such bias. However, it should be noted that generating         memory, as defined in (Miller 1956). We believe that
representative sequences of three to five numbers that are        preferred operations are easier to cognitively track, whereas
amenable to human solution is non-trivial. One cannot             less likely (or more difficult) operations have a greater
simply produce them randomly, e.g., by typing                     impact in limiting the size of the overall expression. Thus,
round(rand(1,4)*10)) in Matlab, and expect to                     we are not explicitly modeling the expression size. Rather,
produce a sequence that holds any meaning or lends itself to      the probability threshold implicitly limits the complexity of
an obvious generative formula. Thus, while there are an           the internal mathematical computation.               This seems
infinite number of “solvable” sequences, they are somewhat        cognitively reasonable and agrees with our observed results.
sparsely distributed and must be selected with some care.
   We therefore generated a large list of sequences and                                       Results
randomly selected from among them those solvable upon
                                                                  We now examine some sample outputs of our system. They
inspection. However, one of the benefits of our constructed
                                                                  demonstrate how its behavior changed after acquiring the
generative framework is that we can use it to automatically
                                                                  inductive bias of the observed population and show how it
generate sequences for future experimentation that capture
the types of operations people prefer. In this sense, we can
                                                                    4
use our grammar to produce rather than recognize                      For the sake of efficiency, the overall probability is adjusted
                                                                  dynamically during rule expansions and backtracking.
                                                              2732

                                                                      the human answer is more than an order of magnitude more
   (A)
                                                                      likely than the system’s original, untrained solution to this
                                                                      problem. This change is due to the system having learned
                                                                      not only that division is less likely but also to its preference
                                                                      for using small numbers over more distant terms
                                                                      (Previousindex−2) in the sequence’s generating formula.
                                                                      Modifying Familiar Sets
                                                                      We now look at an example that demonstrates the benefits
                                                                      of not restricting our approach to a predetermined set of
                                                                      sequences. Put somewhat differently, we can see the power
               f(index) = Previousindex-2 / (2 × Previousindex-1)
                                                                      of a generative inductive framework in examining how it
                                                                      copes with functional transformations to familiar sequences,
   (B)                                                                such as the prime numbers. We presented our system with
                                                                      the sequence [1,2,4,6], which was not part of its training
                                                                      data.      It predicted the generating function was
                                                                      f(index)=Prime(index) – 1 with a confidence level of 91%
                                                                      based on its acquired inductive probabilities, which agrees
                                                                      with human subjects. We see the second most likely
                                                                      candidate for this sequence in Figure 4, which has a
                                                                      probability of approximately 1%.5 Note that the most likely
                    f(index) = Previousindex-1 + index – 6                 (A)
 Figure 3. Our system’s output on the sequence [8,4,1],
 before training (A) and after training (B). Without any
 inductive bias, the system predicts the explanation in (A),
 as we assume a uniform distribution over each production
                                                                                               f(index) = Prime(index) – 1
 rule in the absence of any bias. After deriving the
 production probabilities via Gibbs sampling using the
 inside-outside algorithm, our system acquires the priors                  (B)
 representing the inductive biases of our sample population.
 It then changes its answer to (B), agreeing with 88% of
 human subjects, even though it has never seen this
 sequence before. It now assigns the answer in (B) more
 than ten times the probability as the answer in (A).
generalizes to handle out-of-set examples, namely,
sequences it has never seen before. We then discuss
implications of this work, particularly what it reveals about                          f(index)=Floor((index + Previousindex-1 ) / 2 )
the capabilities for performing mental arithmetic in people.
                                                                       Figure 4. Generated solutions to the sequence [1, 2, 4, 6].
The Effects of Learning                                                The solution in (A) has a probability of 91%. The second
We presented subjects with the sequence [8, 4, 1]. The test            most likely solution, show in (B), is determined to be only
subjects overwhelming (88%) guessed the generating                     1% likely. The example demonstrates the need for a
function was f(index) = Previousindex-1 + index – 6. Before            generative mathematical framework for playing the guess-
training, our sequence guesser, using uniform distributions            the-next-number game, as opposed to enumerating huge
on its production rules, predicted the function was:                   numbers of predetermined training sets. Many familiar sets
          f(index) = Previousindex-2 / (2 × Previousindex-1)           are easily recognizable under various simple functional
                                                                       transformations, e.g., subtracting one from them.
However, after training on examples that did not include
                                                                         5
this sequence, our system changed its answer to agree with                  Final probabilities for expressions are determined by
the solution provided by the vast majority of human subjects          generating all possible explanatory functions within our threshold,
on this problem. The expansion of these formulae in terms             calculating their probabilities according to the SCFG, and then
                                                                      normalizing these into a probability distribution. In the event we
of our grammar is displayed in Figure 3. We note the                  simply want the most likely expression, it is unnecessary to
acquisition of human inductive bias now leads it to predict           enumerate every generating function.
                                                                  2733

solution is simply a straightforward modification to a              of Wisconsin-Madison. Thanks to W. Richards, C. Dyer,
familiar set. There is any number of modifications to such          and M.H. Ansari for helpful comments.
sets that are recognizable, where the likelihood of
recognition depends upon the complexity of the applied                                         References
operations. The advantage of working within a generative            Baxter, J. A. (2000). Model of Inductive Bias Learning. Journal of
framework is clear from this example, as opposed to                 Artificial Intelligence Research. 149(12).
exhaustively listing sets that might be encountered and then
                                                                    Carey, S. (2002). Evidence for numerical abilities in young infants:
defining a metric that attempts to compute human
                                                                    a fatal flaw? Developmental Science, 5(2), pp202-205.
preferences among them.
                                                                    Collins, M. (2003). Head-Driven Statistical Models for Natural
                                                                    Language Parsing. Computational Linguistics 29:4, pp589-637.
                        Conclusions
   This paper has presented a framework for solving the             Forney, G. D. (1973). The Viterbi algorithm. Proceedings of the
                                                                    IEEE. 61(3), pp268-278, March.
guess-the-next-number game that is based upon acquiring a
realistic model of human inductive bias. As these brief             Gerwin, D and Newsted, E. (1977). A comparison of some
sequence problems are highly unconstrained and yet                  inductive inference models. Behavioral Science．22:1-11.
different people often arrive at identical results, we find the     Haverity, L.A., and Koedinger, K.R. (2000). Solving inductive
hypothesis that there are innate cognitive preferences              reasoning problems in Mathematics: Not-so-Trivial Pursuits.
guiding mathematical reasoning extremely reasonable.                Cognitive Science: A multidisciplinary Journal. 24(2), pp249-298.
While these may vary by particular age groups and                   Huesmann, L.R. and Cheng, C. (1973). A model for the induction
educational background, our preliminary results agree with          of mathematical functions. Psychological Review. 80, pp126-138.
previous psychological studies of induction in mathematical
                                                                    Hume, D. (1739). A Treatise of Human Nature. (eds.) Norton,
problem solving, notably including the work of (Huesmann
                                                                    D.F., and Norton, M.J. Oxford University Press. New York.
and Cheng, 1973, Gerwin and Newsted 1977, Qin and                   2000.
Simon 1990). Specifically, we verified that people have
clear preferences among operators and their formulation of          Johnson, M., Griffiths, T.L., and Goldwater, S. (2007) Bayesian
generative functions is very much driven by the underlying          Inference for PCFGs via Markov Chain Monte Carlo. Proceedings
                                                                    of the 2007 Conference of the North American Chapter of the
data. Our results also agreed with people having a clear            Association for Computational Linguistics. pp139-146.
preference for linear functions, in cases where the data make
them possible.                                                      Leslie, A.M., Gelman, R., and Gallistel, C.R. (2008). The
   As part of this work, we constructed a system that               generative basis of natural number concepts. Trends in cognitive
                                                                    sciences. 12(6), pp213-218.
acquires mathematical inductive biases observed in our
sample population. In doing so, it is able to imitate their         Miller, G.A. (1956). The Magical number seven, plus or minus
problem solving, even in cases where it must ignore more            two: some limits on our capacity of processing information. The
compact functions because they are mathematically                   Psychological Review. 63(2).
complex according to the acquired bias. It thus employs a           Mitchell, T. (1980). The need for biases in learning
different notion of simplicity than would be described by           generalizations. Technical Report CBM-TR-117, Department of
formalizations of generative brevity.                               Computer Science, Rutgers University.
   Our future plans are to test different age groups to track       Myung, I. and Pitt, M. (1997). Applying Occam’s Razor in
the temporal development of their mathematical inductive            Modeling Cognition: A Bayesian approach. Psychonomic Bulletin
biases. We are particularly interested in bias invariants that      & review. 4(1), pp79-95.
persist over time and in educational strategies that may be         Qin, Y., and Simon, H.A. (1990). Imagery and problem solving.
suggested by elucidating limitations in how people approach         Proceedings of the 12th Annual Conference of the Cognitive
mathematical problem solving.                                       Science Society. pp646-65.
   We also believe the framework in this paper is quite
                                                                    Rissanen, J. (1978) Modeling by the shortest data description.
general and can acquire inductive biases in a wide variety of       Automatica 14, pp465-471.
areas that have similar probabilistic generative structure.
We intend to employ it for modeling and predicting human            Sato, T. and Kameya, Y. (2008). New advances in logic-based
                                                                    probabilistic modeling by PRISM. In Probabilistic Inductive Logic
behavior in these realms. Here, the primary challenge will
                                                                    Programming, LNCS 4911, Springer, pp118-155.
be modeling actions or decisions via the SCFG formalism,
so we may derive their probabilities through observation            Sloane, N. J. A. (2008). The On-Line Encyclopedia of Integer
using the Bayesian framework presented here.                        Sequences.             Electronically        published           at:
                                                                    www.research.att.com/~njas/sequences/.
                   Acknowledgments                                  Tenenbaum, J.B. (1999) A Bayesian Framework for Concept
                                                                    Learning. Ph.D. Thesis, Massachusetts Institute of Technology.
This work was supported by the School of Medicine and
Public Health, the Wisconsin Alumni Research Foundation,            Wolpert, D. H. (1996). The lack of a priori distinctions between
the Department of Biostatistics and Medical Informatics,            learning algorithms. Neural Computation, 8(7), pp1341-1390.
and the Department of Computer Sciences at the University
                                                                2734

