UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Phonetic Categories by Learning a Lexicon
Permalink
https://escholarship.org/uc/item/2s9953qz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Feldman, Naomi
Griffiths, Thomas
Morgan, James
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          Learning Phonetic Categories by Learning a Lexicon
                                        Naomi H. Feldman (naomi feldman@brown.edu)
                 Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                    Department of Psychology, University of California at Berkeley, Berkeley, CA 94720 USA
                                         James L. Morgan (james morgan@brown.edu)
                 Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA
                              Abstract                                 the same or different vowels (e.g. send vs. act). Simulations
                                                                       demonstrate that using information from segmented words to
   Infants learn to segment words from fluent speech during
   the same period as they learn native language phonetic cate-        constrain phonetic category acquisition allows more robust
   gories, yet accounts of phonetic category acquisition typically     category learning from fewer data points, due to the inter-
   ignore information about the words in which speech sounds           active learner’s ability to use information about which words
   appear. We use a Bayesian model to illustrate how feed-
   back from segmented words might constrain phonetic category         contain particular speech sounds to disambiguate overlapping
   learning, helping a learner disambiguate overlapping phonetic       categories.
   categories. Simulations show that information from an artifi-          The paper is organized as follows. We begin with an intro-
   cial lexicon can successfully disambiguate English vowel cat-
   egories, leading to more robust category learning than distri-      duction to the mathematical framework for our model, then
   butional information alone.                                         present toy simulations to demonstrate its qualitative proper-
   Keywords: language acquisition;           phonetic categories;      ties. Next, simulations show that information from an artifi-
   Bayesian inference                                                  cial lexicon can disambiguate formant values associated with
                                                                       English vowel categories. The last section discusses potential
   Infants learning their native language need to extract sev-         implications for language acquisition, revisits the model’s as-
eral levels of structure, including the locations of phonetic          sumptions, and suggests directions for future research.
categories in perceptual space and the identities of words
they segment from fluent speech. It is often implicitly as-            Bayesian Model of Phonetic Category Learning
sumed that these steps occur sequentially, with infants first
                                                                       Recent research on phonetic category acquisition has focused
learning about the phonetic categories in their language and
                                                                       on the importance of distributional learning. Maye, Werker,
subsequently using those categories to help them map word
                                                                       and Gerken (2002) found that the specific frequency dis-
tokens onto lexical items. However, infants begin to segment
                                                                       tribution (bimodal or unimodal) of speech sounds along a
words from fluent speech as early as 6 months (Bortfeld, Mor-
                                                                       continuum could affect infants’ discrimination of the contin-
gan, Golinkoff, & Rathbun, 2005) and this skill continues
                                                                       uum endpoints, with infants showing better discrimination of
to develop over the next several months (Jusczyk & Aslin,
                                                                       the endpoints when familiarized with the bimodal distribu-
1995; Jusczyk, Houston, & Newsome, 1999). Discrimina-
                                                                       tion. This work has inspired computational models that use a
tion of non-native speech sound contrasts declines during the
                                                                       Mixture of Gaussians approach, assuming that phonetic cate-
same time period, between 6 and 12 months (Werker & Tees,
                                                                       gories are represented as Gaussian, or normal, distributions of
1984). This suggests an alternative learning trajectory in
                                                                       speech sounds and that learners find the set of Gaussian cat-
which infants simultaneously learn to categorize both speech
                                                                       egories that best represents the distribution of speech sounds
sounds and words, potentially allowing the two learning pro-
                                                                       they hear. Boer and Kuhl (2003) used the Expectation Max-
cesses to interact.
                                                                       imization (EM) algorithm (Dempster, Laird, & Rubin, 1977)
   In this paper we explore the hypothesis that the words in-
                                                                       to learn the locations of three such vowel categories from for-
fants segment from fluent speech can provide a useful source
                                                                       mant data. McMurray, Aslin, and Toscano (2009) introduced
of information for phonetic category acquisition. We use a
                                                                       a gradient descent algorithm similar to EM to learn a stop
Bayesian approach to explore the nature of the phonetic cat-
                                                                       consonant voicing contrast, and this algorithm has been ex-
egory learning problem in an interactive system, where infor-
                                                                       tended to multiple dimensions for both consonant and vowel
mation from segmented words can feed back and constrain
                                                                       data (Toscano & McMurray, 2008; Vallabha, McClelland,
phonetic category learning. Our interactive model learns
                                                                       Pons, Werker, & Amano, 2007).
a rudimentary lexicon and a phoneme inventory1 simulta-
                                                                          Our model adopts the Mixture of Gaussians approach from
neously, deciding whether acoustic representations of seg-
                                                                       these previous models but uses a non-parametric Bayesian
mented tokens correspond to the same or different lexical
                                                                       framework that allows extension of the model to the word
items (e.g. bed vs. bad) and whether lexical items contain
                                                                       level, making it possible to investigate the learning outcome
    1 We make the simplifying assumption that phonemes are equiv-      when multiple levels of structure interact. As in previous
alent to phonetic categories, and use the terms interchangeably.       models, speech sounds in our model are represented using
                                                                   2208

                                                                    guson, 1973), C ∼ DP(α, GC ). This distribution encodes bi-
                 A                           D
                            B      C                                ases over the number of categories in the phoneme inven-
                                                                    tory, as well as over phonetic parameters for those categories.
                                                                    Prior beliefs about the number of phonetic categories allow
      time                                                          the learner to consider a potentially infinite number of cate-
                                                                    gories, but produce a bias toward fewer categories, with the
                                                                    strength of the bias controlled by the parameter α.2 This re-
                                                                    places the winner-take-all bias in category assignments that
Figure 1: A fragment of a corpus presented to the model.            has been used in previous models (McMurray et al., 2009;
Asterisks represent speech sounds, and lines represent word         Vallabha et al., 2007) and allows explicit inference of the
boundaries. The model does not know which categories gen-           number of categories needed to represent the data.
erated the speech sounds, and needs to recover categories A,           The prior distribution over phonetic parameters is defined
B, C, and D from the data.                                          by GC , which in this model is a distribution over Gaussian
                                                                    phonetic categories that includes an Inverse-Wishart prior
phonetic dimensions such as steady-state formant values or          over category variances, Σc ∼ IW (ν0 , Σ0 ), and a Gaussian
voice onset time. Words are sequences of these phonetic val-        prior over category means, µc |Σc ∼ N(µ0 , Σν0c ). The param-
ues, where each phoneme corresponds to a single discrete set        eters of these distributions can be thought of as pseudodata,
(e.g. first and second formant) of phonetic values. A sample        where µ0 , Σ0 , and ν0 encode the mean, covariance, and num-
fragment of a toy corpus is shown in Figure 1. The phoneme          ber of speech sounds that the learner imagines having already
inventory has four categories, labeled A, B, C, and D; five         assigned to any new category. This prior distribution over
words are shown, representing lexical items ADA, AB, D,             phonetic parameters is not central to the theoretical model,
AB, and DC, respectively. Learning involves using the speech        but rather is included for ease of computation; the number of
sounds and, in the case of an interactive learner, information      speech sounds in the pseudodata is made as small as possible3
about which other sounds appear with them in words, to re-          so that the prior biases are overshadowed by real data.
cover the phonetic categories that generated the corpus.               Presented with a sequence of acoustic values, the learner
                                                                    needs to recover the set of Gaussian categories that generated
   Simulations compare two models that differ in the hy-
                                                                    those acoustic values. Gibbs sampling (Geman & Geman,
pothesis space they assign to the learner. In the dis-
                                                                    1984), a form of Markov chain Monte Carlo, is used to re-
tributional model, the learner’s hypothesis space contains
                                                                    cover examples of phoneme inventories that an ideal learner
phoneme inventories, where phonemes correspond to Gaus-
                                                                    believes are likely to have generated the corpus. Speech
sian distributions of speech sounds in phonetic space. In
                                                                    sounds are initially given random category assignments, and
the lexical-distributional model, the learner considers these
                                                                    in each sweep through the corpus, each speech sound in turn
same phoneme inventories, but considers them only in con-
                                                                    is given a new category assignment based on all the other cur-
junction with lexicons that contain lexical items composed of
                                                                    rent assignments. The probability of assignment to category
sequences of phonemes. This allows the lexical-distributional
                                                                    c is given by Bayes’ rule,
learner to use not only phonetic information, but also infor-
mation about the words that contain those sounds, in recover-                            p(c|wi j ) ∝ p(wi j |c)p(c)                  (1)
ing a set of phonetic categories.
                                                                    where wi j denotes the phonetic parameters of the speech
Distributional Model                                                sound in position j of word i. The prior p(c) is given by
                                                                    the Dirichlet process and is
In the distributional model, a learner is responsible for re-                       (
covering a set of phonetic categories, which we refer to as a                             nc
                                                                                       ∑c nc +α
                                                                                                 for existing categories
phoneme inventory C, from a corpus of speech sounds. The                    p(c) =         α                               (2)
                                                                                       ∑ nc +α
                                                                                                 for a new category
                                                                                           c
model ignores all information about words and word bound-
aries, and learns only from the distribution of speech sounds       making it proportional to the number of speech sounds
in phonetic space. Speech sounds are assumed to be produced         nc already assigned to that category, with some proba-
by selecting a phonetic category c from the phoneme inven-          bility α of assignment to a new category. The like-
tory and then sampling a phonetic value from the Gaussian           lihood p(wi j |c) is obtained by integrating over all pos-
associated with that category. Categories differ in their means     sible  means and covariance matrices for category c,
µc , covariance matrices Σc , and frequencies of occurrence.
                                                                    RR
                                                                         p(wi j |µc , Σc )p(µc |Σc )p(Σc )dµc dΣc , where the probability
   Following previous work in morphology (Goldwater, Grif-          distributions p(µc |Σc ) and p(Σc ) are modified to take into ac-
fiths, & Johnson, 2006), word segmentation (Goldwater, Grif-        count the speech sounds already assigned to that category.
fiths, & Johnson, in press), and grammar learning (John-                2 This bias is needed to induce any grouping at all; the maximum
son, Griffiths, & Goldwater, 2007), learners’ prior beliefs         likelihood solution assigns each speech sound to its own category.
about the phoneme inventory are encoded using a non-                    3 To form a proper distribution, ν needs to be greater than d − 1,
                                                                                                          0
parametric Bayesian model called the Dirichlet process (Fer-        where d is the number of phonetic dimensions.
                                                                 2209

This likelihood function has the form of a multivariate t-           with a probability proportional to the number of times that
distribution and is discussed in more detail in Gelman, Carlin,      lexical item has already been seen, with some probability β
Stern, and Rubin (1995). Using this procedure, category as-          reserved for the possibility of seeing a new lexical item. The
signments converge to the posterior distribution on phoneme          likelihood is a product of the likelihoods of each speech sound
inventories, revealing an ideal learner’s beliefs about which        having been generated from its respective category,
categories generated the corpus.
                                                                                           p(wi |k) = ∏ p(wi j |ck j )                    (5)
Lexical-Distributional Model                                                                              j
This non-parametric Bayesian framework has the advantage             where j indexes a particular position in the word and ck j is
that it is straightforward to extend to hierarchical structures      the phonetic category that corresponds to position j of lexical
(Teh, Jordan, Beal, & Blei, 2006), allowing us to explore the        item k. Any lexical item with a different length from the word
influence of words on phonetic category acquisition. In the          wi is given a likelihood of zero, and samples from the prior
lexical-distributional model, the learner recovers not only the      distribution on lexical items are used to estimate the likeli-
same phoneme inventory C as in the distributional model, but         hood of a new lexical item (Neal, 1998).
also a lexicon L with lexical items composed of sequences of            The second sweep uses Bayes’ rule
phonemes. This creates an extra step in the generative pro-
cess: instead of assuming that the phoneme inventory gen-                               p(c|w{k} j ) ∝ p(w{k} j |c)p(c)                   (6)
erates a corpus directly, as in the distributional model, this
                                                                     to assign a phonetic category to position j of lexical item k,
model assumes that the phoneme inventory generates the lex-
                                                                     where w{k} j is the set of phonetic values at position j in all
icon and that the lexicon generates the corpus. The corpus is
                                                                     of the words in the corpus that have been assigned to lexical
generated by selecting a lexical item to produce and then sam-
                                                                     item k. The prior p(c) is the same prior over category assign-
pling an acoustic value from each of the phonetic categories
                                                                     ments as was used in the distributional model, and is given
contained in that lexical item.
                                                                     by Equation 2. The likelihood p(w{k} j |c) is again computed
   The prior probability distribution over possible lexicons is
                                                                     by integrating    over all possible means and covariance ma-
a second Dirichlet process, L ∼ DP(β, GL ) where GL defines a                RR
prior distribution over lexical items. This prior favors shorter
                                                                     trices,     ∏wi ∈k p(wi j |µc , Σc )p(µc |Σc )p(Σc )dµc dΣc , this time
                                                                     taking into account phonetic values from all the words as-
lexical items, assuming word lengths to be generated from a
                                                                     signed to lexical item k. The sampling procedure converges
geometric distribution, and assumes that a category for each
                                                                     on samples from the joint posterior distribution on lexicons
phoneme slot has been sampled from the phoneme inventory
                                                                     and phoneme inventories, allowing learners to recover both
C. Thus, the prior probability distribution over words is de-
                                                                     levels of structure simultaneously.
fined according to the phoneme inventory, and the learner
needs to optimize the phoneme inventory so that it generates          Qualitative Behavior of an Interactive Learner
the lexicon. Parallel to the bias toward fewer phonetic cate-
                                                                     In this section, toy simulations demonstrate how a lexicon can
gories, the model encodes a bias toward fewer lexical items
                                                                     provide disambiguating information about overlapping cat-
but allows a potentially infinite number of lexical items.
                                                                     egories that would be interpreted as a single category by a
   Presented with a corpus consisting of isolated word tokens,
                                                                     purely distributional learner. We show that it is not the sim-
each of which consists of a sequence of acoustic values, the
                                                                     ple presence of a lexicon, but rather specific disambiguating
language learner needs to recover the lexicon and phoneme
                                                                     information within the lexicon, that increases the robustness
inventory of the language that generated the corpus. Learning
                                                                     of category learning in the lexical-distributional learner.
is again performed through Gibbs sampling. Each iteration
                                                                        Corpora were constructed for these simulations using four
now includes two sweeps: one through the corpus, assign-
                                                                     categories labeled A, B, C, and D, whose means are located
ing each word to the lexical item that generated it, and one
                                                                     at -5, -1, 1, and 5 along an arbitrary phonetic dimension (Fig-
through the lexicon, assigning each position of each lexical
                                                                     ure 2 (a)). All four categories have a variance of 1. Because
item to its corresponding phoneme from the phoneme inven-
                                                                     the means of categories B and C are so close together, being
tory. In the first sweep we use Bayes’ rule to calculate the
                                                                     separated by only two standard deviations, the overall distri-
probability that word wi corresponds to lexical item k,
                                                                     bution of tokens in these two categories is unimodal.
                      p(k|wi ) ∝ p(wi |k)p(k)                (3)        To test the distributional learner, 1200 acoustic values were
                                                                     sampled from these categories, with 400 acoustic values sam-
Parallel to Equation 2, the prior is                                 pled from each of Categories A and D and 200 acoustic values
                                                                     sampled from each of Categories B and C. Results indicate
                  (     nk
                     ∑k nk +β
                                for existing categories              that these distributional data are not strong enough to disam-
          p(k) =         β                                   (4)     biguate categories B and C, leading the learner to interpret
                     ∑ n +β
                                for a new category
                       k k                                           them as a single category (Figure 2 (b)).4 While this may
where nk is the number of word tokens already assigned to                4 Simulations in this section used parameters α = β = 1, µ = 0,
                                                                                                                                      0
lexical item k. A word is therefore assigned to a lexical item       Σ0 = 1, and ν0 = 0.001; each simulation was run for 500 iterations.
                                                                 2210

 (a)                    Original Categories       (b)                  Distributional Model
                                                                                                       In this model it is non-minimal pairs, rather than minimal
       Frequency                                        Frequency
                    A                         D                                                     pairs, that help the lexical-distributional learner disambiguate
                              B    C
                                                                                                    phonetic categories. While minimal pairs may be useful when
                   −5           0          5                        −5           0          5
                   Location in Acoustic Space                       Location in Acoustic Space      a learner knows that two similar sounding tokens have differ-
 (c) Lexical−Distributional Model, Corpus 1       (d) Lexical−Distributional Model, Corpus 2        ent referents, they pose a problem in this model because the
       Frequency                                        Frequency
                                                                                                    learner hypothesizes that similar sounding tokens represent
                                                                                                    the same word. Thiessen (2007) has made a similar obser-
                   −5           0          5
                   Location in Acoustic Space
                                                                    −5           0          5
                                                                    Location in Acoustic Space
                                                                                                    vation with 15-month-olds in a word learning task, showing
                                                                                                    that infants may fail to notice a difference between similar-
Figure 2: Toy data with two overlapping categories as (a) gen-                                      sounding object labels, but are better at discriminating these
erated, (b) learned by the distributional model, (c) learned by                                     words when familiarized with non-minimal pairs that contain
the lexical-distributional model from a minimal pair corpus,                                        the same sounds.
and (d) learned by the lexical-distributional model from a cor-
pus without minimal pairs.                                                                                          Learning English Vowels
                                                                                                    The prototypical examples of overlapping categories in natu-
                                                                                                    ral language are vowel categories, such as the English vowel
be due in part to the distributional learner’s prior bias toward
                                                                                                    categories from Hillenbrand, Getty, Clark, and Wheeler
fewer categories, simulations in the next section will show
                                                                                                    (1995) shown in Figure 4 (a).5 We therefore use English
that the gradient descent learner from Vallabha et al. (2007),
                                                                                                    vowel categories to test the lexical-distributional learner’s
which has no such explicit bias, shows similar behavior.
                                                                                                    ability to disambiguate overlapping categories that are based
   Two toy corpora were constructed for the lexical-
                                                                                                    on actual phonetic category parameters.
distributional model from the 1200 phonetic values sampled
above. The corpora differed from each other only in the dis-                                           Two corpora were constructed using phonetic categories
tribution of these values across lexical items. The lexicon                                         based on the Hillenbrand et al. (1995) vowel formant data.
of the first corpus contained no disambiguating information                                         Categories in the first corpus were based on vowels spoken
about speech sounds B and C. It was generated from six lex-                                         by men, and had only moderate overlap (Figure 3 (a)); cat-
ical items, with identities AB, AC, DB, DC, ADA, and D.                                             egories in the second corpus were based on vowels spoken
Each lexical item was repeated 100 times in the corpus for a                                        by men, women, and children, and had a much higher degree
total of 600 word tokens. In this corpus, Categories B and C                                        of overlap (Figure 4 (a)). In each case, means and covari-
appeared only in minimal pair contexts, since both AB and                                           ance matrices for the twelve phonetic categories were com-
AC, as well as both DB and DC, were words. As shown in                                              puted from corresponding vowel tokens. Using the generative
Figure 2 (c), the lexical-distributional learner merged cate-                                       model, a hypothetical set of lexical items consisting only of
gories B and C when trained on this corpus. Merging the                                             vowels was generated for each corpus, and 5,000 word tokens
two categories allowed the learner to condense AB and AC                                            were generated based on this lexicon from the appropriate set
into a single lexical item, and the same happened for DB and                                        of Gaussian category parameters.
DC. Because the distribution of these speech sounds in lex-                                            These corpora were given as training data to three mod-
ical items was identical, lexical information could not help                                        els: the lexical-distributional model, the distributional model,
disambiguate the categories.                                                                        and the multidimensional gradient descent algorithm used by
   The second corpus contained disambiguating information                                           Vallabha et al. (2007).6 Results for the corpus based on
about categories B and C. This corpus was identical to the first                                    men’s productions are shown in Figure 3, and results from the
except that the acoustic values representing the phonemes B                                         corpus based on all speakers’ productions are shown in Fig-
and C of words AC and DB were swapped, converting these                                             ure 4. In each case, the lexical-distributional learner recov-
words into AB and DC, respectively. Thus, the second cor-                                           ered the correct set of vowel categories and successfully dis-
pus contained only four lexical items, AB, DC, ADA, and D,                                          ambiguated neighboring categories. In contrast, the models
and there were now 200 tokens of words AB and DC. Cat-                                              lacking a lexicon mistakenly merged several pairs of neigh-
egories B and C did not appear in minimal pair contexts, as                                             5 These vowel data were obtained through download from
there was a word AB but no word AC, and there was a word                                            http://homepages.wmich.edu/˜hillenbr/.
DC but no word DB. The lexical-distributional learner was                                               6
                                                                                                     Parameters
                                                                                                                    for
                                                                                                                       the Bayesian
                                                                                                                                        models were α = β = 1, µ0 =
able to use the information contained in the lexicon in the                                             500              1 0
second corpus to successfully disambiguate categories B and                                            1500 , Σ0 = 0 1 , and ν0 = 1.001, and each simulation
C (Figure 2 (d)). This occurred because the learner could cat-                                      was run for 600 iterations. No attempt was made to optimize these
                                                                                                    parameters, and they were actually different from the parameters
egorize words AB and DC as two different lexical items sim-                                         used to generate the data, as α = β = 10 was used to help produce
ply by recognizing the difference between categories A and                                          a corpus that contained all twelve vowel categories. Using the gen-
D, and could use those lexical classifications to notice small                                      erating parameters during inference did not qualitatively affect the
                                                                                                    results. Parameters for the gradient descent algorithm were identical
phonetic differences between the second phonemes in these                                           to those used by Vallabha et al. (2007); optimizing the learning rate
lexical items.                                                                                      parameter produced little qualitative change in the learning outcome.
                                                                                                 2211

 (a)                               Vowel Categories (Men)                      (b)                          Lexical−Distributional Model          (a)                         Vowel Categories (All Speakers)    (b)                         Lexical−Distributional Model
                                                                                                                                                                       200                                                             200
                      300                                                                           300
                                   i                                                                                                                                                                       u
                                                                       u                                                                                               400             i    ɪ        ɝ                                 400
                      400                                                                           400                                                                                                  Ț o
 First Formant (Hz)                                                                                                                               First Formant (Hz)
                                               ɪ
                                                                               First Formant (Hz)                                                                                                                First Formant (Hz)
                                                       ɝ           Ț                                                                                                   600         e
                      500              e                                   o                        500                                                                                                                                600
                                                                                                                                                                                             ɛ       ʌ     ɔ
                                           æ       ɛ                                                                                                                                   æ
                      600                                      ʌ                                    600                                                                800                                                             800
                                                                       ɔ
                      700                                                                           700                                                                1000                      a                                    1000
                                                           a
                      800                                                                           800
                                                                                                                                                                       1200                                                           1200
                      900                                                                           900
                            2500         2000   1500    1000                                              2500     2000   1500    1000                                          3000     2000      1000                                      3000     2000      1000
                                       Second Formant (Hz)                                                       Second Formant (Hz)                                              Second Formant (Hz)                                          Second Formant (Hz)
 (c)                                   Distributional Model                    (d)                          Gradient Descent Algorithm            (c)                              Distributional Model          (d)                         Gradient Descent Algorithm
                                                                                                                                                                        200                                                           200
                      300                                                                           300
                                                                                                                                                                        400                                                           400
                      400                                                                           400
 First Formant (Hz)                                                            First Formant (Hz)                                                 First Formant (Hz)                                             First Formant (Hz)
                                                                                                    500                                                                 600                                                           600
                      500
                      600                                                                           600                                                                 800                                                           800
                      700                                                                           700                                                                1000                                                           1000
                      800                                                                           800
                                                                                                                                                                       1200                                                           1200
                      900                                                                           900                                                                         3000     2000      1000                                      3000     2000      1000
                            2500         2000   1500    1000                                              2500     2000   1500    1000
                                       Second Formant (Hz)                                                       Second Formant (Hz)                                              Second Formant (Hz)                                          Second Formant (Hz)
Figure 3: Ellipses delimit the area corresponding to 90%                                                                                      Figure 4: Ellipses delimit the area corresponding to 90% of
of vowel tokens for Gaussian categories (a) computed from                                                                                     vowel tokens for Gaussian categories (a) computed from all
men’s vowel productions from Hillenbrand et al. (1995) and                                                                                    speakers’ vowel productions from Hillenbrand et al. (1995)
learned by the (b) lexical-distributional model, (c) distribu-                                                                                and learned by the (b) lexical-distributional model, (c) distri-
tional model, and (d) gradient descent algorithm.                                                                                             butional model, and (d) gradient descent algorithm.
boring vowel categories. Positing the presence of a lexi-                                                                                     not wish to suggest that a purely distributional learner cannot
con therefore showed evidence of helping the ideal learner                                                                                    acquire phonetic categories. The simulations presented here
disambiguate overlapping vowel categories, even though the                                                                                    are instead meant to demonstrate that in a language where
phonological forms contained in the lexicon were not given                                                                                    phonetic categories have substantial overlap, an interactive
explicitly to the learner.                                                                                                                    system, where learners can use information from words that
    Pairwise accuracy and completeness measures were com-                                                                                     contain particular speech sounds, can increase the robustness
puted for each learner as a quantitative measure of model                                                                                     of phonetic category learning.
performance (Table 1). For these measures, pairs of vowel
tokens that were correctly placed into the same category were                                                                                                                                             Discussion
counted as a hit; pairs of tokens that were incorrectly assigned                                                                              This paper has presented a model of phonetic category acqui-
to different categories when they should have been in the                                                                                     sition that allows interaction between speech sound and word
same category were counted as a miss; and pairs of tokens                                                                                     categorization. The model was not given a lexicon a priori,
that were incorrectly assigned to the same category when                                                                                      but was allowed to begin learning a lexicon from the data at
they should have been in different categories were counted                                                                                    the same time that it was learning to categorize individual
as a false alarm. The accuracy score was computed as                                                                                          speech sounds, allowing it to take into account the distribu-
        hits                                            hits
hits+false alarms and the completeness score as hits+misses .                                                                                 tion of speech sounds in words. This lexical-distributional
Both measures were high for the lexical-distributional learner,                                                                               learner outperformed a purely distributional learner on a cor-
but accuracy scores were substantially lower for the purely                                                                                   pus whose categories were based on English vowel cate-
distributional learners, reflecting the fact that these models                                                                                gories, showing better disambiguation of overlapping cate-
mistakenly merged several overlapping categories.                                                                                             gories from the same number of data points.
    Results suggest that as predicted, a model that uses the                                                                                     Infants learn to segment words from fluent speech around
input to learn word categories in addition to phonetic cate-                                                                                  the same time that they begin to show signs of acquiring
gories produces better phonetic category learning results than                                                                                native language phonetic categories, and they are able to
a model that only learns phonetic categories. Note that the                                                                                   map these segmented words onto tokens heard in isolation
distributional learners are likely to show better performance                                                                                 (Jusczyk & Aslin, 1995), suggesting that they are perform-
if they are given dimensions beyond just the first two formants                                                                               ing some sort of rudimentary categorization on the words
(Vallabha et al., 2007) or if they are given more data points                                                                                 they hear. Infants may therefore have access to information
during learning. These two solutions actually work against                                                                                    from words that can help them disambiguate overlapping cat-
each other: as dimensions are added, more data are necessary                                                                                  egories. If information from words can feed back to constrain
to maintain the same learning outcome. Nevertheless, we do                                                                                    phonetic category learning, the large degree of overlap be-
                                                                                                                                           2212

                            Lexical-                Gradient         Sharon Goldwater, Mark Johnson, and members of the com-
                                         Distrib.
                             Distrib.               Descent          putational modeling reading group for helpful comments and
     (a)   Accuracy           0.97        0.63       0.56            discussion.
           Completeness       0.98        0.93       0.94
     (b)   Accuracy           0.99        0.54       0.40                                        References
           Completeness       0.99        0.85       0.95            Boer, B. de, & Kuhl, P. K. (2003). Investigating the role of infant-
                                                                       directed speech with a computer model. Acoustics Research Let-
                                                                       ters Online, 4(4), 129-134.
Table 1: Accuracy and completeness scores for learning               Bortfeld, H., Morgan, J. L., Golinkoff, R. M., & Rathbun, K. (2005).
vowel categories based on productions by (a) men and (b) all           Mommy and me: Familiar names help launch babies into speech-
speakers. For the Bayesian learners, these were computed at            stream segmentation. Psychological Science, 16(4), 298-304.
                                                                     Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum
the annealed solutions; for the gradient descent learner, they         likelihood from incomplete data via the EM algorithm. Journal
were based on maximum likelihood category assignments.                 of the Royal Statistical Society, B, 39, 1-38.
                                                                     Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric
                                                                       problems. Annals of Statistics, 1(2), 209-230.
tween phonetic categories may not be such a challenge as is          Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995).
                                                                       Bayesian data analysis. New York: Chapman and Hall.
often supposed.                                                      Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs dis-
   In generalizing these results to more realistic learning sit-       tributions, and the Bayesian restoration of images. IEEE-PAMI,
uations, however, it is important to take note of two simpli-          6, 721-741.
                                                                     Goldwater, S., Griffiths, T. L., & Johnson, M. (2006). Interpolat-
fying assumptions that were present in our model. The first            ing between types and tokens by estimating power-law generators.
key assumption is that speech sounds in phonetic categories            Advances in Neural Information Processing Systems 18.
follow the same Gaussian distribution regardless of phonetic         Goldwater, S., Griffiths, T. L., & Johnson, M. (in press). A Bayesian
                                                                       framework for word segmentation: Exploring the effects of con-
or lexical context. In actual speech data, acoustic character-         text. Cognition.
istics of sounds change in a context-dependent manner due            Hillenbrand, J., Getty, L. A., Clark, M. J., & Wheeler, K. (1995).
to coarticulation with neighboring sounds (e.g. Hillenbrand,           Acoustic characteristics of American English vowels. Journal of
                                                                       the Acoustical Society of America, 97(5), 3099-3111.
Clark, & Nearey, 2001). A lexical-distributional learner hear-       Hillenbrand, J. L., Clark, M. J., & Nearey, T. M. (2001). Effects of
ing reliable differences between sounds in different words             consonant environment on vowel formant patterns. Journal of the
might erroneously assign coarticulatory variants of the same           Acoustical Society of America, 109(2), 748-763.
                                                                     Johnson, M., Griffiths, T. L., & Goldwater, S. (2007). Adaptor gram-
phoneme to different categories, having no other mechanism             mars: a framework for specifying compositional nonparametric
to deal with context-dependent variability. Such variability           Bayesian models. Advances in Neural Information Processing
may need to be represented explicitly if an interactive learner        Systems 19.
                                                                     Jusczyk, P. W., & Aslin, R. N. (1995). Infants’ detection of the
is to categorize coarticulatory variants together.                     sound patterns of words in fluent speech. Cognitive Psychology,
   A second assumption concerns the lexicon used in the                29, 1-23.
vowel simulations, which was generated from our model.               Jusczyk, P. W., Houston, D. M., & Newsome, M. (1999). The begin-
                                                                       nings of word segmentation in English-learning infants. Cognitive
Generating a lexicon from the model ensured that the                   Psychology, 39, 159-207.
learner’s expectations about the lexicon matched the struc-          Maye, J., Werker, J. F., & Gerken, L. (2002). Infant sensitivity
ture of the lexicon being learned, and allowed us to examine           to distributional information can affect phonetic discrimination.
                                                                       Cognition, 82, B101-B111.
the influence of lexical information in the best case scenario.      McMurray, B., Aslin, R. N., & Toscano, J. C. (2009). Statistical
However, several aspects of the lexicon, such as the assump-           learning of phonetic categories: Computational insights and limi-
tion that phonemes in lexical items are selected independently         tations. Developmental Science, 12(3), 369-378.
                                                                     Neal, R. M. (1998). Markov chain sampling methods for Dirichlet
of their neighbors, are unrealistic for natural language. In fu-       process mixture models. Technical Report No. 9815, Department
ture work we hope to extend the present results using a lexi-          of Statistics, University of Toronto.
con based on child-directed speech.                                  Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006). Hier-
                                                                       archical Dirichlet processes. Journal of the American Statistical
   Infants learn multiple levels of linguistic structure, and it       Association, 101, 1566-1581.
is often implicitly assumed that these levels of structure are       Thiessen, E. D. (2007). The effect of distributional information
acquired sequentially. This paper has instead investigated             on children’s use of phonemic contrasts. Journal of Memory and
                                                                       Language, 56(1), 16-34.
the optimal learning outcome in an interactive system using          Toscano, J. C., & McMurray, B. (2008). Using the distributional
a non-parametric Bayesian framework that permits simulta-              statistics of speech sounds for weighting and integrating acoustic
neous learning at multiple levels. Our results demonstrate             cues. In B. C. Love, K. McRae, & V. M. Sloutsky (Eds.), Pro-
                                                                       ceedings of the 30th Annual Conference of the Cognitive Science
that information from words can lead to more robust learning           Society (p. 433-438). Austin, TX: Cognitive Science Society.
of phonetic categories, providing one example of how such            Vallabha, G. K., McClelland, J. L., Pons, F., Werker, J. F., & Amano,
interaction between domains might help make the learning               S. (2007). Unsupervised learning of vowel categories from infant-
                                                                       directed speech. Proceedings of the National Academy of Sci-
problem more tractable.                                                ences, 104, 13273-13278.
                                                                     Werker, J. F., & Tees, R. C. (1984). Cross-language speech percep-
Acknowledgments. This research was supported by NSF                    tion: Evidence for perceptual reorganization during the first year
grant BCS-0631518, AFOSR grant FA9550-07-1-0351, and                   of life. Infant Behavior and Development, 7, 49-63.
NIH grant HD32005. We thank Joseph Williams for help in
working out the model and Sheila Blumstein, Adam Darlow,
                                                                 2213

