UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An ART Neural Network Model of Discrimination Shift Learning
Permalink
https://escholarship.org/uc/item/639417gw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Berkeljon, Arjan
Coffey, Emily
Raijmakers, Maartje
et al.
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                An ART Neural Network Model of Discrimination Shift Learning
                                    Maartje E. J. Raijmakers (m.e.j.raijmakers@uva.nl)
                                       Department of Psychology, University of Amsterdam
                                             Emily Coffey (emily@flighteducation.net)
                                                  CSCA, University of Amsterdam
                                        Claire Stevenson (CStevenson@fsw.leidenuniv.nl)
                                                  CSCA, University of Amsterdam
                                           Jasper Winkel (jasperwinkel@gmail.com)
                                                  CSCA, University of Amsterdam
                                            Arjan Berkeljon (Arjan@Berkeljon.com)
                                       Brigham Young University (BYU) in Provo, UT, USA
                             Abstract                                    (non)reversal-shift learning and reviews by [4-7]). In the
   We present an ART-based neural network model (adapted from            current paper we focus on the developmental findings
[2]) of the development of discrimination-shift learning that            related to the difference between reversal shifts (RS) and
models the trial-by-trial learning process in great detail. In           nonreversal shifts (NRS). After a RS, all contingency
agreement with the results of human participants (4–20 years of          relations are reversed leaving the same stimulus dimension
age) in [1] the model revealed two distinct learning modes in the
learning process: (1) a discontinuous rational learning process by       relevant for discrimination (e.g., black is correct instead of
means of hypothesis testing; and (2) a slow, yet discontinuous           white). A NRS makes a formerly irrelevant dimension
learning process. Categorical differences in behavior are the result     relevant, shifting only two out of four contingency relations
of uniformly distributed dimensional preferences. In addition, it
models the developmental differences between reversal and                (e.g., circle is correct instead of white).
nonreversal-shift learning. The network implements attention-               Developmental differences in discrimination learning, the
guided learning by selective sensory processing based on                 first phase of the task, are mainly manifested as an increase
dimensional preferences mediated through reinforcement. The              in learning efficiency from childhood to young adulthood
developmental differences consist of separate adjustment of the
valuation of negative reinforcement, which is proposed in the            [6]. In the Levels-of-function theory two distinct learning
empirical neuroscience literature [3].                                   modes are posited to account for the observed difference:
                                                                         (1) an hypothesis-testing, rational learning mode; and (2) a
   Keywords:      discrimination-shift     learning,   cognitive
   development, neural network.                                          slow and incremental learning mode.
                                                                            Subsequent analysis of these hypothesized learning modes
                        Introduction                                     has led to a refining of their proposed nature. A finite
                                                                         mixture model analysis on the error distribution of simple
   Discrimination-shift learning is a long-standing paradigm             discrimination learning data gathered from subjects aged 6
in the investigation of human category learning and concept              to 10 was conducted in [8]. The error distribution was found
formation. A typical discrimination-shift learning task                  to be bimodal, i.e. the distribution was best described as
consists of two phases: discrimination learning and shift                composed of two components. One component best
learning. Discrimination learning requires participants to               describes a model of learning through hypothesis testing.
choose the correct stimulus from a pair of stimuli. The                  The other component included slow learners who did not
stimuli presented differ on two or more dimensions (e.g.                 show any increase in performance during the first 16 trials
color and shape). Each of the dimensions has two possible                (although criterion was reached in 48 trials).
values (e.g. white and black; circle and triangle), resulting in            These findings are extended by a more detailed
four possible stimuli. Reinforcement contingencies are such              investigation of the aforementioned learning modes [1].
that only the choice of one value on a particular dimension,             Data from a simple discrimination-learning task (230
the so-called relevant dimension, is reinforced positively.              subjects, ages 4--20) was analyzed by fitting several
Choosing the opposing value on the relevant dimension                    mathematical learning models to the sequences of responses
leads to negative reinforcement. The other dimension is                  produced by the participants, i.e. the trial-by-trial learning
irrelevant with respect to reinforcement and thus choices                process was modeled. Learning models were specified as
based on either value on this dimension lead to positive                 latent Markov models (or mixtures thereof). The best fit on
reinforcement in 50% of the choices made.                                the data was obtained by a mixture model with a component
   During the second, shift phase of the task, which starts              of discontinuous rational learning through hypothesis testing
once a specified learning criterion has been reached in the              and a component of slow, yet discontinuous learning (as
first phase, reinforcement contingencies are shifted.                    opposed to incremental learning). The relative proportions
Numerous developmental differences have been found in                    of the components depend on age and relevant dimension
comparing performance of children and adults on                          (with respect to reinforcement contingencies) in the
discrimination-shift tasks (see the literature on                        following two ways: (1) the proportion of rational learners
                                                                     791

increases with age; and (2) in the younger age groups, the          researchers argue that overtraining leads to differences in
proportion of rational learners is higher when the relevant         children and adults on these tasks. This coincides with
dimension is color than when the relevant dimension is              findings in which overtraining preschoolers leads task
shape. In the older age groups there was no such difference.        performance similar to that of older subjects [5,6]. This is
   Several phenomena in childrens’ and adults’                      implemented in the network through a lowering of the
discrimination-shift learning (DSL) are evident in the              allowed discrepancy between desired input and output. This
literature. One basic effect, with regard to discrimination         lowered score-threshold fine-tunes pattern discrimination
learning in adults (and children older than 10), is that RS are     through extensive training, leading to fewer learning trials
executed more quickly than NRS.                                     for child as opposed to adult networks; this is in contrast
   In contrast, for children below 10, some studies show that       with humans. The results do reflect empirical data in that RS
the NRS require fewer trials to learn than the RS, whereas          are learned more quickly than NRS in adult networks and
other studies fail to show any difference [5,6,9,12]. Another       equally quickly in child networks. The authors did not
relevant finding is that learning RS on a child’s ‘preferred’       compare the trial-by-trial learning in the pre-shift phase of
dimension (e.g. color as opposed to shape) results in better        the network. However, it is typically expected that these
performance than on a relatively less salient dimension             incremental learning models show an incremental learning
relative to NRS; this was not found in experiments with             process in contrast to the all-or-none learning observed by
adult subjects. Finally, the overtraining effect is also            humans, both children and adults.
apparent, where training, continued after the learning
criterion is reached, facilitates RS but not NRS in children.       Model specification
   The purpose of this paper is to model the two modes of           To model sequential learning behavior on a simple
discontinuous learning and the basic developmental results          discrimination task, an ART (Adaptive Resonance Theory,
of discrimination-shift learning using a neural network             [23]) neural network architecture was chosen that was
architecture with incremental-learning rules. The network           adapted to simulate perseveration behavior on the
architecture will be implemented such that a cognitively            Wisconsin Card Sorting Task (WCST) [2]. The original
plausible and developmentally relevant parameter accounts           architecture consists of an input-to-category node structure
for systematic variation in learning behavior.                      mediated by an attentional gating system of biases acting
                                                                    selectively on dimensional attributes in the input. Biases are
Models of Discrimination-Shift Learning                             mediated by so-called habit nodes which detect how often
Krushke [9] developed the AMBRY connectionist model of              classifications have been made irrespective of feedback.
DSL that qualitatively fits adult data. Two important               Bias activation is controlled by a reinforcement signal. This
components are contextual bias and response-to-category             signal is mediated by the reinforcement gain parameter.
mapping. The biased attention to input leads to preservation        Lowering the value of this parameter will increase the
of dimensional attention, which would account for ease of           likelihood of dimensional perseveration through the bias
RS over NRS. The separation of responses from the internal          nodes. It thus models a decrease in the ability to adjust
category structure is considered essential for shift learning.      categorization behavior based on changing reinforcement
   Raijmakers, et al. [10] created a neural network utilizing       contingencies.
the backpropagation error correction algorithm and one              The model was adapted to perform a simple discrimination
hidden layer to model DSL. The networks often learned               task. Two major structural changes were made:
NRS faster than RS, although a trial-by-trial analysis              1. The reinforcement gain parameter was split in two to
performed similarly to preschool children during shift                   reflect the different influences of positive (α+) and
learning. Overtraining however did not help the network to               negative (α-) reinforcement on responding. Frank et al.
perform as adults, but also not as children. This lead to the            [15] made a similar distinction. They report empirical
conclusion that these particular feedforward networks that               support for the dissociation between learning from
have been applied as models of higher cognitive tasks such               positive reinforcement and learning from negative
as balance-scale learning, being bottom-up associative                   reinforcement in a study on how dopamine affects these
systems, are inadequate models of human learning as it is                distinct learning processes in Parkinson's patients. They
involved in discrimination learning, as they cannot                      present a computional model [16, 17] that accounts for
incorporate the top-down categorization of adults and                    this dissociation by proposing that two distinct
children on discrimination shift learning.                               dopamine-dependent pathways exist in the basal
   Sirois & Shultz [12] use a cascade-correlation algorithm              ganglia (BG), one excitatory pathway for positive
to overcome the shortcomings of previous DSL neural                      reinforcement learning and one inhibitory pathway for
network models. Given that these feedforward networks can                negative reinforcement learning. These pathways are
adapt their topology, and therefore allow for structural                 part of a larger model incorporating cortical structures
plasticity, they are of great relevance in modeling                      as well as the basal ganglia and thalamus. The basal
developmental phenomena. Essentially, the architecture of                ganglia are influenced top-down by the orbitofrontal
the adult and child networks do not differ in DSL modeling               cortex (OFC) which represents positive and negative
as discrimination shifts are linearly separable problems. The
                                                                792

     reinforcement values separately in the medial OFC and        dimensional preference of the network. Bias activation is
     lateral OFC, respectively.                                   determined by six factors. A bias node is excited by (1) its
2. A layer of two response nodes was added on top of the          own activation, (2) positive reinforcement mediated by the
     category layer. This allows the network to differentiate     learning parameter, (3) by a match-signal computed
     not only between categories, but also between                between input and the category node with the highest
     responses to categories (as in AMBRY where internal          activation value, and (4) by the corresponding habit node
     category knowledge is separated from overt category          (provided its activation exceeds a specified threshold). It is
     responses in a similar manner). A stochastic process         inhibited by (5) the competing bias node and by (6) negative
     based on the negative reinforcement gain parameter (α-)      reinforcement. The separation of bias and habit subsystems
     controls the update of the connections between the           in the current model is an implementation of the idea that
     category nodes and the response nodes.                       memory for reinforcement value and (motor) response
Learning in the ART network takes place in two ways: (1)          memory are functions of interacting, but distinct,
updating the bias nodes and (2) updating the weights from         subsystems in the brain [2]. A similar distinction has been
category to response nodes. The final network topology is         shown to account for different types of errors subjects make
given in figure 1 (A full specification is supplied as an         on the WCST, namely failures to maintain set and
appendix). As can be seen a total number of four nodes code       perseverative errors [14]. Cortical-subcortical computational
for input features. There are two nodes per dimension of the      models have also been developed that make a similar
discrimination task (color and shape). Input signals              distinction (e.g. [13,16,17]; see also [20] for a
propagate to category nodes, one for each of the four             neurobiological discussion).
possible stimuli (white circle, white triangle, black circle,     In [18] a neural model of perseveration is developed based
and black triangle). The category nodes mutually inhibit          on a distinction between active (in the prefrontal cortex) and
each other. Signals from the input to the category nodes are      latent memory representations (in the posterior cortex). This
mediated by attentional gating through bias node activation.      is comparable to, although not identical with, the distinction
Response nodes determine the response of the network by           between active bias memory and latent habit memory in the
comparing the activation values of the category nodes using       current model. For any category-learning task (e.g. WCST,
weighted connections between category and response nodes.         discrimination learning etc.) distinguishing between
These weights are randomly initialized per dimensional pair       perseverative errors and errors of set-maintenance seems to
as either zero or one. This thus expresses a response             require a distinction between a memory system that can
preference within each dimension.                                 implement active and flexible responses and a memory
                                                                  system for latent representations of previous responses. This
                                                                  is what we have attempted to implement using the bias and
                                                                  habit subsystems.
                                                                     With respect to the current model, these two views are not
                                                                  radically different. The bias system is the driving force
                                                                  behind dimensional preference and thus responding in the
                                                                  network. Mediation through reinforcement controls
                                                                  response shifting (i.e. new response-behavior). The habit
                                                                  system keeps track of the responses gives and influences the
                                                                  bias-system to continue in its previous response-behavior
                                                                  (i.e. maintaining response-behavior).
                                                                  Modeling Development Whereas originally reinforcement
                                                                  mediation was used to model differences in perseveration
                                                                  behavior on the WCST between healthy subjects and frontal
                                                                  patients [2], in the current study such mediation is used to
                                                                  model developmental differences in simple discrimination-
                                                                  shift learning. It has been observed that the perseverative
                                                                  errors seen in frontal patients behavior are comparable to the
                                                                  mistakes children make [21]. Our hypothesis is that these
                                                                  mistakes are the result of a reduced effect of corrective
             Figure 1: ART discrimination network.
                                                                  feedback, i.e. children have trouble shifting from one rule to
                                                                  another because responding to changing reinforcement
   Attentional gating of signals from input to category nodes
                                                                  contingencies is not as efficient in children as it is in adults.
is done through bias nodes acting on input dimensions. Bias
                                                                     We test this hypothesis by modeling a simple discrimi-
nodes are initialized uniformly random as either high or low
                                                                  nation task (which is similar to the WCST) using the
such that if one bias has a value greater than two, the other
                                                                  specified neural network model. Developmental differences
bias node is smaller than two by the same amount. The
deviation from two is uniform. This expresses an initial
                                                              793

are modeled using different values for the negative feedback         the data. For the simulation data generated by the adult
component of the reinforcement gain parameter.                       networks, a single component discontinuous-learning model
  As mentioned, individual networks differ with respect to           provided the best fit. The learning parameter, i.e. the
their dimensional preference (bias) and response preference          probability of moving from the presolution state to the
per dimension (response nodes). These differences are not,           learned state, was estimated at 0.19. For the data generated
however, related in any systematic way to the value of the           by the child networks, a two-component model consisting of
reinforcement gain parameter. Therefore, any difference in           two discontinuous learning modes provided the best fit. For
performance between adult and child networks will be the             the slow, discontinuous learning component (describing
result of a difference in negative reinforcement gain.               70% of the data), the learning parameter was estimated at
                                                                     0.064. For the fast learning component the learning
                 Simulations and Results                             parameter was estimated at 0.35. Estimated parameters were
The networks were submitted to RS and NRS shift tasks.               very similar to the parameter estimates in [1].
Stimuli follow one of four learning rules: (1) black maps to
A, white maps to B, (2) white maps to A, black maps to B,
(3) triangle maps to A, circle maps to B and (4) circle maps
to A, triangle maps to B. Through a binary encoded vector
of one of four stimuli, a black or white triangle or circle,
was presented to the network. The network then chooses
response category A or B and is given reinforcement
dependent upon correct categorization. One fourth of each
developmental group began with each rule. Initial
dimensional preference was uniformly distributed over all
networks via random initialization. The networks were
given 48 trials to reach a learning criterion of 10
consecutive correct responses for this initial discrimination-
learning rule. The rule was then shifted with either a
reversal or non-reversal rule and 48 trials were presented.
Networks that failed to reach the 10 sequentially correct              Figure 2: Backward learning curves of simulation data.
criteria were removed from analyses (as is standard                  Note that the peak at trial 0 is an artifact of the method
procedure in human studies). Further details and parameter           because all sequences are aligned at the last error.
values are presented in the Appendix.
                                                                     Backward Learning Curves The discontinuity of the
  Discrimination Learning An analysis of variance on the             learning process is apparent from the backward learning
number of trials in the preshift phase reveals a main effect         curves. As can be seen from figure 3 for both adult (a) and
of age (i.e. adult versus child) on the number of trials. Adult      child (b) networks the curve is flat at the 0.5 level before
networks learn significantly faster than child networks. F(1;        learning (left of the dashed line). It then jumps to near zero
418) = 466.46, p < 0.001. An interaction effect between              once the criterion is mastered (right of the dashed line). This
initial bias and rule was also found. Networks with an initial       clearly indicates the existence of a discontinuous learning
bias-matching rule (i.e. the initial bias matches the rule to be     process.
learned) learn significantly faster than networks with an
initial bias not matching rule (non-matching networks). F(3;
418) = 160.63, p < 0.001. Finally, an interaction effect was
found between age, initial bias and rule. F(3; 418) = 67.09,
p < 0.001. That is, the difference in number of trials to
criterion between matching and non-matching networks is
greater for child networks than for adult networks (a similar
effect is reported in [1]). The reduced effect of corrective
feedback for child networks thus magnifies, as expected, the
differences in learning rate between matching and non-
matching networks.
Markov Model Analysis Several hidden Markov models
were fit to the trial-by-trial data generated by adult and child
networks. These statistical models include single component
models of incremental learning and of discontinuous
learning. In addition also combination of all single                      Figure 3: Shift learning in adult and child networks
component models, i.e. two-component models, are fitted to
                                                                 794

Discrimination-Shift Learning The mean number of trials             whereby child networks with further training beyond
to learning criterion post-shift is less for adult than child       criterion would perform like adult networks.
networks. An ANOVA with number of trials to learning
criterion as dependent variable and age (adult/child), rule                            Acknowledgments
matching bias (yes/no) and shift type (reversal/non-reversal)       The work of Maartje Raijmakers was funded by the
revealed a significant main effect for age: F(1,625)=586.22,        Netherlands Organisation of Dutch research (NWO). We
p<.001. Matching nor shift type revealed main effects,              thank Meindert Kamphuis and Vincenzo Truppo, Master
F(1,625)=1.83, p=.176 and F(1,625)=1.92, p=.678                     students of the Cognitive Science Center Amsterdam, for
respectively. A significant effect of shift type on trials-to-      their valuable contributions.
criterion is found for adult networks: F(1,398)=67.15,
p<.001. No significant result for child networks was found:
F(1,398)=.869, p=.176. See Figure 3.
                                                                                            References
                                                                    [1] V. Schmittmann, I. Visser, and M. Raijmakers,
                                                                    “Multiple learning modes in the development of
                        Conclusion                                  performance on a rule-based category learning task,”
                                                                    Neuropsychologia, vol. 44, pp. 2079–2091, 2006.
                                                                    [2] D. Levine and P. Prueitt, “Modeling some effects of
We have presented a neural network model of the                     frontal lobe damage—novelty and perseveration,” Neural
development of discrimination-shift learning that shows             networks, vol. 2, pp. 103– 116, 1989.
discontinuous learning in distinct learning modes. The              [3] van Duijvenvoorde, A.C.K., Zanolie K., Rombouts,
effectiveness of negative reinforcement was varied between          S.A.R.B., Raijmakers, M.E.J., and Crone E.A. (2008).
adult networks (high) and child networks (low). Networks            Evaluating the Negative or Valuing the Positive? Neural
modeling adult performance learn fast and are modeled by a          Mechanisms Supporting Feedback-Based Learning across
fast, discontinuous learning process. The learning process is       Development. J. Neurosci. 28: 9495-9503 .
discontinuous for all child networks, and is best modeled by        [4] J. Wolff, “Concept-shift and discrimination-reversal
a two-component model with components for fast and slow             learning in humans,” Psychological Bulletin, vol. 68, no. 6,
learners. That is, only a subset of the child networks learn        pp. 369–408, 1967.
slower than the adult networks. Differences in learning rate        [5] N. Esposito, “Review of discrimination shift learning in
were caused by an interaction between initial bias and the          young children,” Psychological Bulletin, vol. 82, pp. 432–
rule to be learned. Child networks are less able to switch          455, 1975.
dimensional preference and category-to-response connec-             [6] T. Kendler, “The development of discrimination
tions because of the decreased influence of negative                learning: A levels-of-functioning explanation,” Advances in
reinforcement compared to the adult networks. For child             Child Development and Behavior, vol. 13, pp. 83–117,
networks this results in a mixture of learning modes as             1979.
found in the empirical study [1]. Note that the two-                [7] T. Kendler, Levels of cognitive development. Mahwah,
component structure of the trial-by- trial learning data of the     NJ: Erlbaum, 1995.
child networks is the result of a uniform distribution of           [8] M. Raijmakers, C. Dolan, and P. Molenaar, “Finite
initial dimensional preferences. That is, a continuous              mixture distribution models of simple discrimination
variation in networks’ initial state results in categorical         learning,” Memory & Cognition, vol. 29, no. 5, pp. 659–
difference in performance.                                          677, 2001.
   The implementation of the stochastic process within the          [9] J. Kruschke, “Dimensional relevance shifts in category
category-to-response connection based on the negative               learning,” Connection Science, vol. 8, pp. 201–223, 1996.
reinforcement parameter provided the required different-            [10] Raijmakers, M.E.J., Van Koten, S., & Molenaar,
tiation between stages of development after the shift has           P.C.M. (1996): On the validity of Simulating Stagewise
occurred. As expected, child networks executed shifts               Development by means of PDP Networks: Application of
slower than adult networks. Also, reversal shifts were easier       Catastrophe Analysis and an Experimental Test of Rule-
than non-reversals for adult networks, yet for child networks       Like Behavior. Cognitive Science, 20 (1): 101-136.
no significant difference was found. These results concur           [11] J. Kruschke, “Alcove: An exemplar-based
with empirical evidence (e.g. Espirito, 1975). From the             connectionist model of category learning,” Psychological
model we can predict that children have a larger variation in       Review, vol. 99, pp. 22–44, 1992.
the number of trials they need for shift learning than adults.      [12] S. Sirois and T. Shultz, “Neural network modeling of
This would also explain the inconsistent results presented in       developmental effects in discrimination shifts,” Journal of
literature.                                                         Experimental Child psychology, vol. 71, pp. 235–274, 1998.
   A next step towards a more complete model of                     [13] R. O’Reilly, D. Noelle, T. Braver, and J. Cohen,
discontinuous discrimination-shift learning is a network that       “Prefrontal cortex and dynamic categorization tasks:
is able to induce its own representational categories based         Representational organization and neuromodulatory
purely on input. In addition, future research could examine         control,” Cerebral Cortex, vol. 12, pp. 246–257, 2002.
whether the overtraining effect can also be reproduced,
                                                                795

[14] I. Visser, M. Raijmakers, and P. Molenaar, “Fitting                Feature nodes xi activate category nodes yj weighted by zij
hidden markov models to psychological data,” Scientific              and bias nodes Ωk (k=1,2: 1=color, 2=shape):
Programming, vol. 10, pp. 185–199, 2002.
[15] M. Frank, L. Seeberger, and R. O’Reilly, “By carrot or                                                            (3)
by        stick:   Cognitive   reinforcement    learning     in
parkinsonism,” Science, vol. 306, pp. 1940–1943, 2004.                  with I=100 and g defined by:
[16] M. Frank, B. Loughry, and R. O’Reilly, “Interactions
between frontal cortex and basal ganglia in working                                                                             (4)
memory: a computational model,” Cognitive, Affective and
Behavioral Neuroscience, vol. 1, pp. 137–160, 2001.                     Weights zij and zji between x and y are fixed. If i=j , zij=5
[17] M. Frank and E. Claus, “Anatomy of a decision:                  and otherwise zij=0.; zji=zij/5 resulting in 0 and 1 values. The
Striato-orbitofrontal interactions in reinforcement learning,        category node yi with the highest activation value and the
decision making and reversal,” Psychological Review, vol.            negative reinforcement gain α-determines the response a
113, pp. 300–326, 2006.                                              weighted by the weights between the two, initialized randomly
[18] Morton, J.B. & Munakata, Y. (2002). Active versus               at 1 or 0 and updated by:
latent representations: A neural network model of
perseveration, dissociation, and decalage. Developmental                                                       (5)
psychobiology, 40, 255 – 265.
 [19] G. Kaplan, N. Sengör, H. Gürvit, I. Genc¸, and C.                 with S=6.5 and ychoice represents y node activity where
Güzelis¸, “A composite neural network model for                      ychoicej=1 if yj=max(y) and 0 otherwise. The stochastic
                                                                     process added to augment response choice samples from a
perseveration and distractibility in the Wisconsin card
                                                                     uniform random distribution between 0 and 6.5; if the value is
sorting test,” Neural Networks, vol. 19, pp. 375–387, 2006.
                                                                     < α- then σ is 1 and otherwise 0. Weights wyr are updated as
[20] H. Atallaha, M. Frank, and R. O’Reilly, “Hippcampus,
                                                                     follows:
cortex and basal ganglia: Insights from computational
models of complementary learning systems,” Neurobiology
                                                                                                                            (6)
of Learning and Memory, vol. 82, pp. 253–267, 2004.
[21] E. Crone, K. Ridderinkhof, M. Worm, R. Somsen, and
                                                                        with tr set to 1 if response is correct and 0 if incorrect. The
M. van der Molen, “Switching between spatial stimulus-               bias node activates itself, is inhibited by the competing bias
response mappings: a developmental study of cognitive                node and is activated by the corresponding habit node and the
flexibility,” Developmental Science, vol. 7, no. 4, pp. 443–         reinforcement value mediated by positive and negative
455, 2004.                                                           reinforcement, α+ and α- respectively.
[22] Berkeljon, A. & Raijmakers, M.E.J. (2007). An ART
Neural Network Model of Discrimination Learning.                                                                            (7)
Proceedings of the IEEE ICDL, Imperial college London,
11 – 13 july 2007.
[23] G. Carpenter and S. Grossberg, “A massively parallel
architecture for a self-organizing neural pattern recognition           with E=.01, F=3, G=1.1 and θ1=1. The reinforcement
machine,” Computer, Vision, Graphics and Image                       value is 1 for correct and -1 for incorrect response. Initial bias
Processing, vol. 37, pp. 54–115, 1987.                               values are sampled from a uniform random distribution
                                                                     between 1.7 and 2.3 for adult and child networks. Positive and
              Appendix: Network Specification                        negative reinforcement gain is set to 1.2 and 8 for adult
The network specification agrees with [22] with a few                networks and 1.2 and .8 for child networks. Variable is a
modifications for this DSL model. Input Ii is set to 5 when          match signal occurring between input I and category node y:
feature i (i=1,2,3,4: 1=black, 2=white, 3=triangle, 4=circle) is
present in the stimulus and 0 otherwise. The feature nodes                                                  (8)
xi=1,2,3,4 are activated depending on Ii and category node
activation, represented by yj (j=1,2,3,4: 1=black, 2=white,             in which i is the index of feature node x. J is the index of
3=triangle, 4=circle) and weighted by zji:                           max(y), and k the index of the corresponding bias node. Ii is
                                                                     the input signal (5 or 0). Activation of the habit nodes is given
                                                         (1)         by with H=.1, J=3 and θ2=.5.
                                                                                                                   (9)
   with A=10, B=5, C=1, D=1 and f defined by:
                                           (2)
                                                                 796

