UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Navigating word association norms to extract semantic information
Permalink
https://escholarship.org/uc/item/63z135z5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Arenas, Alex
Borge-Holthoefer, Javier
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

              Navigating word association norms to extract semantic information
                                         Javier Borge-Holthoefer (javier.borge@urv.cat)
                             Department of Computer Science and Mathematics, Avda. Paı̈sos Catalans,
                                                    Tarragona, 43007 Catalonia, Spain
                                              Alex Arenas (alexandre.arenas@urv.cat)
                             Department of Computer Science and Mathematics, Avda. Paı̈sos Catalans,
                                                    Tarragona, 43007 Catalonia, Spain
                             Abstract                                   (Steyvers et al., 2004; Andrews, Vigliocco, & Vinson, 2005;
   We present a simple model that allows the extraction of se-
                                                                        Silberman, Bentin, & Miikkulainen, 2007).
   mantic similarity relations from free association information.          The description of semantic knowledge as a complex net-
   In our study, we use two acclaimed databases of linguis-
   tic relationships between pairs of words, feature-based and          work of interactions between words, does not suffice to get a
   association-based. We apply a complex networks methodol-             clear picture of the specific relations between complex net-
   ogy to disentangle feature based relationships on top of a free
   association network. As a consequence, we broaden complex            works representing different semantic empirical data sets.
   networks’ applications in the field of psycholinguistics, from a     One of the main reasons for this is that while the notion of
   merely descriptive to a predictive level. Results are systemat-      node is quite uncontroversial (in our case a word), the con-
   ically compared to those of two powerful well-known compu-
   tational models (LSA and WAS).                                       cept of edge is not so because it must be committed to a
   Keywords: complex networks; random walks; Latent Seman-              definition of relationship. In what semantics is concerned,
   tic Analysis; Word Association Space; free association norms;        we can consider that a word is related to another one if they
   feature production norms
                                                                        belong to the same class (category-related, such as car and
                         Introduction                                   wagon); or if they tend to co-occur in many contexts (car
                                                                        and road); or if they have a cause-effect relationship (fire and
The problem of semantic representation has been one of the
                                                                        smoke), and so on. For some of these types of relationship
focus of attention of the cognitive psychology community
                                                                        there exist empirical data that quantify how strong two words
over the past decades. Although computational approaches to
                                                                        are related. (Notice that two words may have several of these
such problem were proposed as soon as the late ’60s (Collins
                                                                        relationships).
& Quillian, 1969; Collins & Loftus, 1975), only in the past
decade technology has made it feasible to deal with huge                   It is clear that different semantic networks will arise de-
amounts of empirical data, where models could be tested                 pending on the type of association used to link words by the
with reliability. It is in this scenario where the most pow-            subjects of a cognitive experiment. Moreover, given the in-
erful models have emerged (Lund & Burgess, 1996; Lan-                   tricate complexity of human mind, the more free the asso-
dauer & Dumais, 1997; Blei, Ng, & Jordan, 2003; Griffiths               ciation scenario, the more rich the types of relationship will
& Steyvers, 2004; Steyvers, Shiffrin, & Nelson, 2004; Grif-             appear. These different association scenarios can reflect se-
fiths, Steyvers, & Tenenbaum, 2007). Along with these, re-              mantic or episodic memory contents, depending on the ex-
cent studies have used also the perspective offered by the the-         periment. One of the main challenges is to understand the in-
ory of complex networks to gain insight on language-related             teraction between both memory representations. In Steyvers
problems (Sigman & Cecchi, 2002; Steyvers & Tenenbaum,                  et al. (2004) the authors propose the prediction of semantic
2005). The main idea behind these works is to map em-                   similarity effects in episodic memory using empirical data.
pirical data onto a graph (usually called complex network)              The procedure applied is a modification of the general LSA
that summarizes the observed relations between words in a               scheme, using singular value decomposition and multidimen-
given experiment. Once the structure is set up, it is possible          sional scaling over a specific data set (Nelson, McEvoy, &
to statistically characterize it (with a wide range of existing         Schreiber, 1998). The results show the emergence of fea-
descriptors) and elucidate properties that can help to better           ture association groups in a multidimensional space known
understand the large-scale structure of semantic relations in           as Word Association Space (WAS).
the specific set.                                                          We will consider the same problem from a complex net-
   However, while the network approach has been merely                  work perspective adding a different interpretation of the dis-
descriptive up to now, computational models like LSA                    entanglement process with plausible cognitive implications.
(Landauer & Dumais, 1997), HAL (Lund & Burgess, 1996),                  In our work, this prediction is reformulated in the following
WAS (Steyvers et al., 2004) or the more recent Topic Model              terms: whether is possible to disentangle similarity relation-
(Griffiths & Steyvers, 2004) have an intrinsic predictive capa-         ships from general association words networks by the nav-
bility. In particular, some of these models are used to reveal          igation of the semantic network. We address this question
interaction between episodic and semantic memory, consid-               assuming that: (1) Each available data set is a partial expo-
ering empirical data that reflects the impact of environmen-            sure to semantic knowledge; (2) Some data sets are more
tal (i.e. nonlinguistic) experience upon linguistic phenomena           general than others, they grasp the heterogeneity of the se-
                                                                    2777

mantic knowledge more precisely; and (3) as a consequence            weights represent the frequency of association in the sample.
of (2), some information from a less general data set might          Although free association data is often transformed into sym-
be partially implicit in a more general one. We will construct       metric information (as in Steyvers et al., 2004), FA has been
upon these hypothesis and propose an algorithm that allows           treated here in its original form.
the disentanglement of a type of relationship embedded on the           Generally speaking, Free-Association Norms (FA from
structure of a more general association network. In particu-         now on) represent a more complex scenario than FP when
lar, we will focus on two well-known data sets in English: the       considering the semantics of edges. FA is heterogeneous by
free-association database constructed by Nelson et al. (1998),       construction, it may grasp any relation between words e.g.
and the semantic feature production norms by McRae, Cree,            a causal-temporal relation (fire and smoke), an instrumental
Seidenberg, and McNorgan (2005). Interestingly, the algo-            relation (broom and floor) or a conceptual relation (bus and
rithm takes advantage of both a probabilistic and a semantic         train), among others. This heterogeneity will be on the ba-
space approach.                                                      sis of our approach because we assume that some similarity
                                                                     information is implicit in FA.
Feature Production Norms description
Feature Production Norms (FP from now on) were produced                    The Random Inheritance Model (RIM)
by McRae et al. by asking subjects to conceptually recognize         Our specific goal is to propose a computational model to ex-
features when confronted with a certain word. This feature           tract a FP-like network from the track of a dynamical process
collection is used to build up a vector of characteristics for       upon FA. Dynamics is in this framework related to the naviga-
each word, where each dimension (vector component) rep-              tion of the network (diffusion process), whereas temporal dy-
resents a feature, with a value that represents its production       namics (growth and change) of the network is not considered
frequency across participants. These norms include 541 con-          in this work (see Steyvers & Tenenbaum, 2005 for a complex
cepts, for which semantic closeness or similarity is computed        network approach to this problem).
as the cosine (overlap) between pairs of vectors of character-          The idea is to simulate a naı̈ve cognitive navigation on top
istics. The cosine is obtained as follows:                           of a general association semantic network to relate words
                                                                     with a certain similarity, in particular we want to recover
                        v1 w1 + v2 w2 + . . . + vn wn
                cos θ =                                      (1)     feature similarities. We schematize this process as uncorre-
                                k v kk w k                           lated random walks from node to node that propagate an in-
that is, the dot product between two concept vectors v and w,        heritance mechanism among words, converging to a feature
divided by the product of their lengths.                             vectors network. Our intuition about the expected success of
   As a consequence, words like banjo and accordion are very         our approach relies on two facts: the modular structure of
similar (i.e. they have a projection close to 1) because their       the FA network surely retains significant meta-similitude re-
vector representations show a high overlap, essentially pro-         lationships, and random walks are a the most simple dynami-
voked by their shared features as musical instruments, while         cal processes capable of revealing the local neighborhoods of
the vectors for banjo and spider are very different, showing         nodes when they persistently get trapped into modules. The
an overlap close to 0 (almost orthogonal vectors).                   inheritance mechanism is a simple reinforcement of similari-
   We will represent this information under the form of a net-       ties within these groups. We call this algorithm the Random
work, where each node represents a word, and an edge (or             Inheritance Model (RIM).
link) is set up between a pair of nodes whenever their vectors          The results obtained show macro-statistical coincidences
projection is different from 0. The meaning of an edge in this       (functional form of the distributions and descriptors) between
network is thus the features similarity between two words.           the real and the synthetic FP network, moreover, the model
The network is undirected (since similarity is symmetric) and        yields also significant success at the microscopic level, i.e.
weighted by the value of the projections.                            is able to reproduce to a large extent FP empirical relation-
                                                                     ships. These results support the general hypothesis about im-
Free-Association Norms description                                   plicit entangled information in FA, and also reveals a possi-
Nelson et al. produced these norms by asking over 6000 par-          ble mechanism of navigation to recover feature information
ticipants to write down the first word (target) that came to         in semantic networks. Finally, we compare these results with
their mind when confronted with a cue (word presented to             those obtained using LSA and WAS on the same data sets.
the subject). The experiment was performed using more than              FA and FP norms can be represented as semantic networks
5000 cues. Among other information, a frequency of coin-             of words, which in turn can be analyzed in terms of descrip-
cidence between subjects for each pair of words is obtained.         tors (see last section). Both empirical networks are topologi-
As an example, words mice and cheese are neighbors in this           cally different, that is, the statistical local and global proper-
database, because a large fraction of the subjects related this      ties differ significantly from each other. The main differences
target to this cue. Note, however, that the association of these     are concerned to the sparsity of FA, in contrast to the strong
two words is not directly represented by similar features but        density of FP. Since our goal is to compare a synthetic net-
other relationships (in this case mice eat cheese). The net-         work obtained from FA, to FP up to a microscopic level, we
work empirically obtained is directed and weighted, where            need both networks to have the same nodes (words). To this
                                                                 2778

end, we have extracted from the databases those words that              The process is performed in parallel, i.e. the update of the
appear in both, which has left two subnetworks of 376 nodes             feature vectors is done after completion of the inheritance for
each (polysemous words, such as bat, were left aside). The              every word. At the end, we have a synthetic vector of features
statistical characteristics of the extracted subnetworks do not         for every word in the network.
differ very much from their complete versions but they do                  3. Averaging:
between them, see Table 1. From now on, the subnetworks of              Once the feature vectors have been computed, we build up a
376 words common in FA and FP will be used for comparison               synthetic feature similarity network. The network is the result
purposes.                                                               of projecting all pairs of vectors and prescribing a weighted
                                                                        link between two words according to this projection. The
Table 1                                                                 whole process is iterated (by simulating several runs) up to
Main statistical descriptors of the networks FA and FP data,            convergence of the average of the synthetic feature similarity
and their respective common words’ subnetworks (FA-s and                networks generated at each run. The average, after conver-
FP-s). N is the number of nodes; hsi is the average strength;           gence, is the synthetic feature similarity network we compare
L is the average shortest path length; D is the diameter of the         with FP.
network; C is clustering coefficient and r is the assortativity
coefficient (see last section).
                                                                           Let us define the transition probability of the FA network.
  Descriptor     FA (whole)      FP (whole)     FA-s         FP-s       The elements of FA (ai j ) correspond to frequency of first
       N            5018             541        376          376        association reported by the participants of the experiments.
      hsi            0.77           20.20       0.26        13.43       However, note that the 5018 words that appear on the data
       L             3.04           1.68        4.41         1.68       set are not all the words that appeared at the experiment, but
       D              5                5          9            3        only those that where at the same time cues in the experiment.
       C           0.1862          0.6344      0.1926       0.6253      That means that the data have to be normalized before hav-
        r           0.097          0.2609      0.3258       0.2951      ing a transition probability matrix. We define the transition
                                                                        probability matrix P as:
   Finally, it is worthwhile to mention the fact that the                                                    ai j
databases, although they both belong to the psycholinguistic                                        Pi j =                           (3)
                                                                                                           ∑ j ai j
field, they were created in different places and years (affect-
ing the use of language); a different number of subjects were              Note that this matrix is asymmetric, as well as the original
used to build up the norms (affecting the robustness of data),          matrix FA. We maintain this asymmetry property in our ap-
etc. Even the intention (i.e. the type of problem they seek             proach to preserve the meaning of the empirical data. Once
to tackle) of the collections is different. It is important to          the matrix P is constructed, the random walkers of different
realize about all these facts in order to understand the amount         lengths are simply represented by powers of P. For example,
of uncertainty any model faces when trying to reproduce a               if we perform random walks of length 2, after averaging over
particular empirical dataset.                                           many realizations we will converge to the transition matrix
                                                                        P2 , every element (P2 )i j represents the probability of reach-
   Keeping in mind all these general considerations we can
                                                                        ing j, from i, in 2 steps, and the same applies to other length
move on to specify how our model works. In what follows,
                                                                        values. The inheritance process proposed, corresponds, in
we first specify the logic behind our proposal and, after, we
                                                                        this scenario, to a change of basis, from the canonical basis
describe the mathematical framework that unifies the differ-
                                                                        of the N-dimensional space, to the new basis in the space of
ent steps. The main logic stages in RIM are:
                                                                        transitions T :
   1. Initialization:
First, every word in the FA network is tagged with an initial                                                  S
vector of characteristics. To avoid initial bias, we choose the                                    T = lim
                                                                                                         S→∞
                                                                                                             ∑ Pi                    (4)
vectors to be orthogonal in the canonical basis. That means                                                  i=1
that every word has associated a vector of N-dimensions, be-               The convergence of Eq.(4) is guaranteed by the Perron-
ing N the number of words in the network, with a component              Frobenius theorem. In practice, the summation in Eq.(4) con-
at 1 and the rest at zero.                                              verges very fast, limiting the dependence on indirect associa-
   2. Navigation and Inheritance:                                       tive strengths (Nelson & Zhang, 2000). We tested the behav-
Then a random walk of S steps1 starting at a node i is per-             ior up to S=10, although with S=4 we already achieve con-
formed. At every step of the walk, we propose an inheritance            vergence in T up to 10−4 in terms of the Hamming distance.
mechanism that changes vi (the initial vector of the word i)            All the results for RIM will be expressed for S = 4 from now
depending on the visited nodes. Let s = s1 , s2 , ..., sn the set of    on. Finally, the matrix that will represent in our model the
visited nodes. Then the new vector for node i is computed as:           feature similarity network (synthetic FP), where similarity is
                                                                        calculated as the cosine of the vectors in the new space, is
                                     n
                         vi := vi + ∑ vs j                       (2)       1
                                                                             A random walk is a time-reversible finite Markov chain, see
                                    j=1                                 (Lovàsz, 1996) for a survey on the topic.
                                                                    2779

given by the scalar product of the matrix and its transpose,              Table 2
FS = T T † .                                                              Statistical parameters for Free Association norms FA (sub-
                                                                          strate of the dynamic process), Feature Production norms FP
                                                                          (empirical target) , and the synthetic networks obtained using
                           Results of RIM                                 Latent Semantic Analysis LSA, Word Association Space WAS
                                                                          and Random Inheritance Model RIM.
In this section we present the performance of the RIM in
disentangling a Feature Production Norm from the empirical                 Descriptor       FA       FP     LSA      WAS       RIM
FA. To compare the results with the empirical FP we define                     N           376      376      376      376       376
a set of measures that can be classified in macroscopic and                   hsi          0.26    13.43    39.60    10.29     15.62
microscopic similarities. To evaluate macroscopic similari-                    L           4.41     1.68    0.02     2.00      1.77
                                                                               D             9        3       2        4         3
ties we will use basic descriptors of complex networks (see
                                                                               C          0.192    0.625    0.961    0.492     0.584
Appendix), the strength distribution P(s), along with average                  r          0.325    0.295    0.125    0.303     0.305
(global) quantities already computed in Table 1. To evaluate
the microscopic similarities we will compute the success rate
on the local structure of the neighborhood of words in both
real and synthetic networks. We will also compare the re-                 Macroscopic similarities
sults of our model with those obtained using the well known               First we plot the cumulative strength distribution of the em-
Latent Semantic Analysis (LSA; Landauer & Dumais, 1997;                   pirical network FP, and the respective synthetic networks pro-
Landauer, McNamara, Dennis, & Kintsch, 2007) and Word                     vided by LSA or by applying WAS and RIM to FA, see Fig-
Association Space (WAS; Steyvers et al., 2004). Although                  ure 1. The statistical agreement between FP and RIM and
LSA’s applicability goes beyond the scope of this work, it                WAS is remarkable. The general observation is that all distri-
stands as an appropriate benchmark model to compare the                   butions present an exponential decay instead of a power-law
performance of our proposal. In particular, we have used                  decay. This specific form of the distributions is characteristic
LSA vector representation based on the corpus TASA for the                of random homogeneous networks. In Table 2 we present the
subset of common words in FA and FP, with a space dimen-                  main descriptors of the four previous networks, plus the sub-
sionality of d = 300. This LSA TASA-based representation                  strate of the dynamic process FA, for comparison purposes.
is suitable for comparison because it has been assessed as                Again, the agreement between the empirical FP and RIM is
a simulation of human vocabulary test synonym judgments                   marked, RIM reproduces with significant accuracy the aver-
(Landauer, Foltz, & Laham, 1998). WAS model is specially                  age strength, the average path length, diameter, clustering and
pertinent for the current comparison because: on one hand,                assortativity, of the FP target network. WAS also succeeds
the model is formally similar to LSA; on the other, it makes              largely on the determination of macroscopic properties of the
use of mediated strength between non-direct associates as in              network, while LSA can not be so similar.
RIM, and has been reported on the same data set we use in
order to extract semantic information. Accordingly, we have               Microscopic similarities
performed the procedure described in the cited article upon               The necessity for a detailed comparison between synthetic
the whole network, and after extracted the mentioned 376                  and empirical sets is double: first, the statistical characteri-
subset of words. We only compare to the best results of WAS               zation presented before is informative and important, but not
for this data set (which correspond to Singular Value Decom-              definitive to state the validity of the model to disentangle ac-
position under S(2), N = 5018, and d = 400, see Steyvers et               tual information in the original FA network. At most, it is
al., 2004 for details).                                                   capable of a correct description of the empirical target’s struc-
                                                                          ture. And second, the difference between our particular net-
         1                               1                                work, and general examples used in complex networks theory
                                                                          is that nodes are tagged and then not interchangeable. The
P(s)    0.1                             0.1                               specific neighborhood of every word matters, because it re-
              FP                              LSA
                                                                          veals semantic relations, and then the degree or the clustering
       0.01                          0.01
                   1       10    100             1         10       100   become less relevant than the specific list of neighbors syn-
          1                             1
                                                                          thetically obtained. Therefore a model dealing with tagged
                                                                          elements can only be predictive if it succeeds at this level of
P(s)    0.1                             0.1
              RIM                             WAS
                                                                          comparison.
       0.01                            0.01
                                                                             To this end, we pursue the evaluation of RIM in predicting
                   1       10    100             1         10       100
                       s                               s                  the specific words of each node’s neighborhood.
Figure 1. Log-log plot of the cumulative strength distribution of the        The first question concerning the models at stake (WAS
networks: Feature Production norms FP (empirical target) , and the
synthetic networks obtained using Latent Semantic Analysis LSA,           and RIM, since they are here built upon FA) is whether they
Word Association Space WAS and Random Inheritance Model RIM.              are useful at all: that is, whether they are capable of uncov-
                                                                          ering semantic relations which were not in FA already (i.e.
                                                                    2780

       4                                                                         85%
                                                  WAS                                                                                          LSA (d=300)
                                                  RIM                                                                                          WAS (d=400)
      3.5
                                                                                 80%                                                           RIM
       3
                                                                                 75%
  d   2.5                                                                    E
                                                                                 70%
       2
      1.5                                                                        65%
       1
            0   50   100     150   200    250   300     350   400                      0   1   2   3   4   5   6     7     8     9   10   11   12   13   14   15
                                   word                                                                            List Length
Figure 2. Inheritance distances for l = 15 for WAS and RIM, evi-         Figure 3. For each synthetic network (LSA, WAS and RIM) we
dencing the fact that both models are capable of capturing semantic      have measured the mean error (for l = 1 to l = 15) against FP, ac-
relationships beyond a word’s immediate neighborhood.                    cording to Eq. 5.
as first neighbors). Figure 2 points at this question by evi-            one. Notice that measure E is strongly increased when a
dencing that both WAS and RIM are actually collecting se-                mismatch appears, whereas misplacements are less punished.
mantic relationships further than d = 1. The bulk of words               In particular, E = 0 when the vectors under comparison are
is inheriting features from approximately d = 2 on average,              identical. In the other extreme, if a vector has only one match
which means that the models are in fact capturing semantic               with the other one, and the matching element is not placed
similarities from a significant distance (it is important to keep        correctly, then E = l, where l is the length of the involved
in mind that FA has a diameter of 5, see Table 1).                       vectors. Beyond this, there exists only a worse situation, i.e.
   A second problem we face on this microscopic evaluation               complete mismatch between vectors. In this case E = ∞.
of the model is that of proposing pertinent measures. We pro-            Since we intend to compute an average error score, we can not
ceed as follows, given a specific word i, we start sorting its           allow an ∞ value, and therefore we prescribe E = l + 1, ex-
neighbors according to their linking strength. We apply this             pressing the fact that such score is worse than any case where
for each word in our data sets forming lists. The reference list,        any match occurs.
is the list of each word in FP, and the lists we want to compare            The error defined in formula 5 is plotted in Figure 3, on
with, are those obtained for each word in the synthetic data             average the error of RIM is about 10% lower than the error of
sets, RIM, WAS and LSA. We restrict our analysis up to the               LSA, and 4% lower than that of WAS.
first 15 ordered neighbors, assuming that these are the most
significant ones. We have designed an expression that assigns                                              Conclusions
an error score between a list and its reference, depending on
                                                                         In this work, we have proposed an algorithm to extract feature
the number of mismatches between both lists, and also on the
                                                                         similarity information from an empirical words’ Free Associ-
number of misplacements in them. A mismatch (M) corre-
                                                                         ation network. Building upon the idea that free association
sponds to a word that exist in the reference list and not in the
                                                                         entangles, in particular, semantic traits of association based
synthetic list and vice versa, these are considered the main
                                                                         on similar characteristics between concepts, we have pro-
errors in our approach. Since the model seeks powerful pre-
                                                                         posed a simple algorithm to disentangle this information. The
dictive capacity, misplacements are also taken into account.
                                                                         results reproduce to a large extent the findings in an empirical
A misplacement (O) is an error in the order of appearance of
                                                                         Feature Production norms network. The simple strategy of a
both words in each list. The error score E is then defined as:
                                                                         random navigation process of the actual FA topology and a
                                        EO                               reinforcement inheritance mechanism suffice to produce re-
                           E = EM +                            (5)       lationships comparable to those experimentally obtained.
                                      l − EM
                                                                            The comparison with the powerful LSA and WAS models
where EM stands for the number of mismatches, EO the num-                is indicative of the level of macroscopic and microscopic suc-
ber of displacements and l the length of the list.                       cess of our proposal, notwithstanding the fact that both these
   This quantity recalls well-known edit distances such as               models provide useful semantic spaces, from a theoretical and
Levenshtein Distance (Levenshtein, 1966) or its general-                 an empirical point of view. Furthermore, beyond the level
ization, Damerau-Levenshtein Distance (Damerau, 1964),                   of success of any of these models, we propose that RIM is
where the similarity between two strings depends on the                  an approach that enriches other existing models, in the sense
amount of insertions/deletions (mismatches in our case) and              that it introduces a dynamical perspective to the formation of
transpositions (movements or misplacements) that one has to              semantic spaces. The random navigation mechanism intro-
perform on a string in order to completely match the other               duced, far from been an optimal strategy in the search space,
                                                                      2781

ought to start exploring dynamic approaches to the problem                 Barabási, A., & Albert, R. (1999). Emergence of scaling in random
of semantic cognition. Finally, the network representation                    networks. Science, 286, 509.
and the dynamics on it provide an intuitive and useful rep-                Blei, D., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation.
                                                                              Journal of Machine Learning Research, 3, 993-1022.
resentation, which can be broadened with more realistic nav-               Boccaletti, S., Latora, V., Moreno, Y., Chavez, M., & Hwang, D.-U.
igation and inheritance strategies.                                           (2006). Complex networks: Structure and dynamics. Phys. Rep.,
                                                                              424, 175-308.
   Assuming that Free Association semantic networks are                    Collins, A., & Loftus, E. (1975). A spreading activation theory of
good exposures of human semantic knowledge, we speculate                      semantic memory. Psychological Review, 82, 407-428.
                                                                           Collins, A., & Quillian, M. (1969). Retrieval time from semantic
that some cognitive tasks can rely on a specific navigation                   memory. Journal of Verbal Learning and Verbal Behavior, 8,
of this network, in particular a simple navigation mechanism                  240-247.
                                                                           Damerau, F. J. (1964). A technique for computer detection and
based on randomness, structure of the network and reinforce-                  correction of spelling errors. comm. ACM.
ment could be enough to reproduce non trivial relationships                Griffiths, T., & Steyvers, M. (2004). Finding scientific topics. Pro-
                                                                              ceedings of the National Academy of Sciences, 101(1), 5228-
of feature similarity between concepts represented as words.                  5235.
Moreover, explicit metadata associated to semantic structural              Griffiths, T., Steyvers, M., & Tenenbaum, J. (2007). Topics in se-
patterns seem to play an important role on information recov-                 mantic representation. Psychological Review, 114(2), 211-244.
                                                                           Landauer, T., & Dumais, S. (1997). A solution to plato’s problem:
ery, that could be extended to other cognitive tasks. Given                   The latent semantic analysis theory of acquisition, induction, and
the already detected importance of modular structure in the                   representation of knowledge. Psychological Review, 104, 211-
                                                                              240.
study of semantic representation (see Topic Model, Griffiths               Landauer, T., Foltz, P. W., & Laham, D. (1998). Introduction to
et al., 2007) we think that disambiguation is perhaps the next                latent semantic analysis. Discourse Processes, 25, 259-284.
                                                                           Landauer, T., McNamara, D., Dennis, S., & Kintsch, W. (Eds.).
affordable challenge along this line of research.                             (2007). Handbook of latent semantic analysis. Mahwah, N.J.:
                                                                              Lawrence Erlbaum Associates.
            About Complex Network Theory                                   Levenshtein, V. I. (1966). Binary codes capable of correcting dele-
                                                                              tions, insertions, and reversals. Soviet Physics Doklad., 10(8),
Complex networks refers to networks (graphs) whose topo-                      707-710.
                                                                           Lovàsz, L. (1996). Random walks on graphs: A survey. Bolyai Soc.
logical characteristics are non-trivial in contrast with simple               Math. Stud., 2, 353-397.
networks where regularities and symmetries dominate their                  Lund, K., & Burgess, C. (1996). Producing high-dimensional se-
                                                                              mantic spaces from lexical co-occurrence. Behavior Research
structure. Complex network are found in the representation                    Methods, Instruments, & Computers, 28, 203-208.
of interacting elements, in many fields of science ranging                 McRae, K., Cree, G., Seidenberg, M., & McNorgan, C. (2005).
                                                                              Semantic feature production norms for a large set of living and
from biology to social sciences (Newman, 2003). Related                       nonliving things. Behavior Research Methods, 37(4), 547-559.
to cognitive science, the application of complex networks has              Mehler, A. (2007). Corpus linguistics. an international handbook.
                                                                              In A. Ludeling & M. Kyto (Eds.), (chap. Large Text Networks as
been specially successful to better understand some aspects                   an Object of Corpus Linguistic Studies). Berlin/New York: de
of brain dynamics in neuroscience (Sporns, Chialvo, Kaiser,                   Gruyter.
& Hilgetag, 2004) and linguistics (Sigman & Cecchi, 2002;                  Motter, A. E., Moura, A. P. S. de, Lai, Y., & Dasgupta, P. (2002).
                                                                              Topology of the conceptual network of language. Physical Re-
Motter, Moura, Lai, & Dasgupta, 2002; Bales & Johnson,                        view E, 65.
2005; Mehler, 2007), and there is evidence that a step further             Nelson, D. L., McEvoy, C. L., & Schreiber, T. A.
                                                                              (1998).       The university of south florida word associa-
to psycholinguistics is also on its way (Steyvers & Tenen-                    tion, rhyme, and word fragment norms.             Available from
baum, 2005; Solé, Corominas, Valverde, & Steels, 2008).                      http://www.usf.edu/FreeAssociation/
                                                                           Nelson, D. L., & Zhang, N. (2000). The ties that bind what is
   Complex networks main assets comprise a wide range of                      known to the recognition of what is new. Psychonomic Bulletin
measures that help on the quantification of its structural char-              and Review, 7, 604-617.
                                                                           Newman, M. E. J. (2003). The structure and function of complex
acteristics, either at a micro (node), meso (group) and macro                 networks. SIAM Review, 45, 167-256.
(network) level. For extensive network theory reviews and                  Sigman, M., & Cecchi, G. (2002). Global organization of the word-
                                                                              net lexicon. Proceedings of the National Academy of Sciences,
foundational works, see Newman, 2003; Watts & Strogatz,                       99(3), 1742-1747.
1998; Barabási & Albert, 1999; Boccaletti, Latora, Moreno,                Silberman, Y., Bentin, S., & Miikkulainen, R. (2007). Semantic
Chavez, & Hwang, 2006.                                                        boost on episodic associations: An empirically-based computa-
                                                                              tional model. Cognitive Science: A Multidisciplinary Journal,
                                                                              31(4), 645-671.
                                                                           Solé, R., Corominas, B., Valverde, S., & Steels, L. (2008). Lan-
Acknowledgments We thank T. L. Griffiths, M. Steyvers, G.                     guage networks: their structure, function and evolution. Trends
Zamora and S. Gómez, for helpful comments. This work has                     in Cognitive Science.
                                                                           Sporns, O., Chialvo, D., Kaiser, M., & Hilgetag, C. (2004). Orga-
been supported by the Spanish DGICYT Project FIS2006-                         nization, development and function of complex brain networks.
13321-C02-02.                                                                 Trends in Cognitive Science, 8, 418-425.
                                                                           Steyvers, M., Shiffrin, R., & Nelson, D. (2004). Experimental cog-
                                                                              nitive psychology and its applications. In A. Healy (Ed.), (p. 237-
                             References                                       249). Washington, D.C: American Psychological Association.
Andrews, M., Vigliocco, G., & Vinson, D. (2005). The role of               Steyvers, M., & Tenenbaum, J. B. (2005). The large-scale struc-
    attributional and distributional information in semantic represen-        ture of semantic networks: statistical analyses and a model of se-
    tation in bara. In B. Bara, L. Barsalou, & M. Bucciarelli (Eds.),         mantic growth. Cognitive Science: A Multidisciplinary Journal,
    Proceedings of the twenty seventh annual conference of the cog-           29(1), 41-78.
    nitive science society.                                                Watts, D., & Strogatz, S. (1998). Collective dynamics of ’small-
Bales, M., & Johnson, S. (2005). Graph theoretic modeling of large-           world’ networks. Nature, 393, 440.
    scale semantic networks. Journal of Biomedical Informatics.
                                                                       2782

