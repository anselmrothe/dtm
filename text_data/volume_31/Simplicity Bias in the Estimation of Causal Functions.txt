UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Simplicity Bias in the Estimation of Causal Functions
Permalink
https://escholarship.org/uc/item/3d85q7zt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Little, Daniel R.B.
Shiffrin, Richard
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                          Simplicity Bias in the Estimation of Causal Functions
                                               Daniel R. Little (drlittle@indiana.edu)
                                            Richard M. Shiffrin (shiffrin@indiana.edu)
                                  Department of Psychological and Brain Sciences, Indiana University
                                               1101 E. 10th St, Bloominton IN 47405 USA
                              Abstract                                  Kalish, Lewandowsky, & Kruschke, 2004). Typically, the
                                                                        focus in function learning is on the type and relative diffi-
   We ask observers to make judgments of the best causal func-          culty of learning different functions; most function learning
   tions underlying noisy test data. This method allows us to ex-
   amine how people combine existing biases about causal rela-          studies have not directly examined prior beliefs. However,
   tions with new information (the noisy data). Participants are        there have been some recent investigations of function biases
   shown n data points representing a sample of noisy data from         in the iterated learning paradigm (see e.g., Griffiths, Kalish,
   a supposed experiment. They generate points on what they
   believe to be the true causal function. The presented functions      & Lewandowsky, 2008; Kalish, Griffiths, & Lewandowsky,
   vary in noise, gaps, and functional form. The method is similar      2007). In an iterated learning task, participants are trained
   to function learning studies, but minimizes the roles of learn-      on stimuli drawn from the learning outcomes of the previous
   ing and memory. To what degree do the participants exhibit
   a bias for simple linear functions? We describe a hierarchical       participant. Each learner is assumed to have a set of hypothe-
   Bayesian polynomial regression model to quantify complex-            ses about the causal relationship between the input and output
   ity. The results show the expected bias for simplicity, but with     variables. A rational learner assigns probabilities to each hy-
   some interesting individual differences.
                                                                        pothesis in accordance with the posterior probability of that
   Keywords: causal models, function learning; prior knowl-
   edge; hierarchical Bayesian regression.                              hypothesis given the data. According to Bayes’ rule the pos-
                                                                        terior probability, p(h|d), is:
   In science generally, in statistical inference, in studies of                                        p(d|h)p(h)
model selection, and in psychological theorizing, we design                                  p(h|d) =                                (1)
                                                                                                           p(d)
our inference approaches to trade off fit to observed data
(models are good that fit well) and complexity (models or               where p(d|h) is the probability or likelihood of the data
explanations that fit or explain everything are bad). In the            given a hypothesis, p(h) is the prior probability of that hy-
present research we explore how observers form causal mod-              pothesis and p(d) is the marginal probability of the data,
els for noisy data by asking observers to estimate functions            ∑i p(d|hi )p(hi ). In an iterated function learning task, re-
as explanations for a set of noisy data points. We ask: How             sponses tend to converge to a positive linear function indicat-
do mental causal models balance fit and complexity? Fur-                ing a bias towards positive linear functions in people’s prior
ther, how does the balance of fit and complexity achieved by            beliefs about functions(Kalish et al., 2007).
observers compare with the balance imposed by a rational                   Non-Bayesian approaches to function learning also pro-
system? We explore the former question by fitting Bayesian              vide hints of a simplicity bias. Humans learn positive lin-
hierarchical models to the causal functions produced by the             ear functions faster than negative linear functions or non-
observers. The latter question is addressed by comparing the            linear functions (Carroll, 1963). One successful theoretical
model for the observers to a Bayesian hierarchical model fit            approach assumes that performance is driven by combining
to the presented noisy data.                                            a pre-existing set of linear functions (i.e., POLE; Kalish et
   Recent research into beliefs about casual relationships              al., 2004). However, the representations formed in function
has demonstrated that people prefer simple explanations                 learning studies are determined in part by cognitive limita-
over more complex explanations (see e.g., Lombrozo, 2006,               tions on, and the processes of, attention, learning, and mem-
2007). In fact, simplicity biases have been forwarded as                ory. For example, the relation between an input value and
underlying many fundamental cognitive processes (Chater                 output value presented at one point in time might be partially
& Vitanyi, 2003). Here we are primarily concerned with                  or wholly forgotten later, and possibly distorted through in-
how people form mental models summarizing relationships                 ference in the direction of simplicity. As another example,
between continuous input and output quantities (i.e., data).            extreme points might be attended and remembered best, lead-
Learning about continuous variables has typically been stud-            ing to a different set of distortions. In the current paper, we
ied as a function learning phenomena. Within this paradigm,             present an experimental method for studying function repre-
input values from a single function are displayed one at a              sentations in which all the data is presented simultaneously,
time, the participant responds with an output value and re-             in a form easy to perceive and process. In this method, one
ceives corrective feedback about the true output value. Over            should not have to parcel out the effects due to faulty mem-
time, participants form some representation about the un-               ory for the data points on the function, or inadequate learn-
derlying functional relationship between the two variables              ing of the function over time. We use the method to address
(i.e., exemplar-based associations, DeLosh, Busemeyer, &                two related questions: do people show a bias towards sim-
McDaniel, 1997; or a mixture of linear function ’experts’,              ple (possibly linear) functional relationships when faced with
                                                                    1157

noisy data generated from a range of different function types?      the distributions at higher levels. For example, if we spec-
and b) how do people’s prior beliefs about functional rela-         ify a probability distribution over polynomial degree, we can
tionships interact with information given to them about the         estimate the shape of this distribution using MCMC; hence,
function? Let h represent a causal functional form. We as-          any bias towards using less complex functions is reflected by
sume that people are rational to the degree that they properly      increased mass over lower degree polynomials.
combine prior expectations, p(h), with information from the             The first step in hierarchical regression is to specify the
problem, p(d|h). In our modeling, we will estimate p(h), and        probability distributions over the parameters within a model
the distribution of likely values of p(h), from the responses.      class to a given set of data (e.g., a single trial j from the cur-
We assume that the distribution of of p(h) is a direct mea-         rent experiment). If we assume that people explicitly repre-
surement of the prior biases.                                       sent functions, then the likelihood of the observed data given
                                                                    a specific model class k is given by:
A Method for Studying Function Biases
                                                                                     p y|β, σ2 , Xk ∼ N Xk β, σ2 I ,
                                                                                                                      
In a typical function learning task, participants are shown                                                                           (2)
values from a single function, values are presented one at
                                                                    where β is the vector of regression coefficients, Xk is the
a time and participants only learn about one function in the
                                                                    matrix of predictor variables for model k, and ∼ means ”is
course of the experiment. In order to separate the applica-
                                                                    distributed as” (in this case, the distribution is a multivariate
tion of functional knowledge from learning and memory, the
                                                                    normal distribution with a specified mean and covariance ma-
current experimental method showed participants a selection
                                                                    trix). Here we consider the models to be polynomials of the
of data points all generated by the same function and pre-
                                                                    form y = ∑ki=0 βi xi + ε where ε ∼ N 0, σ2 .1
sented simultaneously (with Gaussian noise added to obscure
                                                                        Within each particular model class k, if we assume nonin-
the true function; see Figure 1). The participants were asked
                                                                    formative priors over the coefficients, β, and the variance, σ2 ,
to demonstrate their best estimate of the true underlying func-
                                                                    then the posterior probability of the coefficients is given by:
tion by placing a series of points at specified places in the
presented graph. They were told that these response points                                                    
should lie on their best estimate of the underlying causal                                 β ∼ N β̂X, Vβ σ2 ,                         (3)
function for the data on the current trial. The data on each                            −1 T                        −1
trial were drawn from different functions and across trials the     where βˆ = XT X         X y and Vβ = XT X . The posterior
functions varied in number of data points that were displayed       probability of the variance is:
and the amount of Gaussian noise added to the output.
                                                                                       σ2 |y ∼ Inv − χ2 n − k, s2 ,
                                                                                                                     
                                                                                                                                      (4)
                                                                    where n is the number of data points, k is the
                                                                    degree of polynomial under consideration, and s2 =
                                                                                  T         
                                                                      1
                                                                           y − X ˆ
                                                                                 β     y −  X ˆ . Using noninformative priors can
                                                                                              β
                                                                    n−k
                                                                    result in improper posterior distributions unless a) k < n and
                                                                    b) the matrix of predictors, X, is full rank (Gelman, Car-
                                                                    lin, Stern, & Rubin, 2003). The above distributions de-
                                                                    scribe how to determine the posterior probabilities for the pa-
Figure 1: A: Example of ”data” presented to participants dur-       rameters of a given model class given one set of data; the
ing the function estimation task. B: Example of the ”data”          focus of our analysis, however, is on how people choose
presented to participants along with a response column. The         amongst model classes over the entire set of experimental
participant places a ’best’ point with the response column.         data. Hence, the main quantity of interest is the marginal
Many such columns are presented on each trial.                      distribution of the data for each model class k (the prob-
                                                                    ability of the data over all parameter settings within each
                                                                    model, p(y|k) = ∑i p(y|βki , σ2ki )p(βki , σ2ki ) for each problem
Bayesian polynomial regression                                       j weighted by the prior probability of selecting k for each
In a regression problem, the goal is to describe the functional     particular problem).
relationship between two continuous variables. In Bayesian              The marginal probability of the data is used to compute the
terms, we need to compute the posterior probability of each         posterior probability that the model class is k by using Equa-
possible function given the data. The hierarchical approach         tion 1; hence, we also need to define a prior over polynomial
allows us to specify probability distributions to capture our       degrees representing the probability of selecting model k for
uncertainty about not only the best-fitting parameter values        any problem j. We assume a prior distribution over the model
within a model class (e.g., the slope and intercept of a linear     class k given by:
function) but also at higher levels of abstraction, such as the         1 The data, matrix of predictors, the coefficients, the variance,
entire set of model classes (all polynomials). Using Markov         and the number of data points, n, change across trials, j, but for
Chain Monte Carlo (MCMC) techniques we can sample from              notational simplicity we suppress indexing by trial.
                                                                1158

                                                                         for each participant; hence, each participant saw a different
                   k ∼ Categorical (p1 , ..., pK ) .              (5)    instantiation of the base functions. 3
Polynomial degree is thus treated as a categorical variable
with the probability of selecting a particular degree given by
the categorical distribution parameters, p1 , ..., pK . Intuitively,
these parameters index prior belief about model classes of dif-
ferent complexity. If we were solely concerned with selecting
the appropriate model for the data, we could set the multino-
mial parameters to reflect our assumption of a simplicity bias
over the model class (i.e., p1 > p2 > ... > pK ). However,
we want to measure prior bias without committing ourselves
to any a priori assumptions about simplicity; hence, a non-
informative prior distribution over the multinomial parameter
must also be specified. We use the conjugate prior to the cat-
egorical as follows:
                 p1 , ...pK ∼ Dirichlet (α1 , ..., αK )           (6)
where K is the number of different polynomial degrees being
tested and α1 , ..., αK = 1, which is equivalent to a multivari-
ate uniform distribution. MCMC techniques including Gibbs
Sampling (see e.g., Gelman et al., 2003) allow the prior dis-
tribution of the model degree k to be approximated.2
   This analysis when conducted on each participant’s re-
sponses gives a direct measure of the prior expectation for
polynomials of different complexity in the distribution of
p1 , ...pK . This analysis also provides the posterior distribu-
tion of the degrees, (hereafter, kresponse ), for each problem.
When conducted on the data that are shown to the partici-
pants the distribution of the degrees, (hereafter, kdata ) gives
a measure of optimal model selection for each problem (i.e.,
the degree k of the model that best captures the data). By
comparing the data and response distributions of k for each
problem, we can investigate how information from the prob-
lem is combined with prior biases.
                              Method                                     Figure 2: Example of data and responses from three different
                                                                         trials for one participant.
Participants and Design
Five Indiana University psychology graduate students partic-
ipated in the experiment and received $16 dollars reimburse-             Procedure
ment. Each participant responded to 108 functions which
                                                                         On each trial, a data set (e.g., see Figure 1) was presented
varied in a) the number of data points displayed on screen
                                                                         on the screen, and participant estimated the function that
(6, 16, or 52 points),    b) the variance of the Gaussian noise         best summarized the causal relationship between the input
  µ = 0, σ2 ∈ (.25, 1) added to each y-value, and c) the gen-
                                                                         and output values. Their response function was queried by
erating function, which was either a polynomial of 1, 2, 3, 4
                                                                         prompts at several locations on the screen. There were 15
or 5 degrees (i.e., linear, quadratic, cubic, etc), or a sin, tan or
                                                                         successive response requests. Each highlighted an x-value
exponential function. Function coefficients were uniformly
                                                                         for the entire height of the screen (see Figure 1). Participants
generated with replacement from the integers 1 to 10. Differ-
                                                                         responded with their function value for a given x-value by
ent random coefficients and Gaussian noise were generated
                                                                         clicking on their best guess location in the response column
    2 Here we use the pseudo-prior sampling method from Carlin and
Chib (1995) to ensure that the sampling method switches between              3 For each participant, each function was presented three times.
models. The parameter distributions for β are updated from Equa-         Participants tended to respond with functions of the same complex-
tion 3 for each model m when the selected model k 6= m. When             ity across repetitions. However, there was variability in the actual
k = m, β is sampled from the distribution in Equation 3 but with the     function generated, and we therefore do not aggregate across repe-
variance in that distribution multiplied by C, where C << 1; hence,      titions. The proportion of response functions that differed in their
the distribution of the coefficients is more diffuse for model j when    polynomial degree across with problem repetitions was .17, .06, .13,
k = j.                                                                   .06, and .06 for each participant, respectively.
                                                                     1159

with the mouse. During the initial response stage, the se-             two participants, the average intercept value is less than 1,
lected function value was removed until all of the response            which corresponds to the prior distribution of kresponse shown
columns had been presented. After responding to the 15 re-             in Figure 3 (bottom row, columns 3 and 4). The means of
sponse columns, all of the participant’s initial responses were        the kdata distributions are also less than one indicating that
presented simultaneously on the screen, and participants had           for these two participants an increase in the complexity of the
the opportunity to adjust their functions until they were satis-       data is accompanied by an lesser increase in the complexity
fied with the result.                                                  of the response. For the other three participants, the influence
                                                                       of the intercept and the complexity of the data are augmented
                               Results                                 by other factors. For participant #2, the coefficient for the
An example of the data shown to the participants, the initial          estimated noise, σ2P , is also greater than zero indicating a ten-
function response and the final function response are shown            dency to respond with higher kresponse with noisier data (see
in Figure 2. We used the Bayesian regression model outlined            Figure 4, row B). For this participant, the estimated intercept
above to estimate the complexity (i.e., the degree of the poly-        value also accords well with the prior distribution shown in
nomial with the highest mean posterior probability) of the             Figure 3 (bottom row, column 2). Responses from partici-
data and the complexity of each participant’s final response           pants 1 and 5, in addition to the intercept and kdata variables,
function. A scatterplot of the estimated polynomial degrees            are also influenced by the interaction between the complexity
for the data and the responses are shown in Figure 3 (top row).        of the data and the number of data points, kdata × N, reveal-
   The hierarchical Bayesian regression allows examination             ing a tendency to increase the complexity of their responses
of the prior density over the polynomial degree for each par-          for complex data with higher N (see Figure 4, rows A and E).
ticipant (that is, p1 , ..., pk for all degrees k). The prior den-     Participant #1 also showed a tendency to increase response
sities for each participant are shown in Figure 3 (bottom              complexity for noisy complex functions (see Figure 4, row
row). These densities clearly show that all of the participants        A).
demonstrate a bias towards functions of lower polynomial de-
gree. Three of the participants show a higher mass over linear                                    Discussion
functions, while two of the participants show a higher mass            The posterior distributions over kresponses clearly show a bias
over horizontal functions. Consideration of the Bayesian so-           toward simple functions. This bias is warranted by posterior
lution (see the top row of in 3) indicates that the a bias towards     distribution over kdata . This finding accords well with results
simple functions is appropriate for this task. The Bayesian            from related domains using a single function type such as it-
solution also prefers simple functions for many of the prob-           erated learning or function learning.
lems. We consider the relationship between the optimal func-
                                                                          Not surprisingly, all participants showed a tendency to in-
tion and the generated function in more detail below.
                                                                       crease the complexity of their responses whenever the com-
Combining data with prior expectations                                 plexity of the data increased. This combination of data and
                                                                       prior follows directly from Bayes’ rule (see Equation 1); in
To examine the factors that influenced responding, we used             the order for the posterior distribution over kresponse to shift to-
the hierarchical Bayesian regression model to predict the              wards higher degree polynomials, the probability of the data
mean polynomial degree of responses, kresponse , using the fol-        given higher degree polynomials must be high enough to out-
lowing predictors: a) The mean polynomial degree of the                weigh the prior bias towards lower degree polynomials. It is
data, kdata (i.e., the complexity of the data that participants        reasonable to assume that this only occurs when the data are
were shown). b) An estimate of the noise (σ2P ) generated              complex, and the results seem to support this notion.
using Equation 4 with βˆ and X estimated from the response                The Bayesian analysis also offered a straightforward way
function. c) The number of data points, N. In addition, the            to handle individual differences in not only the prior distri-
pairwise interactions between kdata , σ2P , and N, the three-way       butions over kresponse but also in how information is extracted
interaction between all variables plus an intercept. For this          about the problem with some participants also utilizing the
analysis we are only concerned with the posterior probabili-           number of data points and the noise in the data in producing
ties of the coefficients; hence, we integrate out σ2 and com-          their responses. One limitation of the analysis presented here
pute the posterior probability of the coefficients
                                                       asa mul-      is that the polynomial regression returns a complete distribu-
tivariate t-distribution, β|kresponse ∼ tn−k β̂X, Vβ σ2 . The          tion over k for each problem; however, the follow-up regres-
posterior distributions of the coefficients for each participant       sion analysis only utilizes the mean k. Future analysis would
are shown in Figure 4. Coefficients whose 95% credible in-             combine the regression analyses by making the polynomial
tervals do not overlap zero are shown in bold.                         degree of the responses contingent on the outcome of the re-
   Clearly, all of the participants responses are predicted by         gression model used in the follow-up analysis.
the intercept (which provides a measure of the base polyno-               In typical function learning studies, the processes and lim-
mial degree) and the complexity in the data, kdata . For two           itations associated with attention, learning, and memory can
of the participants (numbers 3 and 4; see rows C and D in              obscure the underlying way that observers trade fit and com-
Figure 4), only these two variables predict kresponse . For these      plexity in their formation of mental causal models. By con-
                                                                   1160

                                                Subject #: 1                                          Subject #: 2                                          Subject #: 3                                          Subject #: 4                                          Subject #: 5
                                        4                                                     4                                                     4                                                     4                                                     4
                Mean Responsek                                        Mean Responsek                                        Mean Responsek                                        Mean Responsek                                        Mean Responsek
                                        3                                                     3                                                     3                                                     3                                                     3
                                        2                                                     2                                                     2                                                     2                                                     2
                                        1                                                     1                                                     1                                                     1                                                     1
                                        0                                                     0                                                     0                                                     0                                                     0
                                            0     1   2    3      4                               0     1   2    3      4                               0     1   2    3      4                               0     1   2    3      4                               0     1   2    3      4
                                                Mean Datak                                            Mean Datak                                            Mean Datak                                            Mean Datak                                            Mean Datak
                                        1                                                     1                                                     1                                                     1                                                     1
                Response Prior, p(k)                                  Response Prior, p(k)                                  Response Prior, p(k)                                  Response Prior, p(k)                                  Response Prior, p(k)
                                       0.8                                                   0.8                                                   0.8                                                   0.8                                                   0.8
                                       0.6                                                   0.6                                                   0.6                                                   0.6                                                   0.6
                                       0.4                                                   0.4                                                   0.4                                                   0.4                                                   0.4
                                       0.2                                                   0.2                                                   0.2                                                   0.2                                                   0.2
                                        0                                                     0                                                     0                                                     0                                                     0
                                                0 1 2 3 4 5 6 7                                       0 1 2 3 4 5 6 7                                       0 1 2 3 4 5 6 7                                       0 1 2 3 4 5 6 7                                       0 1 2 3 4 5 6 7
                                                      k                                                     k                                                     k                                                     k                                                     k
Figure 3: Top Row: Average estimated degree of best fitting polynomial for the data plotted against the average best fitting
polynomial for the responses. The data points have been jittered to reduce overlap. Bottom Row: Histogram the sampling
distribution of the prior density of polynomial degrees for each participants responses (p(k)). The estimated mean and 95%
credible intervals are also shown.
trast, the presented methodology offers several advantages:                                                                                                        is that each input that is presented during learning is stored
1) the influence of memory and learning on responses is re-                                                                                                        in memory along with an associated output, new inputs are
duced and 2) data from a large number of functions that differ                                                                                                     compared to the stored values and responses are generalized
in type and complexity can be collected efficiently. 3) Addi-                                                                                                      based on the similarity (i.e., psychological distance) between
tionally, important factors, such as noise and the amount of                                                                                                       the new value and all of the stored values (DeLosh et al.,
data, can be easily manipulated without any detriment in per-                                                                                                      1997). Griffiths, Lucas, Williams, and Kalish (in press) have
formance. In function learning, people are not very good at                                                                                                        recently demonstrated that the function-based view and the
learning about noisy functions (see e.g., Carroll, 1963). These                                                                                                    similarity-based view are related: Bayesian linear regression
three points have particular value when studying the prior bi-                                                                                                     requires predicting y by specifying a prior over the class of
ases underlying function representation. However, there are                                                                                                        polynomials; if we instead specify a covariance matrix on the
several other findings from function learning which are not                                                                                                        different input values (e.g., by taking, for example, the expo-
addressed here. For instance, extrapolation tends to be lin-                                                                                                       nential of the squared distance between all pairwise combina-
ear and is an important diagnostic result (see DeLosh et al.,                                                                                                      tions of x-values) then Gaussian process regression achieves
1997); only a small extrapolation region was included in the                                                                                                       the same result. Gaussian process regression is essentially a
current experiment (see Figure 1). Future work will increase                                                                                                       similarity-based view of function learning that is isomorphic
the number of responses collected from the extrapolation re-                                                                                                       to Bayesian linear regression. The experimental method em-
gion to allow examination of extrapolation biases when learn-                                                                                                      ployed in this paper offers a promising way to explore both
ing and memory are removed from information processing.                                                                                                            views of function-based performance.
   Finally, one clear divergence in the present work from pre-
vious work is the use of a polynomial regression model to                                                                                                                                                                Acknowledgments
capture function-based performance. While early work in                                                                                                            The authors would like to thank Woojae Kim helpful com-
function learning assumed that people explicitly represented                                                                                                       ments on the computational analysis. Preparation of this ar-
the to-be-learned functions (Carroll, 1963), more recent work                                                                                                      ticle was facilitated by an NIH-NIMH training grant #:T32
has emphasized similarity-based processing. The basic idea                                                                                                         MH019879-14 to the first author.
                                                                                                                                                             1161

                                                                        kdata                          σ2                                                         kdata × σ2                      kdata × N                      σ2 × N                     kdata × σ2 × N
        A                     0.4
                                    Intercept
                                                              0.4                                0.4                         0.4
                                                                                                                                     N
                                                                                                                                                            0.4                             0.4                            0.4                             0.4
                 Proportion                      Proportion                         Proportion                  Proportion                     Proportion                      Proportion                     Proportion                      Proportion
                              0.2                             0.2                                0.2                         0.2                            0.2                             0.2                            0.2                             0.2
                               0                                0                                 0                           0                               0                               0                             0                               0
                               1.5      2 2.5                  -0.5      0 0.5                    -1   0    1                 -1     0     1                 -0.5    0 0.5                   -0.5    0 0.5                  -1    0       1                 -1     0    1
                                        β1                               β2                            β3                            β4                              β5                              β6                           β7                               β8
        B                     0.4                             0.4                                0.4                         0.4                            0.4                             0.4                            0.4                             0.4
                 Proportion                      Proportion                         Proportion                  Proportion                     Proportion                      Proportion                     Proportion                      Proportion
                              0.2                             0.2                                0.2                         0.2                            0.2                             0.2                            0.2                             0.2
                               0                               0                                  0                            0                              0                               0                             0                                0
                                    1   2    3                      0   0.2 0.4                   -1   0    1                 -0.5   0 0.5                   -0.5    0 0.5                   -0.2    0 0.2                  -1    0       1                 -0.5   0 0.5
                                        β1                              β2                             β3                            β4                              β5                              β6                           β7                               β8
        C                     0.4                             0.4                                0.4                         0.4                            0.4                             0.4                            0.4                             0.4
                 Proportion                      Proportion                         Proportion                  Proportion                     Proportion                      Proportion                     Proportion                      Proportion
                              0.2                             0.2                                0.2                         0.2                            0.2                             0.2                            0.2                             0.2
                               0                               0                                  0                           0                               0                               0                             0                                0
                                    0   1    2                      0   0.5     1                 -2   0    2                 -1     0     1                 -0.5    0 0.5                   -0.5    0 0.5                  -2    0       2                 -0.5   0 0.5
                                        β1                              β2                             β3                            β4                              β5                              β6                           β7                               β8
        D                     0.4                             0.4                                0.4                         0.4                            0.4                             0.4                            0.4                             0.4
                 Proportion                      Proportion                         Proportion                  Proportion                     Proportion                      Proportion                     Proportion                      Proportion
                              0.2                             0.2                                0.2                         0.2                            0.2                             0.2                            0.2                             0.2
                               0                               0                                  0                           0                               0                               0                             0                               0
                                    0   1    2                      0.6 0.8 1                     -1   0    1                 -1     0     1                 -0.5    0 0.5                   -0.5    0 0.5                  -2    0       2                 -1     0    1
                                        β1                             β2                              β3                            β4                              β5                              β6                           β7                               β8
                              0.4                             0.4                                0.4                         0.4                            0.4                             0.4                            0.4                             0.4
         E
                 Proportion                      Proportion                         Proportion                  Proportion                     Proportion                      Proportion                     Proportion                      Proportion
                              0.2                             0.2                                0.2                         0.2                            0.2                             0.2                            0.2                             0.2
                               0                                0                                 0                           0                               0                               0                             0                                0
                               1.5      2 2.5                  -0.5      0 0.5                    -1   0    1                 -1     0     1                 -0.5    0 0.5                   -0.5    0 0.5                  -1    0       1                 -0.5   0 0.5
                                        β1                               β2                            β3                            β4                              β5                              β6                           β7                               β8
Figure 4: Distribution of regression coefficients for A: participant 1, B: participant 2, C: participant 3, D: participant 4, E:
participant 5. Coefficients whose 95% credible intervals do not overlap zero are shown in bold.
                                        References                                                                                                     tive biases on cultural evolution. Philosophical Transcripts
                                                                                                                                                       of the Royal Society B, 363, 3503-3514.
Carlin, J. B., & Chib, S. (1995). Bayesian model choice via                                                                                          Griffiths, T. L., Lucas, C., Williams, J. J., & Kalish, M. L.
  markov chain monte carlo methods. Journal of the Royal                                                                                               (in press). Modeling human function learning with gaus-
  Statistical Society, Series B, 57, 473-484.                                                                                                          sian processes. Advances in Neural Information Processing
Carroll, J. (1963). Functional learning: The learning of con-                                                                                          Systems, 21.
  tinuous functional mappings relating stimulus and response                                                                                         Kalish, M. L., Griffiths, T. L., & Lewandowsky, S. (2007). It-
  continua. Educational Testing Service Research Bulletin                                                                                              erated learning: Intergenerational knowledge transmission
  (RB-62-26).                                                                                                                                          reveals inductive biases. Psychonomic Bulletin & Review,
Chater, N., & Vitanyi, P. (2003). Simplicity: A unifying prin-                                                                                         14, 288-294.
  ciple in cognitive science? Trends in Cognitive Sciences, 7,                                                                                       Kalish, M. L., Lewandowsky, S., & Kruschke, J. K. (2004).
  19-22.                                                                                                                                               Population of linear experts: Knowledge partitioning and
DeLosh, E., Busemeyer, J., & McDaniel, M. (1997). Ex-                                                                                                  function learning. Psychological Review, 111(4), 1072-
  trapolation: The sine qua non for abstraction in function                                                                                            1099.
  learning. Journal of Experimental Psychology: Learning,                                                                                            Lombrozo, T. (2006). The structure and function of explana-
  Memory & Cognition, 23, 968-986.                                                                                                                     tions. Trends in Cognitive Science, 10, 464-472.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2003).                                                                                      Lombrozo, T. (2007). Simplicity and probability in causal
  Bayesian data analysis (2nd ed.). London: Chapman &                                                                                                  explanation. Cognitive Psychology, 55, 232-254.
  Hall CRC.
Griffiths, T. L., Kalish, M. L., & Lewandowsky, S. (2008).
  Theoretical and empirical evidence for the impact of induc-
                                                                                                                                          1162

