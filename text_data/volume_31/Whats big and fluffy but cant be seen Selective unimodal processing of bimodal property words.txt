UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
What's big and fluffy but can't be seen? Selective unimodal processing of bimodal property
words
Permalink
https://escholarship.org/uc/item/9b15g75r
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Connell, Louise
Lynott, Dermot
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                                   What's Big and Fluffy But Can't Be Seen?
                      Selective Unimodal Processing of Bimodal Property Words
                                      Louise Connell (louise.connell@manchester.ac.uk)
                                      Dermot Lynott (dermot.lynott@manchester.ac.uk)
                                     School of Psychological Sciences, University of Manchester
                                               Oxford Road, Manchester M13 9PL, UK
                            Abstract                                  conceptual representations has focussed on unimodal object
                                                                      properties (i.e., properties that can be perceived using one
   Recent work has shown that perceptual and conceptual               sense alone). For example, using fMRI, Gonzáles and
   processing share a common, modality-specific neural                colleagues (2006) found that passively reading scent-related
   substrate and appear to share the same attentional
                                                                      words (e.g., cinnamon) increased activation in the primary
   mechanisms. However, this work has been largely limited to
   the conceptual processing of unimodal properties (i.e., that       olfactory areas of the piriform cortex. Regarding visual
   involve information from only one sensory modality) even           processing, Simmons et al. (2007) showed that verifying
   though most perceptual properties are actually multimodal          colour properties in text (e.g., that a banana is yellow) led to
   (i.e., involve information from more than one sensory              activation in the same region of the left fusiform gyrus in
   modality). In two experiments, we investigate whether the          the visual cortex as a perceptual task that involved judging
   conceptual processing of bimodal properties (e.g., fluffy,         colour sequences. Further comparisons by Goldberg,
   jagged) requires representation on both modalities or if it is     Perfetti and Schneider (2006) found that verification of
   instead possible for individual modalities to carry the            colour, sound, touch and taste properties activated cortical
   representational burden. Results show that bimodal properties
                                                                      regions respectively associated with encoding visual,
   must be processed on both component modalities when
   attentional control is governed by incoming stimuli (i.e.,         auditory, haptic and gustatory experiences, illustrating that
   exogenously), but a “quick and dirty” unimodal representation      perceptual experience and conceptual knowledge share a
   can suffice when selective conscious (i.e. endogenous)             common neural substrate.
   attention has time to suppress the non-target modality. We            Other recent work has focused on the emergence of
   discuss these findings with reference to embodied views of         perceptual phenomena, such as modality switching costs, in
   cognition.                                                         conceptual processing (e.g., Marques, 2006; Pecher,
                                                                      Zeelenberg & Barsalou, 2003; van Dantzig, Pecher,
                         Introduction                                 Zeelenberg & Barsalou, 2008). For example, Spence,
How do we think about objects that are not in front of us at          Nicholls and Driver (2001; see also Turatto, Galfano,
the time? Do we see with the mind's eye and touch with the            Bridgeman & Umiltà, 2004) asked people to indicate the
mind's fingers? Embodied cognition research represents a              left/right location of a series of perceptual stimuli, and
recent trend to cease viewing conceptualisation and mental            found that switching modalities from one trial to the next
representation in terms of abstract information processing            (e.g., from a visual light flash to an auditory tone) incurred a
and rather in terms of grounded, sensorimotor simulation.             processing cost. Pecher et al. (2003) replicated this
As a common thread, embodied theories (Barsalou, 1999;                paradigm in a conceptual task by asking people to verify a
Gibbs, 2003; Glenberg, 1997; Johnson-Laird, 1983; Pecher              series of unimodal object properties presented as text
& Zwaan, 2005) hold that conceptual thought is grounded in            onscreen, and found that people were slower to verify a
the same neural systems that govern sensation, perception             property in a given modality (e.g., auditory leaves:rustling)
and action. For example, according to Barsalou’s (1999,               after verifying a property in a different modality (e.g., visual
2008) Perceptual Symbol Systems, concepts are essentially             apple:shiny) than after verifying a property in the same
partial recordings of the neural activation that arises during        modality (e.g., auditory blender:loud), and that this effect
perceptual and motor experiences and these recordings can             was not due to associative priming. Van Dantzig et al.
later be re-enacted as a perceptual simulation of that                (2008) took the paradigm one step further by showing that
concept. Such grounded accounts of cognition are in                   property verification was also slowed when it followed a
contrast to other accounts that assume concepts to be                 perceptual stimulus from a different modality (e.g., auditory
discrete representations stored in semantic memory,                   bee:buzzes following a visual light flash). Like the modality
separated from systems governing perception and action                switching costs found by Spence et al. during perceptual
(e.g., Collins & Quillian, 1969, Katz & Fodor, 1963;                  tasks, such costs during conceptual tasks result from the re-
Kintsch & van Dijk, 1978; Fodor, 1975; Newell & Simon,                allocation of attention from one modality-specific system to
1972; Pylyshyn, 1984; Tulving, 1972).                                 another.
   A growing body of empirical work has emerged in                       However, it is overly simplistic to assume that object
support of embodied representations of conceptual                     properties are conceptually processed only in single,
information, and most research on modality-specific                   modality-specific regions of the brain. From philosopher
                                                                  1465

John Locke (1975/1690) to modern empirical studies                 followed by a unimodal visual target from the same
(Amedi, von Kriegstein, van Atteveldt, Beauchamp &                 modality (e.g., pond:murky) or a bimodal visuohaptic target
Naumer, 2005; Connell, 2007; Ernst & Bülthoff, 2004),              that shares a component modality (e.g., blade:jagged).
many have pointed out that object properties exist in both            Our main interest here is in how unimodal properties
multimodal (that can be perceived by multiple senses) and          differentially facilitate targets from the same versus shared
unimodal (that can be perceived by only one of our senses)         modality. If bimodal properties are always automatically
forms. For example, while the colour yellow is normally            represented using both component modalities, then it should
perceived only through the single modality of vision (i.e.,        take people longer to process bimodal targets (e.g.,
colour is a unimodal property), the property round would be        visual→visuohaptic) compared to unimodal targets (e.g.,
considered multimodal as it can be perceived both haptically       visual→visual), because there will be costs involved in
and visually. Indeed, the bimodal overlap of touch and             allocating attention to the haptic modality.
vision in the conceptual processing of object properties has
emerged in imaging work: Newman, Klatzky, Lederman and             Method
Just (2005) noted that the intraparietal sulcus, a region          Participants Twenty-seven native speakers of English,
usually involved in visual imagery, was found to be highly         with no reported reading or sensory deficits, participated in
activated when participants were processing the roughness          the experiment for course credit.           Modal specificity
of objects (e.g., which is rougher? pear or egg). The              (unimodal or bimodal) was manipulated within-participants,
unimodal / multimodal distinction has recently been                and each participant received one of nine counterbalanced
highlighted in a norming study conducted by Lynott and             lists of concept:property pairs.
Connell (2009), who collected ratings of experiential
strength on five perceptual modalities (visual, haptic,            Materials Fifty-four concept-property matches were created
auditory, olfactory, gustatory) for hundreds of object             to act as test items, divided between unimodal (visual or
properties. They found that most properties are multimodal         haptic) and bimodal (visuohaptic) items. Property words
rather than unimodal, with particular bimodal clustering of        were taken from Lynott & Connell's (2009) modality
visual-haptic and olfactory-gustatory modalities.                  exclusivity norms. These norms comprise 423 adjectives,
                                                                   each describing an object property, with mean ratings (0-5)
                                                                   of how strongly that property is experienced through each of
The Current Study
                                                                   five perceptual modalities (auditory, gustatory, haptic,
Neuroimaging research suggests that automatic and                  olfactory, visual) plus a number of other useful statistics.
extensive interaction between modalities may be the norm           For this experiment, all words chosen had a familiarity score
rather than the exception in perception (Shimojo & Shams,          of at least 90% in the norms. Unimodal words had the
2001). Furthermore, a strong interpretation of embodied            highest strength rating in the visual or haptic modality
theories of representation (e.g., Barsalou, 1999) would            (minimum strength of 3 on a 0-5 scale) with all other
suggest that the conceptual processing of a visuohaptic word       modalities at least one full point lower on the ratings scale.
would involve simulating both the visual and haptic                Likewise, bimodal words had joint highest strength ratings
modalities (e.g., representing round would involve                 in visual and haptic modalities (both ratings over 3 and
simulating both sight and touch). Since most object                within one ratings point of each other) and all other
property words combine two or more perceptual modalities           modalities were at least one full point lower. There were no
(Lynott & Connell, 2009), our aim in the following                 differences between unimodal and bimodal properties in
experiments is to examine whether conceptual processing            target strength [t(52) = 1.63 p > .1]. Each property was then
involves a similar automatic interaction between modalities        matched with an appropriate concept, such as
or if it is instead possible for individual modalities to carry    water:rippling (unimodal visual), draft:cold (unimodal
the representative burden. If bimodal and unimodal                 haptic), or cactus:spiky (bimodal visuohaptic). Two
properties entail different perceptual representations, then       independent judges verified the appropriateness of all 54
differential modality switching effects (Experiment 1) and         attributions. There were no differences between unimodal
modality detection rates (Experiment 2) should emerge              and bimodal items in summed concept-property British
during their conceptual processing as attention is directed to     National Corpus (BNC, 2001) word frequencies [t(52) =
the perceptual modalities needed for simulation.                   1.34, p > .15], or orthographic length [t(52) = 0.285 p > .8].
                                                                   We then formed pairs of concept-property items for
                       Experiment 1                                sequential presentation by selecting a unimodal item (to be
In the modality switching effect (Marques, 2006; Pecher et         presented first) and pairing it with another unimodal or
al., 2003; van Dantzig et al., 2008), there is a processing        bimodal item (the target). The pairing of each target item
cost involved in representing a modality-specific object           with its preceding modality was fully rotated over nine lists:
property when attention has to be shifted from a different,        for example, a visual item would appear as the first item in a
already active perceptual modality. The present experiment         pair in one list and the second item in another, or a
uses a property verification task with the same basic              visuohaptic item would be presented following a visual item
methodology as Pecher et al. but will instead present a            in one list and a haptic item in another. Each participant
combination of unimodal and multimodal target properties.          saw every item, but in only one of these nine possible
For example, a visual property (e.g., window:misty) may be         critical pairs.
                                                               1466

   Table 1: Sample concept:property pairs per modal specificity condition in Experiment 1 with mean verification times and
                                             standard deviations (in milliseconds).
         Modal specificity               Sample pairs                     Transition type            M           SD
         Unimodal                window:misty → pond:murky               (visual → visual)          1039        157
                                 sunburn:stinging → wool:itchy           (haptic → haptic)
         Bimodal                 magazine:glossy → cactus:spiky       (visual → visuohaptic)        1064        151
                                   marble:cool → fabric:silky         (haptic → visuohaptic)
         Engagement cost                                                                             25
   A list of 96 concept-property fillers was also created, 72     remaining focused on the original. In other words, results
false and 24 true, to provide an overall balance of 50:50         suggest that verifying a bimodal property such as fluffy
true:false responses per participant. As in Pecher et al.'s       requires representation on both visual and haptic component
Experiment 1, most of the false fillers were associated in        modalities because both sight and touch are involved in its
Nelson, McEvoy, and Schreiber's (2004) word association           perceptual simulation.
norms (e.g.. oven:baked, coffin:dead) in order to ensure
participants could not perform the task using simple word                                Experiment 2
association strategies (Solomon & Barsalou, 2004).                In property verification tasks, each new stimulus that comes
                                                                  along directs attention to its particular modality (or
Procedure Participants read instructions that asked them to       modalities) for processing.        This type of attentional
press the button labelled “true” (the comma key) if the           mechanism is exogenous control, where the modality
property was usually true of the concept but to press the         involved in processing a word (or perceptual stimulus)
button labelled “false” (the full-stop key) if not. We used       automatically and obligatorily grabs attention. There is also
Pecher et al.'s (2003) example “carnation can be black” to        endogenous attentional control, where participants
highlight that, although carnations could theoretically be        consciously and voluntarily focus their attention on the
black, it would be highly unusual and should be judged as         target modality.      Perceptual studies have shown that
false. Each trial began with a fixation cross for 200 ms          endogenous attention on a particular modality creates
followed by the item in the form “concept can be property”        anticipatory activation in the relevant area of the cortex
which stayed onscreen until the participant responded.            (Foxe, Simpson, Ahlfors & Saron, 2005) and allows
Participants received immediate feedback if they responded        information from the target modality to be processed faster
incorrectly or too slowly (more than 2000 ms), and each           than information from other modalities (Spence et al., 2001;
trial ended with a 200 ms blank screen. A practice session        Turatto et al., 2004). Furthermore, endogenous attention on
of 24 items, half true and half false, preceded the main          a particular modality during presentation of bimodal stimuli
experiment. Critical pairs and fillers appeared in a random       can suppress activation in the cortex corresponding to the
order with a self-paced break every 48 trials.                    unattended modality (e.g., attending to vision for an
                                                                  audiovisual stimulus results in suppression in the auditory
Results & Discussion                                              cortex: Johnson & Zattore, 2005). Our aim in this
Two participants’ data were removed prior to analysis due to      experiment is, therefore, to see if selective endogenous
achieving less than 70% accuracy. Any targets that received       attention can overcome the obligatory exogenous grab of
error responses, and any targets where the preceding item in      attention that we observed for bimodal stimuli in
the pair was in error, were excluded from analysis (7.3% of       Experiment 1. We use a modality detection task to examine
data in total). There was no difference in the error rate         conceptual processing of unimodal and bimodal property
between unimodal (M = 5.7%, SD = 7.1%) and bimodal (M             words, in a variant of the paradigm used to examine the
= 4.0%, SD = 6.9%) targets, t(24) = 0.96, p = .346.               positive/negative detection of emotionally affective words at
Response time means (in milliseconds) were calculated as          near-subliminal thresholds (Dijksterhuis & Aarts, 2003).
the mean of the medians per participant per condition to          Endogenous attention will be directed to a target modality
minimise the effect of outliers.                                  (visual or haptic) for a particular block of stimuli and
   As predicted, people were slower to verify bimodal             participants will be asked to judge whether each presented
properties than unimodal properties (see Table 1: directional     property corresponds to the target modality for that block.
t(24) = 1.96, p = .031). When a single modality was already       For example, in a visual block, participants should detect
active, people encountered a processing cost for bimodal          both unimodal (e.g., misty, green) and bimodal (e.g., big,
properties that shared a component modality (i.e., a partial      fluffy) stimuli as properties with visual information. We
overlap) compared to processing the same single modality          expect accuracy to improve from near-chance performance
again (i.e, a complete overlap). This effect is not a             over successive blocks, both because of practice effects and
switching cost, as described by Pecher et al. (2003) or           because longer display durations increase the probability of
Spence et al. (2001), because attention is not being              successful detection. However, by measuring accuracy rates
decoupled from one modality and coupled to another; rather,       for a range of increasing display times, we can test how
the effect is an engagement cost because attention must be        endogenous and exogenous attention interact.
split and coupled to an additional modality while still              If bimodal properties must always be represented using
                                                                  both component modalities, then even focusing endogenous
                                                              1467

attention on a single modality will not be enough to prevent       and all fillers did not, there was an equal ratio of yes:no
a bimodal word exogenously directing attention towards its         responses within each block. At the start of each block,
other modality (e.g., processing visuohaptic fluffy in a visual    participants were told which sensory modality they would
block will still need haptic attention). In this case, we          be making judgements about. When participants had
would expect accuracy rates for bimodal properties to be           completed both haptic and visual modality blocks with a
lower than those for unimodal properties in the same block.        display duration of 17s, the same blocks were repeated at
On the other hand, if the effects of selective attention found     33ms, then 50ms, 67ms, and lastly 100ms (the presentation
for perceptual processing (Foxe et al., 2005; Johnson &            of visual or haptic blocks first was counterbalanced across
Zattore, 2005) extend to conceptual processing, then it            participants). Items were presented randomly within each
should be possible to represent bimodal properties                 block, with each trial beginning with a central fixation
unimodally if a single component modality is the focus of          (250ms), followed by a word (displayed for different
endogenous attention, and so we would expect the bimodal           durations depending on the block), followed by a mask (a
and unimodal properties in a block to be detected equally          row of Xs) until the participant responded. Response times
accurately.                                                        were measured from mask onset to keypress1.
Method                                                                    Table 2: Mean percentage accuracy, with standard
Participants Sixty native speakers of English, with no                 deviations, per modal specificity and display duration of
reported reading or sensory deficits, participated in the                              properties in Experiment 2.
experiment for course credit. Modal specificity (unimodal
or bimodal) and display duration (17ms, 33ms, 50ms, 67ms,             Display                         Modal specificity
100ms) were both manipulated within-participants. and each            Duration (ms)            Unimodal                Bimodal
participant received one of two counterbalanced lists.                                       M          SD           M           SD
                                                                      17                   47.4         20.3        42.9         21.3
Materials A set of 128 words were taken from Lynott &                 33                   63.4         23.1        65.7         20.1
Connell's (2009) modality exclusivity norms: 64 test items            50                   71.6         20.5        73.5         20.5
and 64 fillers. Unimodal and bimodal test items (32 of                67                   76.8         13.6        79.0         13.6
each) were selected with the same criteria as in Experiment           100                  79.2         13.9        79.5         15.1
1. Bimodal properties were split into two lists: one to
appear in visual blocks and one in haptic blocks                   Results & Discussion
(counterbalanced).      There was no difference between            Responses to test words less than 200 ms or more than three
unimodal and bimodal words in target modality strength             standard deviations away from a participant's mean per
(i.e., properties were equally perceptible by sight in visual      display duration were removed as outliers (2.1% of data).
blocks [t(46) = 1.65 p > .1] and by touch in haptic blocks         The percentage of correctly detected test words per modal
[t(46) = 1.01 p > .3]). In addition, lexical decision times        specificity per display time is shown in Table 2.
[t(62) = 0.45 p > .6] and accuracy [t(62) = 0.42 p > .6] were          As expected, there was an overall main effect of display
equivalent for unimodal and bimodal words (English                 duration [F(4, 236) = 80.07, p < .0001], with planned
Lexicon Project database: Balota et al., 2007).                    contrasts showing that people became more accurate with
   Thirty-two filler items were selected per block so that         each increasing duration up to 67ms (all ps < .003) and
each filler word had a low strength rating (less than 2) on        performance levelling out between 67ms and 100ms (p > .
the target modality. This meant that all fillers had               2). Modal specificity had no main effect [F(1, 59) = 0.09, p
significantly lower strength on the target modality than the       = .771] but did interact significantly with duration [F(4,
corresponding test words [t(158) = 54.97 p < .0001]. In            236) = 4.78, p = .001]. In simple effects analysis, accuracy
order to minimise possible interference with bimodal               for bimodal properties was worse than that for unimodal
stimuli, filler items also had lower strength on the               properties at the shortest display duration [17ms: t(59) =
unattended modality than bimodal test items (e.g., in visual       2.43, p = .018], even though the strength on the target
blocks, the haptic strength of filler items was significantly      modality and the lexical decision times for each word were
less than the haptic strength of visuohaptic items), [t(158) =     equal for both unimodal and bimodal words. Longer
9.47 p < .0001].                                                   exposure to the properties, however, made this difference
                                                                   disappear [33ms: t(59) = 1.16, p > .2; 50ms t(59) = 1.04, p
Procedure Participants were instructed that they would be          > .3; 67ms t(59) = 1.27, p > .2; 100ms t(59) = 0.17, p > .8].
asked to judge whether or not words appearing onscreen                 These results show a mixture of complete and “quick and
could be experienced through a particular sense; either felt       dirty” conceptual processing of bimodal properties,
through touch or seen. They were told that words would
appear onscreen one at a time and be covered very quickly          1
                                                                     It could be argued that the button-pressing nature of the task could
by a row of Xs, and that they should press “Yes” (the              interfere with the simultaneous processing of haptic words (e.g.,
comma key) if the word could be perceived through that             Kaschak et al., 2005). However, in a related study (Connell &
sense or “No” (the full stop key) if it could not. Stimuli         Lynott, 2009) we compared modality detection performance using
were arranged into blocks of test and filler words for each        this methodology to that using a verbal task (where participants
modality; since all test items pertained to the given modality     respond with a voice trigger rather than a keypress) and found no
                                                                   evidence of any such interference.
                                                               1468

suggesting that endogenous attention can selectively              touch are involved in its perceptual simulation. Consciously
modulate modality-specific representation at least some of        focusing endogenous attention on a particular perceptual
the time. When a word is displayed for only 17ms, and             modality, on the other hand, limits the ease with which
people are not necessarily conscious of having read it,           incoming stimuli can grab attention. Experiment 2 showed
bimodal properties are more difficult to detect as visible or     that the processing of modality-specific information is rapid
touchable than unimodal properties.          This difference      and automatic, with performance differences between
suggests that bimodal properties are being simulated on both      unimodal and bimodal words after just 17ms exposure. For
component modalities and are exogenously directing                example, people found it more difficult to detect bimodal
attention towards whichever modality is not the current           words like fluffy as pertaining to the sense of vision than
subject of endogenous focus. Dividing attention in this way       unimodal words like colourful because the haptic
means that 17ms display time is not enough to process             component of fluffy exogenously grabbed attentional
whether a visuohaptic word like fluffy or round corresponds       control. Endogenous attention on the visual modality,
to the target sense of vision (or touch) as easily as a           however, was able to suppress this unwanted haptic
unimodal word like glossy (or clammy). With longer                simulation if the word was displayed for longer (33ms
display durations, however, people are able to resolve the        onwards). When bimodal perceptual stimuli are presented,
difficulties caused by dividing attention between modalities      endogenous attention on one modality can suppress
and so bimodal accuracy closely follows unimodal accuracy.        activation in the cortex corresponding to the unattended
The lack of difference between unimodal and bimodal               modality (Johnson & Zattore, 2005). The current findings
performance suggests that 33ms exposure offers enough             suggest a similar mechanism operates in the conceptual
opportunity to suppress the exogenous attentional grab of         processing of bimodal properties.
bimodal stimuli (Johnson & Zattore, 2005) and allow the              The modality engagement cost we report in the present
bimodal property to be processed only on the target               paper is different to the modality switching cost found for
modality that is the subject of endogenous attention. In          unimodal processing of perceptual (Spence et al., 2001;
short, this experiment's findings suggest that, although the      Turatto et al., 2004) and conceptual (Marques, 2006; Pecher
conceptual processing of a bimodal property such as big or        et al., 2003; van Dantzig et al., 2008) stimuli because it
fluffy automatically attempts representation on both sight        does not involve decoupling attention from the original
and touch, endogenous attention can effect a “quick and           modality. This raises the question of whether modality
dirty” perceptual simulation on just one of those modalities.     switching costs are actually composed of two summed
                                                                  costs: the time required to decouple attention from the first
                   General Discussion                             perceptual modality plus the time required to engage
This study investigated an issue largely neglected in             attention with a second modality. Our findings suggest that
conceptual processing research – the representational nature      the modality engagement, at least, incurs a sizeable
of multimodal properties – and specifically asked whether         processing cost. Future research will investigate whether
bimodal properties must always be represented bimodally           modality decoupling is similarly costly in processing terms.
(i.e., an obligatorily complete simulation) or whether a             Selective endogenous attention in perception is an
partial unimodal representation can sometimes suffice (i.e.,      efficient means of filtering the complex stream of incoming
a “quick and dirty” simulation). Results showed that both         information according to task demands. But is there any
perspectives were partly right: processing bimodal                such efficiency benefit for selective attention in conceptual
properties such as fluffy or round automatically attempts         processing? Or is the role of attentional mechanisms in
representation on both visual and haptic component                conceptual processing merely an artifact of the conceptual
modalities, but conscious attention on one of these               system co-opting the perceptual system for representational
modalities can selectively produce a unimodal                     purposes? We would suggest that there are indeed some
representation. These findings support the embodied view          advantages in selective processing of certain aspects of
that the conceptual system utilises modality-specific             conceptual information. For example, if selective attention
perceptual resources (e.g., Barsalou, 1999) and adds novel        allows people to create a partial “quick and dirty” perceptual
insights into the role of attentional mechanisms in modality-     simulation when task demands do not require anything more
specific conceptual processing.                                   complex, it frees up cognitive resources for other tasks.
   In Experiment 1, we found people were faster to verify a       Whether or not this benefit emerged from the adaptation of
unimodal property that used exactly the same modality as its      the attentional system to offline processing, or whether it is
predecessor (e.g., visual→visual) than a bimodal property         a happy accident of shared neural substrate between
that only shared one component modality with its                  perception and conception, remains an open question.
predecessor (e.g., visual→visuohaptic). Simply processing
stimuli as they arrive allows attention to be exogenously                             Acknowledgments
grabbed by whatever perceptual modality is needed, and so         This work was funded by grant RES-000-22-2407 from the
bimodal properties incur a processing cost when an                UK Economic and Social Research Council to the first
additional modality must be engaged. For example, when            author. Thanks to Felix Dreyer for help with data
the visual modality is already active, verifying a bimodal        collection.
property such as blade:jagged requires additional attention
to be allocated to the haptic modality because both sight and
                                                              1469

                       References                                 Review, 85, 363-394.
                                                                Locke, J. (1975) . An essay concerning human
Amedi, A., von Kriegstein, K., van Atteveldt, N. M.,
                                                                  understanding. In P.H. Nidditch (Ed.), Oxford: Clarendon
  Beauchamp, M. S., & Naumer, M. J. (2005). Functional
                                                                  Press. (Original work published 1690).
  imaging of human crossmodal detection and object
                                                                Lynott, D. & Connell, L. (2009). Modality exclusivity
  recognition. Experimental Brain Research, 166, 559-571.
                                                                  norms for 423 object properties. Behavior Research
Balota, D. A., Yap, M. J., Cortese, M.J., Hutchison, K. A.,
                                                                  Methods, 41, 558-664.
  Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L.,
                                                                Marques, J. M. (2006). Specialization and semantic
  Simpson, G. B., & Treiman, R. (2007). The English
                                                                  organization: Evidence for multiple semantics linked to
  Lexicon Project. Behavior Research Methods, 39, 445-
                                                                  sensory modalities. Memory & Cognition, 34, 60–67.
  459.
                                                                Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (2004).
Barsalou, L. W. (1999). Perceptual symbol systems.
                                                                  The University of South Florida free association, rhyme,
  Behavioral and Brain Sciences 22, 577–660.
                                                                  and word fragment norms. Behavior Research Methods,
Barsalou, L. W. (2008). Grounded cognition. Annual
                                                                  Instruments, & Computers, 36, 402-407.
  Review of Psychology, 59, 617–645.
                                                                Newell, A. & Simon, H. A. (1972). Human Problem
Collins, A. M., & Quillian, M. R. (1969). Retrieval time
                                                                  Solving. Englewood Cliffs, NJ: Prentice-Hall.
  from semantic memory. Journal of Verbal Learning and
                                                                Newman, S. D., Klatzky, R. L., Lederman, S. J., Just, M. A.
  Verbal Behaviour, 8, 240-248.
                                                                  (2005). Imagining material versus geometric properties
Connell, L. (2007). Representing object colour in language
                                                                  of objects: An fMRI study. Cognitive Brain Research, 23,
  comprehension. Cognition, 102, 476-485.
                                                                  235-246.
Connell, L., & Lynott, D. (2009). Hard to put your finger
                                                                Pecher, D., Zeelenberg, R., & Barsalou, L.W. (2003).
  on it: Haptic modality disadvantage in conceptual
                                                                  Verifying properties from different modalities for
  processing. Proceedings of the 31st Annual Meeting of the
                                                                  concepts produces switching costs. Psychological
  Cognitive Science Society.
                                                                  Science, 14, 119-124.
Dijksterhuis, A., & Aarts, H. (2003). On wildebeests and
                                                                Pecher, D., & Zwaan, R. A. (2005). Introduction to
  humans: The preferential detection of negative stimuli.
                                                                  grounding cognition. In D. Pecher & R. A. Zwaan (Eds.),
  Psychological Science, 14, 14–18.
                                                                  Grounding cognition: the role of perception and action in
Ernst, M. O., & Bülthoff, H. H. (2004). Merging the senses
                                                                  memory, language, and thinking. Cambridge: CUP.
  into a robust percept. Trends in Cognitive Sciences, 8,
                                                                Pylyshyn, Z. W. (1984). Computation and cognition.
  162–169.
                                                                  Cambridge, MA: MIT Press.
Fodor, J. A. (1975). The language of thought. New York:
                                                                Shimojo, S., & Shams, L. (2001). Sensory modalities are
  Crowell.
                                                                  not separate modalities: plasticity and interactions.
Foxe, J. J., Simpson, G. V., Ahlfors, S. P., & Saron, C. D.
                                                                  Current Opinion In Neurobiology, 11, 505-509.
  (2005). Biasing the brain’s attentional set: I. Cue driven
                                                                Simmons, W.K., Ramjee, V., Beauchamp, M.S., McRae, K.,
  deployments of intersensory selective attention.
                                                                  Martin, A., & Barsalou, L.W. (2007). A common neural
  Experimental Brain Research, 166, 370–392.
                                                                  substrate for perceiving and knowing about color.
Gibbs, R. W. (2003). Embodied experience and linguistic
                                                                  Neuropsychologia, 45, 2802-2810.
  meaning. Brain and Language, 84, 1-15.
                                                                Solomon, K. O., & Barsalou, L. W. (2004). Perceptual
Glenberg, A. M. (1997). What memory is for. Behavioral
                                                                  simulation in property verification. Memory & Cognition,
  and Brain Sciences, 20, 1–55.
                                                                  32, 244-259.
Goldberg, R. F., Perfetti, C. A., & Schneider, W. (2006).
                                                                Spence, C., Nicholls, M. E. R., & Driver, J. (2000). The cost
  Perceptual knowledge retrieval activates sensory brain
                                                                  of expecting events in the wrong sensory modality.
  regions. Journal of Neuroscience, 26, 4917-4921.
                                                                  Perception & Psychophysics, 63, 330-336.
González, J., Barros-Loscertales, A., Pulvermüller, F.,
                                                                Tulving, E. (1972). Episodic and semantic memory. In E.
  Meseguer, V., Sanjuán, A., Belloch, V., & Ávila, C.
                                                                  Tulving & W. Donaldson (Eds.), Organization and
  (2006). Reading cinnamon activates olfactory brain
                                                                  memory. New York: Academic Press.
  regions. Neuroimage, 32, 906-912.
                                                                Turatto, M., Galfano, G., Bridgeman, B., & Umiltà, C.
Johnson, J. A., & Zatorre, R. J. (2005). Attention to
                                                                  (2004). Space-independent modality-driven attentional
  simultaneous unrelated auditory and visual events:
                                                                  capture in auditory, tactile and visual systems.
  Behavioral and neural correlates. Cerebral Cortex, 15,
                                                                  Experimental Brain Research, 155, 301-310.
  1609–1620.
                                                                van Dantzig, S., Pecher, D., Zeelenberg, R., & Barsalou,
Johnson-Laird, P. N. (1983). Mental models. Cambridge,
                                                                  L.W. (2008). Perceptual processing affects conceptual
  MA: Harvard University Press.
                                                                  processing. Cognitive Science, 32, 579-5.
Kaschak, M. P., Madden, C. J., Therriault, D. J., Yaxley, R.
  H., Aveyard, M., Blanchard, A. A., & Zwaan, R. A.
  (2005). Perception of motion affects language processing.
  Cognition, 94, B79–B89.
Katz, J. J., & Fodor, J. A. (1963). The structure of a
  semantic theory. Language, 39, 170-210.
Kintsch, W., & van Dijk, T. A. (1978). Toward a model of
  text comprehension and production. Psychological
                                                            1470

