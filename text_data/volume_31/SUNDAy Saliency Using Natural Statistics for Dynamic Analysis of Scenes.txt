UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
SUNDAy: Saliency Using Natural Statistics for Dynamic Analysis of Scenes
Permalink
https://escholarship.org/uc/item/6z26h76d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Cottrell, Garrison
Tong, Matthew
Zhang, Lingyun
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       SUNDAy: Saliency Using Natural Statistics for Dynamic Analysis of Scenes
                                               Lingyun Zhang (lingyun@cs.ucsd.edu)
                                              Matthew H. Tong (mhtong@cs.ucsd.edu)
                                              Garrison W. Cottrell(gary@cs.ucsd.edu)
                                            Deptartment of Computer Science and Engineering
                                                     University of California, San Diego
                                         9500 Gilman Dr., Dept. 0404, La Jolla, CA 92037-0404
                              Abstract                                   again, one sees variants of local outliers. Gaborski, Vain-
   The notion that novelty attracts attention is core to many ac-        gankar, Chaoji, Teredesai, and Tentler (2004) used mixtures
   counts of visual saliency. However, a consensus has not been          of Gaussians to model what has occurred over a spatiotem-
   reached on how to best define novelty. Various interpretations        poral region of a video; an event is novel and salient if it
   of novelty lead to different bottom-up saliency models that
   have been proposed for static images and more recently for            cannot be accounted for by the model. Gao and Vasconce-
   dynamic scenes. In previous work, we assumed that a basic             los (2007b) extended their static image saliency to dynamic
   goal of the visual system is to locate targets such as predators      scenes: saliency is measured as KL divergence between the
   and food that are potentially important for survival, and devel-
   oped a probabilistic model of salience (Zhang, Tong, Marks,           histogram of features in a location and the surround region,
   Shan, & Cottrell, 2008). The probabilistic description of this        with the features implemented as optic flow. Itti and Baldi
   goal naturally leads a definition of novelty as self-information,     (2008) related saliency to Bayesian surprise which defines
   an idea that has appeared in other work. However, our notion
   uses the idea that the statistics used to determine novelty are       saliency as a deviation from what is expected based on a set
   learned from prior experience, rather than on the current im-         of internal models of the local visual world.
   age, leading to an efficient implementation that explains sev-           It is reasonable to assume that one goal of the visual system
   eral search asymmetries other models fail to predict. In this pa-
   per, we generalize our saliency framework to dynamic scenes           is to locate targets that are potentially important for survival.
   and develop a simple, efficient, and online bottom-up saliency        In our previous work, we developed a visual saliency model
   algorithm. Our algorithm matches the performance of more              that is based on this simple assumption. From the resulting
   complex state of the art algorithms in predicting human fixa-
   tions during free-viewing of videos.                                  probabilistic description of this goal, the self-information of
                                                                         the features falls out as bottom-up, task-independent saliency
                          Introduction                                   (Zhang et al., 2008). Self-information in this context, learned
It is of great research interest to understand how the visual            from natural statistics over development, corresponds with
system rapidly and efficiently samples the available visual in-          findings that novel items attract attention in visual search
formation. One major line of this research stems from the                (Wolfe, 2001). The reliance of learned natural statistics forms
intuition that novel objects or statistical outliers attract at-         the basis of our model: Saliency Using Natural statistics
tention. Koch and Ullman (1985) introduced the notion of                 (SUN). The definition of novelty in SUN, however, is dif-
a saliency map based around the notion that a region is intrin-          ferent from that has been used in previous computational
sically salient if it differs substantially from its surroundings.       saliency models in that statistical outliers are not based only
A number of models stem from a similar intuition that being              on the current image. In all the models discussed, the statis-
a local outlier makes a point salient (Itti, Koch, & Niebur,             tics were local; for static images, the statistics were gath-
1998; Gao & Vasconcelos, 2007a; Bruce & Tsotsos, 2006;                   ered solely from the current image, while for video they are
Torralba, Oliva, Castelhano, & Henderson, 2006). As the                  gathered over some local spatiotemporal region. In previous
small foreground items are often statistically different from            work, we showed that feature distributions learned from ex-
the large background, locating statistical outliers in an image          perience with natural scene images provide a straightforward
can facilitate detecting interesting objects. In addition, as low        account for human search asymmetries, a phenomenon that
probability events contain more information (in an informa-              is difficult for models that rely solely on the current image’s
tion theoretic sense), the definition of saliency as low proba-          statistics, as they would find a vertical bar among tilted bars
bility event connects the selective process of visual attention          just as salient as a tilted bar among vertical bars. Further-
with maximally sampling information.                                     more, the implementation of SUN performs as well as or bet-
   Since humans live in a dynamic world, video and interac-              ter than previous models in predicting human fixations when
tive environments provide a more faithful representation of              free viewing images, and is computationally much more effi-
the task facing the visual system than the static images fre-            cient(Zhang et al., 2008).
quently used in experiments. Studies also show that static                  In this paper, we use spatiotemporal visual features to gen-
measures of saliency do not perform as well as measures that             eralize the static image saliency model to dynamic scenes.
use temporal information in predicting human fixations(Itti,             We develop an efficient algorithm in which saliency is up-
2005). Thus it is of interest to investigate saliency for dy-            dated online upon each new frame. The model’s performance
namic scenes. The notion that statistical outliers attract atten-        in predicting human fixations while watching videos is com-
tion applies equally well to the spatiotemporal domain and               parable to previous methods, with the advantage of being sub-
                                                                     2944

stantially simpler.                                                   a more thorough discussion of the differences, see Zhang et
                                                                      al., 2008), while we learn the distributions of the features
                   Saliency is Information                            from previous experience. The remaining terms describe the
Our definition of bottom-up saliency emerges from a more              target appearance and likely locations respectively. Work
general goal of the visual attention system: detecting po-            with SUN’s appearance model is described in (Kanan, Tong,
tentially important targets and allocating computational re-          Zhang, & Cottrell, in press).
sources to them for further processing. To achieve such a                 When the organism is not actively searching for a partic-
goal, the pre-attentive process must estimate the probability         ular target (the free-viewing condition), the organism’s atten-
of a target given the visual features at every location in the        tion should be directed to any potential targets in the visual
visual field. We have proposed elsewhere that this probability        field, despite the fact that the features and locations associ-
is visual saliency (Zhang, Tong, & Cottrell, 2007; Zhang et           ated with the target class are unknown. In this case, the log-
al., 2008).                                                           likelihood term and location prior are unknown, so we omit
     To make this more explicit, we can calculate the probability     these terms from the calculation of saliency. Because the goal
that the target is present at a point, z, in the visual field. We     of the SUN model is to find potential targets in the surround-
use the term “point” loosely here; in this work, it refers to a       ing environment, the probabilities should reflect the natural
pixel in an image, but elsewhere it can refer to a single object      statistics of the environment and the learning history of the
(e.g. Zhang et al., 2007). This point contains two pieces of          organism, rather than just the statistics of the current image.
information our model makes use of: its location, denoted by          For our bottom-up model, this means that attention will be
L = lz , and the visual features present there, denoted F = fz .      drawn to novel targets, an idea that has been in the psychol-
If we define C as a binary variable that is 1 when the target         ogy literature for decades at least. For example, Fantz (1964)
is present at the current point and 0 otherwise, the probability      showed that novel objects attract the attention of infants.
of interest is sz = p(C = 1|F = fz , L = lz ). Applying Bayes’
rule and making the (unwarranted) simplifying assumptions                  Implementation of Bottom-up Saliency on
that features and locations are independent and conditionally                               Dynamic Scenes
independent given that C = 1, this can be rewritten as:               In this section, we describe an algorithm that estimates the
                                                                      bottom-up saliency in videos. First we apply a bank of spa-
     sz  =    p(C = 1|F = fz , L = lz )
                                                                      tiotemporal filters to each video; these filters are designed to
              p(F = fz , L = lz |C = 1)p(C = 1)                       be both efficient and in line with the human visual system.
         =
                       p(F = fz , L = lz )                            The probability distributions of these spatiotemporal features
              p(F = fz |C = 1)p(L = lz |C = 1)p(C = 1)                are learned from a set of videos from natural environments.
         =
                          p(F = fz )p(L = lz )                        Then for any video, we calculate its features and estimate the
                    1                                                 bottom-up saliency of each point as − log p(F = fz ). In the
         =               · p(F = fz |C = 1) · p(C = 1|L = lz )        rest of the paper, the features are indexed by pixel coordinates
              p(F = fz )
                                                                      so we drop the index z for notational simplicity.
To compare this probability across locations in an image, it
suffices to estimate the log probability (since logarithm is a        Features
monotonically increasing function). For this reason, we take          Let r, g and b denote the red, green, and blue components of
the liberty of using the term saliency to refer both to sz and to     an input video pixel. The intensity (I), red-green (RG) and
log sz , which is given by:                                           blue-yellow (BY ) channels are calculated as I = r + g + b,
                                                                                                      min(r,g)
                                                                      RG = r − g, BY = b − r+g   2 −     2     .
logsz =− logp(F = fz )+logp(F = fz|C = 1)+logp(C = 1|L = lz )             The spatiotemporal filters we used are separable linear fil-
         |      {z      } |         {z       } |          {z      }
                             Log Likelihood        Location prior     ters. The feature response function has the form F = V ∗ g ∗ h,
             Self-info
            (salience)
                           |                  {z                  }   where V is a channel of the video, g is the component that
                                       Dependent on target            applies only along the spatial dimensions and h is the com-
                                     (top-down knowledge)             ponent that applies only along the temporal dimension. The
                                                                      filter responses are then used as features.
The first term on the right side of this equation, − log p(F =            Difference of Gaussians (DoG) filters are used as the spa-
 fz ), contains no knowledge of the target and depends only           tial component, g. These linear filters loosely model the re-
on the visual features observed at the point. In informa-             sponse of cells in the lateral geniculate nucleus (LGN) and
tion theory, − log p(F = fz ) is known as the self-information        elsewhere. Mainly we choose these features to keep the
of the random variable F when it takes the value fz . Self-           implementation as simple as possible in order to verify the
information increases when the probability of a feature               power of the underlying model.
decreases—in other words, rarer features are more informa-                The DoG filters are generated using
tive. While both Torralba et al. (2006) and Bruce and Tsot-
                                                                                            2
                                                                                               x + y2
                                                                                                                         2
                                                                                                                            x + y2
                                                                                                                                   
sos (2006) also define bottom-up saliency as related to self-                       1                            1
information, they base their statistics on the current scene (for     g(x, y; σ) = 2 exp −               −           exp −            .
                                                                                   σ              σ2         (1.6σ)2        (1.6σ)2
                                                                  2945

                                                                            where F 0 (x, y,t; σ, τ) = V (x, y,t) ∗ g(x, y; σ) ∗ h0 (t; τ). This can
                                                                            be calculated efficiently, as:
                                                                                                               F 0 (x, y, −1; σ, τ)
                                                                                    F 0 (x, y, 0; σ, τ) =
                                                                                                                       1+τ
                                                                                                                    τ
                                                                                                               +           ·V (x, y, 0) ∗ g(x, y; σ)
                                                                                                                 1+τ
                                                                            To estimate the response to spatiotemporal feature g(x, y; σ) ∗
                                                                            h0 (t; τ) at the current frame, F 0 (x, y, 0; σ, τ), we simply require
Figure 1: On the left is the temporal filter when τ = 0.1. Plot-            the spatiotemporal filter response at the previous frame and
ted are h0 (t; τ) (blue line), h0 (t; 2τ) (black line) and h(t; τ) (red     the spatial filter response at the current frame. Besides the
line). The right plot shows the temporal filters for the five               advantage in calculation speed, this also removes the need
time scales used (values of τ of 0.025, 0.05, 0.1, 0.2, and                 for memory of earlier frames, a property not enjoyed by pre-
0.4).                                                                       viously used spatiotemporal filters. The final response can
                                                                            then be easily calculated by F(x, y,t; σ, τ) = F 0 (x, y,t; σ, 2τ) −
                                                                            F 0 (x, y,t; σ, τ).
We applied DoG filters to all three channels (I, RG, and BY )
using 5 scales (σ = 2, 4, 8, 16 or 32 pixels), resulting in 15              Learning the distribution
spatial filters in total.                                                   As described above, there are 15 features on the spatial di-
   The temporal filter h takes the form:                                    mension: 5 from each channel. On the temporal dimension
                     h(t; τ) = h0 (t; 2τ) − h0 (t; τ)                       there are 5 scales and they are combined with each spatial
                                                                            feature. Thus there are in total 75 feature responses. By
where                                                                       computing these feature responses on natural videos (about
                                     τ
                      h0 (t; τ) =        · (1 + τ)t                         2 hours of animal/plant documentary videos), we obtained an
                                   1+τ
                                                                            estimate of the probability distribution over the observed val-
t ∈ (−∞, 0] is the frame number relative to the current frame               ues of each of 75 features.
(0 is the current frame, −1 is last frame, etc.) and τ is a tem-                We used Song’s algorithm (Song, 2006) to fit a generalized
poral scale parameter that determines the shape of the tempo-               Gaussian distribution to the estimated distribution for each
ral filter. We used 5 temporal scales in our implementation                 feature:
τ = 0.025, 0.05, 0.1, 0.2, 0.4. Figure 1 shows how h(t; τ) is
                                                                                                                                          !
                                                                                                                θ                     r θ
formed and how it varies with τ. We will refer to h(t; τ) as                                   p(r; ς, θ) =             exp −                 .
a DoE (Difference of Exponentials) due to h0 (t; τ)’s similar-                                               2ςΓ( θ1 )                ς
ity with the exponential distribution. We choose DoE as a                   In this equation, θ is the shape parameter, ς is the scale pa-
temporal filter for the following reasons:                                  rameter and r is the filter response. This resulted in one shape
                                                                            parameter, θi, j , and one scale parameter, ςi, j , for each of the
• limt→−∞ h(t; τ) = 0. Therefore frames in the distant past
                                                                            75 filters: i = 1, 2, ..., 15 is the index for spatial filters, and
   do not contribute to the current saliency.
                                                                             j = 1, 2, ..., 5 is the index for temporal scales. The general-
• Σ0−∞ h(t; τ)dt = 0. If a part of the scene does not change for            ized Gaussians provide an excellent fit to the data.
   a extended period of time, it ceases to be salient.                          Taking the logarithm, we obtain the log probability over
                                                                            the possible values of each feature:
• h(t; τ) is largest near t = 0 and falls off rapidly. This says
   that DoE has a strong response to onset and offset of ob-                                                         fi, j θi, j
   jects.                                                                                       log p(Fi, j ) = −                + const.              (1)
                                                                                                                    ςi, j
• The DoE bears some resemblance to the temporal re-                        These feature responses are not independent, but we proceed
   sponses of some neurons in LGN of cats(Cai, Deangelis,                   as if they are for simplicity. Saliency can then be calculated
   & Freeman, 1997).                                                        with a simple formula:
• Using DoE as temporal filters enables very efficient online                                                           5   15          θi, j
                                                                                                                                  fi, j
   calculation of the spatiotemporal filter responses (shown                        log s = − log p(F = f ) =         ∑∑                      + const.
   below).                                                                                                            j=1 i=1     ςi, j
   With the exception of the last property, these properties are            This equation shows how easily bottom-up saliency is to cal-
all shared with the DoG. Because all filters are linear:                    culate in SUN; raw filter responses are scaled and shaped
                                                                            by the learned parameters and combined through summation.
F(x, y,t; σ, τ) = V (x, y,t) ∗ g(x, y; σ) ∗ h(t; τ)
                                                                            It’s worth repeating that aside from feature selection, all pa-
                   = V (x, y,t) ∗ g(x, y; σ) ∗ (h0 (t; 2τ) − h0 (t; τ))     rameters of the model are completely determined by natural
                   = F 0 (x, y,t; σ, 2τ) − F 0 (x, y,t; σ, τ)               statistics.
                                                                        2946

              Table 1: Summary of initial results.
                 Method               KL      ROC area
                 Chance                0          0.5
            Bayesian Surprise        0.133       0.647
                  SUN                0.100       0.626
         SUN (w/ 8 pixel border) 0.181           0.660
           Centered Gaussian         0.441       0.764
                           Results
                                                                      Figure 2: Center bias and border effects. Top left: Overall
We evaluate our saliency algorithm on the human fixation              average of human fixations on the Itti (2005) dataset. Top
data from (Itti, 2005). Eye movements were recorded from 8            right: A 2D gaussian fit to the fixation data from (Bruce &
subjects viewing 50 videos from indoor and outdoor scenes,            Tsotsos, 2006). Bottom row: The average saliency map over
television broadcasts, and artificial environments totaling           all frames of the Itti (2005) dataset for three models: Bayesian
over 25 minutes of video at 640 × 480 (at 60.27 Hz, a viewing         Surprise, SUN, and SUN with an 8 pixel zeroed-out border.
distance of 80 cm, and with a field of view of 28 deg ×21 deg).
Data was collected using an ISCAN RK-464 tracking the
right eye. Two hundred eye movement traces were used (four            our saliency map because of filter convolutions extending be-
subjects for each video clip). See (Itti, 2005) for more details.     yond the image. We therefore set the border of our map to
   Itti and Baldi (2008) reports results of their saliency mea-       zero remove the invalid portion of the convolution and ap-
sure (Bayesian surprise) on this data set. Under this the-            proximate the borders present in (Itti & Baldi, 2008). Surpris-
ory, organisms form models of their environment, and assign           ingly, this drastically improved the evaluation scores. This
probability distributions over the possible models. Upon the          was most apparent in the KL measurement; modifying the
arrival of new data, the distribution over possible models is         border had large effects on the random-saccading distribution
updated with Bayes rule, and the KL divergence between the            of salience, but little effect on the distribution of salience for
prior distribution and posterior distribution is measured. The        human saccades. Hence, depending on how edges were han-
more the new data forces the distribution to change, the larger       dled, we could report performance that was better or worse
the divergence. These KL scores of different distributions            than Itti and Baldi (2008).
over models combine to produce a saliency score.                         This appears to be a function of a phenomenon in such data
   Saliency maps were sampled at the target location of a sac-        sets known as center bias (Parkhurst & Niebur, 2003; Tatler,
cade at the time the saccade was initiated. By histogram-             Baddeley, & Gilchrist, 2005; Zhang et al., 2008). Hence we
ming the number of actual fixations for each value of salience,       decided to look at how well a “saliency measure” based on
a distribution of saliency was formed for human fixations.            a simple Gaussian fit to the distribution of human saccades
This could be compared with the distribution of fixations over        from another data set (on the static images in (Bruce & Tsot-
saliency for random ”fixations” chosen uniformly over the             sos, 2006)) would perform on this data set (see Figure 2).
image. By looking at the KL divergence (”distance”) between           This simple technique drastically outperformed our results
the two distributions, we get the KL score we report (Itti &          and the the surprise model using these metrics (Table 1). The
Baldi, 2008). The farther from the distribution of salience           reason for this is clear by visual inspection of the data in Fig-
over random locations, the better.                                    ure 2. The human data is highly center-biased, and so adding
   We also use the ROC area evaluation method, where each             a larger border increases performance. The width of the bor-
saliency map is treated as a binary classifier that, for a given      der added to SUN in the right hand image was approximately
threshold, classifies points with salience above the threshold        equal to the darkened borders of Itti and Baldi (2008), and
as fixated and those below the threshold as not fixated. By           led to our model outperforming theirs. This finding for fix-
varying the threshold and comparing the performance at each           ations while viewing video is consistent with earlier studies
threshold to the human fixations (which are treated as ground         involving static images, showing that a simple model that pre-
truth), an ROC curve is obtained. The area under the curve re-        dicts that saliency falls off with distance from the center of
flects how well the saliency map predicts the human fixations         the screen outperforms other models (Le Meur, Le Callet, &
(Gao & Vasconcelos, 2007a; Bruce & Tsotsos, 2006).                    Barba, 2007; Zhang et al., 2008). It is hard to tell whether the
   The results are shown in Table 1. ”Chance” indicates base-         difference between two algorithms is due to the model or sim-
line performance. The KL score for Bayesian Surprise is               ply differences in treating the filter responses on the border.
smaller than that reported in (Itti & Baldi, 2008) because they       Clearly, a better method of evaluation is needed.
use an extra step of taking the maximum in a local window                The fundamental problem is that sampling image locations
on the saliency map. We found this step systematically in-            uniformly is not at all indicative of how human saccades tend
creases all the reported measurements but does not change             to be distributed. (Parkhurst & Niebur, 2003; Tatler et al.,
our qualitative conclusions. We observed border artifacts in          2005) have suggested that the random fixations should be
                                                                  2947

       Table 2: Summary of results with shuffled metric.
                 Method            KL      ROC area
            Chance/Gaussian         0          0.5
            Bayesian Surprise     0.034      0.581
           Dynamic Saliency       0.041      0.582
drawn with the location distribution of human fixations. We
therefore modified the KL measurement to account for this.
Instead of forming the baseline (comparison) distribution of
salience by counting how often each salience value occurs at
random locations sampled from the image, we form it instead
by counting how frequently salience values occur at human
fixations in an image. We use the locations of human fix-
ations from a different frame of the video and measure the
salience values at those locations. Then we compare the dis-
tribution of salience values at the locations humans fixated in
                                                                      Figure 3: The saliency maps for several frames of video from
each frame to the salience values in that same frame, but using
                                                                      (Itti, 2005).
fixations from a different frame. I.e., the comparison distri-
bution is created by shuffling the frames of the saliency maps        person who is calmly listening.
over each movie, giving them the human spatial distribution               Despite the similarity in performance, our model is signif-
but not the temporal distribution. Put another way, rather than       icantly simpler. We use 15 spatial filters and learn 75 distri-
determining whether subjects looked at the most salient loca-         butions offline. Bayesian Surprise, in contrast, uses 72 spatial
tion in each frame, we instead measure whether they look at a         filters and must maintain 432,000 distributions that must be
fixated point when it is most salient. This has the desired ef-       updated with each frame. This difference in complexity has
fect of causing the simple static measure of fitting a Gaussian       consequences on runtime; on a Pentium 4 3.8 GHz dual core
to the human distribution to have a score of zero; since this         PC with 1 GB RAM, SUN runs through a video of about 500
static version of saliency does not change, shuffling has no ef-      frames in minutes while Bayesian Surprise requires hours. A
fect on the distribution of salience value counts. We modified        version of SUN designed to run in faster than real time with
the ROC metric similarly. A related method was proposed in            only modest decreases in performance is described by Butko,
(Tatler et al., 2005) and used in (Zhang et al., 2008) for static     Zhang, Cottrell, and Movellan (2008) and was shown to have
images. However, the temporal component makes the metric              applications in a social robotics environment1 .
more stringent than when shuffling a set of independent im-
ages - for video, this necessitates accurate prediction of the                                    Discussion
timing of fixations. As discussed in (Carmi & Itti, 2006) this        In this paper we generalized our principled probabilistic mea-
metric underestimates the model performance since the cen-            sure of saliency (Zhang et al., 2008) to video. In our formu-
ter of the screen for pictures and video genuinely tends to be        lation, bottom up saliency emerges as the self-information of
the most salient part of the scene due to cameraman (or direc-        visual features when estimating the probability of finding a
tor) bias. However, this is still a useful measure for relative       target. We designed a feature space that can be calculated
model comparisons, serving as a lower bound assessment of             very efficiently, which leads to a simple, fast algorithm.
models’ prediction ability.                                               Our findings also agree with (Parkhurst & Niebur, 2003;
   Nevertheless, our method continues to do better than               Tatler et al., 2005) in pointing out some disadvantages of
chance, and slightly better numerically than Itti and Baldi’s         using some of the previously proposed evaluation metrics.
surprise model (Itti & Baldi, 2008) on this data set, as shown        Data collected in a lab often show a strong center bias that
in Table 2. Scores of both models may appear low, but the             confounds proper evaluation of the results. By shuffling the
strictness of our evaluation metric needs to be remembered;           frames but maintaining the patterns of fixations, we effec-
we’re evaluating whether the model predicts when a fixation           tively remove the effects of this bias. However, as (Carmi
will be made to a location, not simply where as in Table 1.           & Itti, 2006) points out, there is also a central bias introduced
This demonstrates that saliency models outperform the base-           by having humans center the camera on interesting parts of
line Gaussian after compensating for the center bias.                 the scene - the center is inherently more likely to be salient.
   Figure 3 shows the saliency maps on some frames of differ-             Overall, our results show comparable performance with Itti
ent videos. This provides some insight into what SUN finds            and Baldi’s surprise model (Itti & Baldi, 2008) in predicting
salient. For instance, motion is one of the most salient fea-         human fixations despite the relative simplicity of the SUN
tures, as shown clearly in the first row; the person that is              1 FastSaliency code is available for download at http://mplab.
moving while talking is far more salient than the face of the         ucsd.edu/˜nick/NMPT/
                                                                  2948

model. The efficiency of the SUN model is due to two main                     ence on Computer Vision. Rio de Janeiro, Brazil: IEEE
factors: First, we give our model experience with other videos                Computer Society.
that allow us to precompute what is novel, rather than what           Gao, D., & Vasconcelos, N. (2007b). The discriminant
is “currently unexpected.” Second, the particular form of our                 center-surround hypothesis for bottom-up saliency. In
temporal component, the Difference of Exponentials, allows                    Advances in Neural Information Processing Systems
for a linear, nearly memoryless updating of the salience map.                 20. Cambridge, MA: MIT Press.
Both of these lead to a model that could plausibly be com-            Itti, L. (2005). Quantifying the contribution of low-level
puted by neurons. Furthermore, the search asymmetries dis-                    saliency to human eye movements in dynamic scenes.
cussed by Zhang et al. (2008) provide additional motivation                   Visual Cognition, 12(6), 1093-1123.
for using prior statistics. We do not deny that there are ef-         Itti, L., & Baldi, P. F. (2008). Bayesian surprise attracts hu-
fects of more recent history, and in the end, the right an-                   man attention. Vision Research.
swer might be some combination of precomputed statistics              Itti, L., Koch, C., & Niebur, E. (1998). A model of saliency-
and more temporally local statistics.                                         based visual attention for rapid scene analysis. IEEE
   In our future work, we intend to investigate such is-                      Transactions on Pattern Analysis and Machine Intelli-
sues such as the effects of a foveated retina (currently both                 gence, 20(11), 1254-1259.
Bayesian Surprise and our model are applied to the entire im-         Kanan, C., Tong, M. H., Zhang, L., & Cottrell,
age at the same resolution), and generalizing our notion of                   G. W. (in press). Sun: Top-down saliency us-
salience to one of utility, as in (Nelson & Cottrell, 2007).                  ing natural statistics.     Visual Cognition.       (DOI:
                                                                              10.1080/13506280902771138)
                    Acknowledgements                                  Koch, C., & Ullman, S. (1985). Shifts in selective visual
The authors would like to thank Laurent Itti & Pierre Baldi                   attention: towards the underlying neural circuitry. Hu-
and Neil Bruce & John Tsotsos for sharing their human fix-                    man Neurobiology, 4(4), 219–27.
ation data and algorithms. We would also like to thank                Le Meur, O., Le Callet, P., & Barba, D. (2007). Predict-
Dashan Gao, Dan Hill, Honghao Shan, Piotr Dollar, Nick                        ing visual fixations on video based on low-level visual
Butko, Javier Movellan, and Tim Marks for helpful discus-                     features. Vision Research, 47(19), 2483–2498.
sions. This work was supported by NIMH grant MH57075                  Nelson, J., & Cottrell, G. (2007). A probabilistic model of
to GWC and NSF grant #SBE-0542013 to the Temporal Dy-                         eye movements in concept formation. Neurocomput-
namics of Learning Center, Garrison W. Cottrell, PI.                          ing, 70(13-15), 2256-2272.
                                                                      Parkhurst, D., & Niebur, E. (2003). Scene content selected
                          References                                          by active vision. Spatial Vision, 16(2), 125-154.
Bruce, N., & Tsotsos, J. (2006). Saliency based on infor-             Song, K. (2006). A globally convergent and consistent
       mation maximization. In Y. Weiss, B. Schölkopf, &                     method for estimating the shape parameter of a gen-
       J. Platt (Eds.), Advances in Neural Information Pro-                   eralized gaussian distribution. IEEE Transactions on
       cessing Systems 18 (p. 155-162). Cambridge, MA:                        Information Theory, 52(2), 510-527.
       MIT Press.                                                     Tatler, B., Baddeley, R., & Gilchrist, I. (2005). Visual cor-
Butko, N. J., Zhang, L., Cottrell, G. W., & Movellan, J. R.                   relates of fixation selection: effects of scale and time.
       (2008). Visual saliency model for robot cameras. In                    Vision Research, 45(5), 643–59.
       Proceedings of the 2008 IEEE international conference          Torralba, A., Oliva, A., Castelhano, M. S., & Henderson,
       on robotics and automation (icra) (pp. 2398–2403).                     J. M. (2006). Contextual guidance of eye move-
       Pasadena, CA, USA.                                                     ments and attention in real-world scenes: the role of
Cai, D., Deangelis, G., & Freeman, R. (1997). Spatiotempo-                    global features in object search. Psychological Review,
       ral receptive field organization in the lateral geniculate             113(4), 766–786.
       nucleus of cats and kittens. Journal of Neurophysiol-          Wolfe, J. (2001). Asymmetries in visual search: An intro-
       ogy, 78(2), 1045–1061.                                                 duction. Perception & Psychophysics, 63(3), 381–389.
Carmi, R., & Itti, L. (2006). The role of memory in guiding           Zhang, L., Tong, M. H., & Cottrell, G. W. (2007). Infor-
       attention during natural vision. J. Vision, 6(9), 898–                 mation attracts attention: a probabilistic account of the
       914.                                                                   cross-race adavantage in visual search. In D. S. McNa-
Fantz, R. (1964). Visual experience in infants: Decreased at-                 mara & J. G. Trafton (Eds.), Proceedings of the 29th
       tention to familiar patterns relative to novel ones. Sci-              Annual Conference of the Cognitive Science Society (p.
       ence, 146(3644), 668.                                                  749-754). Nashville, Tennessee: Cognitive Science So-
Gaborski, R., Vaingankar, V., Chaoji, V., Teredesai, A., &                    ciety.
       Tentler, A. (2004). Detection of inconsistent regions in       Zhang, L., Tong, M. H., Marks, T. K., Shan, H., & Cot-
       video streams. In Proceedings of SPIE (pp. 202–210).                   trell, G. W. (2008). SUN: A Bayesian framework for
       Bellingham, WA: SPIE.                                                  saliency using natural statistics. Journal of Vision, 8(7),
Gao, D., & Vasconcelos, N. (2007a). Bottom-up saliency is                     1-20.
       a discriminant process. In IEEE International Confer-
                                                                  2949

