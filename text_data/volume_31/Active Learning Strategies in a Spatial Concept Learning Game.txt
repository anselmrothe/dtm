UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Active Learning Strategies in a Spatial Concept Learning Game
Permalink
https://escholarship.org/uc/item/55b5m116
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 31(31)
Authors
Gureckis, Todd
Markant, Doug
Publication Date
2009-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                  Active Learning Strategies in a Spatial Concept Learning Game
                                            Todd M. Gureckis (todd.gureckis@nyu.edu)
                                              Doug Markant (doug.markant@nyu.edu)
                                              New York University, Department of Psychology
                                              6 Washington Place, New York, NY 10003 USA
                               Abstract                                     In order to analyze participant’s performance in such a
   Effective learning often involves actively querying the envi-         complex and dynamic learning task, we develop a formal
   ronment for information that disambiguates potential hypothe-         model of information search based on Bayesian learning prin-
   ses. However, the space of observations available in any situ-        ciples. The model specifies how past observations should in-
   ation can vary greatly in potential “informativeness.” In this
   report, we study participants’ ability to gauge the informa-          fluence current beliefs, and how uncertainty should translate
   tion value of potential observations in a cognitive search task       into information sampling behavior on a trial-by-trial basis.
   based on the children’s game Battleship. Participants selected        By comparing the utilities the model assigned to each possi-
   observations to disambiguate between a large number of po-
   tential game configurations subject to information-collection         ble observation with the selections that participants make, we
   costs and penalties for making errors in a test phase. An “ideal-     are able to characterize the efficiency of participants’ search
   learner” model is developed to quantify the utility of possi-         strategies relative to an ideal learner who had experienced the
   ble observations in terms of the expected gain in points from
   knowing the outcome of that observation. The model was used           same set of previous observations. In addition, the model al-
   as a tool for measuring search efficiency, and for classifying        lowed us to objectively classify particular information collec-
   various types of information collection decisions. We find that       tion decisions as being either “exploitative” of known contin-
   participants are generally effective at maximizing gain relative
   to their current state of knowledge and the constraints of the        gencies or “exploratory” of relatively unknown parts of the
   task. In addition, search behavior shifts between an slower, but      game board. We find interesting behavioral differences be-
   more efficient “exploitive” mode of local search and a faster,        tween these two modes of information search.
   less efficient pattern of “exploration.”
                                                                         Active Sampling in Concept Acquisition
   Traditional experimental approaches to human learning
tend to emphasize passive learning situations. For example,              The ultimate goal of the present work is to understand how
in a typical concept learning task, subjects are presented with          people actively seek information when acquiring new con-
examples one at a time, the order of which are selected by               cepts. Previous work has shown that allowing learners to
the experimenter (often at random and with exhaustive sam-               make their own decisions about what information to sample
pling of the training set). However, this procedure ignores              can have an impact on both the efficiency of learning (Castro
the fact that real-world learning often requires learners to ac-         et al., 2008) and what is learned (Fazio, Eiser, & Shook,
tively create their own learning experiences by constructing             2004) in concept acquisition tasks. For example, Castro, et
revealing queries or engaging in exploration of unknown con-             al. (2008) found that allowing learners to actively select train-
tingencies (Nelson, 2005; Skov & Sherman, 1986; Sutton &                 ing examples greatly improved the efficiency by which they
Barto, 1998). For example, children might ask about partic-              learned a linear decision boundary. Fazio, Eiser, and Shook
ular objects in their environment and receive feedback from              (2004) studied the impact of experiential sampling on cat-
an adult (e.g., “What is that?”, “What does that do?”). In or-           egory learning. Participants were presented with different
der for such sampling behavior to be effective, queries should           “beans” and were asked on each trial if they would like to
be directed to maximize the potential information that could             (virtually) eat the bean and find out if it was healthy or poi-
be obtained from an answer. We need not ask about things                 sonous. Decisions to not eat a particular bean thus provided
that we already know, and, all else being equal, should pre-             no information (i.e., feedback in the task was contingent on
fer questions whose answers are expected to be most reveal-              sampling decisions). Experiential learners were found to be
ing (Klayman & Ha, 1987; Nelson, 2005; Oaksford & Chater,                risk averse, in that they showed a bias to think that novel
1994).                                                                   beans were bad and were more accurate classifying bad beans
   In this paper, we present an initial study examining the mu-          than good beans. Interestingly, this asymmetry in learning
tual unfolding of information search and learning in a task              only occurred in situations where learners made the sampling
modeled on the classic children’s game Battleship. Partici-              decisions themselves as opposed to conditions where full in-
pants attempted to learn a hidden “concept” (the shape and               formation was provided on each trial.
spatial configuration of three hidden rectangles) by sequen-                While allowing learners to make decisions about what they
tially uncovering points on a large grid (see Figure 1). Each            want to learn about can have interesting consequences for
observation cost points and participants were motivated to               what is learned, these studies leave aside the question of ex-
minimize the points accumulated in each game. As a result,               actly how people decide which observations to make. How-
they had to minimize the number of observations they made                ever, assessing the “usefulness” of potential observations has
in order to correctly identify the hidden rectangle configura-           a long history of study in psychology, particularly in hypoth-
tion.                                                                    esis testing situations (Skov & Sherman, 1986; Klayman &
                                                                     3145

                  What is the hidden rectangle concept?                  with the fewest number of observations possible. In order
                                                                         to formalize the costs of information collection, participants
                ?            ?            ?               ?              start each game with zero points, and each observation (i.e.,
                                                                         choice to turn over a point on the grid) added one point to
                 What should I sample next to figure it out?
                                                                         their score. Subjects could choose to end the sampling phase
                ?            ?            ?               ?              at any point by clicking on a button at the bottom of the screen
                                                                         which would begin the test phase.
                                                                             In the test phase, subjects were presented with an empty
                                                                         grid, and were asked to “paint in” the correct position, shape,
                                                                         and color of each of the three rectangles using the computer
                                                                         mouse. Participants were informed (prior to the start of the
                                                                         task and at the beginning of the test phase) that each incor-
                                                                         rectly colored grid point would cost two (2) additional points.
Figure 1: The information search problem studied in this report.         Thus, errors were overall more costly than collecting addi-
Three rectangles of unknown sizes, shapes, and positions are hid-
den in a 10x10 grid. On each trial, participants make selections of      tional samples. Following the painting phase, participants
which grid cell to turn over (revealing either a miss or a part of a     were shown their final score for that game which was a com-
hidden rectangle). The goal is to discover the true hidden concept       bination of the points incurred due to making observations in
by actively choosing which new observations to make. Efficiency is
encouraged via a cost structure imposed on the task.                     the first phase, and the points incurred for making errors in
                                                                         the test phase.
Ha, 1987; Oaksford & Chater, 1994). A classic finding is                                   A Bayesian Search Model
that participants often show a bias towards using confirma-
                                                                         In the following section, we describe a simple Bayesian
tory test strategies when testing between alternative hypothe-
                                                                         model of the task. It is important to point out that the model
ses (Wason, 1960). While these original findings raised con-
                                                                         we describe is not meant to mimic the specific cognitive strat-
cerns about the intuitive reasoning abilities of humans, other
                                                                         egy that participant’s use while learning. Instead, our goal is
researchers have developed probabilistic approaches which
                                                                         to formally specify the behavior of an “ideal” learner who
calculate the expected information gain (among other norms)
                                                                         searches for information under the cost structure imposed by
of potential questions given an appropriate set of prior beliefs
                                                                         the task, and to use this model as a tool for understanding
and find that such utilities can often account for participants’
                                                                         human performance1 .
choices (Nelson, 2005; Oaksford & Chater, 1994).
   Our work is inspired by these previous probabilistic ap-                  In formal terms, players in the game are presented with a
proaches to hypothesis testing and information search as well            NxN grid of squares and are asked to sequentially make ob-
as research on “active learning” algorithms in the machine               servations in order to learn the identity of the hidden game
learning literature which seek to optimize data selection for            board concept, ghidden ∈ G, where G is the universe of legal
training artificial learners (Mackay, 1992). However, most               game boards. Each individual game board is defined by a set
                                                                                                                      g g g
studies in the hypothesis testing literature have focused on             of three, non-overlapping rectangles, {r1 , r2 , r3 }, and each in-
simple, one-trial judgments and reasoning tasks with a rela-             dividual rectangle rn is denoted by a quadruple (xn , yn , wn , hn )
tively constrained hypothesis space (often times there is only           where xn and yn are the coordinates of the top left corner of
a small set of possible hypotheses participants are attempt-             the rectangle in the grid and wn and hn are the width and
ing to distinguish). In contrast, our goal is to understand how          height, respectively.
people search for information in complex, ongoing learning               Learning On each trial, the player selects a single square
tasks where they must continually updated expectations based             in the grid, xi j , and receives feedback about if it belongs to
on past observations and use these expectations to drive new             r1 , r2 , or r3 , or isn’t part of any rectangle. We denote the
information-seeking behaviors.                                           feedback (or observed label) as ln where l0 means that the ob-
                                                                         served point is empty, l1 means it falls within rectangle r1 ,
The Rectangle Search Game                                                and so on (for short-hand, we simply denote a sampled loca-
In the rectangle search game (see Figure 1), the player is pre-          tion and its associated label as xi j = ln ). Since each point in
sented with a 10x10 grid that contains three, non-overlapping            the grid is assigned to either one or zero rectangles and this
hidden rectangles of different colors (i.e., the unknown con-
cept). On each trial, players can choose to turn over one                     1 In this sense our model provides a “rational analysis” of the
square in the grid, revealing either part of a hidden rectan-            task. However, we are unable at the current stage of this work to call
                                                                         our model the complete rational solution. One reason is that (for
gle or a blank space. Each game is divided into two phases:              computational reasons) the current model assumes that participants
an information collection phase, where participants make se-             always choose the option with the highest expected saving on each
lections of grid points to uncover and receive feedback, and             trial (i.e., they assume the game will end on the next trial). It is
                                                                         possible that participants engaged in multi-step planning which may
a test phase. In the first phase, participants are told that their       change the model’s valuation of particular observations, an issue we
goal is to discover the identity of all three hidden rectangles          hope to evaluate in future work.
                                                                     3146

assignment is deterministic, we assume that the likelihood of
a particular observation and associated label given a particu-                                     4
lar game board configuration is given by:                                     EC(G) = ∑ ∑ ∑            p(xi j = ln |G) · [Chit · p(xi j = ln |G)
                                      (                                                     i  j n=0
                                                        g
                                        1 if xi j ∈ rn ,                                                  +Cmiss · (1 − p(xi j = ln |G))]        (5)
                   p(xi j = ln |g) =                                 (1)
                                        0 otherwise.                        which simply says that the cost associated with painting
for n > 0 (see Tenenbaum, 1999 for a similar formulation in                 square xi j = ln is related to the probability that we currently
a similar task). Alternatively, if xi j = l0 then,                          believe that square x should be painted color ln (or not) times
                                                                            the cost associated with being either correct or incorrect.
                      (
                                        g             g           g
                                                                            These expected costs are then weighted by our overall esti-
                          0 if xi j ∈ r1 or xi j ∈ r2 or xi j ∈ r3 ,        mate that the true state of affairs is that square xi j actually is
   p(xi j = l0 |g) =                                                 (2)
                          1 otherwise.                                      colored with label ln .
                                                                               We can also calculate the saving (S) from making any ob-
This captures the basic intuition that empty squares provide                servation as:
support for hypotheses that don’t place the observation within
a rectangle, while hits provide evidence for hypotheses that                      S(G, xi j = ln ) = EC(G) − [EC(G|xi j = ln ) +Cobs ]           (6)
do.                                                                         and the expected savings (ES) from observation xi j is found
   The prior belief about the likelihood of each individual                 by calculating a weighted average of the savings based on our
game board is represented by p(g). In our experiments, par-                 current belief about the possible labels associated with xi j :
ticipants were instructed that rectangles were chosen at ran-
dom and that each possible game board configuration was                                                4
equally likely (i.e., p(g) = 1/|G| for all g, a uniform prior).                       ES(G, xi j ) =  ∑ p(xi j = ln |G)S(G, xi j = ln )          (7)
                                                                                                      n=0
This prior, along with a piece of new data, (xi j = ln ), gives us
the following posterior belief about the identity of the hidden             The choice that maximizes expected savings is predicted to
board according to Bayes rule:                                              be the best choice on any trial according to the “greedy” strat-
                                                                            egy2 .
                                        p(xi j = ln |g)p(g)
          p(g|xi j = ln ) =                                          (3)    Classifying Search Behavior as “Exploration” or “Ex-
                                   ∑gh ∈G p(xi j = ln |gh )p(gh )
                                                                            ploitation” In addition to predicting which observations
On each trial, the posterior belief p(g|xi j = ln ) following from          participants should make on each trial, the model provides a
the last observation is used as the new prior allowing for in-              simple way to classify each observation as either Exploiting
cremental updating of our beliefs with each new observation.                local evidence for a rectangle or Explore-ing relatively un-
After each update, the new prior is also used in predicting the             known regions of the board. Prior to making a “hit” (i.e., an
label associated with any point yi j in the grid. The predicted             observation that reveals a rectangle rather than empty space),
probability that location yi j = lk (for k ∈ 0, 1, 2, 3) given our          all observations are treated as Explore. Following a hit for
current belief p(g) is:                                                     rectangle rn , we can compute the posterior probability that
                                                                            each grid location belongs to rectangle rn using Equation 4.
               p(yi j = lk ) =   ∑    p(yi j = lk |gh )p(gh )        (4)    Observations where the actual sample, xi j , match the con-
                                gh ∈G                                       straint 0 < p(xi j = ln ) < 1.0 suggest targeted attempts to
Assessing the Value of Future Observations We now con-                      decrease uncertainty about rectangle rn and are considered
sider how agents might use their beliefs at any point to select             Exploit. In contrast, actions where p(xi j = ln ) = 0 and
the best new samples to learn about (i.e., active learning).                0 < p(xi j = lk ) < 1.0 for any rectangle rk that has not yet been
In our experiment, participants were given the explicit goal                discovered are Explore. Additionally, observations may be
of minimizing the number of points they accumulated during                  classified as errors if they fail to resolve any uncertainty ac-
each game, where each individual observation cost them Cobs                 cording to the model (i.e., sampling where p(xi j = ln ) = 1 for
point, each error during painting/recall cost Cmiss points, and             one rectangle, or p(xi j = ln ) = 0 for all rectangles).
each correctly colored square cost Chit points. Given these
                                                                                                     The Experiment
costs, we can quantify the value of particular observations
                                                                            Participants and Apparatus Six undergraduates at New York
with respect to the overall objective of minimizing accumu-                 University participated in the study to fulfill part of a class require-
lated points.                                                               ment. The experiment was run on standard Macintosh computers
   Formally, we assume that the goal of the learner on each                 over a single session.
trial is to select the observation xi j that minimizes the ex-                  2 Our model ties sampling behavior to the cost structure imposed
pected points that would be incurred during the recall phase if             on the task. However, another solution we considered but do not
the game were to end after that observation was made. With                  report (due to space) assumes that agents to make observations that
                                                                            provide the greatest reduction in uncertainty about the game board in
costs Cmiss and Chit as defined above, we compute the ex-                   information theoretic terms (Kruschke, 2008; Nelson, 2005; Oaks-
pected cost EC(G) given the current beliefs as:                             ford & Chater, 1994).
                                                                        3147

                                     Exploit            Exploit                   Exploit
       A                                                                                          Best available            B
                        4                                                                                                              r3
                                                                                                 Sampled point
    Expected Savings
                                                                                                  Average of
                        3
                                                                                                  uncertain points
                                                                                                                            Feedback
                                                                                                                                       r2
                                                                                                  (with 1 std. dev.)
                        2
                                                                                                Rectangle 1
                                                                                                                                       r1
                        1                                                                       Rectangle 2
                                                                                                Rectangle 3
                        0                                                                                                          Miss
                            0        5         10       15        20         25       30                                                    0           5    10         15        20        25         30
                            Trial in Game                                                                                                   Trial in Game
   C Trial
             1                  2        3          4         5          6        7         8    9        10           11              12          13       14    ...        29        30         31        32
   Samples and Hidden Rectangles
                                                                                                                                                                  ...
                       Explore                                         Exploit                                 Explore                           Exploit          ...                   Exploit
                                                                                                                                                                  ...
Figure 2: Comparison of a typical human search pattern in a single game along with the predictions of the model (subject 3, game 1). The
solid line in Panel A reflects the expected savings assigned to the participant’s sampling choices (Eqn. 7). The top dashed line demonstrates
the maximum expected savings that could be obtained from any choice on a given trial. The participant’s choices were maximally informative
if the solid line matches this upper bound. In order to determine a lower bound, we computed the average expected savings assuming that the
participant chose randomly among the remaining uncertain grid points (lower line, along with one std.dev. of that mean). The shaded regions
indicate choices the model classified as Exploit, while the unshaded regions reflect Explore trials. Regions are color coded according to
which rectangle is being exploited. Panel B shows the feedback the participant received on each trial. Finally, Panel C plots individual
observations super-imposed on the hidden game board (top row) along with the dynamic evolution of ES(G, xi j ) over the entire grid as
evidence accumulates (bottom row). In both sequences, the small black ‘x’ shows the participant’s sample. The shaded regions in the top
row show the hidden rectangles for this game. In the bottom row, darker colors indicate that the model predicts higher expected savings from
selecting that point. Note how the model’s estimate of the most useful information accumulates in the regions surrounding hits (e.g., trials
2-8 or 11-14). See http://smash.psych.nyu.edu/projects/activelearning/ for some supplementary information including movies.
Procedure At the start of the task, participants were given on-                                           the screen throughout the information collection phase. There were
screen instructions detailing the rules and objectives, followed by                                       no time constraints on sampling, and subjects could choose to end
a single practice game. In addition, participants were shown a set                                        the first phase at any point by clicking on a button at the bottom of
of 50 randomly generated legal gameboards on the screen to help                                           the screen which would begin the test phase (described above).
develop an appropriate prior expectation about what they might en-                                           The memory demands of the test phase were significant. How-
counter in the task. Participants also completed a questionnaire                                          ever, the game was self-paced and thus, prior to terminating search
which tested for understanding of the game. After the experimenter                                        participants could spend as much time as they wanted memorizing
was confident that participants had a complete understanding of the                                       the layout of the uncovered rectangles. Overall, we were less inter-
task, the experiment began.                                                                               ested in participant’s performance during the test phase, as much as
   For the remainder of the session, participants played a sequence                                       using that phase to set up an incentive for participants to actually
of games at their own pace. In order to facilitate between-subject                                        discover the true board during the search phase.
comparisons, the sequence of games experienced by each player was
identical. Each game-board contained three hidden rectangles that                                         Results
were drawn randomly from a fixed set of eight possible sizes (1x3,
3x1, 1x4, 4x1, 2x2, 3x2, 2x3, 3x3, 2x4, 4x2, 3x4, 4x3) and were                                           Participants completed a variable number of games before the
placed on the grid with the constraint that the entire rectangle laid                                     end of the session; however, in our initial analysis (and due
inside the boundaries of the grid, and there were no overlapping rect-                                    to the computational complexity of computing the full model
angles As a result, there were 563,559,150 possible game boards that
the participant had to distinguish in order to identify the true game                                     solution of each trial of every game) we considered only the
board.                                                                                                    first twenty games that everyone completed (a total of 120
   During the information collection phase, participants made ob-                                         unique games). On average, participants made 34.4 (SD=3.6)
servations by clicking on a grid point, after which the point changed                                     observations and 1.8 sampling errors (observations that were
color according to its category membership (rectangle 1=red, rect-
angle 2=blue, rectangle 3=green, or no rectangle=grey). Throughout                                        redundant given their current knowledge) per game.
the entire information collection phase, a visual reminder of the pos-                                       Figure 2 presents the data from a typical game and gives
sible shapes and size of the rectangles was provided on screen to the                                     some intuition for the dynamics of the model (see the cap-
right of the current gameboard. After each observation, the subject’s
total number of samples was incremented and displayed at the top                                          tion for a full description). In particular, Figure 2A shows the
of the screen as a reminder. All previous observations remained on                                        overall expected savings of a participant’s choices compared
                                                                                                     3148

                                      0.12
    A                                               Rank 1                        B                          1.0                                                          possible choices on any given trial).
                                                                                          Cumulative Freq.
                                      0.10
                     Relative Freq.
                                                                                                             0.8
                                      0.08                                                                                                                                Search strategies The preceding analysis might suggest
                                      0.06
                                                                                                             0.6                                 75%
                                                                                                                                                                          that participants’ decisions were generally well calibrated to
                                                                                                             0.4
                                      0.04
                                                                                                                                                 50%                      the structure of the task and reasonably followed the pre-
                                      0.02                                                                   0.2
                                      0.00
                                                                                                                                                                          dictions of the model, but there were also systematic devi-
                                             1       20     40      60      80    100                              1                    20      40   60     80     100
                                             Rank                                                                  Rank                                                   ations between the model and human data (e.g., the exam-
                                                                                                                                                                          ple game in Figure 2A shows that choices during Exploit
    C
                              0.8                                           Explore                                    D
                                                                            Exploit                                                     1200
                                                                                                                                                                          trials were somewhat more effective compared to Explore
                              0.6
                                                                                                                                                                          trials). In order to quantify this effect, we computed a “rela-
                                                                                                                       Median RT (ms)
                                                                                                                                        1000
         Average RSI
                                                                                                                                                                          tive savings index” (RSI) which compares the expected sav-
                                                                                                                                        800
                              0.4                                                                                                                                         ings of the participant’s observation on each trial to the max-
                                                                                                                                        600
                                                                                                                                                                          imum expected savings available on the current gameboard
                              0.2                                                                                                       400                                           ES(G,xi j )
                                                                                                                                                                          (RSI = Max[ES(G,y      i j )]
                                                                                                                                                                                                        , where the Max[] is computed over all
                                                                                                                                        200
                                                                                                                                                                          possible observations yi j ). An RSI of 1.0 denotes selection of
                              0.0
                                                                                                                                          0
                                                 Sampled                 Average                                                               Explore Exploit
                                                                                                                                                                          the maximally informative sample available, and a RSI of 0.0
  E                    0.30                                                           0.30
                                                                                                                                                                          denotes a completely uninformative, redundant observation.
                       0.20                                                           0.20                                                                                   As seen in Figure 3C, average RSI was significantly greater
                                                                            All                                                                              All
Relative Frequency
                       0.10                                                           0.10                                                                                during trials the model classified as Exploit (M=0.72,
                                 0                                                              0
                       0.30                                                           0.30                                                                                SD=0.05) relative to trials that were classified as Explore
                       0.20
                                                                         Explore
                                                                                      0.20
                                                                                                                                                          Explore         (M=0.63, SD=0.07), (t(5) = −4.36, p < 0.01), and both types
                       0.10                                                           0.10
                                  0                                                             0
                                                                                                                                                                          of trials were significantly better than would be expected from
                       0.30                                                           0.30                                                                                a random sampling strategy (Explore: t(5) = 4.36, p < 0.01;
                       0.20                                                           0.20
                                                                         Exploit                                                                          Exploit         Exploit: t(5) = 9.47, p < 0.001). In addition, we found that
                       0.10                                                           0.10
                                  0
                                        1                                             0
                                                                                                0
                                                                                                              1                                                    0
                                                                                                                                                                          mean reaction time (RT) increased for Exploit (M=1126,
                                                          All points                                                                    Actual Observations
                                                                                                                                                                          SD=265) trials relative to Explore (M=841, SD=147) tri-
                                                                 Relative Savings Index (RSI) bin                                                                         als, t(5) = −5.03, p < 0.005 (see Fig. 3D), suggesting that
                                                                                                                                                                          choices made during Exploit trials were not only more ef-
Figure 3: Panel A shows the frequency of participants choices as                                                                                                          fective, but also more effortful, than in Explore trials.
a function of the rank-order ES(G, xi j ) on each trial. Panel B shows
                                                                                                                                                                             To check that greater RSI of Exploit trials is a genuine im-
the same data in cumulative distribution form. Panel C shows an
increased relative savings index (RSI) for participants’ observations                                                                                                     provement of performance and does not simply reflect aspects
(left) as compared to a random strategy (right) considered separately                                                                                                     of the distribution of ES(G, xi j ) scores available on different
for Exploit and Explore trials. Panel D shows a corresponding                                                                                                             trials, we computed the full distribution of RSI and compared
increase in median reaction time for Exploit trials over Explore
trials. In Panel E, left, the distribution of RSI values that occur                                                                                                       this distribution to the distribution of participant’s choices.
across all games is shown for all trials (top), explore trials (middle),                                                                                                  Figure 3E shows the relative frequency of RSI values across
and exploit trials (bottom), with values counted in intervals of 0.05                                                                                                     the entire gameboard (left) and the the frequency of the RSI
RSI. At right is shown the distribution of RSI values of participants’
selections for the same groups of trials.                                                                                                                                 of participants’ choices (right). Particularly on Exploit trials
                                                                                                                                                                          participants select points that fall within the top 5 percent RSI
                                                                                                                                                                          at a much greater frequency than would be expected given the
                                                                                                                                                                          overall distribution.
with the model, as well as the average expected savings for                                                                                                               Learning to Learn We were also interested if participants
remaining unknown points at each trial. Qualitatively, this                                                                                                               would show evidence of learning-to-learn (i.e., improvement
participant performed better than would be expected on aver-                                                                                                              in sampling efficiency in novel games as a function of expe-
age, and frequently selected the best possible observation as                                                                                                             rience with other games). A 2 way ANOVA on reaction time
scored by the model.                                                                                                                                                      using game and search mode (Explore/Exploit) as repeated
How effective were people’s sampling decisions? In or-
                                                                                                                                                                          factors found a main effect of search mode (F(1, 195) =
der to characterize the overall match of participant’s choices
                                                                                                                                                                          96.5, p < 0.001), main effect of game (F(1, 195) = 3.87, p <
relative to the model, on each trial of each game, we cal-
                                                                                                                                                                          0.001), but no interaction (F(1, 195) = 0.32, p = 0.99) (see
culated the ES(G, xi j ) for each possible choice and rank or-
                                                                                                                                                                          Figure 4A). A similar 2 way ANOVA on RSI using game
dered these choices. Then, we counted the number of trials in
                                                                                                                                                                          and search mode as repeated factors also found a main effect
which each subject actually made an observation at each rank
                                                                                                                                                                          of search mode (F(1, 195) = 45.2, p < 0.001), but no main
(given by the model). As Fig. 3A & B shows, participants
                                                                                                                                                                          effect of game (F(1, 195) = 1.42, p = 0.12) and no interac-
chose the optimal choice (rank 1) on more than 20% of the
                                                                                                                                                                          tion (F(1, 195) = 1.0, p = 0.46), Figure 4B. Thus, although
trials, and chose an option in the top 10 approximately 50%
                                                                                                                                                                          there is no evidence of systematic improvements in RSI over
of the time (remember that participants made approximately
                                                                                                                                                                          games, the general decrease in RT suggests that participants
30 samples per game, thus there were usually more than 70
                                                                                                                                                                       3149

                          2000                                           quence of adaptive information acquisition and the structure
         A                                        Explore
                          1800                                           of the hypothesis space.
                                                  Exploit
         Median RT (ms)
                          1600                                              More importantly, our results suggest a number of interest-
                          1400                                           ing facts about human search behavior. First, the ability to de-
                          1200                                           vise efficient queries – those that return the most information
                          1000
                                                                         or are most useful in reaching the learning goal – has previ-
                          800
                                                                         ously been shown to vary across task domains, such that peo-
                                                                         ple perform highly efficient search in some perceptual tasks
                          600
                                 0   5     10      15       20
                                                                         (Najemnik & Geisler, 2005) but exhibit biased search strate-
                                          Game                           gies in more abstract or conceptual tasks (Klayman & Ha,
                          0.9                                            1987). The rectangle game combines both these elements:
         B
                                                                         reasoning about the next observation may require hypothesis
                          0.8
                                                                         testing, but it is also possible to “perceive” certain informa-
          Average RSI
                          0.7                                            tion affordances directly (such as the relative density of pre-
                                                                         viously uncovered locations on the board). The levels of effi-
                          0.6                                            ciency observed in our task may reflect the confluence of both
                                                                         perceptual and conceptual factors guiding search behavior.
                          0.5                     Explore
                                                  Exploit
                                                                         Second, our modeling allowed us to objectively classify indi-
                          0.4                                            vidual trials as either exploiting a local information “patch” or
                                 0   5     10      15       20
                                          Game                           exploring relatively unknown regions of the game board. We
                                                                         find that participants’ response time and search efficiency dif-
Figure 4: Panel A shows changes in average median reaction time          fers between these two “modes.” In contrast to theories which
(RT) over the course of the games. RT is consistently lower for          effectively equate information seeking behaviors with ran-
Explore trials than Exploit trials, but there is a reduction in RT for   dom exploration (such as the soft-max rule or epsilon-greedy
both types with practice. In contrast, the average RSI of subjects’
observations (an index of maximizing utility) shows no significant       rule often used in modeling reinforcement learning tasks), we
change over time (Panel B).                                              suggest that information generating behaviors may naturally
                                                                         take two forms: one is relatively fast and undirected while the
                                                                         other is slower, more effortful, and efficiently exploits local
were able to make the same “quality” of choices in less time             information constraints.
as task experience increased.                                               Finally, note that in this preliminary report, we focused on
                                                                         how people select new observations in learning about a un-
                                     Discussion                          known “concept,” but an interesting line of future work is to
In this paper, we present a quantitative analysis of human be-           consider how allowing learners to create their own learning
havior in a learning task which requires participants to contin-         experiences can impact the acquisition and retention of new
ually update expectations based on past observations and to              concepts.
use these expectations to drive new information-seeking deci-
                                                                         Acknowledgements We thank Larry Maloney, Matt Jones, Art
sions. One interesting feature of our task is that it effectively        Markman, Noah Goodman, Nathaniel Daw, Jason Gold, and the
separates information seeking actions from the exploitation              Concepts and Categories (ConCats) group at NYU for helpful dis-
of that information (which happens during the separate test              cussion.
phase). By doing so, we were better able to measure how
these information generating actions relate to ongoing learn-
                                                                                                                References
                                                                         Castro, R., Kalish, C., Nowak, R., Qian, R., Rogers, T., & Zhu, X. (2008). Human active learning. In
ing. In addition, unlike other sequential learning and decision                     Advances in neural information processing systems (Vol. 21). Cambridge, MA: MIT Press.
                                                                         Fazio, R., Eiser, J., & Shook, N. (2004). Attitude formation through exploration: Valence asymmetries.
making tasks like the n-armed bandit or foraging tasks, there                       Journal of Personality and Social Psychology, 87(3), 293-311.
                                                                         Hills, T. (2006). Animal foraging and the evolution of goal-directed cognition. Cognitive Science(3-
are better and worse choices to be made even when exploiting                        41).
                                                                         Klayman, J., & Ha, Y. (1987). Confirmation, disconfirmation, and information in hypothesis testing.
a single “patch.”                                                                   Psychological Review, 94(2), 211-228.
                                                                         Kruschke, J. (2008). Bayesian approaches to associative learning: From passive to active learning.
   One interesting aspect of our results is the fact that both                      Learning and Behavior, 36(3), 210-226.
                                                                         Mackay, D. (1992). Information-based objective functions for active data selection. Neural Computa-
participants (and the model) generally adopt a sequential                           tion, 4, 590-604.
                                                                         Najemnik, J., & Geisler, W. (2005). Optimal eye movement strategies in visual search. Nature, 434,
“hunt and kill” strategy when trying to disambiguate a com-                         387-391.
                                                                         Nelson, J. (2005). Finding useful questions: On bayesian diagnosticity, probability, impact, and
plex hypothesis space (often disambiguating one rectangle at                        information gain. Psychological Review, 112(4), 979-999.
                                                                         Oaksford, M., & Chater, N. (1994). A rational analysis of the selection task as optimal data selection.
a time). This search strategy is a natural consequence of infor-                    Psychological Review, 101(4), 608-631.
                                                                         Skov, R., & Sherman, S. (1986). Information-gathering processes: Diagnosticity, hypothesis-
mation search in a highly “clustered” environment and paral-                        confirmatory strategies, and perceived hypothesis confirmation. Journal of Experimental
                                                                                    Social Psychology, 22, 93-121.
lels search patterns adopted by biological organisms in spatial          Sutton, R., & Barto, A. (1998). Reinforcement learning: An introduction. Cambridge, MA: MIT
                                                                                    Press.
foraging tasks where costs of traveling between patches fa-              Tenenbaum, J. B. (1999). Bayesian modeling of human concept learning. In M. Kearns, S. Solla,
                                                                                    & D. Cohn (Eds.), Advances in neural information processing systems (Vol. 11, p. 59-65).
vors local exploitation (Hills, 2006). However, since the costs                     Cambridge, MA: MIT Press.
for switching between spatially disparate patches in our task            Wason, P. (1960). On the failure to eliminate hypotheses in a conceptual task. Quarterly Journal of
were negligible, local search here is more an emergent conse-                      Experimental Psychology, 12, 129-140.
                                                                     3150

