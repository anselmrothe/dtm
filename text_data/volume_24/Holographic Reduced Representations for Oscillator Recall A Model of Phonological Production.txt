UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Holographic Reduced Representations for Oscillator Recall: A Model of Phonological
Production
Permalink
https://escholarship.org/uc/item/4pf5511x
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Author
Harris, Harlan D
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Holographic Reduced Representations for Oscillator Recall:
                            A Model of Phonological Production
                                    Harlan D. Harris (hharris@uiuc.edu)
                                      University of Illinois at Urbana-Champaign
                                      Department of Computer Science, MC-258
                                                    Urbana, IL 61801
                         Abstract                                 Type                    Rate     Example
                                                                  anticipations           35.1%    det the dog
   This paper describes a new computational model                 perseverations          26.0%    pet the pog
   of phonological production, Holographic Reduced
   Representations for Oscillator Recall, or HORROR.              exchanges               10.6%    det the pog
   HORROR’s architecture accounts for phonological                non-contextual slips    17.3%    pet the log
   speech error patterns by combining the hierarchical            mixed errors            11.0%    let the pog
   oscillating context signal of the OSCAR serial-order
   model (Vousden, Brown, and Harley 2000; Brown,
   Preece, and Hulme 2000) with a holographic asso-             Table 1: Error type proportions. Target utterance
   ciative memory (Plate 1995). The resulting model             is “pet the dog.” Mixed errors include any error not
   is novel in a number of ways. Most importantly, all          in the other categories.
   of the noise needed to generate errors is intrinsic to
   the system, instead of being generated by an exter-
   nal process. The model features fully-distributed
   hierarchical phoneme representations and a single               An even stronger constraint is the consonant-
   distributed associative memory. Using fewer pa-              vowel category constraint, or C-V constraint. Errors
   rameters and a more parsimonious design than OS-             very rarely involve the replacement of a consonant
   CAR, HORROR accounts for error type propor-
   tions, the syllable-position constraint, and other           by a vowel or vice versa. A superset of these er-
   constraints seen in the human speech error data.             rors, those that violate language-specific rules (the
                                                                phonotactic regularity e↵ect), occur in less than 1%
                                                                of errors (Stemberger 1983).
                    Introduction                                   The distance constraint is the observation that
The phonological production subsystem is the part               phonemes tend to move only short distances (one
of the language production apparatus that sequences             or two syllables) in movement errors.
the sounds in individual words and groups of words.                When movement errors occur, they are more likely
Phonological production is the mapping from lex-                than chance to involve phonemes that share phonetic
ical units, morphemes and words, to sequences of                features. For example, “pig bull” for the intended
phonological units, phonemes. This paper presents               “big pull” is a more likely exchange than “bill pug,”
a new model of the phonological production system,              since [p] and [b] are more similar than are [g] and [l].
a model that o↵ers a new explanation for errors and             This is the phonological similarity e↵ect.
serial order in speech.
                                                                Language Sequencing Models
Speech Error E↵ects                                             Phonological production models can be categorized
Numerous constraints and patterns have been ob-                 by how they generate serial order. I follow Vousden
served in speech error patterns, including error type           et al. (2000) and use the terms associative chaining
proportions (see Table 1), the syllable position con-           model, frame-based model, and control signal model.
straint, the C-V category constraint, the distance                 Associative chaining models account for serial or-
constraint, the phonological similarity e↵ect, and the          der by having each subsequent phoneme be trig-
phonotactic regularity e↵ect (Fromkin 1971). Unless             gered by a combination of the pattern of previous
otherwise specified, the numbers in the descriptions            phonemes and a representation of the target utter-
below are from the (Vousden et al. 2000) analysis of            ance (Dell, Juliano, and Govindjee 1993). These
the (Harley and MacAndrew 1995) error corpus.                   models successfully account for phonotactic regular-
   A strong constraint on movement errors (the first            ity e↵ects and the C-V constraint, but they do not
three error types in Table 1) is the syllable position          generate anticipations and exchanges well, nor do
constraint, or SPC. 89.5% of movement errors retain             they account for SPC e↵ects.
their position in the syllable (onsets move to onsets,             Frame-based models (Dell 1986; Roelofs 1997) use
vowels to vowels, etc.).                                        strict phonological frames to slot phonemes into pre-

specified positions, such as the onset, nucleus, and                      1
coda positions of a syllable. These models often use
chains of sequencing nodes to activate the slots se-
quentially (Eikmeyer and Schade 1991). Although                   0.75
frame-based models are influential, sequencing nodes
are often criticized as being poorly motivated.
   To address this point, control signal models              Similarity
                                                                      0.5
(Burgess and Hitch 1992; Hartley and Houghton
1996; Vousden et al. 2000) replace discrete syllable              0.25
frames with continuous time-varying signals. Prior
to production, di↵erent parts of the word are as-
sociated with di↵erent parts of the signal. Then, as                      0
                                                                           0   5                10       15
the signal changes during production, the associated                               Temporal Separation
phonemes are output sequentially. Simple control
signal models explain how phonemes could be pro-
duced in order, but don’t account for SPC e↵ects.        Figure 1: HORROR’s PCV self-similarity function.
   The OSCAR model (Vousden et al. 2000), de-
scribed below, is a complex control signal model
that accounts for SPC e↵ects by using a multi-              The PCV itself is generated by multiplying to-
dimensional control signal with biological motiva-       gether selected oscillator signals to form a 32-
tion. It contains an implicit frame in the way that      element vector. Each element is a product of four
the control signal is structured, but does not require   oscillator signals, all of which are selected from the
explicit slots or sequencing nodes for production.       same group (repeating or non-repeating). The pat-
                                                         tern of multiplications results in an automatically
               Building Blocks                           normalized PCV, allowing easy comparisons for sim-
                                                         ilarity.
The HORROR model combines elements of two pre-              A key feature of OSCAR is that the PCV is self-
viously existing models: the OSCillator-based Asso-      similar in a hierarchical manner. Each state of the
ciative Recall (OSCAR) model of serial-order and         PCV is most similar to states that are multiples of
phonological production (Brown et al. 2000; Vous-        three time-steps away, but nearby states are also
den et al. 2000), and the Holographic Reduced Rep-       somewhat similar (see Figure 1).
resentations (HRR) model of hierarchical associative
                                                            The process for producing a “word” (a randomly
memory (Plate 1995). Prior to describing HOR-
                                                         generated 18-segment sequence of six CVC syllables)
ROR, I review its two ancestral models.
                                                         is as follows. A PCV is initialized, and starts to
OSCAR                                                    change with time. At each time step, the PCV is
                                                         associated with a phoneme feature vector in a Heb-
OSCAR works by associating item vectors (phoneme         bian weight matrix. Each time step uses a separate
representations) and phonological context vectors        weight matrix. This entire process is performed nine
(PCVs) in a Hebbian associative memory. The              times (in parallel), to create a total of 81 weight ma-
PCVs are inspired by oscillating signals in the brain,   trices, nine replications of nine time steps. To pro-
and have an important hierarchical self-similarity       duce the sequence, the PCV is re-instated to its ini-
pattern, described below. As the PCVs are itera-         tial state, then sequentially re-produces each step’s
tively presented to the associative memory, the orig-    state. The PCV is usually associated with the cor-
inal item vectors are recalled and become available      rect weight matrices to generate an approximation
for production. The self-similarity pattern gener-       of the phoneme feature vector. In addition, a proba-
ated by the oscillators, when combined with noise,       bilistic process is used to generate errors. 70% of the
generates patterns of errors that previously required    time, segments which are associated with PCVs that
the use of syllable frames.                              are similar to the current PCV are combined with
   In OSCAR, there are 30 oscillators in two groups      the output from the correct weight matrix. The re-
of 15. In the non-repeating group, the oscillators       sult is an output vector, a potentially noisy approxi-
generate sinusoidal values at frequencies ranging        mation to the correct phoneme. Also, a post-output
from very slow to very fast. Initial phases and fre-     suppression mechanism is used to reduce excessive
quencies are generated with sufficient randomness        perseveration and facilitate exchange errors. The
that the non-repeating group’s state does not re-        generated output vectors for each of the nine repli-
peat for many steps. In the repeating group, the         cations are compared to an item memory containing
initial phases of the oscillators are random, but the    each phoneme, such that each phoneme is activated
frequencies are identical. The state of this group re-   to an extent proportional to the similarity with the
peats precisely every three time steps, representing     nine output vectors. The most active phoneme is
the period of a three-segment CVC syllable.              then produced in a winner-take-all process.

OSCAR’s Pros and Cons In many ways, OS-                                         T1 = a ⇤ b
CAR is important work in the literature of phono-            ⇤:I⇥I)T            a#T1 ! b + noise
logical production and speech error modeling, but            #:I⇥T)I            T2 = a ⇤ b + c ⇤ d + e ⇤ f
it has significant problems that may limit its ap-           +:T⇥T)T            d#T2 ! c + noise
plicability. Its contributions include making good                              T3 = g ⇤ T1 + h
use of an independently-motivated context signal to                             g#T3 ! T1 + noise
create serial order, accounting for SPC e↵ects with-
out position-specific phonemes, and using an im-
                                                           Figure 2: Holographic Associative Memory. a h
plicit rather than explicit syllable frame. Overall,
it accounts for various error patterns better than do      are item vectors; Ti are memory vectors. ⇤, #, and
chaining models.                                           + symbolize circular convolution (encoding), corre-
   However, several limitations lead me to question        lation (decoding), and addition (composition).
the extent of the model’s successes. Most impor-
tantly, the noise-addition procedure is unprincipled.
As well, the artificial words did not include repeated     Distributed Associative Memories
phonemes, the associations between the context and
phonemes are stored separately, and there are a con-       For several decades, mathematical psychologists
cerning number of parameters.                              have looked at distributed representations for mod-
   Consider the noise-addition procedure. Cognitive        els of memory (Murdock 1982; Eich 1982), and
models should use reasonable sources of noise to gen-      have accounted for many recognition and recall ef-
erate error phenomena. Many models add Gaus-               fects. Compared to localist connectionist models,
sian noise, while others use intrinsic noise from dis-     where representations consist of features and micro-
tributed representations and imprecise network com-        features, distributed representations use long quasi-
putation. Although OSCAR uses well-motivated               random vectors. These vectors are generated and
oscillator signals to provide serial-order e↵ects, its     manipulated such that similarity between two rep-
noise-generation procedures are much more weakly           resentations is defined by the dot product or cosine.
motivated. As described above and in Appendix C            Distributed representations can be combined in var-
of Vousden et al. (2000), phonemes associated with         ious ways. Two symbols may be associated by op-
states of the PCV that are selected by their similar-      erations such as convolution or the outer product,
ity to the correct PCV are recalled in parallel and        resulting in another large vector. Retrieval from
used to corrupt the winner-take-all process.               memory vectors is performed by inverting the asso-
   That this procedure generates impressive error re-      ciation operation, correlation. Distributed memories
sults is not surprising. The noise in OSCAR is gener-      can store a number of associations at once, simply by
ated only by interference from particular phonemes         adding the vectors together. As vectors are overlaid,
in the current sequence, not by any sort of random         the amount of noise increases. This intrinsic noise
numerical noise or other natural interference. OS-         is part of the model, and resulting simulations can
CAR claims to explain why most errors are move-            account for list-length and item-similarity e↵ects.
ment errors – in their model, it’s because the gener-
ated noise is movement noise.                                 A limitation in much of the work on distributed
                                                           memories is that the operations that generate asso-
   A related concern with OSCAR is that the as-
                                                           ciations greatly expand the size of the vector, with
sociations between the PCV and phoneme vectors
                                                           the result that hierarchies of associations are imprac-
are stored separately. Although it is reasonable to
                                                           tical. HORROR utilizes one of several approaches
use Hebbian learning to associate a PCV signal with
                                                           that overcome this problem, the Holographic Re-
phoneme representations, it is difficult to explain
                                                           duced Representations (HRRs) of Plate (1995).
why each segment need be stored in entirely sep-
arate sets of weights. A more parsimonious solution           With HRRs, the representations and associations
would use a single set of weights and would treat the      are always fixed-length vectors. A circular version of
resulting noise as an asset, not a weakness.               convolution is used to associate vectors. The result-
   HORROR adopts the oscillating PCV system                ing memory vectors are the same length as the input
from OSCAR, but replaces the movement-based                vectors, at the cost of increased noise. The greatest
noise-creation system with the noise inherent in an        benefit is that hierarchies of associations can be eas-
associative memory system with overlaid weights. It        ily generated and stored. See Figure 2 for simple
also uses a more parsimonious unified memory sys-          examples and notation. An auto-associative item
tem, allows repeated phonemes within a sequence,           memory (a Hopfield network or a nearest-neighbor
and requires fewer free parameters1 .                      search through a list) is necessary to identify the
                                                           result of each correlation.
    1
      In addition to the five listed in Table 7 of Vousden
et al. (2000), there are these four: the ratio of correct-
to-incorrect activation, 0.6; the number of redundant      phoneme to be added as noise, 0.5; and the similarity
associations, 9; the similarity threshold for allowing a   exponentiation factor in the item memory, 3.4.

                     HORROR                             Param.     Value    Description
HOlographic Reduced Representations for Oscillator      nrep       17       # of non-repeating oscillators
Recall (HORROR) is a model of serial-order process-     vw         2048     Representation vector width
ing that combines the self-similar context vectors of   cc         3        Repeating oscillator inv. freq.
OSCAR with the hierarchical representations and         D          4        Speech-rate (larger = slower)
memory of HRRs. The result is a fully-distributed       Inhib      .121     Post-activation inhibition level
phonological production model that accounts for er-     InDec      .5       Inhib. decay (lower = faster)
rors in the serial-order part of the system by using    ds         3        Phoneme dis-similarity factor
the intrinsic noise from the associative-memory part
of the system.                                                Table 2: Free parameters in HORROR
   OSCAR and HRRs fundamentally both represent
similarity by distance between vector representa-
tions. In OSCAR, the extent to which pairs of con-
                                                      lators. The procedure of generating the PCV from
text vectors which are near in time are also near in
                                                      the oscillators in HORROR is very similar to the
space determines retrieval accuracy and error pat-
                                                      procedure used in OSCAR, but since HORROR’s
terns. With HRRs, capacity and noise levels are de-
                                                      PCV is very wide (2048 elements), the process was
termined by the extent to which composed vectors
                                                      repeated with di↵erent random initial phases and
are near (not orthogonal) to each other. In OSCAR
                                                      frequencies in order to fill up the vector, which was
these similarity metrics can be complex and hier-
                                                      then normalized. See Table 2 for the list of PCV and
archical, determined by the oscillator patterns, and
                                                      other parameters used in the experiments described
in HRRs, the similarity metrics can also be hierar-
                                                      below.
chical, by the process of overlaying associations. In
both models, item memories are used to clean up          Vousden et al. (2000) use an articulatory-feature
and to select a single item.                          representation of phonemes. Each phoneme is 17 el-
                                                      ements long, with binary features representing place
   HORROR is a new model based on the general
                                                      and manner of articulation, nasality, voicing, and
framework of OSCAR. It takes a variation of the
                                                      vowel position and tenseness. We converted these lo-
PCV from OSCAR, and combines it with an HRR
                                                      calist features into distributed features for the fully-
associative memory, replacing the simple associative
                                                      distributed representations used in HORROR.
memory used by OSCAR. In addition, the feature-
vector phonological representations used in OSCAR        Phonological representations were built in a fully-
are replaced with fully-distributed hierarchical rep- distributed manner by generating random Gaussian
resentations in HORROR. A critical aspect of HOR-     vectors (of width vw) for each feature, then summing
ROR is that all of the phoneme-context pairs that     the appropriate features together and normalizing.
make up a sequence are stored together in a single    Each vector thus has an intrinsic similarity metric,
large vector, rather than in OSCAR’s many separate    defined by the number of shared features. In or-
weight matrices. The noise in this memory vector,     der to partially “drown out” the similarity between
combined with representational similarity and the     otherwise very-similar phonemes, additional random
PCV structure, provide sufficient opportunities for   vectors (ds) were added to each phoneme vector.
appropriately distributed error patterns to arise.       Decoding consists of sequentially correlating each
                                                      time-step of the PCV with the single stored memory
                                                      vector. The result is a series of approximations to
                   Experiments                        the target phonemes, corrupted by the noise intrinsic
A major goal of this work is to account for the same  to a holographic memory. Each recalled vector is
human speech error data as does OSCAR, using          compared to an item memory containing possible
a simpler structure, more parsimonious procedures,    phonemes. The phoneme that is most similar to the
and fewer parameters.                                 recalled vector is then produced.
   As with OSCAR, PCVs are generated sequentially        The item memory has three features that help it
and convolved with phoneme representations to form    best account for the error patterns. First, each item
memory traces. Unlike OSCAR, these traces are         in the item memory has a persistent activation level,
summed to form a single vector representing the en-   a. Activation is added to similarity to determine
tire sequence. To produce the sequence, the vector    which phoneme is selected. At each step, each item’s
is correlated with the PCVs in order, resulting in    a is increased by the item’s distance from the recalled
noisy versions of the phonemes. The phonemes are      vector, weighted by the Inhib parameter. Second,
cleaned up in an item memory, and the results are     after a phoneme is selected, it is suppressed by set-
analyzed for various types of errors.                 ting a to be the negation of Inhib. Post-output sup-
   The oscillators used to generate the PCV were      pression is a common feature of this type of model
the same as used by OSCAR. HORROR addition-           (Vousden et al. 2000; Dell 1986). Finally, at every
ally includes a parameter, nrep, that specifies the   time step, activation decays toward zero according
proportion of repeating versus non-repeating oscil-   to the decay constant InDec.

                 70                                                                       60
                                                      HORROR                                                                      HORROR
                 60                                   Human Data                                                                  Human Data
                                                      HORROR (no repeats)                 50
                                                                                                                                  Chance
                 50
                                                                                          40
   % of Errors
                 40
                                                                               % Errors
                                                                                          30
                 30
                                                                                          20
                 20
                                                                                          10
                 10
                 0                                                                        0
                      Antic.   Persev.       Exch.     Non-cont.    Mixed                      1   2            3             4                5
                                         Error Type                                                    Distance (syllables)
   Figure 3: HORROR’s error type proportions.                               Figure 4: Distance gradients of the anticipation er-
                                                                            rors produced by HORROR, compared with the hu-
                                                                            man data and chance baseline of Vousden et al.
Experimental Results                                                        (2000). Adjacent syllables have a distance of 1.
2000 6-syllable “words” were generated, associated,                         Same-syllable errors (separation 0) are not shown.
and output. Errors were determined by an auto-
matic categorization process. Error type propor-                               Error type               n        Mean shared features
tions, SPC violations, distance constraint statistics,                         Movements               895               2.4
and phonetic similarity constraint statistics were                             Exchanges                22               3.1
counted.
                                                                               Non-contextual          306               3.1
Error proportions 1538 errors occurred during                                  Chance                                    1.9
production of 36,000 segments, resulting in an over-
all error rate of 4.3%. Figure 3 shows the proportions
                                                                            Table 3: Average similarity for consonant errors.
of error types. The results compare fairly well with
the human data reported in Vousden et al. (2000).                           Movements are anticipations and perseverations.
Exchanges, however, were under-represented in the
model, raising the question of whether HORROR’s
                                                                            to other error types, has been observed in human
exchanges are true exchanges or merely the joint
                                                                            error data (Nooteboom 1973).
event of independent anticipations and persevera-
tions. Other speech error models (Dell et al. 1993;                         Phonetic similarity constraint Vousden et al.
Roelofs 1997) are unable to produce true exchanges,                         (2000) concentrate their analysis of the phonetic
and are therefore seen as incomplete.                                       similarity constraint on consonant exchanges. HOR-
   To address this, I calculated the expected num-                          ROR produced only 22 consonant exchanges, 20 of
ber of exchanges, assuming that they are coinciden-                         which shared 75% or more of their phonetic features.
tal. This number, 0.33 per 2000 sequences, was                              Table 3 compares the categories of consonant errors
more than sixty times smaller than the number of                            to chance. Chance was determined by randomly se-
exchanges actually observed (22), demonstrating a                           lecting 1000 pairs of consonants, and counting the
true tendency for exchanges. Exchanges in HOR-                              number of shared phonemes for each pair. The pho-
ROR occur because post-activation inhibition helps                          netic similarity constraint is clearly present in these
to prevent an erroneously anticipated phoneme from                          results. Note that exchange errors were significantly
then appearing in its correct location. Instead, the                        more similar than were other movement errors. This
earlier, replaced phoneme may be triggered via the                          is true for human exchanges, and also lends further
PCV, turning an anticipation into an exchange.                              support to the observed exchanges being real.
Distance constraint The model’s movement gra-                               Syllable-position constraint 29.0% of the
dients parallel the distance constraints seen in hu-                        model’s errors violated the SPC, compared to
man data, with disproportionately small separa-                             10.5% of errors in human data (Vousden et al.
tions. Exchange errors shifted least, an average                            2000). To confirm that this number still reflects
0.95 syllables, followed by anticipations, averaging                        a constraint, and is not just the chance rate of
1.4 syllables, and perseverations, averaging 2.9 syl-                       violations, it’s necessary to look at the probabilities
lables. Figure 4 shows a comparison on anticipa-                            of errors being in each syllable position. In this
tions between HORROR and human data. The                                    set of data, 50.7% of errors were in the onset,
shorter movements made by exchanges, compared                               8.6% in the vowel, and 40.8% in the coda. To

calculate the expected rate of SPC violations,                          References
assume that the consonant-vowel constraint is         Brown, G. D. A., T. Preece, and C. Hulme (2000).
never violated, and that consonant errors have a         Oscillator-based memory for serial order. Psy-
50% chance of movement from onsets and codas.            chological Review 107 (1), 127–183.
Therefore, the expected SPC violation rate is
1 (.086 + .507 ⇤ .5 + .408 ⇤ .5) = 45.7%. Although    Burgess, N. and G. J. Hitch (1992). Toward a net-
the SPC is violated more often by the model than         work model of the articulatory loop. Journal
it is in human data, it is still a real e↵ect.           of Memory and Language 31, 429–460.
                                                      Dell, G. S. (1986). A spreading-activation theory
Consonant-vowel constraint Only 2.3% of the
                                                         of retrieval in sentence production. Psycholog-
errors violated the C-V constraint, showing that the
                                                         ical Review 93 (3), 283–321.
model is generally respecting the consonant-vowel
categorical distinction seen in natural errors.       Dell, G. S., C. Juliano, and A. Govindjee (1993).
                                                         Structure and content in language production:
Repeated phonemes In order to investigate the            A theory of frame constraints in phonological
role of repeated phonemes in the model, the same         speech errors. Cognitive Science 17, 149–195.
experiment was re-run with repeated phonemes dis-
abled. Since repeated items are known to strongly     Eich, J. M. (1982). A composite holographic
a↵ect performance in distributed memories, it was        associative recall model. Psychological Re-
expected that the e↵ects on HORROR would be sig-         view 89 (6), 627–661.
nificant as well. The error rate without repeated     Eikmeyer, J.-J. and U. Schade (1991). Sequential-
items was reduced to 1.3%, and the proportion of         ization in connectionist language-production
non-contextual errors was greatly reduced (see Fig-      models. Cognitive Systems 3 (2), 128–138.
ure 3). HORROR is more error prone when rep-
                                                      Fromkin, V. A. (1971). The non-anomalous nature
etitions occur, as in human data (Dell 1986). Re-
                                                         of anomalous utterances. Language 47 (1), 27–
peated phonemes appear to be an important trigger
                                                         52.
for speech errors, including non-contextual errors.
                                                      Harley, T. A. and S. B. G. MacAndrew (1995). In-
                     Discussion                          teractive models of lexicalization: Some con-
                                                         straints from speech error, picture naming,
The HORROR model combines the best features              and neuropsychological data. In D. Bairak-
of OSCAR, a serial-order phonological model with         taris, J. Bullinaria, and D. Cairns (Eds.), Con-
a hierarchical context signal, and HRRs, a holo-         nectionist models of memory and language.
graphic associative memory using hierarchical rep-       London: UCL Press.
resentations. Its aim is to account for speech error
patterns using more parsimonious mechanisms than      Hartley, T. and G. Houghton (1996). A lingusti-
previous related models.                                 cally restrained model of short-term memory
                                                         for non-words. Journal of Memory and Lan-
   HORROR succeeds in a number of ways. It al-
                                                         guage 35, 1–31.
lows repeated phonemes in the sequences, it com-
bines associative memory traces into a single dis-    Murdock, B. B. (1982). A theory for the stor-
tributed association vector, and its error mechanism     age and retrieval of item and associative recall.
relies entirely on the intrinsic noise from the asso-    Psychological Review 89 (6), 609–626.
ciative memory with no generated noise at all. It     Nooteboom, S. (1973). The tongue slips into pat-
uses fewer parameters than does OSCAR, and ac-           terns. In V. A. Fromkin (Ed.), Speech Errors
counts for a number of error patterns in human           as Linguistic Evidence. The Hague: Mouton.
data. Specifically, the model’s error type propor-
tions, distance constraint, phonological similarity   Plate, T. (1995). Holographic reduced represen-
constraint, and C-V category constraint results were     tations. IEEE Transactions on Neural Net-
largely similar to human data. The SPC results           works 6 (3), 623–641.
were real, if modeled less accurately. HORROR ac-     Roelofs, A. (1997). The WEAVER model of word-
counts for these major speech error patterns by us-      form encoding in speech production. Cogni-
ing fully-distributed hierarchical representations, a    tion 64, 249–284.
single intrinsically-noisy associative memory, and an Stemberger, J. P. (1983). Speech errors and theo-
oscillating phonological context signal.                 retical phonology: A review. Bloomington: In-
                                                         diana University Linguistics Club.
               Acknowledgments                        Vousden, J. I., G. D. A. Brown, and T. A. Harley
I appreciate the advice and contributions of Gary        (2000). Serial control of phonology in speech
Dell, Janet Vousden, and Franklin Chang. This            production: A hierarchical model. Cognitive
work was supported by NSF grant SBR 98-73450             Psychology 41, 101–175.
and NIH grant DC-00191.

