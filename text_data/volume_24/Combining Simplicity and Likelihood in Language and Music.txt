UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Combining Simplicity and Likelihood in Language and Music
Permalink
https://escholarship.org/uc/item/4d3296hv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Author
Bod, Rens
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Combining Simplicity and Likelihood in Language and Music
                                                 Rens Bod (rens@science.uva.nl)
                                 Cognitive Science Center Amsterdam, University of Amsterdam
                                      Nieuwe Achtergracht 166, Amsterdam, The Netherlands
                             Abstract                            that the linguistic tree structure is labeled with syntactic
                                                                 categories, whereas the musical and visual tree structures
   It is widely accepted that the human cognitive system         are unlabeled. This is because in language there are
   organizes perceptual input into complex hierarchical          syntactic constraints on how words can be combined into
   descriptions which can be represented by tree structures.     larger constituents, while in music (and to a lesser extent
   Tree structures have been used to describe linguistic,        in vision) there are no such restrictions: in principle any
   musical and visual perception. In this paper, we will         note may be combined with any other note.
   investigate whether there exists an underlying model that
   governs perceptual organization in general. Our key idea is                     List the sales of products in 1973
   that the cognitive system strives for the simplest structure
   (the "simplicity principle"), but in doing so it is biased by                                   S
   the likelihood of previous experiences (the "likelihood
                                                                                                     NP
   principle"). We will present a model which combines
   these two principles by balancing the notion of most likely
                                                                                                NP
   tree with the notion of shortest derivation. Experiments
   with linguistic and musical benchmarks (Penn Treebank
                                                                                          NP          PP            PP
   and Essen Folksong Collection) show that such a
   combination outperforms models that are based on either                       V    DT      N    P      N       P     N
   simplicity or likelihood alone.
                                                                                List the sales of products in 1973
                         Introduction
It is widely accepted that the human cognitive system
organizes perceptual input into complex, hierarchical
descriptions which can be represented by tree structures.
Tree structures have been used to describe linguistic
perception (e.g. Chomsky 1965), musical perception (e.g.
Lerdahl & Jackendoff 1983) and visual perception (e.g.
Marr 1982). Yet, there seems to be little or no work which
emphasizes the commonalities between these different
forms of perception and which searches for a general,
underlying mechanism which governs all perceptual
organization (cf. Leyton 2001). This paper aims to study
exactly that question: acknowledging the differences                        Figure 1: Examples of tree structures.
between linguistic, musical and visual information, is
there a general, unifying model which can predict the            Apart from these differences, there is also a fundamental
perceived tree structure for sensory input? In studying this     commonality: the perceptual input undergoes a process of
question, we will use a strongly empirical methodology:          hierarchical structuring which is not found in the input
any model that we might hypothesize will be tested               itself. The main problem is thus: how can we derive the
against benchmarks such as the linguistically annotated          perceived tree structure for a given input? That this
Penn Treebank (Marcus et al. 1993) and the musically             problem is not trivial may be illustrated by the fact that
annotated Essen Folksong Collection (Schaffrath 1995).           the inputs above can also be assigned the following,
While we will argue for a unified model of language,             alternative tree structures in figure 2.
music and vision, we will carry out experiments only with
linguistic and musical benchmarks, since no benchmarks                                           S
of visual tree structures are currently available, to the best
of our knowledge.                                                                               NP
  Figure 1 gives three simple examples of linguistic,
musical and visual input with their corresponding tree                                   NP           PP             PP
structures given below.
  Thus a tree structure describes how parts of the input                        V    DT      N     P      N       P      N
combine into constituents and how these constituents
combine into a representation for the whole input. Note                        List the sales of products in 1973

                                                               (2000), Charniak (2000) and Bod (2001a) obtain around
                                                               90% precision and recall on the Wall Street Journal. Also
                                                               in the field of psycholinguistics, the likelihood principle is
                                                               widely used: Jurafsky (1996), Crocker and Brantz (2000)
                                                               and Hale (2001) are examples of psycholinguistically
                                                               inspired probabilistic parsers.
                                                                 The likelihood principle has also been applied to
      Figure 2: Alternative tree structures for figure 1.      musical perception, e.g. in Raphael (1999) and Bod
                                                               (2001b/c). As in probabilistic natural language processing,
These alternative structures are possible in that they can     the most probable musical tree structure can be computed
be perceived. But while the alternative tree structures are    from the probabilities of rules or fragments taken from a
all possible, they are not plausible: they do not correspond   large annotated musical corpus, for instance from the
to the structures that are actually perceived by the human     Essen Folksong Collection (Bod 2001b).
perceptual system.                                               In visual perception psychology and vision science,
   The phenomenon that the same input may be assigned          there has recently been a resurgence of interest in
different structural organizations is known as the             probabilistic models (e.g. Hoffman 1998; Kersten 1999).
ambiguity problem. This problem is one of the hardest          Mumford (1999) has seen fit to declare the Dawning of
problems in modeling human perception. Even in                 Stochasticity.
language, where a phrase-structure grammar may specify
which words can be combined into constituents, the             Simplicity
ambiguity problem is notoriously hard. Charniak (1997:         The simplicity principle has a long tradition in the field of
37) argues that almost every sentence from the Wall            visual perception psychology (e.g. Restle 1970;
Street Journal has many, often more than one million
                                                               Leeuwenberg 1971; Simon 1972; Buffart et al. 1983; van
different parse trees. The ambiguity problem for musical
                                                               der Helm 2000). In this field, a visual pattern is
and visual input is even harder. Talking about rhythm          formalized as a constituent structure by means of a
perception in music, Longuet-Higgins and Lee (1987)            "visual coding language" based on primitive elements
note that "Any given sequence of note values is in             such as line segments and angles. Perception is described
principle infinitely ambiguous, but this ambiguity is
                                                               as the process of selecting the simplest structure
seldom apparent to the listener."                              corresponding to the "shortest encoding" of a visual
                                                               pattern.
     Two principles: likelihood and simplicity                   The notion of simplicity has also been applied to
How can we predict from the set of all possible tree           musical perception. Collard et al. (1981) use the coding
structures the tree that is actually perceived by the human    language of Leeuwenberg (1971) to predict the metrical
cognitive system? In the field of visual perception, two       structure for four preludes from Bach's Well-Tempered
competing principles have traditionally been proposed to       Clavier. More well-known in musical perception is the
govern perceptual organization. The first, initiated by        theory proposed by Lerdahl and Jackendoff (1983) who
Helmholtz (1910), advocates the likelihood principle:          use a system of preference rules based on the Gestalt-
sensory input will be organized into the most probable         preferences identified by Wertheimer (1923), and which
organization. The second, initiated by Wertheimer (1923)       can therefore also be seen as an embodiment of the
and developed by other Gestalt psychologists, advocates        simplicity principle.
the simplicity principle: the perceptual system is viewed        Notions of simplicity also exist in language processing
as finding the simplest perceptual organization (see           (e.g. Frazier 1978; Gorrell 1995; Osborne 2000). Bod
Chater 1999 or Van der Helm 2000 for an                        (2000a) defines the simplest tree structure of a sentence
overview). These two principles are not only relevant for      as the structure generated by the smallest number of
visual perception, but also for linguistic and musical         subtrees from a given treebank.
perception. In the following, we briefly discuss these
principles for each modality, after which we go into the                   Combining the two principles
question of how the two principles can be integrated.
                                                               The key idea of the current paper is that both principles
                                                               play a role in perceptual organization: the simplicity
Likelihood                                                     principle as a general cognitive preference for economy,
The likelihood principle is particularly influential in the    and the likelihood principle as a probabilistic bias due to
field of natural language processing (see Manning and          previous perceptual experiences. Informally stated, our
Schütze 1999, for a review). In this field, the most           working hypothesis is that the human cognitive system
appropriate tree structure of a sentence is assumed to be      strives for maximal economy (the simplest structure), but
its most likely structure. The likelihood of a tree is usually that in doing so it is biased by the likelihood of previous
computed from the probabilities of its parts (e.g. phrase-     experiences (in the last section we will discuss some
structure rules) taken from a large annotated language         other combinations of simplicity and likelihood that have
corpus (a treebank). A widely used treebank for testing        been proposed). To formally instantiate our working
and comparing probabilistic natural language parsers is        hypothesis, we need a parsing model to start with which
the Penn Wall Street Journal Treebank (Marcus et al.           can incorporate these principles. In principle any parsing
1993). State-of-the-art probabilistic parsers such as Collins  model might do, as long as it can assign tree structures to

perceptual input according to some criterion. For the                                                  example, DOP also produces the following alternative
current paper, we have chosen to start with the Data-                                                  parse tree for She saw the dress with the telescope:
Oriented Parsing model (Bod 1998) because (1) it has                                                         S              V        PP         =        S
                                                                                                                      °        °
several other models as special cases, such as context-
                                                                                                                                                    NP        VP
free parsing models and lexicalized models, and (2) it has                                               NP       VP       saw    P      NP
been quite successful in predicting tree structures for both
                                                                                                         she V         NP        with the telescope she V           NP
linguistic input (Bod 2001a) and musical input (Bod
2001b).                                                                                                                                                 saw
     The basic idea of DOP is that it learns a grammar by                                                         NP      PP                                  NP          PP
extracting subtrees from a given treebank and uses these                                                                                                               P      NP
subtrees to analyze fresh input. Suppose we are given the                                                     the   dress                                 the   dress
following linguistic treebank of only two trees (we will                                                                                                              with the telescope
come back to musical treebanks in the next section),                                                      Figure 6: Different derivation producing different tree.
         S                                                   S
                                                                                                       The original DOP model in Bod (1993) uses the
  NP          VP                                                                                       likelihood principle to predict the perceived tree structure.
                                                  NP                   VP
                                                                                                       We will refer to this model as L i k e l i h o o d - D O P .
  she V             NP                                                                                 Likelihood-DOP selects the most likely tree structure from
                                                  she        VP                PP
                                                                                                       among all possible tree structures on the basis of the
     wanted
                                                        V        NP          P      NP                 probabilities of its subtrees. The probability of a subtree t
              NP          PP
                                                                                                       is estimated as the number of occurrences of t seen in the
                               NP
                                                      saw the        dog with the telescope            corpus, divided by the total number of occurrences of
         the    dress P
                                                                                                       corpus-subtrees that have the same root label as t. The
                      on the      rack                                                                 probability of a derivation is computed as the product of
                                                                                                       the probabilities of the subtrees involved in it. Finally, the
                       Figure 3: An example treebank                                                   probability of a parse tree is equal to the sum of the
                                                                                                       probabilities of all distinct derivations that produce that
then the DOP model can parse a new sentence, e.g. She                                                  tree. In Bod (2001a) and Goodman (2002), efficient
saw the dress with the telescope, by combining subtrees                                                algorithms are given that compute for an input string the
from this treebank by means of a node-substitution                                                     most probable parse tree.
operation (indicated as °):                                                                              Likelihood-DOP does not do justice to the preference
          S         °         NP       °       PP          =              S                            humans display for the simplest structure, e.g. the one that
                                                                                                       is generated by the shortest derivation consisting of the
                                            P      NP
  NP            VP        the   dress                              NP            VP                    fewest subtrees. This is what we will call Simplicity-DOP.
                                           with the telescope                                          Instead of producing the most probable parse tree for an
                                                                  she
 she     VP            PP                                                 VP            PP             input, Simplicity-DOP produces the parse tree generated
                                                                             NP       P     NP
                                                                                                       by the fewest corpus-subtrees, independent of the
      V      NP                                                        V
                                                                                                       probabilities of these subtrees. For example, given the
     saw                                                              saw the dress with the telescope corpus in Figure 3, the simplest parse tree for She saw the
                                                                                                       dress with the telescope according to Simplicity-DOP is
      Figure 4: Parsing a sentence by combining subtrees                                               given in Figure 5, since that parse tree can be generated
Thus the node-substitution operation combines two                                                      by a derivation of only two corpus-subtrees, while the
subtrees by substituting the second subtree on the leftmost                                            parse tree in Figure 6 (and any other parse tree) needs at
nonterminal leaf node of the first subtree. Since DOP uses                                             least three corpus-subtrees to be generated. In Bod
subtrees of arbitrary size, there are typically several                                                (2000a) it is shown how the shortest derivation can be
derivations, involving different subtrees, that produce the                                            efficiently computed by means of a best-first bottom-up
same parse tree; for instance:                                                                         chart parsing algorithm. Simplicity-DOP obtains quite
          S                               NP            =              S
                                                                                                       impressive results on the WSJ, though its results are lower
                           °                                                                           than Likelihood-DOP (Bod 2000a). Yet, the set of
 NP                 P                 the   dress           NP                 VP
                                                                                                       correctly predicted parse trees of Simplicity-DOP is not a
                                                                                                       subset of the set of correctly predicted parse trees of
 he                                                         she
                                                                                                       Likelihood-DOP. This suggests that we may expect an
            P                P                                        VP                PP
                                                                                                       accuracy improvement if simplicity and likelihood are
      V       NP        P       NP                               V        NP        P        P         combined into a new model, which we will call
                                                                                                       Combined-DOP.
     saw              with he telescope                         saw the      ress   ith the elescope      The underlying idea of Combined-DOP is that the
                                                                                                       human perceptual system searches for the shortest
       Figure 5: Different derivation producing same tree.                                             derivation (i.e. the simplest tree structure), but that in
The more interesting case occurs when there are different                                              doing so it is biased by the "weights" of the subtrees. The
derivations that produce different parse trees. This                                                   length of a derivation is then not defined simply as the
happens when a sentence is structurally ambiguous; for                                                 sum of the derivation steps (as in Simplicity-DOP), but as
                                                                                                       the sum of the weights of these steps, where a low weight
                                                                                                       should be seen as an easy step and a heavy weight as a

difficult step. As a measure for weight of a subtree, we      Collection consists of the label "S" for each top node of
have worked out various proposals that were                   each whole song and the label "P" for each underlying
experimentally tested over the last few years. Most of        phrase. In this way, we obtained conventional parse trees
these proposals have been reported in the literature (e.g.    that can directly be used by our DOP models to parse new
Bod 2000a; Cormons 1999). The best measure for subtree        input strings (see also Bod 2001b). The Essen Folksong
weight so far is based on the rank of a subtree. First, all   Collection is freely available via http://www.esac-
subtrees are grouped with respect to their root label. Next,  data.org.
for each root label the weight of a subtree is defined as its   As mentioned in the introduction, no visual treebank is
rank in the frequency ordering in the corpus. Thus, the       currently available, to the best of our knowledge. We are
most frequent subtree in each ordering gets a weight of 1,    currently developing a treebank of analyzed architectural
the second most frequent subtree gets a weight of 2, etc.     plans, and will report on experiments with that treebank in
The weight of a derivation is then defined as the sum of      due time.
the weights of the subtrees in the derivation. The
derivation with the lowest weight is taken as the "best"                        Experimental Evaluation
derivation producing the perceived parse tree. Thus, the      To evaluate our DOP models, we used the blind testing
best derivation is not determined by the smallest sum of
                                                              method which dictates that a treebank be randomly
the subtrees (as in Simplicity-DOP), but by the smallest      divided into a training set and a test set, where the strings
sum of the weights of the subtrees.                           from the test set are parsed by means of the subtrees from
   We performed one additional adjustment to the weight       the training set. We applied the PARSEVAL metrics of
of a subtree. This adjustment consists in a smoothing         precision and recall to compare a proposed parse tree P
technique which averages the weight of a subtree by the       with the corresponding correct test set parse tree T (see
weights of its own sub-subtrees. That is, instead of taking   Black et al. 1991):
only the rank of a subtree as its weight, we compute the
weight of a subtree as the (arithmetic) mean of the           Precision =
                                                                          # correct constituents in P
                                                                                                       Recall =
                                                                                                                # correct constituents in P
weights of all its sub-subtrees (including the subtree                       # constituents in P                   # constituents in T
itself). The effect of this smoothing technique is that it
                                                              A constituent in P is "correct" if there exists a constituent
redresses a very low-frequency subtree if it contains high-
frequency sub-subtrees.                                       in T of the same label that spans the same elements (i.e.
                                                              words or notes). To balance precision and recall into a
                    The Test Domains                          single measure, we will employ the widely used F-score:
                                                              F-score = 2*Precision*Recall / (Precision+Recall).
Our linguistic test domain consists of sections 02-21 of the    We will use this F-score to quantitatively evaluate our
Wall Street Journal portion of the Penn Treebank, which       models on the Wall Street Journal and the Essen Folksong
contains approx. 40,000 phrase-structure trees. Since the     treebanks. We divided both treebanks into 10 training/test
Penn Treebank has been extensively described in the           set splits, where 90% of the trees was used for training
literature (e.g. Marcus et al. 1993; Manning & Schütze        and 10% for testing. These splits were random, except for
1999), we will not go into it any further here.               one constraint: that all the primitive elements (i.e. words
   The musical test domain consists of the European           and notes) in the test set also occurred in the training set.
folksongs in the Essen Folksong Collection (Schaffrath        In this way, we did not have to worry about unknown
1995), which correspond to approx. 6,200 musical              words or unknown notes (the latter being actually
grouping structures. The Essen Folksong Collection has        inexistent for our musical treebank). Although there are
been previously used by Bod (2001b) and Temperley             various statistical ways to cope with unknown words, we
(2001) to test their musical parsers. The musical coding      wanted to rule out this problem as it might obscure our
language used in the Essen Folksong Collection is based       comparison.
on the Essen Associative Code (ESAC). The pitch                 In our experiments we were first of all interested in
encodings in ESAC resemble "solfege": scale degree            comparing the three DOP models (Likelihood-DOP,
numbers are used to replace the movable syllables "do",       Simplicity-DOP and Combined-DOP) on the two domains.
"re", "mi", etc. Thus 1 corresponds to "do", 2 corresponds    For computational reasons, we limited the maximum size
to "re", etc. Chromatic alterations are represented by        of the subtrees to depth 14, as in Bod (2001a). Table 1
adding either a "#" or a "b" after the number. The plus       shows the average F-scores for each of the models.
("+") and minus ("-") signs are added before the number if
a note falls resp. above or below the principle octave           Table 1: F-scores obtained by the three DOP models
(thus -1, 1 and +1 refer al to "do", but on different
octaves). Duration is represented by adding a period or an
underscore after the number. A period (".") increases                       Likelihood-DOP          Simplicity-DOP        Combined-DOP
duration by 50% and an underscore ("_") increases
duration by 100%; more than one underscore may be              Language          90.4%                 88.1%                  91.7%
added after each number. If a number has no duration
                                                               Music             86.0%                 84.3%                  86.9%
indicator, its duration corresponds to the smallest value. A
pause is represented by 0, possibly followed by duration
indicators. No loudness or timbre indicators are used in      The table shows that Likelihood-DOP outperforms
ESAC. The only extra information we (automatically)           Simplicity-DOP, but that Combined-DOP outperforms
added to the grouping structures in the Essen Folksong        Likelihood-DOP. According to paired t-testing, the

improvement of Combined-DOP over Likelihood-DOP was          the context of Kolmogorov's complexity theory
statistically significant both for language (p<.0001) and    (Kolmogorov 1965). And in the context of Information
for music (p<.04).                                           Theory the simplicity principle can be defined in terms of
  We also performed a series of experiments where we         bit length, such that maximizing likelihood corresponds to
restricted the size of the subtrees. Recall that by          minimizing bit length (cf. Rissanen 1978). First note that
restricting the subtrees to depth 1, Likelihood-DOP          the likelihood principle aims at maximizing the
becomes equivalent to a probabilistic context-free           probability of a structure given an input,
grammar, while Simplicity-DOP would just return the          p(structure | input). Next, define the simplicity principle
smallest possible tree structure. While Likelihood-DOP       as minimizing the informatic-theoretical notion of bit
still obtained relatively good results at depth 1 for both   length, which is the (negative) logarithm of the
language and music (resp. 75.1% and 76.6%), Simplicity-      probability of a structure given an input:
DOP scored very badly for language (22.5%) though still      − log p (structure | input). Now it is easy to see that
reasonably for music (70.0%). Interestingly, Combined-       maximizing p (structure | i n p u t ) leads to the same
DOP scored worse than Likelihood-DOP at depth 1 (resp.       structure as minimizing −log p(structure | input). Thus the
68.2% vs. 74.6%). Only after subtree depth 6 for language    two principles lead to the same result.
and subtree depth 2 for music, Combined-DOP                           However, in the context of DOP we defined the
outperformed Likelihood-DOP. The highest F-scores were       simplest structure as the one generated by the shortest
obtained with the "unrestricted" subtrees (in table 1).      derivation consisting of the smallest number of subtrees
  Elsewhere we have shown that virtually any constraint      (reflecting the smallest number of steps needed to parse
on the subtrees results in an accuracy decrease (Bod         an input). And this notion of simplest structure is provably
2001a/b). This is because in language, almost any relation   different from the most probable structure given an input.
between words (including between so-called non-              Although it is possible to redefine our notion of simplest
headwords) can be important for predicting the perceived     structure in terms of bit length, it would not lead to any
parse tree of a sentence. The same counts for music,         new model, and to no improved result. By conceptually
where there is a continuity between "jump-phrases" and       separating between simplicity and likelihood in DOP and
"non-jump-phrases", which can only be captured by large      by combining them in a novel way, we have shown that
subtrees (see Bod 2001b/c for an extensive discussion).      an improved model can be obtained.
                                                                      What we have not done in this paper is to isolate
  Discussion: Other Combinations of Simplicity               the perceptual properties for which no prior expectations
                      and Likelihood                         are needed. Even Simplicity-DOP, albeit non-
We have seen that our combination of simplicity and          probabilistic, is heavily based on previously perceived
likelihood is quite rewarding for linguistic and musical     data. It is very likely that there are perceptual grouping
                                                             properties for which no prior expectations are necessary.
perception, suggesting a deep parallel between the two
modalities. Yet, we should raise the question whether a      DOP does not contribute to the discovery of such
model which massively stores and re-uses previously          properties, but it does neither neglect them, as they are
perceived structures has any cognitive plausibility.         implicit in the treebank. Bod (2001b) shows that
                                                             Wertheimer's Gestalt principles are reflected in about
Interestingly, there is quite some evidence that people
                                                             85% of the phrases in the Essen Folksong Collection
store various kinds of previously heard fragments, both in
music (Saffran et al. 2000) and language (Jurafsky 2002).    (where phrases have boundaries that fall on large time or
But do people store fragments of arbitrary size, as          pitch intervals). DOP automatically takes these principles
proposed by DOP? In his overview article, Jurafsky (2002)    into account by subtrees that contain such phrases, but
                                                             DOP also takes into account phrases whose boundaries do
reports on a large body of psycholinguistic evidence
                                                             not fall on large intervals (so-called "jump-phrases"). By
showing that people not only store lexical items and
bigrams, but also frequent phrases and even whole            using all subtrees, DOP mimics the preferences humans
sentences. For the case of sentences, people not only        have used in analyzing the perceptual data, whatever
store idiomatic sentences, but also "regular" high-          these preferences may have been.
frequency sentences. Thus, at least for language it seems
that humans store fragments of arbitrary size provided that                         References
these fragments have a certain minimal frequency.            Black, E. et al. (1991). A Procedure for Quantitatively
However, there seems to be no evidence that people store         Comparing the Syntactic Coverage of English,
all fragments they hear, as suggested by DOP. Only high-         Proceedings DARPA Speech and Natural Language
frequency fragments seem to be memorized. However, if            Workshop, Pacific Grove, Morgan Kaufmann.
the human perceptual faculty needs to learn which            Bod, R. (1993). Using an Annotated Language Corpus as
fragments will be stored, it will initially need to keep         a Virtual Stochastic Grammar, Proceedings AAAI-93,
track of all fragments (with the possibility of forgetting       Menlo Park, Ca.
them) otherwise frequencies can never accumulate. This       Bod, R. (1998). Beyond Grammar: An Experience-Based
results in a model which continuously and incrementally          Theory of Language, Stanford: CSLI Publications.
updates its fragment memory given new input, which is in     Bod, R. (2000a). Parsing with the Shortest Derivation.
correspondence with the DOP approach.                            Proceedings COLING-2000, Saarbrücken, Germany.
          There have been other proposals for integrating or Bod, R. (2001a). What is the Minimal Set of Fragments
reconciling the principles of simplicity and likelihood.         that Achieves Maximal Parse Accuracy? Proceedings
Chater (1999) argues that the principles are identical in        ACL'2001, Toulouse, France.

Bod, R. (2001b). Memory-Based Models of Music                Jurafsky, D. (2002). Probabilistic Modeling in
   Analysis. Proceedings International Computer Music           Psycholinguistics: Comprehension and Production. In
   Conference (ICMC'2001), Havana, Cuba.                        R. Bod et al. 2002a.
Bod, R. (2001c). Memory-Based Models of Melodic              Kersten, D. (1999). High-level vision as statistical
   Analysis: Challenging the Gestalt Principles. Journal        inference. In S. Gazzaniga (ed.), The New Cognitive
   of New Music Research, 30(3), in press.                      Neurosciences, Cambridge, The MIT Press.
Bod, R., J. Hay and S. Jannedy (eds.) (2002a).               Kolmogorov, A. (1965). Three approaches to the
   Probabilistic Linguistics. Cambridge, The MIT Press.         quantitative definition of information. Problems in
   (in press)                                                   Information Transmission 1, 1-7.
Bod, R., R. Scha and K. Sima'an (eds.) (2002b). Data-        Leeuwenberg, E. (1971). A Perceptual Coding Language
   Oriented Parsing. Stanford, CSLI Publications. (in           for Perceptual and Auditory Patterns. American Journal
   press)                                                       of Psychology. 84, 307-349.
Buffart, H., E. Leeuwenberg and F. Restle (1983).            Lerdahl, F. and R. Jackendoff (1983). A Generative
   Analysis of Ambiguity in Visual Pattern Completion.          Theory of Tonal Music. Cambridge, The MIT Press.
   Journal of Experimental Psychology: Human Perception      Leyton, M. (2001). A Generative Theory of Shape.
   and Performance. 9, 980-1000.                                Heidelberg, Springer-Verlag.
Charniak, E. (1993). Statistical Language Learning,          Longuet-Higgins, H. and C. Lee, (1987). The Rhythmic
   Cambridge, The MIT Press.                                    Interpretation of Monophonic Music. In: M e n t a l
Charniak, E. (1997). Statistical Techniques for Natural         Processes: Studies in Cognitive Science, Cambridge,
   Language Parsing, AI Magazine, Winter 1997, 32-43.           The MIT Press.
Charniak, E. (2000). A Maximum-Entropy-Inspired Parser.      Manning, C. and H. Schütze (1999). Foundations of
   Proceedings ANLP-NAACL'2000, Seattle, Washington.            Statistical Natural Language Processing. Cambridge,
Chater, N. (1999). The Search for Simplicity: A                 The MIT Press.
   Fundamental Cognitive Principle? The Quarterly            Marcus, M., B. Santorini and M. Marcinkiewicz (1993).
   Journal of Experimental Psychology, 52A(2), 273-302.         Building a Large Annotated Corpus of English: the
Chomsky, N. (1965). Aspects of the Theory of Syntax,            Penn Treebank, Computational Linguistics 19(2).
   Cambridge, The MIT Press.                                 Marr, D. (1982). Vision. San Francisco, Freeman.
Collard, R., P. Vos and E. Leeuwenberg, (1981). What         Mumford, D. (1999). The dawning of the age of
   Melody Tells about Metre in Music. Zeitschrift für           stochasticity. Based on a lecture at the Accademia
   Psychologie. 189, 25-33.                                     Nazionale dei Lincei. (http://www.dam.brown.edu/
Collins, M. (2000). Discriminative Reranking for Natural        people/mumford/Papers/Dawning.ps)
   Language Parsing, Proceedings ICML-2000, Stanford,        Osborne, M. (1999). Minimal Description Length-Based
   Ca.                                                          Induction of Definite Clause Grammars for Noun
Cormons, B. (1999). Analyse et désambiguisation: Une            Phrase Identification. Proceedings EACL Workshop on
   approche à base de corpus (Data-Oriented Parsing) pour       Computational Natural Language Learning, Bergen,
   les répresentations lexicales fonctionnelles. PhD thesis,    Norway.
   Université de Rennes, France.                             Raphael, C. (1999). Automatic Segmentation of Acoustic
Crocker, M. and T. Brants (2000). Wide-coverage                 Musical Signals Using Hidden Markov Models. IEEE
   probabilistic sentence processing. Journal of                Transactions on Pattern Analysis and Machine
   Psycholinguistic Research 29, 647-669.                       Intelligence, 21(4), 360-370.
Frazier, L. (1978). On Comprehending Sentences:              Restle, F. (1970). Theory of Serial Pattern Learning:
   Syntactic Parsing Strategies. PhD. Thesis, University of     Structural Trees. Psychological Review. 86, 1-24.
   Connecticut.                                              Rissanen, J. 1978. Modeling by the shortest data
Goodman, J. (2002). Efficient Parsing of DOP with PCFG-         description. Automatica, 14, 465-471.
   Reductions. In R. Bod et al. 2002b.                       Saffran, J., M. Loman and R. Robertson (2000). Infant
Gorrell, P. (1995). Syntax and Parsing. Cambridge               Memory for Musical Experiences. Cognition 77, B16-
   University Press.                                            23.
Hale, J. (2001). A Probabilistic Earley Parser as a          Schaffrath, H. (1995). The Essen Folksong Collection in
   Psycholinguistic Model. Proceedings NAACL'01,                the Humdrum Kern Format. D. Huron (ed.). Menlo
   Pittsburgh, PA.                                              Park, CA: Center for Computer Assisted Research in
van der Helm, P. (2000). Simplicity versus Likelihood in        the Humanities.
   Visual Perception: From Surprisals to Precisals.          Simon, H. (1972). Complexity and the Representation of
   Psychological Bulletin, 126(5), 770-799.                     Patterned Sequences os Symbols. Psychological
von Helmholtz, H. (1910). Treatise on Physiological             Review. 79, 369-382.
   Optics (Vol. 3), Dover, New York.                         Temperley, D. (2001). The Cognition of Basic Musical
Hoffman, D. (1998). Visual Intelligence. New York, Norton       Structures. Cambridge, The MIT Press.
   & Company, Inc.                                           Wertheimer, M. (1923). Untersuchungen zur Lehre von
Jurafsky, D. (1996). A probabilistic model of lexical and       der Gestalt. Psychologische Forschung 4, 301-350.
   syntactic access and disambiguation, C o g n i t i v e
   Science, 20, 137-194.

