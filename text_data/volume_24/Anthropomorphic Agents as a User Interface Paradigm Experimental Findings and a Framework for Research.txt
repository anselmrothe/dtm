UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Anthropomorphic Agents as a User Interface Paradigm: Experimental Findings and a
Framework for Research
Permalink
https://escholarship.org/uc/item/4vq9d62j
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Catrambone, Richard
Stasko, John
Xiao, Jun
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                         Anthropomorphic Agents as a User Interface Paradigm:
                          Experimental Findings and a Framework for Research
                                         Richard Catrambone (rc7@prism.gatech.edu)
                                School of Psychology, Georgia Institute of Technology, 274 5th Street
                                                       Atlanta, GA 30332-0170 USA
                                                 John Stasko (stasko@cc.gatech.edu)
                                                  Jun Xiao (junxiao@cc.gatech.edu)
                                        College of Computing, Georgia Institute of Technology
                                                       Atlanta, GA 30332-0280 USA
                             Abstract                                     An anthropomorphic interface could use intonation, gaze
                                                                       patterns, and facial expressions, in addition to words, for
   Research on anthropomorphic agent interfaces has produced           conveying information and affect. The human face seems
   widely divergent results. We suggest that this is due to
                                                                       to occupy a privileged position for conveying a great deal
   insufficient consideration of key factors that influence the
   perception and effectiveness of agent-based interfaces. We
                                                                       of information, including relatively subtle information,
   propose a framework for studying anthropomorphic agents             efficiently (Fridlund & Gilbert, 1985). Anthropomorphic
   that can systematize the research. The framework emphasizes         interfaces could make a computer more human-like,
   features of the agent, the user, and the task the user is           engaging, entertaining, approachable, and understandable
   performing. Our initial experiment within this framework            to the user, thus harboring potential to build trust and
   manipulated the agent’s appearance (lifelike versus iconic)         establish relationships with users, and make them feel more
   and the nature of the user’s task (carrying out procedures          comfortable with computers.
   versus providing opinions). We found that the perception of            These potential advantages are balanced by strong
   the agent was strongly influenced by the task while features        negatives. Anthropomorphic agent interfaces are viewed by
   of the agent that we manipulated had little effect.
                                                                       some researchers as being impractical and inappropriate.
                                                                       Current speech recognition, natural language
                         Introduction                                  understanding, and learning capabilities of computers still
If you could ask for assistance from a smart, spoken natural           fall far short of any human assistant.
language help system, would that be an improvement over                   More specifically, it has been proposed that agent
an on-line reference manual? Presumably the answer, in                 systems disempower users by clouding issues such as who
most cases, is yes, for two reasons. First, the spoken natural         is responsible for a system’s actions (Lanier, 1995). Others
language aspect would allow you to speak your questions                feel that user interfaces are more beneficial when they
rather than having to type them. Generally this is a faster            clearly reflect the commands available to a user and present
approach for most people. Second, the smart aspect would               the objects that a user can act upon (Shneiderman, 1997).
improve the chance of the help system finding the                      Furthermore, critics argue that agent interfaces may
information you want even if you do not state the query                mislead both users and designers, increase user anxiety,
using the correct or most appropriate terms.                           reduce user control, undermine user responsibility, and
   The state of the art in this style of interface is a human          destroy a user’s sense of accomplishment (Shneiderman &
user consultant. Does it matter that the user consultant has           Maes, 1997). Many current anthropomorphic or personified
a face and that the face can have expressions and convey a             interfaces are viewed as being annoying, silly characters
personality? Would a face affect you in terms of your                  who hinder rather than enhance productivity (e.g., the
comfort and satisfaction with the interaction? Would the               Microsoft Paper Clip).
presence of a face make the help or advice you receive                    Although strong opinions have been voiced on both sides
more persuasive? The answers to such questions have                    of this issue, relatively little careful empirical research on
implications for the design of training systems, customer              anthropomorphic interfaces has been conducted, and the
service, information kiosks, and many other applications.              results from this research have been contradictory or
   Many people believe that anthropomorphic computer                   equivocal (Cassell, 2000).
interfaces have great potential to be beneficial for a number             Our goal is to develop a framework to systematically
of reasons. Agents could act as smart assistants, much like            evaluate and understand the autonomous agent as a user
travel agents or investment advisors, helping people                   interface paradigm. The present paper outlines the
manage the ever-growing amount of information                          framework and an initial study that examines two issues
encountered today (Lyman & Varian, 2002). Further, a                   within this framework. The first is whether the degree to
conversational interface appears to be a natural dialog style          which an interface agent is anthropomorphic has a
in which the user does not have to learn complex command               measurable effect on users. Note that anthropomorphism is
structure and functionality (Laurel, 1990).                            not a dichotomy but rather a continuum. One can think of
                                                                       interfaces with full fidelity video or 3D images of people to

more caricature-style characters to 2D cartoons of people or better than do those with different personalities) apply even
personified characters such as dogs or toasters.             when one of the two participants is a machine.
   The second issue is to what extent the nature of the task   The studies cited above, and others, suggest that people
will influence a user’s perception of an agent. Some tasks   are inclined to attribute human-like characteristics to agents
might be more likely to induce a user to imbue the agent     and that a variety of factors might influence how positively
with human-like qualities (such as if the user had to engage the agents are viewed. Dehn and van Mulken (2000)
the agent in a debate) while other tasks might lead the user provide a more extensive review of this literature.
to view the agent simply as a reference tool (e.g., for
providing reminders of keystrokes for a software                         A Framework for Research on
application) with no "individuality."                                Anthropomorphic Interface Agents
                                                             To effectively and systematically investigate the use of
                      Related Work                           anthropomorphic interface agents, one needs to consider
A few studies have revealed that anthropomorphic agents      the key factors that will affect the usefulness of such
are attention-grabbing and people make natural               interfaces. We propose an investigative framework
assumptions about the intelligence and abilities of those    composed of three key components: characteristics of the
agents. King and Ohya (1996) found that a dynamic 3D         user, attributes of the agent, and the task being performed.
human form whose eyes blinked was rated more intelligent       We believe that serious empirical study in this area must
than any other form, including non-blinking 3D forms,        systematically address each of these factors and understand
caricatures, and geometric shapes.                           how it affects human users. Below, we provide examples of
   One common trend discovered in studies is that            individual variables within each factor that could
anthropomorphic interfaces appear to command people’s        potentially influence user performance and impressions.
attention, both in positive and negative senses. Takeuchi
and Nagao (1995) created conversational style interaction    Factor 1: Features of the User
systems that allowed corresponding facial displays to be     Potential users vary, of course, in many ways. However,
included or omitted. According to their metrics, the         there are certain features that may be quite likely to affect
conversations with a face present were more "successful."    how useful a user finds an agent. These features include:
Across two experiments they found that the presence of a       Personality: Researchers have identified what are
face provided important extra conversational cues, but that  referred to as the "Big Five" traits that seem to be quite
this also required more effort from the human interacting    useful in describing human personalities: extraversion,
with the system and sometimes served as a distraction.       openness,        agreeableness,          neuroticism,     and
   Other studies have shown that the attention garnered by   conscientiousness (e.g., McCrae & Costa, 1987). While
an anthropomorphic interface had a more positive effect.     any such breakdown is debatable, it seems reasonable to
Walker, Sproull, and Subramani (1994) found that people      examine whether users’ positions on these, or other, trait
who interacted with a talking face spent more time on an     dimensions predicts how they will respond to agents.
on-line questionnaire, made fewer mistakes, and wrote          Background Knowledge: A user who has a good deal
more comments than those who answered a text                 of background knowledge in a domain might prefer an
questionnaire. Koda (1996) created a Web-based poker         agent that is reactive and that the user can call upon when
game in which a human user could compete with other          he or she needs some low-level bit of information or has a
personified computer characters including a realistic image, low-level task that needs to be done. Conversely, a user
cartoon male and female characters, a smiley face, no face,  who is learning how to carry out tasks in a particular
and a dog. She gathered data on people’s subjective          domain might welcome strategy advice from an agent,
impressions of the characters and found that people’s        particularly if the agent can analyze the strategy and
impressions of a character were different in a task context  provide reasons for why the strategy might be altered.
than in isolation and were strongly influenced by perceived    Other Variables: Other user-related variables include
agent competence.                                            gender, age, and computer experience.
   An influential body of related work is that of Nass and
his colleagues. Their efforts focus on the study of          Factor 2: Features of the Agent
"Computers as Social Actors." They have conducted a          Fidelity: Earlier studies suggest that more realistic-
number of experiments that examined how people react to      appearing, 3D human representations are perceived as
computer systems and applications that have certain          being more intelligent, which could be viewed positively or
personified characteristics (Nass, Isbister, & Lee, 2000;    negatively. However, realistic-appearing agents are more
Nass, Steuer, & Tauber, 1994; Rickenberg & Reeves,           difficult to implement, so if user performance is improved
2000). Their chief finding is that people interact with and  by the presence of an agent, but does not vary according to
characterize computer systems in a social manner, much as    appearance, simpler caricature style characters would be
they do with other people. Furthermore, they suggest that    advantageous.
findings in the social psychology literature (e.g.,            Presence: Is an agent’s face always present on the screen
individuals with similar personalities tend to get along     or does the agent only appear when it is engaged in a dialog

with the user? One might hypothesize that an ever-present      subjective end, a user is likely to have a number of
agent would make users uneasy by producing an effect of        affective reactions to an agent. These reactions might
being watched or evaluated all the time.                       manifest themselves in terms of how much users liked the
   Role: Should an agent act as a partner in the task or       agent, how intrusive they found the agent, how they
should it contribute only in clearly specified ways? For       perceived the agent’s personality, and how willing they are
instance, an agent might be able to offer strategy guidance    to use the agent in the future. We can certainly assess a
for design tasks. Alternatively, it might provide only         user’s liking and satisfaction towards an agent, but if the
lower-level procedural "how to" information.                   user can carry out the tasks more effectively with the agent
   Initiative:       Should an agent proactively make          regardless of liking and satisfaction, then how important
suggestions and offer guidance or should it respond only       are those variables? On the other hand, long-term use of an
when directly addressed? A proactive agent might be            agent might be predicted by liking and satisfaction.
viewed as being "pushy" and might bother users, or it could       The likelihood of a user following an agent’s advice
be viewed as being extremely helpful and intelligent if it     might be another interesting measure of the usefulness of
acts in situations in which the user is unsure of how to       an agent. While advice-following would certainly be at
proceed or is so confused that he or she is unable to form a   least partly a function of the quality of the advice, it will
coherent help request.                                         also be impacted by how the user feels about the agent
   Other Variables: Other agent-related variables to           (how many children ignore the advice of their parents
consider are expressiveness, speech quality, "gender,"         merely because it is the parents giving the advice?).
"personality," and competence.
                                                                                       Experiment
Factor 3: Features of the Task
                                                               Overview
Tasks can vary in a variety of ways. Some tasks can be
opinion-like (e.g., choosing what to bring on a trip) while    One fundamental issue in the quality of agent interfaces is
others are more objective (e.g., solving a puzzle) in terms    competence (Maes, 1994). It appears obvious that
of assessing the quality of a solution. Some involve a good    perceptions of anthropomorphic agent interfaces will be
deal of high-level planning (e.g., writing a talk) while       strongly influenced by the competence of the supporting
others are more rote (e.g., changing boldface words into       software system and the quality of the replies and
italics). Tasks might be classified along some or all of the   suggestions made by the agent. We chose to factor out
dimensions listed below:                                       competence as an influence. If our experiments uncover
   Objectiveness: The situation might be an opinion-based      that people’s performance is not enhanced and they dislike
one in which the user is seeking advice and                    anthropomorphic user interfaces even though the system is
recommendations on some topic (e.g., which items to pack       competent, then that is an important and strong result that
for a trip to Europe). Alternatively, the user might be        other researchers and developers need to understand. To
carrying out an objective task such as acquiring facts (e.g.,  remove competence as a factor, we employed a "Wizard of
finding the keystroke combination for a particular             Oz" (Dahlback, Jonsson, & Ahrenberg, 1993) methodology
command in a software application).                            (described below).
   I n t e n t : The user could have a learning goal or           The experiment manipulated the agent fidelity and the
alternatively may be carrying out a set of steps in a familiar task objectiveness variables because prior work and our
domain. In the latter, the user might want help with low-      framework suggest they seemed likely candidates to have
level details whereas in the former the user is looking for    an affect on the perception of agents. Usefulness was
guidance as to the structure of the domain.                    evaluated via both the performance and satisfaction
   Other Variables: Other task-related variables to            dimensions. We hypothesized that user reactions to the
consider are domain, degree of time pressure, duration, and    agent would vary as a function of the objectiveness of task.
consequences of the quality of task performance.               A task that required the user to debate the merits of his or
                                                               her opinion (about items to pack on a trip) might lead the
   The number of variables within each factor is certainly     user to feel the agent had more of a personality (for good or
larger than the number we have identified here. No doubt       for bad) compared to a task in which the user made use of
these factors will also interact. For instance, a novice       the agent more as a reference tool (i.e., reminding the user
attempting to carry out a task in a particular domain might    of keystroke commands for a text editor). We also
welcome proactive comments/advice from an agent while          hypothesized that users might find the agent to be more
someone with more experience could get annoyed.                useful in its role as a reference source rather than as an
   With respect to measuring the usefulness of an agent, we    entity that provides opinions. Finally, we expected that the
have to consider which dependent measures are most             more life-like the agent appeared, the more likely the user
appropriate. Towards the more objective end, a user’s          might be to ascribe qualities such as personality and
performance on a task in terms of accuracy and time--when      intelligence to the agent, but objective performance would
such measures are meaningful--can give one indication of       likely not be affected by appearance.
usefulness. Thus, time and errors would be appropriate
measures for a text-editing task. Towards the more

                                                                again filled out a questionnaire about the agent and
Participants                                                    answered questions from the experimenter.
Thirty-nine undergraduates participated for course credit          The agent was controlled through a Wizard of Oz
and were randomly assigned to conditions. Participants          technique. One experimenter was in the room with the
had a variety of majors and computer backgrounds.               participant to introduce the experimental materials, and a
                                                                second experimenter was in an adjacent room, monitoring
Procedure and Design                                            the questions and responses made by the participant. The
                                                                second experimenter insured that the agent responded in a
Participants were run individually using a computer
                                                                consistent manner using a prepared set of replies.
equipped with a microphone and speaker. Participants
                                                                   Two between-subjects variables were manipulated: type
performed two tasks: a travel task and an editing task. The
                                                                of agent (animated, stiff, iconic) and task order (travel task
travel task was chosen to be a type of creative, opinion-
                                                                then editing task or vice versa). The left side of Figure 1
based task in which interacting with an agent might be
                                                                shows the face of the agent in the animated and stiff
viewed as an opportunity to think more deeply about the
                                                                conditions. The animated agent (donated by Haptek Corp.)
task by discussing points of view about the importance of
                                                                was 3D, with a female appearance--though somewhat
travel items. The editing task was chosen to represent an
                                                                androgynous--that blinked, moved its head, and produced
opportunity to use an agent primarily as a reference source
                                                                certain facial expressions in addition to moving its mouth
rather than as a guide or teacher.
                                                                in synchronization with the synthesized voice. The stiff
   The travel task involved a hypothetical situation in which
                                                                agent had the same face as the animated agent but moved
the participant had a friend who was flying overseas on his
                                                                only its mouth. The iconic agent (see the right side of
first international trip. The task was to recommend six
                                                                Figure 1) was a light-bulb icon that had arrows appear
items for the person to take with him from a pool of 12
                                                                whenever it spoke.
items and to rank the six items in order of importance.
                                                                   One design issue about this experiment should be
   After the participant did the initial ranking using a simple
                                                                flagged. Although our key task manipulation was the
software interface, a computer agent who supposedly had
                                                                "objectiveness" of the task (i.e., the travel task being less
knowledge about international trips appeared. The agent
                                                                objective and the editing task being more objective), the
made a predefined set of suggestions in which it
                                                                nature of the agent also was varied as a function of the task.
recommended changing the rankings of four of the six
                                                                The agent was completely reactive in the editing task; it
choices and it agreed with the ranking of two other items.
                                                                provided information only when requested. However, in the
For example, the agent first suggested promoting the
                                                                travel task the agent provided feedback regardless of the
person’s fourth item to the first position, demoting the first
                                                                participants’ desire. A cleaner version of the experiment
item but keeping it in the top six. The agent explained the
                                                                would have been to hold the "nature" of the agent constant
reasoning for its suggestion at every stage and asked the
                                                                across the tasks. We allowed this confounding to occur here
participant what he or she thought about the suggestion.
                                                                because were interested in getting participants’ reactions to
After the participant responded to the agent’s comment on a
                                                                certain human-like attributes of the agent but did not have
particular item, the agent would say one of several
                                                                the resources to run the additional conditions that would
conversational conventions (e.g., "OK, let’s continue") so
                                                                have been required to completely cross this factor with the
that it could move on to the next suggestion. After the agent
                                                                task and appearance manipulations. In future work we plan
finished providing feedback on the rankings, the original
                                                                to systematically investigate this reactive/proactive
rankings were displayed on the screen and the participant
                                                                dimension.
was given the opportunity to change the rankings. After
doing the re-ranking, participants filled out a questionnaire
about the agent and were asked a few questions about the
agent and related issues by the experimenter.
   The editing task required participants to use an
unfamiliar text editor to modify an existing document by
making a set of prescribed changes to the document.
Participants first viewed a short video that described the
various functions (e.g., copy, paste) and the specific key
combinations needed to issue the commands. Participants
were then shown a marked-up document that required a set
of changes such as deletions, insertions, and moves, and
they were instructed that if at any time they could not             Figure 1: Appearance of Agent in Animated and Stiff
remember the keystrokes for a particular function, they                 Conditions (left) and Iconic Condition (right).
could ask the agent for help. Pilot testing was conducted to
ensure that the number of commands was sufficiently large       Measures
so that participants would be likely to need to ask the agent   Both objective and subjective measures were used. One
for help. After completing the editing tasks, participants      objective measure was, for the travel task, whether

participants changed their rankings as a function of the        number of interesting and insightful comments about the
agent’s feedback. For the editing task we measured how          agent in response to questions from the experimenter, a
long it took participants to complete the edits. The primary    simple tally of responses shows reactions to the agent that
subjective variables were the responses to the individual       again varied as a function of task. Virtually all participants
items in the questionnaires and the answers to the questions    found the agent helpful for both tasks. Participants were
posed by the experimenter. The questionnaire items used a       much less likely to consider the agent to have a personality
five-point Likert scale (1 = strongly agree, 5 = strongly       after doing the editing task compared to the travel task.
disagree) that addressed a number of qualities of the agent     This makes sense because the agent was merely providing
(see Table 2). The questions posed by the experimenter          subjects with information on commands in the editing task.
were open-ended and provided participants an opportunity        In the travel task the agent expressed its "opinions."
to give their impressions about the agent’s personality,           Finally, it is worth noting that in general the agent was
helpfulness, and intelligence.                                  perceived as more intelligent after the travel task than after
                                                                the editing task. At one level this seems odd because the
Results                                                         agent had all the answers for the editing task. However, as
In the data analyses we found that the task order               demonstrated by some participants’ comments, the agent
manipulation did not have an effect, so in the interest of      was perceived as very limited in the editing task; it knew
simplicity we will collapse across that factor in the           about editing commands and probably little else (despite
presentation and discussion of the results.                     the fact that it also appeared to understand spoken
   Performance Measures. With respect to more objective         language!). In the travel task though it presumably gave
measures, Table 1 shows that participants were more likely      the impression of having sufficiently deep knowledge about
to change the rankings of items that the agent disagreed        travel such that it could give feedback on the importance of
with compared to items that the agent agreed with, F(1, 36)     various items one might take on a trip. While some of the
= 38.48, MSE = .07, p < .0001). There was no effect of          participants’ responses to the agent indicated that they
type of agent, F(2, 36) = 0.9, MSE = .11, p = .42. There        disagreed with its suggestions, they appeared to believe that
was no interaction, F(2, 36) = 1.25, p = .30.                   the suggestions were at least thoughtful.
      Table 1: Proportion of Travel Items with Changed
                                                                Discussion
 Rankings as a Function of Type of Agent & Agent Advice.
                                                                In addition to the results reported above, we learned a great
                            Animated       Stiff     Iconic
                             (n =14)     (n =12)    (n =13)
                                                                deal by observing participants’ behaviors and responses in
     Suggested Change          .82         .90         .77      the sessions. One key question we had was how would the
     Keep Rank                 .57         .42         .38      participants interact with the agent in the two different
                                                                tasks. In the editing task, participants seemed very
   The time (in seconds) to do the editing task did not differ  comfortable asking the agent for assistance. Participants
as a function of agent (animated: 714.8, stiff: 568.7, iconic:  requested help an average of 6.5 times. However, in the
671.1); F(2, 31) = 1.78, M S E = 37637.22, p = .19 (5           travel task participants seemed reluctant to engage the
participants did not do the editing task).                      agent in a dialog. Only a few replied with more than a few
   Questionnaire Responses. Table 2 shows the mean              words when the agent engaged them. There was clearly
responses to the questionnaire items for the different agent    awkwardness to the interaction.
conditions after the travel and editing tasks (there were 5        The agent’s social abilities and personality (or lack
participants who did not do both tasks and they are             thereof) were noted by a number of the participants. In the
excluded from Table 2). There was no effect of agent type       travel task, we intentionally had the agent begin the session
for any of the questions. For two of the items, worthwhile      saying, "Hello, [person’s name]." Three participants
and intrusive, there was an effect of task (worthwhile: F(1,    explicitly mentioned this feature, one stating, when asked if
31) = 15.68, MSE = .45, p = .0004; intrusive: F(1, 31) =        the agent had a personality, "Yes, respectful. It said, ’[my
20.28, MSE = .23, p = .0001). The agent was rated more          name]’, and ’I agree with this.’...I thought that was very
worthwhile and less intrusive after the editing task            funny. That was really cool."
compared to the travel task. These results make sense.             Other comments implying a personality included,
First, the editing task required most participants to rely      "Seemed a lot like a travel agent that was in a hurry," and
heavily on the agent to remind them of commands, thus           "helpful, but kind of annoying," and "he seemed almost
making the agent seem worthwhile. Second, the uninvited         irritated when I didn’t agree with him." One participant who
critique of participants’ rankings of travel items could        did the editing task first, stated after the task that the agent
certainly have seemed intrusive.                                did not have a personality, "It was just directed at
   While group differences did not exist on most of the         answering questions. It had no inflections." But when
questionnaire items, it is interesting that for most items, the asked again after the travel task, the participant responded,
average response tended to be in the positive direction.        "It was still mechanical, but you could feel the attempt at
Participants felt positively, on average, about the agent.      being more personable. It acknowledged my responses,
                                                                asking me to elaborate. The responses were at a more
   Interview Responses. While participants made a

personal level." Participants’ willingness to ascribe a           work has been its lack of systematicity in examining key
personality to the agent based on a few comments by the           factors and the use of dependent measures that often did not
agent in one task suggests that people might be predisposed       appropriately assess subjective experience and objective
to "finding" a personality in an agent. If the effects of         performance.
seeing a personality in an agent can be better understood,           In this paper we introduced a framework for
such a predisposition might be exploited for good purpose         systematically examining the effects of anthropomorphic
by designers.                                                     agents on user performance and subjective responses. We
                                                                  performed an initial experiment within this framework that
Conclusion                                                        suggested that type of task may play an outsized role in the
Anthropomorphic interface agents might be one of the best         perception of agents. We plan to use our framework to
interface approaches ever devised. Or they might not.             guide additional studies and hope other researchers find it
Equivocal results from prior research make it virtually           useful and that it will allow future experiments to build on
impossible to decide this matter. The difficulty with prior       each other more effectively than in the past.
                      Table 2: Responses to Questionnaire Items as a Function of Type of Agent and Task.
                                         Animated (n=12)            Stiff (n =12)            Iconic (n =10)          AVG
  Agent was                             Travel       Edit       Travel         Edit        Travel        Edit    Travel/Edit
  Worthwhile                             2.50        1.58        2.25          1.42         2.30         2.10     2.35/1.57
  Intrusive                              2.83        3.50        3.50          4.00         3.40         3.80     3.24/3.76
  Friendly                               2.67        2.67        2.42          2.50         2.40         2.80     2.50/2.65
  Annoying                               3.25        3.33        2.83          3.25         3.20         3.80     3.09/3.44
  Intelligent                            2.58        2.92        2.58          2.50         2.40         2.70     2.53/2.71
  Cold                                   3.25        3.08        3.00          2.67         3.70         3.30     3.29/3.00
  Agent has clear voice                  2.33        2.58        2.58          2.33         2.50         2.40     2.47/2.44
  Enjoyed interacting with Agent         3.08        3.17        2.75          2.83         2.70         2.90     2.85/2.97
  Agent helped with task                 2.25        1.50        1.67          1.50         2.00         2.30     1.97/1.74
  Like to have agent                     2.83        2.67        2.58          2.33         2.20         2.40     2.56/2.47
Note: Responses were on a scale from 1 (strongly agree) to 5 (strongly disagree).
                                                                  Maes, P. (1994). Agents that reduce work and information
                         References                                 overload. Communications of the ACM, 37, 31-40.
Cassell, J. (2000). Embodied conversational interface             McCrae, R. & Costa, P. (1987). Validation of the five-factor
   agents. Communications of the ACM, 43, 70-78.                    model of personality across instruments and observers.
Dahlback, N., Jonsson, A. & Ahrenberg, L. (1993). Wizard            Journal of Personality and Social Psychology, 52, 81-90.
   of Oz studies — why and how. In Proceedings of the 1993        Nass, C., Isbister, K. & Lee, E. (2000). Truth is beauty:
   International Workshop on Intelligent User Interfaces,           Researching embodied conversational agents. In J.
   (Orlando, FL), 193-200.                                          Cassell, S. Prevost, J. Sullivan, and E. Churchill (Eds.),
Dehn, D.M. & van Mulken, S. (2000). The impact of                   Embodied conversational agents. Cambridge, MA: MIT
   animated interface agents: A review of empirical research.       Press, 374-402.
   International Journal of Human-Computer Studies 52, 1-         Nass, C., Steuer, J., & Tauber, E. (1994). Computers are
   22.                                                              social actors. In Proceedings of CHI ’94, 72-78.
Fridlund, A.J. & Gilbert, A.N. (1985). Emotions and facial        Rickenberg, R. & Reeves, B. (2000). The effects of
   expression. Science, 230, 607—608.                               animated characters on anxiety, task performance, and
King, W.J. & Ohya, J. (1996). The representation of agents:         evaluations of user interfaces. In Proceedings of CHI
   Anthropomorphism, agency and intelligence.              In       2000, 329-336.
   Proceedings CHI ’ 96 Conference Companion, 289-290.            Shneiderman, B. & Maes, P. (1997). Direct manipulation vs.
Koda, T. (1996). Agents with faces: A study on the effect of        interface agents. Interactions, 4, 42-61.
   personification of software agents. Masters thesis, MIT        Shneiderman, B. (1997). Direct manipulation versus agents:
   Media Lab, Cambridge, MA.                                        Paths to predictable, controllable, and comprehensible
Lanier, J. (1995). Agents of alienation. Interactions 2,            interfaces. In J.M. Bradshaw (Ed.), Software agents.
   66—72.                                                           Cambridge, MA: MIT Press, 97-106.
Laurel, B. (1990). Interface agents: Metaphors with               Takeuchi, A. & Nagao, K. (1995). Situated facial displays:
   character. In B. Laurel (Ed.), The art of human-computer         Towards social interaction. In Proceedings of CHI ’ 95,
   interface design. Reading, MA: Addison-Wesley, 355-              450-455.
   365.                                                           Walker, J.H., Sproull, L., & Subramani, R. (1994). Using a
Lyman, P. & Varian, H. (2002). How Much Information?                human face in an interface. In Proceedings of CHI ’ 94,
   Find at http://www.sims.berkeley.edu/how-much-info/.             85-91.

