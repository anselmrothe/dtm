UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A probabilistic approach to semantic representation
Permalink
https://escholarship.org/uc/item/44x9v7m7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Griffiths, Thomas L
Steyvers, Mark
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               A probabilistic approach to semantic representation
                                   Thomas L. Griﬃths & Mark Steyvers
                                    {gruffydd,msteyver}@psych.stanford.edu
                                               Department of Psychology
                                                   Stanford University
                                            Stanford, CA 94305-2130 USA
                         Abstract                             its rank. One component of this approximation is a
                                                              matrix that gives each word a location in a high di-
   Semantic networks produced from human data have            mensional space. Distances in this space are predic-
   statistical properties that cannot be easily captured
   by spatial representations. We explore a probabilis-       tive in many tasks that require the use of semantic
   tic approach to semantic representation that explic-       information. Performance is best for approximations
   itly models the probability with which words occur         that used less singular values than the rank of the
   in diﬀerent contexts, and hence captures the proba-        matrix, illustrating that reducing the dimensional-
   bilistic relationships between words. We show that         ity of the representation can reduce the eﬀects of
   this representation has statistical properties consis-
   tent with the large-scale structure of semantic net-       statistical noise and increase eﬃciency.
   works constructed by humans, and trace the origins            While the methods behind LSA were novel in scale
   of these properties.                                       and subject, the suggestion that similarity relates to
                                                              distance in psychological space has a long history
   Contemporary accounts of semantic representa-              (Shepard, 1957). Critics have argued that human
tion suggest that we should consider words to be              similarity judgments do not satisfy the properties of
either points in a high-dimensional space (eg. Lan-           Euclidean distances, such as symmetry or the tri-
dauer & Dumais, 1997), or interconnected nodes in a           angle inequality. Tversky and Hutchinson (1986)
semantic network (eg. Collins & Loftus, 1975). Both           pointed out that Euclidean geometry places strong
of these ways of representing semantic information            constraints on the number of points to which a par-
provide important insights, but also have shortcom-           ticular point can be the nearest neighbor, and that
ings. Spatial approaches illustrate the importance            many sets of stimuli violate these constraints. The
of dimensionality reduction and employ simple al-             number of nearest neighbors in similarity judgments
gorithms, but are limited by Euclidean geometry.              has an analogue in semantic representation. Nelson,
Semantic networks are less constrained, but their             McEvoy and Schreiber (1999) had people perform a
graphical structure lacks a clear interpretation.             word association task in which they named an as-
   In this paper, we view the function of associa-            sociated word in response to a set of target words.
tive semantic memory to be eﬃcient prediction of              Steyvers and Tenenbaum (submitted) noted that the
the concepts likely to occur in a given context. We           number of unique words produced for each target fol-
take a probabilistic approach to this problem, mod-           lows a power law distribution: if k is the number of
eling documents as expressing information related             words, P (k) ∝ k γ . For reasons similar to those of
to a small number of topics (cf. Blei, Ng, & Jordan,          Tversky and Hutchinson, it is diﬃcult to produce a
2002). The topics of a language can then be learned           power law distribution by thresholding cosine or dis-
from the words that occur in diﬀerent documents.              tance in Euclidean space. This is shown in Figure 1.
We illustrate that the large-scale structure of this          Power law distributions appear linear in log-log co-
representation has statistical properties that corre-         ordinates. LSA produces curved log-log plots, more
spond well with those of semantic networks produced           consistent with an exponential distribution.
by humans, and trace this to the fidelity with which
                                                              Semantic networks Semantic networks were pro-
it reproduces the natural statistics of language.
                                                              posed by Collins and Quillian (1969) as a means
Approaches to semantic representation                         of storing semantic knowledge. The original net-
                                                              works were inheritance hierarchies, but Collins and
Spatial approaches Latent Semantic Analysis
                                                              Loftus (1975) generalized the notion to cover arbi-
(LSA; Landauer & Dumais, 1997) is a procedure
                                                              trary graphical structures. The interpretation of this
for finding a high-dimensional spatial representation
                                                              graphical structure is vague, being based on connect-
for words. LSA uses singular value decomposition
                                                              ing nodes that “activate” one another. Steyvers and
to factorize a word-document co-occurrence matrix.
                                                              Tenenbaum (submitted) constructed a semantic net-
An approximation to the original matrix can be ob-
                                                              work from the word association norms of Nelson et
tained by choosing to use less singular values than

        0
       10
                 Word association data
                                                  10
                                                    0
                                                             Latent semantic analysis
                                                                                                      Intuitively, P (w|z = j) indicates which words are
                                                                                    d = 400
                                                                                    d = 200        important to a topic, while P (z) is the prevalence
                                                                                                   of those topics within a document. For example,
        −1                                          −1
                                                                                    d = 50
       10                                         10
                                                                                                   imagine a world where the only topics of conversa-
P(k)
        −2
       10                                         10
                                                    −2
                                                                                                   tion are love and research. In such a world we could
                                                                                                   capture the probability distribution over words with
                                                                                                   two topics, one relating to love and the other to re-
        −3                                          −3
       10                                         10
        −4                                          −4
                                                                                                   search. The diﬀerence between the topics would be
       10
             0
            10     10
                     1
                                10
                                  2          3
                                            10
                                                  10
                                                       10
                                                         0
                                                                10
                                                                   1
                                                                             10
                                                                                2              3
                                                                                              10   reflected in P (w|z = j): the love topic would give
                                                                                                   high probability to words like joy, pleasure, or heart,
                          k
Figure 1: The left panel shows the distribution of the                                             while the research topic would give high probability
number of associates named for each target in a word                                               to words like science, mathematics, or experiment.
association task. The right shows the distribution                                                 Whether a particular conversation concerns love, re-
of the number of words above a cosine threshold for                                                search, or the love of research would depend upon
each target in LSA spaces of dimension d, where the                                                the distribution over topics, P (z), for that particu-
threshold was chosen to match the empirical mean.                                                  lar context.
                                                                                                      Formally, our data consist of words w =
                                                                                                   {w1 , . . . , wn }, where each wi belongs to some doc-
al. (1999), connecting words that were produced as                                                 ument di , as in a word-document co-occurrence ma-
responses to one another. In such a semantic net-                                                  trix. For each document we have a multinomial dis-
work, the number of associates of a word becomes                                                   tribution over the T topics, with parameters θ (di ) ,
the number of edges of a node, termed its “degree”.                                                so for a word in document di , P (zi = j) = θj i .
                                                                                                                                                       (d )
Steyvers and Tenenbaum found that the resulting                                                    The jth topic is represented by a multinomial dis-
graph had the statistical properties of “small world”                                              tribution over the W words in the vocabulary, with
graphs, of which a power law degree distribution is                                                                                          (j)
                                                                                                   parameters φ(j) , so P (wi |zi = j) = φwi . To make
a feature (Barabasi & Albert, 1999).
                                                                                                   predictions about new documents, we need to as-
   The fact that semantic networks can display these
                                                                                                   sume a prior distribution on the parameters θ (di ) .
properties reflects their flexibility, but there is no in-
                                                                                                   The Dirichlet distribution is conjugate to the multi-
dication that the same properties would emerge if
                                                                                                   nomial, so we take a Dirichlet prior on θ (di ) .
such a representation were learned rather than con-
structed by hand. In the remainder of the paper, we                                                   This probability model is a generative model: it
present a probabilistic method for learning a rep-                                                 gives a procedure by which documents can be gen-
resentation from word-document co-occurences that                                                  erated. First we pick a distribution over topics from
reproduces some of the large-scale statistical prop-                                               the prior on θ, which determines P (zi ) for words
erties of semantic networks constructed by humans.                                                 in that document. Each time we want to add a
                                                                                                   word to the document, we pick a topic according
                   A probabilistic approach                                                        to this distribution, and then pick a word from that
                                                                                                   topic according to P (wi |zi = j), which is determined
Anderson’s (1990) rational analysis of memory and
categorization takes prediction as the goal of the                                                 by φ(j) . This generative model was introduced by
learner. Analogously, we can view the function of                                                  Blei et al. (2002), improving upon Hofmann’s (1999)
associative semantic memory to be the prediction                                                   probabilistic Latent Semantic Indexing (pLSI). Us-
of which words are likely to arise in a given con-                                                 ing few topics to represent the probability distribu-
text, ensuring that relevant semantic information is                                               tions over words in many documents is a form of
available when needed. Simply tracking how often
words occur in diﬀerent contexts is insuﬃcient for                                                 dimensionality reduction, and has an elegant geo-
this task, as it gives no grounds for generalization.                                              metric interpretation (see Hofmann, 1999).
If we assume that the words that occur in diﬀerent                                                    This approach models the frequencies in a word-
contexts are drawn from T topics, and each topic
can be characterized by a probability distribution                                                 document co-occurrence matrix as arising from a
over words, then we can model the distribution over                                                simple statistical process, and explores the parame-
words in any one context as a mixture of those top-                                                ters of this process. The result is not an explicit rep-
ics                                                                                                resentation of words, but a representation that cap-
                   !T
                                                                                                   tures the probabilistic relationships among words.
                    P (wi ) =               P (wi |zi = j)P (zi = j)
                                                                                                   This representation is exactly what is required for
                                                                                                   predicting when words are likely to be used. Be-
                                      j=1
 where zi is a latent variable indicating the topic                                                cause we treat the entries in a word-document co-
from which the ith word was drawn and P (wi |zi = j)                                               occurrence matrix as frequencies, the representation
is the probability of the ith word under the jth topic.                                            developed from this information is sensitive to the
The words likely to be used in a new context can                                                   natural statistics of language. Using a generative
be determined by estimating the distribution over                                                  model, in which we articulate the assumptions about
topics for that context, corresponding to P (zi ).                                                 how the data were generated, ensures that we are

able to form predictions about which words might                                 Simulation 1:
be seen in a new document.                                    Learning topics with Gibbs sampling
   Blei et al. (2002) gave an algorithm for finding
                                                             The aim of this simulation was to establish the sta-
estimates of φ(j) and the hyperparameters of the
                                                             tistical properties of the sampling procedure and to
prior on θ (di ) that correspond to local maxima of
                                                             qualitatively assess its results, as well as to demon-
the likelihood, terming this procedure Latent Dirich-
                                                             strate that complexities of language like polysemy
let Allocation (LDA). Here, we use a symmetric
                                                             and behavioral asymmetries are naturally captured
Dirichlet(α) prior on θ (di ) for all documents, a sym-      by our approach. We took a subset of the TASA
metric Dirichlet(β) prior on φ(j) for all topics, and        corpus (Landauer, Foltz, & Laham, 1998), using the
Markov chain Monte Carlo for inference. An advan-            4544 words that occurred both in the word associa-
tage of this approach is that we do not need to ex-          tion norm data and at least 10 times in the complete
plicitly represent the model parameters: we can in-          corpus, together with a random set of 5000 docu-
tegrate out θ and φ, defining model simply in terms          ments. The total number of words occurring in this
of the assignments of words to topics indicated by           subset of the corpus, and hence the number of zi to
the zi . 1                                                   be sampled, was n = 395853. We set the parame-
   Markov chain Monte Carlo is a procedure for ob-           ters of the model so that 150 topics would be found
taining samples from complicated probability distri-
butions, allowing a Markov chain to converge to the          (T = 150), with α = 0.1, β = 0.01.
target distribution and then drawing samples from               The initial state of the Markov chain was estab-
the Markov chain (see Gilks, Richardson & Spiegel-           lished with an online learning procedure. Initially,
halter, 1996). Each state of the chain is an assign-         none of the wi were assigned to topics. The zi were
ment of values to the variables being sampled, and           then sequentially drawn according to Equation 1
transitions between states follow a simple rule. We          where each of the frequencies involved, as well as W ,
use Gibbs sampling, where the next state is reached          reflected only the words that had already been as-
by sequentially sampling all variables from their dis-
tribution when conditioned on the current values of          signed to topics.2 This initialization procedure was
all other variables and the data. We will sample only        used because it was hoped that it would start the
the assignments of words to topics, zi . The condi-          chain at a point close to the true posterior distribu-
tional posterior distribution for zi is given by             tion, speeding convergence.
                                 (w )
                                    i          (d )
                                                  i
                                                                Ten runs of the Markov chain were conducted,
         P (zi = j|z−i , w) ∝
                               n−i,j   +β    n−i,j   +α
                                                         (1) each lasting for 2000 iterations. On each iteration
                               (·)            (d )
                              n−i,j + W β n−i,· i
                                                    + Tα     we computed the average number of topics to which
                                                             a word was assigned, ⟨k⟩, which was used to evaluate
 where z−i is the assignment of all zk such that k ̸= i,     the sampling procedure for large scale properties of
         (wi )                                               the representation. Specifically, we were concerned
and n−i,j      is the number of words assigned to topic
                                   (·)
                                                             about convergence and the autocorrelation between
j that are the same as wi , n−i,j is the total number        samples. The rate of convergence was assessed using
of words assigned to topic j, n−i,j
                                        (d )
                                          i
                                              is the number  the Gelman-Rubin statistic R̂, which remained be-
of words from document di assigned to topic j, and           low 1.2 after 25 iterations. The autocorrelation was
  (di )                                                      less than 0.1 after a lag of 50 iterations.
n−i,·   is the total number of words in document di , all
                                                                A single sample was drawn from the first run of the
not counting the assignment of the current word wi .         Markov chain after 2000 iterations. A subset of the
α, β are free parameters that determine how heavily          150 topics found by the model are displayed in Table
these empirical distributions are smoothed.                  1, with words in each column corresponding to one
   The Monte Carlo algorithm is then straightfor-            topic, and ordered by the frequency with which they
ward. The zi are initialized to values between 1 and         were assigned to that topic. The topics displayed are
T , determining the initial state of the Markov chain.       not necessarily the most interpretable found by the
The chain is then run for a number of iterations,            model, having been selected only to highlight the
each time finding a new state by sampling each zi            way in which polysemy is naturally dealt with by
from the distribution specified by Equation 1. Af-           this representation. More than 90 of the 150 topics
ter enough iterations for the chain to approach the          appeared to have coherent interpretations.3
target distribution, the current values of the zi are           The word association data of Nelson et al. (1999)
recorded. Subsequent samples are taken after an ap-          contain a number of asymmetries – cases where peo-
propriate lag, to ensure that their autocorrelation is       ple were more likely to produce one word in response
low. Gibbs sampling is used in each of the following         to the other. Such asymmetries are hard to ac-
simulations in order to explore the consequences of
this probabilistic approach.                                     2
                                                                   Random numbers used in all simulations were gener-
                                                             ated with the Mersenne Twister, which has an extremely
    1                                                        deep period (Matsumoto & Nishimura, 1998).
      A detailed derivation of the conditional probabilities
                                                                 3
used here is given in a technical report available at              The 20 most frequent words in these topics are listed
http://www-psych.stanford.edu/∼gruﬀydd/cogsci02/lda.ps       at http://www-psych.stanford.edu/∼gruﬀydd/cogsci02/topics.txt

   COLD       TREES      COLOR        FIELD           GAME                ART                       BODY                 KING         LAW
 WINTER        TREE       BLUE       CURRENT           PLAY             MUSIC                       BLOOD               GREAT       RIGHTS
 WEATHER     FOREST        RED      ELECTRIC           BALL              PLAY                      HEART                 SON        COURT
   WARM      LEAVES     GREEN      ELECTRICITY         TEAM              PART                      MUSCLE               LORDS        LAWS
 SUMMER      GROUND       LIKE          TWO          PLAYING             SING                       FOOD               QUEEN          ACT
    SUN        PINE      WHITE         FLOW           GAMES              LIKE                       OTHER             EMPEROR        LEGAL
   WIND       GRASS     BROWN          WIRE         FOOTBALL           POETRY                        BONE                OWN         STATE
   SNOW       LONG       BLACK        SWITCH        BASEBALL             BAND                       MADE               PALACE       PERSON
    HOT        LEAF     YELLOW         TURN           FIELD             WORLD                        SKIN                DAY          CASE
 CLIMATE        CUT      LIGHT         BULB          SPORTS            RHYTHM                       TISSUE             PRINCE      DECISION
   YEAR       WALK      BRIGHT       BATTERY         PLAYER              POEM                       MOVE                LADY         CRIME
   RAIN       SHORT       DARK         PATH           COACH              SONG                     STOMACH              CASTLE     IMPORTANT
    DAY        OAK        GRAY          CAN            LIKE          LITERATURE                     PART                ROYAL       JUSTICE
  SPRING       FALL      MADE          LOAD             HIT               SAY                      OXYGEN                MAN       FREEDOM
   LONG      GREEN      LITTLE        LIGHT           TENNIS         CHARACTER                       THIN               MAGIC       ACTION
    FALL       FEET       TURN         RADIO          SPORT           AUDIENCE                     SYSTEM              COURT          OWN
   HEAT        TALL       WIDE         MOVE        BASKETBALL          THEATER                      CHEST              HEART           SET
     ICE      GROW         SUN         LOOP          LEAGUE              OWN                         TINY              GOLDEN       LAWYER
    FEW      WOODS      PURPLE        DEVICE            FUN             KNOWN                       FORM               KNIGHT        YEARS
  GREAT       WOOD        PINK       DIAGRAM            BAT           TRAGEDY                        BEAT              GRACE          FREE
Table 1: Nine topics from the single sample in Simulation 1. Each column shows 20 words from one topic,
ordered by the number of times that word was assigned to the topic. Adjacent columns share at least one
word. Shared words are shown in boldface, providing some clear examples of polysemy
count for in spatial representations because distance      number of meanings of a word follows a power law
is symmetric. The generative structure of our model        distribution. Zipf’s law was established by analyz-
allows us to calculate P (w2 |w1 ), the probability that   ing dictionary entries, but appears to describe the
the next word seen in a novel context will be w2 ,         same property of language.
given that the first word was w1 . Since this is a
conditional probability, it is inherently asymmetric.
The asymmetries in P (w2 |w1 ) predict 77.47% of the        topic          topic          topic
                                                                                                             0
                                                                                                            10             γ = −3.65
asymmetries in the word association norms of Nel-                                                                          <k> = 1.69
son et al. (1999), restricted to the 4544 words used
                                                                                                     P(k)
in the simulation. These results are driven by word
frequency: P (w2 ) should be close to P (w2 |w1 ), and
77.32% of the asymmetries could be predicted by the         word    word   word    word   word
frequency of words in this subset of the TASA cor-                                                           −5
pus. The slight improvement in performance came
                                                                                                            10
                                                                                                                  0        1             2
                                                                                                                 10       10            10
from cases where word frequencies were very similar
                                                                                                                           k
or polysemy made overall frequency a poor indicator        Figure 2: The left panel shows a bipartite semantic
of the frequency of a particular sense of a word.          network. The right shows the degree distribution a
                                                           network constructed from Roget’s Thesaurus.
       Bipartite semantic networks
                                                              Our probabilistic approach specifies a probability
The standard conception of a semantic network is           distribution over the allocation of words to topics. If
a graph with edges between word nodes. Such a              we form a bipartite graph by connecting words to the
graph is unipartite: there is only one type of node,       topics in which they occur, we obtain a probability
and those nodes can be interconnected freely. In           distribution over such graphs. The existence of an
contrast, bipartite graphs consist of nodes of two         edge between a word and a topic indicates that the
types, and only nodes of diﬀerent types can be con-        word has some significant probability of occurring in
nected. We can form a bipartite semantic network           that topic. In the following simulations, we explore
by introducing a second class of nodes that medi-          whether the distribution over bipartite graphs re-
ate the connections between words. One example of          sulting from our approach is consistent with the sta-
such a network is a thesaurus: words are organized         tistical properties of Roget’s Thesaurus and Zipf’s
topically, and a bipartite graph can be formed by          law of meaning. In particular, we examine whether
connecting words to the topics in which they occur,        we obtain structures that have a power law degree
as illustrated in the left panel of Figure 2.              distribution.
   Steyvers and Tenenbaum (submitted) discovered
that bipartite semantic networks constructed by hu-                         Simulation 2:
mans, such as that corresponding to Roget’s (1911)
Thesaurus, share the statistical properties of unipar-              Power law degree distributions
tite semantic networks. In particular, the number of       We used Gibbs sampling to obtain samples from
topics in which a word occurs, or the degree of that       the posterior distribution of the zi for two word-
word in the graph, follows a power law distribution        document co-occurrence matrices: the matrix with
as shown in the right panel of Figure 2. This result       the 4544 words from the word association norms
is reminiscent of Zipf’s (1965) “law of meaning”: the      used in Simulation 1, and a second matrix using

             Random words, 50 topics          Random words, 150 topics   Random words, 250 topics        Initialization     Random documents
                            γ = −2.93                     γ = −2.95                  γ = −2.78                  γ = −1.88            γ = −2.55
                           <k> = 2.11                    <k> = 2.59                 <k> = 2.97                 <k> = 3.43           <k> = 3.68
                 Norm words, 50 topics         Norm words, 150 topics     Norm words, 250 topics     Constant frequencies   Constant documents
        0
       10                   γ = −2.47                     γ = −2.65                  γ = −2.94                  γ = −1.90            γ = −0.70
                           <k> = 3.33                    <k> = 4.24                 <k> = 4.80                 <k> = 2.19           <k> = 2.22
P(k)
        −5
       10
             0              1             2
            10           10              10
                          k
 Figure 3: Degree distributions for networks constructed in Simulations 2 and 3. All are on the same axes.
4544 words drawn at random from those occurring                                         portunity to establish the origin of this distribution,
at least 10 times in the TASA corpus (n = 164401).                                      to see whether it is a consequence of the modeling
Both matrices used the same 5000 random docu-                                           approach or a basic property of language.
ments. For each matrix, 100 samples were taken
with T = 50, 100, 150, 200 and 250. Since the re-                                                         Simulation 3:
sults seemed unaﬀected by the number of topics, we                                                  Origins of the power law
will focus on T = 50, 150, 250. Ten samples were
obtained in each of 10 separate runs with a burn-in                                     To investigate the origins of the power law, we first
of 1000 iterations in which no samples were drawn,                                      established that our initialization procedure was not
and a between-sample lag of 100 iterations.                                             responsible for our results. Using T = 150 and the
   For each sample, a bipartite semantic network was                                    matrix with random words, we obtained 100 samples
constructed by connecting words to the topics to                                        of the degree distribution immediately following ini-
which they were assigned. For each network, the                                         tialization. As can be seen in Figure 3, this produced
degree of each word node was averaged over the 100                                      a curved log-log plot and higher values of γ and ⟨k⟩
samples.4 The resulting distributions were clearly                                      than in Simulation 2.
power-law, as shown in Figure 3. The γ coeﬃcients                                          The remaining analyses employed variants of this
remained within a small range and were all close                                        co-occurrence matrix, and their results are also pre-
to γ = −3.65 for Roget’s Thesaurus. As is to be                                         sented in Figure 3. The first variant kept word fre-
expected, the average degree increased as more top-                                     quency constant, but assigned instances of words to
ics were made available, and was generally higher                                       documents at random, disrupting the co-occurrence
than Roget’s. Semantic networks in which edges are                                      structure. Interestingly, this appeared to have only
added for each assignment tend to be quite densely                                      a weak eﬀect on the results, although the curva-
connected. Sparser networks can be produced by                                          ture of the resulting plot did increase. The second
setting a more conservative threshold for the inclu-                                    variant forced the frequencies of all words to be as
sion of an edge, such as multiple assignments of a                                      close as possible to the median frequency. This was
word to a topic, or exceeding some baseline proba-                                      done by dividing all entries in the matrix by the
bility in the distribution represented by that topic.                                   frequency of that word, multiplying by the median
   Our probabilistic approach produces power law                                        frequency, and rounding to the nearest integer. The
degree distributions, in this case indicating that the                                  total number of instances in the resulting matrix was
number of topics to which a word is assigned follows                                    n = 156891. This manipulation reduced the aver-
a power law. This result is very similar to the prop-                                   age density in the resulting graph considerably, but
erties of Roget’s Thesaurus and Zipf’s observations                                     the distribution still appeared to follow a power law.
about dictionary definitions. This provides an op-                                      The third variant held the number of documents in
                                                                                        which a word participated constant. Word frequen-
   4
     Since power law distributions can be produced by av-                               cies were only weakly aﬀected by this manipulation,
eraging exponentials, we also inspected individual sam-                                 which spread the instances of each word uniformly
ples to confirm that they had the same characteristics.                                 over the top five documents in which it occurred

and then rounded up to the nearest integer, giving       by their probabilities of arising in diﬀerent contexts.
n = 174615. Five was the median number of docu-          We can easily compute important statistical quan-
ments in which words occurred, and documents were        tities from this representation, such as P (w2 |w1 ),
chosen at random for words below the median. This        the probability of w2 arising in a particular context
manipulation had a strong eﬀect on the degree dis-       given that w1 was observed, and more complicated
tribution, which was no longer power law, or even        conditional probabilities. One advantage of an ex-
monotonically decreasing.                                plicitly probabilistic representation is that we gain
   The distribution of the number of topics in which     the opportunity to incorporate this representation
a word participates was strongly aﬀected by the dis-     into other probabilistic models. In particular, we
tribution of the number of documents in which a          see great potential for using this kind of represen-
word occurs. Examination of the latter distribution      tation in understanding the broader phenomena of
in the TASA corpus revealed that it follows a power      human memory.
law. Our approach produces a power law degree dis-       Acknowledgments The authors were supported by a
tribution because it accurately captures the natural     Hackett Studentship and a grant from NTT Communi-
statistics of these data, even as it constructs a lower- cations Sciences laboratory. We thank Tania Lombrozo,
dimensional representation.                              Penny Smith and Josh Tenenbaum for comments, and
                                                         Tom Landauer and Darrell Laham for the TASA corpus.
                                                         Shawn Cokus wrote the Mersenne Twister code.
               General Discussion
We have taken a probabilistic approach to the prob-
lem of semantic representation, motivated by con-                             References
sidering the function of associative semantic mem-       Anderson, J. R. (1990). The Adaptive Character of
ory. We assume a generative model where the words        Thought. Erlbaum, Hillsdale, NJ.
that occur in each context are chosen from a small          Barabasi, A. L. & Albert, R. (1999). Emergence of
number of topics. This approach produces a lower-        scaling in random networks. Science, 286, 509-512.
                                                            Blei, D. M., Ng, A. Y., & Jordan, M. I. (2002). Latent
dimensional representation of a word-document co-        Dirichlet allocation. In Advances in Neural Information
occurrence matrix, and explicitly models the fre-        Processing Systems 14.
quencies in that matrix as probability distributions.       Collins, A. M. & Loftus, E. F. (1975). A spreading
Simulation 1 showed that our approach could ex-          activation theory of semantic processing. Psychological
                                                         Review, 82, 407-428.
tract coherent topics, and naturally deal with issues       Collins, A. M. & Quillian, M. R. (1969). Retrieval
like polysemy and asymmetries that are hard to ac-       time from semantic memory. Journal of Verbal Learning
count for in spatial representations. In Simulation 2,   & Verbal Behavior, 8, 240-248.
we showed that this probabilistic approach was also         Gilks, W., Richardson, S., & Spiegelhalter, D. J.
capable of producing representations with a large-       (1996). Markov Chain Monte Carlo in Practice. Chap-
                                                         man & Hall, Suﬀolk.
scale structure consistent with semantic networks           Hofmann, T. (1999). Probablistic latent semantic in-
constructed from human data. In particular, the          dexing. In Proceedings of the Twenty-Second Annual In-
number of topics to which a word was assigned fol-       ternational SIGIR Conference.
lowed a power law distribution, as in Roget’s (1911)        Landauer, T. K. & Dumais, S. T. (1997). A solu-
                                                         tion to Plato’s problem: The Latent Semantic Analysis
Thesaurus and Zipf’s (1965) law of meaning. In Sim-      theory of acquisition, induction, and representation of
ulation 3, we discovered that the only manipulation      knowledge. Psychological Review, 104, 211-240.
that would remove the power law was altering the            Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
number of documents in which words participate,          Introduction to latent semantic analysis. Discourse Pro-
which follows a power law distribution itself.           cesses, 25, 259-284.
                                                            Matsumoto, M. & Nishimura, T. (1998). Mersenne
   Steyvers and Tenenbaum (submitted) suggested          twister: A 623-dimensionally equidistributed uniform
that power law distributions in language might be        pseudorandom number generator. ACM Transactions on
traced to some kind of growth process. Our results       Modeling & Computer Simulation, 8, 3-30.
indicate that this growth process need not be a part        Nelson, D. L., McEvoy, C. L., & Schreiber, T. A.
                                                         (1999). The University of South Florida word associ-
of the learning algorithm, if the algorithm is faith-    ation norms. http://www.usf.edu/FreeAssociation.
ful to the statistics of the data. While we were able       Roget, P. (1911). Roget’s Thesaurus of English Words
to establish the origins of the power law distribu-      and Phrases. Available from Project Gutenberg.
tion in our model, the growth processes described by        Shepard, R. N. (1957). Stimulus and response gen-
Steyvers and Tenenbaum might contribute to under-        eralization: a stochastic model, relating generalization
                                                         to distance in psychological space. Psychometrika, 22,
standing the origins of the power law distribution in    325-345.
dictionary meanings, thesaurus topics, and the num-         Steyvers, M. & Tenenbaum, J. B. (submitted). The
ber of documents in which words participate.             large-scale structure of semantic networks: Statistical
   The representation learned by our probabilistic       analyses and a model of semantic growth.
                                                            Tversky, A. & Hutchinson, J. W. (1986). Nearest
approach is not explicitly a representation of words,    neighbor analysis of psychological spaces. Psychological
in which each word might be described by some set of     Review, 93, 3-22.
features. Instead, it is a representation of the prob-      Zipf, G. K. (1965). Human behavior and the principle
abilistic relationships between words, as expressed      of least eﬀort. Hafner, New York.

