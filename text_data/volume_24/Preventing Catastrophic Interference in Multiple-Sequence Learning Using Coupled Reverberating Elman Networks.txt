UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Preventing Catastrophic Interference in Multiple-Sequence Learning Using Coupled
Reverberating Elman Networks
Permalink
https://escholarship.org/uc/item/92f796wd
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Ans, Bernard
Rousset, Stéphane
French, Robert M
et al.
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

 Preventing Catastrophic Interference in Multiple-Sequence Learning Using
                              Coupled Reverberating Elman Networks
                 Bernard Ans*, Stéphane Rousset*, Robert M. French † & Serban Musca*
                                           *Experimental Psychology Laboratory
                          Université Pierre Mendès-France, Grenoble 2 – CNRS UMR 5105
                                         BP 47, 38040 Grenoble cedex 09, France
                                           email: Bernard.Ans@upmf-grenoble.fr
                                    † Quantitative Psychology and Cognitive Science
                                        Université de Liège (Bât B32), Sart Tilman
                                                      4000 Liège, Belgique
                                                    email: rfrench@ulg.ac.be
                         Abstract                                             Catastrophic interference
Everyone agrees that real cognition requires much more            The problem of catastrophic interference (or
than static pattern recognition. In particular, it requires the   forgetting) has been with the connectionist
ability to learn sequences of patterns (or actions) But
                                                                  community for well over a decade now (McCloskey
learning sequences really means being able to learn multiple
sequences, one after the other, wi thout the most recently
                                                                  & Cohen, 1989; Ratcliff, 1990; for a review see
learned ones erasing the previously learned ones. But if          Sharkey & Sharkey, 1995). Catastrophic forgetting
catastrophic interference is a problem for the sequential         occurs when newly learned information suddenly and
learning of individual patterns, the problem is amplified         completely erases information that was previously
many times over when multiple sequences of patterns have          learned by the network, a phenomenon that is not only
to be learned consecutively, because each new sequence            implausible cognitively, but disastrous for most
consists of many linked patterns. In this paper we will           practical applications. The problem has been studied
present a connectionist architecture that would seem to           by numerous authors over the past decade (see
solve the problem of multiple sequence learning using
pseudopatterns.
                                                                  French, 1999 for a review). The problem is that the
                                                                  very property – a single set of weights to encode
                                                                  information – that gives connectionist networks their
                      Introduction                                remarkable abilities of generalization and graceful
Building a robot that could unfailingly recognize and             degradation in the presence of incomplete information
respond to hundreds of objects in the world – apples,             are also the root cause of catastrophic interference
mice, telephones and paper napkins, among them –                  (see, for example, French, 1992).
would unquestionably constitute a major artificial                    Various authors (Ans & Rousset, 1997, 2000;
intelligence tour de force. But everyone agrees that              French, 1997; Robins, 1995) have developed systems
real cognition requires much more than static pattern             that rehearse on pseudo-episodes (or pseudopatterns),
recognition. In particular, it requires the ability to            rather than on the real items that were previously
learn sequences of patterns (or actions). This was the            learned. The basic principle of this mechanism is
primary reason for the development of the simple                  when learning new external patterns to interleave
recurrent network (SRN, Elman, 1990) and the many                 them with internally-generated pseudopatterns. These
variants of this architecture.                                    latter patterns, self-generated by the network from
    But learning sequences means more than being                  random activation, reflect (but are not identical to) the
able to learn a single, isolated sequence of patterns: it         previously learned information. It has now been
means being able to learn multiple sequences, one                 established that this pseudopattern rehearsal method
after the other, without the most recently learned ones           effectively eliminates catastrophic forgetting.
erasing the previously learned ones. But if                           A serious problem remains, however, and that is
catastrophic interference – the phenomenon whereby                this: cognition involves more than being able to
new learning completely erases old learning – is a                sequentially learn a series of "static" (non-temporal)
problem with static pattern learning (McCloskey &                 patterns without interference. It is of equal importance
Cohen, 1989; Ratcliff, 1990), the problem is                      to be able to serially learn many of temporal
amplified many times over when multiple sequences                 sequences of patterns. We will propose an
of patterns have to be learned consecutively, because             pseudopattern-based architecture that can effectively
each sequence consists of many new linked patterns.               learn multiple temporal pat terns consecutively.
What hope is there for a previously learned sequence
of patterns to survive after the network has learned a            The key insight of this paper is this:
new sequence consisting of many individual patterns?                  Once an SRN has learned a particular sequence,
    In this paper, we will present a connectionist                    each pseudopattern generated by that network
architecture that solves the problem of multiple                      reflects the entire sequence (or set of sequences)
sequence learning.                                                    that has been learned.

From which our key result follows:
    When learning a new sequence, simple rehearsal                  Dual-Network Architectures
    with these sequence-encoding pseudopatterns will     But where are these internally generated
    prevent catastrophic forgetting of the previously    pseudopatterns stored so that they can be interleaved
    learned sequence(s).                                 with the new patterns? One answer is to generate
                                                         them on the fly in a separate network (French, 1997;
We will use a connectionist architecture using two       Ans & Rousset, 1997, 2000). Let us consider one way
coupled “auto-associative         recurrent networks     that a single new pattern, Q, might be learned in this
(AARN)” (Maskara & Noetzel, 1992, 1993;                  dual-networ k model. Assume that new patterns are
Cleeremans & Destrebecqz, 1997) that pass                learned by NET 1 (this can be considered to be the
information back and forth to each other by means of     “Performance Network”), while previously learned
pseudopatterns. We will refer to auto-associative        patterns are stored in NET 2 (which could be
recurrent networks as Reverberating SRNs (RSRN),         considered to be the “Storage Network”). NET 1
in order to emphasize the manner in which they use       learns Q as follows:
pseudopatterns to eliminate catastrophic interference       i) pattern Q is input to NET 1, which modifies its
in multiple sequence learning.                              weights once; ii) noise is input to NET 2, which
    The remainder of this paper is organized as             generates a pseudopattern; iii) this pseudopattern is
follows. We will briefly review the standard dual-          presented to NET 1, which modifies its weights
network pseudopattern solution to the problem of            once; iv) if Q has been learned to criterion, stop;
catastrophic forgetting in static pattern learning. We      otherwise go to i).
will then show (Simulation 1) that multiple-sequence     We call this the “awake state”. Once the output error
learning is particularly susceptible to catastrophic     for pattern Q has dropped below criterion, we transfer
forgetting. We will then show how our                    the information in NET1 to NET 2 as follows:
pseudopatterns-based dual-network architecture can          Loop N times: i) noise is input to NET 1, which
be used to effectively overcome catastrophic                   generates a pseudopattern; ii) this pseudopattern is
interference in multiple sequence learning.                    input to NET 2, which modifies its weights once.
                                                         We call this phase, when information is transferred to
   Overcoming catastrophic interference                  NET 2, the “sleep state.” This will be the basis of all
                with pseudopatterns                      learning in the dual-network framework described in
Before discussing catastrophic interference in           this paper.
multiple-sequence learning, we need to briefly
describe what pseudopatterns are and how they can be            “Reverberating” backpropagation
used to reduce catastrophic interference in the simpler  A reverberating backpropagation (RBP) network is a
case of static pattern learning.                         standard three-layer network that has a built -in
    Assume we have a three-layer feedforward             autoassociator (“reverberator”) for the input of the
network that learns a number of binary patterns drawn    patterns to be learned (Ans & Rousset, 1997, 2000).
from some distribution. Assume, thereafter, that these   We have shown this network in an “unfolded” manner
patterns are no longer available. How can one            (Figure 1). In this visualization, the output layer is
determine, even approximately, what the network has      divided into the “autoassociative” nodes for the input
learned? Answer: By "bombarding" the input of the        component of the patterns to be learned (on the left in
network with random binary vectors and collecting        Figure 1) and the “target” nodes for the targets of the
the associated output vectors. Each input-output pair    patterns to be learned (on the right in Figure 1).
of vectors produced in this way will constitute a
pseudopattern that will be a reflection of the function                “autoassociative” nodes   “target” nodes
previously learned by the network.                                                                 Target
                                                           Teacher               Input
    The basic idea to use pseudopatterns to reduce
                                                                                                                Error
catastrophic interference is due to Robins (1995). It      Output layer
works as follows. The network learns a first set of
patterns, {Pi }. Then before it begins to learn a second
set of patterns, {Qi}, noise is fed through the network    Hidden layer
to produce a set of pseudopatterns, {ψ i}, as above.
These pseudopatterns are added to the new patterns to
be learned and the network trains on this larger set       Input layer                     Input
until all of the new patterns, {Q i}, are learned to     Figure 1. An RBP network
criterion. When the network is tested on the originally
learned patterns, {Pi }, it has not forgotten them            Assume the network is to learn a pattern P: I ? T,
catastrop hically. Had there been no pseudopatterns      consisting of an input, I, and a target, T. I is presented
mixed in with the new patterns that were learned, the    on input and is sent through the network to the output
network would have completely forgotten the              layer. For all of the nodes in the output layer, an error
originally learned patterns.                             is calculated. For the “autoassociative” output nodes,
                                                                                                                      2

the error is based on the difference between the              layer associated with the previous item in the
network output and the original input, I, whereas for         sequence, S(t -1).
the “target” nodes, this error is based on the                  “Attractor” pseudopatterns in an RSRN
difference between the “heteroassociative” output and         The principle is identical to pseudopattern generation
the desired target T. The errors associated with both         in an RBP network. Crucially, once the RSRN has
the autoassociator and the target output nodes are then       learned a sequence of items, each “reverberated”
backpropagated through the network in order to                pseudopattern generated by it reflects the entire
change the network’s weights.                                 sequence learned by the recurrent network. Each
                                                              pseudopattern can be thought of as a very compact
          “Attractor” pseudopatterns                          representation of the entire previously learned
                in an RBP Network                             sequence.
To generate pseudopatterns in a reverberating
network, a random input iψ is present ed to the input                            “autoassociative”
                                                                                                         “target”
                                                                                  (Input) nodes
layer of the network and fed through to the output
                                                                Teacher           S(t)        H(t-1)     S(t+1)
layer. The activation values of the “autoassociative”                                                             Error
nodes in the output layer (nodes on the left of the             Output
output layer in Fig. 1) constitute a new input,         iψ′ ,
which is then sent through the network (the activation          Hidden layer                   H(t)
values on the “target” nodes in the output layer are
ignored). This produces a pattern of activation on the          Input                     S(t)       H(t-1)
autoassociative output nodes, iψ′′ , which is then                                      Standard     Context
presented to the input nodes of the network, and so                                            Input
on. After a number of reverberating cycles through
                                                              Figure 2. A Reverberating SRN (RSRN)
the network, a final “reverberated” input,    iψR , is sent
through the network and the activation vector of all             A dual-network architecture with self-
the output nodes (the “autoassociative” and the                       refreshing memory to overcome
“target” output nodes), oψ , is used to produce a                   catastrophic forgetting in multiple
pseudopattern      ψ : iψR → oψ .     The      significant                     sequence learning
                                                                   Ans & Rousset (1997, 2000) proposed a
advantages of using an input autoassociator with a            reverberating dual-network architecture with a self -
feedforward backpropagation network have been                 refreshing memory to avoid catastrophic forgetting in
shown elsewhere (Ans & Rousset, 1997, 2000).                  static pattern learning. The basic dual-network
Suffice it to say that the reverberation process              architecture consists of two coupled RBP networks,
transforms a pure random input,    iψ , into an attractor     denoted NET 1 and NET 2. NET 1 is the primary
                    R
                                                              network that interfaces with the environment and that
of the system, i . An “attractor” pseudopattern
                    ψ                                         learns the new patterns. The secondary network,
produced provides a much better reflection of the old         NET 2, is a “storage” network because information
patterns than a pseudopattern produced from simple            initially learned in NET 1 will ultimately be
random noise on input. It is this reverberation               transferred to NET 2.
technique that is largely responsible for the power of             The basic principles of RSRN dual-network
this technique.                                               learning, where each of the networks is an RSRN, are
                                                              virtually identical to those underlying dual-networks
       Reverberating Simple Recurrent                         composed of RBP networks. Dual-network RSRNs
                                                              are designed to learn multiple sequences of patterns.
                 Networks (RSRN)                                   Assume we have two identical RSRNs, NET 1 and
We will assume that the reader is familiar with the
                                                              NET 2. New sequences will be learned only by
design of a Simple Recurrent Network (SRN, Elman,             NET 1, while NET 2 will store the previously learned
1990). An RSRN (Figure 2) works very much like a              information. The learning procedure is similar to that
standard SRN (Maskara & Noetzel, 1992, 1993;
                                                              of the basic RBP dual-network. A sequence, S = S(0),
Cleeremans & Destrebecqz, 1997). Just as the RBP              S(1), ..., S(n) is presented to NET 1. The network
network involves adding “autoassociative” nodes to            makes a single pass through the entire sequence,
the output layer of a BP network, a reverberating SRN
                                                              updating its weights once for each item in the
involves adding “autoassociative” nodes to the output
                                                              sequence. This defines one learning "epoch"
layer of an SRN. The full input to the network
                                                              corresponding to one presentation of all items in the
consists of the “standard” input, i.e., the input from
                                                              sequence in order. Next, NET 2 generates a number of
the sequence item, S(t), and the “context” input,
                                                              pseudopatterns (e.g., 10 per learning epoch). These
H(t -1), which is the activation vector of the hidden
                                                              pseudopatterns are close to attractor states of NET 2,
                                                                                                                      3

which makes them particularly good vehicles for          pseudopattern that will be used is created. Ten
information transfer from NET 2 to NET 1. For each       pseudopatterns from NET 2 are interleaved with each
NET 2 pseudopattern, NET 1 performs one                  epoch of the sequence learned by NET 1. During
feedforward-backpropagation learning pass. Once this     transfer of the information from NET 1 to NET 2, 104
is completed, a new learning epoch starts and NET 1      pseudopatterns are used.
makes another pass through the sequence S. NET 2
generates new pseudopatterns, each of which is                 Measuring learning and forgetting
learned for one feedforward-backpropagation pass by      For each item, S(t), of the sequence fed forward
NET 1. And so on. This is the awake state for an         through the network, we calculate the difference
RSRN dual-network.                                       between the activation values of “target” units in the
    It is extremely important to notice that each        output layer and the desired target item in the
pseudopattern generated by NET 2 is a static input-      sequence, S(t+1). We calculate this difference for
output pattern that represents a dynamic state (i.e.,    each of the 100 “target” output units and count the
the previously learned sequence or sequences). This is   number of output units for which the absolute value of
what gives this system its power: there is no need to    this difference is above the learning criterion of 0.1.
attempt to reproduce an entire pseudo-sequence that      So, for example, assume a given item, S(t), in the
will then be interleaved with the new sequence being     sequence is sent through the network. If, on the output
learned. Rather, we only need to interleave with the     layer, 14 of the “target” output units differ from the
new sequence to be learned non -temporal                 corresponding units of S(t+1) by more than 0.1, we
pseudopatterns, each of which reflects (at least         say that there are 14 “incorrect” units. A sequence is
partially) the information in the entire previously      considered to have been learned if, for each of its
learned temporal sequence (or sequences).                elements, S(t), the network produces a vector of
    To transfer the sequence newly learned by NET 1      “target” output values, each of which is within 0.1 of
to NET 2, we again make use of pseudopatterns. This      the corresponding element of S(t+1). The overall
time the pseudopatterns are generated by NET 1 and       measure of how well the network has learned (or
learned by NET 2. For each pseudopattern generated       forgotten) a sequence after a given number of learning
by NET 1, NET 2 performs a single feedforward-           epochs will be the total number of incorrect units over
backpropagation learning pass.                           all items of the sequence.
            Overview of Simulations                        Simulation 1: Catastrophic forgetting in
We will present two simulations. The first will                     multiple sequence learning
demonstrate the severity of catastrophic interference    To illustrate the severity of catastrophic forgetting in
in multiple sequence learning in standard SRNs. The      multiple sequence learning, we will consider two
second     will     demonstrate     that    interleaving sequences, A and B, and have an SRN attempt to learn
pseudopatterns (reflecting the whole previously
                                                         them sequentially. The sequences are constructed as
learned sequence) with the new sequence effectively      follows. Twenty-two distinct random binary vectors
eliminates catastrophic interference.                    of length 100 are created. Half of these vectors are
    A standard SRN netw ork was used for the first
                                                         used to produce the first ordered sequence of items, A,
simulation demonstrating the severity of catastrophic    denoted by A(0), A(1), …, A(10). The remaining 11
interference in multiple sequence learning. In the       vectors are used to create a second sequence of items,
second simulation, a dual-network architecture
                                                         B, denoted by B(0), B(1), …, B(10). In order to
consisting of two coupled RSRNs will be used. Each       introduce a degree of ambiguity into each sequence
RSRN has an input layer with 100 “standard” input        (so that a simple BP network would not be able to
units (corresponding to the size of the items, S(t), in
                                                         learn them), we modify each sequence so that A(5) =
the sequence) and 50 “context” units. The hidden         A(8) and B(1) = B(5). First, sequence A is completely
layer consists of 50 units. The output layer consists of learned by the network. Then sequence B is learned
150 “autoassociative” inputs that are identical to the
                                                         and, during the course of learning, we monitor at
input layer plus 100 “target” units (Figure 2).          regular intervals how much of sequence A has been
    Learning a given sequence consists of presenting     forgotten by the network.
it repeatedly to the network until each item in the           Fig. 3a shows the progression of the network’s
sequence can predict the subsequent item with a pre-     learning of sequence B. The number of incorrect units
defined degree of precision. The network weights are     for each serial position of the sequence, as defined
updated by backpropagation once per presentation of      above, is shown. As expected, it is harder for the
each sequence item. A cross-entropy error function is    network to learn B(2) and B(6) since their immediate
used (Hinton, 1989; Plaut, McClelland, Seidenberg &      predecessors are identical and, in order to distinguish
Patterson, 1996) with a learning rate of 0.01, a         them, the network needs to additionally take into
momentum of 0.5 and a 1.0 bias term. All weights are     consideration the context of the preceding items in the
randomly init ialized between –0.5 and 0.5.              sequence. After 450 epochs, the network has
    To create the “attractor” pseudopatterns, noise on
                                                         completely learned the entire sequence.
input is “reverberated” 5 times before the actual             Then, during learning of sequence B, we
                                                                                                                4

                                                                                                                                                                (a)
                                         (a)                                                                                                    100
                           100                                                                                                                     90
                              90
                                                                                                                                                     80
       Recal l of Seque nce B                                                                                                 Recal l of Seque nce B
                               80
                                                                                                                                                     70
                               70
                                                                                                                                                     60
                                                                                                                            Incorr ect outpu t units (%)
                                    60
                                                                                                                                                      50
     Incorr ect outpu t units (%)
                                    50
                                                                                                                                                      40
                                    40
                                                                                                                                                      30
                                    30
                                                                                                                                                           20
                                    20
                                                                                                                                                           10
                                    10
                                                                                                                                                            0
                                     0
                                                                                                                                                                 1
                                           1                                                                                                                          2                                                                0
                                               2                                                                0                                                         3
                                                   3
                                                       4                                                 100                                                      Serial        4                                                100
                                           Seria                                                                                                                         positio 5 6                                                    chs
                                                l pos                                                         chs
                                                            5
                                                                                                                                                                                n of S                                               epo
                                                                                                                                                                                                                           200
                                                     ition                                                 epo
                                                                  6                                200
                                                           of
                                                                                                        ning
                                                                                                                                                                                      equen 7 8                                 ning
                                                                Sequ 7 8                                                                                                                   ce B                             lear
                                                                                                                                                                                                                     300
                                                                                                    lear ce B
                                                                                             300
                                                                    ence                                                                                                                        items 9                             e B
                                                                         B ite 9                                                                                                                               400       of       nc
                                                                                                  f       n
                                                                                                                                                                                                                      ber eque
                                                                                                                                                                                                          10
                                                                              ms   10   450
                                                                                              er o Seque
                                                                                            mb
                                                                                          Nu for                                                                                                                   Num for S
                                         (b)                                                                                                                (b)
                           100                                                                                                                  100
                              90                                                                                                                   90
                               80
       Recal l of Seque nce A
                                                                                                                                                     80
                                                                                                                              Recal l of Seque nce A
                               70
                                                                                                                                                     70
                                    60
                                                                                                                                                     60
     Incorr ect outpu t units (%)                                                                                           Incorr ect outpu t units (%)
                                    50
                                                                                                                                                      50
                                    40
                                                                                                                                                      40
                                    30
                                                                                                                                                      30
                                    20
                                                                                                                                                           20
                                    10
                                                                                                                                                           10
                                     0
                                                                                                                                                            0
                                           1
                                               2                                                               450
                                                   3                                                                                                             1
                                                        4                                                300                                                          2                                                                0
                                           Seria                                                                                                                          3
                                                l pos       5
                                                                                                                                                                  Seria        4                                                 100
                                                     ition        6                                200
                                                                                                                                                                       l posit     5
                                                           of                                                                                                                                                                          chs
                                                                      7
                                                                Sequ                         100              chs                                                             ion of 6                                     200
                                                                                                                                                                                                                                    epo
                                                                    ence 8 9                               epo                                                                       Sequ 7
                                                                                                                                                                                         ence 8                                ning
                                                                        A ite
                                                                             ms                        ning                                                                                  A item 9                300
                                                                                                                                                                                                                           lear
                                                                                                   lear ce B
                                                                                   10   0
                                                                                                                                                                                                   s                               e B
                                                                                                 f
                                                                                             er o Seque
                                                                                                         n                                                                                                10   400      of      enc
                                                                                            b                                                                                                                        ber equ
                                                                                         Num for                                                                                                                  Num for S
Figure 3. Catastrophic forgetting in an SRN during                                                                     Figure 4. Recall performance for sequences B and A
multiple sequence learning. (a): Learning of                                                                           during learning of sequence B by a dual-network
sequence B (after having previously learned                                                                            RSRN. (a): By 400 epochs, the second sequence B
sequence A). By 450 epochs (an epoch corresponds                                                                       has been completely learned. (b): The previously
to one pass through the entire sequence), sequence                                                                     learned sequence A shows virtually no forgetting.
B has been completely learned. (b): The number of                                                                      Catastrophic forgetting of the previously learned
incorrect units for sequence A during learning of                                                                      sequence A has been completely overcome.
sequence B. After 450 epochs, the SRN has, for all
intents and purposes, completely forgotten the
previously learned sequence A.
monitored the forgetting of the previously learned                                                                   initialized to random weight settings between –0.5
sequence, A (Fig. 3b). Initially (i.e., before the                                                                   and 0.5. NET 1 then completely learns sequence A
network began learning sequence B), it can be seen                                                                   and then generates 104 pseudopatterns in order to
that sequence A was completely learned. But very                                                                     transfer this learning to NET 2. (There are 2150
early on, as sequence B is being learned, the                                                                        possible distinct states for the input layer of each
network’s memory of sequence A is overwritten. By 5                                                                  network, and hence, there is a very little possibility
epochs after beginning to learn the sequence B (not                                                                  that the random binary vectors used to produce the
shown in Fig. 3b), the network gets an average of                                                                    “attractor pseudo-input” would be actual input
40% of the units of the items of sequence A wrong.                                                                   patterns already seen by the network.)
By 250 epochs, the network’s performance on                                                                              Now, NET 1 begins to learn sequence B. After
sequence A is essentially no better than chance and,                                                                 each learning epoch (consisting of the entire sequence
by 450 epochs, sequence A is completely forgotten. In                                                                of items in B), NET 1 receives 10 pseudopatterns
short, learning sequence B causes severe catastrophic                                                                from NET 2         and    does    one    feedforward-
forgetting of sequence A.                                                                                            backpropagation pass for each of them. (The number
                                                                                                                     of pseudopatterns is not related to the length of the
 Simulation 2: Catastrophic forgetting is                                                                            previously learned sequences and can be varied.)
    overcome with pseudopatterns                                                                                         Fig. 4a shows that the NET 1 does, in fact, learn
An RSRN dual-network architecture was used with                                                                      sequence B completely by 400 epochs. In other
the parameters indicated above. Both networks are                                                                    words, for all items in sequence B, all of the units in
                                                                                                                     the network output are within 0.1 of the desired target

output. Notice that the sequence items B(2) and B(6)         Ans, B. & Rousset, S. (2000) Neural networks with a self-
are learned more slowly by the network. This was, of            refreshing memory: knowledge transfer in sequential
course, expected since these two items are preceded             learning tasks without catastrophic forgetting.
                                                                Connection Science, 12, 1–19.
by identical items, hence creating ambiguity having to       Cleeremans, A. & Destrebecqz, A. (1997). Incremental
be solved by the temporal context. During learning of           Sequence learning. In Proceedings of the Nineteenth
sequence B, we tested the performance of the network            Annual Meeting of the Cognitive Science Society, NJ:
on all of the items of sequence A. Fig. 4b shows that           LEA, 119-124
there is virtually no forgetting of sequence A as the        Elman, J.L. (1990) Finding structure in time. Cognitive
network learns sequence B. In short, catastrophic               Science, 14, 179–211.
forgetting has been completely overcome by the               French, R.M. (1992) Semi-distributed representations and
coupled system of RSRNs using pseudopattern                     catastrophic forgetting in connectionist networks.
information transfer.                                           Connection Science, 4, 365–377.
                                                             French, R.M. (1997) Pseudo-recurrent connectionist
                                                                networks: An approach to the 'sensitivity-stability'
               Concluding remarks                               dilemma. Connection Science, 9, 353–379.
We have shown that the reverberating dual-network            French, R.M. (1999) Catastrophic forgetting in
architecture, originally proposed earlier to overcome           connectionist networks. Trends in Cognitive Sciences, 3,
catastrophic forgetting in non -temporal pattern                128–135.
learning (Ans & Rousset, 1997, 2000) can be                  Hinton, G.E. (1989) Connectionist learning procedures.
generalized to sequential learning of multiple                  Artificial Intelligence, 40, 185–234.
temporal sequences. The basic principle of                   Maskara, A., & Noetzel, A. (1992). Forced simple recurrent
                                                                neural network and grammatical inference. In
interleaving internally-generated pseudopatterns from           Proceedings of th e Fourteenth Annual Conference of the
a long-term storage network with patterns from the              Cognitive Science Society, 420-425, NJ: LEA.
environment being learned by a second network has            Maskara, A. & Noetzel, A. (1993). Sequence learning with
been developed elsewhere (Ans & Rousset, 1997,                  recurrent neural networks. Connection Science, 5, 139-
2000; French, 1997; Robins, 1995).                              152.
    When learning multiple sequences of patterns, it         McClelland, J.L., McNaughton, B.L. & O'Reilly, R.C.
turns out that interleaving simple input -output                (1995) Why there are complementary learning systems in
pseudopatterns, each of which reflect the entire                the hippocampus and neocortex: Insights from the
                                                                successes and failures of connectionist models of
previously learned sequence(s), reduces (or eliminates
                                                                learning and memory. Psychological Review, 102, 419–
entirely) forgetting of the initially learned                   457
sequence(s).                                                 McCloskey, M. & Cohen, N.J. (1989) Catastrophic
    Further, we demonstrate the power of a network              interference in connectionist networks: The sequential
architecture that allows us to produce “reverberated”           learning problem. In G.H. Bower (Ed.), The Psychology
pseudopatterns that are, in reality, attractors of the          of Learning and Motivation, Vol. 24. New York:
entire network and therefore reflect, in a highly               Academic Press, pp. 109–165.
compressed manner, the previously learned                    Plaut, D.C., McClelland, J.L., Seidenberg, M.S. &
sequences.                                                      Patterson, K. (1996) Understanding normal and impaired
                                                                word reading: Computational principles in quasi-regular
    We have demonstrated a technique that effectively
                                                                domains. Psychological Review, 103, 56–115.
allows multiple sequences to be learned                      Ratcliff, R. (1990) Connectionist models of recognition
consecutively. Of course, these networks can be                 memory: Constraints imposed by learning and forgetting
made to forget gradually. This gradual forgetting               functions. Psychological Review, 97, 285–308.
depends on the size of the learning network and on           Robins, A.V. (1995) Catastrophic forgetting, rehearsal and
the number, the overlap, the length and the                     pseudorehearsal. Connection Science, 7, 123–146.
complexity of the successively learned sequences.            Sharkey, N.E. & Sharkey, A.J.C. (1995) An analysis of
But the problem of sudden, “catastrophic” forgetting            catastrophic interference. Connection Science, 7, 301–
of previously learned sequences caused by learning a            329.
new sequence of patterns would seem to have been
overcome.
                 Acknowledgments
This research was supported in part by a research
grant from the European Commission (HPRN-CT -
1999-00065) and by the French government (CNRS
UMR 5105).
                     References
Ans, B. & Rousset, S. (1997) Avoiding catastrophic
   forgetting by coupling two reverberating neural
   networks. C.R. Acad. Sci. Paris, Life Sciences, 320, 989–
   997.
                                                                                                                      6

