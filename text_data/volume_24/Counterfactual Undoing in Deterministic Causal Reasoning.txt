UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Counterfactual Undoing in Deterministic Causal Reasoning
Permalink
https://escholarship.org/uc/item/0wx8s5js
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Sloman, Steven A
Lagnado, David A
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Counterfactual Undoing in Deterministic Causal Reasoning
                                         Steven A. Sloman (Steven_Sloman@brown.edu)
                                        Department of Cognitive & Linguistic Sciences, Box 1978
                                              Brown University, Providence, RI 02912 USA
                                        David A. Lagnado (David_Lagnado@Brown.Edu)
                                        Department of Cognitive and Linguistic Sciences, Box 1978
                                              Brown University, Providence, RI 02912 USA
                               Abstract                               intervention. Some of those questions can be answered
                                                                      through mental intervention, by imagining a counterfactual
   Pearl (2000) offers a formal framework for modeling causal         situation in which a variable is manipulated and determining
   and counterfactual reasoning. By virtue of the way it              the effects of change. People attempt this, for example,
   represents intervention on a causal system, the framework          whenever they wonder "if only..." (if only I hadn’t made that
   makes predictions about how people reason when asked
   counterfactual questions about causal relations. Four studies
                                                                      stupid comment... If only my data were different...).
   are reported that test the application of the framework to             Pearl (2000) offers a causal modeling framework that
   deterministic causal and conditional arguments. The results        covers such counterfactual reasoning. The framework makes
   support the proposed representation of causal arguments,           predictions about how people reason when asked
   especially when the nature of the counterfactual intervention      counterfactual questions about causal relations. Pearl’s
   is made explicit. The results also show that conditional           analysis extends to relations of probabilistic causality but
   relations are construed in different ways.                         this paper is limited to studies of deterministic arguments.
                                                                      Before describing those studies, we briefly review the
                          Introduction                                relevant aspects of Pearl’s analysis.
Many questions are decided by causal analysis. In the law,
issues of negligence concern who caused an outcome and, at                          Observation vs. Causation
least under common law, the determination of guilt requires                               (Seeing vs. Doing)
evidence of a causal chain leading to a crime. Evidence that
might increase the probability of guilt (e.g., an accused’s           Seeing
race) is impermissible if it doesn’t support a causal analysis
of the crime. Some legal scholars (Lipton, 1992) claim that           In general, observation can be represented using the tools of
legal analyses of causality are in no sense special, that             conventional probability. The probability of observing an
causation in the law derives from everyday thinking about             event (say, that a logic gate is working properly) under some
causality. Causal analysis is just as prevalent in science,           circumstance (e.g., the temperature is low) can be
engineering, politics, indeed in every domain that involves           represented as the conditional probability that a random
human prediction and control.                                         variable G, representing the logic gate, is at some level of
   Causal analysis is often difficult because it depends not          operation g when temperature T is observed to take some
only on what happened, but also on what might have                    value t:
happened (Mackie, 1974). Thus the claim that A caused B
will often imply that if A had not occurred, then B would                                                 Pr{G = g & T = t}
                                                                              Pr{G = g|T = t} defined as                    .
not have occurred. Likewise, the fact that B would not have
                                                                                                              Pr{T = t}
occurred if A had not often suggests that A caused B.
    This explains a fundamental law of experimental
                                                                      Conditional probabilities are symmetric in the sense that, if
science: Mere observation can only reveal a correlation, not
                                                                      well-defined, their converses are well-defined too. In fact,
a causal relation. That’s why causal induction requires
                                                                      given the marginal probabilities of the relevant variables,
manipulation, control over an independent variable such that
                                                                      Bayes’ rule tells us how to evaluate the converse:
changes in its value will determine the value of the
dependent variable whilst holding other relevant conditions
constant. Everyday causal induction has these same                                                          Pr{T = t}
                                                                      Pr{T = t|G = g} = Pr{G = g | T = t}              .     (1)
requirements. Causal inductions in everyday contexts are                                                    Pr{G = g}
aided by manipulation of potential causes, by people
intervening on the world rather than just observing it (the           Doing
conditions favoring intervention are spelled out in Pearl,
2000; Spirtes, Glymour, & Scheines, 1993).                            To represent action, Pearl proposes an operator do(•) that
                                                                      controls both the value of a variable that is manipulated as
    If we already have some causal knowledge, then certain
                                                                      well as a graph that represents causal dependencies.
causal questions can be answered without actual

do(X=x) has the effect of setting the variable X to the value                      A causes B.
x and also changes the graph representing causal relations                         A causes C.
by removing any directed links from other variables to X                           B causes D.
(i.e., by cutting X off from the variables that normally cause                     C causes D.
it). For example, imagine that you believe that temperature                        D definitely occurred.
T causally influences the operation of logic gate G, and that
altitude A causally influences T. This could be represented     On the basis of these facts, please answer the following 2
in the following causal diagram:                                questions:
                                                                i. If B had not occurred, would D still have occurred?___
        A                     T                      G                 (yes or no)
                                                                ii. If B had not occurred, would A have occurred?___ (yes
                                                                       or no)
Presumably, changing the operation of the logic gate would
not affect temperature (i.e., there’s no causal link from G to       Pearl (2000) gives the following analysis of such a
T). We can decide if this is true by acting on the logic gate   system. First, we can graph the causal relations amongst the
to change it to some operational state g and then measure       variables as follows:
the temperature; i.e., by running an experiment in which the
operation of the logic gate is manipulated. We could not in
general determine a causal relation by just observing                                           B
temperatures under different logic gate conditions, because
                                                                           A                                     D
observation provides merely correlational information.
Measurements taken in the context of action, as opposed to
                                                                                                C
observation, would reflect the probability that T=t under the
condition that do(G=g):
                                                                You are told that D has occurred. This implies that B or C or
           Pr{T = t|do(G = g)}                                  both occurred, which in turn implies that A must have
                                                                occurred. A is the only available explanation for D. Thus, all
Obtained by, first, constructing a new causal model by          4 events have occurred. When asked what would have
removing any causal links to G:                                 happened if B had not occurred, we should apply the do
                                                                operator, do(B = did not occur) with the effect of severing
                                                                the links to B from its causes:
        A                     T                      G
                                                                                                B
The rationale for this is that if I have set G=g, then my
intervention renders other potential causes of g irrelevant. I             A                                     D
am overriding their effects, so I should not make any
inferences about them. Now I can examine the probability                                        C
distribution of T in the causal graph. But in doing so, I
should not take into account the prior probability of g,        Therefore, we should not draw any inferences about A from
because I have set its value, making its value certain by       the absence of B. So the answer to the counterfactual
virtue of my action. Because the do operation renders T and     question ii. above is "yes" because we already decided that
G probabilistically independent, the result is that:            A occurred, and we have no reason to change our minds.
                                                                The answer to counterfactual question i. is also "yes"
                     Pr{T = t|do(G = g)} = Pr{T = t}.           because A occurred and we know A causes C which is
                                                                sufficient for D.
     The do operator is used to represent experimental               Other theories of propositional reasoning, mental models
manipulations. It provides a means to talk about causal         theory (Johnson-Laird & Byrne, 1991) and any theory based
inference through action. It can also be used to represent      on logic (e.g., Rips, 1994), don’t really make predictions in
mental manipulations. It provides a means to make               this context because the argument uses causal relations and
counterfactual inferences by determining the representation     therefore lies outside the propositional domain. The closest
of the causal relations relevant to inference if a variable had they can come is to posit that causal relations are interpreted
been set to some counterfactual value.                          as material conditionals (an assumption made by Goldvarg
                                                                & Johnson-Laird, 2001). To see if such an interpretation of
                        Do we "do"?                             Causal Argument (1) is valid, we can consider Abstract
                                                                Conditional Argument (1):
Consider the following Causal Argument (1) in which A, B,
C, and D are the only relevant events:
                                                                                   If A then B.
                                                                                   If A then C.

                     If B then D.                                 one pair of arguments concerned a robot. Here is the causal
                     If C then D.                                 version of that problem (Robot Causal Argument 1):
                     D is true.
                                                                       A certain robot is activated by 100 (or more) units of
The corresponding questions were:                                      light energy. A 500 unit beam of light is shone through a
i. If B were false, would D still be true? ___ (yes or no)             prism which splits the beam into two parts of equal
ii. If B were false, would A be true? ___ (yes or no)                  energy, Beam A and Beam B, each now travelling in a
                                                                       new direction. Beam A strikes a solar panel connected to
The causal modeling framework makes no particular                      the robot with some 250 units of energy, causing the
prediction about such an argument except to say that,                  robot’s activation. Beam B simultaneously strikes
because it does not necessarily concern causal relations,              another solar panel also connected to the robot. Beam B
responses could well be different from those for the causal            also contains around 250 units of light energy, enough to
argument. The predictions made by a "material conditional"             cause activation. Not surprisingly, the robot has been
account will depend on assumptions about how people                    activated.
interpret the questions; i.e., how they modify the original set
of premises. To answer question i. people may suppress the        i. If Beam B had not struck the solar panel, would the robot
statement that D is true, whilst adding the statement that B         have been activated?
is false. If they do, the truth of D is indeterminate, because it ii. If Beam B had not struck the solar panel, would the
is not entailed by the falsity of B. Alternatively, people           original (500 unit) beam have been shone through the
might not suppress D. The answer would then be "yes"                 prism?
because the original premises state that D is true. Such an
account yields a less ambiguous answer to question ii. Once            The same 238 undergraduates were given either the
people suppose that B is false, they are licensed to infer, by    causal or conditional version of this problem. Their
modus tollens, that A is false. If these "material conditional"   responses are shown in Table 2.
theories make any prediction for the causal arguments, these
should correspond to their prediction for comparable                  Table 2: Percentages of participants responding "yes" to
conditional arguments.                                                      Robot Causal and Conditional Arguments (1).
Experiment 1                                                                Question            Causal     Conditional
     Method. 238 University of Texas at Austin                              i. robot activated   80%           63%
                                                                            ii. beam shone       71%           55%
undergraduates were given one of the two arguments shown
and asked the listed questions.                                        The results are very close to those of the abstract
     Results. Responses are shown in Table 1. The                 problem except that a higher percentage of participants said
predictions of the causal modeling framework were                 "yes" in the conditional version of this problem, z = 2.83; p
supported for the causal arguments but not for the                < .01. This may have occurred because a larger proportion
conditional arguments. The predominance of "yes"                  interpreted the "if-then" connectives of the conditional
responses in the causal condition implies that for the            version as causal relations. The clear physical causality of
majority of participants the supposition that B didn’t occur      the robot problem lends itself to causal interpretation.
did not influence their beliefs about whether A or D
occurred. This is consistent with the idea that these             Experiment 2
participants mentally severed (undid) the causal link
between A and B and thus did not draw new conclusions                  One might argue that the difference between the causal
about A or about the effects of A from a counterfactual           and conditional arguments in the previous examples is not
assumption about B. Responses to the conditional argument         due to a greater tendency to counterfactually decouple
were more variable: no one strategy for interpreting and          variables from their causes in the causal over the conditional
reasoning with conditional statements dominated.                  context, but instead to different pragmatic implicatures of
                                                                  the two contexts. In particular, perhaps the causal context
    Table 1: Percentages of participants responding "yes" to      presupposes the occurrence of A more than the conditional
        Abstract Causal and Conditional Arguments (1).            context presupposes the truth of A. It’s more plausible that
                                                                  D would be true in the conditional arguments even if A were
          Question         Causal     Conditional                 false than that D would have occurred in the causal
          i. D holds        80%           57%                     arguments even if A had not. If so, then the greater
          ii. A holds       79%           36%                     likelihood of saying "yes" in the causal scenarios could be
                                                                  due to these different presuppositions rather than different
     These results were replicated with two additional            likelihoods of undoing.
arguments that used an identical causal or logical structure           To control for this possibility as well as to replicate the
but added semantic content to the problems. For example,          effect, we examined causal and conditional versions of
                                                                  arguments with the following structure:

                                          C
                                                                    Table 4: Percentages of participants responding "yes" to
                                                           E             Robot Causal and Conditional Arguments (2).
 A                    B
                                          D                               Question             Causal    Conditional
                                                                          i. robot activated     90%         75%
                                                                          ii. beam shone         55%         45%
Participants were told not only that the final effect, E, had
occurred, but also that the initial cause, A, had too. This
                                                                    A difference between causal and conditional arguments
should eliminate any difference in presupposition of the
                                                                again obtained for Abstract arguments, z = 2.20; p = .01, but
initial variable because its value is made explicit. To
                                                                not for Robot ones, z = 1.18; n.s. The difference for
illustrate with one of the problems shown, here is the causal
                                                                Abstract arguments suggests that the earlier results cannot
version of the abstract problem (Causal Argument 2):
                                                                be attributed entirely to different pragmatic implicatures
                                                                from causal and conditional contexts. The overall reduction
                     A causes B.
                                                                in "yes" responses could be due to either a different
                     B causes C.
                                                                participant population, some proportion of participants
                     B causes D.
                                                                failing to establish an accurate causal model with these more
                     C causes E.
                                                                complicated scenarios, or participants not implementing the
                     D causes E.
                                                                undoing operation in the expected way (i.e., not mentally
                     A definitely occurred.
                                                                disconnecting B from D).
                     E definitely occurred.
                                                                    Failure to undo is not entirely unreasonable for these
                                                                problems because D’s nonoccurrence is not definitively
i. If D did not occur, would E still have occurred?
                                                                counterfactual. The question said "If D did not occur" which
ii. If D did not occur, would B still have occurred?
                                                                does not state why D did not occur; the reason is left
                                                                ambiguous. One possibility is that D did not occur because
           The causal modeling framework predicts that a
                                                                B didn’t. Nothing in the problem explicitly states that the
counterfactual assumption about D should disconnect it
                                                                nonoccurrence of D should not be treated as diagnostic of
from B in the causal context so that participants should
                                                                the nonoccurrence of B.
answer "yes" to both questions. Participants should only
answer "yes" in the conditional context if they interpret the   Experiment 3
problem causally. Once again the predictions of a material
conditional account depend on assumptions about how the             The causal modeling framework predicts that the
questions modify the premises. A plausible assumption is        connection between B and D should be mentally undone
that only statements mentioned in the question are              whenever D is explicitly prevented; when an intervention
suppressed. Thus in answering question ii., belief about the    (mental or physical) outside the model determines the value
truth of D and B might be suspended and not-D supposed.         of D. To simulate such a situation, we repeated Experiment
However, this leads to a conflict because not-D implies not-    2, but made the interventional prevention of D explicit.
B (via modus tollens) but the premises state A and thus             Method. Participants saw exactly the same sets of
imply B (via modus ponens). It is thus unclear whether or       premises in both causal and conditional contexts, but were
not they should infer B. In any case, a material conditional    asked different questions, questions that made the external
account must predict no difference between the causal and       prevention of D explicit (Causal and Conditional Arguments
conditional contexts.                                           2EP). For the abstract causal context, the questions were:
     Method. Twenty Brown University undergraduates
received either the causal or conditional versions of the       i. If somebody stepped in to prevent D from occurring,
abstract and robot problems described above.                       would E still have occurred?
     Results. The results, shown in Tables 3 and 4, are         ii. If somebody stepped in to prevent D from occurring,
comparable to those from the earlier problems, although the        would B still have occurred?
proportion of "yes" responses tended to be lower in the
causal condition, especially for the likelihood of the beam     For the abstract conditional context, the questions were:
shining if the solar panel had not been struck (only 55% in
Table 4).                                                       i. If somebody stepped in and changed the value of D to
                                                                   false, would E still be true?
    Table 3: Percentages of participants responding "yes" to  ii. If somebody stepped in and changed the value of D to
        Abstract Causal and Conditional Arguments (2).            false, would B still be true?
          Question         Causal     Conditional               For the robot context, the questions in the causal and
          i. E holds        70%           45%                   conditional versions were identical (only the paragraphs
          ii. B holds       74%           50%                   describing the situation differed):

                                                                Experiment 4
i. If a lead barrier were placed in the path of Beam B to           The final experiment attempts to replicate the
   prevent it from striking the solar panel, would the robot    observations made thus far by showing the undoing effect as
   have been activated?                                         well as the enhancement of the effect in an explicit
ii. If a lead barrier were placed in the path of Beam B to      prevention context. Moreover, it does so using an if-then
   prevent it from striking the solar panel, would the original statement in order to show that a conditional statement can
   (500 unit) beam have been shone through the prism?           be treated as causal in an appropriate context.
                                                                    Method. The following scenario was described to 78
Responses were obtained from either 18 or 20 Brown              Brown undergraduates:
undergraduates.
    Results. Results are shown in Tables 5 and 6. The               All rocketships have two components, A and B.
probability of saying "yes" was higher in the explicit              Component A causes component B to operate. In other
prevention context than in its absence, but not significantly       words, if A, then B.
so, z = 1.16 and 1.39 for Abstract and Robot arguments,
respectively. The two may not differ statistically because the  The scenario assumes the simplest possible causal graph:
probability of saying "yes" was already so high in the causal
condition of Experiment 2. In any case, the great majority of                    A                     B
participants acted as if explicitly preventing D caused it to
have no diagnostic value for its cause (B), and that therefore  Notice that the relation between A and B is stated using an
other effects of the cause (E) still held. In other words, the  if-then construction. Approximately half the participants, in
effect of explicitly preventing D is well captured by the do    the non-explicit prevention condition, were then asked:
operator.
                                                                i. Suppose component B were not operating, would
    Table 5: Percentages of participants responding "yes" to        component A still operate?
      Abstract Causal and Conditional Arguments (2EP),
                                                                ii. Suppose component A were not operating, would
              prevention of the antecedent explicit.
                                                                    component B still operate?
         Question         Causal     Conditional
                                                                The remaining half, in the explicit prevention condition,
         i. E holds         75%          50%
         ii. B holds        80%          67%                    were asked:
    Table 6: Percentages of participants responding "yes" to    i. Suppose component B were prevented from operating,
        Robot Causal and Conditional Arguments (2EP).               would component A still operate?
                                                                ii. Suppose component A were prevented from operating,
         Question             Causal     Conditional                would component B still operate?
         i. robot activated    75%           83%
         ii. beam shone        75%           67%                    The causal modeling framework predicts the undoing
                                                                effect, that participants will say "yes" to question i.,
    An unexpected byproduct of explicit prevention was to       Component A will continue to operate if B isn’t because A
increase the proportions of "yes" responses in even the         should be disconnected from B by virtue of the
conditional context, z = 1.80; p < .05. This probably           counterfactual supposition about B. It also predicts the
occurred because the explicit prevention context made it        proportion will be higher in the explicit than non-explicit
more likely that the arguments would be construed causally.     prevention conditions because the nature of the intervention
For example, a question beginning "If a lead barrier were       causing B to be nonoperative is less ambiguous. No other
placed in the path of Beam B to prevent it from striking the    framework, logical or otherwise, makes either of these
solar panel," may well have suggested to participants that      predictions. Finally, the causal modeling framework
they should construe the situation in terms of physical         predicts that people should respond "no" to the second
causation and reason about the situation using causal logic.    question regardless of condition. If A is the cause of B, then
    One implication of this observation is that the             B should not operate if A does not.
interpretation of conditionals varies with the theme of the         Results. The results are shown in Table 7. The 68%
text that the statements are embedded in. Conditionals          giving an affirmative answer to the first question in the
embedded in deontic contexts are well known to be               Non-explicit Prevention condition replicates the undoing
interpreted deontically (Manktelow & Over, 1990). The           effect seen in the previous studies. The even greater
Abstract Conditional Arguments (1) and (2) above show           percentage (89%, z = 2.35; p < .01) in the Explicit condition
that when the theme is ambiguous, the interpretation will be    replicates the finding that the undoing effect is greater when
highly variable. Robot Conditional Argument (2EP) shows         the reason that a variable has the specified value is made
that when the theme is causal, conditionals will be             explicit. Responses to the second question were almost all
interpreted causally.                                           negative, demonstrating that people are clearly

understanding that the relevant relation is causal. This rules  mechanisms veridically. Once a veridical representation of
out an alternative explanation for the earlier studies, that    causal mechanisms has been established, learning and
people were treating causes and effects as disconnected         inference can take place by intervening on the
because they didn’t interpret the relations as causal but       representation rather than on the world itself. But none of
merely as correlational.                                        this can be achieved without a suitable representation of
                                                                intervention. The do operator is intended to allow such a
   Table 7: Percentages of participants responding "yes" to     representation and the studies reported herein provide some
  questions in the Rocketship scenario given questions with     evidence that people are able to use it correctly.
       antecedents non-explicitly or explicitly prevented.          Representing intervention is not always as easy as
                                                                forcing a variable to some value and cutting the variable off
         Question                  Non-          Explicit       from its causes. Indeed, most of the data reported here show
                                 explicit       Prevention      some variability in people’s responses. People are not
                                Prevention                      generally satisfied to simply implement a do operation.
         i. if not B, then A?      68%            89%           People often want to know precisely how an intervention is
         ii. if not A, then B?     2.6%           5.3%          taking place. A surgeon can’t simply tell me that he’s going
                                                                to replace my hip. I want to know how, what it’s going to be
                           Discussion                           replaced with, etc. After all, knowing the details is the only
These data show that most people obey a rational rule of        way for me to know with any precision how to intervene on
counterfactual inference, the undoing principle. When           my representation, which variables to do, and thus what can
reasoning about the consequences of a counterfactual            be safely learned and inferred.
supposition of an event, most people do not change their            Causal reasoning is not the only mode of reasoning. But
beliefs about the state of the normal causes of the event.      the presence of a calculus for causal inference removes any
They reason as if the mentally changed event is                 doubt that it’s an important one.
disconnected and therefore not diagnostic of its causes. This
is a rational principle of inference because an effect is                           Acknowledgments
indeed not diagnostic of its causes whenever the effect is not
                                                                This work was funded by NASA grant NCC2-1217. We
being generated by those causes but instead by mental or        thank Brad Love for his help and Daniel Mochon, Ian
physical intervention from outside the normal causal            Lyons, and Peter Desrochers for collecting data. Josh
system. To illustrate, when an experimenter manipulates the     Tenenbaum provided an important insight.
brightness of a computer monitor, one should not assume
that the monitor needs replacing.                                                       References
    The demonstrations all described a deterministic causal
system. The undoing principle also applies to probabilistic     Lipton, P. (1992). Causation outside the law. In H. Gross &
causes however.                                                    R. Harrison (Eds.), Jurisprudence: Cambridge Essays.
    These data support the psychological reality of a central      Oxford: Oxford University Press.
                                                                Goldvarg, E., & Johnson-Laird, P.N. (2001). Na ve
tenet of Pearl’s (2000) causal modeling framework. The
                                                                   causality: a mental model theory of causal meaning and
principle is so central because it serves to distinguish causal
                                                                   reasoning. Cognitive Science, 25, 565-610.
relations from other relations, such as mere probabilistic      Johnson-Laird, P.N., & Byrne, R.M.J. (1991) Deduction.
ones. The presence of a formal operator that enforces the          Hillsdale, NJ: Lawrence Erlbaum Associates.
undoing principle, Pearl’s do operator, makes it possible to    Mackie, J.L. (1974). The cement of the universe. Oxford:
construct representations that afford valid causal induction       Oxford University Press.
and inference -- induction of causal relations that support     Manktelow, K.I., & Over, D.E. (1990). Deontic thought and
manipulation and control and inference about the effect of         the Selection task. In K.J. Gilhooly, M. Keane, R.H.
such manipulation, be it from actual physical intervention or      Logie, & G. Erdos (Eds), Lines of Thinking, Vol. 1,
merely counterfactual thought about intervention. The do           Chichester: Wiley.
operation is precisely what’s required to distinguish           Pearl, J. (2000). Causality. Cambridge: Cambridge
representations of probability like Bayes’ nets from               University Press.
representations of causality.                                   Rips, L. J. (1994). The psychology of proof: Deductive
    More generally, the findings are consistent in a               reasoning in human thinking. Cambridge: The MIT Press.
qualitative sense with the view of cognition assumed by         Spirtes, P., Glymour, C. & Scheines, R. (1993). Causation,
Pearl (2000) following Spirtes, Glymour, and Scheines              prediction, and search. New York: Springer-Verlag.
(1993). Their analysis starts with the assumption that people
construe the world as a set of autonomous causal
mechanisms and that thought and action follow from that
construal. The problems of prediction, control, and
understanding can therefore be reduced to the problems of
learning and inference in a network that represents causal

