UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Simplicity: A cure for overgeneralizations in language acquisition?
Permalink
https://escholarship.org/uc/item/89t224mp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Onnis, Luca
Roberts, Matthew
Chater, Nick
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

          Simplicity: A Cure for Overgeneralizations in Language Acquisition?
                                        Luca Onnis (l.onnis@ warwick.ac.uk)
                                     Department of Psychology, University of Warwick
                                                   CV4 7AL Coventry, UK
                                  Matthew Roberts (m.roberts.2@ warwick.ac.uk)
                                     Department of Psychology, University of Warwick
                                                   CV4 7AL Coventry, UK
                                     Nick Chater (nick.chater@warwick.ac.uk)
             Department of Psychology and Institute for Applied Cognitive Science, University of Warwick
                                                   CV4 7AL Coventry, UK
                           Abstract                                According to Baker’s Paradox (Baker, 1979)
                                                               children are exposed to linguistic structures that they
   A formal model of learning as induction, the simplicity     subsequently overgeneralize, demonstrating that they
   principle (e.g. Chater & Vitányi, 2001) states that the     capture some general structure of the language.
   cognitive system seeks the hypothesis that provides the     However, some generalizations are grammatically
   briefest representation of the available data− here the     incorrect and children do not receive direct negative
   linguistic input to the child. Data gathered from the
                                                               evidence from caretakers (e.g. corrections labeling such
   CHILDES database were used as an approximation of
   positive input the child receives from adults. We           overgeneralizations as disallowed). The paradox is that
   considered linguistic structures that would yield           non-occurrence is not in itself evidence for the
   overgeneralization, according to Baker’s paradox (Baker,    incorrectness of a construction because an infinite
   1979). A simplicity based simulation was run                number of unheard sentences are still correct. The
   incorporating two different hypotheses about the            irregularities that Baker referred to can be broadly
   grammar: (1) The child assumes that there are no            labeled alternations (Levin, 1993; see also Culicover,
   exceptions to the grammar. This hypothesis leads to         2000). For instance, the dative alternation in English
   overgeneralization. (2) The child assumes that some         allows a class of verbs to take both the double-object
   constructions are not allowed. For small corpora of data,
                                                               construction (He gave Mark the book) and the
   the first hypothesis produced a simpler representation.
   However, for larger corpora, the second hypothesis was      prepositional construction (He gave the book to Mark).
   preferred as it lead to a shorter input description and     Hence the verb give alternates between two
   eliminated overgeneralization.                              constructions. However, certain verbs seem to be
                                                               constrained to one possible construction only (He
                       Introduction                            donated the book to Mark is allowed, whereas *He
                                                               donated Mark the book is not). Such verbs are non-
Overgeneralizations are a common feature of language
                                                               alternating. From empirical studies we know that
development. In learning the English past tense,
                                                               children do make overgeneralization errors that involve
children typically overgeneralize the ‘-ed’ rule,
                                                               alternations, such as *I said her no (by analogy to I
producing constructions such as we holded the baby
                                                               told her no, Bowerman, 1996; Lord 1979).
rabbits (Pinker, 1995). Language learners recover from
                                                                   In this paper we present alternation phenomena
these errors, in spite of the lack of negative evidence
                                                               from the CHILDES database (MacWhinney, 2000) of
and the infinity of allowable constructions that remain
                                                               child-directed speech which will be used in the
unheard; it has been argued that this favours the
                                                               computer model. Secondly, we introduce the simplicity
existence of a specific language-learning device (e.g.
                                                               principle (Chater, 1996), based on the mathematical
Chomsky, 1980; Pinker, 1989). This is an aspect of the
                                                               theory of Kolmogorov Complexity (Kolmogorov,
‘Poverty of the Stimulus’ argument. We report on a
                                                               1965). Thirdly, we present an artificial language
statistical model of language acquisition, which
                                                               designed to model the CHILDES data, and describe
suggests that recovery from overgeneralizations may
                                                               simplicity-based models of language processing and the
proceed from positive evidence alone. Specifically, we
                                                               simulations of recovery from overgeneralizations.
show that adult linguistic competence in quasi-regular
                                                               Lastly we discuss the limitations of this specific model
structures may stem from an interaction between a
                                                               and some implications for research on language
general cognitive principle, simplicity (Chater, 1996)
                                                               acquisition.
and statistical properties of the input.

   Causative alternations in child-directed
                         speech                                                           Transitive Intransitive
                                                                        Verb           occurrences occurrences
Suppose we have a language in which verbs belong to
                                                                        bounce                     75             117
three distinct classes (V1, V2, V3). Each class is related
to two syntactic contexts (C1, C2). One class of verbs                  break                   1251              268
(V1) appears in both contexts. Two other classes of                     burn                       86               60
verbs (V2 and V3) occur in one context only. We can                     close                    855                56
produce a simple table to visualize the alternation:                    freeze                     18               61
                                                                        grow                       59             330
Table 1: Alternating and non-alternating verbs across       Category move                        966              560
contexts                                                        V1      open                    1590              232
                                                                        pop                      104              153
                            C1   C2
                                                                        rip                      139                 9
                     V1     1    1
                                                                        roll                     405              164
                     V2     0    1
                     V3     1    0                                      shake                    147                26
                                                                        slide                      65             120
The causative alternation in English is of this kind.                   swing                      38               96
Verbs like break behave both transitively (I broke the                  tear                     167                20
vase) and intransitively (The vase broke), whereas verbs                turn                    2690              600
like disappear behave only intransitively (The rabbit                   arrive                       0              41
disappeared is allowed; but *I disappeared the rabbit is                come                         0          18437
not) and verbs like cut are found only in transitive                    dance                        0            370
contexts (*The bread cuts is not allowed). An analysis      Category die                             0            141
of CHILDES revealed that verbs in child-directed
                                                                V2      disappear                    0              73
speech fit the pattern of the above idealization: a
number of verbs are exclusively transitive or                           fall                         0           2945
intransitive (see Table 2).                                             go                           0          65193
    Children eventually generalize the structures of the                rise                         0              14
language they are exposed to. A typical generalization                  run                          0           1569
occurs when children say Don’t you fall me down                         stay                         0           1413
(Bowerman, 1982; Lord, 1979). This is an                                bring                   3028                 0
overgeneralized use of a non-causative verb as                          cut                     1315                 0
causative. In the causative construction, some verbs like
                                                                        drop                     640                 0
break can be used both transitively with a semantic
                                                            Category kill                        120                 0
element of cause (I broke the vase) and intransitively
(the vase broke). Verbs like break alternate between            V3      lift                     392                 0
two constructions. However, fall can only be used                       push                    1609                 0
intransitively, and hear only transitively. The                         put                    27154                 0
acquisition of verbs’ argument structure seems                          raise                      25                0
particularly complicated as the way verbs behave                        take                    9724                 0
syntactically is largely arbitrary. Semantically similar                throw                   2090                 0
verbs like say and tell, or give and donate allow for
different constructions.                                   irregularization like the irregular past tense. In the case
    Bowerman (1982) and Lord (1979) recorded a total       of goed-went for example, recovery from the
of 100 different cases in which two-argument verbs are     overgeneralized form *goed can be accounted for by
used with three arguments (e.g. You can drink me the       directly invoking a competition strategy (MacWhinney,
milk). The developmental literature suggests that when     1987): as the number of went in the input increases, it
children acquire a new verb they use it productively in    will win over the irregularized form goed, which has 0
both constructions, without specific directional bias      frequency in the input. Alternations are interesting
(Lord, 1979). It is also worth noting that alternations    theoretically in that the competition model does not
can be theoretically distinguished from other forms of     seem applicable for these. The overgeneralized form
Table 2: Verbs in child-directed speech occurring in       does not have an irregular alternative: there is simply a
transitive and intransitive contexts pooled from the       “hole” in the language. This argument was raised by
CHILDES English sub-corpora (MacWhinney, 2000).            Baker in his distinction between benign exceptions (like

the past tense) and truly problematic alternations like
the ones we consider here (Baker 1979).                    K(x)=log2[1/Q(x)]
    For the purpose of showing how such problematic
irregularities can be learnt using a simplicity principle, Finally, the invariance theorem (Li & Vitanyi, 1997)
we take the causative alternation described above as a     assures that the shortest description of any object is
working example. We extracted verb frequencies from        invariant (up to a constant) between different universal
the CHILDES Database. CHILDES contains a total of          languages, thus granting a measure of simplicity that is
nearly ten million words of child-directed speech.         independent of the data and of the programming
Because we are interested in showing that the input the    language used to encode the data. The above
child receives is rich enough for recovery of              formalizations allow us to replace “Complexity” with
overgeneralization by induction, only the adult speech     “Length” and state that “the best theory to infer from a
in the corpus was selected and analysed.                   set of data is the one which minimizes the length of the
                                                           theory and the length of the data when encoded using
                                                           the theory as a predictor for the data” (Quinlan and
              Simplicity and Language                      Rivest, 1989; Rissanen, 1989). It is important to note
    The simplicity principle (Chater, 1996) states that in that whilst the MDL principle is well established as a
choosing among potential models of finite data, there is   machine learning tool for grammar induction, such
a general tendency to seek simpler models over             models typically make use of parsed corpora or other
complex ones and optimize the trade-off between model      psychologically implausible inputs (e.g. Osborne,
complexity and accuracy of model’s description (i.e. fit)  1999). This paper uses MDL as a metric to present
to the training data. Complexity is thus defined as:       simplicity as a specifically psychological principle.
C= C(model) + C(data|model)
                                                           Modelling language learning with simplicity
The favoured model of any finite set of data will be that      In any study of grammar induction, and in particular
which minimizes this term.                                 in the simplicity framework, it is crucial to see a
    In order to compare different grammars we need a       grammar as a hypothesis about the data. The best
measure of simplicity and a “common currency” for          hypothesis is the one that compresses the data
measuring both the model complexity and the error          maximally, so we can also think of a grammar as
term complexity. Fortunately this is possible by           compression of the data. We can see the achievement of
viewing grammar induction as a means of encoding the       adult linguistic competence as a process of building
linguistic input; the grammatical organization chosen      different hypotheses about the language in order to
(the “knowledge” of the language) is that which allows     achieve optimum compression. The essence of
the simplest encoding of the input. A tradition within     compression is to provide a shorter encoding of the
mathematics and computer science, Kolmogorov               data, enabling generalizations and correct predictions.
complexity, shows that the simplest encoding of an         Alternations are particularly informative about the
object can be identified with the shortest program that    possibility of a cognitive system to capture
regenerates the object (Li & Vitanyi, 1997).               dependencies from limited data. If linguistic structures
    Every sentence generated from a lexicon of n words     were completely regular, then generalizing from a few
may be coded into a binary sequence. The length of a       data would be easy. But as alternations are quasi-
message refers to a binary string description of the       regular, meaning there are exceptions to their regularity,
message in an arbitrary universal programming              a learner must capture fine dependencies in order to
language. The binary string can be seen as a series of     generalize whilst avoiding overgeneralizations.
binary decisions needed to specify the message; smaller        The issue is to choose the candidate model of the
lengths correspond to simpler messages. The brevity of     right complexity to describe the corpus data, as stated
an input Ai is associated to its probability P(Ai) of      by the simplicity principle. We can compare different
occurrence. Shannon’s (1948) noiseless coding theorem      hypotheses (grammars) at different stages of learning
specifies that:                                            and choose, for each stage, the one that minimizes the
                                                           sum of the grammar-encoding-length and the data-
Length=Log2[1/P(Ai)]                                       encoding-length. In the following section we compare
                                                           data compression of corpora by two similar models.
More probable events are therefore given shorter codes.    The difference between them is that one posits a
Li & Vitanyi (1997) have shown that the length K(x) of     completely regular rule, whilst the other posits a regular
the shortest program generating an object x is also        rule and some exceptions to it. We can think of the
related to its probability Q(x) by the following coding    second model as having ‘invested’ in exceptions. Each
theorem:                                                   exception initially produces less compression overall,

since the exceptions cost some bits to specify.                               Bits(i)=Log2I        [2]
However, each exception shortens the code-length for
each item in the corpus, and the second model thereby       This section describes how this formula is applied to
‘recoups’ its investment over time.                        calculate the code-length for each section of the model
                                                           and for the data given the model.
The Models                                                    If a language contains r word types and n syntactic
This approach to language acquisition does not focus on    categories, then the probability of specifying one
how learning occurs. Rather, these simulations run         distribution of word types into categories is the inverse
several models concurrently to show that the rate of       of the number of ways in which r word types can be
increase of code-length differs between hypotheses         distributed between n categories, assuming no empty
about language. This section describes the structure of    sets. This given by:
two hypotheses (grammars); the first gives rise to
overgeneralization phenomena whilst the second does                                                    !! − "#
not. These were designed in conjunction with a very                Distributions(r, n)=  ∑   ! −"#
                                                                                                     ! ! − " #$ "$
simple artificial language, which was subsequently used                                    =                       [3]
to test the models. A brief outline of the language is
given here to facilitate the description of the model. A       The codelength for the word-level element is
more detailed consideration of how the artificial          therefore:
language relates to data from corpora of child-directed
speech is given below.                                                          Word-level bits(r, n)
   The artificial language used consists of two syntactic
categories. These can be thought of crudely as nouns                                        !! − "#
                                                                        =Log2  ∑   ! −"#
and verbs. They can be combined to form two-word                                          ! ! − " #$ "$
sentences. Sentences may be of the form NV or VN.
                                                                                 =                           [4]
Forms NN and VV are disallowed. In addition, a
number of sentences are disallowed. Let us imagine that    Specifying a particular sentence-level rule (e.g. that a
there are four nouns (n1-n4) and four verbs (v1-v4) in the sentence may be of the form NV) is a function of the
language, and that v4 is blocked in the sentence final     probability of that sentence type given the number of
position. From this it follows that four sentences are     categories specified in the word-level element. Given
disallowed: each of the four nouns in combination with     that in the artificial language sentences only ever
v4 in an NV-type sentence.                                 contain two words, there are four sentence types
   Each model is comprised of 4 elements: word-level       possible from two syntactic categories (NN, NV, VN,
categories, sentence-level categories, exceptions, and     VV). The probability of any sentence type (e.g. NV) is
code-length. Both models described here contain two        therefore 1/4. When this has been specified, the
word-level categories, comprising nouns and verbs and      probability any remaining sentence type (e.g. VN) is
two sentence-level categories comprising the two           1/3. The code-length for specifying two sentence types
sentence types (NV and VN). The exceptions category        is therefore:
discretely specified all the disallowed sentences. In the
first model this was an empty set. The code-length                Sentence-level bits=Log2(4)+Log2(3)            [5]
specified length of code, in bits, that would be needed
to specify models just described and the corpus data       Specifying the cost of an exception is the same as
given the model structure. The code-length for each        specifying the cost of a sentence. This is done by
sentence in the corpus is consequent on the model          specifying the cost, in bits, of the first word based on
structure.                                                 the probability of its occurrence, and the cost of the
                                                           second word in the same way. The probability of a
Calculating Code-Length For Each Element                   word’s occurrence is the inverse of the total number of
                                                           possible words. The term to specify the first word in
The length of code necessary to specify any object, i, is
                                                           any sentence is therefore:
given by:
                                                                          Bits(i1)=Log2(Tw-Te1)            [6]
                  Bits(i)=Log2(1/pi)  [1]
                                                              where Bits(i1) is the bits required to specify word i in
where pi is the probability of object i. In many cases
                                                           the first position, Tw is the total number of word types
described below, pi can be thought of as choosing one
                                                           in the language and Te1, is the total number of words
of I options. Where this is the case,
                                                           blocked in the sentence initial position as listed in the
                                                           exceptions category.

   The first word specifies which sentence type is being     Two of the four-element models described above
used. The pool of possible words from which the            were exposed to increasingly large corpora of this
second word must come is therefore reduced to the size     language. The first model contained word-level
of the sentence final category as defined by the sentence  information about the 36 nouns and verbs, and
type. For example, if the first word in a sentence is a    sentence-level information about the NV and VN
noun, the sentence type must be NV and the second          sentence types, but the exceptions element was empty:
word must therefore be from the category V. The term       it did not contain any information about the 720
to specify the second word in a sentence is therefore:     disallowed sentences. In this respect it was analogous to
                                                           a learner who has acquired knowledge of word
               Bits(j2)=Log2 (Twc-Te2|1)       [7]         categories and sentence production rules, but has not
                                                           learned that some sentences are illegal. This model
   where Bit(j2) is the number of bits required to specify would therefore be prone to overgeneralizations such as
word j in the second position, Twc is the total number of  I disappeared the rabbit. The second model, by
word types in category c, and Te2|1 is the total number of contrast, did contain information about the disallowed
words specified in the exceptions element as blocked in    sentences. This model therefore required considerably
position two given the word in position 1. The number      more bits to specify initially, but the number of bits
of bits for specifying any sentence i,j is simply:         required to specify each sentence of the corpus was
                                                           fewer. In addition, a language learner who had learned
   sentence bitsi,j=Bits(i1)+Bits(j2)      [8]             these exceptions would not make the same
                                                           overgeneralization errors that the first model would.
   Specifying the code length for each exception is the    Table 3 shows the relative simplicity of each model for
same as specifying code length for a sentence given the    increasingly large corpora as measured by the number
existing exceptions. Each exception in a list of           of bits necessary to encode the model and the corpus
exceptions therefore requires slightly fewer bits to code  data.
than its predecessor.
   It is important to note that these models code corpus   Table 3: Code-lengths of Models 1 and 2 for
data in batch mode – the order in which sentences are      successively large corpora. Code-lengths in bold show
coded is not taken into account. A more psychologically    the shorter codes for the corpus size
realistic (i.e. incremental) algorithm might make use of
the fact that frequently occurring words have a higher                              Model 1:           Model 2:
probability of occurrence and therefore cost less to            Corpus Size       Codelength         Codelength
code.                                                           (sentences)          (bytes)            (bytes)
                                                                      0                 0.1                7.6
Simulating recovery from overgeneralization with                     4000              45.4               51.1
an artificial language                                               8000              90.8              94.7
The models described above were implemented in a
                                                                    12000             136.2              138.3
computer program. They were then exposed to
successively large corpora of sentences from an                     16000             181.5              181.8
artificial language which reflected the structure of the            20000             226.9              225.4
transitive/intransitive alternation phenomena found in              24000             272.2              268.9
the CHILDES database (see Table 2, above). A model
using raw CHILDES data would have been                       It can be seen that for relatively small corpora (up to
computationally impossible, but it is important to note    about 16,000 sentences), Model 1 gives a simpler
that the artificial language closely mirrored the patterns encoding: less bits are required. For a learner who had
of Table 2. The artificial language is outlined above. In  heard relatively few alternation constructions, therefore,
these simulations the word-level categories contained      the tendency would be to code the data in these terms,
36 verbs, reflecting the number of verbs in Table 2, and   resulting in overgeneralizations. For a more
36 nouns. It was decided to keep the number of nouns       experienced learner, however, the simpler encoding
equal to the number of verbs in order to avoid disparity   would be that shown by Model 2, which requires less
between the code-length necessary for different            bits to encode relatively large corpora. The model does
sentence types. There were two sentence-types (NV and      not produce any language, so there are no accuracy
VN) reflecting the transitive and intransitive contexts of statistics. Rather, it is assumed that the learner
the verb constructions. Ten verbs were blocked with all    produces all the sentences available in the current
36 nouns for each sentence type (see Table 2), resulting   (shortest) hypothesis are produced, including any that
in a total of 720 disallowed sentences.                    are incorrect.

        Conclusions and future directions                                         References:
                                                            Baker, C. L. (1979). Syntactic theory and the projection
  These results provide an initial confirmation that          problem. Linguistic Inquiry, 10, 533-581.
simplicity may provide a guiding principle by which         Bowerman, M. (1982). Evaluating competing linguistic
some aspects of language may be learned from                  models with language acquisition data: Implications
experience without recourse to a specific language-           of developmental errors with causative verbs.
learning device. However, the simulations presented           Quaderni di semantica, 3, 5-66.
here are coarse-grained approximations of both the          Bowerman, M. (1996). Argument structure and
language and the language learner. Children do not            learnability: Is a solution in sight? Proceedings of the
process the language in batches of several thousand           Berkeley Linguistics Society, 22, 454-468.
utterances. The models presented here were neither          Chater, N. (1996). Reconciling simplicity and
exposed nor sensitive to different word-type                  likelihood principles in perceptual organization.
frequencies. A number of further studies which would          Psychological Review, 103, 566-581.
provide considerably firmer support for the simplicity      Chater, N. & Vitányi, P. (2001). A simplicity principle
principle as a driving force for language acquisition         for language learning: re-evaluating what can be
suggest themselves.                                           learned from positive evidence. Manuscript submitted
  Firstly, mathematical results show that word-type           for publication.
frequencies are important to the simplicity-driven          Chomsky, N. (1980). Rules and representations.
learner, in that they may be the key as to when it            Cambridge, MA: MIT Press.
becomes advantageous to posit exceptions to rules.          Culicover, P. (2000). Syntactic nuts. Oxford: Oxford
Chater and Vitányi (2001) show that languages are             University Press.
approximately learnable given sufficiently large            Kolmogorov, A. N. (1965). Three approaches to the
amounts of data. The CHILDES data in Table 2                  quantitative definition of information. Problems in
therefore provides an indication of the order in which        Information Transmission, 1, 1-7.
one would expect the learner to cease overgeneralizing      Levin, B. (1993), English verb classes and alternations.
words. An examination of children’s speech that               Chicago: The University of Chicago Press.
confirmed this order would be a major step towards          Li, M. & Vitanyi, P. (1997). An introduction to
providing robust support for the simplicity principle in      Kolmogorov complexity theory and its applications
language. Secondly, it would be useful to compare the         (2nd edition). Berlin: Springer.
timescale of recovery from overgeneralization in            Lord, C. (1979). Don't you fall me down: Children's
children with that of the model. This could be done by        generalizations regarding cause and transitivity.
an examination of CHILDES database to determine an            Papers and Reports on Child Language
approximate relation between a child’s age and the            Development, 17. Stanford, CA: Stanford University
number        of     transitive/intransitive    alternation   Department of Linguistics.
constructions to which they have been exposed. It           MacWhinney, B. (1987). The Competition Model. In B.
would then be possible to compare the learning rate of        MacWhinney (Ed.), Mechanisms of language
the child with that of the model. Again, this would be a      acquisition. Hillsdale, NJ: Lawrence Erlbaum.
useful source of evidence concerning the simplicity         MacWhinney, B. (2000) The CHILDES project : tools
principle in language.                                        for analyzing talk. 3rd ed. London : Lawrence
  In this paper we have suggested that there is sufficient    Erlbaum.
statistical information in the input for a learner to learn Pinker, S. (1989). Learnability and cognition: The
quasi-regular alternating structures. These results are       acquisition of argument structure. Cambridge, MA:
achieved by choosing the model of the language which          MIT Press.
provides the simplest (shortest) description of the         Pinker,     S.    (1995).     The     language    instinct.
linguistic data that has been encountered. These results      Harmondsworth : Penguin.
re-open the question of the viability of language           Quinlan, J. R. & Rivest, R. (1989). Inferring decision
learning from positive evidence under less than ideal         trees using the minimum description length principle.
conditions, with limited computational resources and          Information and Computation, 80, 227-248.
amounts of linguistic data available. They therefore also   Rissanen, J. (1989). Stochastic complexity and
bear, indirectly, on the arguments concerning the             statistical inquiry. Singapore: World Scientific.
balance between nativism and empiricism in language         Shannon, C. E. (1951). Prediction and entropy of
acquisition. More concretely, we suggest that the             printed English. Bell System Technical Journal, 30,
working hypothesis that the search for simplicity is a        50-64.
guiding principle in language acquisition deserves
serious attention.

