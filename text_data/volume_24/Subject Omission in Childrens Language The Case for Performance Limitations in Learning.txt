UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Subject Omission in Children’s Language: The Case for Performance Limitations in Learning
Permalink
https://escholarship.org/uc/item/29q6c17v
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Freudenthal, Daniel
Pine, Julian
Gobet, Fernand
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                               Subject Omission in Children’s Language:
                        The Case for Performance Limitations in Learning
                             Daniel Freudenthal (DF@Psychology.Nottingham.Ac.Uk)
                                  Julian Pine (JP@Psychology.Nottingham.Ac.Uk)
                              Fernand Gobet (FRG@Psychology.Nottingham.Ac.Uk)
                                        School of Psychology, University of Nottingham
                                          University Park, Nottingham, NG7 2RD UK
                           Abstract                               adult Italian and Spanish speakers. Other authors have
                                                                  argued that children actually possess the correct adult
   Several theories have been put forward to explain the          grammar, but drop subjects because they have difficulty
   phenomenon that children who are learning to speak             expressing the (correct) underlying form due to some
   their native language tend to omit the subject of the          kind of processing bottleneck (L. Bloom, 1970; L.
   sentence. According to the pro-drop hypothesis, children
   represent the wrong grammar. According to the
                                                                  Bloom, Miller & Hood, 1975; Pinker, 1984; P. Bloom
   performance limitations view, children represent the full      1990; Valian, 1991). Thus, a child producing an
   grammar, but omit subjects due to performance                  utterance is thought to represent a grammatically
   limitations in production. This paper proposes a third         correct underlying structure, but, due to performance
   explanation and presents a model which simulates the           limitations, some elements have a lower probability of
   data relevant to subject omission. The model consists of       being expressed than others.
   a simple learning mechanism that carries out a                    A number of phenomena have been cited as evidence
   distributional analysis of naturalistic input. It does not     for the performance limitations view. P. Bloom (1990)
   have any overt representation of grammatical categories,
                                                                  showed that, in utterances with a subject, the length of
   and its performance limitations reside mainly in its
   learning mechanism. The model clearly simulates the
                                                                  the Verb Phrase (VP) is shorter than it is in utterances
   data at hand, without the need to assume large amounts         without a subject. The load associated with the
   of innate knowledge in the child, and can be considered        provision of a subject is thought to decrease the
   more parsimonious on these grounds alone. Importantly,         likelihood of expressing a longer verb phrase. Along
   it employs a unified and objective measure of processing       similar lines, the length of the VP is greater when the
   load, namely the length of the utterance, which interacts      subject is a pronoun, than when it is a noun. This is
   with frequency in the input. The standard performance          thought to result from the fact that pronouns are
   limitations view assumes that processing load is               phonetically shorter, and the fact that non-pronominal
   dependent on a phrase’s syntactic role, but does not
                                                                  subjects may be longer than pronominal ones. L. Bloom
   specify a unifying underlying principle.
                                                                  (1970) has also found that subject omission is more
                                                                  likely in negated sentences or in sentences with
                    Subject Omission                              relatively new (unfamiliar) verbs. Presumably, the load
Children who are acquiring English often produce                  associated with negation and novel verbs is such that it
sentences with missing subjects, like those shown                 induces subject omission.
below.                                                               While the performance limitations view makes sense
                                                                  from an information-processing point of view, it is not
      Hug Mummy                                                   very precise in its predictions (Theakston, Lieven, Pine
      Play Bed                                                    & Rowland, 2001). Performance limitations accounts
      Writing Book                                                also tend to be rather ad hoc in nature. Given the
      See Running                                                 imprecise nature of performance limitations, it becomes
                                                                  all too easy to posit a greater processing load whenever
   While these examples clearly do not adhere to adult            the provision of a certain element leads to a greater
English grammar, many contemporary theories of child              likelihood of the omission of another, especially when
language assume that children produce their sentences             there is an interaction with frequency. Furthermore, it is
on the basis of an abstract grammar. Theories differ              not clear whether an explanation of the patterns in the
with respect to how much the hypothesized grammar                 data requires a limitation in production coupled with
differs from the adult grammar. According to the pro-             full knowledge of a language’s grammar (as the
drop hypothesis (Hyams, 1986; Hyams & Wexler,                     performance limitations view typically has it). In fact,
1993), children represent a grammar that is different             as Theakston et al. point out, performance limited
from the adult grammar in that it allows null subjects.           learning of lexical items (independent of syntactic
In this respect, children’s grammar resembles that of             complexity) may well give rise to the same pattern of

results without the need to assume a full representation     development of the net through the three presentations
of the grammar, and a different processing load for          of the sentence.
various types of grammatical roles. The present paper
aims to test these claims by seeing to what extent a
performance limited distributional analysis of
naturalistic input can account for the pattern of
omission and provision of grammatical categories that
is found in children’s speech. To this end, we aim to
simulate the effects that P. Bloom (1990) attributes to
performance limited production. We will now introduce
the model we have used for these simulations.
                        MOSAIC
MOSAIC (Model of Syntax Acquisition In Children) is
an instance of the CHREST architecture, which in turn
is a member of the EPAM family of models. CHREST
models have successfully been used to model
phenomena such as novice-expert differences in chess
and computer programming. In language acquisition,
MOSAIC has been applied to the modelling of the use
of optional infinitives in English and Dutch, the
                                                                         Figure 1: MOSAIC learning an input.
learning of sound patterns and the Verb Island
phenomenon. Due to space limitations, we refer the
                                                                As the model sees more input, it will thus encode
reader to another paper in this volume for the relevant
                                                             longer and longer phrases. Apart from the standard test
references (Freudenthal, Pine & Gobet, 2002).
                                                             links between words that have followed each other in
  The basis of the model is a discrimination net, which
                                                             utterances previously encountered, MOSAIC employs
can be seen as an index to Long-Term Memory. The
                                                             generative links that connect nodes that have a similar
network is an n-ary tree, headed by a root node.
                                                             context. Generative links can be created on every cycle.
Training of the model takes place by feeding utterances
                                                             Whether a generative link is created depends on the
to the network, and sorting them (see Figure 1).
                                                             amount of overlap that exists between nodes. The
Utterances are processed word by word. When the
                                                             overlap is calculated by assessing to what extent two
network is empty, and the first utterance is fed to it, the
                                                             nodes have the same nodes directly above and below
root node contains no test links. When the model is
                                                             them (two nodes need to share 10% of both the nodes
presented with the utterance He walked home, it will
                                                             below and above them in order to be linked). This is
create on its first pass three test links from the root. The
                                                             equivalent to assessing how likely it is that the two
test links hold a key (the test) and a node. The key holds
                                                             words are preceded and followed by the same words in
the actual feature (word or phrase) being processed,
                                                             an utterance. Since words that are followed and
while the node contains the sequence of all the keys
                                                             preceded by the same words are likely to be of the same
from the root to the present node. Thus, on its first pass,
                                                             word class (for instance Nouns or Verbs), the
the model just learns the words in the utterance. When
                                                             generative links that develop end up linking clusters of
the model is presented with the same sentence a second
                                                             nodes that represent different word classes. The
time, it will traverse the net, and find it has already seen
                                                             induction of word classes on the basis of their position
the word he. When it encounters the word walked it will
                                                             in the sentence relative to other words is the only
also recognize it has seen this word before, and will
                                                             mechanism that MOSAIC uses for representing
then create a new link under the he node. This link will
                                                             syntactic classes.
have walked as its key, and he walked in the node. In a
                                                                The main importance of generative links lies in the
similar way, it will create a walked home node under
                                                             role they play when utterances are generated from the
the primitive walked node. On a third pass, the model
                                                             network. When the model generates utterances, it will
will add a he walked home node under the he walked
                                                             output all the utterances it can by traversing the network
chain of nodes. The model thus needs three passes to
                                                             until it encounters a terminal node. When the model
encode a three-word phrase with all new words. (For
                                                             traverses standard links only, it produces utterances or
expository purposes, here we assume that a node is
                                                             parts of utterances that were present in the input. In
created with a probability of 1. As is explained under
                                                             other words, it does rote generation. During generation,
learning rate, this probability is actually lower and
                                                             however, the model can also traverse generative links.
dependent on a number of factors). Figure 1 shows the
                                                             When the model traverses a generative link, it can

supplement the utterance up to that point with a phrase    has been provided by Naigles & Hoff-Ginsberg (1998)
that follows the node that the current node is linked to.  and has been attributed to prosodic highlighting of the
As a result, the model is able to generate utterances that sentence final position (Shady & Gerken, 1999). In
were not present in the input. Figure 2 gives an example   contrast to the standard performance limitations view,
of the generation of an utterance using a generative       processing load in MOSAIC does not vary as a function
link.                                                      of grammatical role. Also note that the version of
                                                           MOSAIC used for these simulations is identical to that
                                                           which Freudenthal, Pine & Gobet (2002) used for the
                                                           simulation of the optional infinitive phenomenon in
                                                           Dutch. No free parameters were fitted to obtain these
                                                           results.
                                                                    Subject Omission in MOSAIC
                                                           MOSAIC creates utterances without subjects because
                                                           the model can output partial utterances, provided that
                                                           the utterance final element has occurred in a sentence
Figure 2: Generating an utterance. Because she and he      final position in the input. As a consequence,
have a generative link, the model can output the novel     constituents that take a position early in the sentence,
utterance she sings. (For simplicity, preceding nodes are  have a higher probability of being omitted than those
ignored in this figure.)                                   that take a position further downstream. Since the
                                                           subject takes first position in English, it has the highest
                     Learning Rate                         likelihood of being omitted. However, this prediction is
                                                           not tied to the English language. MOSAIC would
MOSAIC does not simply learn all the utterances it         generate utterances with omitted subjects in all
encounters. The probability of the creation of a node is   languages that have the subject as the first element in
dependent on the size of the net and the length of the     their underlying word order.
utterance it encodes. This has the effect of making the
learning process frequency sensitive. If an utterance is                            Method
seen more often, it has a higher probability of being
created. Finally, phrases that occur in an utterance final In order to simulate the data presented by P. Bloom
position in the input (have an end marker) have a higher   (1990), two MOSAIC models were trained using
probability of being encoded. The precise formula          corpora of maternal speech available in the CHILDES
governing learning rate is given elsewhere in this         database (MacWhinney & Snow, 1990). We used the
volume (Freudenthal, Pine & Gobet, 2002).                  files of Anne and Becky. The mean length of utterance
                                                           (MLU) in the output generated from the models was
     Performance Limitations in MOSAIC                     2.87 for Anne’s model, and 3.41 for Becky’s model. In
                                                           line with Bloom’s analysis, we limited our analysis to
The only performance limitations in MOSAIC are the         utterances which could not be interpreted as
following:                                                 imperatives. This is necessary as subjectless sentences
                                                           in English are grammatical as imperatives (e.g. Put it
•     Frequency: high frequency items have a higher        down). Bloom selected a list of nonimperative verbs
      likelihood of being encoded, and thus feature in     and past tense verbs for his analysis. Since these verbs
      longer utterances                                    cannot be used in an imperative form, sentences which
•     Short phrases have a higher likelihood of being      contain a verb from these lists, and do not contain a
      encoded than long phrases                            subject, are true examples of subject omission. Tables 1
•     Utterance final phrases have a higher likelihood of  and 2 give the lists of verbs that were used for these
      being encoded.                                       analyses.
•     An utterance will only be produced (generated) if
      its final phrase has occurred in sentence final            Table 1: Nonimperative verbs used for analysis
      position in the input.
                                                                       Care           Laugh           Miss
                                                                       Cary          Laughs          Need
   It may be appropriate to point out that these
                                                                        Fall           Like           See
performance limitations are plausible from general
                                                                       Falls           Live         Sneeze
theorizing in the cognitive psychology and learning
                                                                      Forget          Lives          Want
literature. Huttenlocher et al. (1991) provide evidence
                                                                       Grow           Love           Wants
for the effect of frequency on vocabulary learning.
                                                                      Know            Loves
Evidence for the importance of sentence final position

                                                              A second analysis performed by Bloom was to look
  In line with Bloom’s analysis, we removed from our       at the length of the verb phrase as a function of the type
samples all questions, all utterances that contained the   of subject (no subject, pronoun or non-pronoun). The
words not or don’t, all utterances where the verb was      reasoning was that, since the processing load of a
not used in a productive way, and all utterances where     subject is higher than that of a missing subject, and the
the target verb was part of an embedded clause.            processing load of a non-pronoun subject is higher than
                                                           that of a pronoun (since the pronoun is both
        Table 2: Past tense verbs used for analysis        phonetically shorter as well as shorter in word length),
              Ate          Fixed         Saved             this should again result in length effects on the Verb
              Bit         Folded          Saw              Phrase. The results of this analysis are shown in table 4.
           Bought         Forgot          Sent
            Broke         Found        Sharpened                    Table 4: Mean length of Verb Phrase as a
           Brought         Gave          Spilled                               function of subject size
            Came           Goed         Stepped                                   No Subject Pronoun            Non-
           Caught         Ironed        Stopped                                                                 Pronoun
           Closed           Left        Thought                   Adam                2.60          2.55           2.25
           Cooled          Lost         Throwed                    Eve                2.75          2.30           2.00
           Covered         Made           Took                    Sarah               2.45          1.90           1.50
             Cried        Melted         Tored
           Drinked       Pee-peed       Tripped              Anne’s Model             2.76          2.45           1.60
          Dropped         Pulled         Turned              Becky’s Model            3.31          2.93           1.67
            Dropt          Rode         Washed
            Falled         Said           Went                 Again, it is clear that MOSAIC has no difficulty in
              Fell          Sat          Wrote             simulating these results (though the size of the effect in
                                                           MOSAIC appears to be slightly larger than in the
   Table 3 gives the data for three children that Bloom    children that Bloom analysed). The difference in verb
reports and the two simulations (Anne’s and Becky’s        phrase length between utterances with a pronoun
model). It can be seen that for the children, the Verb     subject and those with a non-pronominal subject is
Phrase length in utterances with a subject is shorter than statistically significant for both Anne’s model (t(64) =
in utterances without a subject. It can also be seen that  3.45, p < .001) and Becky’s model (t(104) = 4.40, p <
MOSAIC readily simulates this result, and the size of      .001). There are two possible reasons why MOSAIC
the effect is quite comparable to that in the children.    might simulate this result. Firstly, non-pronoun subjects
The difference in verb phrase length is statistically      are on average slightly longer than pronoun subjects.
significant for both Anne’s model (t(330) = 4.82, p <      Pronouns are by definition one word long, while non-
.001), and Becky’s model (t(314) = 4.64, p < .001).        pronoun NP’s can contain determiners and adjectives.
                                                           In fact, Bloom indicates that the average non-pronoun
         Table 3: Mean length of Verb Phrases in           subject for the children he analysed was 1.16 words
            sentences with and without subjects            long. Secondly, pronouns have a higher frequency of
             Child             With         Without        occurrence than non-pronominal subjects. In MOSAIC,
                              Subject       Subject        this increases the likelihood that they will be learnt, and
            Adam               2.33           2.60         the likelihood that they will feature in longer utterances.
              Eve              2.02           2.72         We decided to test these two explanations in MOSAIC
             Sarah             1.80           2.46         by performing the analysis on non-pronominal subjects
                                                           of length one and greater separately. As it turned out,
        Anne’s Model           2.14           2.76         only a small proportion of the non-pronominal subjects
       Becky’s Model           2.58           3.31         had a length greater than one. For non-pronominal
                                                           subjects of length one, the size of the VP was 1.58 for
   MOSAIC obtains this result because the probability      Anne’s model, and 1.88 for Becky’s model1. Both
of learning an item in MOSAIC is dependent only on its     values are smaller than the VP length for pronoun
frequency and length, and not on its grammatical role.
There is thus no reason (apart from differences in         1
frequency), why sentences with subjects should, on            One would expect the length of the verb phrase to increase
                                                           when limiting this analysis to subjects of length 1. This is the
average, be longer (or shorter) than those without. The
                                                           case for Becky’s model, but not for Anne’s model. This is due
fact that verb phrases in utterances with subjects should  to the fact that, for Anne’s model, there were relatively few
be longer than verb phrases in utterances without a        long non-pronominal subjects, but one of those that did occur
subject is a straightforward consequence of this fact.     had a particularly long verb phrase.

subjects. Given the low incidence of long non-              This would result in subjects having a higher processing
pronominal subject in both these and Bloom’s data, this     load than objects, and as a result, in them being omitted
clearly indicates that the lower complexity effect that     more often.
Bloom attributes to the fact that pronouns are
phonetically shorter, can be explained by frequency in             Table 6: Omission from obligatory contexts
the input. Note that MOSAIC does not employ a                                            Subjects       Objects
phonetic component. Phonetic differences can therefore            Adam                     57%            8%
not have contributed to MOSAIC’s simulation of the                Eve                      61%            7%
effect.                                                           Sarah                    43%            15%
   The importance of frequency in the input as an
explanation for the difference between pronouns and               Anne’s Model             64%            21%
non-pronouns is also highlighted by a point made by               Becky’s Model            60%            14%
Hyams & Wexler (1993). Though pronouns may be
phonetically shorter, the process of assigning the            The explanation for the effect in MOSAIC is simple.
referent to a (potentially ambiguous) pronoun may           As a result of MOSAIC’s performance limitations, a
actually result in its processing load being higher, rather constituent is less likely to be omitted when it occurs
than lower. This would predict a shorter Verb Phrase        further toward the end of the sentence. Since subjects
length for pronominal than for non-pronominal               take first position, and objects usually come after the
subjects.                                                   verb, the probability of omitting an object is smaller
                                                            than the probability of omitting a subject. Bloom goes
         Subject versus Object Omission                     on to suggest that the hypothesized processing
It has often been shown that subjects are omitted more      asymmetry should cause other differences between
often than objects. In order to test how often objects are  subjects and objects. For example, since pronouns exert
omitted, Bloom selected utterances which contain verbs      less of a processing load, more pronouns will occur in
that require an object, and calculated the proportion of    subject position than in object position. Table 7 shows
object omission from these obligatory contexts. Table 5     the relevant data, both for Bloom’s analysis, and
shows this list of verbs.                                   MOSAIC’s simulations. Again, the assymmetry is
                                                            significant for Anne’s model (X2 (1, N = 243) = 8.08, p
        Table 5: Verbs that take obligatory objects.        < .01), and Becky’s model (X2(1, N = 292) = 27.53, p <
           Bought          Ironed         Saved             .001).
           Broke            Like           Saw
          Brought           Love           See                Table 7: Proportion of overt pronominal Noun Phrases
           Caught          Loves       Sharpened                                         Subjects           Objects
          Covered           Made        Thought                     Adam                   41%                25%
          Drinked           Miss        Throwed                      Eve                   36%                14%
             Fix            Need          Took                      Sarah                  91%                33%
           Folded          Pulled         Want
           Found            Rode         Wants                  Anne’s Model               47%                27%
            Gave            Said         Washed                Becky’s Model               72%                40%
  Table 6 compares the proportion of omitted subjects          There is no specific reason why MOSAIC would
and objects from obligatory contexts (verbs from tables     predict this effect, but the pragmatic factors that Bloom
1 and 2 for subjects, verbs from table 5 for objects). It   mentions may well explain this result. Subjects tend to
can be seen that the proportion of subject omission is      convey given information, and objects tend to convey
considerably higher than the proportion of object           new information. It certainly makes sense to introduce
omission. The subject-object asymmetry was significant      new information using a non-pronoun NP. The use of a
for both Anne’s model (X2 (1, N = 560) = 98.83, p <         pronoun requires the listener to resolve the referent of
.001), and Becky’s model (X 2(1, N = 548) = 125.97, p       the pronoun. The use of a non-pronoun NP is usually
< .001). Bloom suggests several possible causes for this    less ambiguous, which aids the resolution process. In
asymmetry. Firstly, it may be due to pragmatic factors.     fact, several authors have argued that this is the
Since subjects typically convey given information,          preferred argument structure for English (Clancy,
while objects convey new information, it may be more        2001). As such, it is not just a feature of child language,
pragmatically appropriate to omit subjects when             but is actually the preferred structure in adult language.
processing capacity is limited. A second possible cause     The fact that MOSAIC simulates this result is simply a
might be that there is a ‘save the heaviest for last’ bias.

reflection of the fact that it mimics the distribution of  Bloom, L., Miller, P. & Hood, L. (1975). Variation and
the input.                                                   reduction as aspects of competence in language
                                                             development. In A. Pick, (ed.): The 1974 Minnesota
                      Conclusions                            Symposium on Child Psychology, Minneapolis:
MOSAIC clearly simulates all the results that Bloom          University of Minnesota Press.
reports. MOSAIC is not an ad hoc model of subject          Bloom, P. (1990). Subjectless sentences in child
omission, as it has already been shown to account for        language. Linguistic Inquiry, 21, 491-504.
several phenomena in children’s speech, and is firmly      Clancy, P. (2001).        The lexicon in interaction:
grounded in the CHREST/EPAM framework. Though                Developmental origins of preferred argument
MOSAIC has performance limitations, these reside             structure in Korean. In J.W. DuBois, L.E. Kumpf &
mainly in the learning mechanism. Unlike the standard        W.J. Ashby (Eds), Preferred argument structure:
performance limitations view, MOSAIC does not                Grammar as architecture for function. Amsterdam:
assume full competence. In fact, MOSAIC has no built         John Benjamins.
in knowledge regarding grammatical categories or           Freudenthal, D. Pine, J. & Gobet, F. (2002). Modelling
roles. The effects arise in MOSAIC through a                 the development of Dutch optional infinitives in
combination of performance limited distributional            MOSAIC. This Volume.
learning, and frequency sensitivity. Effects that are      Huttenlocher, J. Haight, W., Bryk, A. Seltzer, M. &
present in the input (such as a higher proportion of         Lyons, T. (1991). Early vocabulary growth: relation
pronominal subjects than objects), are mimicked in           to language input and gender. Developmental
MOSAIC’s output because of the fact that it is a             Psychology, 27, 236-248.
distributional analyser.                                   Hyams, N. (1986). Language Acquisition and the
   On a theoretical level, MOSAIC has two main               Theory of Parameters, Dordrecht: Reidel.
strengths over the standard view of performance            Hyams, N. & Wexler, K. (1993). On the grammatical
limitations. Firstly the definition of processing load in    basis of null subjects in child language. Linguistic
the standard view is somewhat ad hoc. If the provision       Inquiry, 24, 421-59.
of certain elements leads to a higher rate of subject      MacWhinney, B. & Snow, C. (1990). The child
omission, this is seen as evidence for a relatively high     language data exchange system: An update. Journal
processing load of these elements. The actual reason for     of Child Language, 17, 457-472.
this high processing load then varies from effect to       Naigles, L. & Hoff-Ginsberg, E. (1998). Why are some
effect. Within MOSAIC, processing load is a function         verbs learned before other verbs. Effects of input
of the interaction of two objectively measurable             frequency and structure on children’s early verb use.
variables: frequency in the input, and length of the         Journal of Child Language, 25, 95-120.
phrase being encoded. When an item is more frequent        Pinker, S. (1984). Language Learnability and Language
in the input, it has a higher likelihood of being encoded,   Development, Cambridge, MA: Harvard University
and therefore features in longer utterances that have a      Press.
higher likelihood of being grammatical (i.e. including     Shady, M. & Gerken, L. (1999). Grammatical and
the subject). If two utterances have equal overall           caregiver cue in early sentence comprehension.
frequency, and one of the two includes a longer element      Journal of Child Language, 26, 163-176.
(verb phrase), then some other element will necessarily    Theakston, A.L., Lieven, E.V.M., Pine, J.M., Rowland,
be omitted. Since the subject is the first element in the    C.F. (2001). The role of performance limitations in
sentence, this has a higher likelihood of being omitted.     the acquisition of verb-argument structure: An
   Secondly, the standard performance limitations view       alternative account. Journal of Child Language, 28,
assumes a large amount of innate knowledge in the            127-152.
child. For the simulation of these results, MOSAIC         Valian, V. (1991). Syntactic subjects in the early speech
assumes no innate syntactic knowledge.                       of American and Italian children. Cognition, 40, 21-
                                                             81.
                   Acknowledgments
This research was funded by the Leverhulme Trust
under grant number F/114/BK.
                       References
Bloom, L. (1970). Language Development: Form and
   Function in Emerging Grammars. Cambridge, MA:
   MIT press.

