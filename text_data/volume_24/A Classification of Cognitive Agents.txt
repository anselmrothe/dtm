UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Classification of Cognitive Agents
Permalink
https://escholarship.org/uc/item/4hb5h97q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Dastani, Mehdi
van der Torre, Leendert
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                              A Classification of Cognitive Agents
                                      Mehdi Dastani (mehdi@cs.uu.nl)
                                   Institute of Information and Computer Sciences
                                                    P.O.Box 80.089
                                          3508 TB Utrecht, The Netherlands
                                  Leendert van der Torre (torre@cs.vu.nl)
                        Department of Artificial Intelligence, Vrije Universiteit Amsterdam
                                                  De Boelelaan 1081a
                                        1081 HV Amsterdam, The Netherlands
                          Abstract                           his mother while his friend goes to cinema. Yet in
                                                             another situation where these agents intend to clean
   In this paper we discuss a generic component of           up their houses, are obliged to help their friends, and
   a cognitive agent architecture that merges beliefs,       believe they cannot do both, they decide to clean up
   obligations, intentions and desires into goals. The
   output of belief, obligation, intention and desire        their houses. Although these agents behave diÆer-
   components may conflict and the way the conflicts         ently, each of them seems to follow a certain be-
   are resolved determines the type of the agent. For        havior pattern under diÆerent situations. The first
   component based cognitive agents, we introduce an         agent seems to be more sensitive to his intentions
   alternative classification of agent types based on        and obligations than to his desires while the second
   the order of output generation among components.
   This ordering determines the type of agents. Given        agent seems to prefer his desires more than his in-
   four components, there are 24 distinct total orders       tentions and obligations. Moreover, the first agent
   and 144 distinct partial orders of output genera-         seems to be indiÆerent towards his intentions and
   tion. These orders of output generation provide the       obligations while the second agent seems to prefer
   space of possible types for the suggested component
   based cognitive agents. Some of these agent types         his intentions above his obligations. These charac-
   correspond to well-known agent types such as real-        teristics and principles that govern agent’s actions
   istic, social, and selfish, but most of them are new      and behavior determine the type of cognitive agents
   characterizing specific types of cognitive agents.        and can be used as the basis for a classification of
                                                             cognitive agents.
                                                                We are motivated by the studies of cognitive
                     Introduction                            agents where the behavior of an agent is defined in
Imagine an agent who is obliged to settle his debt,          terms of rational balance between its mental atti-
desires to go on holiday, and intends to attend a            tudes [1, 9, 5]. A classification of cognitive agent
conference. Suppose that he believes he can only af-         types specifies possible ways to define the rational
ford to finance one of these activities and decides to       balance. Beside the scientific need to study possible
pay his checks to settle his debt. Unfortunately, our        definitions of rational balance in a systematic way,
agent does not earn much money and is in the habit           a classification of cognitive agent types is important
of buying expensive books. Therefore, he runs again          for many applications where it is impossible to spec-
into debt after a short while. Despite the fact that         ify agent behavior in specific and usually unknown
he still has the same obligation, desire, and inten-         situations. In such applications, it is important to
tion and believes that he can only aÆord to finance          specify the behavior of agents in strategic terms and
one of these activities, he decides this time to attend      by means of types of behavior.
the conference. Directly after this decision, he hears          In [2] we investigate the design and implemen-
that the conference is cancelled and he receives a           tation issues of generic component-based cognitive
telephone call from his mother telling him that she          agents. In the present paper, we propose an alter-
is willing to pay his checks for this time. The agent is     native classification of cognitive agent types. There
now happy and decides to go on holiday. Our agent            has been many formal and informal studies propos-
has a friend who has the same obligation, desire, and        ing agent types [1, 8, 4]. In these studies, there is a
intention, and likewise believes that he can only af-        trade-oÆ between the space of possible agent types
ford to finance one of these activities. In contrast to      and their precise and formal definitions. In partic-
our agent, this friend decides to go on holiday. How-        ular, informal studies provide a rich space of possi-
ever, he is late with arranging his holiday; all travel      ble types of cognitive agents and ignore their precise
agencies are sold out. Therefore, he decides to at-          definitions, while formal studies provide precise def-
tend the conference. In a diÆerent situation where           inition of agent types but ignore the richness of the
these two agents are obliged to visit their mothers,         space of possible types. The proposed classification
desire to go to cinema, and believe they cannot do           of cognitive agent types in this paper is formal and
both simultaneously, the first agent decides to visit        in terms of a generic component based architecture.

This classification is systematic and provides a large   is proposed for the design phase and it is thus in
space of possible types for cognitive agents. Some of    terms of agent architecture.
these agent types such as realistic, social, and selfish
are well-known. However, most of these agent types       Agent Architecture
are new and characterize specific types of behavior.     In general, agent architectures are defined in terms
   The layout of this paper is as follows. First,        of knowledge representation (i.e. data) and reason-
we discuss diÆerent ways of classifying agent types.     ing mechanism (i.e. control). The agent type clas-
Since our classification is based on generic compo-      sification, which we introduce in the next section, is
nent based agent architecture, we briefly discuss this   defined in terms of properties of generic component
architecture and explain some of its properties that     based architecture called BOID (BOID stands for
are relevant for the agent type classification. Pos-     Beliefs, Obligations, Intention, and Desire). There-
sible agent types within this architecture are dis-      fore, we first briefly explain this architecture, which
cussed. An example of a conflict situation is formal-    can be seen as a black box with observations as in-
ized and it is shown how diÆerent agent types behave     put and intended actions as output. The architec-
diÆerently in this situation. Finally, we conclude the   ture and the logic of BOID are discussed in more
paper and indicate future research directions.           detail elsewhere [2].
                                                            A BOID agent observes the environment and re-
       Classification based on Agent                     acts to it by means of detectors and eÆectors, respec-
                    Architecture                         tively. Each component in the BOID architecture is
                                                         a process having an input and output behavior. For
Various frameworks with corresponding type classi-       this reason and to model the input/output behavior
fications for cognitive agents are proposed [9, 5, 3].   of each component, the components are abstracted
Considering diÆerent phases in agent oriented soft-      as a rule-based systems that contains a set of de-
ware development process such as analysis, design,       feasible rules. As these components output mental
and implementation phases, most proposed cogni-          attitude only for certain inputs, they represent con-
tive agent frameworks with corresponding type clas-      ditional mental attitudes. In the BOID architecture
sifications are provided for the analysis phase. For     two modules are distinguished: the goal generation
example, Rao and GeorgeÆ’s BDI framework with            module and the plan generation module. The goal
realism and commitment strategies as agents types        generation module generates goals based on beliefs,
[9] have been developed as formal specification tools    desires, intentions and obligations, and the plan gen-
for the analysis phase. In this framework, the single    eration module generates sequences of actions based
minded agent type is thought to be the one which         on these goals. In the rest of this paper, we focus
maintains its commitments until either it believes it    only on the goal generation module since the pre-
has fulfilled its commitments or it does not believe     sented classification of the agent types is defined in
it can ever fulfill its commitments.                     terms of rational balance between agent’s mental at-
   Although these formal tools and concepts are very     titudes. Possible classification of agent types that
useful to specify various types of cognitive agents,     can be defined in terms of the plan generation mod-
they are specifically developed for the analysis phase   ule or in terms of the interaction between the goal
which makes them too abstract for other phases.          or the plan generation modules are out of the scope
In fact, to design and to implement various types        of this paper.
of cognitive agents, we need to define agent types          The BOID architecture diÆers from the Proce-
in terms of tools and concepts available at the de-      dural Reasoning System (PRS) [7], which is devel-
sign and the implementation phases such that they        oped within the BDI (Beliefs, Desires, and Intention)
can be translated into agent architectures and agent     framework, in several aspects. The first diÆerence is
implementations. A closer look at the specification      that BOID extends PRS with obligations as an ad-
formalisms such as Rao and GeorgeÆ’s BDICTL for-         ditional component. One reason for this extension is
malism shows that the space of theoretically possible    to incorporate elements of the social level, i.e. social
cognitive agent types is determined by the expressive    commitments, to formalize for example social agents
power of that formalism. Obviously, other phases of      and social rationality. The second diÆerence is re-
agent development process restrict and narrow down       lated to the conditional nature of mental attitudes
the space of possible agent types since available con-   in BOID. In fact, each mental attitude is abstracted
cepts and tools at those phases should satisfy con-      as a rule-based system containing defeasible rules.
ditions such as realizability and computability. This    This is in contrast with the representation of mental
implies that each agent architecture allows only a       attitudes in PRS which are sets or lists of formula.
subset of possible agent types that can be specified     The third diÆerence is that the BOID components,
at the analysis phase. Therefore, it is essential for    which represent mental attitudes, are processes hav-
each agent architecture to indicate which types of       ing their own control mechanism. Thus, in contrast
agents can be designed in that architecture. The         to the central control mechanism in PRS, in BOID
classification of cognitive agent types in this paper    there are two levels of controls. A central control

mechanism at the agent level coordinates activities         • max(E, ¢) µ B [ O [ I [ D is the set of rules
among components. The control mechanism at the                 (a ,! b) 2 max(E, ¢) strictly applicable to E such
component level determines how and which output                that there does not exists a (c ,! d) 2 B [O[I [D
is generated by each component when it receives in-            strictly applicable to E with Ω(c ,! d) > Ω(a ,! b);
put. Finally, the goals in BOID are generated by the
interactions between agent’s mental attitudes in con-       • E µ L is an extension for ¢ iÆ E 2 Sn and
trast to the PRS where goals are given beforehand              Sn = Sn+1 for the procedure in Figure 1.
and become selected by the central control mecha-
nism.                                                            i := 0; Si := {Obs};
   As noticed, each component can be abstracted as
                                                                 repeat
a rule-based system specified by propositional logical
formulas, in the form of defeasible rules represented               Si+1 := ;;
as a ,! b. The reading of a rule depends on the com-                for all E 2 Si do
ponent in which it occurs. For example, a rule in the                  if exists (a ,! b) 2 B [ O [ I [ D strictly
                                                o                      applicable to E then
obligation component, represented as a ,! b, should
                                                                          for all (a ,! b) 2 max(E, ¢) do
be read as follows: if a is derived as a goal and it is
not inconsistent to derive b, then b is obliged to be a                     Si+1 := Si+1 [ { E [ Lit(w)};
goal. The input and the output of components are                          end for
represented by sets of logical formulas, closed under                  else
logical consequence. Following Thomason [10] these                        Si+1 := Si+1 [ {E};
are called extensions. The logic that specifies exten-                 end if
sions is based on prioritized default logic that takes              end for
an ordering function Ω as parameter. This function                  i:=i+1;
constraints the order of derivation steps for diÆerent           until Si = Si°1 ;
components and characterizes the type of the agent.
We first briefly discuss the BOID conflict resolution            Figure 1: Procedure to calculate extensions
mechanism and then explain how the ordering func-
tion can be used to define various agent types.
                                                               In our model, Ω can assign values to the rules,
Conflict Resolution Mechanism                               such that all rules from one component receive ei-
In the BOID architecture, goals are generated by a          ther larger or smaller values than the rules from an-
calculation mechanism. The calculation starts with          other component. This implies that the rules from
a set of observations Obs, which cannot be overrid-         one components are applied before the rules from an-
den, and initial sets of default rules for the other        other component can be applied. This is the basis of
components: B, O, I, D. Moreover, it assumes an             our idea to define agent types. Of course, in many
ordering function Ω on the rules of the diÆerent com-       practical applications Ω must be specified further.
ponents. The procedure then determines a sequence           For example, an agent may prefer some of his O rules
of sets of extensions S0 , S1 , . . .. The first element in to some of his D rules while conversely preferring
the sequence is the set of observations: S0 = {Obs}.        some other D rules to some other O rules. However,
A set of extensions Si+1 is calculated from a set of        this does not mean that our basic idea has to be
extensions Si by checking for each extension E in           dropped. It just means that the number of compo-
Si whether there are rules that can extend the ex-          nents has to be further specified and the Ω function
tension. There can be none, in which case nothing           has to be defined accordingly. Each component can
happens. Otherwise each of the consequents of the           thus be subdivided in a number of subcomponents
applicable rules with highest Ω-value are added to          such that the Ω can describe the preference of the
the extension separately, to form distinct extensions       rules accordingly. Here we do not further describe
in Si+1 . The operator T h(S) refers to the logical         this division since it is not important for the general
closure of S, and the syntactic operation Lit(b) ex-        idea of agent type classification that we present in
tracts the set of literals from a conjunction of literals   this paper.
b. In practice not the whole set of extensions is cal-         The parameter Ω may assign unique values to the
culated, but only those that are calculated before          rules of all components. In such a case, the BOID
the agent runs out of resources.                            calculation scheme can apply in each iteration loop
Definition 1 A tuple ¢ = hObs, B, O, I, D, Ωi is            only one rule, which implies that the BOID calcula-
called a BOID theory. Let L be a propositional logic,       tion scheme generates only one extension. However,
and an extension E be a set of L literals (an atom          Ω may also assign identical integers to diÆerent rules.
or the negation of an atom). We say that:                   In this case, Ω imposes a partial ordering among the
                                                            rules. For such a Ω, the above BOID calculation
• a rule (a ,! b) is strictly applicable to an extension    scheme can apply more than one rule in each itera-
   E, iÆ a 2 T h(E), b 62 T h(E) and ¬b 62 T h(E);          tion loop, which implies that the BOID calculation

scheme may generate a set of extensions. For exam-      a conference. It is obligatory for the agent not to
ple, consider a scenario in which an agent believes     spend too much money for the conference. In par-
                                               b
that he is in a non-smoking area (i.e. > ,! nsa).       ticular, either the agent should pay for a cheap flight
                                 i                      ticket and stay in a better hotel, or the agent should
He intends to smoke (i.e. > ,! s), but he intends       pay for an expensive flight ticket and stay in a bud-
not to smoke when he is in a non-smoking area (i.e.     get hotel. The agent desires to stay in a better hotel.
       i                                                But, he believes that the secretary has booked an ex-
nsa ,! ¬s). Define Ω as follows:
                                                        pensive flight ticket for him. More examples of these
            b                 i              i          conflicts are presented in [2].
       Ω(> ,! nsa) > Ω(nsa ,! ¬s) > Ω(> ,! s)              A conflict resolution type, which characterizes an
                                                        agent type, is considered here as an order of overrul-
For this Ω, the BOID calculation scheme as defined      ing. Given four components in the goal generation
in Definition 1 generates one single extension which    module of the BOID architecture, there are 24 pos-
is: {nsa, ¬s}.                                          sible orders of overruling. In this paper, we only
Now, suppose Ω is defined as follows:                   consider those orders according to which the belief
            b                 i              i          component overrules any other component. This re-
       Ω(> ,! nsa) > Ω(nsa ,! ¬s) = Ω(> ,! s)           duces the number of possible overruling orders to 6.
                                                        Some examples of conflict resolution with beliefs are
This Ω does assign identical integers to the intention  as follows. A conflict between a belief and an inten-
rules and the BOID calculation scheme generates the     tion means that an intended action can no longer be
following two extensions: {nsa, ¬s} and {nsa, s}.       executed due to the changing environment. Beliefs
                                                        therefore overrule the intention, which is retracted.
                   Agent Types                          Any derived consequences of this intention are re-
Given the presentation of mental attitudes and the      tracted too. Of course, one may allow intentions
BOID calculation scheme, we investigate which type      to overrule beliefs, but this results in unrealistic be-
of interactions between mental attitudes can arise      havior. Conflicts between beliefs and obligations or
within the BOID architecture and how these inter-       desires need to be resolved as well. As observed by
actions can be classified. In principle, there are fif- Thomason [10], the beliefs must override the desires
teen types of conflicts that can occur between the      or otherwise there is wishful thinking. Moreover, a
mentioned four mental attitudes [2]. These conflicts    conflict between an intention and an obligation or
can be solved in diÆerent ways. We explain how dif-     desire means that you now should or want to do
ferent ways of resolving conflicts can be modelled by   something else than you intended before. Here in-
restricting the order of rule application in the BOID   tentions override the latter because it is exactly this
calculation scheme. We argue that these restrictions    property for which intentions have been introduced:
specify diÆerent types of the BOID agent and in-        to bring stability [1]. Only in a call for intention
troduce a classification of the types for the BOID      reconsideration such conflicts may be resolved oth-
agents. Finally, some examples of BOID types and        erwise. For example, if I intend to go to cinema but
their solutions to one and the same conflict situation  I am obliged to visit my mother, then I go to cinema
is presented.                                           unless I reconsider my intentions.
                                                           Using the order of string letters as the overruling
Conflict resolution and agent types                     order and thus as representing the agent type, a re-
One of the main tasks of deliberative agents is to      alistic agent can have any of the following six specific
solve possible conflicts among their mental attitudes.  agent types, i.e. BOID, BODI, BDIO, BDOI, BIOD,
In principle, there are fifteen diÆerent types of con-  and BIDO. These specific agent types are not known
flicts that may arise either within each class or be-   in the literature and we do not have any name for
tween classes. Dependent on the exact interpreta-       them. Note that we overloaded the name BOID in
tion of these classes, some of the conflict types may   this way, because it becomes a specific type of agent
be more interesting or important than others. We        as well as the general name for the agent architec-
distinguish two general types of conflicts: internal    ture. These six specific agent types, in which beliefs
and external conflicts. Internal conflicts are caused   override all other components, can be represented
within each component while external conflicts are      as a constraint on the Ω function resulting in the
caused between them. Internal conflicts can be dis-     well-known agent type, called realistic.
tinguished into four unary subtypes (B ; O ; I ; D).    Definition 2 Realistic agent type is a constraint on
External conflicts can be distinguished into six bi-    the Ω function formulated as follows:
nary conflict subtypes (BO ; BI ; BD ; OI; OD ; ID),    8rb 2 B, ro 2 O, ri 2 I, rd 2 D
and four ternary conflict types (BOI; BOD; BID;         (Ω(rb ) > Ω(ro ) ^ Ω(rb ) > Ω(ri ) ^ Ω(rb ) > Ω(rd ))
OID) and one quadruplicate conflict type (BOID).        or simply
An example of the BOID external conflict type is
the following situation: The agent intends to go to                  B¬O ^ B¬I ^ B¬D

   Now that we have a specific Ω function that char-                              ^
acterizes realistic BOID types, we indicate how the
extension is calculated. Following definition 1, a re-     BfO     BfO       BfO     BfO      BfO     BfO
                                                           BfI     BfI       BfI     BfI      BfI     BfI
alistic BOID agent starts with the observations and        BfD     BfD       BfD     BfD      BfD     BfD
calculates belief extensions by iteratively applying       IfO     OfI       IfD     OfI      DfI     DfO
belief rules. When no belief rule is applicable any-       IfD     OfD       IfO     OfD      DfO     DfI
                                                           OfD     IfD       DfO     DfI      IfO     OfI
more, then either the O, the I, or the D component
is chosen from which one applicable rule is selected
and applied. When a rule from a chosen component
                                                           BfO     BfO       BfO     BfO      BfO     BfO
is applied successfully, the belief component is at-       BfI     BfI       BfI     BfI      BfI     BfI
tended again and belief rules are applied. If there is     BfD     BfD       BfD     BfD      BfD     BfD
no rule from the chosen component applicable, then         IfO     IfD       OfI     IfO      DfI     OfI
                                                           IfD     OfD       OfD     DfO      DfO     DfI
another component is chosen again. If there is no
rule from any of the components applicable, then
the process terminates – a fixed point is reached –        BfO     BfO       BfO     BfO      BfO     BfO
and extensions are calculated.                             BfI     BfI       BfI     BfI      BfI     BfI
   Other agent types can be specified as constraints       BfD     BfD       BfD     BfD      BfD     BfD
                                                           IfD     OfD       IfO     OfI      DfO     DfI
on the Ω function as well. Since we consider in this
paper only realistic agent types, we limit ourselves                             BfO
to those agent types that are subtypes of realistic                              BfI
                                                                                 BfD
agent types. Some of well-known agent types can
now be represented as follows.
                                                           Figure 2: The lattice structure of agent types.
BIDO, BOID, and BIOD are called stable, because
intentions overrule desires, i.e.
                                                       architecture by giving an example that describes the
        B¬O ^ B¬I ^ B¬D ^ I¬D                          following mental attitudes: If I go to Washington
                                                       DC (Go2DC), then I believe that there are no
BDIO, BIDO, and BDOI are called selfish, because
                                                       cheap rooms (ChRm) close to the conference site
desires overrule intentions, i.e.
                                                       (Close2Conf Site). If I go to Washington DC,
        B¬O ^ B¬I ^ B¬D ^ D¬O                          then I am obliged to take a cheap room. If I go to
                                                       Washington DC, then I desire to stay close to the
BOID, BIOD, and BODI are called social, because        conference site. I intend to go to Washington DC.
obligations overrule desires, i.e.                     This example can be represented by the following
                                                       rules:
        B ¬O ^ B ¬I ^ B ¬D^ O¬D
                                                                                      b
The six specific realistic agent types mentioned ear-    Ω = 5 (Go2DC ^ ChRm) ,! ¬Close2Conf Site
lier are subtypes of these three well-known more                                                b
                                                         Ω = 4 (Go2DC ^ Close2Conf Site) ,! ¬ChRm
general realistic agent types. Other agent types,                         d
for which we do not have any name, are still pos-        Ω = 3 Go2DC ,! Close2Conf Site
sible. The relation between these and other realis-                       o
                                                         Ω = 2 Go2DC ,! ChRm
tic agent types forms a lattice illustrated in Figure               i
2. The level in this hierarchy indicates the gener-      Ω = 1 > ,! Go2DC
ality of agent types. The bottom of this lattice is    Lets examine a specific type of social agent,
the realistic agent type that is characterized by the  i.e. BIOD. Let the input of the agent be empty.
least number of constraints on the Ω function. Each    Then, following the extension calculation mech-
higher layer adds additional constraints resulting in  anism, we first derive all beliefs and intentions,
more specific agent types. At the second level, the    resulting in the following extension:
stable, social, and selfish agent types result, and at
the fourth level the mentioned six specific and un-       {Go2DC}
known agent types (BIDO, BIOD, BDIO, BDOI,
BOID, and BODI) result. The top of this lattice        Because it is a social agent (i.e. the fourth rule has
is the falsum, which indicates that adding any ad-     a higher priority than the fifth rule), the obligation
ditional constraint to the Ω function results in an    rule is applied first. This results in the following
inconsistent ordering.                                 intermediate extension:
Example                                                   {Go2DC, ChRm}
In this section, we illustrate how conflicts between   This extension is fed back into the B component
mental attitudes can be solved within the BOID         where it triggers the first belief rule (i.e. the first

rule), because the second belief rules is not appli-                        References
cable as we already have ChRm. This produces the         [1] M. E. Bratman. Intention, plans, and practical
following final extension:                                   reason. Harvard University Press, Cambridge
   {Go2DC,ChRm,¬Close2ConfSite}                              Mass, 1987.
This extension denotes the situation in which the        [2] J. Broersen, M. Dastani, Z. Huang, J. Hulstijn
agent has decided to go to Washington DC and takes           and L. van der Torre. The BOID Architec-
a cheap room not close to the conference site, which         ture: Conflicts between beliefs, obligations, in-
is indeed social behavior.                                   tentions, and desires. Proceedings of Fifth In-
However, if we exchange the priority of the fourth           ternational Conference on Autonomous Agents
and the fifth rules the agent becomes a selfish agent        (AA’01), ”9–16”,ACM Press (2001)
‘BIDO’. Then, the D-rule would be applied before
any obligation rule is applied, resulting in the fol-    [3] R. A. Brooks. A robust layered control system
lowing final extension:                                      for a mobile robot. IEEE J. Robotics Auromat.,
                                                             RA-2(7):14–23, Apr. 1986.
   {Go2DC,¬ChRm, Close2ConfSite}
                                                         [4] C. Castelfranchi. Prescribed Mental Attitudes
Sending the results back to the belief component             in Goal-Adoption and Norm-Adoption. In AI
does not make any diÆerence here. This extension             and Law, Special Issue on Agents and Norms,
denotes the situation in which an agent has decided          7, 1999, 37-50.
to go to Washington DC and takes an expensive
room close to the conference site, which is indeed       [5] P. Cohen and H. Levesque.          Intention is
selfish behavior.                                            choice with commitment. Artificial Intelligence,
                                                             42:213–261, 1990.
             Concluding Remarks
We have briefly discussed the generic component          [6] M. Gelfond and T. Cao Son. Reasoning with
based BOID architecture that is developed for cog-           Prioritized Defaults. Proceedings of Logic Pro-
nitive agents. Each component in the BOID archi-             gramming and Knowledge Representation 1997,
tecture represents a mental attitudes of the agent.          164-223, Port Jeerson, New York, October
The output of components may conflict. Some of               1997.
the conflicts that may arise among BOID’s compo-         [7] M. P. GeorgeÆ and A. L. Lansky. Reactive rea-
nents are presented. In the BOID architecture the            soning and planning. In Proceedings of the
conflicts are resolved by the order of output gener-         Sixth National Conference on Artificial Intel-
ation from diÆerent components. We have shown                ligence (AAAI-87), pages 677–682, 1987.
that the order of output generation determines the
type of an agent. In general, the order of output        [8] A. Rao and M. GeorgeÆ. An abstract architec-
generation can be used to identify diÆerent types of         ture for rational agents. In Proceedings of the
agents. We have shown that these conflict resolution         KR92, 1992.
mechanisms provide some well-known agent types
and an interesting set of unknown agent types. In        [9] A. Rao and M. GeorgeÆ. BDI agents: From
particular, we have shown that for a realistic agent         theory to practice. In Proceedings of the First
beliefs are generated before obligations, intentions         International Conference on Multi-Agent Sys-
or desires; for a stable agent intentions are gener-         tems (ICMAS’95), 1995.
ated before desires; and for selfish agents desires are
generated before intentions.                            [10] R. Thomason. Desires and defaults. In Proceed-
   We believe that the way the BOID components are           ings of the KR’2000. Morgan Kaufmann, 2000.
updated depends also on the type of the agent. The
integration of updating various components have the
highest priority in our research agenda. Another is-
sue which in on our future research agenda is the
incorporation of agent types derived from plan gen-
eration module and its interaction with goal gener-
ation modules. In the BOID architecture, the plan
generation module influences the computation of ex-
tensions and therefore may play an important role
in agent type classification. For example, when a
generated extension cannot be transformed into a
sequence of actions, another extension should be se-
lected. The exact choice for a new extension should
depends on the type of agent as well.

