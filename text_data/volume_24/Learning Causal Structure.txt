UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Causal Structure
Permalink
https://escholarship.org/uc/item/6kf9h3c7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
Lagnado, David A
Sloman, Steven
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                          Learning Causal Structure
                                David A. Lagnado (David_Lagnado@Brown.Edu)
                             Department of Cognitive and Linguistic Sciences, P.O.Box 1978
                                                 Providence, RI 02912 USA
                                   Steven Sloman (Steven_Sloman@Brown.Edu)
                             Department of Cognitive and Linguistic Sciences, P.O.Box 1978
                                                 Providence, RI 02912 USA
                         Abstract                               interventional learning, and whether people represent
                                                                their interventions in the manner suggested by this
  The central aims of this experiment were to compare           normative account.
  observational and interventional learning of a simple
  causal chain, and to ascertain whether people represent
  their interventions in accordance with the normative
  model proposed by Pearl (2000). In the observation                                 Causal Models
  condition people treated putative causes as independent,      The causal model framework offers a method for
  and systematically selected the wrong model. In the           representing causal knowledge and formal rules for
  intervention condition performance improved, in               updating this knowledge in the light of either
  particular greater sensitivity was shown to the relevant      observation or intervention. Central to this formalism is
  conditional independencies. However, participants’
  likelihood judgments approximated the observed
                                                                the use of directed graphs to represent the mechanisms
  frequencies rather than reflecting the appropriate causal     that underpin our causal knowledge of a domain, and
  model.                                                        the use of probability theory to reflect the uncertain and
                                                                defeasible nature of this knowledge.
                      Introduction
Our causal knowledge of the world is closely tied to our               Bronchitis          Cough             Insomnia
ability to control or manipulate certain aspects of it. On
the one hand, we often learn about cause-effect
relations by observing the effects of our own
                                                                                  Figure 1: A causal graph
interventions (e.g., running controlled experiments). On
the other, we can exploit such knowledge by                        A causal model is made up of a set of nodes, a set of
manipulating the causes appropriate to our desired ends.        directed links between nodes, and a conditional
Further, our causal knowledge allows us to predict or
                                                                probability distribution for each node. The nodes
imagine the consequences of our actions, and is thus a          correspond to variables relevant to the domain (the pre-
prerequisite for deliberative decision-making.                  selection of which may be non-trivial); these may be
  Given the central role that intervention plays in
                                                                binary, or take on a range of values. The directed links
causal reasoning, it has received scant attention in most       between variables correspond to the autonomous
accounts of human causal learning. In part this is due to       mechanisms that are supposed to mediate between these
the lack of a formal analysis of intervention, and the
                                                                variables, and hence reflect the dependencies between
failure of standard probability theory to distinguish
                                                                them.
action from observation (Pearl, 2000). These lacunas               A simple causal graph is depicted in Fig. 1. In this
appear to be addressed by a body of recent research in
                                                                example the model is restricted to three binary
AI, which provides a normative analysis of causal
                                                                variables: Bronchitis, Cough, and Insomnia. There is
inference and a formal means of representing the                presumed to be one mechanism that leads from
difference between observation and intervention (e.g.,
                                                                Bronchitis to Cough, and another that leads from Cough
Glymour, 2001; Pearl, 2000).
                                                                to Insomnia.
  The formulation of a normative model is at best only             Typically the dependencies between variables are
a first step towards an understanding of how people
                                                                probabilistic – reflecting either the incompleteness of
acquire and employ causal knowledge. The current
                                                                the causal model or genuine noise. This uncertainty is
experiment aims to gather some preliminary evidence             represented by conditional probability distributions for
about the difference between observational and
                                                                each node (referred to as the parameterization of the

graph). Thus in our simple example the strength of              but does not warrant any change in the probability we
dependency between Bronchitis and Cough is                      assign to him having Bronchitis.
represented by two conditional probabilities – the                 More generally, the probabilistic inferences we are
probability of Cough given Bronchitis, and the                  licensed to draw after observing the value of a variable
probability of Cough given no Bronchitis. A high                may not be the same as those after intervening to set
probability for the former would correspond to the              that variable to the same value. Bayesian updating,
belief that Bronchitis is very likely to cause Cough; a         indeed any formal probability model, fails to recognize
high probability for the latter would correspond to the         this. It does not differentiate between observing and
belief that Cough is also very likely to be caused by           acting. That is, the same conditional probability P(X|Y)
other variables not represented in our simple model.            is used to represent the probability of X given that Y is
   Given certain assumptions,1 the structure of a causal        observed, and the probability of X given that we do Y.
graph will fully capture the probabilistic dependencies         But these can be quite different, as our example
amongst all of the represented variables. A fundamental         illustrates – the probability of Bronchitis given the
relation here is that of ‘screening off’ or conditional         absence of a cough is distinct from the probability of
independence. For any three variables A, B, C: A and B          Bronchitis given that we remove the cough.
are conditionally independent given C if P(A|B&C) =
P(A|C); once you know the value of C, learning the                     The Representation of Intervention
value of B does not provide additional information              One of the innovative features of the causal model
about A. One causal graph representation that implies           framework is that it proposes a normative account for
screening off is when C intercepts all directed paths           the representation of interventions, and for the
between A and B. Thus in the causal graph in Fig.1, the         inferences that they license. In so doing, it formalizes
fact that the Cough node is in between the nodes for            the difference between observation and intervention.
Bronchitis and Insomnia implies that Bronchitis and                Pearl (2000) achieves this through the introduction of
Insomnia are conditionally independent given Cough.             the ‘do( •)’ operator. In short, this amounts to
Once you know the value of Cough, learning the value            representing an intervention in terms of a minimal
of Bronchitis tells you nothing more about the value of         modification of the causal graph. Thus a simple
Insomnia.                                                       intervention to set a variable to a particular value is
   By representing conditional independencies in this           represented by the removal of all arrows into that
way, causal graphs provide a powerful tool for                  variable, without altering the other directed links in the
organizing knowledge, and for inferring the effects of          graph. The effects of the intervention are then
new observations. As the graphs increase in size, these         computable through Bayesian updating on this
independence relations can greatly simplify such                ‘mutilated’ graph.
computations. For example, one could supplement the                To illustrate using the graph in Fig.1, consider an
                                                                intervention (e.g., use of a cough suppressant) that sets
simple model in Fig.1 with a comple x network of nodes
                                                                Cough to the value low. This leads to the modified
and links between Bronchitis and Cough, but so long as
                                                                graph in Fig. 2: The directed link from Bronchitis to
the variable Cough still intercepts all links from
                                                                Cough is deleted whilst the link from Cough to
Bronchitis to Insomnia, knowledge of Cough is all one           Insomnia is left unchanged. In effect the intervention
needs to make inferences about Insomnia.                        amounts to placing the variable Cough under the
                                                                influence of a new mechanism that sets its value to low.
  Making inferences given new information
The structure of a causal graph, in combination with the
parameterization of its nodes, determines what
inferences we can make on the basis of new                                Bronchitis         Cough            Insomnia
information. When this information takes the form of
an observation, then Bayesian updating tells us how we                                  Set Cough = Low
ought to modify our probabilities. For example, given
the causal model in Fig. 1, if we find out that Jim has a                Figure 2: Causal graph after intervention
cough, we should increase (to some degree, depending
on the parameters) both the probability that Jim has            This account provides a normative model for the
Bronchitis, and the probability that he has Insomnia.           representation of both actual and imagined
However, what if we changed the value of Cough by               interventions, and tells us how these interventions will
giving him a cough suppressant? Such an action                  (or would) affect the values of the other variables in the
warrants a change in our belief that he has Insomnia,           system. In particular it dictates which probabilistic
                                                                inferences we are entitled to make. Thus the modified
1                                                               graph in Fig. 2 permits us to infer a lower probability of
  For example, the explicit representation of any variable that
                                                                Insomnia, but no change in the probability of
affects two or more other variables in the model.

Bronchitis. The latter prohibition is reflected in Pearl’s        basis of observation alone, one must determine whether
terminology by the difference between P(Bronchitis|               or not blurred vision and headache are independent
~Cough) and P(Bronchitis| do(~Cough)), and captures               given       high     wine    consumption      (conditional
the basic asymmetry of the cause-effect relation:                 independence would only hold if the data were
manipulating a cause can change an effect but not vice-           produced by the model on the right). This may require
versa.                                                            many observations and careful tracking of the relevant
                                                                  relative frequencies.
              Learning causal structure
The appropriate representation of intervention is not                        Wine                     Wine
just critical to predicting the effects of our actions; it is
also important for the discovery or learning of causal
structure. Causal models can be learned from explicit
instruction about how the world works, but we can also                    Blurred             Blurred          Headache
learn about causal structure through observation or                       Vision              Vision
through intervention. These are not exclusive, but it is
useful to distinguish cases in which one is restricted to
observational data alone from those in which one also                     Headache
has the opportunity to intervene.
Observational learning                                                        Figure 3: Two possible causal models
The causal model literature in AI has developed various
algorithms for inferring causal structure from
observational data, many of which exploit the                     Interventional learning
conditional dependencies encoded in the structure of a            Another way to learn about causal structure is to
causal graph. So far none of these have been proposed             actively interact with the system under study and to
as models of actual human discovery, although they do             observe the consequences. This seems to apply to the
suggest some general principles that are relevant to              infant playing with a new toy as much as to the scientist
such enquiries. For example, the establishment of                 running controlled experiments. Whilst this is often
conditional dependencies is a crucial starting point for          recognized as an important source of causal knowledge,
the construction of a causal graph, so it is important that       it has received less attention in the human causal
people are able to make judgments of conditional                  learning literature. 3
dependence versus independence. In contrast, the                     Intuitively, the ability to intervene on a system
precise parameterization of those dependencies is not             should facilitate our learning about its causal structure.
always required to discover correct causal structure              To take the simplest example, consider two variables
    Moreover, the graphical approach clarifies which              that are known to be probabilistically dependent.
causal structures can be differentiated on the basis of           Assuming no other relevant variables, the direction of
observational data alone. It establishes equivalence              this link can be determined by manipulating one of the
classes of structures ("Markov equivalence") that share           variables and observing whether or not the other also
conditional dependencies and are thus indistinguishable           changes. In a noisy system such learning may still
on the basis of observation alone.2 For example, in a             require multiple trials and sensitivity to the observed
                                                                  frequencies. But interventional learning has several
model made up of just two nodes, A and B, ascertaining
                                                                  advantages over passive observation. Not only can it
their probabilistic dependence does not tell us whether
                                                                  help to determine the direction of the causal links, it
A causes B, or B causes A.                                        also allows selection of the kind of data to see, and thus
    Even if causal structures are from different Markov
                                                                  to test out critical relations between variables. For
equivalence classes, it might be difficult for people to
                                                                  example, let us return to the task of distinguishing
distinguish them on the basis of observational data.              between the two possible causal models in Fig. 3. One
Indeed, selecting between certain structures requires
                                                                  possible intervention is simply to drink a large amount
careful tracking of observed frequencies and subtle
                                                                  of wine and then keep your eyes closed. If you don’t get
inferences based on what one would expect to see. For             a headache, you can be reasonably sure that the chain
example, consider the two possible causal structures
                                                                  model is the correct one. If the system is rather noisy
depicted in Fig. 3. In order to distinguish these on the
                                                                  you may have to repeat this experiment several times,
2
  One important qualification here is in the case of graphs in    3
which causal links are necessary but not sufficient; that is, for   The dominant approaches to human causal learning (e.g.
a directed link from A to B: 1 > P(B|A) > 0 and P(B|~A) = 0.      Cheng, 1997; Dickinson, 2001; Shanks, 1995) concentrate on
Networks built from such links may be distinguishable even        observational learning.
though they are Markov equivalent.

but it will still lead to greater confidence than making    test trials. On each trial they were shown the values of
the distinction on the basis of observation alone.          the two relevant variables, and then clicked on a button
                                                            to view whether or not the outcome occurred. The
               Overview of Experiment                       learning set was constructed according to a chain model
The central aims of this experiment were to compare         (see Fig. 4) and is shown in Table 1 (order of
the observational and interventional learning of a          presentation was randomized for each participant).
simple causal model, and to ascertain whether people
represent their interventions in accordance with the               Table 1: Frequency of presented instances in
normative model proposed by Pearl (2000). We used a                        Observational Learning condition.
typical observational learning paradigm (e.g. Shanks,         Temperature      Pressure    Rocket Launch    No   Prob
1995), but adapted it to include an interventional            High             High        Yes              16   0.32
learning condition and a model selection task. The            High             High        No               4    0.08
learning data were generated from a simple chain model        High             Low         Yes              0    0
(see Fig. 4).                                                 High             Low         No               5    0.1
   Learning performance was assessed both through a           Low              High        Yes              0    0
model selection task and through the sensitivity of           Low              High        No               0    0
people’s probability judgments to the appropriate             Low              Low         Yes              0    0
conditional dependencies.                                     Low              Low         No               25   0.5
                                                               Participants then proceeded to a test phase, in which
                                                     Rocket they made various conditional likelihood judgments
    Temperature              Pressure                Launch (e.g., given that Temperature is high, and Pressure low,
                                                            what is the likelihood that the rocket launches?) plus a
  P(Temperature) = 0.5                                      model selection question. This question presented
  P(Pressure | Temperature) = P(Rocket | Pressure) = 0.8    participants with five candidate causal models – two
  P(Pressure |~Temperature) = P(Rocket |~Pressure) = 0      chains, two forks, and a collider (Fig.5 shows one
                                                            model from each category) – and asked them to select
                                                            the model that they believed was most likely to have
Figure 4: Causal graph used to generate stimuli for both    produced the data.
          observational and interventional tasks
Method                                                              Temp             Pressure        Rocket
Participants. Thirty-three undergraduates from Brown
University received course credit for their participation.      a) Chain 2
                                                                                        Temp
Materials and procedure. Initial instructions to the                Pressure
participants included an introduction to the notion of a
causal model with examples of five candidate models.                                    Rocket
Each participant then completed both an observational           b) Fork 2
and an interventional learning task. Two cover stories
                                                                    Temp
were used, one for each task (task order and scenario
were counterbalanced across participants). Participants                                 Rocket
were asked to imagine that they were space engineers                Pressure
(chemists) running tests on a new rocket (perfume) in
order to discover the underlying causal structure. They
                                                                c) Collider
were told that previous tests had identified two
variables as relevant to the success of the test. In the
space engineer scenario the relevant variables were            Figure 5: Three models from the selection task
Temperature (either high or low) and Pressure (either
high or low), and the outcome variable was whether or          In the learning phase of the intervention task,
not the rocket launched. In the chemist scenario the        participants were able to set the value of one of the two
variables were Acid level (either high or low) and Ester    relevant variables. They then viewed the resulting
level (either high or low), and the outcome variable was    values of the outcome variable and the variable they
whether or not the perfume was produced. In the             had not intervened on. This learning set was generated
observation task participants viewed the results of 50      from a pseudo-random table constructed in accordance
                                                            with the same chain model. After running 50 tests they

proceeded to an identical test phase as                               in the   substantially, and only 8 out of 33 participants judged
observation task.                                                              them equal.
Results and discussion
                                                                                                         100              observe     intervene
                                                                               Mean likelihood ratings
Model Selection. The results for the model selection
task are shown in Fig. 6, with the correct chain model                                                    90
                                                                                                          80
designated as chain 2. 4 There were more correct model
                                                                                                          70
selections in the intervention than in the observation
                                                                                                          60
condition. However, whilst the correct model was the
                                                                                                          50
modal response in the intervention condition, overall                                                     40
responses were not significantly different from the                                                       30
uniform distribution (χ2 (4) = 2.91, ns.). By contrast in                                                 20
the observational condition there was an overwhelming                                                     10
bias in favor of the collider (χ2 (4) = 40.79, p < 0.001).                                                 0
                                                                                                               P(R|T&P)             P(R|P)
                      0.7                                      observe
                                                               intervene                Figure 7: Mean conditional likelihood ratings for the
                      0.6
                                                                                                outcome variable R (rocket launch).
Proportion Selected
                      0.5
                                                                               Compatibility of judgments with the do operator.
                      0.4                                                      One way to assess the extent to which participants
                                                                               represent their interventions in line with the do operator
                      0.3                                                      is to look at their judgments of the likelihood that
                                                                               Pressure was high given that Temperature was low,
                      0.2
                                                                               P(P|~T). Recall that the correct judgment for this
                      0.1                                                      likelihood is zero; Pressure is never high if
                                                                               Temperature is low. However, when participants
                       0                                                       intervene on the Pressure variable and set it to high
                            chain 1   chain 2   collider   fork 1   fork 2     they temporarily break the link between Temperature
                                                                               and Pressure. In such cases the value of Temperature is
 Figure 6: Model selection results in interventional and                       equally likely to be high or low (its base rate = 0.5). If
 observational conditions (the correct model is chain 2).                      participants fail to represent their interventions
                                                                               appropriately, by not ‘mentally’ removing the link from
Derived judgments of conditional independence. On                              Temperature to Pressure when they intervene on
the model used to generate the learning set (see Fig. 4),                      Pressure, they may erroneously judge that P(P|~T) > 0.
Temperature was independent of Rocket launch                                   This is because 50% of the time when they set Pressure
conditional on Pressure, that is: P(R|T&P) = P(R|P).                           high they will observe Temperature as low. In other
Participants' mean ratings for these two likelihoods are                       words, they might fail to mark the distinction between
shown in Fig. 7. No significant difference obtained                            action and observation.
between the two likelihoods in the intervention                                   To test out this possibility we compared people’s
condition, suggesting that participants were sensitive to                      judgments for P(P|~T) with the relative frequencies
this conditional independence. This is reinforced by the                       they actually observed; i.e., with the proportion of times
fact that 19 out of 33 participants judged the two                             they observed both low Temperature and high Pressure
likelihoods equal. This contrasts with the observation                         (regardless of whether they intervened on Temperature
condition, in which the mean likelihoods differed                              or Pressure). As shown in Fig. 8, participants' mean
                                                                               judgments for P(P|~T) were very close to the
                                                                               frequencies they observed, and significantly different
4
                                                                               from the normative value of zero.
  One complication is that the chain model used to generate
the data is Markov equivalent to fork 2. However, although
not inconsistent with the observational data, this model
requires an idiosyncratic parameterization whereby one effect
(temperature) occurs more often than its sole cause (pressure).
Very few people chose this model in the observation
condition.

                                                              learning (e.g., Shanks, 1995), and multiple cue
                      100
                       90
                                 Judged     Observed          probability learning (e.g., Hammond, 1996), where
                                                              models that assume the independence of causes fit the
    Mean likelihood
                       80
                       70                                     human data well. One factor likely to encourage this
                       60                                     kind of learning was the manner in which the data were
                       50                                     presented (e.g., indicator variables followed by
                       40                                     outcome variable).
                       30                                       Interventional learning increased sensitivity to the
                       20                                     appropriate conditional independencies and eliminated
                       10                                     the bias for the collider, but the effect on model
                        0                                     selection was not entirely beneficial. Although the
                            P(P|T)         P(P|~T)            correct chain was the modal choice, the majority of
                                                              participants still chose the wrong model. Taken together
                                                              with the observational results this implies that
Figure 8: Mean likelihood ratings and observed relative       participants might have experienced too few trials to
       frequencies in the intervention condition.
                                                              confidently discriminate between the models.
                                                                Whatever the precise reasons for sub-optimal
                                                              performance in these tasks, the experiment shows that
   This result could indicate a failure by participants to
                                                              the automatic mechanisms that allow us to engage in
implement the do operation when inferring the relation
                                                              the predictive learning and encoding of noisy
between Pressure and Temperature. However, there are          information can sometimes override our discovery of
alternative explanations for this finding. One possibility
                                                              the causal model that generates this information.
is that participants interpreted the likelihood question in
                                                              Nevertheless, the difference we did find between
terms of observational rather than interventional
                                                              observational and interventional learning encourages us
probabilities, and accurately reported the relative
                                                              that people are able to make use of the special kind of
frequency with which low Temperature and high
                                                              information afforded by intervention, and that future
Pressure co-occurred, regardless of whether they
                                                              models of learning need to incorporate methods that
believed that low Temperature would cause high
                                                              represent the effect of action.
Pressure. This fits with numerous studies showing that
people encode the relative frequencies of events
automatically, and often use these as a basis for their
                                                                              Acknowledgments
likelihood judgments (e.g., Hasher & Zacks, 1984).            This work was funded by NASA grant NCC2-1217 to
    Second, on Pearl’s account the notion of an               Steven Sloman. We thank Sean Stromsten and Dave
intervention is only well defined relative to a specific      Sobel for valuable comments.
causal model. Thus if people uphold an incorrect model
(as the majority of the participants did) they are                                References
unlikely to give appropriate estimates for the                Cheng, P. (1997). From covariation to causation: a
interventional probabilities. Moreover, even those              causal power theory. Psychological review,104. 367-
participants that do select the correct model will have         405.
entertained various incorrect ones through the course of      Dickinson, A. (2001). Causal learning: an associative
learning, and it may be very hard for them to                   analysis. Quarterly Journal of Experimental
retrospectively revise their prior observations.                Psychology, 49B, 60-80.
                                                              Glymour, C. (2001). The mind’s arrows. Cambridge,
                             Conclusions                        MA: MIT Press.
                                                              Hasher, L., & Zacks, R.T. (1984). The automatic
This experiment demonstrated a contrast between                 processing of fundamental information: The case of
observational and interventional learning, both with            frequency of occurrence. American Psychologist, 39,
respect to people’s model selection and their likelihood        1372-1388.
judgments. Under observational learning, participants         Hammond, K.R. (1996). Human judgment and social
exhibited a strong bias for the collider, despite the fact      policy. New York: Oxford University Press.
that the variables they judged to be independent were         Pearl, J. (2000). Causality. Cambridge: Cambridge
highly correlated in the data. This suggests that they          University Press.
were engaged in predictive learning of the outcome            Shanks, D.R. (1995). The psychology of associative
variable (e.g., Rocket launch) on the basis of two              learning. Cambridge: Cambridge University Press.
indicator cues (e.g., Temperature and Pressure),
effectively treating them as independent causes of the
outcome. This resonates with research on associative

