UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Bayesian Learning at the Syntax-Semantics Interface
Permalink
https://escholarship.org/uc/item/5rk8v7v7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Author
Niyogi, Sourabh
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

             Bayesian Learning at the Syntax-Semantics Interface
                                      Sourabh Niyogi (niyogi@mit.edu)
                                        Massachusetts Institute of Technology
                                                 Cambridge, MA USA
                         Abstract                            /fill/ and have the same sort of argument struc-
                                                             ture. A diﬀerent distribution informs the learner
   Given a small number of examples of scene-                of a diﬀerent verb class. Considerable evidence has
   utterance pairs of a novel verb, language learners
   can learn its syntactic and semantic features. Syn-       mounted in support of this hypothesis (c.f. Naigles
   tactic and semantic bootstrapping hypotheses both         1990, Fisher et al 1994). In contrast, the semantic
   rely on cross-situational observation to hone in on       bootstrapping hypothesis (Pinker 1989) is that learn-
   the ambiguity present in a single observation. In         ers use what is common across scenes to constrain
   this paper, we cast the distributional evidence from
   scenes and syntax in a unified Bayesian probablistic      the possible word argument structures. If a learner
   framework. Unlike previous approaches to model-           sees a liquid undergoing a location change when /S
   ing lexical acquisition, our framework uniquely: (1)      glipped F/ is uttered, then /glip/ is likely to be in
   models learning from only a small number of scene-        the same verb class as /pour/ and have the same
   utterance pairs (2) utilizes and integrates both syn-     sort of meaning.
   tax and semantic evidence, thus reconciling the
   apparent tension between syntactic and semantic              Both hypotheses require the distribution of cross-
   bootststrapping approaches (3) robustly handles           situational observations. Prior accounts to model
   noise (4) makes prior and acquired knowledge dis-         word learning have either ignored the essential role of
   tinctions explicit, through specification of the hy-      syntax in word learning (Siskind 1996, Tenenbaum
   pothesis space, prior and likelihood probability dis-
   tributions.                                               and Xu 2000), or require thousands of training ob-
                                                             servations (Regier et al 2001) to enable learning. In
                                                             this paper we present a Bayesian model of learning
Learning Word Syntax and Semantics                           the syntax and semantics of verbs that overcomes
Given a small number of examples of scene-utterance          these barriers, by demonstrating how word-concept
pairs of a novel word, a child can determine both the        mappings can be achieved from very little evidence,
range of syntactic constructions the novel word can          where the evidence is information from both scenes
appear in and inductively generalize to other scene          and syntax.
instances likely to be covered by the concept repre-
sented (Pinker 1989). The inherent semantic, syn-                   Bayesian Learning of Features
tactic, and referential uncertainty in a single scene-       We illustrate our approach with a Bayesian analysis
utterance pair is well-established (c.f. Siskind 1996).      of a single feature. On some accounts, verbs pos-
In contrast, with multiple scene-utterance pairs, lan-       sess a cause feature which may be valued 1, *, or 0
guage learners can reduce the uncertainty of which           (Harley and Noyer 2000); depending on the value of
semantic features and syntactic features are associ-         the cause feature, the verb may appear in frame F1,
ated with a novel word.                                      F0, or both:
   Verbs exemplify the core problems of scene-                   1 Externally caused - Ex: touch, load
utterance referential uncertainty. Verbs selectively               F1: He touched the glass.
participate in diﬀerent alternation patterns, which                F0: *The glass touched.
are cues to their inherent semantic and syntac-                  * Externally causable - Ex: break, fill
tic features (Levin 1993). How are these features                  F1: He broke the glass.
                                                                   F0: The glass broke.
of words acquired, given only positive evidence of               0 Internally caused - Ex: laugh, glow
scene-utterance pairs?                                             F1: *He laughed the children.
   The syntactic bootstrapping hypothesis (Gleitman                F0: The children laughed.
1990) is that learners exploit the distribution of           Assuming this analysis, learners who hear utterances
“syntactic frames” to constrain possible semantic            containing a novel verb, not knowing the value of its
features of verbs. If a learner hears /glip/ in frames       cause feature, must choose between 3 distinct hy-
of the form /S glipped G with F/ and rarely hears /S         potheses H1 , H§ , and H0 . Clearly, one utterance
glipped F into G/, the learner can with high confi-          cannot uniquely determine the value of the feature:
dence infer /glip/ to be in the same verb class as           if learners hear F1 (/S Ved O/), the feature sup-

ports H1 or H§ ; similarly, if learners hear F0 (/O      Any number of situations may be analyzed as such:
Ved/), the feature may be H0 or H§ . Two utter-
ances cannot determine the feature uniquely either.          Evidence X               p(H1 |X) p(H§ |X) p(H0 |X)
Learners might receive both F1 and F0, supporting         1  F0                         .033      .333    .633
H§ uniquely. But they may also accidentally receive       2  F0, F0                     .002      .216    .781
2 utterances of the same form (F0, F0 or F1, F1),         3  F0, F0, F0, F0, F0, F0     2e-8      .021    .979
                                                          4  F0, F1                     .137      .724    .137
thus not resolving the ambiguity. If learners received    5  F0, F1, F0, F1, F0, F1     .007      .986    .007
6 utterances of the same form F0 or F1, however,          6  F0, F1, F1, F1, F1, F1     .712      .288    5e-6
then there is overwhelming support for H0 or H1
respectively, and H§ seems far less likely.              When only F0 is given as evidence (situation 1),
                                                         while both H0 and H§ are consistent with the obser-
   A Bayesian analysis renders the above analysis        vation, H0 is nearly twice as likely. However, with
precise and quantitative. Knowledge is encoded in        2 observations of F0 (situation 2) or 6 observations
three core components: (1) the structure of the hy-      (situation 3), it is increasingly likely that H0 is the
pothesis space H; (2) the prior probability p(Hi ) on    correct hypothesis. With both F0 and F1 as evi-
each hypothesis Hi in H, before learners are pro-        dence (situation 4), in contrast, H§ is far more likely;
vided any evidence; (3) the likelihood of observing      with more evidence (situation 5), it becomes more
evidence X given a particular Hi , p(X|Hi ). Given       so. Finally, if the first frame is a “noise” frame and
evidence X = [x1 , . . . , xN ] of N independent obser-  followed by 5 representative frames of F1 (situation
vations, by Bayes’ rule the posterior probability of a   6), then H1 is more likely instead.
particular hypothesis Q   Hi is:                            Given this framework, just one or two observa-
                            N
                            j=1 p(xj |Hi )p(Hi )         tions is suﬃcient to make an informed judgement.
            p(Hi |X) =                               (1)
                             p(x1 , . . . , xN )         Note that each additional observation increases cer-
signaling the support for a particular hypothesis Hi     tainty, and noise is handled gracefully.
given evidence X.
   In this case, xj is the observation of a syntactic       Modeling Semantic Bootstrapping
frame (F0 or F1), and X is a distribution of syn-        In this section, we extend the single feature anal-
tactic frames. One simple prior probability model        ysis to multiple features, where each feature repre-
p(Hi ) has each of the 3 hypotheses are equally likely,  sents information from scenes (from any modality,
encoding that a verb is equally likely to be of the      whether perceptual, mental, etc.). Setting aside ver-
/touch/, /laugh/ or /break/ class:                       bal aspect, we may model possible verb meanings as
                                                 1       a set of M features, where each feature represents
              p(H1 ) = p(H§ ) = p(H0 ) =             (2) a predicate on one or more of the arguments of the
                                                 3
and a likelihood model p(xj |Hi ) encoding how likely    verb. For example, a set of single argument predi-
we are to observe frames F0 or F1 for the 3 diﬀerent     cates might include:
feature values of cause:                                     moving(x), rotating(x), movingdown(x),
    p(xj = F 1|H1 ) = .95 p(xj = F 0|H1 ) = .05              supported(x), liquid(x), container(x)
    p(xj = F 1|H§ ) = .50 p(xj = F 0|H§ ) = .50 (3)      specifying the perceived situation about the argu-
    p(xj = F 1|H0 ) = .05 p(xj = F 0|H0 ) = .95          ment of the verb (e.g. if it is moving, or moving in a
The above likelihood model says that when a verb         particular manner, etc.) while a second set of two-
has cause=1, we expect frames of the form /S Ved         argument predicates might specify the relationships
O/ 95% of the time; when a verb has cause=0, we          between arguments, given that this is an externally
expect /O Ved/ 95% of the time; when a verb has          caused (cause=1) event:
cause=*, we expect both syntactic frames.                    contact(x, y), support(x, y), attach(x, y)
                                                         Using these predicates, an idealized (partial) lexicon
   Both the prior probability model and likelihood
                                                         might contain the following word-concept mappings:
model are stipulated, encoding a learner’s prior                      cause One arg x Two arg x, y
knowledge of grammar. Given these probability                /lower/    1      1*11**         11*
models, this allows for explicit computation of the          /raise/    1      1*01**         11*
support of each hypothesis. Suppose a learner re-            /rise/     0      1*0***
ceives F0. Then the support for each of the 3 hy-            /fall/     0      1*1***
potheses may be computed to be:                          specifying, in linear order, the value of each of the
                                                         one and two-argument predicates above, e.g. that
                          (.05)(.33)                     /lower/ has cause=1, moving(x)=1, rotate(x)=*,
      p(H1 |F 0) =                            = .033
                    (.05 + .50 + .95)(.33)               movingdown(x)=1, etc. – and thus its concept cov-
                          (.50)(.33)                     ers externally-cased motion events where an agent
      p(H§ |F 0) =                            = .333 (4) moves a theme downwards through supported con-
                    (.05 + .50 + .95)(.33)
                                                         tact. The verb /raise/ is nearly identical except it
                         (.95)(.33)
     p(H0 |F 0) =                            = .633      has movingdown(x)=0, while /fall/ and /rise/ in-
                   (.05 + .50 + .95)(.33)
                                                         volve internally-caused motion (cause=0) and do not

specify any two argument predicates. The values of *       Modeling Syntactic Bootstrapping
for the 4 rotating(x), liquid(x), container(x), and at- In this section, we demonstrate a Bayesian model
tach(x,y) predicates signal that these features are ir- of how the distribution of syntactic frames, as en-
relevant to the verb’s concept. Perception of a scene   visioned by Gleitman (1990), may be used to de-
amounts to evaluating these predicates; scenes may      termine the semantic features of a verb. To do so,
or may not fall under the verb concept, conditioned     we introduce a new notion of semantic agreement,
on the values of these predicates. The presence of q    wherein features of a lexical head must agree with its
of “irrelevant” features valued as * implies 2q possi-  complement. Consider the following idealized lexi-
ble scenes consistent with the concept.                 con:
   Given a hypothesis space of possible verb concepts           /fill/   fig: [0] con: [1]
                                                                                             /into/    fig: [1]
formed by M of these sorts of predicates, the task              /pour/ fig: [1] liq: [1]
                                                                                             /with/    fig: [0]
of learning a verb’s meaning given N observations                                            /glass/   con: [1]
                                                                /load/ fig: [*]
                                                                                             /water/   liq: [1]
X = [x1 . . . xN ] of scenes, is to determine which of
the 3M possible concepts is the most likely. Just       A lexical head /fill/ agrees with a complement of /a
as before, a Bayesian model does so by computing        glass with water/ but not with /water into a glass/,
the posterior probability distribution p(Hi |X) over    because the lexical head and its complement have
concepts, given a prior distribution on hypotheses      a value 1 along the fig dimension. Likewise, a lexi-
p(Hi ) and a likelihood distribution of generating a    cal head /pour/ agrees with a complement of /wa-
particular xjΩexample given Hi :                        ter into a glass/ but not /a glass with water/, be-
                   1
                       if xj 2 Hi              1        cause of the opposite value of fig. Finally, a lexical
  p(xj |Hi ) =    2q                ; p(Hi ) = M (5)    head such as /load/, because § agrees with 0 and
                  0    otherwise              3
We can use Bayes’ rule (Eq (1)) to compute the like-    1, accepts both complements. Thus, both /load the
lihood of any hypothesis given N independent ex-        wagon with hay/ and /load hay into the wagon/ are
amples. Intuitively, the above likelihood model says    valid derivations. A large number of verb classes can
that out of the 2q possible scenes that might fall un-  be seen to pattern into three classes along diﬀerent
der the concept Hi , all of them are equally likely;    feature dimensions in this way (Nomura et al 1994).
likewise, the prior probability model holds that all       Any number of feature dimensions may be hypoth-
of the 3M concepts are equally likely.                  esized, and may include selectional features, such as
                                                        /fill/ requiring a container (con:[1]) or /pour/ re-
   Consider a reduced hypothesis space where M =        quiring a liquid (liq:[1]) as its complement.
3:                                                         Suppose a learner hears /S glipped a glass with
    q Concepts                                          water/. The features of the novel verb /glip/ are un-
    0 000, 001,    010,  011, 100,  101, 110, 111
    1 00*, 01*,    10*,  11*, 0*0,  0*1,                known and the features of its complement /a glass
      1*0, 1*1,    *00,  *01, *10,  *11                 with water/ are known. For the fig feature dimen-
    2 0**, *0*,    **0,  1**, *1*,  **1                 sion of /glip/, there are 3 possible values, with 3
    3 ***                                               corresponding hypotheses H0 , H1 , H§ . As before,
Given any distribution of scenes X, we can directly     one observation is insuﬃcient to infer H0 , as H§ is
compute the posterior probability p(Hi |X) of any       also possible. The following likelihood model for an
of the 27 diﬀerent concepts. Four are shown here,       unknown verb feature value V and the feature value
of increasing generality from a very specific concept   of its complement C agreeing can be used for each
(H000 ) covering only one scene (000) to the most       feature dimension (fig, loc, con, etc.) to compute a
general concept H§§§ covering 2M possible scenes:       probability distribution over the Hi :
      Observation X:           H000 H00§ H0§§ H§§§                     p(V, C) V = 0 V = 1 V = §
    1 000                       .30 .15 .07 .03                        C=0         .22     .01      .11
    2 000, 000, 000             .70 .09 .01 .001
    3 000, 001                  .00 .64 .16 .04                        C=1         .01     .22      .11
    4 000, 001, 000             .00 .79 .10 .01                        C=°         .11     .11      .12
    5 000, 001, 000, 001, 000 .00 .94 .03 .001
    6 000, 101, 010, 111, 000 .00 .00 .00 1.0           Intuitively, the above says that with high proba-
                                                        bility, V and C agree, and with low probability
A single scene observation 000 is explained by all 4    (i.e. .01), they do not agree. The above joint dis-
hypotheses (situation 1) in a graded fashion. How-      tribution encodes both the prior distribution on V
ever, with 3 repeated observations (situation 2),       and the conditional distribution p(C|V ):
most of the mass is concentrated on H000 . When
scene observations require abstracting away irrele-                p(V = 0) = p(V = 1) = p(V = §) =
                                                                                                            1
                                                                                                                 (6)
vant features, the more specific concepts must be                                                           3
discarded in favor of more general concepts (situ-        p(C = V |V = 0 or 1) = .65
ation 3 vs 6). Each example consistent with the           p(C 6= V |V = 0 or 1) = .03      p(C = 0, 1|V = §) = .32
general concept further reduces ambiguity over the        p(C = §|V = 0 or 1) = .32        p(C = §|V = §) = .35
possible concepts (situation 4 vs 5).                                                                            (7)

                                                           Scene s          Description/Semantic Features
                                                           pour-fill        Person pouring water into a glass, filling it
Given an assumption of perfect knowledge of the fea-       G001             Glass: Manner: None (0) State: Full (1)
                                                           W110             Water: Manner: Pouring (1) State: None (0)
ture values of the complement, over multiple obser-        splash-fill      Person splashes water into a glass, filling it
vations, the distributional evidence X in support of       G001             Glass: Manner: None (0) State: Full (1)
                                                           W120             Water: Manner: Splashing (2) State: None (0)
the 3 hypotheses can be readily evaluated. We can          spray-fill       Person sprays water into a glass, filling it
test how diﬀerent distributions of syntactic frames        G001             Manner: None (0) State: Full (1)
                                                           W130             Manner: Spraying (3) State: None (0)
correctly yield diﬀerent probability distributions of      pour-empty       Person pouring water out of glass, emptying it
a verbs syntactic and semantic features; this is thus      G002             Manner: None (0) State: Empty (2)
a Bayesian model of Gleitman’s (1990) “syntactic           W110
                                                           splash-empty
                                                                            Manner: Pouring (1) State: None (0)
                                                                            Person splashes water out of glass, emptying it
bootstrapping”. Suppose a learner gets 4 syntactic         G002             Manner: None (0) State: Empty (2)
frames of /glip/, all of the form /S glipped O with        W120
                                                           pour-none
                                                                            Manner: Splashing (2) State: None (0)
                                                                            Person pouring some water into a glass
Z/. This is equivalent to having 4 perfect obser-          G000             Manner: None (0) State: None (0)
vations of fig:[0], which we annotate as X = 0000.         W110
                                                           spray-none
                                                                            Manner: Pouring (1) State: None (0)
                                                                            Person sprays water into a glass
Then the likelihood p(X|V ) and posterior probabil-        G000             Manner: None (0) State: None (0)
ity p(V |X) of the 3 possible hypotheses can be eval-      W130             Manner: Spraying (3) State: None (0)
uated directly via Bayes’ rule:                          where features are ordered as:
        Likelihood p(X|V )    Posterior p(V |X)                   fig, manner-of-motion, change-of-state
        p(X|V = 0) = (.65)4   p(V = 0|X) = .941
        p(X|V = 1) = (.03)4   p(V = 1|X) = .000          for each utterance u and scene possibility s. The
        p(X|V = §) = (.32)4   p(V = §|X) = .059          subscripts on G and W annotate the observation of
                                                         that argument for each of the 3 dimensions.
This is shown below, along with other distributions      We may describe, just as before, how the cross-
of syntactic frames:                                     situational distributional evidence X of N indepen-
                                                         dent scene-utterance pairs:
 Sit Utterances (X)                V =0 V =1 V =§                         X = [(s1 , u1 ), . . . , (sN , uN )]              (8)
  1 4 /S Ved O with Z/ (0000)       .941 .000      .059  yields diﬀerent word-concept mappings p(Hi |X)
  2 4 /S Ved O/ (****)              .292     .292 .416   through independent combination of the two sources
      2 /S Ved O with Z/,
  3 2 /S Ved O into Z/ (0011)       .032     .032 .936   of evidence:          QN
                                                                                  j=1 p(sj |Hi )p(uj |Hi )p(Hi )
      2 /S Ved O/,
  4 2 /S Ved O with Z/ (**00) .769 .000            .230        p(Hi |X) =                                                   (9)
                                                                                              p(X)
      23 /S Ved O with Z/                                For expository purposes, we will consider how the
  5                                 1.00 .000      .000
      10 /S Ved O/                                       learner would rank each of the 6 precise hypotheses,
      23 /S Ved O with Z/                                and will assume they entertain only these:
  6 5 /S Ved O into Z/              .960 .000      .040
      10 /S Ved O/                                                     English Verb Hypothesis Feature
                                                                       pour                Hpour           11*
With only 4 examples, the uncertainty of the value                     spray               Hspray          12*
of the feature V is rapidly reduced (situations 1-4).                  splash             Hsplash          13*
As the number of examples increases (situation 4                       fill                 Hf ill         0*1
                                                                       empty               Hempty          0*2
vs 5), the evidence supports “all-or-none” or “rule-                   move                Hmove           1**
like” behavior, even with a significant number noisy
frames (situation 5 vs 6).                                  The likelihood p(sj |Hi ) for each of the D indepen-
                                                         dent dimensions (D = 3) is:
                                                                                                     D
   Modeling Integrated Syntactic and                                                                Y
                                                                      p(sj = s1 . . . sD |Hi ) =         p(sk |Hi )        (10)
           Semantic Bootstrapping                                                                   k=1
We now integrate the two forms of bootstrapping          where our model for scene observations along the
described above, where given a distribution of both      kth dimension8is:
scenes and syntactic frames, a probability distribu-                        >
                                                                            >   1 ° dk ≤ ifsk = 0, Hik = §
                                                                            <
tion over concepts consistent with both sources of                              ≤           ifsk 6= 0, Hik = §
                                                            p(sk |Hi ) =                                                   (11)
evidence is determined. Consider the following pos-                         >
                                                                            >   1 ° dk ± ifsk = Hik , Hik 6= §
                                                                            :
sible syntactic frames:                                                         ±           ifsk 6= Hi , Hi 6= §
                                                                                                         k    k
    Utterance                        u Attention         We annotate the value of the kth dimension of hy-
    /Glipping!/                    ***         -         pothesis Hi as Hik above. The first two lines model
    /S glipped water from a glass/ 1**        W
    /S glipped water into a glass/ 1**        W
                                                         that when a feature is not valued (Hik = §), then
    /S glipped water/              ***        W          scenes typically have 0 for the kth dimension (d1 =
    /S glipped a glass with water/ 0**        G          2; d2 = 3; d3 = 3), but do not match with probabil-
    /S glipped a glass/            ***        G          ity ≤. That is, observing pouring, spraying, splashing
and perceptually-derived        semantic    features  of manners (s2 = 1, 2, or 3), and observing filling, emp-
scenes:                                                  tying, or breaking change-of-states (s3 = 1, 2, or 3)

   Situation             Scene s                       Utterance u              Hpour Hspray Hsplash Hf ill Hempty Hmove
        1     pour-fill     {G001 , W110 }/S glipped water into a glass/ (1**) .889 .008 .008 .000 .000 .093
        2     pour-fill     {G001 , W110 }/S glipped glass with water/ (0**) .000 .000 .000 .990 .009 .000
        3     pour-fill     {G001 , W110 }/Glipping!/                     (***) .468 .004 .004 .468 .004 .049
        4     none                         /S glipped water into a glass/ (1**) .246 .246 .246 .004 .004 .254
        5     none                         /S glipped glass with water/ (0**) .007 .007 .007 .485 .485 .007
        6     none                         /Glipping!/                    (***) .166 .166 .166 .166 .166 .170
        7     pour-fill     {G001 , W110 }/Glipping!/                     (***)
              pour-empty {G002 , W110 }/S glipped water from a glass/(1**) .998 .000 .000 .000 .000 .001
              pour-none {G000 , W110 }/S glipped water/                   (***)
        8     pour-fill     {G001 , W110 }/Glipping!/                     (***)
              splash-fill   {G001 , W120 }/S glipped a glass with water/(0**) .000 .000 .000 .999 .000 .000
              spray-fill    {G001 , W100 }/S glipped a glass/             (***)
        9     pour-fill     {G001 , W110 }/Glipping!/                     (***)
              splash-empty{G001 , W120 }/S glipped water/                 (***) .064 .064 .064 .000 .000 .808
              spray-none {G001 , W100 }/S glipped water/                  (***)
     Figure 1: Word-concept mapping p(Hi |X), given scene-utterance evidence X of a novel verb, /glip/
is far less likely than observing no manner of motion            as equivalent to /S is glipping Z1/ with probability
(s2 = 0) or change of state (s3 = 0) at all. Since               p(z1 ) = .5 and /S is glipping Z2/ with probability
observing a diﬀerent value sj 6= 0 is unlikely to have           p(z2 ) = .5. For simplicity, we assume A = 2 where
occurred by accident, it may be an important feature             Z1 is water, Z2 is the glass – but further referential
to the concept. The second two lines of (11) model               uncertainty can be modeled with higher A. Because
that if a feature is valued (Hik 6= §), then scenes typ-         of the conditioning on each of A possibilities, this
ically match that feature in value, but do not match             yields a less certain word-concept mapping.
with probability ±. That is, for example, given hy-                 In situation 4 through 6, the same syntactic
pothesis Hpour , then most of the scenes will contain            frames are provided as in situations 1 through 3,
pouring in them. In our examples, ≤ = .1, ± = .01;               but without the scene information. When some syn-
qualitatively, results are not sensitive to changes in           tactic information is provided by the frame (situa-
these values.                                                    tion 4, /S is glipping water into a glass/), then the
   The output of our model is shown in Figure 1.                 manner-of-motion locative verbs are preferred over
   Suppose, as in Situation 1, a learner is given a              the change-of-state locative verbs, but no diﬀerenti-
single scene-utterance pair (pour-fill, /S glipped wa-           ation is possible without the scenes. Likewise, when
ter into the glass/): X = [(s1 = {G110 , W110 }, u1 =            the frame provides the opposite cue (situation 5, /S
1 § §, W )], and we wish to compute p(Hi |X) for all             is glipping a glass with water/), the opposite pref-
Hi 2 H. We assume the learner can attend to the                  erence is achieved, again with no diﬀerentiation be-
argument so as to extract relevant features from the             tween possible change-of-state verb concepts. When
scene. Given the scene pour-fill paired with utter-              zero syntactic information is available (situation 6,
ance /S glipped water into a glass/, our Bayesian                /Glipping!/), all hypotheses prove equally likely.
model places high weight on Hpour .                                 Whereas in situation 3 the verb-concept mapping
   In Situation 2, the scene is the same, but now the            was ambiguous, primarily between Hpour and Hf ill ,
syntax /S glipped a glass with water/ provides the               in situation 7 and 8, learners are provided 2 addi-
learner with the information to attend not to the                tional examples to disambiguate. Both the scenes
water’s manner-of-motion but to the glass’ change                and syntactic frames in situation 7 support Hpour ,
of state. Given X = [(s1 = {G110 , W110 }, u1 =                  while in situation 8 the scenes and syntactic frames
0 § §, G)] our model weights Hf ill heavily.                     support Hf ill .
   In Situation 3, the scene is the same, but now                   Finally, in situation 9, 2 diﬀerent scene-utterance
the syntax /Glipping!/ gives the learner less infor-             pairs primarily support the “superordinate” con-
mation, since the argument in the scene that the                 cept Hmove , and not any “subordinate” manner-of-
speaker may be referring to is unknown: X = [(s1 =               motion concept Hpour , Hsplash , or Hspray .
{G110 , W110 }, u1 = ° ° °)] If there are A argu-
ments in the scene, the speaker must have had a                                       Discussion
particular argument z in mind. The learner must                  The reason why our analysis is able to infer so much
condition on all the possibilities of z:                         from so little evidence is because so much is embed-
                           XA                                    ded in the given knowledge sources:
             p(sj |Hi ) =      p(sj |Hi , za )p(za )     (12)
                           a=1                                   • the structure of the hypothesis space H. Our ex-
If learners consider all arguments equally salient                  amples contained a small number of feature di-
(p(zi ) = A1 ) then this eﬀectively models /Glipping!/              mensions and their possible values, but these may

   be specified by interfaces to perceptual, motor,     Gleitman 2002). Qualitatively, our models’ per-
   memory, or other “theory” representations. If so,    formance matches the preferences of child learners,
   whether these are innate or acquired are condi-      modeling their acquisition from as little as one ex-
   tional on their source.                              ample.
•  priors p(Hi ) on hypotheses in H. We used equal         Our use of statistics does not imply any commit-
   priors, but updating p(Hi ) based on language in-    ment to radical empiricism. Much prior knowledge
   put is natural. In the verbal domain, such phe-      is stipulated: the structure of the hypothesis space,
   monena are commonly observed (e.g. manner vs.        the priors on hypotheses, and the likelihood of scene-
   path, tight/loose-fit biases).                       utterance pairs given the hypotheses. It is not spec-
                                                        ified whether these stipulations are innate or them-
•  likelihood of scenes s given the word concept        selves learnable. Linguistics and lexical semantics
   p(sj |Hi ). We stipulated static values of ≤ and ±,  provide detailed theories of a much larger syntactic
   but this can be acquired from observation.           and semantic hypothesis space, and little prevents
•  perfect knowledge of the features of the comple-     their inclusion in this framework.
   ment. We made this simplifying assumption to
   illustrate the essential elements of our model, but  Acknowledgements
   learners must acquire these features in parallel.    Many thanks to Robert C. Berwick for motivating and
•  the likelihood of agreement, p(C|V ), between a      supporting this work. Jesse Snedeker and Josh Tenen-
   feature of a novel verb V and its complement C.      baum provided many stimulating discussions. This work
   We speculate that there is suﬃcient structure in     was funded by a provost grant to Prof. Joel Moses.
   partially learned words so as to acquire the struc-  References
   ture in the joint distribution of feature values.
                                                        Desai, R. (2001). Bootstrapping in Miniature Language
                                                           Acquisition. In Proceedings of the Fourth Interna-
This richness of knowledge is in contrast to the           tional Conference on Cognitive Modeling, pp. 61-66.
models employed by Regier et al (2001) and De-             Hillsdale, NJ: Erlbaum.
sai (2001), who train connectionist neural networks     Harley, H. and Noyer, R. (2000) Licensing in the non-
so as to learn the word-scene associations for ad-         lexicalist lexicon. In Bert Peeters, (Ed.) The Lexicon-
jectives/ nouns and verbs respectively. The high           Encyclopedia Interface, Amsterdam: Elsevier Press.
dimensionality of their models forces the need for      Fisher, C., Hall, D., Rakowitz, S., and Gleitman, L.
thousands of training trials, and the interpretation       (1994). When it is better to receive than to give:
of the weights is notoriously diﬃcult. The assump-         Syntactic and conceptual constraints on vocabulary
                                                           growth. Lingua, 92:333-375.
tions behind these models are not justified by these
authors. In contrast, our Bayesian approach makes       Gleitman, L. (1990) The structural sources of verb mean-
                                                           ings. Language Acquisition, 1990, 1:3-55.
the hypotheses, priors, and likelihoods explicit, hold-
ing this structure to be central.                       Levin, B. (1993) English Verb Classes and Alternations:
                                                           A Preliminary Investigation, University of Chicago
   Siskind (1996) views lexical acquisition as con-        Press, Chicago, IL.
straint satisfaction, and oﬀers a robust algorithm
                                                        Naigles, L. (1990). Children use syntax to learn verb
where the mapping between input and hypothesis             meanings. Journal of Child Language, 117:357-374.
space is accomplished by pruning hypotheses that
                                                        Nomura, N., Jones, D.A. and Berwick, R.C. (1994) An
do not occur cross-situationally. Provided an ideal-       Architecture for a Universal Lexicon: A Case Study
ized tokenization of the world, the algorithm does         on Shared Syntactic Information in Japanese, Hindi,
not need a large number of examples. However,              Bengali, Greek, and English. COLING 1994, 243-249.
Siskind’s model does not yield any form of pref-        Pinker, S. (1989) Learnability and Cognition. MIT Press,
erence between diﬀerent concepts, which is espe-           Cambridge, MA.
cially important when two or more concepts may          Regier et al (2001). The Emergence of Words. In Pro-
be equally constrained by the data. We have shown          ceedings of the 23rd Annual Conference of the Cogni-
how a Bayesian analysis explicitly yields preferences      tive Science Society.
between concepts in the posterior probability distri-   Siskind, J. (1996) A Computational Study of Cross-
bution p(Hi |X).                                           Situational Techniques for Learning Word-to-Meaning
   Tenenbaum and Xu (2000) take the important              Mappings. Cognition, 61:39- 91
step of putting word learning in the Bayesian frame-    Snedeker, J. and Gleitman, L. (2002) Why it is hard to
work that we adopt here, showing how noun learn-           label our concepts. In G. Hall and S. Waxman (eds.),
                                                           Weaving a Lexicon, Cambridge, MA: MIT Press.
ing can occur with a small number of examples in a
continuous-variable input space.                        Tenenbaum, J.B. and Xu, F. (2000) Word learning as
                                                           Bayesian inference. In Proceedings of the 22nd Annual
   Crucially, however, the above models ignore the         Conference of the Cognitive Science Society (pp. 517-
constraining role of syntax, despite considerable ev-      522)
idence that children use syntax to guide their verb-
concept hypothesis space (Gleitman 1990, Naigles
1990, Naigles 1994, Fisher et al 1994, Snedeker and

