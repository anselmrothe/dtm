UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Four Problems with Extracting Human Semantics from Large Text Corpora
Permalink
https://escholarship.org/uc/item/4c51156n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 24(24)
Authors
French, Robert M
Labiouse, Christophe
Publication Date
2002-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                         Four Problems with Extracting
                              Human Semantics from Large Text Corpora
                                        Robert M. French and Christophe Labiouse
                                        Quantitative Psychology and Cognitive Science
                                    Psychology Department, University of Liege, Belgium
                                                  {rfrench, clabiouse}@ulg.ac.be
                            Abstract
   We present four problems that will have to be overcome
                                                                              The intrinsic deformability of
   by text co-occurrence programs in order for them to be                              semantic space
   able to capture human-like semantics. These problems             We will take issue with one of the main principles
   are: the intrinsic deformability of semantic space, the          underlying LSA (Landauer and Dumais, 1997), HAL
   inability to detect co-occurrences of (esp. distal) abstract
                                                                    (Lund & Burgess, 1996) and other programs based on
   structures, their lack of essential world knowledge,
   which humans acquire through learning or direct
                                                                    lexical analysis of large corpora – namely, that “The
   experience with the world and their assumption of the            meaning of a word can be thought of as a location in
   atomic nature of words. By looking at a number of very           semantic space and the dimensionality of that space and
   simple questions, based in part on how humans do                 the location of any word within it can be recovered from
   analogy-making, we show just how far one of the best of          estimates of the distance between word pairs.” (Fletcher
   these programs is from being able to capture real                & Linzie, 1998). The implication is that words have
   semantics.                                                       stable, fixed locations in semantic space. While this is
                                                                    obviously not entirely false, this principle overlooks the
                        Introduction                                fact that these locations in semantic space are highly
“You shall know a word,” wrote J. R. Firth in 1957, “by             context dependent. They not only can, but must be able
the company it keeps.” This idea, in one form or                    to move considerably in semantic space depending on
another, underlies the statistical study of the co-                 the context in which they are to be used.
occurrence of lexical items in large text corpora. This                 Consider a very simple example. A “claw hammer”
burgeoning field of research has been made possible to              would, under most circumstances, be close in semantic
a large extent by the ready availability of vast databases          space to terms like “ball-peen hammer,” “hit,” “pound,”
of text that can be automatically scanned by computer.              “nail,” “saw” and, even, “club.” However, if, while
While we certainly do not dispute the value of the                  nailing a floor, you suddenly have a back itch, the
statistical study of large text corpora, we take issue with         “claw” part of the hammer will likely become much
the claim that lexical co-occurrence alone can capture              more salient as a back-scratcher, rather than a nail-
real-world semantics. We focus on four main problems                remover. Your realization that you can use the hammer
with co-occurrence analysis programs:                               as a back-scratcher temporarily moves the object in
  • they do not take into account the intrinsic                     semantic space much closer to “back-scratcher,” “itch,”
    deformability of semantic space due to context-                 etc., than when it is perceived only as an object with
    dependence;                                                     which one can drive in nails. This “relocation” of the
  • the cannot detect co-occurrences of abstract                    meaning of a word/concept in semantic space based on
    structures, especially when they are highly distal;             context is at the very heart of analogy-making, of
  • they lack of essential world knowledge, which                   perceiving one object as an instance of another class of
    humans acquire through learning or direct                       objects (Chalmers, et al, 1992; Hofstadter, 1995). It is
    experience with the world;                                      therefore essential to any algorithm that claims to be
  • they assume that words are “atomic” entities.                   able to automatically extract word meaning from very
These issues are ones that will have to be effectively              large text corpora And it is precisely this ability to
dealt with by text analysis techniques in order for them            relocate in semantic space in a context-dependent
to capture even elementary human semantics. These                   manner that is currently beyond the reach of all co-
points will be examined in the context of human                     occurrence techniques.
analogy-making.                                                         In short, while co-occurrence techniques may
    The remainder of this paper is organized as follows.            plausibly situate a word in semantic space with respect
We begin by discussing the relationship between                     to its average usage, this is not sufficient to capture the
analogy-making and concept meaning and show why                     context-dependent shifts in word meaning required to
current co-occurrence programs would have so much                   understand even the simplest analogies on which so
difficulty with this broader view of concept meaning.               much of our cognition is based. For this we need to
We then consider one of the best recent co-occurrence               somehow extract, at the very least, abstract relational
programs, PMI-IR (Turney, 2001a) and show just how                  information concerning the word.
far this program is from being able to plausibly respond
(i.e., in a human-like manner) to even the simplest
possible analogies.

    Detection of (distal) abstract structures                     • Flugly as the name of an accountant in a W. C.
                                                                    Fields movie.”
Acquiring the semantics of a particular word allows us
                                                             Humans, of course, can do these two ratings without
to rate the quality of associations between that word and
                                                             difficulty: Flugly is a decidedly lousy name for a
other words. To plausibly claim that a program has
                                                             glamorous Hollywood actress and a fine name for an
acquired, or even partially acquired, the semantics of a
                                                             accountant in a W. C. Fields movie or a teddy bear. But
word means that it should give word-association ratings
                                                             how do we “know” this, since you have never seen the
that are at least approximately similar to those given by
                                                             word Flugly before? You know, at least in part, that
humans (French, 1990). We will use this rating
                                                             Flugly doesn’t work for the name of a glamorous
technique to judge the performance of text co-
                                                             actress because of its component parts (French, 1990).
occurrence programs.
                                                             In particular, it contains an unpleasing-to-the-ear
    There are (at least) two different bases for these
                                                             guttural "g,” to say nothing of the syllable “ug” or the
associations, even if this distinction is not always easy
                                                             entire word "ugly." Similarly, we rate it as a good name
to characterize (Chalmers, French, & Hofstadter, 1992).
                                                             for an accountant in a W. C. Fields’ movie because, in
To say, “John is a real beanstalk,” refers to largely
                                                             our mind’s ear, we hear him pronouncing the name as
“surface” attributes of John and beanstalks ⎯ namely,
                                                             “Flugleeee.” This requires phonetic information
they are both tall and thin. On the other hand, when we
                                                             acquired by having heard W. C. Fields’ unique manner
say “John is a real wolf with the ladies,” we don’t mean
                                                             of speaking (or having heard others imitating this
John grows long gray hair around women and bites
                                                             manner) and by the fact that various components of
them, but rather that his relation with women is socially
                                                             Flugly, namely, “ly,” can be transformed into a
predatory, analogous to a wolf’s relation of physical
                                                             drawling “leeee.”
predation with its prey. The first analogy is largely
                                                                 The point is that words contain parts that contain
attributional, based essentially on common surface
                                                             crucial information that contributes to the overall
features (in this case, the attributes “tall” and “thin”) of
                                                             meaning of the word. Co-occurrence programs are
John and beanstalks, whereas the latter analogy is
                                                             currently insensitive to this information. And it is not
primarily relational, based on a mapping between
                                                             clear that by extending their analyses to the letter or
John’s behavioral interactions with women and wolves
                                                             syllable level that i) there would not be a problem of
behavioral interactions with prey. The first kind of
                                                             combinatorial explosion and ii) that this would be an
association can be captured by co-occurrence
                                                             appropriate way to acquire this information.
techniques, whereas the latter — the basis of virtually
all deep analogy-making (Gentner, 1983) — is still well
beyond the reach of these techniques.                                                PMI-IR
                                                             In the examples that follow, we will consider the
      Incorporating semantic information                     performance of one recent program, PMI-IR (Turney,
                                                             2001a, b), that, according to its author, outperforms all
An equally important difficulty involves the
                                                             other current programs on the most widely used
unavailability to these programs’ of crucial semantic
                                                             benchmark for programs that attempt to extract word
information that cannot be acquired merely by
                                                             meaning from large text corpora. This benchmark is
examining word co-occurrences. In two of the examples
                                                             their performance on the standard synonym selection
below this lack of crucial contextual knowledge ⎯ that       tasks that are part of the Test Of English as a Foreign
fathers are always male in one example, and the fact         Language (TOEFL) and the test of English as a Second
that there is an undeclared war going on between the         Language (ESL).
Israelis and the Palestinians in the other ⎯ causes the          The co-occurrence technique used by PMI-IR is one
particular text co-occurrence analysis program under         of a family of “Pointwise Mutual Information” (PMI)
consideration to fail completely in responding to the        techniques developed by Church & Hanks (1989) and
simplest questions involving word meaning. Humans            Church et al. (1991). In order to calculate the
acquire this information through direct experience with      conditional probability scores on which it bases its
the world or through explicit learning, whereas these        choice of the correct synonym, the program queries 350
programs currently have no way of acquiring it. The          million pages of English text indexed by the AltaVista
point is that, when making judgments about word              search engine. The most sophisticated version of PMI-
meaning, people ⎯ unlike co-occurrence programs ⎯            IR is able to make use of local (proximal) context in
make use of a wealth of relational and semantic              order to correctly answer questions such as, “Every year
information that is unrelated to word co-occurrence.         in the early spring farmers [tap] maple syrup from their
                                                             trees (drain; boil; knock; rap).” As Peter Turney, the
           Words are not atomic entities                     author of PMI-IR, points out, “the problem word tap,
Consider an example of a “subcognitive” question from        out of context, might seem to best match the choice
French (1988, 1990) involving the rating of a                words knock or rap, but the context maple syrup makes
neologism. “On a scale of 1 (awful) to 10 (excellent)        drain a better match for tap” (Turney, 2001b). The
please rate:                                                 program factors in the context provided by “maple
                                                             syrup” to correctly answer this question.
     • Flugly as the name of glamorous Hollywood                 The program does, indeed, perform impressively on
        actress,                                             the synonym recognition task. According to Turney, the

program produced the following results on the standard
                                                            10
TOEFL and ESL synonym recognition task:                              Humans
                                                                     PMI-IR
  “The task of synonym recognition is, given a               8
  problem word and a set of alternative words,               6
  choose the member from the set of alternative              4
  words that is most similar in meaning to the               2
  problem word. PMI-IR has been evaluated using
  80 synonym recognition questions from the Test of          0
                                                                                          kangaroos                                    telephones                   computers
                                                                                                                                                                                bastards
                                                                                                      stones
                                                                         robins                                                                              dogs
                                                                                                               horses
                                                                                                                                                                                           slimeballs
                                                                                                                                                    sharks
  English as a Foreign Language (TOEFL) and 50
                                                                  fish            birds                                 flies   cats
  synonym recognition questions from a collection
  of tests for students of English as a Second
  Language (ESL). On both tests, PMI-IR scores                   Figure 1. A comparison of PMI-IR and human
  74% . . . For comparison, the average score on the             data. The two profiles are clearly very different.
  80 TOEFL questions, for a large sample of
  applicants to US colleges from non-English                We also found that PMI-IR gave an extremely high
  speaking countries, was 64.5% (Landauer and               rating to “Lawyers as children,” higher, in fact, than any
  Dumais, 1997). . . . Latent Semantic Analysis             of the choices tested in Figure 1. Clearly, something is
  (LSA), another statistical technique, scores 64.4%        wrong here: first, lawyers cannot even be children
  on the 80 TOEFL questions (Landauer and                   (something which PMI-IR has no way of knowing) and,
  Dumais, 1997).”                                           even metaphorically, it just doesn’t seem right to us.
                  Three examples                            Rating the plausibility of Jewish/Palestinian
In what follows we use a word-rating technique from         ministers’ names
French (1988, 1990) and similar to standard similarity
judgment techniques used to study how word meanings         Next we used PMI-IR to judge how good various first
are represented (see, for example, Rips, Shoben, &          names would be for an Israeli or a Palestinian minister.
Smith, 1973). The key idea is that these simple             We chose ten traditional Jewish names (Uri, Ariel,
questions require non-local context for their answers       Moshe, Yitzhak, Yehudi, David, Samuel, Benjamin,
(French & Labiouse, 2001).                                  Shimon, and Zeev) and nine traditional Arab names
                                                            (Saddam, Usama, Ahmed, Mohammed, Salah, Amin,
Rating lawyers                                              Khalil, Ashrawi, and Yasser). We asked two separate
                                                            questions, each processed independently by the
                                                            program. The first was “How good is X [one of the
Our first example involves the rating of lawyers as
                                                            names, e.g., Ahmed) as the name of an Israeli minister?”
various other entities.
                                                            All nineteen names were rated for this question. Then a
   “Rate on a scale of 1 (terrible) to 10 (excellent)
                                                            second question was asked: “How good is X [again, one
  rate lawyers as: horses, fish, telephones, stones,
                                                            of the 19 names] as the name of a Palestinian minister?”
  sharks, cats, flies, birds, slimeballs, kangaroos,
                                                            All 19 names were rated for this second question. We
  robins, dogs, and bastards.”
                                                            then compared the ratings for each name for the two
                                                            questions to determine their degree of correlation.
We applied the PMI-IR search technique described in
                                                                Once again, PMI-IR fails rather spectacularly: for
Turney (2001b) using the Alta-Vista search engine and
                                                            example, it considers Yasser to be almost as good a first
found that it gave the lowest (i.e., poorest) ratings to
                                                            name for an Israeli minister as for a Palestinian
“Lawyers as slimeballs” (1.06) and “Lawyers as
                                                            minister! Similarly, Ariel is judged to be the best name,
bastards” (1.15), the latter being roughly equivalent
                                                            out of all ten Jewish names and all nine Arab names, for
PMI-IR’s rating of “Lawyers as kangaroos” (1.17)! We
                                                            either an Israeli minister or a Palestinian minister . The
then asked a group of 26 undergraduates at Willamette
                                                            results for the other names are shown in Figure 2.
University (Oregon) to also do these ratings. These
                                                                Why does the program rate Yasser as a highly
results (Figure 1) are much more in line with one might
                                                            probable name for an Israeli minister and Ariel as
expect for humans with a clear understanding of the
                                                            highly probable for a Palestinian minister? The reason is
semantics of the word “lawyer” — namely, lawyers are
                                                            simple: Because the program is concerned only with the
judged (fairly or unfairly) to be most like slimeballs,
                                                            co-occurrence of words, in this case the words Yasser,
bastards, dogs and sharks, and least like telephones,
                                                            Ariel, Israeli, Palestinian and minister. The fact that
kangaroos, and birds. PMI-IR, on the other hand, judges
                                                            Israel and Palestine are currently waging an undeclared
lawyers to be most like computers, cats, and telephones
                                                            war is known to PMI-IR only through higher than
and least like slimeballs, bastards, kangaroos and
                                                            normal co-occurrences of war-related words and words
robins. Lawyers as sharks or fish are judged to be
                                                            like Israel, Palestine, intifada, etc. It knows nothing
equally bad. A comparison of human vs. PMI-IR results
                                                            about wars, about their causes and effects, about their
can be seen in Figure 1. In short, it is amply clear that
                                                            effects on societies and individuals in those societies,
even for this straightforward question about lawyers, the
                                                            about hatred, about destruction, about refugees, about
human semantics of “lawyer” does not even vaguely
                                                            Israel, about Palestine, etc. ad infinitum. It knows only
resemble the semantics extracted by PMI-IR.
                                                            that sometimes these words co-occur with higher

frequency than others. The complete absence in PMI-IR                                                                                                                    names for the question: “How good is X as the name of
of this deep relational structure in which the words that                                                                                                                a father?” Not so PMI-IR. The program concludes that
it encounters (and concepts these words represent) are                                                                                                                   “John” is the best name out of all twenty names for a
embedded is precisely why PMI-IR fails to                                                                                                                                father and for a mother. It rates “Mary” as being a very
convincingly answer even the simplest questions that                                                                                                                     good name for a father or for a mother. Ditto for the
require deeper relational structure and knowledge to be                                                                                                                  name “William.” As in the above example, the
answered plausibly.                                                                                                                                                      appropriateness of a particular name for a father
    So, to return to our example, in the context of the                                                                                                                  correlates essentially perfectly (+0.99) with the
current crisis in the Middle East and of cultural                                                                                                                        appropriateness of that same name for a mother! (Figure
specificities of first-names, good names for Palestinian                                                                                                                 3)
ministers should be perceived as bad names for Israeli                                                                                                                       Once again, the program fails because extracting co-
ministers and vice-versa. PMI-IR is, as we have said,                                                                                                                    occurrences of words in a large corpus of text is simply
unaware of the cultural context surrounding these                                                                                                                        not good enough to answer questions that require
questions. Specifically, PMI-IR is ignorant of the                                                                                                                       abstract contextual knowledge or experience. Again, the
obvious (to us) cultural fact that some first names are                                                                                                                  problem is that PMI-IR has neither abstract rules nor
typically Jewish while others are typically Arab and the                                                                                                                 world experience that it can rely on. And since, in any
relation of that cultural fact to the currently perceived                                                                                                                text where the word “father” occurs, the word “mother”
inappropriateness of Palestinian ministers with Jewish                                                                                                                   will generally not be far away, PMI-IR fails completely
names and vice-versa. So, according to PMI-IR, the                                                                                                                       on this simple rating task.
appropriateness of a name for a Palestinian minister
correlates almost perfectly (+0.98) with the                                                                                                                                                                                                                                                                  Father
                                                                                                                                                                         10
appropriateness of the same name for an Israeli minister                                                                                                                 8                                                                                                                                    Mother
(See Figure 2).                                                                                                                                                          6
                                                                                                                                                                         4
10                                                                                                         Israeli minister                                              2
 8                                                                                                         Palestinian minister                                          0
 6
                                                                                                                                                                                                                                                                  Barbara
                                                                                                                                                                                                        Peter   Robert
                                                                                                                                                                                                                                Gary                                                                                     Karen
                                                                                                                                                                                                                                                                                                                                                     Dorothy
                                                                                                                                                                              John                                                     Steve
                                                                                                                                                                                                                                                                                   Patricia
                                                                                                                                                                                                                                                                                              Linda   Susan
                                                                                                                                                                                                                                                                                                              Jennifer
                                                                                                                                                                                               Stuart                                          Albert                       Mary                                                 Nancy
                                                                                                                                                                                                                                                                                                                                         Elizabeth
                                                                                                                                                                                     William                                                            Michael
                                                                                                                                                                                                                         Jack
 4
 2
 0
     Uri
                                     Yehudi   David
                                                                                          Saddam
                                                                                                                   Mohammed
                                                                                                                              Salah
                                                      Samuel
                                                                                   Zeev
                   Moshe
                                                               Benjamin
                                                                          Shimon                   Usama   Ahmed                                      Ashrawi
           Ariel
                                                                                                                                                                Yasser
                                                                                                                                      Amin   Khalil
                           Yitzhak
                                                                                                                                                                              Figure 3. Two questions were asked: “How good
                                                                                                                                                                              is X as the name of a father?” and “How good is X
                                                                                                                                                                              as the name of a mother?” Lacking all context
                                                                                                                                                                              about what “fathers” and “mothers” actually are,
  Figure 2. For two separate questions: “How good
                                                                                                                                                                              PMI-IR produces an almost a perfect correlation
  is X as the name of an Israeli minister?” and “How
                                                                                                                                                                              between the appropriateness of the names, male or
  good is X as the name of a Palestinian minister?”
                                                                                                                                                                              female, for a father or for a mother!
  PMI-IR produces an almost a perfect correlation
  between the appropriateness of a given name as
  either that of an Israeli or a Palestinian minister.                                                                                                                   Why PMI-IR works so well on the synonym
                                                                                                                                                                                     selection task
Rating names of mothers and fathers                                                                                                                                      Given PMI-IR’s poor performance on the simple
                                                                                                                                                                         examples above, how could it be so good in the
Finally, we decided to pick an example, simple in the                                                                                                                    synonym selection task, a task that would seem to
extreme and far removed from politics and current                                                                                                                        require      a    relatively     sophisticated   semantic
events, that relied on a very specific piece of contextual                                                                                                               understanding of words in order to be done
information that would be available to all humans but                                                                                                                    successfully? In what follows we will briefly examine
not to a word co-occurrence analysis. We compared                                                                                                                        why this program, and other similar programs, most
PMI-IR’s answers to the following two questions: How                                                                                                                     notably LSA, are able to perform so well on this task, in
good is X [a first name] as the name of a father?” and                                                                                                                   spite of their inability to do the examples above.
“How good is X [the same first name as in the first                                                                                                                          The author of PMI-IR claims that his program can
question] as the name of a mother?” For each question                                                                                                                    do better on the TOEFL and ESL synonym tests than
we asked PMI-IR to rate ten very common men’s names                                                                                                                      any other current computer program (Turney, 2001a,b).
(John, William, Stuart, Peter, Robert, Jack, Gary, Steve,                                                                                                                This is believable and reasonable. Turney illustrates
Albert, and Michael) and ten very common women’s                                                                                                                         PMI-IR’s performance on the synonym-finding task
names (Barbara, Mary, Patricia, Linda, Susan, Jennifer,                                                                                                                  with the word levy (as in “to levy taxes”). Four choices
Karen, Nancy, Elizabeth, and Dorothy).                                                                                                                                   are proposed — imposed, believed, requested,
    When judging the appropriateness of a particular                                                                                                                     correlated — and the program chooses one of them as
name as the name of a father (or mother), humans partly                                                                                                                  the best synonym based on how often that word is close
rely on a simple fact that the program does not have —                                                                                                                   to “levy” in many Web pages. The reason for PMI-IR’s
namely, that fathers are invariably men, while mothers                                                                                                                   success does, indeed, reflect the semantics of the word
are invariably women. Consequently, humans will                                                                                                                          under consideration, but is tied most directly to the
necessarily rate women’s names lower than men’s                                                                                                                          stylistic reasons for which we use synonyms — viz., so

as not to repeat the same word too often in a given text    school safety poster was eliminated from the city
or, especially, in the same paragraph. This purely          competition, etc.
stylistic constraint imposes the proximity of synonyms,         In other words, describing one word in terms of
which is detected by PMI-IR.                                another usually involves much more than the above
    Assume you are writing an article to be put on a        kind of “blunder-mistake-mishap-slip” synonym
Web page about some blunder that occurred. In               searching. It involves mentally placing the both words
describing this blunder, you are aware that it is bad style in a variety of relational as well as attributional
to repeat the word blunder over and over again in your      contexts (that can shift fluidly) and converging on a
text, so you resort to synonyms, such as failure, mishap,   context that fits both words (for detailed discussions of
mistake, slip, bungle, mess, and so on. This obviously      this see: Chalmers, French, & Hofstadter, 1992;
produces co-occurrences of blunder and mistake, of          Hofstadter, 1995; etc.) If both words fit that context
blunder and slip, etc., and this is precisely what PMI-IR   very well, then we give the association a high rating.
detects. A blunder IS (to a first approximation) a          The more difficult it is to converge on an appropriate
mistake, which IS a slip, etc. These words all have         context for both words, the lower the rating.
approximately the same dictionary definitions. In other         PMI-IR, however, is incapable of extracting these
words, the features that describe them are largely          all-important relational and contextual characteristics of
identical. This is what we called above attributional       situations. Specifically, for questions of the form, “Rate
similarity. The point is that we can expect                 X as a Y,” the program is incapable of grasping the
attributionally similar words, if only for stylistic        relational structure in which each of the words is
reasons, to occur close to one another in a text. Hence,    embedded and then of mapping those two structures
PMI-IR’s excellent performance on this task.                onto one another in order to determine the relational
    This technique can, indeed, incorporate proximal        similarity of the words.
context, as in the example of the word tap in the context
of “maple syrup.” But most analogical association                                  Conclusions
involves abstract context derived from examples that, if
                                                            While we acknowledge the impressive performance on
they exist at all in the text corpus, may well be
                                                            certain lexical tasks of programs that employ co-
separated by millions of pages from the word under
                                                            occurrence analyses on large text corpora, our
consideration. It is an open question in the field of
                                                            contention is that these programs lack the capabilities
computational analogy-making as to how this abstract
                                                            necessary to acquire real (i.e., human) semantics. This
relational structure might be stored and indexed fluidly
                                                            paper must not be read as a criticism of these methods
enough to be accessible for later retrieval in a wide
                                                            per se, but rather as an incentive for researchers to
variety of contexts (see Chalmers, French, &
                                                            develop new techniques that can incorporate more of
Hofstadter, 1992, for a detailed discussion of this issue),
                                                            the mechanisms by which we humans acquire
but one thing is clear: it is not accessible to programs
                                                            semantics. These requirements go well beyond the
that rely only on local word co-occurrence to produce
                                                            often-cited problems of the lack of syntactic knowledge
their semantics.
                                                            (Perfetti, 1998) and conceptual disambiguation
    And, to be fair, this is one way in which humans
                                                            (Landauer & Dumas, 1997). We have pointed to four
learn attributionally similar words/concepts. But there
                                                            problem areas for these programs, areas in which we
is much more to “semantic similarity” than surface
                                                            believe future research should be focused. These areas
similarity.
                                                            are i) the ability to cope with the context-dependent
    To reiterate, relationally (or metaphorically) similar
                                                            deformability of semantic space, ii) the detection of co-
words require a great deal more than the detection of
                                                            occurrences of abstract structures, especially similar,
attributional similarity and physically proximal context.
                                                            but distal, abstract structures, iii) the means of providing
Consider rating a banana split as medicine (French,
                                                            the programs with essential world knowledge, and iv)
1988, 1990). The number of times that these two items
                                                            the elimination of the assumption of words as “atomic”
will occur together in any text anywhere is now, and
                                                            entities. In other words, we maintain that to know a
will forever be, infinitesimally small compared to the
                                                            word in a manner even approximately equivalent to how
other associations involving banana splits or medicine.
                                                            we humans know it, requires far more than merely
For programs that extract semantics only from text
                                                            knowing the “company it keeps.”
corpora this poses a serious problem, referred to as the
                                                                In short, while the area of text analysis of large
problem of data sparseness (Dagan et al., 1994). But the
                                                            corpora is a fascinating and promising one, we believe
problem is unavoidable. Of course the number of Web
                                                            that in order for real, human semantics to emerge from
pages containing the terms “banana split” and
                                                            these techniques, the problems raised in this paper will
“medicine” will be vanishing small because is it not a
                                                            have to be squarely confronted and overcome.
common association at all, but it remains a perfectly
valid, readily understandable one that we can judge
without difficulty because we understand it in relation                       Acknowledgments
to our experience with the world, i.e., to facts like the   This work was supported in part by grant HPRN-CT-
doctor bringing us a bowl of ice-cream after we have        1999-00065 from the European Commission.
had our tonsils out, with our mother taking us for a        Christophe Labiouse is supported by a Belgian NFSR
sundae to pick up our spirits when our junior high          Research Fellowship. The authors would also like to
                                                            thank Jim Friedrich at Willamette University, Oregon,

for his help in conducting the informal survey cited in      relations. Journal of Verbal Learning and Verbal
the text.                                                    Behavior, 12, 1-20.
                                                          Turney, P.D. (2001a). Mining the Web for synonyms:
                       References                            PMI-IR versus LSA on TOEFL. Proceedings of the
                                                             Twelfth European Conference on Machine Learning
Chalmers, D. J., French, R. M. and Hofstadter, D. R.
                                                             (ECML-2001), in press.
    (1992). High-level Perception, Representation, and
                                                          Turney, P. D. (2001b). Answering subcognitive Turing
    Analogy: A Critique of Artificial Intelligence
                                                             Test questions: A reply to French. J. of Experimental
    Methodology. J. of Experimental and Theoretical
                                                             and Theoretical Artificial Intelligence.
    and Artificial Intelligence, 4(3), 185-211.
                                                          Yarowsky, D. (1992). Word sense disambiguation using
Church, K.W., and Hanks, P. (1989). Word association
                                                             statistical models of Roget’s categories trained on
    norms, mutual information and lexicography. In
                                                             large corpora. In Proceedings of the 14th
    Proceedings of the 27th Annual Conference of the
                                                             International Conference on Computational
    Association of Computational Linguistics, pp. 76-83.
                                                             Linguistics (COLING-92), 454-46.
Church, K.W., Gale, W., Hanks, P., and Hindle, D.
    (1991). Using statistics in lexical analysis. In Uri
    Zernik (ed.), Lexical Acquisition: Exploiting On-
    Line Resources to Build a Lexicon. New Jersey:
    Lawrence Erlbaum, pp. 115-164.
Dagan, I., Pereira, F. & Lee, L. (1994). Similarity-based
    estimation of word co-occurrence probabilities.
    Proceedings of the 32nd Annual Meeting of the
    Assoc. for Computation Linguistics, 272-278.
Firth, J.R. (1957). A synopsis of linguistic theory 1930-
    1955. In Studies in Linguistic Analysis, pp. 1-32.
    Oxford: Philological Society. Reprinted in F.R.
    Palmer (ed.), Selected Papers of J.R. Firth 1952-
    1959, London: Longman (1968).
Fletcher, C. & Linzie, B. (1998). Motive and
    Opportunity: Some Comments on LSA, HAL, KDC,
    and Principal Components. Discourse Processes,
    25(2&3), 355-361.
French, R.M. (1988). Subcognitive Probing: Hard
    Questions for the Turing Test. Proceedings of the
    Tenth Annual Cognitive Science Society Conference,
    Hillsdale, NJ: LEA. 361-367.
French, R.M. (1990). Subcognition and the Limits of
    the Turing Test. Mind, 99(393), 53-65.
French, R. M. and Labiouse, C. (2001). Why co-
    occurrence information alone is not sufficient to
    answer subcognitive questions. J. of Experimental
    and Theoretical Artificial Intelligence.
Gentner, D. (1983). Structure-mapping: A theoretical
    framework for analogy. Cognitive Science, 7(2),
    155-170.
Hofstadter, D. R. and the Fluid Analogies Research
    Group (1995). Fluid Concepts and Creative
    Analogies, New York, NY: Basic Books.
Landauer, T. & Dumais, S. (1997). A solution to Plato’s
    Problem: The Latent Semantic Analysis theory of
    acquisition, induction, and representation of
    knowledge. Psychological Review, 104, 211-240.
Lund, K. & Burgess, C. (1996). Producing high-
    dimensional semantic spaces from lexical co-
    occurrence.      Behavior       Research     Methods,
    Instruments and Computers, 2, 203-208.
Perfetti, C. A. (1998). The limits of co-occurrence:
    Tools and theories in language research. Discourse
    Processes, 25, 363-377.
Rips, L. J., Shoben, E. J. & Smith, E. E. (1973).
    Semantic distance and the verification of semantic

