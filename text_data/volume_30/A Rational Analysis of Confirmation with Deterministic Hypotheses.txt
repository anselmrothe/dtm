UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Rational Analysis of Confirmation with Deterministic Hypotheses
Permalink
https://escholarship.org/uc/item/6n60k9zn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Austerweil, Joseph L.
Griffiths, Thomas L.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

             A Rational Analysis of Confirmation with Deterministic Hypotheses
                                      Joseph L. Austerweil (Joseph.Austerweil@gmail.com)
                                       Thomas L. Griffiths (Tom Griffiths@berkeley.edu)
                  Department of Psychology, University of California, Berkeley, Berkeley, CA 94720-1650 USA
                               Abstract                                  ple still use positive tests in situations where negative tests are
                                                                         more likely to yield falsification, such as those encountered
   Whether scientists test their hypotheses as they ought to has in-
   terested both cognitive psychologists and philosophers of sci-        in Wason’s (1960) experiment. We complement this analysis
   ence. Classic analyses of hypothesis testing assume that peo-         by showing that the PTS is more likely to yield falsification
   ple should pick the test with the largest probability of falsi-       and optimally reduces uncertainty provided the world is in-
   fying their current hypothesis, while experiments have shown
   that people tend to select tests consistent with that hypothesis.     herently deterministic (i.e., given the rule is true, there is only
   Using two different normative standards, we prove that seek-          one possible next outcome). This suggests we might explain
   ing evidence predicted by your current hypothesis is optimal          use of the PTS as the result of an assumption of determinism
   when the hypotheses in question are deterministic and other
   reasonable assumptions hold. We test this account with two            on the part of human learners, consistent with recent results
   experiments using a sequential prediction task, in which peo-         showing that children assume that many causal relationships
   ple guess the next number in a sequence. Experiment 1 shows           are deterministic (e.g., Schulz & Sommerville, 2006; Gel-
   that people’s predictions can be captured by a simple Bayesian
   model. Experiment 2 manipulates people’s beliefs about the            man, Coley, & Gottfried, 1994). This emphasis on the struc-
   probabilities of different hypotheses, and shows that they con-       ture of the environment parallels similar strategies pursued in
   firm whichever hypothesis they are led to believe is most likely.     other rational analyses (e.g., Oaksford & Chater, 1994).
   Keywords: confirmation bias; rational analysis; hypothesis               The plan of the paper is as follows, first we introduce the
   testing; Bayesian inference.
                                                                         task of predicting the next event in a sequence. Under the as-
   How should a scientist seek evidence to help her find the             sumption that hypotheses are deterministic (given a sequence
hypothesis that explains a phenomenon? Does this differ                  of events, a hypothesis predicts only one next event), we
from how people do seek evidence? Popper (1935/1990) ar-                 prove that the PTS is optimal in many situations. Next, we
gued that scientists ought to follow the strategy of falsifica-          define a Bayesian model of sequence prediction for numeri-
tion, seeking evidence most likely to falsify their current the-         cal stimuli, and use a behavioral experiment to show that it
ory. Interested in whether people adhere to this strategy, Wa-           captures human predictions. If people are seeking evidence
son (1960) investigated how people intuitively test their the-           optimally, then they should choose to verify the next num-
ories. In the classic 2-4-6 task, participants were asked to un-         ber predicted by the hypothesis they believe is most likely.
cover a relational rule after being told that one triplet, (2, 4, 6),    In a second experiment, we demonstrate that changing a per-
conforms to the rule. The true rule, increasing numbers, sub-            son’s beliefs about the probability of hypotheses affects their
sumes most potential rules (e.g., two more than the previous             evidence-seeking strategy. We conclude by discussing how
number) with every triplet predicted by a potential rule also            our results relate to previous work.
being valid under the increasing numbers rule. Thus, the true
rule can only be discovered by testing numbers that are not
                                                                             Sequence prediction and hypothesis testing
predicted by your current best guess at the rule (negative test          Given a sequence of events, how do we predict what will oc-
strategy or NTS). Rather than follow the NTS, participants               cur next? For example, suppose you see a woman outside of
choose to test triplets predicted by their current hypothesis            an airport and then at the security checkpoint. How likely is
(the positive test strategy or PTS) even though it is impossi-           it that she stays at the security checkpoint (she is a security
ble to find the true rule this way. For example, many partic-            guard) or walks to a gate (she is a passenger or crewmember)?
ipants in the 2-4-6 task followed the PTS by entertaining the            Clearly, the probability of each possible next event depends
hypothesis that each number is two more than the previous                on the probability of the hypotheses explaining the observed
number and testing sequences consistent with this hypothe-               events and the probability of the next event under these hy-
sis, such as (1, 3, 5). The tendency to follow the PTS is just           potheses. Since there is no means of predicting the next event
one instance of what has become known as the confirmation                with complete certainty, this is an inductive task.
bias or the general human tendency to interpret and seek ev-                This problem can be expressed in terms of probability the-
idence fitting their current theory differently from evidence            ory. Given a sequence of previous events or objects (~x =
against it (Klayman & Ha, 1987).                                         (x1 , . . . , xi−1 )) the probability of a next event (xi ) is
   In this paper, we outline a set of environmental conditions
                                                                                                 P(xi |~x) = ∑ P(xi |h,~x)P(h|~x)        (1)
under which the PTS is actually an optimal strategy. Previ-
                                                                                                             h
ous work has identified settings in which the PTS or NTS is
more likely to yield falsification (e.g., Klayman & Ha, 1987).           where P(xi |h,~x) is the probability of the next event under hy-
However, this normative analysis produces predictions that               pothesis h, and P(h|~x) is the posterior probability of that hy-
are quite different from human behavior. For example, peo-               pothesis given the sequence ~x. This posterior probability can
                                                                     1041

be obtained from Bayes’ rule, with                                     with
                                                                                        P(r|c,~x) = ∑ P(r|h, c,~x)P(h|~x)
                                   P(~x|h)P(h)                                                        h
                    P(h|~x) =                                   (2)
                                 ∑h′ P(~x|h′ )P(h′ )                   being the probability of the outcome r from the test c given
                                                                       our previous observations~x. In sequential prediction, the out-
being the normalized product of the likelihood, P(~x|h), and           come of a test is either that the queried event is next in the se-
the prior probability of the hypothesis P(h). For the above            quence or not. The probability of a positive response (r = +)
example, the probability that the woman is a security guard            to a query c is simply the probability that c is the next event
instead of a passenger depends on the relative probabilities of        in the sequence, which depends on h and ~x.
a security guard and a passenger going to the security check-             Since the outcome of a test is unknown prior to performing
point and the base rates with which passengers and security            the test, the information gain cannot be used directly. Instead,
guards appear at the airport.                                          we define the optimal test to be the test that has the largest
   Suppose we now meet the woman’s husband, and get to                 expected information gain (EIG). The optimal choice ĉ is
ask him one (yes or no) question about where she will be
next. What is the best question to ask in order to discover                        ĉ = arg max Er|c,~x [I(P(h|~x), P(h|~x, r, c))]
                                                                                               c
her role (i.e., whether she’s a security guard, passenger, or
crewmember)? This is equivalent to a scientist determing the           where Er [ f (r)] = ∑r f (r)P(r) is the expectation of the func-
best question to test her hypothesis. In the remainder of the          tion f with respect to the distribution P. This reduces to
section, we show that there is a simple answer to this question
provided our hypotheses are deterministic, allowing only one              ĉ  = arg max ∑ [H(P(h|~x)) − H(P(h|~x, r, c))] P(r|c,~x)
                                                                                         c    r
value for x given~x (ie., that there is only one place the woman
will go for each hypothesis about her identity). In this case,                = arg min ∑ H(P(h|r, c,~x))P(r|c,~x)
                                                                                         c   r
the positive test strategy (asking about the event that corre-
sponds to the most probable hypothesis) is optimal. Thus, the          being that which minimizes uncertainty after the response.
best question is to ask her husband is whether she will be in
the location that our best guess about her identity predicts.          The optimality of positive test strategies
   We will use two methods to identify what question we                Instead of directly deriving general results on the usefulness
should ask. The first is the probability of falsification - asking     of positive test strategies, we first consider the problem with
the qeustion that gives us the highest probability of falsifying       simplifying assumptions. We narrow our hypothesis space to
our current hypothesis (Popper, 1935/1990; Klayman & Ha,               deterministic hypotheses which all make different predictions
1987). The second is a measure based on information theory             for the next event in the observed sequence. Under these con-
(Klayman, 1987; Oaksford & Chater, 1994). According to                 ditions, every test is a positive test for some hypothesis, and
information theory, the entropy                                        a positive response from such a test yields conclusive verifi-
                                                                       cation of the tested hypothesis, while a negative response fal-
                  H(P(x)) = − ∑ P(x) log2 P(x)                         sifies the tested hypothesis but is ambiguous about all other
                                   x                                   hypotheses. We show that testing the event predicted by the a
measures the amount of randomness in a probability distri-             posteriori most probable hypothesis maximizes the probabil-
bution P(x). For example, the entropy of a fair coin is 1              ity of falsifying that hypothesis and the EIG.
(.5 log2 .5 + .5 log2 .5 = 1) and the entropy of a two-headed             Using maximizing the probability of falsifying the current
coin is 0 (1 log2 1 + 0 log2 0 = 0, where 0 log2 0 is defined to       working hypothesis as our normative standard (Klayman &
be 0). This matches our intuition that we are far more cer-            Ha, 1987), the analysis is simple. The probability that test-
tain of the outcome from the toss of a two-headed coin. The            ing the choice c, consistent with hypothesis hc , falsifies that
amount of information gained from observing an outcome is              hypothesis is 1 − P(hc |~x). If you want to falsify a particular
the difference between the entropy of the distribution char-           hypothesis, then it is best to test the choice it predicts since
acterizing our beliefs before and after that observation. Thus,        1 − P(hc |~x) is the sum of the probabilities of all other hy-
the information gained about the a set of hypotheses for which         potheses. Consequently, to falsify the current hypothesis, you
our current beliefs are described by the posterior distribution        should test the choice it predicts and thus the PTS is optimal.
P(h|~x), given a sequence of objects from performing a test c             The same result holds when we take maximizing the EIG as
and learning its outcome r, is                                         our goal. As shown above, maximizing the EIG is equivalent
                                                                       to minimizing the expected entropy of the posterior distribu-
      I(P(h|~x), P(h|~x, r, c)) = H(P(h|~x)) − H(P(h|~x, r, c))        tion informed by the results of the test. As the hypotheses all
                                                                       predict different events next, if we learn that c is in fact the
where P(h|~x, r, c) reflects the information provided by (r, c),       next event, than we know with certainty that its correspond-
                                                                       ing hypothesis is true, resulting in an entropy of 0. Thus, the
                                  P(r|h, c,~x)P(h|~x)                  expected entropy reduces to the product of the posterior prob-
                  P(h|~x, r, c) =                                      ability that the tested hypothesis is false and the entropy of the
                                       P(r|c,~x)
                                                                   1042

renormalized posterior without the tested hypothesis                               hypothesis with posterior probability greater than or equal to
                                                                                 a half, then confirming that hypothesis (which is the current
                            P(h|~x)                                                best hypothesis) is the optimal strategy. If this is not the case,
                   H                      (1 − P(hc |~x))
                         1 − P(hc |~x)                                             confirming the current best hypothesis can be suboptimal, as
                                                                                   it may be possible to construct an amalgam of hypotheses that
where hc is the hypothesis corresponding to the choice C.
This simplifies to                                                                 agree on some c and have posterior probabilities that sum to
                                                                                   a value closer to 0.5. However, such circumstances are un-
                            P(h|~x)              P(h|~x)
  −(1 − P(hc |~x))   ∑    1 − P(hc |~x)
                                        log2
                                              1 − P(hc |~x)
                                                                                   usual, and our result thus indicates that in many cases where
                    h6=hc                                                          we believe there is a rule governing a sequence of events, the
        =−    ∑    P(h|~x) log2 P(h|~x) +   ∑    P(h|~x) log2 (1 − P(hc |~x))      positive test strategy is optimal.
             h6=hc                         h6=hc
                                                                                       A Bayesian model for numerical sequences
 The first of the two sums is the entropy of the posterior with-
out the contribution from the tested hypothesis, and the sec-                      The analysis of the positive test strategy outlined above re-
ond simplifies because the log portion does not vary over the
sum. Consequently, we can rewrite this quantity as                                 lies upon the assumption that we can accurately characterize
                                                                                   people’s predictions about sequences in terms of Bayesian in-
 H(P(h|~x)) + P(hc |~x) log2 P(hc |~x) + (1 − P(hc |~x)) log2 (1 − P(hc |~x))      ference. In the remainder of the paper, we develop a Bayesian
                                                                                   model of a particular kind of sequence prediction – prediction
 Since the entropy of the posterior does not depend on the
choice c, it does not influence the optimal choice. This means                     of the next element in a sequence of numbers – and use this
that the choice that maximizes the EIG is                                          model to test this basic assumption, and to show that people
                                                                                   are sensitive to the relative probabilities of different hypothe-
 ĉ = arg min P(hc |~x) log2 P(hc |~x) + (1 − P(hc |~x)) log2 (1 − P(hc |~x))
           c                                                                       ses in exactly the way that this account predicts.
 which is the negative entropy of a distribution in which hc                          The domain of our model of sequence prediction is num-
and its alternatives are the only two possible outcomes.                           bers. We assume that the sequence of observed numbers,
    The entropy of a distribution is concave (there is one global                 ~x = (x1 , . . . , xi−1 ), is generated from some relational rule
maximum) and is maximized when the distribution is uniform                         h → ~x, and that people try to identify this rule in order to
(Cover & Thomas, 1991). Thus, the optimal strategy is to                           make accurate predictions. Our model is based upon the con-
make the choice corresponding to the hypothesis with poste-                        cept learning framework presented in Tenenbaum (1999) and
rior probability closest to 0.5. It is easy to show that this is                   Tenenbaum and Griffiths (2001), a version of which was ap-
the hypothesis with highest posterior probability.1 There are                      plied to a simple “number game” similar to our task. In this
two cases. If all probabilities P(h|~x) are less than 0.5, then the                model, a hypothesis or concept is a set of numbers. Although
hypothesis for which P(h|~x) is greatest is clearly the closest                    this model captures people’s generalization judgments (e.g.,
to 0.5. If the probability of some hypothesis is greater than                      given 8 is in the set, what is the probability that 16 is in
0.5 there is only one such hypothesis, and the distance of the                     the set?), it does not allow for inferences about sequences
probability of all other hypotheses from 0.5 will be at least                      of numbers. Thus, we extend this Bayesian model to make
as great, as these hypotheses divide the remaining probability                     predictions about sequences. The goal of the model is not to
mass. Thus, confirmation – choosing to test the hypothesis                         capture all the intricacies of human sequence prediction, but
with highest posterior probability – maximizes the EIG.                            rather to be a reasonable approximation that we can use to
    We can now generalize this analysis for the EIG, relaxing                      understand human hypothesis testing.
the assumption that all hypotheses make distinct predictions                          Instead of defining the hypotheses as sets of numbers, each
for the next event. In the general case, every choice c parti-                     hypothesis is a rule from kh previous numbers to the possi-
tions the hypothesis space into two sets. Let H c be the set                       ble next numbers of the sequence. The likelihood assigns a
of hypotheses that predict c as the next event and H c̄ be the                     probability distribution over next numbers given the previous
set of hypotheses that do not. The set that makes the wrong                        kh observed numbers. We divide the types of hypotheses into
prediction will be eliminated, receiving probability 0, and the                    two separate categories: deterministic and non-deterministic.
set that makes the right prediction will have their posterior                      A deterministic hypothesis, such as increasing odd numbers,
probabilities renormalized. The analysis then proceeds simi-                       has only one correct next number and conforms to the fol-
larly to the derivation given above, replacing hc with H c , with                  lowing form: h(xi−1 , . . . , xi−k+1 ) : X k → X. For example, the
P(H c |~x) = ∑h∈H c P(h|~x), although there is an extra wrinkle                    likelihood function for the sum of the last two numbers rule
produced by the fact that confirmation does not guarantee an                       (Fibonacci sequence, kh = 2) is:
entropy of 0. This analysis shows that the optimal test is that                                                     
which produces P(H c |~x) closest to 0.5. If there is a single                                                         1    if xi = xi−1 + xi−2
                                                                                           P(xi |h, xi−1 , xi−2 ) =
                                                                                                                       0    otherwise
    1 More precisely, choosing the hypothesis with highest posterior
probability is always at least as good as choosing any other hypothe-
sis, with equality holding in the case where just two hypotheses have              Conversely, more than one number may conform to a non-
non-zero posterior probability.                                                    deterministic hypothesis. For example, the following likeli-
                                                                              1043

hood function models the increasing numbers (kh = 1):                 survey containing one subsequence of each rule. Each par-
                                                                      ticipant received one survey, with approximately 11 partic-
                                 xi ≥ xi−1 ∧ xi − xi−1 ≤ ν
                           1
     P(xi |h, xi−1 ; ν) =   ν+1                                       ipants seeing each survey. To provide the strongest test of
                           0     xi < xi−1                            our model, we asked participants to write down what they
                                                                      believed the next number would be, without imposing any
where ν is the largest increase possible from the last number.        constraints on this choice. Participants were told that the se-
   These hypotheses are partitioned into seven different sets         quences may have been generated by a simple relational rule
of the same rule type: ×C + K, sum of the last two numbers,           which may not be deterministic, with “decreasing numbers”
pairwise mixtures of ×C +K rules, repeat the last kh numbers,         being given as an example, and asked to make predictions for
the i-th power (for i = 2 and 3), primes, and the random rules        each sequence independently.
(decreasing, increasing, and random numbers). The ×C + K
hypotheses cover any rule of the form xi = Cxi−1 + K, and             Results
we considered C ∈ {−3, . . . , 3} except zero, K ∈ −5, . . . 5. In
                                                                      As shown in Figure 1, the model and human prediction distri-
total, this yields 135 hypotheses. The prior probability of all
                                                                      butions are in close correspondence. The predictions shown
rules of a given type is uniform within that set, and the prior
                                                                      for the model were obtained by optimizing the prior proba-
probabilities of the rules of different types are free parame-
                                                                      bility of the different hypothesis types to fit the human data,
ters. Since rules are based on the values of preceding num-
                                                                      but are somewhat robust to variation in the prior. The cor-
bers, we also need a scheme for generating the initial numbers
                                                                      relation between the sets of predictions is r = 0.87. Since
in a sequence. We do this by sampling x0 from a distribution
                                                                      the increasing numbers pattern is random, both the partici-
assigning probability 1/|1 + x0 | to the positive and negative
                                                                      pant and model predictive distributions are diffuse, lowering
integers, and subsequent initial numbers from the same dis-
                                                                      this correlation. The predictive distributions are nearly iden-
tribution centered around the preceding number. This acts as
                                                                      tical for the four deterministic sequences, with r = 0.98. The
an implicit penalty against rules for which kh is high, as they
                                                                      estimated prior probabilities of the seven types of hypothe-
require more draws from this distribution.
                                                                      ses are: ×C + K is 0.85, sum of the last two is 10−4 , mix-
   The model defined in this section provides all we need to
                                                                      tures of ×C + K is 1.5 × 10−4 , repeat the last kh numbers is
compute the posterior distribution over hypotheses given a se-
                                                                      4.5 × 10−5 , i-th power is 0.05, primes is 6.9 × 10−7 , decreas-
quence of numbers (Equation 2) and consequently to predict
                                                                      ing is 0.008, random is 0.006, and increasing is 0.09.
the next number in a sequence (Equation 1). Experiment 1
examines how well this model characterizes the predictions               Having verified that a Bayesian model can capture human
that people make about sequences of numbers.                          sequence predictions, we can use it to test how human hy-
                                                                      pothesis testing is affected by prior knowledge. The analysis
        Experiment 1: Predicting predictions                          of optimal hypothesis testing given above predicts that people
                                                                      should seek to confirm the hypothesis that they assign high-
In this experiment, participants were asked to predict the
                                                                      est posterior probability. To test this prediction, Experiment
next number for five sequences, each generated by a differ-
                                                                      2 manipulated the prior probability of different types of hy-
ent rule. There were five patterns, four deterministic and one
                                                                      potheses to see if we could induce people to change which
stochastic, each expressed in four sequences of increasing
                                                                      hypotheses they sought to confirm.
size (length ranging from three to six). The four determin-
istic patterns were chosen to illustrate participants’ and the
                                                                          Experiment 2: Manipulating confirmation
model’s ability to make judgments on simple and complex
rules and when the given sequence was ambiguous as to the             Methods
underlying rule. The stochastic pattern was chosen to demon-
                                                                      Participants A total of 67 undergraduates participated in
strate both participants and the model make sensible related
                                                                      exchange for course credit. Participants were split into three
judgments when the generating rule is not deterministic.
                                                                      conditions, with 22 participants in the ×C + K condition, 22
Methods                                                               participants in the “sum last two” condition, and 23 partici-
                                                                      pants in the control condition.
Participants A total of 146 undergraduates participated in
the experiment for course credit or a free ice cream voucher.         Stimuli In order to establish the priors in different sequence
                                                                      prediction environments, participants in the ×C +K and “sum
Stimuli Five relational rules were tested: repeat the
                                                                      last two” conditions were trained on 100 sequences of num-
last number (1,1,1,1,1,1 - simple), sum of the last two
                                                                      bers. The training sequences in the ×C + K condition had a
numbers (1,1,2,3,5,8 - complex), increasing odd num-
                                                                      high prevalence (87%) of sequences generated by rules of the
bers (3,5,7,9,11,13 - ambiguous), increasing prime num-
                                                                      form ×C + K and no sequences generated by summing the
bers (3,5,7,11,13,17 - ambiguous), and increasing numbers
                                                                      last two numbers, and vice versa in the “sum last two” con-
(2,5,17,33,94,100 - stochastic).
                                                                      dition (with 89% of sequences conforming to the target rule).
Procedure The four subsequences of each rule were ran-                Test selection was probed with 21 sequences consistent with
domly distributed across four different surveys, with each            both the sum of the last two numbers and the ×C + K rule,
                                                                  1044

                                                 1,1,1                                                     1,1,1,1                                                   1,1,1,1,1                                            1,1,1,1,1,1
                              1                                                          1                                                         1                                                           1                          human
                 p(choice)                                                  p(choice)                                                 p(choice)                                                   p(choice)
                             0.5                                                        0.5                                                       0.5                                                         0.5                         model
                              0                                                          0                                                         0                                                           0
                                   0               2               4                          0               2             4                           0                2              4                           0          2              4
                                                 1,1,2                                                     1,1,2,3                                                   1,1,2,3,5                                            1,1,2,3,5,8
                              1                                                          1                                                         1                                                           1
                 p(choice)                                                  p(choice)                                                 p(choice)                                                   p(choice)
                             0.5                                                        0.5                                                       0.5                                                         0.5
                              0                                                          0                                                         0                                                           0
                                   0               2               4                              2    4    6 8       10 12                             4        6          8     10                                8   10    12     14      16
                                                 3,5,7                                                     3,5,7,9                                                   3,5,7,9,11                                          3,5,7,9,11,13
                              1                                                          1                                                         1                                                           1
                 p(choice)                                                  p(choice)                                                 p(choice)                                                   p(choice)
                             0.5                                                        0.5                                                       0.5                                                         0.5
                              0                                                          0                                                         0                                                           0
                                   8        10           12    14                             8       10    12 14      16                               8   10      12 14         16                                8   10 12 14 16
                                                 3,5,7                                                     3,5,7,11                                              3,5,7,11,13                                             3,5,7,11,13,17
                              1                                                          1                                                         1                                                           1
                 p(choice)                                                  p(choice)                                                 p(choice)                                                   p(choice)
                             0.5                                                        0.5                                                       0.5                                                         0.5
                              0                                                          0                                                         0                                                           0
                                   4    6      8    10         12                         10           12     14      16                                    14           16            18                       18       20      22     24
                                             2,5,17                                                      2,5,17,33                                               2,5,17,33,94                                           2,5,17,33,94,100
            p(choice)                                                  p(choice)                                                 p(choice)                                                   p(choice)
                         0.1                                                        0.1                                                       0.1                                                         0.1
                        0.05                                                       0.05                                                      0.05                                                        0.05
                           0                                                          0                                                         0                                                           0
                                       20       40            60                                      40       60           80                   90                    100             110                   90              100             110
                                             choice                                                        choice                                                     choice                                                choice
Figure 1: Results of Experiment 1. Each row of plots shows the predictions for one sequence as the number of elements
increases from 3 to 7 across the columns. The five rules used to generate the sequences are (from top to bottom) repeating ones,
sum of the last two numbers, increasing odd numbers, increasing odd prime numbers, and increasing numbers. The scale of the
increasing numbers is different and may omit some values of both distributions for visual clarity.
shown to participants in all conditions. For example, one se-                                                                                of the current hypothesis, which in turn is determined by the
quence, (3, 6, 9), can be interpreted as ×1 + 3 or the sum of                                                                                prior probabilities established by the training condition.
the last two numbers (3 + 6 = 9).                                                                                                               The responses produced by the participants for all se-
Procedure In the training phase, participants were asked to                                                                                  quences were grouped into three categories: ×C + K, sum
predict the next number in the sequence and the underlying                                                                                   of the last two numbers, or other. Two coders, one blind
rule, and then told whether their responses were correct. The                                                                                to the hypothesis and both blind to condition, assigned the
group of participants in the control condition were not trained                                                                              rules people selected as belonging to these three groups, with
any sequences and only were given the test portion of the ex-                                                                                high inter-rater reliability (κ = 0.90). As the model pre-
periment. In the test phase, participants were told that they                                                                                dicts, participants were sensitive to the environment given in
could pick one number and find out whether that number was                                                                                   their training condition and changed their responses appro-
the next in the sequence, being told to select the number that                                                                               priately (see Figure 2). Although participants did not con-
would help them figure out the underlying rule the best. They                                                                                firm the appropriate hypothesis for every sequence as the
were asked to write down both what they thought the rule was                                                                                 model predicts, the variation was statistically significant. Par-
and their number choice. The experiment was administered                                                                                     ticipants in the “sum last two” condition tested the sum of
on a computer with instructions given by the experimenter.                                                                                   the last two numbers significantly more often than partici-
The participants were also provided a calculator.                                                                                            pants in either the ×C + K condition (χ2 (2) = 9.71, p < 0.01)
                                                                                                                                             or the control condition (χ2 (2) = 196.25, p < 0.01). Ad-
Results                                                                                                                                      ditionally, the responses for the sum of the last two num-
                                                                                                                                             bers and control conditions were not significantly different
                                                                                                                                             (χ2 (2) = 1.11, p > 0.55). Thus, when testing their theories
If participants are sensitive to the prior probabilities of dif-
                                                                                                                                             and hypotheses, people are sensitive to the prior probabili-
ferent environments, then they should choose to confirm the
                                                                                                                                             ties in the environment, choosing to confirm the hypothesis
same rule as their training condition. Since the priors in both
                                                                                                                                             rendered most probable by that environment.
the control (established by the priors learned from Experi-
ment 1) and ×C + K conditions are similar, our main concern
                                                                                                                                                                         Discussion and Conclusions
is whether participants are more likely to confirm the sum of
the last two rule when trained in the “sum last two” condition.                                                                              We have shown that the PTS is optimal under the assump-
For all of the test sequences, the model predicts confirmation                                                                               tion that the hypotheses under consideration are determinis-
                                                                                                                           1045

                                                                                                      the original 2-4-6 task, the alternative hypothesis (increasing
                                           20
                                                                                                      numbers for Wason (1960), multiples of five for Nelson and
                                                     *C +K
Average Number of Responses of Each Type
                                           18                                                         Movellan (2001)) picks a superset of the outcomes consistent
                                                     Sum Last Two
                                           16        Other                                            with the most probable hypothesis. This is where our analysis
                                           14
                                                                                                      differs from previous work: by assuming that hypotheses are
                                                                                                      deterministic, we require them to pick only a single predic-
                                           12
                                                                                                      tion and thus no hypothesis strictly subsumes another.
                                           10                                                            Our analysis indicates that the positive test strategy is opti-
                                            8                                                         mal in a particular setting: when hypotheses are deterministic
                                            6
                                                                                                      in their predictions. This is precisely the setting that people
                                                                                                      face in our numerical prediction task, where hypotheses are
                                            4
                                                                                                      relational rules. However, in other settings – namely those
                                            2                                                         where one hypothesis can be a superset of another – the PTS
                                            0                                                         is suboptimal. In the spirit of previous rational analyses of
                                                Sum Last Two              *C +K          Control
                                                                    Training Condition
                                                                                                      confirmation (Oaksford & Chater, 1994), we propose explain-
                                                                                                      ing the fact that people pursue a suboptimal strategy in these
Figure 2: Results of Experiment 2, averaged over participants                                         non-deterministic settings as a consequence of assumptions
in each group. Error bars show one standard error.                                                    about the structure of their environment – in our case, that
                                                                                                      rules are deterministic. If we live in a deterministic world,
                                                                                                      then choosing tests that confirm our expectations might be a
tic, using both maximizing the probability of falsification and                                       simple adaptive strategy for this environment. We are in the
reduction of uncertainty as measures of test utility. Our ex-                                         process of developing a means of confirming this hypothesis.
periments provide the pieces of evidence needed to connect
this result to human behavior. In the first experiment, we                                            Acknowledgments. We thank David McNamee and Kevin Canini
showed that a Bayesian model of sequential prediction ac-                                             for thoughtful discussions, our research assistants (especially Matt
curately characterizes human expectations. Our formal anal-                                           Cammann) for help running experiments, four anonymous reviewers
ysis predicts that changing the relative prior probabilities of                                       for their comments, and the Air Force Office of Scientific Research
two hypotheses that could both have generated ambiguous se-                                           (grant number FA9550-07-1-0351) and the UC Berkeley Chancel-
quences should change the test that people choose. In the sec-                                        lor’s Faculty Partnership Fund for support.
ond experiment, we demonstrated that people behave in a way                                                                     References
that matches this prediction, selecting tests that confirmed the                                      Cover, T. M., & Thomas, J. A. (1991). Elements of information
hypothesis most probable in each environment. Thus, partici-                                            theory. New York: Wiley.
pants are not blindly testing the same choice regardless of the                                       Gelman, S. A., Coley, J. D., & Gottfried, G. M. (1994). Essentialist
environment, but are identifying the most probable hypoth-                                              beliefs in children: The acquisition of concepts and theories. In
                                                                                                        Mapping the mind: Domain specificity in cognition and culture
esis and then systematically seeking to confirm that hypoth-                                            (p. 341-365).
esis. We now consider how these results relate to previous                                            Klayman, J. (1987). An information theory analysis of the value of
work on the confirmation bias, and their implications for un-                                           information in hypothesis testing (Tech. Rep. No. 119a). Univer-
                                                                                                        sity of Chicago.
derstanding why people might exhibit such a bias.                                                     Klayman, J., & Ha, Y.-W. (1987). Confirmation, disconfirmation,
                                                                                                        and information. Psychological Review, 94, 211-228.
   Klayman and Ha (1987) proposed exploring the role of the                                           Nelson, J. D., & Movellan, J. R. (2001). Active inference in concept
set of possible hypothesis on testing; however, few papers                                              learning. In Advances in neural information processing systems
have used constrained hypothesis spaces to analyze hypoth-                                              (Vol. 13, p. 45-51).
                                                                                                      Oaksford, M., & Chater, N. (1994). A rational analysis of the se-
esis testing. One exception is Nelson and Movellan (2001)                                               lection task as optimal data selection. Psychological Review, 101,
who explored directly applying the Bayesian generalization                                              608-631.
model and EIG to a task similar to the 2-4-6 task. In their                                           Popper, K. R. (1935/1990). The logic of scientific discovery. Boston,
                                                                                                        MA: Unwin Hyman.
task, hypotheses were sets of numbers and the goal was to                                             Schulz, L. E., & Sommerville, J. (2006). God does not play dice:
find the hypothesis most likely to have generated a given set                                           Causal determinism and preschool causal inferences. Child De-
of numbers. The participants were allowed to ask whether                                                velopment, 77(2), 427-442.
                                                                                                      Tenenbaum, J. B. (1999). A Bayesian framework for concept learn-
one other number followed the rule. Nelson and Movellan                                                 ing. Unpublished doctoral dissertation, Massachussetts Institute
(2001) found that in cases of high posterior uncertainty, the                                           of Technology, Cambridge, MA.
choices predicted by EIG matched the choices given by con-                                            Tenenbaum, J. B., & Griffiths, T. L. (2001). Generalization, similar-
                                                                                                        ity, and Bayesian inference. Behavioral and Brain Sciences, 24,
firmation; however, in cases of low uncertainty, the choices                                            629-641.
predicted by EIG conflicted with choices given by confir-                                             Wason, P. C. (1960). On the failure to eliminate hypotheses in a
mation (and human participants). One representive example                                               conceptual task. Quarterly Journal of Experimental Psychology,
                                                                                                        12, 129-140.
where human responses deviate from EIG is for the given set
{60, 80, 10, 30}, the working hypothesis is multiples of ten,
but multiples of five is also possible. In this case, analogous to
                                                                                                   1046

