UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using the Distributional Statistics of Speech Sounds for Weighting and Integrating Acoustic
Cues
Permalink
https://escholarship.org/uc/item/379129t6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Toscano, Joseph C.
McMurray, Bob
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

Using the Distributional Statistics of Speech Sounds for Weighting and Integrating
                                                               Acoustic Cues
                                             Joseph C. Toscano (joseph-toscano@uiowa.edu)
                                            University of Iowa, Department of Psychology, E11 SSH
                                                            Iowa City, IA 52242 USA
                                              Bob McMurray (bob-mcmurray@uiowa.edu)
      University of Iowa, Department of Psychology and Iowa Center for Developmental and Learning Sciences, E11 SSH
                                                            Iowa City, IA 52242 USA
                                Abstract                                   learns the number of categories in the input and their statis-
                                                                           tical distributions, reflecting the developmental trajectory of
   A great deal of behavioral evidence suggests that infants can           speech category formation in the process. This demonstrates
   use distributional statistics to learn speech sound categories.
   Recently, a number of computational approaches have demon-              that unsupervised statistical learning mechanisms are able to
   strated the feasibility of statistical learning by showing that the     describe how infants can acquire these categories.
   distributional statistics of linguistically-relevant acoustic cues
   can be learned in an unsupervised way. However, speakers                   However, many critical aspects of this process have yet to
   and listeners use a large number of acoustic cues to distinguish        be addressed. In particular, acoustic analyses have revealed
   phonetic categories, and it is not clear how multiple cues are          that most phonemic distinctions are marked by multiple cues.
   combined during perception. We propose a model of speech
   sound category acquisition that learns the distributions of mul-        Lisker (1978) cites that there are at least 16 acoustic cues that
   tiple cues that lie along the same dimension and combines               distinguish voiced and voiceless sounds in word-medial po-
   them. We demonstrate that the model is able to account for              sition in English. Existing models of statistical learning and
   trading relations between cues (an indicator of the size of the
   effect of each cue) for word-initial voicing contrasts in English.      speech sound category acquisition are insufficient to describe
   Keywords: speech perception; speech development; mixture                how listeners learn multiple cue distributions and integrate
   of Gaussians; cue integration; statistical learning.                    these cues when perceiving speech. These challenges are also
                                                                           not limited to understanding speech development and percep-
                            Introduction                                   tion. Similar difficulties would be encountered when trying
                                                                           to combine multiple features in other domains as well.
The sound systems of human languages vary greatly. One of
the first steps in language acquisition is for infants to deter-              Can statistical learning approaches be extended to capture
mine the sound structure of their native language. In a given              listeners’ use of multiple acoustic cues? How could this prob-
language, phonetically-relevant acoustic cues are distributed              lem be instantiated in a model of speech sound categoriza-
such that they tend to cluster into categories. For example,               tion? One way would be to present each cue along a separate
voice-onset time (VOT; the delay between the opening of the                dimension, as in Figure 1B, leading to n-dimensional cate-
vocal tract and the onset of vocal energy) values in English               gories, where n is the number of cues to be learned. This type
tend to cluster into voiced and voiceless categories, near 0               of model has the advantage of allowing us to fully represent
and 50 ms, respectively (Lisker & Abramson, 1964). Thus,                   the acoustic space. However, it can also lead to computational
the distributional statistics of this cue contains information             complexity (representing a 16-dimensional category for the
about the voicing categories of English.                                   cues to voicing), and it would present the learner with a sparse
   Previous work has demonstrated that infants can track                   space from which to extract the categories, since many of the
these distributional statistics and use this information to learn          possible combinations of cues would never be heard.
categories (Maye, Werker, & Gerken, 2002). Recently, re-                      An alternative approach would be to weight and combine
searchers have begun to use computational models to under-                 sets of cues that lie along the same phonetic dimension, as
stand this process more deeply. These models have been used                shown in Figure 1C. Cues to voicing, for example, are not
to describe how learning unfolds over development and leads                orthogonal; they each indicate the same category structure:
to stable speech sound categories.                                         voiced or voiceless for English. Indeed, they are similar to
   One way to model this process is to represent each phonetic             visual cues to depth that observers integrate during percep-
category (such as voiced or voiceless) as a Gaussian distri-               tion to determine the three-dimensional structure of objects
bution, providing us with a representation that corresponds                (Jacobs, 2002). Thus, it is possible to integrate them into a
to the frequency distribution of an acoustic cue. McMurray,                single phonological dimension upon which the relevant cat-
Aslin, and Toscano (in press) present a model that uses this               egories can be learned. This approach reduces the computa-
approach. Figure 1A shows how the model might represent                    tional complexity of the problem, and it can be expanded to
the VOT distribution of English. It contains two categories,               account for a large number of acoustic cues.
one centered at the mean of the voiced VOT values and the                     This paper seeks to address several questions about how
other centered at the mean of the voiceless values. The model              sets of multiple cues can be learned:
                                                                       433

• Can we base a statistical learning model of speech sound         multiple acoustic cues. The two models differ in how they
   categories on a weighted combination of cues or will only       combine information from multiple cues. The cue weight-
   a full (multi-dimensional) model work?                          ing model weights cues and sums the weighted estimates
                                                                   provided by each cue, reducing them to a single dimension
• Can such a model account for the available data?                 (whose distribution is learned by another MOG). The multi-
• What does this tell us about cue integration principles?         dimensional model instead uses a set of two-dimensional
                                                                   Gaussians to track the combined distribution of cue-values,
We present two models and simulations designed to an-              representing all possible combinations of cues.
swer these questions. The models are mixtures of Gaussians
(MOGs) that have been used previously to model the acqui-          Cue weighting model
sition of a single acoustic cue (McMurray, Aslin, & Toscano,       Architecture The model consists of several mixtures of
in press) and multiple cues along orthogonal dimensions (Val-      Gaussian distributions. Each MOG contains a series of K
labha, McClelland, Pons, Werker, & Amano, 2007). Here, we          Gaussians along a particular acoustic dimension. Each Gaus-
will examine how these types of models can be used to learn        sian represents a potential phonetic category. Since the
a set of cues along a single sub-phonemic dimension.               number of categories is not known beforehand and must be
                                                                   learned over development, the mixture contains more Gaus-
                            Models                                 sians than it needs. One problem with MOG models is deter-
We contrast two models of speech sound categorization that         mining the correct number of Gaussians. To solve this prob-
attempt to demonstrate how listeners can learn and combine         lem, each Gaussian contains a frequency parameter, φ, corre-
                                                                   sponding to its prior probability. The model can then reduce
                                                                   the φ-values of categories that are not needed.
                                                                      The likelihood of a particular value along that cue dimen-
                                                                   sion, for each Gaussian (i) is defined by the posterior of that
                                                                   Gaussian times its φ-value:
                                                                                                           (x − µ2i )
                                                                                                                     
                                                                                               1
                                                                                 Gi (x) = φi q       exp                       (1)
                                                                                              2πσ2i          2σ2i
                                                                   where µ is the mean of the distribution and σ is the standard
                                                                   deviation. The sum of the probabilities for each Gaussian in
                                                                   the mixture determines the likelihood of a cue value:
                                                                                                    K
                                                                                          M(x) = ∑ Gi (x)                      (2)
                                                                                                    i
                                                                      For example, if a MOG represents the voicing categories
                                                                   for English along the VOT dimension, it would contain two
                                                                   Gaussians (K=2) − one corresponding to the voiced category
                                                                   (µ=0, σ=10) and one corresponding to the voiceless category
                                                                   (µ=50, σ=20) with equal prior probabilities. The likelihood of
                                                                   a 0 ms VOT would be the sum of the relatively high probabil-
                                                                   ity for the voiced category and the relatively low probability
                                                                   for the voiceless category. The likelihood of a 20 ms VOT, in
                                                                   contrast, would be the sum of the low probabilities from both
                                                                   categories. Thus, the category structure of this model would
                                                                   match the structure of the VOT categories for English.
                                                                      The model contains a MOG for each acoustic cue and an
                                                                   additional MOG for representing the categories based on the
                                                                   combination of the cues for the phonetic distinction being
Figure 1: (A) VOT distribution of English represented by two       learned.
Gaussian distributions corresponding to voiced (lighter color)     Learning The model learns the category structure of an
and voiceless (darker) categories. The gray bars show the          acoustic cue by adjusting the parameters of the Gaussians in
likelihood of VOT values obtained from the acoustic mea-           the mixture for that cue. Learning is accomplished via maxi-
surements in Allen and Miller (1999). (B) A representation         mum likelihood estimation by stochastic gradient descent.
of categories along two cue dimensions. (C) Integration of            As mentioned above, the mixture represents the likelihood
multiple cues into a single dimension.                             of a particular cue value given a set of parameters (µ, σ, and
                                                               434

φ). Since learning is iterative and we assume there are no pri-      Cue weighting and integration In order to implement cue
ors on the parameters, Bayes’ theorem says that the mixture          integration in the model, this learning procedure is applied
also represents the likelihood of a set of parameters given a        to multiple MOGs representing different acoustic cues for a
particular cue value. We can therefore use gradient descent          given phonetic contrast. Cue integration occurs by weighing
to update the parameters of each Gaussian given a particu-           the input along each dimension and summing the weighted
lar data point. Gradient descent updates the Gaussians by the        cue values, which serve as input to a separate MOG that lies
derivative of the likelihood function, (2), with respect to each     along a dimension corresponding to the phonetic distinction
parameter (See McMurray, Horst, Toscano, and Samuelson               being learned. This MOG is also trained and produces a set
(in press) for the learning rules used in the model).                of speech categories that contain information from multiple
   Learning proceeds by presenting the model with individual         cues. The individual MOGs for each cue are only used to
cue values, calculating the change in each parameter value,          compute the weights and inputs to the combined MOG.
and updating the parameters. The φ values are normalized so             As with the parameters of the Gaussians, cue weights are
they sum to one and reflect the likelihood of each category.         computed on the basis of the distributional statistics of the
   The model uses winner-take-all competition so that φ is           input. Cues with a higher reliability receive a greater weight.
only changed for the Gaussian that has the highest likelihood.       This approach is similar to a Kalman filter (Kalman, 1960;
This solves the problem mentioned above of not knowing the           Jacobs, 2002), which computes reliability for the individual
number of categories a priori. Competition is necessary for          cues and then uses a linear combination rule to integrate data
the model to determine the correct number of categories along        from each source:
                                                                                                     n
each dimension (McMurray, Aslin, & Toscano, in press). In                                      x = ∑ wi xi                          (3)
addition, this is psychologically plausible, since a particular                                      i
input corresponds to only a single category, and learning only       where x is the estimate based on the combined input, xi is the
needs to occur for that category.                                    estimate for a particular cue (i), wi is the weight for that cue,
                                                                     and n is the number of cues. In a Kalman filter, weights are
                                                                     determined by the formula
                                                                                                         1
                                                                                                        σ2i
                                                                                               wi = n                               (4)
                                                                                                           1
                                                                                                     ∑ σ2
                                                                                                       j    j
                                                                     where σ2 is the variance of the distribution along a particular
                                                                     cue dimension. Thus, cues that are more reliable (i.e. those
                                                                     with a smaller variance) will have larger weights.
                                                                        This measure of reliability has been used previously to
                                                                     model sensory integration given unimodal cue distributions
                                                                     along a common dimension (Jacobs, 2002). For the types
                                                                     of distributions we are examining here, however, the over-
                                                                     all variance cannot be used, since each cue contains multiple
                                                                     Gaussians. A single variance estimate does not adequately
                                                                     describe a dimension that is distributed in this way, since a
                                                                     high variance could be achieved by a mixture of two narrow
                                                                     Gaussians in which the means are far apart (highly reliable,
                                                                     Figure 2A) and a low variance could be achieved by a mix-
                                                                     ture of two broad Gaussians that are highly overlapping (un-
                                                                     reliable, Figure 2B). Thus, we developed a new estimate of
                                                                     cue reliability that can be used to determine the weight of a
                                                                     multimodal distribution:
                                                                                                                   !
                                                                                           K K
                                                                                                 φm φn (µm − µn)2
                                                                                    g = ∑∑                           /2             (5)
                                                                                           n m         σm σn
                                                                        This equation takes into account the variance of each dis-
Figure 2: (A) More reliable categories (solid lines) with a          tribution along a dimension, as well as the means and prior
high overall σ. The line in the center represents the approxi-       probabilities for each distribution. It calculates the reliability
mate overall σ. (B) Less reliable categories with a lower over-      of a particular cue dimension by summing all pairwise com-
all σ. (C) Schematic diagram of the cue weighting model.             parisons between the Gaussians along that dimension (m and
                                                                 435

n refer to particular Gaussians; K refers to the total number       Learning and simulation procedure Learning and test-
of Gaussians along that cue dimension). Pairs of Gaussians          ing follow the same basic procedure as in the cue weighting
that are close together do not contribute much to g, since the      model. On each trail, a pair of cue values is given as input
difference between their µs is minimal. Similarly, pairs of         and the parameters of the MOG are updated.
Gaussians with high σ values do not contribute as much as
pairs with low σs, and low frequency (φ) Gaussians do not                                    Simulations
contribute much to g. This results in a measure of overall re-
liability of a dimension analogous to d′ . (6) normalizes the       Simulation 1: Two cues
reliability estimates to compute the weight for each cue:           The first simulation involved a trading relation between two
                                                                    acoustic cues and was designed to determine whether the cue
                                   gi
                            wi =  n                         (6)     weighting or multi-dimensional model better accounts for cue
                                 ∑g j                               integration in speech along a single phonetic dimension.
                                  j                                    The cue weighting model contained three MOGs: one
                                                                    for representing VOT, one for vowel length (VL), and one
Simulation procedure For a single run of the model, train-          for representing overall voicing based on both cues. The
ing data is generated by randomly sampling from distribu-           multi-dimensional model contained a single two-dimensional
tions corresponding to the speech categories of interest. The       MOG, with one dimension for VOT and one for VL. 50 mod-
model is initialized by randomly setting µ for K Gaussians          els of each type were run in the simulation.
to a value in the range of the data to be learned. σ is set to
                                                                    Training The models were trained on VOT and VL val-
a constant value for each Gaussian, and φ is set to 1/K. On
                                                                    ues randomly sampled from distributions based on the acous-
each trial, the model is presented with individual exemplars
                                                                    tic measurements from Allen and Miller (1999) for VL and
to each MOG representing an acoustic cue, and the param-
                                                                    Lisker and Abramson (1964) for VOT. The models were run
eters of each Gaussian are updated using the learning rules
                                                                    for a sufficient amount of time for them to settle on a stable
and competition described above. Next, the cue dimensions
                                                                    set of categories. A total of 70,000 trials were run for the cue
are weighted, and the input along each dimension is normal-
                                                                    weighting model, and 200,000 trials were run for the multi-
ized relative to the parameters of the Gaussians along that
                                                                    dimensional model. For the correlation between the cues used
dimension. A new input is computed using the input to the
                                                                    by the multi-dimensional model, the value from the Allen &
individual cues and their weights. The MOG representing the
                                                                    Miller dataset was used. Table 1 shows the means and stan-
combined percept receives this as input, and learning occurs
                                                                    dard deviations of the distributions used to generate the train-
along this dimension. Figure 2C shows the basic organiza-
                                                                    ing data. Models that overgeneralized, that is, those that had
tion of a model that integrates two cues to voicing, VOT and
                                                                    only a single category after training for any of the MOGs (as
vowel length (VL).
                                                                    determined by the number of Gaussians with φ-values greater
   After training, the model is tested on its categorization of     than 0.1) were excluded from analysis.
stimuli varying along each acoustic dimension. From this, we
can see how it uses multiple cues by measuring trading rela-        Testing Each model was tested on the VOT and VL values
tions, differences in categorization depending on the values        used with human listeners in McMurray, Clayards, Tanen-
of the cues.                                                        haus, and Aslin (submitted): nine VOT steps (from 0 to 40
                                                                    ms) and two VL values (125 and 225 ms). The /b/ and
Multi-dimensional model                                             /p/ categories were identified by finding the Gaussians with
                                                                    the highest posterior probabilities for the best /b/ exemplar
Architecture The multi-dimensional model uses the same              (VOT=0 ms; VL=225 ms) and the best /p/ exemplar (VOT=40
basic MOG framework as the cue weighting model. However,            ms; VL=125 ms) in either the combined MOG for the cue
rather than representing each cue along a separate dimension        weighting model or the two-dimensional MOG in the multi-
and combining them, this model consists of a mixture of n-          dimensional model. These two Gaussians were used to com-
dimensional Gaussians. For the simulations presented here, a        pute the likelihood of a /p/ response for each stimulus with the
two-dimensional MOG is used. The likelihood of a particular
set of cue values for each Gaussian is defined by
              
                    1          1                                 Table 1: Descriptive statistics of distributions used to gener-
                                         ⊤ −1
  Gi (x) = φi              exp − (x − µi) Σi (x − µi )      (7)     ate training data. Means and standard deviations are in ms for
                2π|Σi |1/2       2
                                                                    VOT and VL and Hz for F1.
where Σ is the covariance matrix for the two cues. Other pa-                              Voiced                Voiceless
rameters are the same as those in (1) for each cue. As in the                       VOT     VL F1          VOT VL F1
cue weighting model, the overall likelihood of a set of cue                Mean     0       188 260        50      170 300
values is defined by (2). Figure 1B shows how this model                   SD       5       45      10     10      44     10
might represent categories defined by two cues.
                                                                436

                                                                     Simulation 2: Three cues
                                                                     The second set of simulations was designed to determine if
                                                                     the cue weighting model could also account for changes in
                                                                     the size of trading relations depending on the value of addi-
                                                                     tional acoustic cues in the signal. Toscano and McMurray
                                                                     (in preparation) found that the size of the trading relation be-
                                                                     tween VOT and VL was dependent on whether a third cue to
                                                                     voicing, F1 at voicing onset, was ambiguous or informative.
                                                                     In natural speech, this cue covaries with VOT, reducing the
                                                                     apparent size of the VOT/VL trading relation. Toscano and
                                                                     McMurray (in preparation) examined this in human listeners
                                                                     using synthetic speech that contained formant onsets that ei-
                                                                     ther covaried with VOT (similar to natural speech) or were
                                                                     held constant at an ambiguous value. This simulation will
                                                                     look at whether the cue weighting model produces a corre-
                                                                     sponding change in the size of the trading relation.
                                                                        50 models with four MOGs (VOT, VL, F1, and a combined
                                                                     MOG), were trained and their categorization was tested for F1
                                                                     values that covaried with VOT or were held constant.
                                                                     Training VOT, VL, and F1 values were randomly sampled
                                                                     from the distributions in Table 1. F1 values were estimated
                                                                     from acoustic measurements of the stimuli in Toscano and
                                                                     McMurray (in preparation). Each model was run for 90,000
                                                                     trials. Other parameters were identical to Simulation 1.
                                                                     Testing The testing procedure was the same as in Simula-
                                                                     tion 1, except that the model was presented with three acous-
                                                                     tic cues. The model was tested under two conditions. In one
                                                                     condition (constant), F1 values were held constant at 280 Hz.
Figure 3: (A) Data from human listeners from a picture iden-         In the second condition (covaried), F1 values covaried with
tification task with stimuli varying in VOT and VL. (B) Data         VOT in 10 Hz increments from 240 to 320 Hz.
from the cue weighting model. (C) Data from the multi-               Results Figure 4A shows the results for human listeners’
dimensional model.                                                   from Toscano and McMurray (in preparation) and Figure 4B
                                                                     shows the results from the cue weighting model. The model
                                                                     shows a reduced trading relation between VOT and VL when
Luce choice rule. This produced a response corresponding to
                                                                     F1 covaries with VOT, replicating the basic effect observed
the proportion of /p/ responses obtained for human listeners.
                                                                     with human listeners. These results indicate that the cue
Results Figure 3A shows behavioral data from human lis-              weighting model was able to account for the difference in the
teners from McMurray et al. (submitted), displaying propor-          size of the trading relation observed in the human data.
tion of /p/ responses as a function of VOT and VL. A moder-
ate shift of the VOT category boundary of about 5 ms for                                  General Discussion
the two different VL conditions can be seen. Figures 3B              The results of these simulations suggest that the cue weight-
and 3C show the results from the simulations with the cue            ing model and the reliability metric used here provide a rea-
weighting and multi-dimensional models, respectively. The            sonable account of listeners’ performance. They also suggest
results from the cue weighting model show a similar-sized            that a full model of the entire acoustic space may not be nec-
shift in the VOT boundary to the human data, but the multi-          essary, and, in fact, may not correctly represent the weights
dimensional model shows no observable shift in the predicted         that listeners assign to acoustic cues. Further, these simula-
direction. The RMS difference between the human data and
cue weighting models is 0.141, and between the human data                1 In both models, Gaussians tended to not overlap significantly.
and multi-dimensional models is 0.183.                               This led to a negligible VL effect in the multi-dimensional model.
                                                                     Because Gaussians could be separated along both cue dimensions,
    The results of the simulations indicate that the cue weight-     the model could approximate the close means of the VL data without
ing model provides a better fit to human listeners’ catego-          having overlapping categories. In contrast, VL means in the cue
rization. The multi-dimensional model did not show the pre-          weighting model were further apart than the means in the data, since
                                                                     the model could only reduce overlap for those Gaussians along that
dicted VL effect, suggesting that it did not use this less dis-      dimension. This produced a larger VL effect than we would expect
criminable cue and instead relied on VOT for categorization.1        based on the cue weighting metric alone.
                                                                 437

                                                                     sion, this reduces the computational difficultly of learning
                                                                     speech sound categories.
                                                                        The cue weighting model presented here extends the sta-
                                                                     tistical learning framework to explain how the distributions
                                                                     and weights of multiple cues can be learned over develop-
                                                                     ment. Also, the reliability metric used here provides a gen-
                                                                     eral method for weighting a multimodal distribution. This ap-
                                                                     proach may be useful for understanding other types of feature
                                                                     combination as well.
                                                                                          Acknowledgments
                                                                     We would like to thank J. Sean Allen and Joanne Miller for
                                                                     the acoustic data used to train the model. This research was
                                                                     supported by a University of Iowa Student Government Re-
                                                                     search Grant to JCT and NIH DC008089 to BM.
                                                                                              References
                                                                     Allen, J. S., & Miller, J. L. (1999). Effects of syllable-initial
                                                                        voicing and speaking rate on the temporal characteristics of
                                                                        monosyllabic words. Journal of the Acoustical Society of
                                                                        America, 106, 2031-2039.
                                                                     Jacobs, R. A. (2002). What determines visual cue reliability?
                                                                        Trends in Cognitive Sciences, 6, 345-350.
                                                                     Kalman, R. E. (1960). A new approach to linear filtering and
                                                                        prediction problems. Transcription of the ASME - Journal
Figure 4: Identification responses for three cues to voicing
                                                                        of Basic Engineering, 82, 35-45.
for either covaried or constant F1 values. (A) Data for human
                                                                     Lisker, L. (1978). Rapid vs. rabid: A catalogue of acoustic
listeners from Toscano and McMurray (in preparation). (B)
                                                                        features that may cue the distinction. Haskins Laboratories
Data from the cue weighting model from Simulation 2.
                                                                        Status Report on Speech Research, SR-54, 127-132.
                                                                     Lisker, L., & Abramson, A. S. (1964). A cross-linguistic
                                                                        study of voicing in initial stops: Acoustical measurements.
tions suggest that the information needed to weight cues is
                                                                        Word, 20, 384–422.
available in the statistics of the input − information that in-
                                                                     Maye, J., Werker, J., & Gerken, L. (2002). Infant sensitivity
fants already use to discover the category structure of individ-
                                                                        to distributional information can affect phonetic discrimi-
ual acoustic cues.
                                                                        nation. Cognition, 82, B101–B111.
   A multi-dimensional space may be used to map acoustic             McMurray, B., Aslin, R. N., & Toscano, J. C. (in press).
information onto speech categories at other levels of process-          Statistical learning of phonetic categories: Computational
ing. Indeed, this approach seems particularly useful for com-           insights and limitations. Developmental Science.
bining cues across orthogonal dimensions, such as manner,            McMurray, B., Clayards, M., Tanenhaus, M., & Aslin, R.
place and voicing for consonants, or frontness and backness             (submitted). Tracking the time course of phonetic cue inte-
for vowels (see Vallabha et al. (2007) for simulations with             gration during spoken word recognition.
a three-dimensional MOG learning vowel spaces). However,             McMurray, B., Horst, J., Toscano, J. C., & Samuelson, L. K.
this would not be needed if listeners only had to learn pho-            (in press). Towards an integration of connectionist learning
netic features rather than phoneme-like units and could base            and dynamical systems processing: case studies in speech
perception on these features. Note also that the cue weight-            and lexical development. In J. P. Spencer, M. Thomas, &
ing model may not be able to successfully combine cues if               J. McClelland (Eds.), Toward a new grand theory of de-
the within-category correlations between the cues are highly            velopment: Connectionism and dynamic systems theory re-
different (since it does not track these correlations). However,        considered. Oxford University Press.
we know of no set of acoustic cues for which this is the case.       Toscano, J. C., & McMurray, B. (in preparation). Integration
   The advantage of the weighting and integration approach              of temporally asynchronous cues to voicing in natural and
lies in the fact that a large number of acoustic cues can be            synthetic speech.
combined into a simpler representation. For a distinction            Vallabha, G. K., McClelland, J. L., Pons, F., Werker, J. F., &
such as word-medial voicing, in which there are at least 16             Amano, S. (2007). Unsupervised learning of vowel cate-
cues that contain the same phonetic category structure, it may          gories from infant-directed speech. Proceedings of the Na-
be difficult to learn the distributions of categories in a 16-          tional Academy of Sciences, 104, 13273-13278.
dimensional space. By combining cues into a single dimen-
                                                                 438

