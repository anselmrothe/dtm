UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Principles of Generalization for Learning Sequential Structure in Language
Permalink
https://escholarship.org/uc/item/5pn7s4fv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Frank, MIcheal C.
Ichinco, Denise
Tenenbaum, Joshua B.
Publication Date
2008-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

       Principles of Generalization for Learning Sequential Structure in Language
                                 Michael C. Frank, Denise Ichinco, and Joshua B. Tenenbaum
                                                     {mcfrank, ithink, jbt}@mit.edu
                                       Department of Brain and Cognitive Sciences, 43 Vassar Street
                                                          Cambridge, MA 02139 USA
                               Abstract                                     Proponents of analogical or associative theories have em-
                                                                         phasized the parsimony and neural plausibility of this type
   How do learners discover patterns in the sequential structure
   of their language? Infants and adults have surprising abilities       of proposal. In contrast, dual-route theorists have focused
   to learn structure in simple artificial languages, but the mecha-     on representational or expressive limitations of the analogical
   nisms are unknown. Here we introduce a rule-based Bayesian            approach. There are two dissociable issues captured by this
   model incorporating two principles: minimal generalization
   and representational parsimony. We apply our model to tasks           debate: (1) the number of routes for morphology learning and
   in artificial language learning and inflectional morphology and       (2) the algorithmic form and expressive power of those routes.
   show that it fits behavioral results from infants and adults and      For instance, recent work by Albright & Hayes (2003) com-
   learns inflectional rules from natural data.
                                                                         pared an analogical model with a rule-based model and found
   Keywords: Language acquisition; generalization; artificial
   language learning; inflectional morphology; Bayesian model-           that the greater expressivity of the rule-based model allowed
   ing.                                                                  for tighter generalization and better fit to human experimental
                                                                         data in a novel-word inflection wug task, despite the fact that
                           Introduction                                  both models had only one route for representation.
How do learners discover patterns in the sequential struc-                  Under a more general definition of a rule as a systematic
ture of their language? Experimental work on the unsuper-                regularity, rules can be both broad (as in the regular rule for
vised learning of sequential structure has suggested that in-            the past tense in English orthography, +ed, and narrow (as
fants and adults have access to flexible and powerful learning           in the past tense rule for the verb go: go → went. Within an
mechanisms which may be involved in language acquisition                 expressive enough hypothesis space, a rule could even be for-
(Gomez, 2002; Marcus et al., 1999). However, both the par-               mulated for analogical inferences like using inflections from
ticular mechanisms involved in these tasks and the aspects of            stems with high similarity.1 If we assume that the hypothe-
acquisition to which they apply are at present unknown.                  sis space of rules is broad enough to capture many different
   In our current work we attempt to address these questions             types of regularities, the problem of how to find the right rule
by creating a computational model which embodies two prin-               within this hypothesis space becomes more important.
ciples suggested by this experimental literature: minimal gen-              Our current work is not directly concerned with the exact
eralization and representational parsimony. We show that                 form of the representations used by human learners. Instead,
these principles apply not only to artificial language tasks, but        we assume that learners are attempting to make generaliza-
that they may also have applications to learning inflectional            tions from limited data within some hypothesis space and fo-
morphology, an important task facing language learners.                  cus on the principles by which they find the best generaliza-
   We first describe our model and how it embodies a trade-              tions in that space. Following Albright & Hayes (2003), our
off between these two principles within a hypothesis space               hypothesis space consists of sets of explicit rules, both for
expressive enough to capture many different types of rules.              their ease of interpretation and because Albright and Hayes’
We next show how our model can be applied to artificial lan-             data show that this kind of representation provides a better
guage experiments on learning identity-rules (Gerken, 2006;              fit to human generalizations. However, we take rules to be
Marcus et al., 1999) and non-adjacent dependencies (Gomez,               a representational convenience which we adopt at the high-
2002). We then present an extension of our model to the                  est of Marr’s (1982) levels of analysis: the level of compu-
case of inflectional morphology. Finally, we show prelimi-               tational theory. Thus, we focus here not on testing different
nary data indicating that our model can be applied directly to           kinds of representations, but instead on making explicit and
learning inflectional rules in natural language.                         individually testing the principles of generalization by which
   The representations and learning mechanisms involved in               particular rules are learned.
the acquisition of inflectional morphology have been hotly
debated in the literature on language acquisition. Two ba-                                        Model Design
sic positions have been proposed: a single process of ana-               We formalize the idea of a rule as a set of restrictions on
logical learning (Rumelhart & McClelland, 1986) or a dual                the features of a string. For instance, Marcus et al. (1999)
system consisting of both abstract rules and associative pro-            presented infants with strings like wo f e f e (three-syllable
cesses (Pinker, 1991). While this debate has been taken as               strings where the last two syllables were the same). In our
representative of a wider debate over the format of mental                   1 For instance, though the hypothesis space of our current model
representation, it has nevertheless tended to confound a num-            does not allow similarity-based rules (e.g., ”strings within some edit
ber of independent computational issues.                                 distance of X”), it would be relatively simple to add such rules.
                                                                     763

                                                                                 F1    F2    F3     F12    F13    F23     Translation
                                                  α                              wo     fe    fe     *      *      *      only wo f e f e
                      rule                                                        *     fe    fe     *      *      *      ends f e f e
                        ψ                                                        wo     *     fe     *      *      *      begins wo, ends f e
                                               cluster
                                                index                            wo     *     *      *      *      =      wo BB
                         clusters                  z                              *     *     *      *      *      =      ABB
                                                                                  *     *     *      *      *      *      any string
                                                string                        Table 1: Some of the rules consistent with the string
                                                   s                          wo f e f e, from Marcus et al. (1999). F1, e.g., refers to
                                                                              those features which describe the first element of the string,
                                                     strings                  while F23, e.g., refers to features that relate the second and
                                                                              third element of the string. A * denotes no restriction on a
                                                                              particular feature.
Figure 1: A schematic representation of the generative pro-
cess for our model. This process defines a distribution over
sets of strings by showing how they could be generated by
a set of underlying rules. In practice, we invert this process                “any set of three letters.” But there is a less general descrip-
through Bayesian inference, calculating the posterior proba-                  tion: “ab followed by any letter,” or “mn followed by any let-
bility that an observed set of strings was generated by a partic-             ter.” In this case, the better description seems to contain two
ular set of rules. Circles represent variables, arrows represent              specific rules rather than one more general rule. Taking this
dependencies, and rectangles (plates) group sets of elements                  principle to its logical conclusion, however, produces a very
that are repeated.                                                            unparsimonious set of rules: “abc,” “abd,” “abe,” “mnp,”
                                                                              “mnq,” or “mnr.” Though this description is very specific,
                                                                              it includes too many rules and fails to identify the generaliza-
models of artificial language tasks, we define features over                  tion linking subsets of the strings together. We formalize this
both individual elements (syllables, phonemes, or words de-                   intuition by including a prior on the number of rules used to
pending on the experiment) and pairs of elements (as in Ta-                   describe a set of strings. This prior is known as the Chinese
ble 1). For individual elements, our features simply denote                   Restaurant Process (CRP) (Rasmussen, 2000).
whether an element has a particular value (e.g., the first sylla-                 The Bayesian framework we use here gives us a principled
ble is wo). For pairs of elements, we restricted our hypothesis               method for trading off minimal generalization (which prefers
space to contain a single binary feature: the identity relation-              more specific rules, even if there are more of them) and repre-
ship in which two elements have exactly the same value re-                    sentational parsimony (which prefers fewer rules, even if they
gardless of what it is. By picking combinations of features                   are more general).
we can generate more complex rules like AAx (the first two
elements are the same and the third is x)—the rule used by                    Model details
Gerken (2006).2 The conjunction of a set of features makes                    A generative process (such as the one in Figure 1) is a se-
a rule; a rule is true of a particular string only if the string              quence of steps which jointly define a probability distribu-
contains all the features included in that rule.                              tion. By defining our model generatively we can use Bayesian
   The goal of our model is to find one or a small number of                  inference to calculate the posterior probability that a set of
rules which tightly describe the available data. Imagine the                  unobserved states—in our case, a set of rules and clusters—
set of strings abc abd abe. One description of these strings                  generated an observed product: a set of strings.
might be “a followed by any two letters.” However, intu-                          Following the arrows in Figure 1, in order to generate a
itively it seems as though the less general rule, “ab followed                string, we first decide what cluster c it belongs to (each cluster
by any one letter” is more likely. Our model formalizes this                  has one rule associated with it) by giving it a cluster index z.
principle of minimal generalization, known as the “size prin-                 If this is the first string we have generated, then the string
ciple” (Tenenbaum & Griffiths, 2001) by assigning probabili-                  must go in its own cluster; if we have generated some strings
ties to rules depending on how tightly they fit an observed set               already, we can decide whether the new string will fall in one
of strings.                                                                   of these pre-existing clusters or go in a cluster of its own. This
   Consider a second set of strings: abc abd abe mnp mnq                      process, the CRP, is governed by a concentration parameter α
mnr. Again, one description of this set of strings might be                   which controls how likely a new string is to go in its own
                                                                              cluster.
    2 In this type of hypothesis space it is possible to define inconsis-         Once we have decided on which cluster the string belongs
tent rules (e.g.,“first element is wo, second element is f e, and first       to, we then either use the rule ψ already assigned to that clus-
and second elements are the same”). We deal with this by excluding
inconsistent hypotheses from consideration and renormalizing the              ter or—in the case of a new cluster—randomly pick a rule to
probability of the remaining rules in the hypothesis space.                   go with it out of the space of rules Ψ. We then pick a string s
                                                                          764

uniformly from the set of strings that are consistent with ψ.           sample rule     # of clusters     α = .9      .09       .009
   Formally, the joint probability of a full corpus of strings S            ABA               1           -75.70    -73.47     -73.21
and a partition Z of those strings into rule clusters is given by          le B le            4           -77.84    -82.51     -89.15
                                                                           A di A             4           -83.38    -88.05     -94.70
                  P(S, Z|α) = P(S|Z) · P(Z|α)                 (1)         le di le           16          -112.59   -144.89    -179.16
The probability P(Z|α) of a partition is given by the CRP             Table 2: Log posterior probability of different hypotheses
with concentration parameter α. The probability of the corpus         (shown with an example of the maximum likelihood rule for
given the cluster assignments is the product of independent           one of the clusters for that hypothesis along with the total
terms for each string (corresponding to the plate over strings        number of clusters in that hypothesis) under different CRP
in Figure 1):                                                         parameter values. While the absolute probability of the dif-
                                                                      ferent clusterings changes relative to the value of α, the single
                      P(S|Z) = ∏ P(si |zi )                   (2)     cluster/rule hypothesis was always preferred.
                                    i
Because strings in each cluster c are generated by a particular
rule ψc for that cluster, we group the terms in Equation 2            of a particular rule, we made this choice because using to-
into a product over clusters and then a separate product over         kens rather than types would make a rule less probable with
strings in that cluster:                                              each repetition of the same string (intuitively, an undesirable
                                                                      consequence). One possible extension of our model to deal
             P(S|Z) = ∏      ∏ ∑ P(si |ψc ) · P(ψc )          (3)     with this issue would include another step in the generative
                         c i:zi =c ψc                                 process which generated tokens from types (Goldwater et al.,
However, because ψc is not known, in computing the proba-             2006).
bility of observing the strings associated with cluster c we in-
tegrate the predictions of all rules congruent with the strings              Experiment 1: Learning Identity Rules
in the cluster, weighted by their prior P(ψc ). For simplicity        In our first set of simulations we ran our model on the stimuli
we take this prior to be uniform, equal to the inverse of the         from two sets of experiments on artificial rule learning with
number of possible rules in our description language. The             infants. The first were those of Marcus et al. (1999), who
likelihood function for a rule is then given by                       familiarized infants to rules of the forms ABB, AAB, and ABA
                       (                                              and tested them on their ability to discriminate strings of this
                          1                                           form from strings of an alternate form (e.g., ABB vs. AAB as
                             if si consistent with ψc
          P(si |ψc ) = |ψc |                                  (4)     in the example above). The Marcus et al. stimuli were com-
                         0 otherwise
                                                                      posed of a vocabulary of eight syllables, of which 4 were des-
For each string, this probability is simply the probability of        ignated as A elements and 4 were designated as B elements,
a string being chosen uniformly from the set of strings con-          creating a total set of 16 tokens.
sistent with the rule for that cluster (the size principle). Put         The second set of stimuli came from Gerken (2006), who
another way, the size of a rule |ψ| is given by the rule’s ex-        tested infants on sets of four strings drawn either from an AAB
tension: the number of ways the symbols—syllables, letters,           rule or a narrower AAx rule (the first two elements the same
or phonemes—of a language can be combined that are con-               followed by x). Gerken found that even though both sets of
gruent with the rule. Since larger rules will be less likely to       strings were consistent with the broader AAB rule, infants
generate a particular example, our model favors minimal gen-          showed evidence of learning the AAx rule when all the evi-
eralization by giving highest probability to the smallest rule        dence was consistent with the narrower generalization.
that could have generated the observed data. The CRP prior               For each of the three Marcus et al. (1999) rules and for
in turn ensures representational parsimony by giving higher           the two Gerken (2006) rules, our model assigned the high-
probability to hypotheses with fewer different rules, whatever        est posterior probability to the hypothesis that all the strings
their size.                                                           were generated by the same rule; the rule with the highest
   Because we pose our model as a generative process, we              likelihood for this cluster was the rule posited by the re-
are able to invert that process using Bayes’ rule and compute         searchers (e.g., ABA). Although the likelihood of a partition
the posterior probability of a hypothesis (a partition of strings     of the strings into several more specific rules, e.g. “le B le,”
into clusters and rules to go with those clusters) given a set of     “wi B wi,” “ ji B ji,” or “de B de” was higher, the prior was
strings. In practice, we use a Gibbs sampler to search for the        considerably lower, leading to a consistent preference for the
best partition of strings into clusters (MacKay, 2003).               single cluster hypothesis (Table 2).
   All simulations were conducted using types rather than to-
kens. Accordingly, only one example of a particular string             Experiment 2: Using Variability to Generalize
was included in the training set for our model, even if strings       Does increasing type variability strengthen generalizations?
were presented multiple times in the original experiment.             Gomez (2002) presented learners with three-word strings
Since we used the size principle to determine the likelihood          containing an invariant dependency between the first and third
                                                                  765

                                                    log ! = −2                log ! = −20                log ! = −45              human participants by calculating the probability of choosing
                                              0                          0                          0
log posterior probability
                                                                                                                  1 rule          a correct string over an incorrect string via a Luce choice rule
                                                                                                                  3 rules
                                  −200                                −200                       −200                             (Luce, 1963) comparing the posterior probability of correct
                                                                                                                                  and incorrect strings under the model (Figure 2). While the
                                  −400                                −400                       −400
                                                                                                                                  level of variability at which the model was able to discrim-
                                  −600                                −600                       −600                             inate strings correctly varied widely with different values of
                                  −800                                −800                       −800
                                                                                                                                  α, the qualitative trend remained constant: a tradeoff between
                                                  2 6 12         24          2 6 12         24          2 6 12         24         preferring representational parsimony (prior) with less evi-
                                                                                                                                  dence and minimal generalization (likelihood) as the amount
             posterior choice probability
                                             1                                                                                    of available evidence increased.
                                                                                                          log ! = −2
                                                                                                                                          Experiment 3: Artificial Inflectional
                                            0.5
                                                                                                          log ! = −20                               Morphology
                                                                                                          log ! = −45
                                                                                                          human data              In order to test the performance of the model in fitting a more
                                             0
                                                            2               6             12         24
                                                                                                                                  complex range of human data (including production data as
                                                                 variability of intevening X element                              opposed to forced choice accuracies), we conducted a simple
                                                                                                                                  experiment with adults using artificial morphological stimuli.
Figure 2: Results from our simulations of experiments by
Gomez (2002). Top: log posterior probability of clusterings                                                                       Experimental paradigm
with either 1 or 3 rules (corresponding either to a learn noth-                                                                   Participants Twenty-one participants from the MIT com-
ing rule or a correct generalization) at three different values                                                                   munity participated for payment.
for α, the CRP parameter. Bottom: model performance at test
                                                                                                                                  Materials and Methods Participants were told they were
compared with human data reported by Gomez (2002). Error
                                                                                                                                  learning about the language of a remote island and were given
bars show standard error of the mean.
                                                                                                                                  sets of 20 index cards (each of which had on it a noun from
                                                                                                                                  the island’s language paired with its plural form). They were
                                                                                                                                  told that they could spread out the cards and rearrange them
elements (e.g., generated by the rule aXb, where the identity                                                                     any way they wanted in order to learn the language best. They
of the X element varied). They manipulated how many ele-                                                                          were then given a sheet with fifteen novel nouns and asked to
ments could appear in the X position of the string (data shown                                                                    fill in the plural form for each noun and give a confidence
in Figure 2) and found that participants were able to learn the                                                                   rating. Participants in all three conditions received the same
specific rules of the language only when the variability of the                                                                   test materials.
X element was greater than 12 elements, concluding that vari-                                                                         Participants saw index cards from one of three conditions,
ability in adjacent dependencies might lead to greater atten-                                                                     which we called multiple rules, reduplicative rules, and rule
tion to non-adjacent dependencies.                                                                                                plus exceptions. In all conditions, rules were suffix rules
   We tested whether our principle of representational parsi-                                                                     which required adding material to the end of a stem; no rules
mony (embodied in the CRP prior on the number of rules                                                                            conflicted in their application—in both the training and the
the model learns) could be responsible for the results they                                                                       test sets, only one rule applied to each stem. Stems were
observed. We ran our model on the same set of strings and                                                                         multi-syllabic, pronounceable non-words that did not sound
found that the model showed the same qualitative tradeoff                                                                         recognizably English-like.
as participants, switching between parsimony of representa-                                                                           In the multiple rules condition, we defined five rules, each
tion and minimal generalization by learning only a single rule                                                                    of which was attested in four examples. Each rule applied
(“accept any string”) for |X| = 2, but quickly moving to the                                                                      only to stems with a particular ending: for instance, +ene /
correct generalization (learning three rules, “a b,” “c d,” or                                                                        i j (“add ene if the word ends with t”) was a rule that applied
“e f ”) for variability greater than 2.                                                                                           to the words gimi j, vari j, ipi j, and haspadi j.
   Why does the model prefer to generalize at such a low rate                                                                         In the reduplicative rules condition, there were two rules,
of variability compared with the human participants? One                                                                          each with 10 examples: +em and reduplicate last syllable
reason might be the memory limitations of human learners:                                                                         / a (“repeat the last syllable of words with an a in the
perhaps human learners can only appreciate some of the ev-                                                                        second-to-last position,” as in the stem-inflected form pair
idence for a particular inference at any given time. In other                                                                     vigutap → vigutaptap).
work, we have used a memory decay function over tokens to                                                                             The final condition was the rules-plus-exceptions condi-
simulate this kind of limited use of evidence. Given the sim-                                                                     tion, in which participants saw 17 examples of one suffix
plicity of the current experiment, however, we chose to simu-                                                                     rule, two examples of another, and then a third irregular form,
late the limited use of evidence by lowering the α parameter                                                                      meant to simulate a system like the English plural or past
on the CRP until the model strongly dispreferred hypotheses                                                                       tense where there is an overwhelmingly frequent rule with
with more rules. We then modeled the forced-choice task of                                                                        only a relatively small number of exceptions.
                                                                                                                            766

Figure 3: For each condition of Experiment 3, we show the clusters found by our model (left side) and the clusters in participants
responses (right side). Each inflection (e.g., +em) was given a different grayscale value. For the human participants, the 15
rows in each plot represent the items in the generalization test and the 7 columns represent the 7 participants in each condition.
Results Average pairwise similarity between participants in                 Fit to data
the multiple rules condition was 81.9%; in the reduplicative
rules condition, 83.4%; and in the rules plus exceptions con-               To test the fit of our model to the data we collected, we ran the
dition, 86.8% (Figure 3). These pairwise similarities differed              model on each training set. For each of the three conditions,
significantly from chance (computed via permutation of par-                 the clustering with the maximum posterior probability was
ticipants’ responses): all ps < .0001, all ts > 18.                         the one we intended; the maximum likelihood rule (schema
                                                                            and inflectional rule) for each cluster similarly matched our
Inflectional Model                                                          intended design. To model generalization to novel test items
To adapt our model to inflectional data, we added a step to                 in each condition, we chose the maximum a posteriori hy-
the generative process. Each cluster was assigned both a rule               pothesis in the model and used it to generate the maximum a
schema (what we referred to as a rule in the initial model: a               posteriori inflected form for each stem (Figure 3).
set of features) and an inflectional rule (a procedure for mod-                The sets of rules preferred by the inflectional models pro-
ifying a stem—what we called a string in the initial model—                 duced generalizations that were highly similar to those of our
to create an inflected form). The rule schemata in this model               human participants (and performance was robust to manipu-
were defined over phonemes and positions in the stem (count-                lation of the CRP parameter α). In the multiple rules condi-
ing backwards from the end of the stem). For instance, pos-                 tion, the model produced the same form as the human partici-
sible features could be the last phoneme is e or the second to              pants in 93 of 105 cases (88.6%); in the reduplicative rules
last phoneme is t.                                                          condition, 98 of 105 cases (93.3%); and in the rules plus
   Inflectional rules were defined as a set of deterministic                exceptions condition, 95 of 105 cases (90.5%). In each of
transformations to be performed on each stem. For instance,                 the three experiments, three participants produced exactly the
if the inflectional rule were +ed / t, the full rule (schema                same pattern of judgments as the model while the other four
and inflectional rule) would be consistent if all stems in the              differed by no more than 4 of 15 judgments. These results
cluster ended with t and all inflected forms were suffixed by               suggest that the model effectively recovered the same struc-
ed. In practice we included four possible operations for in-                ture from the training data as the human participants.
flection, which could be combined as necessary to create the
proper inflected form: adding a suffix, reduplicating a suffix,                      Experiment 4: Natural Morphology
substituting a vowel, and substituting an entire word. Because
                                                                            In order to test the generality of the inflectional form of
the space of rules in this and the next experiment was larger
                                                                            our model, we applied the version described in the previ-
than the space in the first two experiments (due to the greater
                                                                            ous section to the problem of learning the English past tense.
length of strings), we calculated only the highest-probability
                                                                            We carried out preliminary simulations using a phonetically-
schema and inflectional rule for each cluster.3
    3 It was not possible to calculate the number of phonetically legal     ing the number of words in the training data that were congruent
words congruent with a particular schema (as we did in the artificial       with that schema. Provided that the training data is a representative
language examples). To compute the likelihood of a string given a           sample of the overall corpus, the relative values of |ψ| should be
rule (Equation 4), we approximated the size of a schema by count-           comparable using this estimate.
                                                                        767

        Frequency         Rule        Example stem (past)            Experiment 4, we ran our model on a subset of English past
            63        +d / p            appear (appeared)            tense forms and found that the model acquired linguistically
            23         +@d / t            want (wanted)              plausible rules and generalized them with relative accuracy.
            12             +d            show (showed)               Taken together, these data suggest that the general principles
            11         +@d / d            need (needed)              of minimal generalization and representational parsimony—
             9          +t / k            look (looked)              combined within an expressive hypothesis space—may be
             9          +t / s        increase (increased)           sufficient to account for a wide range of phenomena in se-
             8          +t / p            stop (stopped)             quential linguistic generalization.
             5          +t /    S        watch (watched)
             4           Ø/ t                put (put)                                   Acknowledgments
             3        o→u/ o               know (knew)               The first author was supported by a Jacob Javits Grant. The
                           ...                                       authors thank Adam Albright, Noah Goodman, Vikash Mans-
             1        go → wEnt             go (went)                inghka, Steve Piantadosi, Gary Marcus, and Ed Vul for valu-
             1         gEt → gat             get (got)               able discussion.
Table 3: A sample of the most frequent rules found by the                                     References
inflectional model (Experiment 4) when run on the 200 most           Albright, A., & Hayes, B.(2003). Rules vs. analogy in english
frequent English present-past verb form pairings.                       past tenses: a computational/experimental study. Cogni-
                                                                        tion, 90(2), 119-161.
                                                                     Gerken, L. (2006). Decisions, decisions: infant language
transcribed corpus of present-past verb form pairs.4 We                 learning when multiple generalizations are possible. Cog-
trained the model on the 200 most frequent past-tense phono-            nition, 98(3), B67-B74.
logical forms in English.
                                                                     Goldwater, S., Griffiths, T. L., & Johnson, M.(2006). Interpo-
   Results for the most frequently applied rules are shown in
                                                                        lating between types and tokens by estimating power-law
Table 3. Since our model was only able to restrict particular
                                                                        generators. Advances in Neural Information Processing
elements of a string to one value (rather than a class of val-
                                                                        Systems, 18.
ues, e.g., unvoiced phonemes), it was not able to capture the
specific selectional regularities of the English past. Despite       Gomez, R. L. (2002). Variability and detection of invariant
this, when we tested it on its generalization to the other forms        structure. Psychological Science, 13(5), 431-436.
in the corpus, it successfully inferred the correct form 88.5%       Luce, R. D.(1963). Detection and recognition. In R. D. Luce,
of the time (1753 of 1981 forms correct). Despite the limits            R. R. Bush, & E. Galanter (Eds.), Handbook of mathemat-
on the hypothesis space for rule schemata, the rules that the           ical psychology. New York: Wiley.
model learned on this small training set were similar to those       MacKay, D. J. C. (2003). Information theory, inference, and
that might be written in a phonology text (Table 3). In fu-             learning algorithms. Cambridge, UK: Cambridge Univer-
ture work we hope to further increase the expressiveness of             sity Press.
our hypothesis space in order to evaluate the generalization         Marcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M.
performance of the model.                                               (1999). Rule learning by seven-month-old infants. Science,
                                                                        283(5398), 77.
                    General Discussion                               Marr, D. (1982). Vision: A computational investigation into
On the basis of previous experimental work, we proposed two             the human representation and processing of visual infor-
principles for sequential generalization in language: minimal           mation. New York: Henry Holt and Co., Inc.
generalization and representational parsimony. We formal-            Pinker, S. (1991). Rules of language. Science, 253, 530-535.
ized these principles in a Bayesian model. In Experiment 1,          Rasmussen, C. E. (2000). The infinite gaussian mixture
we showed that the principle of minimal generalization al-              model. Advances in Neural Information Processing Sys-
lowed our model to fit data on infant rule learning. In Ex-             tems, 12.
periment 2, we showed that increasing evidence via variabil-
                                                                     Rumelhart, D. E., & McClelland, J. L.(1986). On learning the
ity gave greater support for generalization, suggesting a bias
                                                                        past tense of english verbs. In J. McClelland & D. Rumel-
for representational parsimony. In Experiment 3, we further
                                                                        hart (Eds.), Parallel distributed processing: Explorations
tested these principles by altering our model to handle inflec-
                                                                        in the microstructure of cognition (Vol. 2). Cambridge,
tional tasks and comparing this new model with human per-
                                                                        MA: MIT Press.
formance on three simple artificial inflection systems. We
found a tight correspondence between the performance of the          Tenenbaum, J. B., & Griffiths, T. L. (2001). Generalization,
model and the productions of human participants. Finally, in            similarity, and bayesian inference. Behavioral and Brain
                                                                        Sciences, 24(4), 629-640.
    4 Corpus data were obtained from the website of Bruce Hayes
(http://www.linguistics.ucla.edu/people/hayes/learning/).
                                                                 768

