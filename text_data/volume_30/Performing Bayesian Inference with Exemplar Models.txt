UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Performing Bayesian Inference with Exemplar Models
Permalink
https://escholarship.org/uc/item/4kt2j29t
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Shi, Lei
Feldman, Naomi H.
Griffiths, Thomas L.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        Performing Bayesian Inference with Exemplar Models
                                                       Lei Shi (lshi@berkeley.edu)
                Helen Wills Neuroscience Institute, University of California at Berkeley, Berkeley, CA 94720 USA
                                        Naomi H. Feldman (naomi feldman@brown.edu)
                  Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA
                                        Thomas L. Griffiths (tom griffiths@berkeley.edu)
                    Department of Psychology, University of California at Berkeley, Berkeley, CA 94720 USA
                              Abstract                                  kind of answer focuses on the neural level, exploring ways
                                                                        in which systems of neurons could perform probabilistic
   Probabilistic models have recently received much attention as
   accounts of human cognition. However, previous work has fo-          computations. The language of such answers is that of
   cused on formulating the abstract problems behind cognitive          neurons, tuning curves, firing rates, and so forth (e.g., Ma,
   tasks and their probabilistic solutions, rather than considering     Beck, Latham, & Pouget, 2006). A second kind of answer
   mechanisms that could implement these solutions. Exemplar
   models are a successful class of psychological process mod-          is at the level of psychological processes – showing that the
   els that use an inventory of stored examples to solve prob-          Bayesian inference can be performed using mechanisms that
   lems such as identification, categorization and function learn-      are used in psychological process models. The language of
   ing. We show that exemplar models can be interpreted as a
   sophisticated form of Monte Carlo approximation known as             such answers is representations, similarity, activation, and so
   importance sampling, and thus provide a way to perform ap-           forth (e.g., Kruschke, 2006; Sanborn, Griffiths, & Navarro,
   proximate Bayesian inference. Simulations of Bayesian infer-         2006).
   ence in speech perception and concept learning show that ex-
   emplar models can account for human performance with only               Our focus in this paper will be on a class of psychologi-
   a few exemplars, for both simple and relatively complex prior        cal process models known as exemplar models. These mod-
   distributions. Thus, we show that exemplar models provide a          els assume that people store many instances (“exemplars”)
   possible mechanism for implementing Bayesian inference.
                                                                        of events in memory, and evaluate new events by activating
   Keywords: Bayesian inference; exemplar models; speech per-           stored exemplars that are similar to those events (Medin &
   ception; concept learning
                                                                        Schaffer, 1978; Nosofsky, 1986). It is well known that exem-
   Much of cognition and perception involves inference un-              plar models of categorization can be analyzed in terms of non-
der uncertainty, using limited data from the world to evaluate          parametric density estimation, and implement a Bayesian so-
underdetermined hypotheses. Probabilistic models provide a              lution to this problem (Ashby & Alfonso-Reese, 1995). Here
way to characterize the optimal solution to these problems,             we show that exemplar models can be used to solve prob-
with probability distributions encoding the beliefs of agents           lems of Bayesian inference more generally, providing a way
and Bayesian inference updating those distributions as data             to approximate expectations of functions over posterior distri-
become available. As a consequence, probabilistic models are            butions. Our key result is that exemplar models can be inter-
becoming increasingly widespread in both cognitive science              preted as a sophisticated form of Monte Carlo approximation
and neuroscience, providing explanations of behavior in do-             known as importance sampling. This result illustrates how
mains as diverse as motor control (Körding & Wolpert, 2004),           Bayesian inference can be performed using a simple mecha-
reasoning (Oaksford & Chater, 1994), memory (Anderson                   nism that is a common part of psychological process models.
& Milson, 1989), and perception (Yuille & Kersten, 2006).
                                                                                                Background
However, these explanations are typically presented at Marr’s
(1982) computational level, focusing on the abstract problem            Exemplar models
being solved and the logic of that solution. Unlike many other          Human knowledge is formed from examples. When we
formal approaches to cognition, probabilistic models are usu-           learned the concept “dog,” we were not taught to remember
ally not intended to provide an account of the mechanisms               the physiological and anatomical characteristics of dogs, but
underlying behavior – how people actually produce responses             instead, saw examples of various dogs. Based on the large
consistent with optimal statistical inference.                          inventory of examples of dogs we have seen, we are able to
   Understanding the mechanisms that could support                      reason about the properties of dogs, and make decisions about
Bayesian inference is particularly important since probabilis-          whether new objects we encounter are likely to be dogs. Ex-
tic computations can be extremely challenging. Representing             emplar models provide a simple explanation for how we do
and updating distributions over large numbers of hypotheses             this, suggesting that we do not form abstract generalizations
is computationally expensive, a fact that is often viewed               from experience, but rather store examples in memory and
as a limitation of “rational” models. The question of how               use those stored examples as the basis for future judgments
people could perform Bayesian inference can be answered                 (Medin & Schaffer, 1978; Nosofsky, 1986).
at at least two levels (as suggested by Marr, 1982). One                   An exemplar model consists of stored exemplars X ∗ =
                                                                    745

{x∗1 , x∗2 , · · · , x∗n }, and a similarity function s(x, x∗ ), measuring     regarding the hypotheses before seeing d using a probabil-
how closely a new observation x is related to x∗ . On observ-                  ity distribution, p(h), known as the prior distribution. Then,
ing x, all exemplars are activated in proportion to s(x, x∗ ). The             the degrees of belief after seeing d are given by the posterior
use of the exemplars depends on the task (Nosofsky, 1986).                     distribution, p(h|d), obtained from Bayes’ rule
In an identification task, where the goal is to identify the x∗
                                                                                                                      p(d|h)p(h)
of which x is an instance, the probability of selecting x∗i is                                          p(h|d) =                                 (4)
                                                                                                                   H p(d|h)p(h) dh
                                                s(x, x∗i )
                             pr (x∗i |x) =                     ,       (1)     where H is the set of hypotheses under consideration, and
                                            ∑ j=1 s(x, x∗j )
                                              n
                                                                               p(d|h) is a distribution indicating the probability of seeing d
                                                                               if h were true, known as the likelihood.
where pr (·) denotes the response distribution resulting from                      While our analysis applies to Bayesian inference in the
the exemplar model, and we assume that participants use the                    general case, we will focus on a specific example. Assume
Luce choice rule (Luce, 1959) in selecting a response, with no                 we observe a stimulus x, which we believe to be corrupted by
biases towards particular exemplars. In a categorization task,                 noise and potentially missing some accompanying informa-
where each exemplar x∗i is associated with a category ci , the                 tion, such as a category label. Let x∗ denote the uncorrupted
probability that the new object x will be assigned to category                 stimulus, and z denote the missing data. If there is no missing
c is given by                                                                  data (i.e. z is empty), then our goal is simply to reconstruct x,
                                                                               finding the x∗ to which it corresponds. Otherwise, we seek to
                                          ∑ j|c j =c s(x, x∗j )
                             pr (c|x) = n                       ,      (2)     infer both x∗ and the value of z which corresponds to x. We
                                           ∑ j=1 s(x, x∗j )                    can perform both tasks using Bayesian inference.
                                                                                   The application of Bayes’ rule is easier to illustrate in the
where again we assume a Luce choice rule without biases
                                                                               case where there is no missing data z, and we simply wish
towards particular categories.
                                                                               to infer x∗ . We will use the probability distribution p(x|x∗ )
   While exemplar models have been most prominent in the                       to characterize the noise process, indicating the probability
literature on categorization, the same basic principles have                   with which the stimulus x∗ is corrupted to x, and the proba-
been used to define models of function learning (DeLosh,                       bility distribution p(x∗ ) to encode our a priori beliefs about
Busemeyer, & McDaniel, 1997), probabilistic reasoning                          the probability of seeing a given stimulus. We can then use
(Juslin & Persson, 2002), and social judgment (Smith &                         Bayes’ rule to compute the posterior distribution over the
Zarate, 1992). These models pursue a similar approach to                       value of the uncorrupted stimulus, x∗ , which might have gen-
models of categorization, but associate each exemplar with a                   erated the observation x, obtaining
quantity other than a category label. For example, in func-
tion learning each exemplar is associated with the value of                                                          p(x|x∗ )p(x∗ )
                                                                                                     p(x∗ |x) =                         ,        (5)
a continuous variable rather than a discrete category index.                                                        p(x|x∗ )p(x∗ ) dx∗
The procedure for generating responses remains the same as
that used in Equations 1 and 2: the associated information is                  where p(x|x∗ ) is the likelihood and p(x∗ ) is the prior.
averaged over exemplars, weighted by their similarity to the                       This analysis is straightforward to generalize to the case
stimulus. Thus, the predicted value of some associated infor-                  where z contains important missing data, such as the category
mation f for a new stimulus x is                                               from which x was generated. In this case, we need to define
                                                                               our prior as a distribution over both x∗ and z, p(x∗ , z). We
                                       ∑nj=1 f j s(x, x∗j )                    can then use Bayes’ rule to compute the posterior distribution
                                 fˆ =                       ,          (3)     over the uncorrupted stimulus, x∗ , and missing data, z, which
                                        ∑nj=1 s(x, x∗j )
                                                                               might have generated the observation x, obtaining
where f j denotes the information associated with the jth ex-                                                        p(x|x∗ )p(x∗ , z)
emplar. The identification and categorization models can be                                     p(x∗ , z|x) =                                   (6)
                                                                                                                    p(x|x∗ )p(x∗ , z) dx∗ dz
viewed as special cases, corresponding to different ways of
specifying f j . Taking f j = 1 for j = i and 0 otherwise yields               where we also assume that the probability of observing x is
Equation 1, while taking f j = 1 if c j = c and 0 otherwise                    independent of z given x∗ , so p(x|x∗ , z) = p(x|x∗ ).
yields Equation 2. Equation 3 thus provides the general for-
mulation of an exemplar model that we will analyze.                                   Evaluating expectations by Monte Carlo
                                                                               Posterior distributions on hypotheses given data can be used
Bayesian inference                                                             to answer a variety of questions. To return to the example
Many cognitive problems can be formulated as evaluating a                      above, a posterior distribution on x∗ and z can be used to
set of hypotheses about processes that could have produced                     evaluate the properties of x∗ and z given x. For any function
observed data. Bayesian inference provides a solution to                        f (x∗ , z), the expectation of that function given x is
problems of this kind. Letting h denote a hypothesis and d                                                       
the data, assume a learner encodes his or her degrees of belief                             E [ f (x∗ , z)|x] =      f (x∗ , z)p(x∗ , z|x) dx∗ dz (7)
                                                                           746

being the average of f (x∗ , z) over the posterior distribution.                  where we use the assumption that p(x|x∗ , z) = p(x|x∗ ). Sub-
Since f (x∗ , z) can pick out any property of x∗ and z that might                 stituting these weights into Equation 10 , we obtain
be of interest, many problems of reasoning under uncertainty
can be expressed in terms of expectations. However, evalu-                                                           ∑mj=1 f (x∗j , z j )p(x|x∗j )
                                                                                                 E [ f (x∗ , z)|x] ≈                                (12)
ating expectations over the posterior distribution can be chal-                                                          ∑mj=1 p(x|x∗j )
lenging: it requires computing a posterior distribution, which
is a hard problem in itself, and the integrals in Equation 7 can                  where we assume that x∗j and z j are drawn from p(x∗ , z).
range over many values for x∗ and z. Consequently, Monte
Carlo methods are often used to approximate expectations.                              Exemplar models as importance samplers
   The Monte Carlo method approximates the expectation of                         Inspection of Equations 3 and 12 yields our main result: that
a function with respect to a probability distribution with the                    exemplar models can be viewed as implementing a form of
average of that function at points drawn from the distribution.                   importance sampling. More formally, assume X ∗ is a set of
Assume we want to evaluate the expectation of a function                          m exemplars x∗ and associated information z drawn from the
g(y) over the distribution p(y), E p [g(y)]. Let μ denote the                     probability distribution p(x∗ , z), and f j = f (x∗j , z j ) for some
value of this expectation. The law of large numbers justifies                     function f (x∗ , z). Then the output of Equation 3 for an exem-
                                                 m                               plar model with exemplars X ∗ and similarity function s(x, x∗ )
                                             1
    μ = E p [g(y)] =      g(y)p(y) dy ≈           ∑ g(y j ) = μ̂MC       (8)      is an importance sampling approximation to the expectation
                                            m    j=1                              of f (x∗ , z) over the posterior distribution on x∗ and z, as given
where the y j are all drawn from the distribution p(y).                           in Equation 6, for the Bayesian model with prior p(x∗ , z) and
   Using the Monte Carlo method requires that we are able to                      likelihood p(x|x∗ ) ∝ s(x, x∗ ).
generate samples from the distribution p(y). However, this is                        This connection between exemplar models and importance
often not the case: it is quite common to encounter problems                      sampling provides an alternative rational justification for ex-
where p(y) is known at all points y but hard to sample from.                      emplar models of categorization, as well as a more general
If another distribution q(y) is close to p(y) but easy to sam-                    motivation for these models. The justification for exemplar
ple from, a form of Monte Carlo called importance sampling                        models in terms of nonparametric density estimation (Ashby
can be applied (see Neal, 1993, for a detailed introduction).                     & Alfonso-Reese, 1995) provides a clear account of their rel-
Manipulating the expression for the expectation of g gives                        evance to categorization, but does not explain why they are
                                                                                  appropriate in other contexts, such as identification (Equation
                                                                                1) or the general response rule given in Equation 3. In con-
                                                 g(y) q(y)p(y)
                                                                q(y) dy
                             g(y)p(y) dy                                          trast, we can use importance sampling to provide a single ex-
          g(y)p(y) dy =                  =        p(y)                  (9)
                               p(y) dy                                            planation for identification, categorization, and other uses of
                                                        q(y) q(y) dy
                                                                                  exemplar models, viewing each as the result of approximat-
 The numerator and denominator of this expression are each
                                                                                  ing an expectation of a particular function f (x∗ , z) over the
expectations with respect to q(y). Applying simple Monte
Carlo (with the same set of samples from q(y)) to both,                           posterior distribution p(x∗ , z|x). For identification, z is empty
                                                                                  and f (x∗ , z) = 1 for all x∗ within a small range ε of a specific
                                               p(y )
                                 ∑mj=1 g(y j ) q(y jj )                           value x∗i and 0 otherwise. For categorization, z contains the
               μ = E p [g(y)] ≈           p(y j )
                                                         = μ̂IS         (10)      category label, and f (x∗ , z) = 1 for all z = c and 0 otherwise.
                                    ∑mj=1 q(y j )                                 For function learning, z contains the value of the continuous
                                                                  p(y )           variable associated with x∗ , and f (x∗ , z) = z. Similar analy-
 where each y j is drawn from q(y). The ratios q(y j ) can be
                                                                     j            ses apply in other cases, with exemplar models providing a
viewed as “weights” on the samples y j , correcting for hav-                      rational method for answering questions expressed as an ex-
ing sampled from q(y) rather than p(y). Samples with higher                       pectation of a function of x∗ and z.
probability under p(y) than q(y) occur less often than if we
were sampling from p(y), but receive greater weight.                                                            Simulations
   Both simple Monte Carlo and importance sampling can
                                                                                  The success of importance sampling as a scheme for approxi-
be applied to the problem of evaluating the expectation of
                                                                                  mating expectations justifies using exemplar models as an ap-
a function f (x∗ , z) over a posterior distribution on x∗ and                     proximation to Bayesian inference. In this section, we evalu-
z with which we began this section. Simple Monte Carlo
                                                                                  ate exemplar models as a scheme for approximating Bayesian
would draw values of x∗ and z from the posterior distribu-                        inference in two tasks, examining the effect of number of
tion p(x∗ , z|x) directly. Importance sampling would generate                     exemplars on performance in order to evaluate the conse-
from another distribution, q(x∗ , z), and then reweight those
                                                                                  quences of biological and psychological constraints.
samples. One simple choice of q(x∗ , z) is the prior, p(x∗ , z).
If we sample from the prior, the weight assigned to each sam-                     The perceptual magnet effect
ple is the ratio of the posterior to the prior                                    The perceptual magnet effect is a categorical effect in speech
              p(x∗ , z|x)               p(x|x∗ )                                  perception in which discriminability of speech sounds is re-
                   ∗
                          =                                           (11)      duced near phonetic category prototypes and enhanced near
               p(x , z)          p(x|x∗ )p(x∗ , z) dx∗ dz
                                                                              747

category boundaries, presumably due to a perceptual bias to-                  (a)                              Perceived Stimuli Based on 10 Exemplars   (b)                              Perceived Stimuli Based on 50 Exemplars
ward phonetic category centers (Kuhl, Williams, Lacerda,                                                                MDS
                                                                                                                        Model
                                                                                                                                                                                                   MDS
                                                                                                                                                                                                   Model
Stevens, & Lindblom, 1992). Feldman and Griffiths (2007)
                                                                                Location in Perceptual Space                                               Location in Perceptual Space
argued that this effect can be characterized as Bayesian infer-
ence if one assumes that listeners are using their knowledge
of phonetic categories to optimally recover the phonetic de-
tail of a speaker’s target production through a noisy speech
signal. Here we demonstrate than an exemplar model derived
through importance sampling can provide a psychologically
plausible implementation of this Bayesian model, mirroring
human performance with a reasonable number of exemplars.                                                           μ Stimulus Number
                                                                                                                1 2 3 4 5 6 7 8 9 10 11 12 13
                                                                                                                                           μ                                                  μ Stimulus Number
                                                                                                                                                                                           1 2 3 4 5 6 7 8 9 10 11 12 13
                                                                                                                                                                                                                      μ/e/
                                                                                                                    /i/                     /e/                                                /i/
   The Bayesian model assumes that a speaker’s target pro-
duction T is sampled from a Gaussian phonetic category c
with category mean μc and category variance σ2c and that lis-                Figure 1: Locations of stimuli in perceptual space from Iver-
teners hear a speech sound S, perturbed by articulatory and                  son and Kuhl’s (1995) multidimensional scaling data and
acoustic noise, that is normally distributed around the target               from a single hypothetical subject (open circles) and the mid-
production T with noise variance σ2S . The prior on target pro-              dle 50% of hypothetical subjects (solid lines) using an exem-
ductions is therefore a mixture of Gaussians representing a                  plar model in which perception is based on (a) ten and (b)
language’s phonetic categories,                                              fifty exemplars. The labels μ/i/ and μ/e/ show the locations
                                                                             of category means in the model.
                    p(T ) = ∑ N(μc , σ2c )p(c)                 (13)
                              c
and the likelihood function is a Gaussian whose variance is                  in perceptual space in both data and model. The simulations
determined by the speech signal noise,                                       suggest that a relatively small number of exemplars suffices
                                                                             to capture human performance in this perceptual task. Model
                       p(S|T ) = N(T, σ2S )                    (14)          performance using ten exemplars already demonstrates the
                                                                             desired effect, and with fifty exemplars, the model gives a
Listeners hear the speech sound S and use Bayes’ rule to com-                precise approximation that closely mirrors the combined per-
pute the expectation E[T |S] and optimally recover the pho-                  formance of the 18 subjects in Iverson and Kuhl’s multidi-
netic detail of a speaker’s target production.                               mensional scaling experiment.
   To perform this computation using importance sampling,
                                                                                The exemplar model provides several advantages over the
listeners need only store exemplars of previously encountered
                                                                             original Bayesian formulation. It allows listeners to compute
speech sounds, giving them a sample from p(T ), the prior
                                                                             speakers’ target productions without explicit knowledge of
on target productions (Equation 13)1 . Upon hearing a new
                                                                             phonetic categories, thereby giving a more plausible account
speech sound, they weight each stored exemplar by its likeli-
                                                                             of how six-month-olds might acquire enough information to
hood p(S|T ) (Equation 14) and take the weighted average of
                                                                             show the perceptual magnet effect (Kuhl et al., 1992). Listen-
these exemplars to approximate the posterior mean
                                                                             ers can still compute category membership based on labeled
                                  ∑mj=1 T j p(S|T j )                        exemplars using Equation 2, but labeled exemplars are not
                   E[T |S] ≈                                   (15)          required in order to show perceptual warping. Furthermore,
                                   ∑mj=1 p(S|T j )
                                                                             parametric knowledge of category structure is not required for
where T j denotes the phonetic detail (e.g. formant value) of a              either computation: Equation 15 generalizes easily to the case
stored target production.                                                    of non-Gaussian categories, allowing listeners to perform op-
   Figure 1 compares the performance of this exemplar model                  timally for a range of category structures. Finally, similar
to multidimensional scaling data from Iverson and Kuhl                       exemplar-based mechanisms have previously been proposed
(1995), who used an AX discrimination task to generate a per-                by Guenther and Gjaja (1996) and Pierrehumbert (2001) to
ceptual map of thirteen equally spaced stimuli in the /i/ and                create a bias toward category centers, and importance sam-
/e/ categories. Model parameters are the same as those used                  pling provides a way of integrating the Bayesian model with
by Feldman and Griffiths (2007). The figure shows the non-                   these exemplar-based approaches.
linear mapping between psychoacoustic and perceptual space
that is characteristic of the perceptual magnet effect: stim-                The number game
uli near the /i/ and /e/ category means are clustered together               While the perceptual magnet effect is an example where the
   1 Exemplars   in a continuous space that are acquired by sensory          exemplar model is applied in a space of continuous variables
experience may be corrupted by noise and thus are not perfect sam-           (frequency in acoustic space), exemplars can also be hypothe-
ples from the prior. However, often exemplars still closely follow           ses over a discrete space. The “number game” of Tenenbaum
the prior distribution since such noise can be significantly reduced
by averaging over repetitive identical observations and/or weighting         (1999; Tenenbaum & Griffiths, 2001) is a good example. This
over cues from multiple sensory modalities.                                  game is formulated as follows: given natural numbers from
                                                                       748

1 to 100, if number x belongs to an unknown set C (e.g.,                meaning that p(y ∈ C|x) is just the ratio of the summed like-
{59, 60, 61, 62}), what is the probability that y also belongs          lihoods of the hypotheses stored in memory that generate y
to the same set?Here, the exemplars of interest are not num-            to the summed likelihoods of all hypotheses stored in mem-
bers themselves, but sets of numbers following rules, such as           ory. Considering limitations in memory capacity and com-
squares ({1, 4, 9, 16, ...}) or natural numbers between 89 and          putational power, we conducted two sets of simulations. In
91 ({89, 90, 91}).                                                      the computation-limited case, the bottleneck is the number of
   This problem can be addressed by Bayesian inference. Our             exemplars that can be processed simultaneously, but not the
data are the knowledge that x belongs to the set C, and our             supply of qualified hypotheses, being those hypotheses such
hypotheses concern the nature of C. Since C is unknown, we              that x ∈ h. In contrast, the memory-limited case assumes that
should sum over all possible hypotheses h ∈ H when evalu-               only a limited number of hypotheses are stored in memory
ating whether y belongs to C,                                           and those exemplars are not necessarily qualified. When the
                                                                        right hypothesis is missing (say cubes for {1, 8, 27, 64}), the
   p(y ∈ C|x) =      p(y ∈ C|h)p(h|x) =         1(y ∈ h)p(h|x)
                                                                        exemplar model gives incorrect results, as when a person fails
                 ∑                          ∑                  (16)
                                                                        to recognize the underlying rule. Our simulations use the
                h∈H                        h∈H
                                                                        same parameters as those in Tenenbaum (1999) except that
  where 1(y ∈ h) is the indicator function of the statement             the likelihood function assigns a small non-zero probability
y ∈ h, taking value 1 if this is true and 0 otherwise. In the anal-     to all natural numbers from 1 to 100 for every hypothesis to
ysis presented by Tenenbaum (1999; Tenenbaum & Griffiths,               ensure numerical stability.
2001), the likelihood p(x|h) is proportional to the inverse of             Figure 2 (b) and (c) show a single hypothetical subject’s
the size of h (the “size principle”) being 1/|h| if x ∈ h and 0         responses to the number game. The results suggest a small
otherwise. A broad range of hypotheses were used, including             number of exemplars (20 and 50) is sufficient to account for
intervals of numbers spanning a certain range, even numbers,            human performance. The memory limited case needs more
odd numbers, primes, and cubes.                                         exemplars because not all exemplars are qualified hypoth-
   The number game is challenging because any given num-                esis. Therefore, the effective number of exemplars, which
ber (say x = 8) is consistent with many hypotheses (not only            determines the computational load, is small. The consis-
intervals containing 8, but also hypotheses such as even num-           tency of these results with the human judgments indicates that
bers, cube numbers, number with ending digit 8, etc.). In-              this kind of generalized exemplar model provides a plausible
terestingly, the responses of human participants can be cap-            mechanism for performing Bayesian inference that relies on
tured quite accurately with this Bayesian model (Figure 2               reasonable memory and computational resources and can be
(a)). However, this involves instantiating all 6,412 hypothe-           used with highly structured hypothesis spaces.
ses, calculating the likelihood for each rule and integrating
over the product of the prior and likelihood. Human subjects                                      Conclusion
are not likely to perform such computations given limitations
on memory capacity and computational power, so a mecha-                 Our theoretical results indicate that exemplar models can be
nism that approximates the exact solution is desirable.                 interpreted as a form of importance sampling, and thus pro-
   Performing the computations involved in the number game              vide a simple psychological mechanism for producing be-
requires extending our analysis of exemplar models to the               havior consistent with Bayesian inference. Our simulations
general case of Bayesian inference. We can do this by re-               demonstrate that this approach produces predictions that cor-
placing the role of exemplars in the preceding analysis with            respond reasonably well with human behavior and that rela-
hypotheses sampled from the prior p(h). These hypotheses                tively few exemplars are needed to provide a good approx-
are activated in response to how well they explain the data,            imation to the true Bayesian solution to a simple problem.
with activation proportional to p(x|h). Averaging any func-             These simulations also highlight the flexibility of this ap-
tion of h over the distribution defined by normalizing the ac-          proach, since exactly the same model can be used to make
tivations will be an importance sampler for the expectation of          predictions regardless of the form of the prior.
that function over the posterior, p(h|c). Thus, storing a few              The approach that we have taken in this paper represents
hypotheses in memory and activating those hypotheses in re-             one way of addressing questions about the mechanisms that
sponse to data provides a psychologically plausible mecha-              could support probabilistic inference. Our results suggest
nism for performing Bayesian inference.                                 that exemplar models are not simply process models, but a
   We can now apply this framework to the number game.                  kind of “rational process model” – an effective and psycho-
Equation 16 is an expectation of an indicator function over             logically plausible scheme for approximating statistical infer-
the posterior distribution p(h|x). This expectation can be ap-          ence. This approach pushes the principle of optimality that
proximated using a set of m hypotheses h1 , . . . , hm sampled          underlies probabilistic models down to the level of mech-
from the prior and activated in proportion to the likelihood,           anism, and suggests a general strategy for explaining how
                                                                        people perform Bayesian inference: look for connections be-
                               ∑ j 1(y ∈ h j )p(x|h j )                 tween psychological process models and approximate infer-
               p(y ∈ C|x) ≈                                    (17)
                                     ∑ j p(x|h j )                      ence algorithms developed in computer science and statistics.
                                                                    749

           (a) Full Bayesian model                               (b) Computation-limited (20 exemplars)           (c) Memory-limited (50 exemplars)
           1                              X = 60
           0
           1                              X= 60 52 57 55
p(yєC|x)
           0
                                          X = 60 80 10 30
           1
           0                                                 0
                                          X = 81 25 4 36
           1
           0
               1             Number (y)                100   1                     Number (y)              100    1                Number (y)             100
Figure 2: Simulations (dashed line) and behavioral data from Tenenbaum (1999) (gray bars) for the number game. The full
Bayesian model uses 6412 hypotheses. Results of computation-limited and memory-limited exemplar models are based on
a single hypothetical subject with a single set of hypotheses (exemplars) sampled from the prior. Models are tested un-
der conditions suggesting single point generalization x = 60, a consecutive interval x = {60, 52, 57, 55}, multiples of 10
x = {60, 80, 10, 30} and squares x = {81, 25, 4, 36}.
Acknowledgments. This research was supported by grant FA9550-                              blom, B. (1992). Linguistic experience alters phonetic perception
07-1-0351 from the Air Force Office of Scientific Research                                 in infants by 6 months of age. Science, 255(5044), 606-608.
                                                                                         Luce, R. D. (1959). Individual choice behavior. New York: Wiley.
(LS,TLG) and grant HD32005 from the National Institute of Health                         Ma, W. J., Beck, J., Latham, P., & Pouget, A.(2006). Bayesian infer-
(NHF).                                                                                     ence with probabilistic population codes. Nature Neuroscience, 9,
                                                                                           1432-1438.
                                                                                         Marr, D. (1982). Vision. San Francisco, CA: W. H. Freeman.
                                     References                                          Medin, D. L., & Schaffer, M. M. (1978). Context theory of classifi-
                                                                                           cation learning. Psychological Review, 85, 207-238.
Anderson, J. R., & Milson, R.(1989). Human memory: An adaptive                           Neal, R. M. (1993). Probabilistic inference using Markov chain
  perspective. Psychological Review, 96, 703-719.                                          Monte Carlo methods (Tech. Rep. No. CRG-TR-93-1). University
Ashby, F. G., & Alfonso-Reese, L. A. (1995). Categorization as                             of Toronto: University of Toronto.
  probability density estimation. Journal of Mathematical Psychol-                       Nosofsky, R. M.(1986). Attention, similarity, and the identification-
  ogy, 39, 216-233.                                                                        categorization relationship. Journal of Experimental Psychology:
DeLosh, E. L., Busemeyer, J. R., & McDaniel, M. A. (1997). Ex-                             General, 115, 39-57.
  trapolation: The sine qua non of abstraction in function learn-                        Oaksford, M., & Chater, N. (1994). A rational analysis of the se-
  ing. Journal of Experimental Psychology: Learning, Memory,                               lection task as optimal data selection. Psychological Review, 101,
  and Cognition, 23, 968-986.                                                              608-631.
Feldman, N. H., & Griffiths, T. L. (2007). A rational account of the                     Pierrehumbert, J. B. (2001). Exemplar dynamics: Word frequency,
  perceptual magnet effect. In D. S. McNamara & J. G. Trafton                              lenition and contrast. In J. Bybee & P. Hopper (Eds.), Frequency
  (Eds.), Proceedings of the 29th Annual Conference of the Cogni-                          and the emergence of linguistic structure. Amsterdam: John Ben-
  tive Science Society (p. 257-262). Austin, TX: Cognitive Science                         jamins.
  Society.                                                                               Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2006). A more
Guenther, F. H., & Gjaja, M. N.(1996). The perceptual magnet effect                        rational model of categorization. In Proceedings of the 28th An-
  as an emergent property of neural map formation. Journal of the                          nual Conference of the Cognitive Science Society. Mahwah, NJ:
  Acoustical Society of America, 100(2), 1111-1121.                                        Erlbaum.
Iverson, P., & Kuhl, P. K. (1995). Mapping the perceptual mag-                           Smith, E. R., & Zarate, M. A. (1992). Exemplar-based model of
  net effect for speech using signal detection theory and multidi-                         social judgment. Psychological Review, 99, 3-21.
  mensional scaling. Journal of the Acoustical Society of America,                       Tenenbaum, J. B. (1999). A Bayesian framework for concept learn-
  97(1), 553-562.                                                                          ing. Unpublished doctoral dissertation, Massachusetts Institute of
Juslin, P., & Persson, M. (2002). PROBabilities from EXemplars                             Technology, Cambridge, MA.
  (PROBEX): a lazy algorithm for probabilistic inference from                            Tenenbaum, J. B., & Griffiths, T. L. (2001). Generalization, similar-
  generic knowledge. Cognitive Science, 26, 563-607.                                       ity, and Bayesian inference. Behavioral and Brain Sciences, 24,
Körding, K., & Wolpert, D. M. (2004). Bayesian integration in sen-                        629-641.
  sorimotor learning. Nature, 427, 244-247.                                              Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference:
Kruschke, J. K.(2006). Locally Bayesian learning with applications                         analysis by synthesis? Trends in Cognitive Sciences, 10, 301-
  to retrospective revaluation and highlighting. Psychological Re-                         308.
  view, 113, 677-699.
Kuhl, P. K., Williams, K. A., Lacerda, F., Stevens, K. N., & Lind-
                                                                                   750

