UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Grounding Word Learning in Multimodal Sensorimotor Interaction
Permalink
https://escholarship.org/uc/item/4j4897rt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Yu, Chen
Smith, Linda B.
Pereira, Alfredo F.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Grounding Word Learning in Multimodal Sensorimotor Interaction
                         Chen Yu, Linda B. Smith and Alfredo F. Pereira (chenyu@indiana.edu)
               Department of Psychological and Brain Sciences, and Cognitive Science Program, Indiana University
                                                        Bloomington, IN, 47405 USA
                               Abstract                                  infants are sensitive to social cues, such as monitoring and
      A central problem in the study of language acquisition is
                                                                         following another’s gaze, although infants’ understanding of
   word learning – how the child’s mental representation of              the implications of gaze or pointing does not emerge until
   objects and events on the one hand is associated with the             approximately 12 months of age. Based on this evidence,
   linguistic input on the other, and how young children acquire         some researchers (e.g. Bloom, 2000, Tomasello, 2000;
   the vocabulary of their first language so effortlessly and            Woodward, 2004) have suggested that children’s word
   smoothly, with virtually no errors along the way. This paper          learning in the second year of life actually draws extensively
   aims at understanding the mechanisms through which word               on their understanding of the thoughts of speakers.
   learning is grounded in sensorimotor experience, in the                  However, there is an alternative explanation to that of
   physical regularities of the world, and in the time-locked and        “mind-reading”. Smith (2000) has suggested that these
   coupled multimodal interactions between the child’s own
   actions and the actions of their caregivers. We designed and
                                                                         results may be understood in terms of the child’s learning of
   implemented a novel multimodal sensing environment                    correlations among actions, gestures and words of the
   consisting of two head-mounted mini cameras that are placed           mature speaker as predictors of attention and intended
   on both the child’s and the parent’s foreheads, motion                referents. Smith (2000) argued that construing the problem
   tracking of head movements and recording of caregiver’s               in this way does not “explain away” notions of “mind-
   speech. Using this new technology, we captured the dynamic            reading” but rather grounds those notions in the perceptual
   visual information from both the learner’s perspective and the        cues available in the real-time task that infants must solve
   parent’s viewpoint while they were engaged in a naturalistic          (see also Smith & Breazeal, 2007).
   toy-naming interaction, to study the regularities and dynamic           One problem with resolving (or integrating) these two
   structure in the multimodal learning environment. To achieve
   this goal, we also implemented various data processing
                                                                         ideas about the information available in social interactions is
   programs that can automatically extract visual, motion and            that most experiments are designed as demonstrations of
   speech information from raw sensory data. Our results show            children’s sensitivity to social cues and as such, they focus
   that a wide range of perceptual and motor patterns, such as           on macro-level behaviors (such as pointing, or direction of
   the proportion of the named objects in both the child’s and the       eye gaze) in constrained contexts in which the experimenter
   caregiver’s visual fields, the proportion of time that the            presents one or two objects on an uncluttered table and
   child’s hands are holding the named objects when those                where language is also uncluttered by remarks about
   names are uttered, and as well as the child’s head movements,         anything other than those objects on the table (e.g. Baldwin,
   are predictive of successful word learning through social             1993). To truly understand mechanisms of learning,
   interaction. In light of this, we suggest that high-level social-
   cognitive cues in word learning can be grounded in embodied
                                                                         however, we may need to focus on more micro-level
   perceptual and motor patterns that are part of a natural social       behaviors as they unfold in real time in the richly varying
   interaction.                                                          and dynamically complex interactions of children and their
                                                                         mature partners in more naturalistic tasks (such as toy play).
   Keywords: language development, embodied cognition,                   Further, whereas the studies at the macro-level clearly
   learning, computational modeling
                                                                         demonstrate many intelligent behaviors in infant word
                                                                         learning, they have not as yet led to a formal account of the
                           Introduction                                  underlying mechanisms. Thus, we want to know not only
A major recent advance in understanding word learning has                that learners use social cues but also how they do so in terms
been the documentation of the powerful role of social-                   of the real-time processes in the naturalistic tasks where
interactional cues in guiding infants’ attention and in linking          everyday language learning must take place.
the linguistic stream to objects and events in the world                     To this end, we sought to study the dynamics of social
(Baldwin, 1993; Tomasello & Akhtar, 1995; Yu, Ballard &                  cues to word learning at the sensorimotor levels – in the
Aslin, 2005). There can be no doubt that young learners are              bodily gestures and as well as momentary visual and
highly sensitive to the social information in these                      auditory perception of the participants. The study presents a
interactions (e.g., Baldwin, 1993; Bloom, 2000; Woodward,                new design and implementation of a sensing system for
2004). However, the nature of this sensitivity and the                   recording multisensory data from both the child and the
relevant underlying processes are far from clear. Often in               caregiver. With this new methodology, we compare and
this literature, children’s use of social cues is interpreted in         analyze the dynamic structure of natural parent-child
terms of (and seen as diagnostic markers of) their ability to            interaction in the context of language learning, and further
infer the intentions of the speaker. This kind of social                 discover perceptual and motor patterns that are
cognition is called “mind reading” by Baron-Cohen (1995).                informatively time-locked to words and their intended
Butterworth (1991) showed that even by 6 months of age,                  referents and predictive of word learning.
                                                                     1017

          Multimodal Sensing Environment                             was tight enough that the camera did not move (unless the
                                                                     child pulled at the band during the experiment – an event
As shown in Figure 1, the naturalistic interaction of parents
                                                                     that caused the data from that child beyond that point to be
and toddlers in the task of table-top toy play is recorded by
                                                                     excluded unless centering could be re-achieved). The visual
three cameras from different perspectives: one head-
                                                                     angle recorded by the camera was 70o. The cabling was long
mounted camera provides information about the scene from
                                                                     and lightweight enough not to push down on the
the child’s point of view; a second head-mounted camera
                                                                     participant’s head or get tangled during movement. A digital
provides the parent’s viewpoint; and one from a top-down
                                                                     video recorder card in a computer adjacent to the
third-person viewpoint allows a clear observation of exactly
                                                                     experiment room simultaneously recorded the video signal
what was on the table at any given moment (mostly the
                                                                     from these two cameras.
participants’ hands and the objects being played with). In
addition, our multimodal system also records participants’
body movements through a motion tracking system and as               Bird-eye view camera. A high-resolution camera was
well as parents’ speech through a headset.                           mounted right above the table and the table edges aligned
Interaction room setup. Parents and children sat across              with edges of the bird-eye image. This view provided visual
each other on a small table (61cm × 91cm × 64cm) that was            information that was independent of gaze and head
painted white. The child sat on a high chair and the parent          movements of a participant and therefore it recorded the
sat on the floor – which places their head-cameras about             whole interaction from a third-person static view. An
equal distance from the tabletop. Both participants were             additional benefit of this camera lay in the high-quality
asked to wear white outfits. White curtains from floor to            video, which made our following image segmentation and
ceiling surrounded the table. The experimental room was              object tracking software work more robustly compared with
setup in such a way that everything was white as seen from           two head-mounted mini cameras. Those two were light-
the vantage point of the participants – with the exception of        weight but with a limited resolution and video quality due to
heads, faces, hands and objects on the table. This greatly           their small size.
simplified automatic visual object segmentation since any            Head motion tracking. To measure the activity of each
white areas of an image could be considered as background.           partner’s head we used an electromagnetic motion tracking
                                                                     solution, the Liberty system from Polhemus (Polhemus,
Head-mounted cameras. The head-mounted cameras are a                 Colchester, Vermont, USA). This tracker uses passive
lightweight mini camera attached to a sports headband. This          electromagnetic sensors and a source that emits a
allowed us to place the camera on the forehead close to the          electromagnetic field. The source was placed above the
participants’ eyes. A small plastic encasing supported               table. The sensors consist of electromagnetic coils in a
rotation of the camera in order to adjust the camera such that       plastic casing, assembled as small cubes measuring 22.9
during calibration an object to which the participant was            mm x 27.9 mm x 15.2 mm and weighing 23g. A wire
attending was near the center of the field. The headband             connects each sensor to the base and multiple sensors can be
                                                                     acquired simultaneously with high sampling rates and
                                                                     precision. When tracking, the system provides 6DOF data --
                                                                     3D coordinates (x, y, z) and 3D orientation (heading, pitch
                                                                     and roll) relative to the source position.
                                                                     Parent’s speech. To record the parent’s voice we used a
                                                                     standard headset with a noise reduction microphone. The
                                                                     parent wore the headset while interacting with her child.
                                                                           Word Learning through Social Interaction
                                                                     The task is a common one in the everyday lives of children
                                                                     and parents – to take turns in jointly acting on, attending to,
                                                                     and naming objects. This is a common context in which
                                                                     children learn names for things. The toys used in this
                                                                     experiment were novel things. The child and parent sat
                                                                     opposite each other at a small table and the parent was
                                                                     instructed to interact naturally with the child, engaging their
                                                                     attention with the toys while teaching the words for them.
                                                                     Participants. The target age period for this study was 18 to
                                                                     20 months. We invited parents in the Bloomington, Indiana
 Figure 1: Multimodal sensing system. The child and the mother       area to participate in the experiment. 5 dyads of parent and
 play with a set of toys at a table. Two mini cameras were placed    child were part of the study (2 male and 3 female). 3
 onto the child’s and the mother’s heads respectively to collect     additional children were not included because of fussiness
 visual information from two first-person views. A third camera      before the experiment started or failure to keep the head
 mounted on the top of the table records the bird-eye view of the    camera on. For the child participants included, the mean age
 whole interaction. They also wore motion sensors to track their     was 18.5, ranging from 17 to 20 months. All participants
 head movements. A headset was used to record the caregiver’s        were white and middle-class.
 speech.
                                                                 1018

Stimuli. Parents were given three sets, with three toys in                Unimodal Data Processing and Results
each set, in this free-play task. The toys were rigid plastic      The multisensory data recorded include three video
objects of simple shapes and were painted with one primary         sequences from three views, head motion of two
color. Each set had a red, a green and a blue object.              participants, and parental speech. This section presents both
Procedure. The study was conducted by three                        the methods and the results of processing sensory data for
experimenters: one to distract the child, another to place the     each individual modality. The next section presents the
head-mounted cameras and a third one to control the quality        results from an integration of this unimodal data processing.
of video recording. Parents were told that the goal of the
study was simply to observe how they interacted with their         Video Processing and Results
child while playing with toys and that they should try to             The recording rate for each of the three cameras is 10
interact as naturally as possible. Upon entering the               frames per second. There were 3 trials in the interaction,
experiment room, the child was quickly seated in the high          each lasting about 90 seconds. In total, we have collected
chair and several attractive toys were placed on top of the        approximately 8100 (10 × 90 × 3 × 3) image frames from
table. One experimenter played with the child while the            each interaction. The resolution of image frame is 320 ×
second experimenter placed a sports headband with the              240. Figure 2 illustrates the procedure of image processing
mini-camera onto the forehead of the child at a moment that        and results. The technical details can be found in (Yu,
he appeared to be well distracted. Our success rate in             Smith, Christensen & Pereira, 2007). The relevant
placing sensors on children is now at over 60%. After this,        information we extracted from three image streams are
the second experimenter placed the second head-mounted             where objects are in each view at each moment, and what
camera onto the parent’s forehead and close to her eyes.           are the sizes of those objects at each moment. In addition,
Calibration of head-mounted cameras. To calibrate the              we also calculated, from the bird-eye view, which objects
horizontal camera position in the forehead and the angle of        were held by the child’s and the caregiver’s hands. As
the camera relative to the head, the experimenter asked the        illustrated in an example shown in Figure 2, a direct
parent to look into one of the objects on the table, placed        comparison between the child’s and the caregiver’s views
close to the child. The third experimenter controlling the         replicated our finding in our previous studies (Yu et al.,
recording in another room confirmed if the object was at the       2007) – in the same interaction objects occupy about 11% of
center of the image and if not small adjustments were made         the child’s head-camera field but less than 5% of the
on the head-mounted camera gear. The same procedure was            parent’s visual field. This is because parents and children
repeated for the child, with an object close to the child’s        move the child-attended objects close to the child’s head
hands.                                                             and because children also move their head close to attended
Parent-child free play session. The instructions given to the      objects.
parent were to take all three objects from one set, place
them on the table, play with the child and after hearing a
command from the experimenters, remove the objects in this
trial and move to the next set to start the next trial. Parents
were given the names of the objects that they were to use
and were instructed to teach the children those object
names. However, there was no special instruction as to what
the parents had to say or what they had to perform, just that
they were to engage their child. All the names were artificial
words. There were a total of three trials, each about 1.5
minute long. The interaction between parent and child lasted
between 4 and 7 minutes and was free-flowing in form.
Name-comprehension test. After the period of free
interaction, the experimenter tested the child’s
comprehension of the object name for each of the 9 objects.
This was done by placing three objects out of reach of the
child about 30 inches apart, one to the left of the child one
in the middle and one to the right. Then the experimenter
looked directly into the child’s eyes, said the name of one of
the objects and asked for it (e.g. “I want the dax! The
grizzly! Get me the grizzly!”). For this portion of the
experiment, a camera was focused on the child’s eyes.              Figure 2: The overview of data processing using computer vision
Direction of eye gaze – looking to the named object when           techniques. Our program can detect three objects on the table and
named – was scored as indicating comprehension. These              participants’ hands and faces automatically based on pre-trained
recorded eye movements were coded (with the sound off) by          object models and skin models. The extracted information from
a scorer naïve to the purpose of the experiment.                   three video streams will be used in subsequent data analyses.
                                                               1019

Motion data Processing and Results
Two motion tracking sensors on participants’ heads
recorded 6 DOF of their head movement at the frequency of
240 Hz. Given the raw motion data {x, y, z, h, p, and r}
from each sensor, the primary interest in the current work is
the overall dynamics of the head. We grouped the 6 DOF
data vector into position {x, y, z} and orientation data {h, p,
r}, and then we developed a motion detection program that
computes the magnitudes of both head position movements
and orientation movements. Figure 3 shows the proportion
of time that either children or parents were moving their
heads. Head position movements are equally frequent in               Figure 3: The proportion of time that the child’s and the
children and parents. However, children rotate their heads           caregiver’s heads are moving in the interaction.
much more frequently than adults do, in the same                   transcription and used them to define a naming event in
interaction. This result indicates that young children are         time. We will use those naming events to data mine the
more likely to switch their visual attention through head          patterns in visual and motion data streams.
rotation while adults may rely more on gaze shifting. This
                                                                   Results of Word Learning and Naming Events
measure supports our head-camera setup as a means of
                                                                   We correlated the number of naming events for each object
capturing the child’s more dynamic view.
                                                                   name with the learning results at testing and found these two
Speech Processing and Results                                      (r=-0.3; p<0.001) at best weakly and negatively correlated.
We first segmented the continuous speech stream into               The average of naming events for learned object names is
multiple spoken utterances based on speech silence. Next,          2.45 per name and 3.5 per name for unlearned names. Thus,
we asked human coders to listen to the recording and               object names not learned through interaction were actually
transcribe the speech segments. The statistics from the            uttered more than those learned names. For example, some
transcriptions show that on average, parents uttered 365           object names that were provided just once or twice were
words in each interaction and each spoken utterance consists       actually learned and others labelled by caregivers five or six
of 3.5 words. The average size of vocabulary for each              times were not learned. This suggests that what matters are
interaction is 120. Moreover, nine target object names were        the specific contexts where those object names were named,
produced 32 times in total in each interaction. In the whole       what both caregivers and children visually attended to at
dataset of 5 dyads, those object names occurred 161 times in       those moments, and what they were doing at that time. We
spoken language. We extracted the onset and offset                 report those behavioral measurements in the next section.
timestamps wherein an object name occurred in
Figure 4: Continuous data were segmented and grouped into three categories: successful naming events, unsuccessful
naming events and (other) non-naming events. A comparison of visual data and motion data was made based on these three
event categories.
                                                               1020

       Multimodal Data Analysis and Results                            visual fields, hand movements and head movements,
Given the complex multimodal multi-streaming data                      respectively.
collected from two participants, we opted to use the learning          Named objects in visual fields
results collected in testing from young learners as teaching           The proportion of a visual field occupied by named objects
signals to guide us in data mining this fine-grained                   may be viewed as a measure of the named objects’
multimodal data. This method is different from most                    dominance over other objects in the viewers’ attentional
modeling approaches which build a simulated model first to             field. As shown in Figure 5, our analyses indicate that the
make predictions about results and then correlate the                  named objects occupied a larger proportion of the child’s
predictions with actual experimental results. Instead we use           visual field in successful naming events compared with that
here experimental results as supervisory information to                in unsuccessful naming events. The same trend holds with
search for reliable patterns from this complex                         visual data from the parent’s perspective. Putting together,
multidimensional dataset. From a technical viewpoint, this             the results suggest – not surprisingly – that object names are
approach is also different from standard unsupervised data             learned more effectively when the named object is visually
mining approaches because we take advantage of behavioral              salient in both the learner’s view and the teacher’s view,
information to facilitate data mining and pattern detection.           namely, when the child and the caregiver jointly attend to
Figure 4 shows our overall approach for multimodal                     the same object.
information integration, which consisted of two steps. First,          Named objects in hands
we started by grouping naming events (results from speech              The percentage of time in each event category that the
and language processing) into successful naming events (n              named objects are either in the child’s hands or in the
= 65) and unsuccessful naming events (n= 96) based on the              caregiver’s hands can also be viewed as a measure of
testing results measured at the end of each parent-child               attention to that object. As shown in Figure 6, more
interaction. In addition, we grouped the remaining moments             successful naming events are those in which the named
in the interaction as a third kind of event – non-naming               object is in the child’s but not the caregiver’s hands. More
events (n = 132). In this way, a whole temporal data stream            specially, in about 45% of time when a successful naming
can be decomposed and labeled by these three events. Next,             event happened, the named object was in the child’s hands.
we extracted various measures and statistics from visual and           Meanwhile, the named objects were in the caregiver’s hands
motion data, and compared those results across three event             in only 8% of time. Two implications follow from these
groups. Any differences on a certain measurement between               results: First, those learning moments in which the parent
successful and unsuccessful naming events will indicate the            correctly gauges the child’s attention and then provides
potential importance of this pattern in learning-oriented              linguistic labels, may be most effective for word learning.
social interaction. In contrast, similar results across                Second, parents can infer the child’s attention through the
successful and nonsuccessful events suggest that the pattern           child’s hand actions.
under consideration may not play any major role in word
learning. In addition, the third event group – non-naming              Head Movement and Word Learning
event – provides a baseline. The differences between non-              As shown in Figure 7, the third measure asks whether the
naming and two naming events will identify those                       child or the caregiver holds his/her head still during naming
behavioral patterns in a social interaction that caregivers            events. Our first finding is that both the child and the
generate when they teach object names, no matter whether               caregiver move their head more dramatically in
the naming events themselves are successful or not. The                unsuccessful naming events compared with successful
following results will focus on three different measures:
                                                                       Figure 6: The proportion of time that participants’ hands are
                                                                       holding an object. In successful naming events, the child’s hands
 Figure 5: The proportion of the named objects in two views. In        most often held the named objects, which happened less frequently
 both views, the proportion of named objects is much bigger at the     in unsuccessful naming events. Indeed, the caregiver tended to hold
 moments of successful naming events compared with either              the name objects in unsuccessful naming events even compared
 unsuccessful naming events or the baseline (other moments in the      with non-naming moments.
 interaction).
                                                                   1021

naming events or the basic line. Second, the child’s head is          mental states of others must arise from their external bodily
oriented more stably during successful naming events. This            actions, bodily actions that in the real world are highly
suggests that sustained attention is critical to learning object      dynamic. The study reported here is a first step in
names.                                                                understanding these dynamics.
                                                                          In this paper, we use advanced sensing equipment and
                                                                      state-of-the-art experimental paradigms to collect multiple
                                                                      streams of real-time sensory data in parent-child
                                                                      interactions. A further strength of this research is the
                                                                      application of computational techniques to analyze these
                                                                      multisensory data to measure the statistical regularities in
                                                                      the learning environment. Thus, with more fine-grained data
                                                                      and advanced analysis tools, we have the opportunity to
                                                                      discover a more complete mechanistic explanation of early
                                                                      word learning.
                                                                      Acknowledgments: We thank Amara Stuehling, Jillian
 Figure 7: The proportion of time that the child’s or the             Stansell, Saheun Kim, and Mimi Dubner for collection of
 caregiver’s head is moving. We found that the child’s head tends     the data. This research was supported by National Science
 to have a stable orientation in successful naming events. Also       Foundation Grant BCS0544995 and by NIH grant R21
 both the child and the caregiver move more dramatically in           EY017843.
 unsuccessful naming events.                                                                    References
        General Discussions and Conclusion                            Baldwin, D. (1993). Early referential understanding:
Most of children’s word learning takes place in messy                    Infant’s ability to recognize referential acts for what they
contexts – like the tabletop play task used here. There are              are. Developmental psychology, 29, 832-843.
multiple objects, multiple shifts in attention by both                Baron-Cohen, S. (1995). Mindblindness: an essay on autism
partners, and many object names that might be learned. In                and theory of mind. Cambridge: MIT Press.
these contexts, very young children do not always learn the           Bloom, P. (2000). How children learn the meanings of
names of things but they must learn some. The goal of this               words. Cambridge, MA: The MIT Press.
work is to understand the qualities of real world interactions        Butterworth, G. (1991). The ontogeny and phylogeny of
between young word learners and parents that organize that               joint visual attention. In A. Whiten (Ed.), Natural theories
learning. The number of naming events is not the most                    of mind: Evolution, development, and simulation of
important variable. Instead, naming needs to occur at the                everyday mindreading (p. 223- 232). Oxford: Blackwell.
right moment in time, when both parent and child are                  Smith, L. (2000). How to learn words: An associative crane.
attending to the same object. However, looking at an object,             In R. Golinkoff & K. Hirsh-Pasek (Eds.), Breaking the
the metric of attention usually used in highly simplified                word learning barrier (p. 51-80). Oxford: Oxford
artificial learning tasks, may not be the best real-world                University Press.
metric on attention. Instead, active engagement – that is,            Smith, L.B. & Breazeal, C. (2007) The dynamic lift of
manual actions on the object –may be a better metric of the              developmental process. Developmental Science, 10, 61-
child’s interest and thus readiness to learn the name. Finally,          68.
a quieting of head movements, an index of sustained                   Tomasello, M., & Akhtar, N. (1995). Two-year-olds use
orientation to the object, also predicts learning. These three           pragmatic cues to differentiate reference to objects and
dimensions of attention – shared visual attention, manual                actions. Cognitive Development, 10, 201-224.
engagement, and sustained attention – fluctuate dynamically           Woodward, A. L. (2004). Infants’ use of action knowledge
in the interactions between children and parents. Key                    to get a grasp on words. In D. G. Hall & S. R. Waxman
questions for future work are whether dyads of parents and               (Eds.), Weaving a lexicon. MIT Press.
children differ in the dynamic qualities of these interactions        Yu, C., Ballard, D.H., & Aslin, R.N. (2005). The role of
with some modes of interaction being generally more                      embodied intention in early lexical acquisition. Cognitive
effective than others. Also of interest is whether these                 Science, 29 (6), 961–1005.
individual differences in dyads emerge from children’s                Yu, C. & Ballard (2004). A multimodal learning interface
attentional differences, from differences in parent sensitivity          for grounding spoken language in sensory perceptions.
to the child’s attention, or both.                                       ACM Transactions on Applied Perceptions 1(1):57–80.
   Moving away from abstract and mechanistically                      Yu, C., Smith, L. B., Christensen, M., & Pereira, A. F.,
ungrounded ideas such as “mind reading” and inferred                     (2007). Two Views of the World: Active Vision in Real-
intentions, and moving away from sparse experimental                     World Interaction. In McNamara & Trafton (Eds.),
settings unlike the dynamic interactions of real world                   Proceedings of the 29nd annual conference of the
learning, may provide a leap forward in understanding                    cognitive science society (p 731-736). Mahwah.NJ:
natural word learning in humans (and in building                         ErIBaum.
computational devices that can learn words in the same
contexts that children do). Further, inferences about the
                                                                  1022

