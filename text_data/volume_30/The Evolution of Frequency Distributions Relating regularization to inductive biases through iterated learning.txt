UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Evolution of Frequency Distributions: Relating regularization to inductive biases through
iterated learning
Permalink
https://escholarship.org/uc/item/6td4749z
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Reali, Florencia
Griffiths, Thomas L.
Publication Date
2008-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

                                     The evolution of frequency distributions:
             Relating regularization to inductive biases through iterated learning
                                              Florencia Reali (floreali@berkeley.edu)
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                                              Department of Psychology, 3210 Tolman Hall
                                                          Berkeley, CA 94720 USA
                              Abstract                                   which determiners occurred with nouns with varying prob-
   The regularization of linguistic structures by learners has
                                                                         abilities. They found that adult participants produced utter-
   played a key role in arguments for strong innate constraints          ances with probabilities proportional to their frequency in
   on language acquisition, and has important implications for           training (known as “probability matching”). However, they
   language evolution. However, relating the inductive biases of         also found that children were much more likely to regular-
   learners to regularization has proven challenging. In this paper
   we explore how regular linguistic structures can emerge from          ize, producing consistent patterns that were not the same as
   language evolution by iterated learning, in which one person’s        the training stimuli. Wonnacott and Newport (2005) extended
   linguistic output is used to generate the linguistic input pro-       these results using a similar artificial language, showing that
   vided to the next person. We use a model of iterated learn-
   ing with Bayesian agents to show that this process can result         when learners were tested on words different from those in
   in regularization when learners have the appropriate inductive        the training stimuli, adults did regularize. However, other re-
   biases. We then present two experiments demonstrating that            cent experiments seem to be at odds with this idea. For ex-
   simulating the process of language evolution in the laboratory
   can reveal biases towards regularization that might not other-        ample, Vouloumanos (2008) examined how adults track the
   wise be obvious, allowing weak biases to have strong effects.         statistics of multiple-referent relations during word learning.
   The results of these experiments suggest that people tend to          Participants were trained on novel object-word pairs. Ob-
   regularize inconsistent word-meaning mappings.
                                                                         jects were associated with multiple words, which in turn were
   Keywords: iterated learning; Bayesian models; frequency dis-
   tributions; word learning; language acquisition;                      paired with multiple objects with varying probabilities. They
                                                                         were then presented with two objects while one of the words
Languages are passed from one generation of learners to the              in was playing, and asked to select the object that went best
next via processes of cultural transmission. Such processes              with the word. The results indicated that participants tended
introduce linguistic variation, with the generalizations pro-            to select responses in proportion to their frequencies, suggest-
duced by each generation changing the prevalence of linguis-             ing that people might probability match rather than regularize
tic forms. A particular type of change occurs when forms                 in learning multiple-referent relations.
with unpredictable or inconsistent variation become more                    The results outlined in the previous paragraph paint a
regular over time. This process occurs in the creolization of            mixed picture of the inductive biases involved in learning lan-
pidgin and learning of sign languages from non-native speak-             guage from inconsistent input. In this paper, we take a novel
ers (e.g. Bickerton, 1981, see Hudson & Newport, 2005, for               approach to this problem. First, we outline a Bayesian model
a review), and is often taken as evidence for innate language-           that can be used to make different kinds of inductive biases
specific constraints on language acquisition (e.g., Bickerton,           for frequency distributions explicit. We then use this model to
1981; DeGraff, 1999). This line of argument points toward                characterize the consequences of a process of language evolu-
the need to understand how the inductive biases of individual            tion by iterated learning (Kirby, 2001), in which one learner’s
learners contribute to the regularization of inconsistent lan-           linguistic competence is acquired from observations of an-
guage forms. Identifying this relationship can provide insight           other learner’s productions. This gives us a way to identify
into the constraints on the form of languages, and how words             the conditions on the inductive biases of individual learners
and grammars evolve over time. In this paper we begin to ex-             under which iterated learning results in regularization. We
plore this question for the frequencies of linguistic variants.          then simulate language evolution in the laboratory, using a
   Learning a language with any kind of probabilistic varia-             variant on the task studied by Vouloumanos (2008) to show
tion requires learning a probability distribution from observed          that while studying the responses of a single generation of
frequencies. Over the last couple of decades, a number of                participants does not reveal a bias towards regularization, this
studies have accumulated showing that learners are able to ex-           bias becomes extremely clear after a few generations of iter-
tract a variety of statistics from a wide range of linguistic in-        ated learning. The results have implications for understand-
put (see Gomez & Gerken, 2000; Saffran, 2003, for reviews).              ing both language evolution and language learning, revealing
Recent work has explored how the frequencies of linguistic               how weak biases can have a large effect on the languages spo-
forms are learned. In this context, regularization corresponds           ken by a community, and how simulating language evolution
to collapsing inconsistent variation towards a more determin-            in the laboratory can help to make these biases apparent.
istic rule. The empirical evidence as to whether this occurs
with human language learners has been mixed. Hudson and
Newport (2005) trained participants on artificial languages in
                                                                     229

    A Bayesian model of frequency estimation                          We will assume that the frequency of w1 and w2 have a prior
Our goal in studying the estimation of linguistic frequency           probability distribution given by a Beta distribution with pa-
distributions is to understand how the inductive biases of            rameters α2 . This flexible prior corresponds to the distribution
learners influence their behavior. To satisfy this goal, we need                                          Γ( α )     α −1
                                                                                             α α                                 α
a formalism for describing learning that makes these induc-                 p(θ1 ) = Beta( , ) = α 2 α θ12 (θ2 ) 2 −1                     (3)
tive biases explicit. In this section, we outline how the fre-                               2 2       Γ( 2 )Γ( 2 )
quency estimation problem can be solved using methods from            where Γ(·) is the generalized factorial function (Boas, 1983).
Bayesian statistics. This allows us to identify how a rational            The Beta distribution can take on different shapes depend-
learner with particular expectations about the nature of the          ing on the values of α. As shown in Figure 1, when α/2 = 1
frequency distributions in a language should behave, provid-          the density function is simply a uniform distribution. When
ing a basis for exploring the effects of these inductive biases       α/2 < 1, the density function is U-shaped and when α/2 > 1,
on the evolution of frequency distributions and a method for          it is a bell-shaped unimodal distribution. Thus, despite the ap-
inferring such biases from human behavior. Our focus will be          parent complexity of the formula, the Beta distribution cap-
on learning the relative frequencies of word-object associa-          tures prior biases that are intuitive from a psychological per-
tions. However, the models we develop apply to all problems           spective. For example, when α/2 < 1 the prior bias is such
that require learning probability distributions.                      that the learner tends to assign high probability to one of two
   Assume that a learner is exposed to N occurrences of a             competing variants, consistent with regularization strategies.
referent (e.g., an object), which is paired with multiple com-        When α/2 > 1, the learner tends to weight both competing
peting linguistic variants with certain probability. We will          variants equally, disfavoring regularization.
use the example of estimating the relative frequency of two               Substituting the likelihood from Equation 2 and the prior
competing words, but our analysis generalizes naturally to            from Equation 3 into Equation 1 gives the posterior distribu-
larger numbers of variants, and to variants of different kinds.       tion p(θ1 |x1 , x2 ). In this case, the posterior is also a Beta dis-
We will use x1 to denote the frequency of word1 (w1 ) and             tribution, with parameters x1 + α2 and x2 + α2 , due to the fact
x2 = N − x1 to denote the frequency of word2 (w2 ), and θ1            that the Bernoulli likelihood and Beta prior form a conjugate
and θ2 to denote the corresponding estimates of the probabil-                                                       x1 + α2
                                                                      pair. The mean of this distribution is N+α            , so estimates of
ities of these words. The learner is faced with the problem of
                                                                      θ1 produced by a Bayesian learner will tend to be close to the
inferring θ1 and θ2 from x1 and x2 .
                                                                      empirical probability of w1 in the data, xN1 , for a wide range of
   This estimation problem can be solved by applying
                                                                      values of α provided N is relatively large. Thus, even learners
Bayesian inference. The hypotheses being considered by the
                                                                      who have quite different inductive biases can be expected to
learner are all possible values of θ1 (since θ2 follows directly
                                                                      produce similar estimates of θ1 , making it difficult to draw
from this). The inductive biases of the learner are expressed
                                                                      inferences about their inductive biases from these estimates.
in a prior probability distribution p(θ1 ) over this set of hy-
potheses, indicating which hypotheses are considered more                    Language evolution by iterated learning
probable before seeing any data. The degrees of belief that
                                                                      Having considered how a single Bayesian learner should
the learner should assign to these hypotheses after seeing x1
                                                                      solve the frequency estimation problem, we can now explore
are the posterior probabilities p(θ1 |x1 ) given by Bayes’ rule
                                                                      what happens when a sequence of Bayesian learners each
                              P(x1 |θ1 )p(θ1 )                        learn from data generated by the previous learner. In learn-
             p(θ1 |x1 ) = R                                   (1)     ing object-word relations, this corresponds to observing a set
                            P(x1 |θ1 )p(θ1 ) dθ1
                                                                      of objects being named, making an inference about the rel-
where P(x1 |θ1 ) is the likelihood, giving the probability of ob-     ative probabilities of the names, and then producing names
serving each value of x1 for each value of θ1 .                       for a set of objects which are observed by the next learner.
   For the case of two competing words, the likelihood                More formally, we assume that each learner is provided with
P(x1 |θ1 ) is defined by the Bernoulli distribution, with the         a value of x1 produced by the previous learner, forms an esti-
probability of a particular sequence of N object-word pair-           mate of θ1 based on this value, and then generates a value of
ings containing x1 instances of w1 is                                 x1 by sampling from P(x1 |θ1 ), with the result being provided
                                                                      to the next learner. The key question is how the biases of the
                P(x1 |θ1 ) = θx11 (1 − θ1 )N−x1               (2)     learners influence the outcome of language evolution via this
                                                                      process of iterated learning.
where we assume that N is known to the learner. This likeli-              Griffiths and Kalish (2007) analyzed the consequences of
hood is equivalent to the probability of a particular sequence        iterated learning when learners are Bayesian agents. The first
of coin flips containing x1 heads being generated by a coin           step in this analysis is recognizing that iterated learning de-
which produces heads with probability θ1 .                            fines a Markov chain, with the hypothesis selected by each
   Specifying the prior distribution p(θ1 ) specifies the induc-      learner depending only on the hypothesis selected by the pre-
tive biases of the learners, as it determines the conclusions         vious learner. This means that it is possible to analyze the
that a learner will draw when given a particular value for x1 .       dynamics of this process by computing a transition matrix,
                                                                  230

           a) Priors                                                                                                 equivalence makes it possible to identify an approximate sta-
                       " /2 =0.1                          " /2=1                            " /2=5
                                                                                                                     tionary distribution on θ1 , which is a Beta distribution with
                                                                                                                     parameters 1+α α , where N is the total frequency (ie. 10 in
 P(!)                                      P(!)                              P(!)                                                    N
                                                                                                                     the present example). A proof of this equivalence (which as-
                0         0.5          1          0         0.5          1          0         0.5          1         sumes that MAP estimation is performed in the natural pa-
                           !                                !                                 !
                                                                                                                     rameter space of the Bernoulli distribution) is provided in
       b) Sampling
                                                                                                                     Reali and Griffiths (2008). Unlike the case of sampling, fre-
  Generations
                                                                                                                     quencies do not converge to the prior distribution. However,
                                                                                                                     the shape of the stationary distribution depends on the value
                                                                                                                     of priors’ parameter α. For example, it can be shown that for
                                                                                                                                        N
                0          5          10          0         5           10          0         5           10         all values of α < N−1 , the stationary distribution is U-shaped.
         c) MAP estimation                                                                                              The transition matrices associated with these two forms of
                                                                                                                     estimation can also be computed. We will focus on the tran-
  Generations
                                                                                                                     sition matrices for the values of x1 , as these values are easily
                                                                                                                     observed in behavioral data. For the case of sampling, the
                                                                                                                     probability that learner t generates a particular value of x1
                0          5          10          0         5           10          0         5           10
                    Frequency of w1                   Frequency of w1                   Frequency of w1              given the value generated by learner t − 1 is given by
                                                                                                                                                    Z
                                                                                                                                 (t)   (t−1)               (t)           (t−1)
Figure 1: The effects of inductive biases on the evolution of                                                                P(x1 |x1          )=       P(x1 |θ1 )p(θ1 |x1       ) dθ1   (4)
frequencies. (a) Prior distributions on θ1 for α2 = 0.1 (left),
α                α                                                                                                              (t)
 2 = 1 (center), 2 = 5 (right). Iterated learning by (b) sam-                                                        where P(x1 |θ1 ) is the likelihood from Equation 2 and
pling or (c) MAP estimation changes the probability distri-                                                                 (t−1)
                                                                                                                     p(θ1 |x1 ) is computed by applying Bayes’ rule as in Equa-
bution on the frequency of w1 (horizontal axis) over several
                                                                                                                     tion 1. For the MAP case, the value of θ1 produced as an esti-
generations (vertical axis), but depends strongly on this prior.                                                                                           (t−1)      (t) (t−1)
The frequency of w1 was initialized at 5 from a total fre-                                                           mate is deterministically related to x1 , so P(x1 |x1 ) is
                                                                                                                                                       1 2  x +α
quency of 10. White cells have zero probability, darker grey                                                         given by Equation 2 with θ̂1 = N+α    substituted for θ1 . These
indicates higher probability.                                                                                        transition matrices can be used to compute the probability dis-
                                                                                                                                   (t) (0)
                                                                                                                     tribution P(x1 |x1 ) as a function of the initial frequency of
                                                                                                                           (0)
                                                                                                                     w1 , x1 , and the number of generations of iterated learning, t.
indicating the probability of moving from one value of θ1                                                            The predictions of the sampling and MAP models are shown
to another or one value of x1 to another across generations,                                                         in Figure 1. Consistent with the analysis given above, the fig-
and the asymptotic consequences by identifying the station-                                                          ure shows that when the prior distribution is bell-shaped, fre-
ary distribution to which the Markov chain converges as the                                                          quencies of linguistic variants converge over time to a distri-
number of generations increases.                                                                                     bution where the probability mass is concentrated around the
   Further analysis of this Markov chain requires stating how                                                        mean. When the prior is U-shaped, the frequencies converge
the posterior distribution is actually translated into an esti-                                                      to a distribution where the probability mass is concentrated
mate of θ1 . Griffiths and Kalish (2007) identifed two such                                                          in the extremes of the distribution. Under these conditions,
estimation procedures: sampling a hypothesis from the poste-                                                         the most likely situation is that one variant becomes the vast
rior distribution, and choosing the hypothesis with the highest                                                      majority in the population, while the other one becomes very
posterior probability. They demonstrated that when learners                                                          infrequent, regardless of initial conditions. This situation can
sample from the posterior, the stationary distribution of the                                                        be interpreted as a regularization process.
Markov chain on hypotheses is the prior distribution. That                                                              The analyses presented in the last two sections support two
is, as the number of generations increases, the probability of                                                       conclusions. First, since the estimates of θ1 produced by an
selecting a particular hypothesis converges to the prior proba-                                                      individual learner will be only weakly affected by their prior,
bility of that hypothesis. In the case of frequency estimation,                                                      it can be hard to identify inductive biases by studying indi-
this means that we should expect that iterated learning with                                                         vidual learners. Second, iterated learning can magnify these
learners whose priors favor regularization (ie. with α2 < 1)                                                         weak biases, resulting in rapid convergence to a regular lan-
will ultimately produce strongly regularized languages.                                                              guage when learners have priors supporting regularization.
   It is typically more difficult to analyze the case where                                                          These conclusions motivate the two experiments presented
learners choose the hypothesis with highest posterior prob-                                                          in the remainder of the paper. Experiment 1 demonstrates
ability, known as the maximum a posteriori (MAP) hypothe-                                                            the difficulty of inferring the biases of learners by studying
sis. However, in the case of frequency estimation the Markov                                                         a single generation. Experiment 2 uses an iterated version
chain defined by iterated learning is equivalent to a model                                                          of the same task to reveal that human learners favor regular
that has been used in population genetics, the Wright-Fisher                                                         languages, and to explore the consequences of this bias for
model of genetic drift with mutation (Ewens, 2004). This                                                             language evolution by iterated learning.
                                                                                                               231

         Experiment 1: A single generation                                                  6
The design of Experiment 1 was inspired by Vouloumanos
                                                                                            5
(2008, Experiment 1). The experiment had a training phase
where participants were exposed to novel word-object asso-
ciations and a test phase assessing their knowledge of these                                4
                                                                          Frequency of w1
associations. However, the design differs from Vouloumanos
(2008) in that each word was associated with just one object,                               3
and the test trials consisted of a forced choice between words
instead of objects.                                                                         2
Method
                                                                                            1
Participants Thirty undergraduates participated in the
study in exchange for course credit.
                                                                                            0
                                                                                                0   1   2       3        4       5
Materials The materials used in Experiment 1 were the                                                   Condition
same used in Vouloumanos (2008). The auditory stimuli con-
sisted of twelve words recorded by a native English female                Figure 2: Results of Experiment 1, showing the mean fre-
speaker. All words consisted of consonant-vowel-consonant                 quency of w1 selected by participants. Black dots correspond
syllables with consonants p, t, s, n, k, d, g, b, m, l and vowels         to w1 frequency in the training stimuli and error bars indicate
æ, i, a, e, ∧ and u. Place of articulation was controlled both            one standard error.
between and within words. Word pairs assigned to a common
referent (object) were controlled so that they differed in the
place of articulation, the vowels and letters they contained.             two words associated with it where visually presented below
The visual stimuli consisted of six out of the twelve three di-           the object image (bottom left and bottom right). Participants
mensional objects used in Vouloumanos (2008). The objects                 were instructed to select one of the two words pressing a key.
differed in color and shape and were animated to move hor-                The position of the word in the screen (left or right) was ran-
izontally as a cohesive unit. They were presented in short                domized across trials and participants. The six objects were
videos shown on a computer screen.                                        presented 10 times each to match the number of presentations
                                                                          used in the training block. The order of training and test trials
Design and procedure The experiment consisted of a train-                 was randomized for every participant.
ing phase followed by a test phase. Participants were in-
structed that they would learn a novel language. No further               Results
information regarding the nature of the study was given in                There was a significant effect of w1 frequency in the train-
the instructions. During the training block participants were             ing stimuli on mean production of w1 (F(5, 29) = 13.32, p <
exposed to novel word-object associations. Each of the six                .0001). In response to relative frequency values of 0, 1, 2, 3,
objects were presented a total of 10 times, each time paired              4, and 5 in the input, the mean number of w1 in participants’
with one of two words (w1 and w2 ) with varying probabili-                productions were 0.3, 0.9, 1.6, 3.6, 4.7, and 5, respectively.
ties. The frequency with which each object occurred with w1               Figure 2 compares the mean frequencies of w1 produced by
and w2 obeyed one of six different conditions. Conditions 0,              participants to the frequencies of w1 in the training stimuli.
1, 2, 3, 4 and 5, corresponded to w1 frequencies of 0, 1, 2,                 As shown in Figure 2, the mean frequency of w1 in the
3, 4 and 5, and w2 frequencies of 10, 9, 8, 7, 6 and 5 respec-            productions was close to the corresponding frequencies in the
tively. For example, an object assigned to Condition 4 was                training phase. However, this pattern of performance does not
presented 4 times with w1 and 6 times with w2 in the train-               necessarily indicate that participants are probability matching
ing phase. A unique pair of w1 and w2 was presented with                  rather than regularizing. The results displayed in Figure 2 are
a unique object. Therefore, the overall frequency of a word               the group means and they could have resulted from averag-
was determined by the frequency with which it appeared with               ing across individuals who each are using only one of the two
its referent. Each of the six objects were randomly assigned              competing words to name each object. To rule out this pos-
to one of the six frequency conditions for every participant.             sibility, we examined the consistency of production among
The word pairs (w1 and w2 ) used to refer to each object were             individual participants. We found that only 6 out of 30 par-
also randomized for every participant. On each trial, the ob-             ticipants regularized all of their productions.
ject was presented for 3000 ms separated by 3000 ms, and the                 The results of this experiment seem to suggest that people
word was played concurrently with the visual stimuli. In ad-              probability match when learning the probabilities with which
dition to the auditory stimuli, the word was visually presented           words can be used to describe objects. These results are con-
below the moving object.                                                  sistent with the conclusions of Vouloumanos (2008). How-
   The test block consisted of a forced choice selection task.            ever, the formal analyses presented above suggested that it
Participants saw one object in the center of the screen and the           may be difficult to detect a weak bias towards regularization
                                                                    232

             a) Participants’ productions
                    Condition 0                                Condition 1                    Condition 2                     Condition 3                    Condition 4                    Condition 5
                       0                              0                              0                               0                              0                              0
                       1                              1                              1                               1                              1                              1
         Generations
                       2                              2                              2                               2                              2                              2
                       3                              3                              3                               3                              3                              3
                       4                              4                              4                               4                              4                              4
                       5                              5                              5                               5                              5                              5
                           0     2   4   6   8 10         0     2   4   6   8 10         0     2   4   6   8 10          0     2   4   6   8 10         0     2   4   6   8 10         0     2   4   6   8 10
              b) Sampling
                       0                              0                              0                               0                              0                              0
         Generations
                       1                              1                              1                               1                              1                              1
                       2                              2                              2                               2                              2                              2
                       3                              3                              3                               3                              3                              3
                       4                              4                              4                               4                              4                              4
                       5                              5                              5                               5                              5                              5
                           0    2    4   6   8   10       0    2    4   6   8   10       0    2    4   6   8   10        0    2    4   6   8   10       0    2    4   6   8   10       0    2    4   6   8   10
               c) MAP estimation
                       0                              0                              0                               0                              0                              0
         Generations
                       1                              1                              1                               1                              1                              1
                       2                              2                              2                               2                              2                              2
                       3                              3                              3                               3                              3                              3
                       4                              4                              4                               4                              4                              4
                       5                              5                              5                               5                              5                              5
                           0    2    4   6   8   10       0    2    4   6   8   10       0    2    4   6   8   10        0    2    4   6   8   10       0    2    4   6   8   10       0    2    4   6   8   10
                               Frequency of w1                Frequency of w1                Frequency of w1                 Frequency of w1                Frequency of w1                Frequency of w1
Figure 3: Results of Experiment 2. (a) Frequency of w1 produced by participants (horizontal axis) per generation (vertical
axis). Each panel corresponds to increasing values of the frequency of w1 in the input to the first learner (right to left 0, 1 2,
3, 4, 5), and each line to one “family” of participants. Iterated learning with Bayesian agents using (b) sampling and (c) MAP
estimation produce predictions in correspondence with these results. White cells have zero probability, darker grey indicates
higher probability. The sampling model provides a better account of the participants’ responses.
in a single generation of learners, even though such a bias                                                          training items for the participant in the next generation of that
might still have a significant effect on language evolution.                                                         family. Participants were not made aware that their test re-
Experiment 2 was designed to investigate the possibility that                                                        sponses would serve as training for later participants and in-
people have biases towards regularization that only emerge                                                           tergenerational transfer was conducted without personal con-
over several generations of iterated learning.                                                                       tact between participants.
           Experiment 2: Iterated learning                                                                           Results
Method                                                                                                               The results of Experiment 2 are shown in Figure 3. The top
Participants Fifty undergraduates participated for course                                                            row shows participants’ productions for each of the 10 fam-
credit. The participants formed five generations of learners                                                         ilies. The data is broken down across the six different initial
in 10 “families”. The responses of each generation of learn-                                                         conditions of relative frequency of w1 . Across all conditions,
ers during the test phase were presented to the next generation                                                      the frequencies of w1 moved rapidly towards 0 and 10, re-
of learners as the training stimuli.                                                                                 flecting a bias towards regularization. In fact, by the fourth
                                                                                                                     generation, all productions were completely regular.
Materials The materials used in Experiment 2 were the
                                                                                                                        The sampling and MAP models introduced above were
same as in Experiment 1.
                                                                                                                     both fit to these data by maximum-likelihood estimation of
Procedure For the 10 learners who formed the first gen-                                                              the parameter α. The predictions of these models are shown
eration of any family, the methods and procedure of the ex-                                                          in the middle and bottom rows of Figure 3. As can be seen
periment were identical to Experiment 1. In subsequent gen-                                                          from the figure, the models do a good job of capturing the dy-
erations, the method and procedure were the same, except                                                             namics of iterated learning. For the sampling model, the value
that the frequency conditions in the training phase were de-                                                         of α that best fit the data was 0.05, giving a log-likelihood
termined by the productions of the previous participant within                                                       of -266, equivalent to a probability of 0.41 of correctly pre-
a family. That is, intergenerational transfer was implemented                                                        dicting the next value of w1 from the previous one. For the
by letting the frequencies of w1 (and w2 ) produced by a sin-                                                        MAP model, the value of α that best fit the data was 0.09,
gle participant during the test phase be the frequencies of the                                                      with a log-likelihood of -357, equivalent to a probability of
                                                                                                               233

0.3 of correctly predicting the next value of w1 . These results     from misalignments of the learner’s and adult’s hypothesis of
suggest that the human data are better characterized in terms        the data (Pearl & Weinberg, 2007). This means that mod-
of learners sampling from their posterior distributions rather       eling language evolution may provide an effective way to
than MAP estimation.                                                 empirically test hypotheses about language acquisition. It-
   Two aspects of the data are nicely captured by the model.         erated learning offers a method to do this in the laboratory,
First, as shown in the middle and bottom panels in Figure 3,         connecting acquisition and evolution in a way that allows us
the values of w1 selected by learners in early iterations are        to make contributions to understanding both of these pro-
close to the initial frequency of w1 . Thus, the model pre-          cesses. First, consistent with recent work on language evo-
dicts responses that are consistent with probability matching        lution (Kirby, Dowman, & Griffiths, 2007), the experiments
when a single generation is considered. Second, the best fit-        show how weak inductive biases can have strong effects in
ting model is one where the prior distribution is U-shaped           shaping linguistic distributions. Second, the results indicate
(see Figure 1, left panels). This means that the distribution        that inductive biases can be hard to identify by testing indi-
over frequencies should converge to an equilibrium where             vidual learners, while they become evident in the context of
one variant becomes the vast majority in the population, while       language evolution. This suggests that a full understanding
the other one becomes very infrequent. Thus, the model pre-          of the constraints on language acquisition might require the
dicts regularization of inconsistent language forms as a con-        combination of multiple empirical approaches, including the-
sequence of learners’ inductive biases.                              oretical and empirical investigation of language evolution.
                                                                     Acknowledgments. We thank Athena Vouloumanos for providing
                          Discussion                                 the materials used in the experiments, and Carla Hudson-Cam and
                                                                     Fei Xu for suggestions. We also thank Aaron Beppu, Matt Cam-
Experiment 1 revealed that when participants were exposed
                                                                     mann, Jason Martin, Vlad Shut and Linsey Smith for assistance in
to inconsistent word-meaning mappings, the frequencies de-
                                                                     running the experiments. This work was supported by grants BCS-
termined by their responses were close to the frequencies
                                                                     0631518 and BCS-0704034 from the National Science Foundation.
present in training stimuli. This is consistent with the predic-
tions of our Bayesian model. Moreover, the results are in ac-                                    References
cord with the pattern of responses reported by Vouloumanos           Bickerton, D. (1981). Roots of language. Ann Harbor, MI: Karoma.
(2008), which showed that participants were sensitive to fine-       Boas, M. L. (1983). Mathematical methods in the physical sciences
grained patterns of word-meaning mappings. The results of               (2nd ed.). New York: Wiley.
                                                                     DeGraff, M. (1999). Creolization, language change, and language
Experiment 2, however, revealed a trend toward regulariza-              acquisition: An epilogue. In M. DeGraff (Ed.), Language creation
tion that was not obvious in a single generation. The distribu-         and language change: Creolization, diachrony, and development
tion over competing words converged toward an equilibrium               (p. 473-543). Cambridge: MIT Press.
                                                                     Ewens, W. (2004). Mathematical population genetics. New York:
where one of the variants becomes the vast majority in the              Springer-Verlag.
population. The dynamics of convergence again matched the            Gomez, R., & Gerken, L. (2000). Infant artificial language learning
predictions of our Bayesian model. This pattern of results              and language acquisition. Trends in Cognitive Sciences, 4, 178–
                                                                        186.
indicates that weak regularization biases may have a strong          Griffiths, T. L., & Kalish, M. L. (2007). Language evolution by iter-
effect on language change over time, and that iterated learn-           ated learning with bayesian agents. Cognitive Science, 31, 441–
ing provides an effective method for revealing the inductive            480.
                                                                     Hudson, C., & Newport, E. (2005). Regularizing unpredictable vari-
biases of human learners.                                               ation: The roles of adult and child learners in language formation
   The results suggest that learners’ inductive biases may fa-          and change. Language Learning and Development, 1, 151–195.
vor regularization of inconsistent language forms. However,          Kirby, S. (2001). Spontaneous evolution of linguistic structure:
                                                                        An iterated learning model of the emergence of regularity and
the question remains of how these inductive biases – repre-             irregularity. IEEE Journal of Evolutionary Computation, 5, 102–
sented in our model as a prior distribution – should be in-             110.
terpreted from a psychological viewpoint. The model’s prior          Kirby, S., Dowman, M., & Griffiths, T. L. (2007). Innateness and
                                                                        culture in the evolution of language. Proceedings of the National
distribution should not necessarily be interpreted as the re-           Academy of Sciences, 104, 5241-5245.
sult of innate constraints specific to language. Rather, learn-      Pearl, L., & Weinberg, A. (2007). Input filtering in syntactic ac-
ing biases affecting the formation of linguistic representations        quisition: Answers from language change modeling. Language
                                                                        learning and development, 3, 43–72.
could come from a number of domain-general constraints on            Reali, F., & Griffiths, T. (2008). Words as alleles: Equivalence
learning, such as information-processing constraints, limita-           of iterated learning and neutral models from population genetics.
tions on working memory; or the inductive bias associated               (in preparation)
                                                                     Saffran, J. (2003). Statistical language learning: Mechanisms and
with some kind of general-purpose learning algorithm. An-               constraints. Current Directions in Psychological Science, 12,
other possibility is that the biases reflected by the model’s           110–114.
prior distribution are not innately specified but the result of      Vouloumanos, A. (2008). Fine-grained sensitivity to statistical in-
                                                                        formation in adult word learning. Cognition. (in press)
previous domain-specific experience.                                 Wonnacott, E., & Newport, E. (2005). Novelty and regularization:
   Iterated learning models capitalize on the fundamental re-           The effect of novel instances on rule formation. In BUCLD 29:
                                                                        Proceedings of the 29th Annual Boston University Conference on
lation between language acquisition and language evolution.             Language Development. Somerville, MA: Cascadilla Press.
For example, certain types of language change may result
                                                                 234

