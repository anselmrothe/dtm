UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Bayesian Models of Inductive Learning
Permalink
https://escholarship.org/uc/item/7sr0j7rj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Griffiths, Thomas L.
Kemp, Charles
Tenenbaum, Joshua B.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             Bayesian Models of Inductive Learning
                              Thomas L. Griffiths (tom griffiths@berkeley.edu)
                               Department of Psychology and Cognitive Science Program
                              University of California, Berkeley, Berkeley CA 94720 USA
                                       Charles Kemp (ckemp@cmu.edu)
                                               Department of Psychology
                                   Carnegie Mellon University, Pittsburgh PA 15213
                                     Joshua B. Tenenbaum (jbt@mit.edu)
                                      Department of Brain and Cognitive Sciences
                          Massachusetts Institute of Technology, Cambridge MA 02139 USA
   Many of the central problems of cognitive science are       background in Bayesian statistics and a level of mathe-
problems of induction, calling for uncertain inferences        matical sophistication appropriate for an audience with
from limited data. How can people learn the meaning            general interests in computational modeling.
of a new word from just a few examples? What makes                The tutorial will begin with a discussion of the how
a set of examples more or less representative of a con-        Bayesian models fit into the general project of developing
cept? What makes two objects seem more or less sim-            formal models of cognition. We will then outline some of
ilar? Why are some generalizations apparently based            the basic principles of Bayesian statistics that are of rel-
on all-or-none rules while others appear to be based on        evance to modeling cognition, before turning to a series
gradients of similarity? How do we infer the existence         of case studies illustrating these methods, contrasting
of hidden causal properties or novel causal laws? This         multiple models both within the Bayesian approach and
tutorial will introduce an approach to explaining these        across different modeling approaches. Topics will include
everyday inductive leaps in terms of Bayesian statistical      graphical models, hierarchical Bayes, and Monte Carlo
inference, drawing upon tools from statistics (Bernardo        methods, and include discussion of how to do the math
& Smith, 1994; Gelman, Carlin, Stern, & Rubin, 1995),          and programming required by Bayesian models (see Grif-
machine learning (Duda, Hart, & Stork, 2000; Mackay,           fiths, Kemp, & Tenenbaum, in press, for an outline).
2003), and artificial intelligence (Pearl, 1988; Russell &     Through considering case studies of particular cognitive
Norvig, 2002).                                                 tasks, we will also discuss how to relate the abstract com-
   In Bayesian models, learning and reasoning are ex-          putations of Bayesian models to more traditional models
plained as probability computations over a hypothesis          framed in terms of cognitive processing or neurocompu-
space of possible concepts, word meanings, or causal           tational mechanisms.
laws. The structure of the learner’s hypothesis space
reflects their domain-specific prior knowledge, while                                 References
the nature of the probability computations depends on          Bernardo, J. M., & Smith, A. F. M. (1994). Bayesian theory.
                                                               New York: Wiley.
domain-general statistical principles. Bayesian mod-
els of cognition thus pull together two approaches that        Chater, N., Tenenbaum, J. B., & Yuille, A. (Eds.) (2006).
have historically been kept separate, providing a way to       Special issue on “Probabilistic models of cognition”. Trends
                                                               in Cognitive Sciences, 10 (7).
combine structured representations and domain-specific
knowledge with domain-general statistical learning.            Duda, R. O., Hart, P. E., & Stork, D. G. (2000). Pattern
   We will demonstrate how this approach can be used           classification. New York: Wiley.
to model natural tasks where people draw on consid-            Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B.
erable prior knowledge, including abstract domain the-         (1995). Bayesian data analysis. New York: Chapman &
ories and structured relational systems (e.g., biological      Hall.
taxonomies, causal networks). Formalizing aspects of           Griffiths, T. L, Kemp, C.,., & Tenenbaum, J. B. (in press).
these knowledge structures will be critical to specify-        Bayesian models of cognition. In R. Sun (ed.) The Cambridge
ing reasonable prior probabilities for Bayesian inference.     Handbook of Computational Cognitive Modeling.
Specifically, we will show how key principles in people’s      Mackay, D. J. C. (2003). Information theory, inference,
intuitive theories of natural domains can be formalized        and learning algorithms. Cambridge: Cambridge University
as probabilistic generative systems, generating plausi-        Press.
ble hypotheses to guide Bayesian learning and reasoning        Pearl, J. (1988). Probabilistic reasoning in intelligent sys-
(Tenenbaum, Griffiths, & Kemp, 2006).                          tems. San Francisco, CA: Morgan Kaufmann.
   Bayesian inference has become an increasingly pop-          Russell, S. J., & Norvig, P. (2002). Artificial intelligence: A
ular component of formal models of human cognition             modern approach (2nd ed.). Englewood Cliffs, NJ: Prentice
(Chater, Tenenbaum, & Yuille, 2006). This full-day tu-         Hall.
torial aims to prepare students to use these modeling
                                                               Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006).
methods intelligently: to understand how they work,            Theory-based Bayesian models for inductive learning and rea-
the advantages they offer over alternative approaches,         soning. Trends in Cognitive Sciences, 10 (7).
and their limitations. The tutorial will assume minimal
                                                            7

