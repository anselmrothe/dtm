UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Children's Grammars Grow More Abstract with Age
Permalink
https://escholarship.org/uc/item/75v7j2xf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Borensztajn, Gideon
Zuidema, Willem
Bod, Rens
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

         Children’s Grammars Grow More Abstract with Age – Evidence from an
           Automatic Procedure for Identifying the Productive Units of Language
                                           Gideon Borensztajn (gideon@science.uva.nl)
                                           Willem Zuidema (jzuidema@science.uva.nl)
                                                   Rens Bod (rens@science.uva.nl)
                               Institute for Logic, Language and Computation, University of Amsterdam
                                   Plantage Muidergracht 24, 1018 TV, Amsterdam, The Netherlands
                              Abstract                                  memory and attentional abilities (Chomsky, 1980); most of
                                                                        the knowledge of language is in place from birth, and only
   We develop an approach to automatically identify the most
   probable multi-word constructions used in children’s utter-          their parametrization needs to be set by the environmental
   ances, given syntactically annotated utterances from the Brown       triggers (Clahsen, 1996). Hence, in this tradition children
   corpus of CHILDES. The found constructions cover many in-            are assumed to have, at least in competence, the same syn-
   teresting linguistic phenomena from the language acquisition
   literature, and show a progression from very concrete towards        tactic categories and rules as adults; this is referred to as the
   abstract constructions. We show quantitatively that for all chil-    continuity assumption (e.g. Crain & Thornton, 2005).
   dren of the Brown corpus grammatical abstraction, defined as            These opposing views on the units of language acquisition
   the relative number of variable slots in the productive units of
   their grammar, increases globally with age.                          are, of course, best investigated empirically, on the basis of
                                                                        actual language usage. Several in-depth case-by-case anal-
   Keywords: First language acquisition; Usage Based Gram-
   mar; Constructions; Data-oriented Parsing                            yses on the productive units in children’s corpora have been
                                                                        reported. For instance, Hodges, Krugler, and Law (2004) ana-
                          Introduction                                  lyzed the item-based nature of the acquisition of the complex
                                                                        construction ‘I V (NP) to VP-INF’, as in I want (you) to play.
Many contemporary theories of language acquisition assume               Lieven, Behrens, Speares, and Tomasello (2003) traced back
that the basic units of language acquisition are constructions:         the sources of creativity of target utterances in the child’s
associations between a semantic frame and a syntactic pat-              speech. The target utterances were reconstructed (manually)
tern, for which the meaning or form is not strictly predictable         from a set of utterances used in the previous 6 weeks. They
from its component parts. Learning, in this framework, con-             found that 74% of all novel target utterances produced in one
sists of the gradual acquisition of a structured inventory of           day by a 2 year old child could be reduced to previously pro-
constructions, a constructicon, where the constructions are of          duced utterances by using a single combinatorial operation.
various sizes and varying degrees of complexity and abstract-           Their finding supports the hypothesis that the smallest units
ness (Goldberg, 2006; Tomasello, 2003).                                 used in language production are often memorized multi-word
   Empirical studies in this tradition (e.g., Peters, 1983;             constructions, rather than single words.
Tomasello, 2003) show that, first, the primary units of speech
                                                                           To resolve the controversy, however, we believe it is essen-
of children in their first stage of language acquisition are not
                                                                        tial to move beyond the typical handful of linguistic examples
words but complete utterances, or holophrases. Second, in               that support one view over the other. In the current study, we
the earliest stages the child’s language is item-based in nature
                                                                        develop computational tools for automatically identifying the
(Tomasello, 2000). Verb constructions are typically learned
                                                                        most likely primitive units that were used by the child to pro-
case-by-case (so-called verb islands), without reference to a
                                                                        duce the utterances in a given corpus. We apply these tools
general verb-class. The scope of the syntactic rules is limited
                                                                        to a well-known English-language corpus (the Brown corpus
to specific constructions, and system-wide syntactic rules or
                                                                        in CHILDES) with longitudinal data from three children. We
categories are mostly lacking. Third, in subsequent stages the
                                                                        then present a qualitative and quantitative analysis of the pro-
child breaks down the item-based constructions, introducing
                                                                        ductive units that these children employ in progressive stages
variables, such as in Where’s the X?, I wanna X, etc.                   of acquisition. Note that we do not model the actual process
   The acquisition of constructions with variable slots forms           of language acquisition or attempt to directly choose between
the beginning of abstraction and category formation, and it             usage-based and generative theories of language acquisition.
marks the beginning of grammar. Such Usage Based theo-                  Rather, we aim at providing a new way to evaluate predic-
ries of language acquisition assume a dynamically changing              tions from theories of language performance in either tradi-
grammar that follows a route from simple and concrete to                tion about the productive units in child language.
complex and abstract constructions. This view is in sharp
contrast with the view on language acquisition taken in many
                                                                                  Choosing the right representation
versions of generative grammar. Here, grammar rules and
categories are assumed to be universally and innately spec-             In this section, we develop a formal definition of the produc-
ified by a Universal Grammar. The reason that children do               tive units of language and a probability model that defines the
not produce adult-like grammatical sentences is their limited           likelihood of various hypotheses on the units used. The for-
                                                                     47

mal model of choice needs to have the flexibility to allow for                    a leaf node with a fragment ti , of which the root r(ti ) has
elementary syntactic units of variable size, form and level of                    the same label as the leaf node. Enriched with probabilities,
abstraction. Moreover, the model should not assume a pri-                         TSGs become probabilistic tree substitution grammars.
ori that the syntactic units the child uses coincide with the                        Several alternative methods (estimators) for finding the
units used in adult language. The grammar framework should                        probabilities P(ti |r(ti )) of the fragments (the parameters of
therefore be data-oriented: potential syntactic units should be                   the DOP grammar) have been proposed. The earliest estima-
derived from the corpus itself. For instance, the construction                    tor is known as DOP1 (Bod, 1998), and assigns probabili-
“I V to VP-INF” should be a possible building block, even                         ties based on relative frequencies. For the current work we
though it contains multiple words, separated by variable slots                    adopted a recent estimator, “push-n-pull” (Zuidema, 2006,
(i.e., it is discontiguous), as well as “I going V NP-OBJ”, even                  2007), which yields linguistically more plausible results.
though it is agrammatical (it lacks the conjugated “am”).                            Formal details of push-n-pull fall outside the scope of this
   A formalism with the required flexibility is that of Tree                      paper, but the basic idea is as follows. The algorithm uses the
Substitution Grammar (TSG), which forms a generalization                          discrepancy between the observed frequency of the subtrees
over the well-known context-free grammars (CFG) and a sub-                        in the treebank and their expected frequency (as predicted by
class of the Tree Adjoining Grammars (Joshi, 2004). TSGs                          the current parameters of the grammar) to either push prob-
can model complex multi-word syntactic primitives as well                         ability mass from a subtree to the elementary trees involved
as single unit primitives (see Figure 1 for an illustration). The                 in its derivation, or pull probability mass from the elementary
generative components of a TSG are tree fragments of arbi-                        trees to the subtree. The algorithm includes a parameter that
trary size and depth, which can be (partly) lexicalized or ab-                    regulates the strength of a bias toward smaller subtrees. The
stract. In the latter case the fragments contain variable slots                   difference between observed and expected frequency is high-
for syntactic categories (nonterminals), making them suitable                     est for subtrees in the corpus that are most overrepresented
for representing abstract constructions or abstract rules.                        relative to what should be expected based on the frequencies
   TSGs are used extensively in the framework of Data Ori-                        of their components; many of these subtrees correspond to
ented Parsing (DOP) (Bod, 2003; Scha, Bod, & Sima’an,                             linguistically interesting constructions. By iterating the pro-
1999), which provides the techniques to parse new sentences                       cess, probability mass is shifted between subtrees until ex-
using fragments from sentences observed in a corpus. In                           pected frequencies approach observed frequencies.
DOP, the elementary tree fragments of the TSG can in prin-                           Given a TSG as described above and a probability distribu-
ciple be any subtree occurring in an annotated corpus (the                        tion P(ti |r(ti )) as found by push-n-pull, we can use standard
treebank). Two elementary tree fragments can be combined                          statistical parsing techniques to find the most probable deriva-
by means of the substitution operator ◦ if the left-most non-                     tion of any sentence in a corpus. This yields a decomposition
terminal leaf node of the first fragment is identical to the root                 of the sentence into those elementary tree fragments that to-
node of the other fragment. A derivation of a sentence in DOP                     gether constitute a hypothesis on how the sentence was gen-
is a sequence of elementary tree fragments t1 ◦t2 ◦ . . . ◦tn such                erated. This way, we can use DOP as a statistical approach
that the root of the first fragment is S and the leaves of the                    for discovering the constructions in child language.
resulting tree are terminals (see Figure 1).                                         All the analyses reported here were conducted with the
                                                                                  push-n-pull algorithm, with the bias parameter set to 0.3. We
         S          ◦               OBJ      =            S                       have also performed tests with different settings of the bias
   PRO       X               MOD          N
                                                 PRO            X
                                                                                  and with the DOP1 estimator, and found the same, and some-
    I   PART OBJ↓       N:PROP N:PROP fly          I                              times even more pronounced trends than are reported here.
        making            Mr      Grant                PART           OBJ
                                                      making      MOD       N
                                                                                                               Method
                                                             N:PROP N:PROP fly
                                                               Mr    Grant        The studies were conducted on the Brown corpus (Brown,
                                                                                  1973) from the CHILDES database (MacWhinney, 2000).
Figure 1: Derivation for ‘I making Mr Grant fly’ (Adam,                           This corpus contains transcribed longitudinal recordings of
3;3.04). The substitution site is marked with ↓.                                  three children, Adam, Eve and Sarah. We split each of these
                                                                                  subcorpora into three parts of roughly equal size, represent-
   In the DOP-framework, several probability models have                          ing three consecutive time periods (see Table 1). We removed
been worked out. In the simplest set-up, it is assumed that                       the parental speech and any annotation or comments. We also
the probability of any substitution is independent of the con-                    removed from the child’s speech incomplete and interrupted
text; the probability of a derivation is therefore the product of                 sentences (‘+...’, ‘+/.’ and ‘+,’), and sentences containing
probabilities of the fragments used:                                              pauses (‘#’). These account for approximately 20% of the
                                                                                  sentences. Furthermore, we discarded the final punctuation.
                  P(t1 ◦ t2 ◦ . . . ◦ tn ) = ∏ P(ti |r(ti ))                         In splitting the data, we did not attempt to match children
                                              i
                                                                                  on either age, Mean Length of Utterance (MLU) or tradi-
where P(ti |r(ti ) is the probability of a single substitution of                 tional “stages” of language development (Brown, 1973); we
                                                                               48

Table 1: Statistics of input (P1=Period 1; MLU= range                     Table 2: Frequent PoS tags and Grammatical Relations.
of numbers of morphemes per utterance, averaged per file;
a.s.l.= number of words per utterance, averaged per period;
                                                                         Parts of Speech             Category
vocab.= number of distinct words; t/t = type/token frequency-            N, N:PROP                   Noun, Proper Noun
ratio of words).                                                         V, V:AUX                    Verb, Auxiliary verb, including modals
                                                                         DET, DET:NUM                Determiner (the, a), Number
            files age range  #sent.   MLU       a.s.l. vocab.   t/t      ADJ, ADV                    Adjective, Adverb
  Adam                                                                   PRO,       PRO:DEM,         Pronoun, Demonstrative Pronoun (this,
    P1      1-16   2:3-2:11  11184  1.83-2.90   2.23    1407  .056       PRO:WH                      that), Interrogative Pronoun (who, what)
    P2     17-32   2:11-3:6  11578  2.44-4.06   3.29    2010  .053       CONJ                        Conjunction
    P3     33-48    3:6-4:5   9071  3.63-4.97    4.0    2006  .055       INF                         Infinitive marker (to)
   Eve                                                                   PREP                        Preposition
    P1       1-7    1:6-1:9   3485  1.53-2.28   1.88     669  .102       Gramm. Relation             Category
    P2      8-14    1:9-2:0   3395  2.51-3.22   2.80     785  .083       ROOT                        Special relation for the top node
    P3     15-20    2:1-2:3  3535   2.60-3.41   3.13     958  .087
                                                                         SUBJ, OBJ                   Subject, Object
  Sarah
                                                                         PRED                        Predicative (I am not sure)
    P1      1-45    2:3-3:2  11693  1.48-2.70   1.87    1389  .063
    P2     46-90    3:2-4:1  8384   2.23-3.70   2.71    1706  .075       COMP, XCOMP                 Clausal complements, finite (I think I saw
    P3    91-135    4:1-5:0  8525   2.98-4.86    3.2    1944  .071                                   Paul) and non-finite (you have to put it in
                                                                                                     your truck)
                                                                         JCT                         Adjunct (optional modifier of verb)
                                                                         COORD                       Coordination, dependents of the conjunc-
                                                                                                     tion (go and get it)
                                                                         AUX, NEG                    Auxiliary and negation
                                                                         LOC                         Locative arguments of verbs (in your truck)
                                                                       that of (Xia & Palmer, 2001). From Table 3 it can be seen,
                                                                       that at times the conversion introduced a dummy node (la-
                                                                       beled X), to fill up a gap in the (binary) parse tree, where the
                                                                       dependency annotation did not provide this.2
                                                                                                        Results
                                                                       Qualitative analysis
       Figure 2: Sentence length distribution for Adam.
                                                                       In the current setup, the syntactic categories (nonterminals)
                                                                       are pregiven; our method only determines the size of the pro-
can thus only compare grammatical development within each              ductive units involved in the generation of each sentence. We
child, and not between them. Table 1 summarizes the input              are interested in those cases where larger fragments seem nec-
used for our studies, after all preprocessing steps. Note that,        essary than implicit in the existing corpus annotations; for
unsurprisingly, average sentence lengths increase markedly             our analysis, we therefore focus on elementary trees of depth
in each child; in Figure 2 we plot the number of sentences of          larger than 1 and will refer to these as the constructions.
each length for each of the three parts of the Adam corpus.               Our method found linguistically very informative construc-
   Push-n-pull was trained on syntactically annotated sen-             tions in all children. In Table 3 we give Adam’s 15 most
tences from each of the subcorpora. Recently the Brown                 frequently used constructions of each period, as well as the
corpus has been augmented with syntactic dependency an-                15 most frequent discontiguous ones. In the figure, part of
notations by Sagae, Davis, Lavie, MacWhinney, and Wint-                speech tags are indicated in capitals, and grammatical rela-
ner (2007). The authors labeled the dependencies using 37              tions appear in bold capitals. Explanations of the labels are in
distinct grammatical relations (details of the procedure and           Table 2. A few things may be noted from Table 3:
a complete list of the labels can be found in (Sagae et al.,
2007)). Their parser uses the parts of speech from the MOR-            • Whereas in Period 1 most constructions are very concrete,
tagger, described in (MacWhinney, 2000). In Table 2 we list               starting from Period 2 constructions become abstract (as
the most frequent grammatical relations and PoS tags.                     can be seen from the increased number of substitution
   We converted the dependency annotation and labels of                   sites). We further support this observation by quantitative
Sagae et al. (2007) to a constituency annotation for further              results in the next section.
processing1. The conversion heuristic we used is similar to            due to a dependency having more than a single root, or the postag
                                                                       (MOR) and syntax (XSYN) sequence being of unequal length.
    1 Approximately 10% of the sentences failed to convert, mostly         2 Details at http://staff.science.uva.nl/∼gideon/cogsci/
                                                                    49

Table 3: Adam’s most frequent multi-word constructions (shown are only the leaf nodes). To facilitate reading, we have restored
some of the lexical items from the MOR tagger with their original form. For instance, we replaced be-3s by is, go-prog by
going, go-past by went, and zero-forms, such as put-zero by put.
   #  Period 1         #  Period 2                   # Period 3          # Period 1               #    Period 2                     #     Period 3
  88  right there     82  what is this             127 I do not know    33 where N go             9    you V it                   10      you V it
  48  where go        80  PRO:WH is PRO:DEM         69 what is this     11 I V it                 8    I do NEG want INF X          5     you X and PRO X
  45  why not         74  do you want PRO COMP      51 PRO:WH is that    6 what that N doing      7    I V it JCT                   5     will you V it
  42  where is        53  I do not know             46 I going XCOMP     5 take N off             6    you V it                     4     can PRO put X
  36  play toy        52  do you want X             44 it is PRED        4 who N that             6    where PRO went               4     a ADJ one
  33  where N go      41  I going XCOMP             33 I want INF X      4 do NEG V it            6    let me V it                  4     do NEG know PRO:WH PRO V
  30  what happen     36  open it                   27 it is X           4 have N on              5    I can NEG V it               4     do NEG know PRO:WH PRO:DEM V
  28  read that       32  PRO:WH is it              27 I am going INF X  4 you V it               5    going INF make DET N         4     and PRO:WH is that
  24  nineteen twelve 30  it is PRED                27 what is it        3 what N doing           5    let us play DET game         4     can PRO put OBJ LOC
  21  N go            28  you want INF X            23 I think I X       3 put N on               5    what kind N that             4     what is PRO:DEM for
  20  busy bulldozer  26  I going V OBJ             22 I can not         3 put OBJ on             4    going put OBJ in it          4     I can not V it
  19  in there        25  what you want             22 that is PRED      3 do not V me            4    a N cake                     3     how AUX you V PRO:DEM
  19  PRO:DEM a N     24  let me COMP               21 here is SUBJ      3 take OBJ off           4    I V him                      3     maybe PRO is X
  19  that N          23  how do you know           20 is a N            3 where N N go           4    in DET kitchen               3     you V this ADV
  18  that is right   22  I AUX NEG X               20 V it              3 I V some               4    you V me COMP                3     I going X off
                       Most frequent constructions                                               Most frequent discontiguous constructions
• The lists cover many linguistically interesting construc-
                                                                                                    TOP                                        TOP
   tions, such as the progressive, use of auxiliaries, clausal                               PRED           X
                                                                                                                                   X                      ADV:LOC
                                                                                       PRO:WH     N↓    V   PRO:DEM
   constructions with want and think, and particle verbs (take                                                                V         OBJ↓
                                                                                                                                                             down
                                                                                        whose    house  is     that
   OBJ off, going put OBJ in it).                                                                                            put DET↓     PRO:INDEF↓
• Constructions including non-finite clausal complements                                                                          this         one
   (XCOMP) start to appear in Period 2, but become more                                    TOP                                                     TOP
   frequent in Period 3 (sometimes annotated INF X) .                               PRO              X
• There is a tendency to progressively use verb constructions                       you
                                                                                                                                     X                            ADV
                                                                                                                                                                  alone
   in combination with variable pronomina, as is particularly                             V                     X
                                                                                                                             V                OBJ↓
                                                                                         eat
   notable from the increased use of pronominal tags among                                              OBJ         ADV    leave
                                                                                                                                 PRO:POSS↓             X↓
   discontiguous constructions.                                                                PRO:POSS↓      N↓     up
                                                                                                                                       my       PART↓       N↓
• The use of do-support in questions and negations starts in                                      your      animals
                                                                                                                                                packing   things
   the P2 and becomes more abstract in the top discontiguous
   constructions of P3 (how AUX you V PRO:DEM).                                   Figure 3: Examples of derivation trees as found by our
                                                                                  method. Substitution sites are in bold and marked with ↓.
   Another way of looking at the output of our method is by
going through individual sentences from each of the corpora
and checking how sentences get decomposed into their hy-                              of variable slots in linguistic constructions, which can be
pothesized building blocks. Figure 3 gives some typical ex-                           operationalized as the ratio between the number of substi-
amples of derivations, but note that there are also examples of                       tution sites (non-terminals) and the number of lexical items
linguistically less plausible decompositions. The decomposi-                          (terminals) in the elementary trees of each derivation.
tions of the entire Brown-corpus are available as supplemen-                      • Are all elementary trees contiguous? The occurrence of
tary material to this article (see footnote 2).                                       discontiguous constructions, where a substitution site is
                                                                                      preceded and followed by a lexical item would help explain
Quantitative analysis                                                                 long distance dependencies (such as agreement of number
Once we have determined the most probable derivations of a                            and tense) between the lexical items.
child’s utterances as recorded in a corpus, it becomes possi-
                                                                                      A first observation from Table 4 is that constructions
ble to quantify the properties of the child’s grammar at vari-
                                                                                  become ubiquitous with age (see the column #construc-
ous stages in terms of properties of the used elementary trees,
                                                                                  tions/sentence). For all children there is a sharp increase in
such as node count and depth. This, in turn, allows us to start
                                                                                  the number of constructions between P1-P2, and for all ex-
answering questions like:
                                                                                  cept Eve also between P2-P3. The overall averages of most
• What is the size of the primitive building blocks used?                         of the relevant quantities show an increase with age for all
• Does the size of the building blocks decrease with age, as                      children (for instance for the number of nodes, nonterminals,
   would be expected if constructions are broken down into                        terminals, depth, discontiguity: see Table 4). However, be-
   their parts? To operationalize size, we simply counted the                     fore drawing conclusions about changes of the nature of con-
   number of nodes in the elementary trees.                                       structions with time, it is important to rule out the possibility
• Do the productive units of the grammar become more ab-                          that the effects are only due to sentence length distributions,
   stract with age? Abstraction correlates with the number                        which are shifting toward longer sentences for the later peri-
                                                                           50

                               Table 4: Overall averages of the most important measures on constructions.
         Quantity                            Adam                          Eve                            Sarah
                                             Period 1  Period 2  Period 3  Period 1  Period 2   Period 3  Period 1  Period 2  Period 3
         average #nodes in cxs               7.36      8.80      9.20      7.14      8.32       8.76      7.64      8.65      8.86
         #terminals                          2.22      2.46      2.47      2.21      2.50       2.45      2.30      2.35      2.34
         #non-terminals                      5.14      6.35      6.73      4.93      5.83       6.32      5.34      6.3       6.52
         #leaf non-terminals                 0.41      0.93      1.16      0.32      0.59       0.93      0.45      0.98      1.15
         ratio leaf-non-terminals/leaf-nodes 0.13      0.25      0.30      0.11      0.18       0.26      0.14      0.27      0.31
         average depth                       4.44      4.74      4.79      4.35      4.61       4.74      4.52      4.75      4.78
         #constructions/sentence             0.39      0.59      0.67      0.25      0.46       0.41      0.24      0.39      0.50
         #discont. cxs/sentence              0.050     0.085     0.093     0.020     0.051      0.106     0.041     0.064     0.071
         construction coverage               0.33      0.40      0.37      0.26      0.35       0.28      0.29      0.32      0.35
         #construction types                 1409      2658      2543      324       665        685       967       1294      1732
ods. This is a major methodological challenge, because most               constructions decreases. In Table 6 we show the time de-
of the quantities of interest, such as size and depth, depend on          velopment of the average ‘abstraction’ of the constructions,
the length of the sentence in which the construction appears.             which is defined as the ratio between leaf non-terminals and
   Therefore, in all the following studies, we neutralized the            leaf nodes in a construction. It can be seen, that abstraction
MLU factor by comparing sentences across periods accord-                  of the constructions increases with age for all children. The
ing to their length. We computed the average quantity (e.g.,              big variance is due to sentence length 2 (“holophrases”), for
depth, #nodes) for (constructions belonging to) different sen-            which the constructions remain very concrete in all stages;
tence lengths separately. Averages were computed over at                  if we leave these out, abstraction still increases significantly
least 30 constructions or discarded otherwise. We then com-               with age. Note that there is no simple explanation for in-
puted, still for each sentence length separately, growth rates            creasing abstraction in terms of the type/token frequency of
of those quantities. These were averaged, to obtain an av-                the vocabulary, since these quantities are neither positively
erage growth rate for the quantity between any two periods                nor negatively correlated (see Table 1).
(note that average growth rate is different from the growth
rate of the average, as computable from Table 4).
                                                                               Table 6: Growth rates of abstraction of constructions.
   After sentence length has been factored out, there is hardly
any effect left of age on construction size (the total number                                 P1→P2          P2→P3         P1→P3
of nodes in a construction). As can be seen in Table 5, most                        Adam      1.15 (.20)     1.06 (.17)    1.32 (.39)
growth rates are just above one, so the size of the construction                    Eve       1.41 (.37)     1.30 (.14)    1.68 (.39)
within sentences of a certain length remains close to constant                      Sarah     1.33 (.35)     1.01 (.22)    1.25 (.19)
(the variance is written within the parentheses). The same is
true for construction depth, which (unsurprisingly) correlates
                                                                              In Figure 4 we plot the average abstraction per sentence
well with construction size.
                                                                          length for Sarah; results for Adam and Eve are similar.
Table 5: Growth rates of construction size and depth. Shown
are averages of the growth rates per sentence length.
                 P1→P2              P2→P3           P1→P3
     Construction size
     Adam 1.019 (.042)              1.002 (.016)    1.024 (.054)
     Eve         1.020 (.045)       1.002 (.042)    1.013 (.052)
     Sarah 0.998 (.033)             1.006 (.029)    0.992 (.023)
     Construction depth
     Adam 1.016 (.019)              1.001 (.009)    1.022 (.021)
     Eve         1.047 (.029)       1.033 (.033)    1.059 (.037)
     Sarah 0.988 (.015)             1.008 (.009)    0.996 (.009)
   Whereas the number of nodes of the constructions remains               Figure 4: Abstraction (ratio leaf non-terminals/leaf nodes in
constant with age, the number of nonterminals increases with              constructions) against sentence length for Sarah.
age for all sentence lengths independently; the number of
nonterminals in the leaves of constructions increases even                    This result is striking, because when we look at the parse
more quickly. At the same time, the number of terminals in                trees in their entirety, the ratio between non-terminals and ter-
                                                                       51

minals is equal to 2 and does not vary with age (because the         Acknowledgments We thank Alon Lavie and colleagues
input parse trees are binarily branching). This explains the         for supplying us with the syntactic annotation of the Brown
fact that for the nodes in depth 1 subtrees we found a strong        corpus in an early stage, and three anonymous reviewers for
opposite effect of decreasing abstraction:                           valuable comments. This research was funded by the Nether-
                                                                     lands Organization for Scientific Research (NWO), through a
  Table 7: Growth rates of abstraction of depth one subtrees.
                                                                     Vici-grant “Integrating Cognition” (277.70.006) to RB and a
       Adam 0.86 (.071) 1.00 (.053)             0.85 (.082)
                                                                     Veni-grant “Discovering Grammar” (639.021.612) to WZ.
       Eve      0.94 (.031) 0.98 (0.056) 0.92 (.049)
       Sarah 0.90 (.051) 0.95 (.032)            0.86 (.071)
                                                                                                References
               Conclusions and discussion                             Bod, R. (1998). Beyond grammar: An experience-based theory
This study presented a novel and automatic procedure for                of language. Stanford, CA: CSLI Publications.
the discovery of multi-word constructions. Our approach               Bod, R. (2003). An efficient implementation of a new DOP
is a promising alternative to the evaluation of some of the             model. In Proceedings EACL’03.
core assumptions in theories of language acquisition. We be-          Brown, R. W. (1973). A first language: The early stages. Cam-
lieve there is much useful information available in the dis-            bridge, MA: Harvard University Press.
                                                                      Chomsky, N. (1980). Rules and representations. Behavioral
tributional patterns in corpora of child language that remains
                                                                        and Brain Sciences, 3, 1-61.
heretofore underexplored; this information can be accessed
                                                                      Clahsen, H. (1996). Generative perspectives on language ac-
using sophisticated statistical methods, such as used here, that
                                                                        quisition. Amsterdam, The Netherlands: Benjamins.
are flexible enough to accommodate for multi-word construc-
                                                                      Crain, S., & Thornton, R. (2005). Acquisition of syntax and se-
tions. Note that the fact that our method works without infor-          mantics. In M. Traxler (Ed.), Handbook of psycholinguistics.
mation about the semantics and pragmatics should not be in-             Oxford: Elsevier.
terpreted as implying a minor role for semantics in language          Goldberg, A. E. (2006). Constructions at work. the nature of
acquisition. In fact, we believe semantics is central in acqui-         generalization in language. Oxford University Press.
sition as well as use, but we aimed at developing techniques          Hodges, A., Krugler, V., & Law, D. (2004). A corpus study on
that work with the information present in current corpora.              the item-based nature of early grammar acquisition. Colorado
   A fundamental problem for research on the continuity hy-             Research in Linguistics, 17.
pothesis, and language acquisition in general, is that no con-        Joshi, A. K. (2004). Starting with complex primitives pays
sensus exists about reliable methods to identify the produc-            off: complicate locally, simplify globally. Cognitive Science,
tive units of language. Here, we explored an approach to                28(5), 637-668.
identifying the basic building blocks of language based on            Lieven, E., Behrens, H., Speares, J., & Tomasello, M. (2003).
distributional patterns alone, but alternative sources of in-           Early syntactic creativity: a usage-based approach. Journal
formation are also available, such as those explored in ap-             of Child Language, 30, 333-370.
proaches based on processing data (e.g., reading times, er-           MacWhinney, B. (2000). The CHILDES project: Tools for an-
rors) or relations to linguistic input (tracing back sentences to       alyzing talk. Third edition. Mahway, NJ: Lawrence Erlbaum.
child-directed speech, (Lieven et al., 2003)). Our method, ap-        Peters, A. (1983). The units of language acquisition. Cam-
plied to the Brown corpus, confirms the progressive abstrac-            bridge, UK: Cambridge University Press.
tion hypothesis: abstraction, defined as the relative number          Sagae, K., Davis, E., Lavie, A., MacWhinney, B., & Wintner, S.
of non-terminal leaves in multi-word constructions, increases           (2007). High-accuracy annotation and parsing of CHILDES
                                                                        transcripts. In Proc. ACL-2007 workshop on cognitive as-
with age. We show that it does so independently of sentence
                                                                        pects of computational language acquisition (p. 25-32).
length. Complex constructions lose their lexical parts to spe-
                                                                      Scha, R., Bod, R., & Sima’an, K. (1999). A memory-based
cialized lexical rewrite rules, and in the process the construc-
                                                                        model of syntactic analysis: data-oriented parsing. J. of exp.
ticon becomes more abstract.
                                                                        and theoretical artificial intelligence, 11, 409-440.
   This finding is in line with the theory of item-based learn-       Tomasello, M. (2000). The item-based nature of children’s
ing and clearly point to an incremental learning path, but we           early syntactic development. Trends in Cognitive Science,
believe it goes further than the state-of-the-art. By making            4(4), 156-163.
available the found constructions of the entire Brown corpus,         Tomasello, M. (2003). Constructing a language: A usage-
our version of the progressive abstraction hypothesis now be-           based theory of language acquisition. Cambridge, MA: Har-
comes falsifiable: using other approaches to identify the ele-          vard University Press.
mentary units of language, researchers can evaluate the qual-         Xia, F., & Palmer, M. (2001). Converting dependency structures
ity of the hypothesized constructions. Although we cannot               to phrase structures. In Proceedings of HLT 2001.
exclude the possibility that the performance-competence dis-          Zuidema, W. (2006). What are the productive units of natural
tinction might rescue the continuity hypothesis, the onus is            language grammar? A DOP approach to the automatic iden-
now on its defenders to demonstrate that either our hypothe-            tification of constructions. In Proc. CONLL-X, pp.29-36
sized constructions are incorrect, or that a generative perfor-       Zuidema, W. (2007). Parsimonious data-oriented parsing. In
mance theory makes identical predictions.                               Proc. EMNLP-CONLL 2007, pp. 551-560
                                                                  52

