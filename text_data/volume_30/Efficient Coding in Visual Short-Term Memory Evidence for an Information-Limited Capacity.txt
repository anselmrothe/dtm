UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Efficient Coding in Visual Short-Term Memory: Evidence for an Information-Limited Capacity
Permalink
https://escholarship.org/uc/item/7js8r5z9
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Brady, Timothy F.
Konkle, Talia
Alvarez, George A.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Efficient Coding in Visual Short-Term Memory:
                                  Evidence for an Information-Limited Capacity
                                               Timothy F. Brady (tfbrady@mit.edu)
                                                  Talia Konkle (tkonkle@mit.edu)
                                               George A. Alvarez (alvarez@mit.edu)
                                   Department of Brain and Cognitive Sciences, MIT, Cambridge, MA
                               Abstract                                instead depends only on the number of objects to be
                                                                       remembered – consistent with the idea of ‘chunks’ proposed
   Previous work on visual short-term memory (VSTM)                    by Miller (1956) and Cowan (2001).
   capacity has typically used patches of color or simple features        However, it has become clear recently that there is a
   which are drawn from a uniform distribution, and estimated
                                                                       serious cost in memory performance for increasing the
   the capacity of VSTM to be 3-4 items (Luck & Vogel, 1997).
   Here, we introduce covariance information between colors,           information content of an object (e.g., objects with multiple
   and ask if VSTM can take advantage of this redundancy to            colors that need to be stored; Wheeler & Treisman, 2002).
   form a more efficient representation of the displays. We find       This suggests that visual short-term memory (VSTM)
   that observers can successfully remember 5 colors on these          cannot hold an unlimited amount of information just
   displays, significantly higher than the 3 colors remembered         because it has been bound to a single object. Alvarez and
   when the displays were changed to be uniformly distributed          Cavanagh (2004) proposed an alternate framework that
   in the final block of the experiment. We suggest that               specifically takes into account the amount of information
   quantifying capacity in terms of number of objects                  needed to represent each object. They demonstrated that
   remembered fails to capture factors such as object complexity
                                                                       while observers can remember up to four simple objects,
   or statistical redundancy, and that information theoretic
   measures are better suited to characterizing the capacity of        they can remember only 1 or 2 complex objects --
   VSTM. We use Huffman coding to model our data, and                  presumably because a greater amount of information is
   demonstrate that the data are consistent with a fixed VSTM          required for the complex objects to be remembered well
   capacity in bits rather than in terms of number of objects.         enough to succeed at test. However, because of the nature of
                                                                       the real world objects used in their task, Alvarez and
   Keywords: Visual short-term memory; Working memory;
   Information theory; Memory capacity
                                                                       Cavanagh (2004) could not measure the true (information
                                                                       theoretic) information content of their stimuli.
                                                                          In the present study, we had observers remember color
                         Introduction
                                                                       patches because it is possible to exactly quantify the
   It is widely accepted that observers are highly sensitive to        information content of these stimuli in bits (Shannon, 1948).
statistical regularities in the world. This capacity has been          We varied the amount of information per stimulus not by
used to explain effects from speech segmentation to the                changing the physical appearance of the patches, but by
emergence of visual objects (Saffran, Aslin & Newport,                 changing the probability of their co-occurrence. Introducing
1996; Turk-Browne, Isola, Scholl, & Treat, in press). Such             statistical redundancy reduces the amount of information
regularities also provide an opportunity for memory systems            needed to encode the items in the display. This manipulation
to form more efficient representations by eliminating                  enabled us to directly compare VSTM models which
redundancies. This may be especially important for visual              propose a capacity limit in terms of a fixed number objects
short-term memory, which is known to have a severely                   versus a fixed amount of information (in bits).
limited capacity.                                                         First, we conduct two behavioral experiments, in which
   Previous work on VSTM capacity suggests that observers              we draw stimuli from a uniform distribution (Experiment 1)
can remember about four objects, independent of the                    or from a distribution containing covariance information
number of features remembered per object (Luck & Vogel,                between presented colors (Experiment 2). Next, using a
1997; Vogel, Woodman, & Luck, 2001). In one experiment,                hierarchical Bayesian model of the learning process and a
observers were shown lines of different colors and                     Huffman encoding scheme, we show that a computational
orientations. When required to remember either color or                model can predict VSTM performance.
orientation alone, they could remember 4 items.
Surprisingly, when required to remember both color and                            Experiment 1: Uniform Displays
orientation, observers could still remember 4 items. In fact,
performance was the same when observers had to remember                We first assessed the capacity of VSTM for colors drawn
up to four features per object. These data suggested that the          from a uniform distribution. This allowed us to get an
amount of information remembered per object is not a                   estimate of the number of bits of color information people
limiting factor in memory, and that memory capacity                    can remember under circumstances where no compression is
                                                                       possible.
                                                                   887

                                                                     Performance on this task can also be quantified in terms
                                                                   of the amount of information remembered. Because there
                                                                   are 8 colors to choose from, each color requires 3 bits of
                                                                   information to encode (23 = 8). This suggests that the
                                                                   capacity of VSTM for colors drawn from a uniform
                                                                   distribution is approximately 10.1 bits.
                                                                     These data demonstrate that we can quantify performance
                                                                   both in terms of the number of objects and in terms of the
Figure 1: A sample trial. Eight colors are presented and then      number of bits. In Experiment 2 we introduce covariance
disappear, and observers have to indicate what color was at        information and measure its impact on VSTM. We then
a given location.                                                  model the capacity of VSTM in both uniform displays and
                                                                   in displays with covariance information, to examine whether
Method                                                             memory has a fixed information limit in bits.
Eight naïve observers were recruited from the MIT
participant pool (age range 18-35) and received 10 dollars
for their participation. All gave informed consent.
   We presented observers with displays consisting of eight
colored circles, arranged in pairs around the fixation point
(see sample display in Figure 1). Observers were informed
that their task was to remember the locations of each of the
eight colors. At the start of a trial, the colors appeared and
remained visible for 1000ms. Then the colors disappeared,
with placeholder circles present for the next 1000ms (long
enough to prevent observers from relying on iconic
memory; Sperling, 1960), and then one of the placeholder
circles was darkened.
   Observers’ task was to indicate which of the eight colors
had been presented at the indicated location, by pressing one
of eight color-coded keys. Observers completed 600 trials,
presented in 10 blocks of 60 trials each. Afterward, they
completed a questionnaire about the strategies they                Figure 2: Results of Experiment 1. Error bars correspond to
employed and whether they noticed the presence of patterns         within-subject s.e.m. (Loftus & Masson, 1994).
in the displays.
   The stimuli were presented using MATLAB with the                         Experiment 2: Patterned Displays
Psychophysics toolbox extensions (Brainard, 1997; Pelli,
1997). The eight colors used were red, green, blue, magenta,       We next assessed the capacity of VSTM under conditions
cyan, yellow, black and white. The locations of the colors in      where statistical redundancy was present in the displays.
each trial were chosen randomly, with only the constraint          Specifically, we introduced covariance information between
that no color could appear more than once in a given trial.        colors. For example, red most often appeared next to green,
                                                                   blue most often appeared next to yellow, etc. If observers
                            Results                                are able to learn these pairs, then over time they should be
                                                                   able to remember more items from the display. However, if
We estimated the number of objects observers could                 memory is limited by the number of objects, then
successfully hold in memory using the following formula            performance should be stable over time.
for capacity given an eight-alternative forced choice:
                                                                   Method
          K = (PC - 1/8) * (1 + 1/8) * 8
                                                                   Eight new observers were recruited. Methods were the same
                                                                   as Experiment 1, except that stimuli for each trial were not
   The reasoning behind this formula is that by correcting
                                                                   chosen randomly. First, for each subject a joint probability
for chance (1/8) we can examine exactly how many objects
                                                                   matrix was constructed to indicate how likely each color
from each display observers would have had to remember in
                                                                   was to appear to the left or right of each other color. This
order to achieve a given percent correct (PC). Observers in
                                                                   matrix was made by choosing four high probability pairs at
the experiment remembered 3.4 colors on average
                                                                   random (probability = 0.2151), and then assigning the rest
throughout the experiment (see Figure 2), entirely consistent
                                                                   of the probability mass uniformly (probability = 0.0027). As
with previous results on the capacity of VSTM for colors
                                                                   in Experiment 1, all eight colors were present in each
(Luck & Vogel, 1997; Alvarez & Cavanagh, 2004).
                                                                   display. In order to achieve this, the diagonal was set to zero
                                                               888

in order to prevent the same color from appearing twice in          might have a fixed capacity in terms of bits, rather than
the same display.                                                   number of items. Using the data from both Experiment 1
   The pairs were constrained so that each color was                and 2 allowed us to asses whether, across all blocks of both
assigned to exactly one high probability pair. For example,         experiments, observers are remembering the same amount
if (Blue, Red) was a high probability pair in this joint            of information, even though they are successfully encoding
probability matrix, the observer would often see blue and           more colors at some points than others.
red appear together, with blue on the left and red on the
right. High probability pairs accounted for approximately
80% of the mass of the probability distribution, and
consequently about 80% of the pairs displayed during the
experiment.
   In the final block of the experiment, the distribution the
displays were drawn from was changed to a uniform
distribution. This eliminated the regularities in the display,
and allowed us to assess whether observers had used the
regularities present in the displays to improve their
performance.
Results
   We found that observers in the patterned condition could
successfully remember K = 5.1 colors after learning the
regularities in the displays (block 9), significantly higher
than the K = 3.1 colors they were able to remember when             Figure 3: Results of Experiment 2. Block 10 is where the
the displays were changed to be uniformly distributed in            distribution was changed to be uniform. Error bars
block 10 (See Figure 3; t(7) = 8.30, p = 0.00007).                  correspond to within-subject s.e.m.
   This suggests that, if we consider working memory
capacity in terms of the number of items remembered,                                         Modeling
observers were able to use the regularities in the displays to      The purpose of the modeling is to test the hypothesis that
increase their capacity past what has been assumed to be a          there is a fixed bit limit on short-term memory. First, we
fixed limit of approximately four simple objects. However,          model the learning of the color regularities based on the
one concern is that observers might simply have                     number of times they saw each pair of colors. Second, we
remembered one color from each pair and then inferred               assess how these learned statistics translate into
what the other colors were after the display was gone. This         representations in bits, using Huffman coding (Huffman,
would suggest that observers were actually only                     1952).
remembering four objects.                                             To model the learning of the color pairs, we assumed that
   In order to eliminate the possibility that this explicit         observers treated the stimuli as though they were generated
inference was the only reason for our effect, we separated          from rolls of a 64 sided die, with one side for each possible
out trials where the tested item was from a high probability        color pair (Red-Blue, White-Black, Black-White, etc.) We
pair from those where the tested item was from a low                then modeled how they would learn what weight was
probability pair. In other words, if blue often appeared with       associated with each side of the die. We did this by counting
red, we considered only the trials where blue appeared with         the frequency with which each pair appeared, plus a prior
another color. On these trials, an explicit inference process       probability on the die being uniform, using a hierarchical
would cause observers to report the wrong color. When we            Bayesian model.
examine only these trials, we still find that capacity in block       Once we had modeled how observers would learn the
9 is significantly higher than in block 10 (4.9 colors in block     probability distribution, we next assessed how this
9 and 3.1 colors in block 10; t(7)=3.63, p =0.008).                 probability distribution would help them encode the stimuli
   We suggest that observers learned to encode the high             more efficiently. We used a compression algorithm,
probability pairs as a single unit using a more efficient           Huffman coding, to see how effectively observers should
representation. This allows them to both hold more high             have been able to represent stimuli that were either drawn
probability items and more low probability items in memory          from the probability distribution they had learned (blocks 1-
than when the displays were uniformly distributed.                  9) or drawn from a different distribution (block 10).
   While this is consistent with ideas about chunking
(Miller, 1956) as an alternative, we suggest that an                Learning the color pairs
information metric might be more useful than the number of
                                                                      We used a Dirichlet-multinomial model to infer the
items retained. We therefore performed an information
                                                                    probability distribution that the stimuli were being drawn
theoretic analysis of the data to examine whether observers         from, given the color pairs that had been observed. We let d
                                                                889

equal the observations of color pairs. Thus, if the trial                Importantly, however, this naïve method of generating a
represented in Figure 1 is the first trial of the experiment,         code performs quite badly in the case where some letters are
after this trial d = {Yellow-Green, Black-White, Blue-Red,            much more likely to appear than others. So, for example, if
Magenta-Cyan}. We assume that d is sampled from a                     P(A) = 0.5, and P(B) = 0.2, P(C) = 0.2, and P(D) = 0.1, then
multinomial distribution with parameter ө. In other words,            we can achieve a great deal of compression by representing
we assume that at any point in the experiment, the set of             strings from this language using a different code: A = 0, B =
stimuli we have seen so far is a result of repeated rolls of a        10, C = 110, D = 111. Using this code, the string from
weighted 64 sided die (one for each cell in the joint                 above, ACAABAA, would be represented as 0110001000
probability matrix; i.e., one for each color pair), where the         (10 bits), a significant savings even for such a short string
chance of landing on the ith side of the 64 sided die is given        (29%). Note that it can still be uniquely decoded, because
by өi. Note that this is a simplification, since the experiment       no code is a prefix of any other code.
included the additional constraint that no color could appear            Huffman coding (Huffman, 1952) is a way of going from
multiple times in the same display. However, this constraint          the probabilities of a set of symbols to a binary code for
does not have a major effect on the expected distribution of          representing those symbols in a compressed format (the
stimuli once a large number of samples has been obtained,             example codes for A, B, C, D were generated using a
and was thus ignored in our formalization.                            Huffman coding algorithm). Here, we used Huffman coding
   We set our a priori expectations about ө using a Dirichlet         to estimate how much savings observers should show as a
distribution with parameter α. The larger α is, the more              result of the fact that the color pairs in our experiment were
strongly the model starts off assuming that the true                  drawn from a non-uniform distribution.
distribution of the stimuli is a uniform distribution (e.g., that        We used the probabilities of each color pair, as assessed
the multinomial distribution is using a ‘die’ that is weighted        by the hierarchical Bayesian model described above, to
to land on each possible cell equally). Using statistical             generate a unique bit string encoding all of the stimuli from
notation, the model can be written as:                                a given block of the experiment. We supposed that if
                                                                      observers were using some form of compression to take
          ө ~ Dirichlet(α)                                            advantage of the redundancies in the display, the length of
          d ~ Multinomial(ө)                                          the code that our compression algorithm generates should be
                                                                      inversely proportional to how many objects observers were
   To fit the model to data we set a fixed α and assume that          able to successfully encode. In other words, if there were
the counts of the pairs that were shown, d, are observed for          many low frequency color pairs presented (as in block 10),
some time period of the experiment. Our goal is then to               these items should have longer codes, and observers should
compute the posterior distribution p(ө | d, α). The maximum           be able to successfully remember fewer of them.
of this posterior distribution is then our best guess at the true     Alternatively, if there are many high frequency color pairs
probability distribution that the stimuli are being drawn             presented, the better observers’ estimate of the true
from, and the variance in the posterior indicates how certain         probability distribution, the better they should be able to
we are about our estimate. The posterior of this model                compress the input.
reduces to a Dirichlet posterior where the weight for each
color pair is equal to the frequency with which that color            Results
pair appears in d, plus the prior on that pair, αi..                  With these learning and coding models, we can compute a
   This probability model allowed us to infer what observers          prediction about the memory performance for each subject
might believe about the probability of each color pair from           for each block. In order to assess the fit between the model
the data they had observed. The benefits of a Bayesian                and the behavioral data, we used the following procedure.
model over just counting the frequency with which each                For each display in a block, we calculate the number of bits
pair had appeared are mostly evident early on in the                  required to encode that display based on the probabilities
experiment, where observer’s prior beliefs come into play             from the learning model. Next, we correlated the average
most strongly.                                                        number of bits per display from the model with the memory
                                                                      performance of the observers. We expect that the fewer
Huffman coding                                                        bits/display needed, the better the memory performance,
Any finite set of options can be uniquely encoded into a              thus we expect a negative correlation.
string of bits. For example, if we wished to encode strings              This prediction holds quite well, with the maximum fit
consisting of the four letters A, B, C, and D into strings of         between this Huffman code model and the human data at α
bits, we could do so by assigning a unique two bit code to            = 100, where r, the correlation coefficient between the
each letter and then concatenating the codes. Imagine we              human and model data, is -0.94 (p = 0.0003). This large
had assigned the following codes to the letters: A = 00, B =          negative correlation means that when the model says there
01, C = 10, D = 11. The string ACAABAA could then be                  should be long bit strings necessary to encode the stimuli,
written as 00100000010000 (14 bits), and uniquely decoded             human VSTM capacity is low, exactly what you would
to retrieve the original string.                                      expect if VSTM had a fixed size in bits and took advantage
                                                                  890

of a compression scheme to eliminate redundant                                                Discussion
information.
   Importantly, this model allows us to examine if there is a       The current study explored whether the capacity of visual
fixed-bit limit on memory capacity. The Huffman codes               short-term memory is better characterized in terms of the
gives a measure of average bits per object, and the memory          number of objects or the amount of information that can be
performance gives a measure in number of objects                    remembered. We investigated this issue by using
remembered. Thus, if we multiply the average size of the            information theoretic ideas about compression. Specifically,
Huffman code times the number of items remembered, we               we introduced covariance information between colors, and
get an estimate of the number of bits of information a given        asked if VSTM could take advantage of this redundancy to
set of observers recalled in a given block (Figure 4). Notice       form more efficient representations of the displays. We
first that both groups of observers in Experiment 1 and             found that observers could successfully remember 5 colors
Experiment 2 show the same total bits, despite the overall          on the patterned displays, significantly higher than the 3
difference in the number of items remembered between the            colors remembered when the displays were drawn from a
groups. Second, the total bit estimate remains remarkably           uniform distribution
constant between block 9 and block 10 in the Experiment 2              These data suggest several conclusions. First, they
group, even though the memory performance measured in               suggest that VSTM is capable of representing more than
number of items showed a significant cost when the                  four simple objects. In cases where there is statistical
statistical regularities were removed. The fact that this           redundancy, it is not necessary to encode as much
estimate is constant across the entire experiment, whereas          information to represent the objects, and VSTM can
the estimate in terms of number of objects varies a great           represent at least five, and probably more, objects. Together
deal, suggests that bits are the appropriate way to quantify        with experiments showing that increasing the amount of
human VSTM capacity.                                                information stored per object decreases the total number of
                                                                    objects that can be remembered (Alvarez & Cavanagh,
                                                                    2004), this suggests that it is the information content, not the
                                                                    number of objects, which is important for understanding
                                                                    VSTM capacity. Measures based on the number of objects
                                                                    fail to capture object complexity, statistical redundancy, and
                                                                    the difficulty of the test comparison, all of which affect the
                                                                    information load for storing objects.
                                                                       Second, our results suggest that VSTM capacity can be
                                                                    simply modeled by positing a fixed capacity in bits. Across
                                                                    both Experiment 1 and Experiment 2, changes in the
                                                                    estimated capacity of observers are explained by
                                                                    corresponding increases or decreases in the length of the
                                                                    code necessary to represent pairs, resulting in a nearly
                                                                    constant estimate of the total size of memory in bits (about
                                                                    10, see Figure 4). Thus, under conditions where the
                                                                    compression algorithm suggests people should be able to
                                                                    compress the color pairs into fewer bits, they were able to
Figure 4: The size of memory estimated in bits, rather than         remember more objects; under conditions where the model
number of objects (using the Huffman coding model). Error           achieved little or no compression, they were able to
bars represent 1 s.e.m.                                             remember fewer objects. Both the Huffman coding
                                                                    algorithm and humans achieved similar levels of
   Importantly, the fit between the human and the model is          compression, with the Huffman code length being 36%
reasonably good across a broad range of values for the prior,       shorter in block 9 than 10, and human VSTM capacity being
averaging an r value of -0.81 (std: 0.08) in the range α = 1 to     35% greater in block 9 than 10. This suggests that VSTM is
200. The fit is poor where the prior is very low, since with        quite good at eliminating the redundant information present
no prior there is no learning curve – the model immediately         in the input.
decides that whatever stimuli it has seen are completely               The fact that an information limit is applicable in VSTM
representative of the distribution (e.g., like a non-Bayesian       is in line with previous work on the capacity of visual
model would do). The fit is also poor where the prior is very       attention. For example, Verghese and Pelli (1992) suggested
high, because it never learns anything about the distribution       that attention could be accurately modeled at limited by the
of the stimuli, instead generating codes the entire time as         information content of stimuli in bits. This combines well
though the distribution was uniform. However, across much           with the present results to reinforce the tight coupling
of the middle range, the model provides a reasonable                between the capacity of attention and the capacity of
approximation to human performance.                                 memory (e.g., Cowan, 2001).
                                                                       More broadly, our results suggest that people are able to
                                                                    successfully use statistical regularities present in the world
                                                                891

to store more objects in VSTM. This is of particular interest       Cowan, N. (2001). The magical number 4 in short-term
given the limited capacity of VSTM. Since the objects and             memory: A reconsideration of mental storage capacity.
events we use VSTM for in the real world are unlikely to be           Behavioral and Brain Sciences, 24, 87-185.
statistically independent, the number of items we can               Hauser, M. D., Newport, E. L., & Aslin, R. N. (2001).
remember information about may be much larger than is                 Segmentation of the speech stream in a non-human
usually assumed. Importantly, however, our results differ             primate: Statistical learning in cotton-top tamarins.
drastically from typical results of ‘chunking’ (Miller, 1956)         Cognition, 78(3), B53–B64.
– chunking is usually taken to allow you to store more              Huffman, D.A. (1952). A Method for Construction of
information in memory ‘for free’, and in fact Miller                  Minimum-Redundancy Codes. Proceedings of the I.R.E.,
originally proposed chunking as an alternative to                     40(9), 1098-1101.
information theoretic models. We instead propose that a             Kirkham, N. Z., Slemmer, J. A., Johnson, S. P. (2002).
more efficient representation of some groups of items may             Visual statistical learning in infancy: evidence for a
be obtained at the expense of less efficient representation of        domain general learning mechanism. Cognition, 83, 35-
other, less frequent, items. This suggests that the capacity of       42.
memory in bits is constant, and it is how we allocate those         Loftus, G. R. & Masson, M. E. J. (1994). Using confidence
bits that changes as a result of learning regularities.               intervals in within-subject designs. Psychonomic Bulletin
   These results also suggest a reason why we might be                & Review, 1, 476-490.
incredibly sensitive to statistical regularities in the visual      Luck, S.J., & Vogel, E.K. (1997). The capacity of visual
world. In particular, a great deal of recent work has focused         working memory for features and conjunctions. Nature,
on statistical learning mechanisms, which are capable of              390, 279–281.
extracting many different regularities with only minutes of         Miller, G. A. (1956). The Magical Number Seven, Plus or
exposure and appear to be relatively ubiquitous, occurring            Minus Two: Some Limits on our Capacity for Processing
in the auditory, tactile and visual domains, and in infants,          Information. Psychological Review, 63, 81-97.
adults, and monkeys (Brady & Oliva, in press; Conway &              Pelli, D. G. (1997). The VideoToolbox software for visual
Christiansen, 2005; Kirkham, Slemmer & Johnson, 2002;                 psychophysics: Transforming numbers into movies.
Hauser, Newport & Aslin, 2001; Saffran, et al. 1996; Turk-            Spatial Vision, 10, 437-442.
Browne, Junge & Scholl, 2005). The present results                  Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
suggests that one of the primary reasons for being sensitive          Statistical learning by 8-month-old infants. Science, 274,
to such regularities might be that it allows us to remember           1926–1928.
more in working memory by eliminating redundancy in our             Shannon, C.E. (1948). A mathematical theory of
representations.                                                      communication. Bell System Technical Journal, 27, 379–
                                                                      423.
                   Acknowledgments                                  Sperling, G. (1960). The information available in brief
                                                                      visual presentations. Psychological Monographs, 74, 1-
This research is supported by an NIH/NEI fellowship                   29.
#F32EY016982 to GAA, and an NDSEG fellowship                        Turk-Browne, N. B., Jungé, J., & Scholl, B. J. (2005). The
awarded to T.K.                                                       automaticity of visual statistical learning. Journal of
                                                                      Experimental Psychology: General, 134, 552-564.
                         References                                 Turk-Browne, N. B., Isola, P. J., Scholl, B. J., & Treat, T.
Alvarez, G. A., & Cavanagh, P. (2004). The capacity of                A. (in press). Multidimensional visual statistical learning.
   visual short-term memory is set both by visual                     Journal of Experimental Psychology: Learning, Memory,
   information load and by number of objects. Psychological           and Cognition.
   Science, 15, 106-111.                                            Verghese, P. & Pelli, D. G. (1992). The information
Brady, T. F. & Oliva, A. (in press). Statistical learning using       capacity of visual attention. Vision Research, 32 (5), 983-
   real-world scenes: extracting categorical regularities             995.
   without conscious intent. Psychological Science.                 Vogel, E.K., Woodman, G.F., & Luck, S.J. (2001). Storage
Brainard, D. H. (1997). The Psychophysics Toolbox. Spatial            of features, conjunctions, and objects in visual working
   Vision, 10, 433-436.                                               memory. Journal of Experimental Psychology: Human
Conway, C. M., & Christiansen, M. H. (2005). Modality-                Perception and Performance, 27, 92–114.
   constrained statistical learning of tactile, visual, and         Wheeler, M.E., & Treisman, A.M. (2002). Binding in short-
   auditory sequences. Journal of Experimental Psychology.            term visual memory. Journal of Experimental
   Learning, Memory, and Cognition, 31(1), 24-39.                     Psychology: General, 131, 48–64.
                                                                892

