UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Trade-off Between Capacity and Generalization in a Model of Memory
Permalink
https://escholarship.org/uc/item/4961f23d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Tenenbaum, Guy
Yeshurun, Yehezkel
Edelman, Shimon
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Trade-off Between Capacity and Generalization in a Model of Memory
Guy Tannenbaum (guy tannenbaum@yahoo.com) and Yehezkel Yeshurun (hezy@post.tau.ac.il)
                                       School of Computer Science, Tel Aviv University
                                              Ramat Aviv, Tel Aviv 69978, Israel
                                         Shimon Edelman (se37@cornell.edu)
                                         Department of Psychology, Cornell University
                                                    Ithaca, NY 14853, USA
                           Abstract                               understanding of memory, such as that of Minsky (1985),
                                                                  have typically been only partially implemented (Hearn,
    Although computational considerations suggest that a
    resource-limited memory system may have to trade off          2001). An explicit computational model designed to pro-
    capacity for generalization ability, such a trade-off has     vide both storage and generalization has been recently
    not been demonstrated in the past. We describe a sim-         developed by Mueller and Shiffrin, but its reported eval-
    ple model of memory that exhibits this trade-off and
    describe its performance in a variety of tasks.               uation (Mueller, 2006; Mueller & Shiffrin, 2006) seems
    Keywords: memory; computational model; capacity;              to be qualitative rather than quantitative.
    generalization; trade-off.                                       Our goal in the present study has been to investigate
                                                                  the emergence of a trade-off between capacity and gen-
                       Introduction                               eralization under conditions that are as general as pos-
 Because the probability of a cognitive agent encounter-          sible. To that end, we chose to focus on implementing
 ing precisely the same stimulus twice is infinitesimally         a functional (rather than neuromorphic) computational
 small, memories of past experiences are only useful for          model of pattern storage and generalization, which ex-
 guiding future behavior insofar as they can be general-          tends that of Moll and Miikkulainen (1997). This allows
 ized so as to apply to new variations on familiar themes.        us to relate our results to existing methods and findings
 Intuitively, it would seem that in a memory system bet-          regarding memory capacity.
 ter generalization would have to come at the expense of
 reduced capacity. Indeed, the famous mnemonist patient
                                                                          The computational framework
 studied by Luria (1968), whose memory capacity seemed            Our model operates on vectors of positive integers. Its
 practically unlimited, was oblivious even to simple pat-         building blocks are “neurons” with real-valued activa-
 terns in the memorized items.                                    tion, connected via real-valued synaptic weights. Perfor-
    From the functional standpoint, this intuition can be         mance is measured as a function of the resources avail-
 related to the distinction commonly made between “sim-           able to the system, and of how they are allocated be-
 ple” or episodic memory (pertaining to statistically rare        tween the different aspects of each task.
 but important events, such as who did what to whom),
 where capacity is the key goal, and conceptual memory            Capacity
 (pertaining to statistically redundant patterns of events        We measure memory capacity by assessing the model’s
 in the environment), where generalization between “sim-          ability to recognize previously encountered patterns.
 ilar” items is crucial (Merker, 2004). Because these             Given a query pattern, the model returns the probability
 memory functions, along with the many others in the              of having encountered it before. The results are plotted
 human brain (Rolls, 2000), are subject to certain con-           in the form of a receiver operating characteristic (ROC).
 straints on the available resources, one expects memory          Capacity is then defined as the number of patterns that
 systems to exhibit a trade-off between capacity and gen-         the model can memorize while maintaining a given area
 eralization.                                                     under the ROC curve. This metric has a natural psy-
    Surprisingly little computational work has been de-           chological interpretation: human memory is often faced
 voted to testing this prediction. Does the expected              with the task of deciding whether or not a pattern has
 trade-off arise generically, in any resource-limited mem-        been seen before (Koriat, Goldsmith, & Pansky, 2000).
 ory system? Despite important early insights into the            Alternatively, capacity can be defined in terms of com-
 computational underpinnings both of simple and of con-           pletion of partial patterns (Moll & Miikkulainen, 1997).
 ceptual memory (Brindley, 1969; Marr, 1969, 1970), sub-
 sequent “connectionist” memory models, such as those             Generalization
 of Hopfield (1982) or Kanerva (1988), did not consider           Capturing an artificial hierarchical taxonomy. A
 this question, in part because they had not been intended        basic task that involves generalization is similarity esti-
 to provide generalization capabilities. At the same time,        mation: the memory representation of some structured
 the more comprehensive theoretical frameworks for the            collection of objects, such as items drawn from a taxo-
                                                              391

nomic tree, should capture the various relative similari-
ties of these objects. We presented the model with pat-
terns representing items taken from a three-level strictly
hierarchical taxonomy (objects within sub-classes within
classes) and compared the pairwise similarities between
their memory traces to the true similarities, defined by
the shortest paths between the corresponding nodes in
the tree (Tenenbaum, Griffiths, & Kemp, 2006).
Capturing a lexical taxonomy. To obtain another
perspective on its generalization ability, we used the
model to derive a hierarchical clustering of lexical items
from a natural language text corpus (Finch & Chater,
1991). This procedure begins by enumerating the unique          Figure 1: The architecture of the memory model. The
words in the corpus. The model is then presented with           feature maps layer, the binding layer, and the connec-
patterns formed by sliding a window along the text, each        tions between them encode information regarding which
pattern consisting of the list of word tags in the window.      unique patterns have been encountered by the model.
Finally, model’s representations of words are clustered         The classificatory units layer and its connections to the
according to their pairwise similarities (for this, it must     binding layer and global output unit encode the number
be possible to assess similarity between partially speci-       of times a particular pattern has been encountered. The
fied inputs).                                                   model in this illustration has encountered two instances
                                                                of the pattern {1, 1, 2} and one instance of {3, 3, 2}.
                      The model
                                                                Handling input statistics
The convergence-zone episodic memory of Moll and Mi-            Our version of the model adds three new functions: han-
ikkulainen (1997), of which the present model is an ex-         dling repeating input patterns, answering old/new recog-
tension, consists of two layers of real-valued units (the       nition queries, and maintaining and reporting the statis-
feature map layer and the binding layer) and bidirec-           tics of the encountered patterns. In particular, when
tional binary connections between the layers. Initially         queried with a full pattern, our model simply returns
all connections between the binding layer and feature           the frequency with which this pattern has been encoun-
map layer are inactive and have the value of zero. A            tered; when queried with a partial pattern, the model
pattern is stored in the memory in three steps: (i) those       returns the marginal frequency with which such partial
units that represent the appropriate feature values of the      pattern had occurred in the input (e.g., the response to
pattern are activated; (ii) a subset of m binding units are     {3, 4, ∗, 7, ∗} is the frequency of encountering a pattern
randomly selected in the binding layer to to encode this        in which the first, second, and fourth feature maps have
pattern; (iii) the weights of all the connections between       the values of 3, 4 and 7, and the two remaining ones
the active units in the feature maps and the active units       are “don’t cares”). The ability to handle queries about
in the binding layer are set to 1.                              the statistics of encountered patterns can be useful, for
                                                                example, when dealing with patterns representing loca-
   To complete a partial pattern, the corresponding fea-        tion, food availability, and the year’s season: the present
ture maps units are activated. The activation propa-            model can directly support decision making about op-
gates to the binding layer through all connections that         timal foraging strategies. In addition, handling input
have been turned on so far. At this stage units in the          statistics can help the model achieve generalization, as
binding layer can have different activity levels. The ac-       explained shortly.
tivity level of all the units connected to all the active
feature units is the number of the active feature units.        The architecture of the model
Other binding layer units are connected only to a sub-          Compared to that of Moll and Miikkulainen (1997), the
set of the active feature units, and will therefore have a      present model has two new layers (see Figure 1). The
lower activity level. Only those binding layer units with       first is the classificatory layer, each of whose units repre-
the maximal activity level are retained, and the others         sents a different pattern; its connections to the binding
are turned off. The activation of the remaining binding         layer are binary. A classificatory unit becomes active
units is then propagated back to the feature maps. A            only when all the binding layer nodes it is connected
number of units are activated at various levels in each         to are activated. The second new layer has a single
feature map, and again, only the most active unit in each       global output unit, whose connections to the classifica-
feature map is retained, resulting in a complete unam-          tory units are real-valued. Its output is set to the sum
biguous pattern.                                                of all the strengths of the connections to the currently
                                                            392

active classificatory units, normalized by the sum of all
the strengths of the connections to all the classificatory
units. Initially, all the connections between the binding
and classificatory units and between the classificatory
units and the global output unit are inactive.
Storing patterns
When a pattern is presented to the model to be stored,
the activity propagates from the feature map layer on-
wards. In the binding layer, only those units with maxi-
mal activity remain active. The activity than propagates
to the classificatory layer, where a unit is activated only
if all the binding units it is connected to are. Finally,
the activity reaches the global output unit which sums
the strengths of all its incoming connections.
   If the output unit is not activated at this stage, the
pattern is considered novel. A random set of m binding
units and one new classificatory unit are allocated for
representing it. The global output unit is connected to
the new classificatory unit with an initial strength of 1
(Figure 2).
   If, on the contrary, the global output unit is activated
by the initial feedforward sweep, the pattern is consid-        Figure 2: (a) The model, which has stored the pattern
ered familiar. The strength of the connections between          {1, 1, 2}, is presented with the pattern {3, 3, 2}. (b) Ac-
the global output unit and any active classificatory units      tivity propagates to the binding layer. (c) Because none
is increased by 1. Unless the model is overloaded, there        of the nodes have activity level of three (the number of
is only one such active classificatory unit: the one al-        specified values in the input), none of them remain ac-
located when the present pattern was first encountered          tive, and the classificatory nodes and global output node
(Figure 3).                                                     remain inactive. (d) Three random binding nodes and
                                                                a new classificatory node are recruited for representing
Answering queries                                               this pattern.
When the query is a complete pattern, and if an identi-
cal pattern has been stored, a unique classificatory unit       active binding nodes, m is the number of binding nodes
will be activated and the activity of the global output         recruited when storing a new pattern, and bt is the to-
unit will be proportional to the frequency of this pattern      tal number of binding nodes. The rationale behind this
(out of all the patterns presented). If no such pattern         response is that the more binding nodes are currently
has been stored, no classificatory unit will be activated,      active, the more likely it is that the m binding nodes rep-
and the output strength will be 0 (unless the model is          resenting some random pattern will be activated. As the
overloaded, which may lead to errors).                          load on the model increases, so does the number of bind-
   When the query is a partial pattern, the binding nodes       ing layer to feature layer connections, resulting in lower
that become active are the ones that participate in rep-        certainty when answering this type of query. This calcu-
resenting patterns in which the specified features have         lation can also be used for estimating when the model is
the given values. Similarly, among the classificatory           about to become overloaded and should not be used for
units, which respond only when all the relevant bind-           storing more patterns.
ing nodes are, only the nodes representing patterns in
which the relevant features have the given values are           Resources
activated. Summing the connections strengths of these           The present model implements more functions, but also
units to the global output unit has the effect of calcu-        uses more resources (nodes and connections), than the
lating the marginal distribution of the specified feature       original convergence zone model (it needs one classifica-
values.                                                         tory node per unique pattern, making the capacity less
   Old/new queries are processed by first allowing the          than the total number of nodes). It may be instruc-
activity to propagate to the global output node. If it re-      tive to compare the model to one in which there is no
mains inactive, the model responds with 0 (meaning that         binding layer and the feature maps are connected di-
the pattern has definitely not been seen before). If the        rectly to the classificatory units. Such a model would
global output node is active, the response to the query         use fewer nodes, and would not suffer from the errors
is output = 1 − ((ba − m)/bt ), where ba is the number of       generated due to the probabilistic process of recruiting
                                                            393

                                                                      of the following partial patterns would then be queried:
                                                                      {4, 5, ∗, ∗, ∗}, {4, ∗, ∗, ∗, ∗}, {∗, 5, ∗, ∗, ∗}.)
                                                                         A straightforward neuronal implementation of this al-
                                                                      gorithm can be based on maintaining multiple copies of
                                                                      the model which all learn the same patterns. Each of
                                                                      these can be hardwired to output one of the required
                                                                      co-occurrence statistics when answering a pattern com-
                                                                      parison query. As we show later, not all the possible co-
                                                                      occurrence statistics are required to achieve reasonable
                                                                      performance. Using a randomly selected subset of the
                                                                      co-occurrence statistics is likely to provide good perfor-
                                                                      mance, as long as the subset is large enough.1 This ap-
                                                                      proach leads to a trade-off between the number of copies
                                                                      of the model (which limits the number of co-occurrence
                                                                      statistics used for similarity judgments) and the amount
                                                                      of resources per copy (which limits the capacity). If using
                                                                      more the statistical information results in better gener-
                                                                      alization (which happens to be the case, up to a point;
                                                                      see Figure 6), then this trade-off leads in turn to the
                                                                      generalization vs. capacity dilemma.2
                                                                                                                 4
                                                                                                              x 10
                                                                                                         14
Figure 3: (a) The model, which has previously stored the                                                 12
patterns {1, 1, 2} and {3, 3, 2}, is presented with {1, 1, 2}
                                                                             Number of unique patterns
for the second time. (b) Activation propagates to the                                                    10
binding layer. (c) Only those binding layer nodes with
                                                                                                         8
an activity level of three remain active. (d) Activation
propagates to the classificatory layer and the global out-                                               6
put unit and the strength of the connection between the
                                                                                                         4
active classificatory node and the global output node is
increased.                                                                                               2
                                                                                                                                                       Area under ROC: 0.99
                                                                                                                                                       Area under ROC: 0.95
                                                                                                                                                       Area under ROC: 0.9
binding nodes. However, when considering the number                                                      0
                                                                                                          0          0.2    0.4    0.6   0.8   1     1.2   1.4
                                                                                                                                  Number of binding layer nodes
                                                                                                                                                                  1.6   1.8
                                                                                                                                                                                 4
                                                                                                                                                                                     2
                                                                                                                                                                              x 10
of connections required by these models, its disadvantage
becomes clear. In our version of the model, the number
                                                                      Figure 4: Memory capacity vs. the number of binding
of connections is c1 = f × b + b × c + c = b × (f + c) + c,
                                                                      nodes (10 feature maps of size 50, averaged over 3 runs).
where f is the number of feature layer nodes, b the num-
                                                                      Capacity is defined as the maximum number of patterns
ber of binding nodes, and c the number of classificatory
                                                                      that can be stored while still maintaining a given area
nodes. In comparison, in the model without the binding
                                                                      under the ROC curve.
layer, the number of connections would be c2 = f × c + c.
Therefore, when the number of binding nodes is smaller
than half of both the number of feature nodes and the                                                                      Simulation results
number of classificatory nodes (a reasonable constraint),
                                                                      Basic performance characteristics. The model’s ca-
the total number of connections in our version of the
                                                                      pacity and performance under increasing load are plotted
model is smaller. In effect, manipulating the size of the
                                                                      in Figures 4 and 5. The model is assessed on two differ-
binding layer allows controlling the number of connec-
                                                                      ent tasks: differentiating between old and new patterns
tions needed by the model, at the expense of tolerating
                                                                      and recalling the pattern frequencies. Up to a certain
more errors.
                                                                          1
Comparing patterns                                                          In this case line 7 of algorithm 1 needs to be changed
                                                                      to only traverse some subset of all the possible feature map
Algorithm 1 (see next page) is used for calculating the               values.
                                                                          2
                                                                            One of the many issues not addressed in this work is
similarity between two patterns. The co-occurrence                    the effect of projecting the pattern space into a feature space
statistics needed for calculating the surprise factor in              which can support better similarity judgements between pairs
line 9 of the algorithm are estimated by querying the                 of patterns. The trade-off identified above is expected to arise
                                                                      regardless of the feature selection and similarity judgment
model with the corresponding partial patterns (i.e., let              generation methods used, as long as increasing the amount
n = 5, i = 1, k = 2, vij = 4, vk = 5; the frequencies                 of resources at their disposal leads to better performance.
                                                                394

Algorithm 1 Calculating the similarity between a pair of patterns
 1: Input: a pair of patterns p1 = {v11 , v21 , ...vn
                                                    1
                                                      } and p2 = {v12 , v22 , ...vn2 }
 2: Output: the similarity between the pair of patterns s
 3: for i = 1 : n do {n is the number of feature maps}
 4:   for j = 1 : 2 do
 5:      for k = 1 : n do
 6:        if k 6= i then
 7:           for all feature map values m = 1 : M do {M is the number of feature map values}
 8:              Calculate the surprise factor f of encountering a pattern in which vi = vij and vk = m.
                                                             P rob(vi =vij ∧vk =m)
 9:                                                 f←   P rob(vi =vij )×P rob(vk =m)
10:          end for
11:        end if
12:      end for
13:      Concatenate the results of these computations to form a vector dj with the length of M × (n − 1)
14:   end for
15:   ci ← correlation between d1 and d2 .
16: end for
17: s ← mean(ci ) {The correlation scores could instead be weighted by variance, as in the Mahalanobis metric.}
                                                                                                                                                                1
load, performance in both tasks remains good and the
error is almost constant. After this point, performance                                                                                                       0.95
                                                                                                                             Generalization score
degrades rapidly. Figure 6 shows generalization perfor-
                                                                                                                                                               0.9
mance as a function of the resources allocated to this
task.                                                                                                                                                         0.85
                                            1
                                                                               Area under ROC
                                                                                                                           (mean Spearman rank correlation)
                                                                                                                                                               0.8
                                                                               1 − mean frequency error
                                        0.9
         Area under ROC curve
                                                                                                                                                              0.75
                                        0.8
                                                                                                                                                               0.7
                                                                                                                                                                                      Similarities between learned patterns
                                                                                                                                                                                      Similarities between learned and novel patterns
                                                                                                                                                                                      Similarities between novel patterns
                                        0.7                                                                                                                   0.65
                                                                                                                                                                     0   100   200    300    400     500     600     700     800        900
       1 − Frequency representation error
                                                                                                                                                                           Number of co−occurrence statistics used for
                                                                                                                                                                          measuring similarity between feature map values
                                        0.6
                                                                                                                    Figure 6: Generalization vs. the size of the random sub-
                                        0.5
                                                                                                                    set of the co-occurrence statistics used for calculating
                                        0.4
                                                                                                                    pattern similarity (4 feature maps of size 400; 100 bind-
                                                0          5000                 10000                 15000
                                                           Number of unique patterns                                ing nodes; averaged over 10 runs). (), Spearman rank
                                                                                                                    correlation between true tree distances and the similar-
Figure 5: (◦), performance (the area under the ROC                                                                  ities between stored patters. (×), same, for pairs con-
curve) vs. load (stored number of patterns). The model                                                              taining one stored and one new pattern. (◦), same, for
was given an increasing number of patterns to store,                                                                pairs of new patterns. See Algorithm 1 for details.
while being tested on differentiating between stored and
unseen patterns. (+), normalized mean error in report-                                                              ues, leading in turn to degraded generalization perfor-
ing pattern frequency vs. the load.                                                                                 mance. Figure 7 depicts the resulting trade-off between
                                                                                                                    capacity and generalization. It combines the data used
Trade-off between capacity and generalization.                                                                      to generate Figures 4 and 6. The abscissa values covary
Designing a memory system with multiple instances of                                                                with the resource trade-off: the same 300, 000 binding
the model (as suggested earlier) while keeping the total                                                            nodes are divided into 300 instances on the left, and into
number of binding nodes constant necessitates a deci-                                                               only 10 on the right.
sion: how to allocate the nodes among the instances of
the model. Having a small number of copies would allow                                                              Lexical taxonomy. The dendrogram in Figure 8 de-
each one to have a large binding layer and therefore high                                                           picts the contextual similarities among the 500 most fre-
capacity, but at the cost of being forced to use fewer co-                                                          quent words in Lewis Carroll’s Alice in Wonderland, as
occurrence statistics when comparing feature map val-                                                               distilled by our model. The results are similar to those
                                                                                                                    reported by Finch and Chater (1991).
                                                                                                              395

                                      1
                                                                                          framework for cognition (Chater, Tenenbaum, & Yuille,
                                    0.95                                                  2006), and also the need to test it against the body of be-
                                                                                          havioral and neurobiological findings concerning human
       Generalization performance
                                     0.9
                                                                                          memory.
                                    0.85
                                                                                                               References
                                     0.8
                                                                                          Brindley, G. (1969). Nerve net models of plausible size
                                    0.75
                                                                                            that perform many simple learning tasks. Proc R Soc
                                                                                            Lond B Biol Sci., 174(35), 173-191.
                                     0.7                                                  Chater, N., Tenenbaum, J. B., & Yuille, A. (2006). Prob-
                                    0.65
                                                                                            abilistic models of cognition: Conceptual foundations.
                                        0   0.5     1          1.5   2        2.5
                                                   Memory capacity            5
                                                                           x 10
                                                                                            Trends in Cognitive Sciences, 10, 287-291.
                                                                                          Finch, S., & Chater, N. (1991). A hybrid approach to
Figure 7: Trade-off between memory capacity and gen-                                        the automatic learning of linguistic categories. Artif.
eralization (see text for explanation).                                                     Intell. and Simul. Behav. Qtrly., 78, 16-24.
                                                                                          Hearn, R. (2001). Building grounded abstractions for
                                                                                            artificial intelligence programming. Msc thesis, Mas-
                                                                         looked
                                                                         got
                                                                         went
                                                                                            sachusetts Institute of Technology.
                                                                         never
                                                                         like             Hopfield, J. J. (1982). Neural networks and physical sys-
                                                                         think
                                                                         come
                                                                         go
                                                                                            tems with emergent collective computational abilities.
                                                                         get
                                                                         say                Proc. Natl. Acad. Sci., 79, 2554-2558.
                                                                         see
                                                                         know             Kanerva, P. (1988). Sparse distributed memory. Cam-
                                                                         thought
                                                                         said
                                                                         turtle
                                                                                            bridge, MA: MIT Press.
                                                                         little
                                                                         way              Koriat, A., Goldsmith, M., & Pansky, A. (2000). Toward
                                                                         first
                                                                         mock
                                                                         rabbit
                                                                                            a psychology of memory accuracy. Annual Review of
                                                                         mouse
                                                                         queen              Psychology, 51, 483-539.
                                                                         king
                                                                         gryphon          Luria, A. (1968). The mind of a mnemonist. Cambridge,
                                                                         hatter
                                                                                            MA: Harvard University Press.
                                                                                          Marr, D. (1969). A theory of cerebellar cortex. J. Phys-
Figure 8: Hierarchical clustering of words from Alice                                       iol., 202, 437-470.
in Wonderland. A memory model with 20, 000 binding                                        Marr, D. (1970). A theory for cerebral neocortex. Pro-
nodes was presented with patterns generated by mov-                                         ceedings of the Royal Society of London B, 176, 161-
ing a sliding window of length 5 over the text. The                                         234.
model was then queried for similarities between all pairs                                 Merker, B. (2004). Cortex, countercurrent context, and
of words. Clustering was performed according to these                                       dimensional integration of lifetime memory. Cortex,
similarities.                                                                               40, 559-576.
                                                                                          Minsky, M. (1985). The Society of Mind. New York:
                                                  Summary                                   Simon and Schuster.
Human memory is characterized by high capacity, con-                                      Moll, M., & Miikkulainen, R. (1997). Convergence-zone
text sensitivity, and access flexibility. Computational                                     episodic memory: Analysis and simulations. Neural
models of memory need to quantify these properties and                                      Networks, 10, 1017-1036.
explicate the relationships between them. The present                                     Mueller, S. T. (2006). REM-II: A Bayesian model of
work explored one such relationship: between the ca-                                        the organization of semantic and episodic memory sys-
pacity of a memory system and its ability to general-                                       tems. In Proc. Cognitive Neuroscience Society Meet-
ize among similar items. The trade-off that we had ex-                                      ing.
pected and were able to demonstrate in a simple, almost                                   Mueller, S. T., & Shiffrin, R. M. (2006). REM II: A
generic memory model provides a useful perspective on                                       model of the developmental co-evolution of episodic
how memory works.                                                                           memory and semantic knowledge. In Proc. Intl. Con-
   The computational model presented here lacks sophis-                                     ference on Learning and Development (ICDL).
tication and neurobiological realism, yet it is a step in the                             Rolls, E. T. (2000). Memory systems in the brain. An-
right direction, because it is capable not only of storing                                  nual Review of Psychology, 51, 599-630.
and recalling patterns, but also of making certain gener-                                 Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006).
alizations about the stored items. Future work in this di-                                  Theory-based Bayesian models of inductive learning
rection would have to address the need for a well-founded                                   and reasoning. Trends in Cognitive Sciences, 10, 309-
approach to statistical inference on the part of the model,                                 318.
ideally thus bringing it in line with the modern Bayesian
                                                                                    396

