UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Generalization and Systematicity in Echo State Networks
Permalink
https://escholarship.org/uc/item/9ps6p20n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Frank, Stefan L.
Čerňanský, Micheal
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Generalization and Systematicity in Echo State Networks
                                              Stefan L. Frank (sfrank@science.uva.nl)
                               Institute for Logic, Language and Computation, University of Amsterdam
                                   Plantage Muidergracht 24, 1018 TV Amsterdam, The Netherlands
                                           Michal Čerňanský (cernansky@fiit.stuba.sk)
                                   Institute of Applied Informatics, Slovak University of Technology
                                                Ilkovicova 3, 842 16 Bratislava 4, Slovakia
                               Abstract                                 worse on new sentences than on training sentences does not
   Echo state networks (ESNs) are recurrent neural networks that
                                                                        necessarily fail to generalize completely. Second, how much
   can be trained efficiently because the weights of recurrent con-     should new inputs differ from training examples? Clearly, if
   nections remain fixed at random values. Investigations of these      just any indication of generalization would suffice, the issue
   networks’ ability to generalize in sentence-processing tasks         would already be decided in favor of connectionism.
   have resulted in mixed outcomes. Here, we argue that ESNs
   do generalize but that they are not systematic, which we define         To answer the first question, we consider a new input to
   as the ability to generally outperform Markov models on test         be processed sufficiently if the network outperforms Markov
   sentences that violate the training sentences’ grammar. More-
   over, we show that systematicity in ESNs can easily be ob-           models. The rationale behind this is as follows: If a well-
   tained by switching from arbitrary to informative representa-        trained but non-generalizing word-prediction system is faced
   tions of words, suggesting that the information provided by          with the new input sentence wt−x , . . . , wt−2 , wt−1 , the best it
   such representations facilitates connectionist systematicity.
                                                                        can do is base its prediction for wt on the most recent n-word
   Keywords: Recurrent neural networks; Echo state networks;            sequence (i.e., wt−n , . . . , wt−1 ; with n ≤ x) that also appeared
   Markov models; Generalization; Systematicity; Sentence pro-
   cessing; Non-symbolic representations.                               in the training data, ignoring the earlier words. Such a model
                                                                        is called an nth order Markov model. Since we have defined
                          Introduction                                  the system to be non-generalizing, it cannot use an n-word se-
In an influential paper, Fodor and Pylyshyn (1988) argued that          quence that is too long to have appeared in the training data.
neural networks cannot display the systematicity observed in            Therefore, we consider a network to generalize (to some ex-
human language and thought, except by directly implement-               tent at least) if it generally performs better than any Markov
ing a Classical symbol system. Consequently, no progress                model (i.e., for any n) on test sentences.
in cognitive science can be expected from connectionist ap-                In real-life applications, we cannot know which n would be
proaches. Twenty years later, this issue is still debated. Here,        best, so its value needs to be fixed or could depend on the oc-
we investigate systematicity in connectionist sentence pro-             currence of the test sequence in the training data, turning the
cessing, taking next-word prediction as the paradigm task.              model into a Variable Length Markov model (VLMM). Tak-
   In the next-word prediction task, a model is given a set of          ing the best Markov model as the baseline for sufficient per-
training sentences and one or more test sentences. Using the            formance obviously results in a much stricter test than would
information in the training data, the model has to predict the          a fixed n or a VLMM baseline. According to our definition
next word at each position in the test sentence(s). Since cor-          of sufficient performance, therefore, earlier claims of con-
rect prediction is not generally possible, the model is said to         nectionist systematicity that were based on comparisons to
perform perfectly if it gives correct next-word probabilities.          a 1st order Markov model (Frank, 2006a) or VLMM (Frank,
   In this paper, we provide a definition of systematic per-            2006b) are no longer warranted.
formance in next-word prediction, and show that a currently                As for the second question, we argue1 that the differ-
popular type of recurrent neural network, the echo state net-           ence between ‘mere’ generalization and systematicity paral-
work (ESN), fails to be systematic. Switching from symbolic             lels the difference between ergodic and non-ergodic sampling
to non-symbolic representations of words, however, results              of training sentences. In ergodic sampling, the distribution of
in ESN systematicity while retaining the network’s desirable            the sample is guaranteed to converge to the true distribution
property of being efficiently trainable.                                as sample size grows. Presumably, this property gives the
                                                                        network the opportunity to correctly process test sentences
Generalization and systematicity                                        by some sort of interpolation from the sampled training ex-
Fodor and Pylyshyn (1988) failed to operationalize system-              amples. In non-ergodic sampling, on the other hand, the sam-
aticity in a manner that allows for quantifying a neural net-           ple will never come to accurately reflect the true distribution.
work’s systematic behavior. As noted by Hadley (1994), a                For example, particular sentences may be excluded from the
model’s systematicity is apparent in the extent to which it             sample on purpose. In that case, the ‘training grammar’ that
generalizes, that is, its ability to sufficiently deal with un-         generated the training sentences is not identical to the under-
trained inputs. This raises two questions. First, when does
the network perform ‘sufficiently’? A network that performs                 1 And for those not convinced by our argument, we define.
                                                                    733

lying ‘true grammar’ that can generate all grammatical sen-                ing a suggestion by Phillips (1998), who found networks to
tences. No model can reliably learn the true grammar from                  lack systematicity in a symbol-processing task and remarked
a non-ergodic sample. To correctly process a test sentence                 that this might be fixed if, somehow, additional information
that could not have been generated by the training grammar,                would be provided by prior similarity among the represen-
the network needs to generalize to items that are markedly                 tations of inputs that should be treated similarly. Since sys-
different from what it was trained on.                                     tematicity might be trivially obtained if the modeler has com-
   To summarize, we submit that a network displays system-                 plete freedom to choose any desired set of input representa-
aticity if it generally outperforms Markov models when pro-                tions, Phillips rightly argued that the choice of representations
cessing sentences that could not have been generated by the                should be independently justified, for example by being based
grammar that generated the training examples.                              on the training data.
                                                                              Basically, this is the strategy followed here. Using an effi-
Echo state networks                                                        cient and largely task-independent method, informative rep-
Much of recent research into recurrent neural networks                     resentations of words are extracted from the training data, re-
(RNNs) has focused on so-called ‘reservoir computing’.2 In                 placing the ESN’s random (and thereby uninformative) rep-
this approach to RNN training, the weights of the network’s                resentations. The resulting model outperforms the standard
input and recurrent connections remain untrained. The re-                  ESN and the Markov models when tested for systematicity.
current part of the network serves as a task-independent ‘dy-
namical reservoir’, while a non-recurrent ‘read-out’ network                                         The language
is trained to produce some desired output from the fluctuating             The language used in our experiments (based on Hadley,
patterns of reservoir activations.                                         Rotaru-Varga, Arnold, & Cardei, 2001), has a 26-word vo-
   One of the most influential reservoir-computing architec-               cabulary, comprising 12 nouns, 10 transitive verbs, 2 preposi-
tures is the echo state network (ESN; Jaeger, 2001). An                    tions, a relative clause marker, and an end-of-sentence marker
ESN’s read-out network has just a single layer of units, which             denoted [end], which is also considered a word. As there are
means that setting the weights of connections to the output                no semantic constraints, the names of words within each syn-
units is a simple linear regression task, which can be per-                tactic category are irrelevant and only provided to make sen-
formed off-line after a single presentation of the training in-            tences more readable. As explained below, the difference be-
put. This training efficiency is, in fact, one of the main attrac-         tween female nouns (Nfem ; e.g., women), male nouns (Nmale ;
tions of ESNs.                                                             e.g., men) and animal nouns (Nanim ; e.g., bats) is important
   There have been only few attempts to apply ESNs to sen-                 for distinguishing between training and test sentences.
tence processing, and results were mixed. Tong, Bickett,                      Table 1 shows the grammar that generates the language’s
Christiansen, and Cottrell (2007) found ESN performance on                 sentences. These can contain two types of embedded clauses:
the next-word prediction task to be comparable to that of                  subject-relative clauses (SRCs, as in girls that see boys. . . )
the more traditional simple recurrent network (SRN; Elman,                 and object-relative clauses (ORCs, as in girls that boys
1990). Contrary to this, Frank (2006a) reported that general-              see. . . ). Since SRCs can themselves contain a relative clause,
ization by ESNs is impoverished compared to SRNs. Like-                    there is no upper bound to sentence length.
wise, Čerňanský and Tiňo (2007) showed that ESNs cannot
generalize above the level of VLMMs, and claim that SRNs                   Table 1: Probabilistic context-free grammar of the language.
can achieve higher performance than ESNs on some tasks.                    Variable r denotes grammatical role (subject or object). The
   Possibly, the crucial difference between the experiments                probabilities of different productions are equal, except for NP,
by Tong et al. and Frank (2006a, 2006b) lies in ergodic ver-               where they are given in parentheses.
sus non-ergodic sampling. According to our definition above,
Frank tested the ESN for systematicity, while Tong et al.
merely investigated non-systematic generalization. The re-                   S          →     NPsubj V NPobj [end]
sults presented in this paper indeed indicate that ESNs can                  NPr        →     Nr (.7) | Nr SRC (.06) | Nr ORC (.09) |
generalize but are not systematic.                                                            Nr PPr (.15)
   Frank (2006a) showed that an ESN can generalize better                    SRC        →     that V NPobj
than an SRN when a two-layer read-out network is used. Un-                   ORC        →     that Nsubj V
fortunately, training such a network requires a slow, iterative              PPr        →     from NPr | with NPr
algorithm, such as backpropagation, doing away with much                     Nr         →     Nfem | Nmale | Nanim
of the charm of ESNs. Here, we shall show that ESN sys-                      Nfem       →     women | girls | sisters
tematicity is possible without such a painstaking search for                 Nmale      →     men | boys | brothers
proper connection weights, keeping more in line with the                     Nanim      →     bats | giraffes | elephants | dogs | cats | mice
original ESN approach. This is accomplished by follow-                       V          →     chase | see | swing | love | avoid | follow |
                                                                                              hate | hit | eat | like
    2 As is illustrated by the recent publication of a Neural Networks
special issue on this topic (2007, Vol. 20, No. 3).
                                                                       734

Training sentences
                                                                    Table 3: Structure of four types of systematicity-test sen-
To test for systematicity, particular sentences were excluded       tences. Replacing Nfem by Nmale and vice versa turns these
from the training data by setting restrictions on the grammat-      into generalization-test sentences.
ical roles (indicated by r in Table 1) particular nouns can ap-
pear in. Training sentences never have a male noun in subject              Type       Sentence structure
position or a female noun in object position. Animal nouns                 SRC1       Nmale that V         Nfem V          Nfem   [end]
can occur in either position. This means that, for generating              SRC2       Nmale V Nfem that         V          Nfem   [end]
training sentences, the single production rule for nouns (Nr )             ORC1       Nmale that Nmale V        V          Nfem   [end]
in Table 1 was actually replaced by two rules in Table 2.                  ORC2       Nmale V Nfem that         Nmale V           [end]
   The models were trained five times, each time using a
different set of 5 000 randomly generated training sentences
with an average length of 5.9 words.                                is generalization required to process the sentences. This is
                                                                    why, in Figure 1, no generalization results are plotted for the
Table 2: Production rules for nouns when generating training        first two or three words. In fact, 41 out of 10 800 potential
sentences, replacing Nr of Table 1.                                 generalization-test sentences appeared in the training data, so
                                                                    these were not used in the generalization test at all.
                   Nsubj   →     Nfem | Nanim
                   Nobj    →     Nmale | Nanim                                                    The models
                                                                    Markov models
Test sentences                                                      Let N(wt−n , . . . , wt−1 ) denote the number of times that the
                                                                    n-word test sequence wt−n , . . . , wt−1 appears in the training
The models are tested on two groups of new sentences:               data. According to the nth order Markov model, the probabil-
generalization-test sentences and systematicity-test sen-           ity that word i directly follows the sequence equals3
tences. Generalization-test sentences are subject to the re-
strictions on the nouns’ grammatical roles that also apply to                                              N(wt−n , . . . , wt−1 , i)
training sentences. That is, the production rules for nouns                   Pr(i|wt−n , . . . , wt−1 ) =                            . (1)
                                                                                                           N(wt−n , . . . , wt−1 )
were as in Table 2. As a result, the training data formed an
ergodic sample with respect to generalization-test sentences.          Specific Markov models differ in the value of n, that is, in
   According to the true grammar in Table 1, all nouns should       their order. In the simplest case, n = 0, so Pr(i) = N(i)/N,
be treated equally, that is, wherever a noun can occur, any         where N is the number of words in the training data. This
noun can occur. Systematicity-test sentence are grammati-           so-called unigram model ignores the input all together and
cal according to this true grammar but since they violate the       always estimates the probability of a word by its relative fre-
noun-role restrictions of Table 2, they were not generated          quency in in the training data. In the bigram model, n = 1,
by the grammar that generated training sentences. With re-          so only the current input is taken into account and Equation 1
spect to systematicity-test sentences, therefore, the training      reduces to Pr(i|wt−1 ) = N(wt−1 , i)/N(wt−1 ).
data form a non-ergodic sample.                                        Note that larger n does not need to result in more accurate
   Following Frank (2006b), test sentences have either a SRC        predictions. It is even possible that the simple unigram model
or an ORC, that modifies either the first or second noun, mak-      outperforms all higher order Markov models, and in fact it
ing four types of test sentences, labeled SRC1, SRC2, ORC1,         often does in our systematicity tests. Therefore, at each point
and ORC2. When testing for systematicity, the models pro-           of each test sentence type, we take the best Markov model for
cessed all sentence with the structures of those in Table 3.        all n (up to the number of words in the test sentence so far).
Since there are three different (fe)male nouns and ten verbs
(and each of the four types of test sentence has three nouns        Echo state network
and two verbs), the number of systematicity-test sentences is       The architecture of our ESN is basically the same as that of
4 × 33 × 102 = 10 800. An example of each test sentence type        a three-layer SRN: Words are presented at the input layer,
can be found in Figure 1.                                           whose activation is propagated to the hidden layer (called
   Note that each systematicity-test sentence (unlike any           ‘dynamical reservoir’ in an ESN) that also receives its own
training sentence) begins with a male noun, so systematicity-       previous activation state. The output layer receives activa-
test sentences differ from all training sentences from the very     tion from the hidden layer, and is trained to predict the next
first word. This is not the case for generalization-test sen-       input word. As mentioned in the Introduction, the main dif-
tences, which (like all training sentences) begin with a female     ference between an ESN and an SRN is that an ESN has fixed,
noun. As a result, the models do not need to generalize when
                                                                        3 Markov models often involve smoothing to prevent the occur-
processing the first word(s) of generalization-test sentences.
                                                                    rence of a zero in the fraction of Equation 1. We also ran our experi-
Up to a point, each generalization-test sentence will have ap-      ments using Laplace smoothing, but found no qualitative differences
peared in the training data. Only from this point onwards           with the results of the unsmoothed models presented here.
                                                                735

random input and recurrent connections weights, whereas all                        Output The DR sends activation to the network’s 26 out-
weights of an SRN are adjusted during training.                                    put units which correspond to the language’s 26 words. The
Input When word i forms the input to the ESN, it is repre-                         weights of connections from DR units to outputs are collected
sented by a vector wi = (wi,1 , . . . , wi,26 ), the number of ele-                in the 26 × k matrix Wout . Also, each output unit receives a
ments of which equals the number of word types in the lan-                         bias activation. The output at time step t equals
guage.4 In a standard ESN, the values in these vectors are                                              aout (t) = Wout adr (t) + b,
chosen at random. Here, we compare this approach to one
in which word representations are based on the training data.                      where b is the vector of bias activations. Output vector aout
Bullinaria and Levy (2007) compared different techniques for                       is transformed to a probability distribution by setting all its
extracting such representations from text corpora and found                        negative values to 0 and rescaling the rest to sum to 1.
a surprisingly simple method to result in very good perfor-
                                                                                   Training Optimal output connection weights and bias vec-
mance on a variety of syntactic and semantic tasks. Our
                                                                                   tor, Wout and b, are easy to find without any iterative training
model uses this so-called ‘ratios’ method, according to which
                                                                                   method.5 First, we construct a 26 × (N − 1) target matrix
the jth element of the vector representing word i depends on
                                                                                   U = (u(1), u(2), . . . , u(N − 1)), where each u(t) is a column
the number of times words i and j occur next to each other in
                                                                                   vector of 0s except for a single 1 for the element correspond-
the training data:
                                                                                   ing to the input word at t + 1. That is, the vector u(t) forms
                                                                                   the correct prediction of the input at t + 1.
                                      N(i, j) + N( j, i)
                         wi, j = N ×                       .                          Next, the complete training sequence (excluding the last
                                          N(i)N( j)
                                                                                   word) is run through the DR, according to Equation 2. The re-
                                                                                   sulting vectors adr are collected in a matrix A to which a row
   The network that uses these word representations will be
                                                                                   of 1s is concatenated, resulting in a (k + 1) × (N − 1)-matrix.
called ESN+. To make the comparison between ESN and
                                                                                   The connection weights and bias values are now computed
ESN+ as fair as possible, ESN’s random representations are
                                                                                   by multiplying U with A’s pseudoinverse: W = UA−1 . The
obtained by randomly reordering all values wi, j so that word
                                                                                   last column of W forms the bias vector b, while the rest of W
representations in ESN and ESN+ contain the same values.
                                                                                   equals Wout . If the ESN would process the training sequence
Only in ESN+, however, do they provide information about
                                                                                   again, these Wout and b minimize the MSE between network
word co-occurrences in the training sentences.
                                                                                   outputs aout (t) and corresponding targets u(t).
Dynamical reservoir The ESN’s dynamical reservoir (DR)
consists of k units that receive input from some external                                                        Results
source and from each other. The k × k matrix Wdr contains                          Performance on test sentences is rated by computing the
the weights of connections between the DR units. The DR is                         cosines between the estimated next-word probabilities (i.e.,
sparsely connected in that 85% of values in Wdr are 0. All                         the model’s output vectors) and the true probabilities accord-
other values are taken randomly from a uniform distribution                        ing to the grammar of Table 1. Values close to 1 indicate
centered at 0, after which they are rescaled such that the spec-                   good generalization performance, while a cosine of 0 means
tral radius of Wdr (i.e., its largest eigenvalue) equals 1. Each                   that the two probability distributions are perpendicular. All
of the five repetitions of ESN(+) training used another Wdr .                      results presented below are averaged over the five training
   The DR’s activation vector at time step t is denoted adr (t) ∈                  repetitions.
[0, 1]k . At each time step, the vector representing the current
input word i enters the DR, which also receives its own pre-
                                                                                   ESN parameter setting
vious activation. The new activation vector is computed by                         Three ESN parameters were manipulated: DR size k ∈
                                                                                   {100, 200, 300}, DR scaling sdr ∈ {.05, .1, .3, .5, .7, .9, .95},
                adr (t) = f(sdr Wdr adr (t − 1) + sinwi ),                 (2)     and input scaling sin ∈ {.02, .1, .4, 2}. We took the parameter
                                                                                   setting that resulted in best average performance on system-
where sdr and sin are parameters controlling DR and input                          aticity test sentences, for ESN and ESN+ separately. These
scaling respectively, adr (t − 1) is the DR state in the previ-                    values are shown in Table 4. Clearly, ESN+ can perform bet-
ous time step (with adr (0) = .5 at the beginning of each sen-                     ter than ESN. Note that ESN+ (unlike ESN) performs best
tence), and f is the logistic function. Note that the addition in                  at the extreme end of the parameter space that was explored,
Equation 2 is only possible if k = 26. Since we use values of                      suggesting that its performance can be further improved by
k > 26, vector wi should be imagined as having k − 26 zeros                        setting the parameters at more extreme values.
concatenated to it.                                                                Generalization
    4 A more common but equivalent way to denote this is by col-                   The top row of Figure 1 plots the results for ESN, ESN+, and
lecting the input vectors in an input connection weight matrix                     the best Markov model, at each word of each of the four types
Win = (w1 , . . . , w26 ) that is multiplied by an input activation vec-
tor ain = (a1 , . . . , a26 ) with ai = 1 if i is the current input and ai = 0         5 See Jaeger (2001) for a more comprehensive explanation of the
otherwise.                                                                         ESN training procedure.
                                                                               736

                                                                                                              SRC1       SRC2                 ORC1     ORC2
Table 4: Parameter values resulting in highest average perfor-                                         1
                                                                        generalization performance
mance on systematicity test sentences.
                                                                                                     0.75
                       parameter         average
                                                                                                                                ESN
          Model      k     sdr sin     performance                                                    0.5                       ESN+
          ESN       200 .1       .4        .834                                                                                 best Markov
          ESN+      100 .95       2        .923                                                      0.25
                                                                                                             girls
                                                                                                              that      girls
                                                                                                                          like           girls         girls
                                                                                                               like
                                                                                                             boys       boys
                                                                                                                         that            that
                                                                                                                                       women             like
                                                                                                                                                       boys
                                                                                                              see
                                                                                                             men         see
                                                                                                                        men               like          that
                                                                                                            [end]      [end]              see
                                                                                                                                         men         women
                                                                                                                                                        see
                                                                                                                                        [end]         [end]
                                                                                                       1
of generalization test sentences. Overall, ESN(+) performs at
                                                                        systematicity performance
least as well as Markov models and can therefore be said to
                                                                                                     0.75
generalize. Only at the 5th word of ORC2 test sentences does
ESN (but not ESN+) do worse than the best Markov model.
The difference is small but highly significant (N = 1280, z =                                         0.5
29.5, p ≈ 0 in a Wilcoxon matched-pairs signed-rank test).
                                                                                                     0.25
                                                                                                              boys
                                                                                                               that      boys
                                                                                                                           like          boys
                                                                                                                                          that
                                                                                                                                                      boys
                                                                                                                                                        like
Systematicity                                                                                                   like     girls           men          girls
                                                                                                                                                       that
                                                                                                              girls       that             like       men
                                                                                                                                                       see
                                                                                                               see
                                                                                                            women         see
                                                                                                                       women              see
                                                                                                                                       women         [end]
                                                                                                             [end]      [end]           [end]
As shown in the bottom row of Figure 1, Markov models do
quite badly at many points of systematicity test sentences. For
example, performance at the first word (a male noun) is very            Figure 1: Performance at each point in four types of gener-
low. This is because male nouns never occurred in sentence-             alization (top) and systematicity (bottom) test sentences, by
initial position in training sentences. ESNs, however, do not           ESN, ESN+, and the best Markov model. Results are aver-
suffer from this problem: They score nearly perfectly at this           aged over all test sentences of a type; those shown on the
point in systematicity test sentences. In general, ESN+ does            x-axis are just examples.
at least as well as Markov models. Especially when Markov
models perform badly, ESN+ does much better. Only at the
fourth word of SRC1 sentences is ESN+ performance slightly
                                                                        Systematicity and representation
(but significantly: N = 1328, z = 17.9, p ≈ 0) lower than that          Peirce (1903/1985) defines a representation as symbolic if
of Markov models.                                                       its form is related arbitrarily to its meaning. It is clear that
   In contrast to ESN+, the standard ESN model often per-               Markov models treat words as symbols in this sense: The
forms much worse than the best Markov model. According                  word forms i and j (or girls and women, for that matter) pro-
to our definition, this means that ESN (unlike ESN+) does not           vide no information whatsoever about their meaning or the
display systematicity.                                                  positions they can take in a sentence. It is often believed
                                                                        that neural networks are non-symbolic because they use dis-
                        Conclusion                                      tributed vector representations. However, in standard ESNs,
After training an SRN on the prediction task in symbol-                 words are represented by random vectors, which are arbitrary
sequence processing, Čerňanský, Makula, and Beňušková             by definition. ESNs therefore represent words symbolically.
(2007) attributed most of successful generalization to the              In contrast, in ESN+ (and trained SRNs) relations among
learned representations of input symbols. This finding sug-             the words’ representations reflect relations among the words
gests that training recurrent connections may not be very im-           themselves. More precisely, words belonging to the same
portant for some commonly used data sets, and that ESN gen-             grammatical category have similar representations. Conse-
eralization can be improved by adjusting the input represen-            quently, they affect the network’s dynamical reservoir simi-
tations instead of leaving them random. Indeed, this is pre-            larly. This non-symbolic representational scheme is crucial
cisely what we found. Importantly, appropriate representa-              for the systematicity observed in the ESN+ model.
tions could be computed efficiently from the training data.                Tiňo, Čerňanský, and Beňušková (2004) showed that the
   We have shown that an ESN generalizes but is not sys-                state space of an ESN’s dynamical reservoir (and, more gen-
tematic in sentence processing: It generally performs better            erally, of an RNN with small random weights) shows con-
than a Markov model on generalization-test sentences, but not           siderable structural differentiation when processing symbol
on systematicity-test sentences. This finding sheds light on            sequences. Each symbol has an attractor point in the state
the apparent inconsistency between Tong et al.’s (2007) and             space, and every time a symbol is presented to the network,
Frank’s (2006a) conclusions on ESNs’ ability to generalize:             its state moves towards that symbol’s attractor. Since the pre-
Unlike Tong et al., Frank tested for systematicity. Moreover,           vious state was determined mostly by the symbol previously
our results show that ESN+ makes systematic connectionist               presented (which also moved the state towards its attractor),
sentence processing possible without the need for backprop-             the DR’s current state reflects the history of all previously
agation or any other iterative training algorithm.                      presented symbols. An ESN explicitly uses this organization.
                                                                  737

However, since symbol representations in a standard ESN are              putational study. Behavior Research Methods, 39, 510–
random, so are the attractor points. This makes it difficult for         526.
the network to generalize over symbols that should be treated         Čerňanský, M., Makula, M., & Beňušková, Ľ. (2007). Or-
similarly (e.g., because they are all nouns).                            ganization of the state space of a simple recurrent network
   In ESN+, the situation is different: The attractor points             before and after training on linguistic structures. Neural
of words from the same grammatical category are closer to-               Networks, 20, 236–244.
gether than those of words from different categories. This fa-        Čerňanský, M., & Tiňo, P. (2007). Comparison of Echo State
cilitates generalization over words from the same grammati-              Networks with Simple Recurrent Networks and Variable-
cal category. As a result, ESN+ outperforms ESN when faced               Length Markov Models on symbolic sequences. In Pro-
with systematicity test sentences. This finding illustrates              ceedings of ICANN 2007. Berlin: Springer.
the importance of switching from symbolic to non-symbolic             Elman, J. L. (1990). Finding structure in time. Cognitive
representations. Likewise, Frank, Haselager, and Van Rooij               Science, 14, 179–211.
(2008) argue that the use of non-symbolic representations of          Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and
sentential meaning is vital to the semantic systematicity dis-           cognitive architecture: a critical analysis. Cognition, 28,
played in their connectionist sentence-comprehension model.              3–71.
                                                                      Frank, S. L. (2006a). Learn more by training less: system-
Weak and strong systematicity                                            aticity in sentence processing by recurrent networks. Con-
In an investigation of systematicity in connectionist models             nection Science, 18, 287–302.
of sentence processing, Hadley (1994) argued that the models          Frank, S. L. (2006b). Strong systematicity in sentence pro-
that were around at the time did not account for human levels            cessing by an Echo State Network. In Proceedings of
of systematicity because they displayed only ‘weak system-               ICANN 2006. Berlin: Springer.
aticity’, as he called it. Hadley defined weak systematicity as       Frank, S. L., Haselager, W. F. G., & Van Rooij, I. (2008).
the ability to correctly process test sentences that have words          Connectionist semantic systematicity. (Manuscript submit-
occurring only in the same positions they held during training.          ted for publication)
In contrast, ‘strong systematicity’ is the ability to correctly       Hadley, R. F. (1994). Systematicity in connectionist language
process test sentences that have words in positions that differ          learning. Mind & Language, 9(3), 247–272.
from those in the training examples. Moreover, the network            Hadley, R. F., Rotaru-Varga, A., Arnold, D. V., & Cardei,
should also be able to handle test sentences with embedded               V. C. (2001). Syntactic systematicity arising from semantic
clauses containing words in untrained positions. According               predictions in a Hebbian-competitive network. Connection
to Hadley, people display strong systematicity, whereas neu-             Science, 13(1), 73–94.
ral networks are only weakly systematic at best.                      Jaeger, H.        (2001).     The “echo state” approach to
   Hadley’s (1994) notion of weak systematicity subsumes                 analysing and training recurrent neural networks. GMD
our definition of non-systematic generalization (i.e., sufficient        report no. 148. GMD — German National Research
processing of test inputs after ergodic sampling). This is be-           Institute for Computer Science. http://www.faculty.iu-
cause, in a large enough ergodic sample of training sentences,           bremen.de/hjaeger/pubs/EchoStatesTechRep.pdf.
all words will have occurred in all possible positions. There-        Peirce, C. S. (1903/1985). Logic as semiotics: The theory
fore, a non-systematic generalizing model (according to our              of signs. In R. E. Innis (Ed.), Semiotics: An introductory
definition) is weakly systematic (in Hadley’s sense).                    anthology. Bloomington, IN: Indiana University Press.
   Our current test for systematicity comes down to testing           Phillips, S. (1998). Are feedforward and recurrent networks
for Hadley’s strong systematicity. The restrictions on the oc-           systematic? Analysis and implications for a connectionist
currence of particular nouns in particular grammatical roles             cognitive architecture. Connection Science, 10, 137–160.
make sure that all systematicity-test sentences have words oc-        Tiňo, P., Čerňanský, M., & Beňušková, Ľ. (2004). Marko-
curring in novel positions, both in their main clause and in the         vian architectural bias of recurrent neural networks. IEEE
embedded clause. Therefore, not only have we shown that                  Transactions on Neural Networks, 15, 6–15.
ESN+ can behave systematically, we have also met Hadley’s             Tong, M. H., Bickett, A. D., Christiansen, E. M., & Cottrell,
challenge of displaying strong connectionist systematicity.              G. W. (2007). Learning grammatical structure with Echo
                                                                         State Networks. Neural Networks, 20, 424–432.
                     Acknowledgments
The research presented here was supported by grant 451-04-
043 of the Netherlands Organization for Scientific Research
(NWO) and grant APVV-20-030204 of the Slovak Research
and Development Agency (APVV).
                           References
Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic
   representations from word co-occurrence statistics: a com-
                                                                  738

