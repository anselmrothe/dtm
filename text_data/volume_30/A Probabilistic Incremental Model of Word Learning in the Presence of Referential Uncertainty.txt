UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Probabilistic Incremental Model of Word Learning in the Presence of Referential
Uncertainty
Permalink
https://escholarship.org/uc/item/22b1058q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Fazly, Afsaneh
Alishahi, Afra
Stevenson, Suzanne
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          A Probabilistic Incremental Model of Word Learning in the Presence of
                                                      Referential Uncertainty
                                      Afsaneh Fazly, Afra Alishahi and Suzanne Stevenson
                                                        Department of Computer Science
                                                              University of Toronto
                                                   {afsaneh,afra,suzanne}@cs.toronto.edu
                               Abstract                                       Many computational models of word learning have been
                                                                           used to simulate and account for the observed patterns such
   We present a probabilistic incremental model of early word
   learning. The model acquires the meaning of words from ex-              as fast mapping and the vocabulary spurt. However, most
   posure to word usages in sentences, paired with appropriate             of these models use input data that consists of pairings of a
   semantic representations, in the presence of referential uncer-         single word and its semantic representation, and ignore the
   tainty. A distinct property of our model is that it continually re-
   vises its learned knowledge of a word’s meaning, but over time          problem of finding the right referent for each word in an utter-
   converges on the most likely meaning of the word. Another               ance (Regier, 2005; Li et al., 2004; Xu & Tenenbaum, 2007).
   key feature is that the model bootstraps its own partial knowl-         Other models simulate learning the meaning of words in the
   edge of word–meaning associations to help more quickly learn
   the meanings of novel words. Results of simulations on nat-             context of an utterance, but they either use artificial input data
   uralistic child-directed data show that our model exhibits be-          with controlled referential uncertainty, which may deviate
   haviours similar to those observed in the early lexical acquisi-        from children’s naturalistic learning environments (Siskind,
   tion of children, such as vocabulary spurt and fast mapping.
                                                                           1996), or rely on cognitively implausible learning strategies
                                                                           and ignore the problem of referential uncertainty (Yu, 2005).
                    Early Word Learning                                    These properties make the existing computational models still
Acquiring the meaning of words is a challenging task for chil-             inadequate for a careful investigation of the patterns of child
dren: For an utterance that describes a scene, a child must                word learning in a realistic setting.
align each word with the right referent in the scene. Over                    We propose a novel incremental model of early word learn-
time, such alignments must be used to extract a meaning for                ing in the face of referential uncertainty. Our computa-
each word that is consistent across all of its usages. One well-           tional model proposes a probabilistic interpretation of cross-
known problem in word learning is that of referential uncer-               situational learning, and bootstraps its own partially-learned
tainty, that is, the child may perceive many aspects of the                knowledge of the previously-observed words to accelerate
scene that are unrelated to the perceived utterance (Quine,                word learning over time. We evaluate our model on naturalis-
1960; Gleitman, 1990). For example, a child may hear the                   tic child-directed data, and show that the overall behaviour of
sentence Jo rolled the ball, but observe that “Jo is happily               the model is reminiscent of the general patterns observed in
touching a red ball with her hand and slowly rolling it while              children. Moreover, our experimental results show that learn-
her mother is talking to her”. However, over time, the child               ing the meaning of words is a much harder task when the
can establish an association between the word “ball” and the               input contains referential uncertainty, illustrating the impor-
round object that the word refers to.                                      tance of modeling this aspect of word learning. Our model
   Learning the meaning of words has been suggested to be                  thus provides an appropriate testbed for investigating the im-
based on cross-situational observation (Pinker, 1989): The                 pact of referential uncertainty on the word learning process.
meaning of a word is consistent across multiple usages, and
can be learned by detecting the set of meaning elements that                          Related Computational Models
are common across all situations in which a word occurs. In
its original form, this hypothesis is not precisely specified;             The rule-based model proposed by Siskind (1996) is the first
moreover, it does not provide the flexibility needed for word              to simulate the process of learning word meanings in the pres-
learning, especially in handling noisy or ambiguous data. A                ence of referential uncertainty. The model relies on a set
detailed account of this mechanism is needed in order to ex-               of principles to constrain hypotheses about the meaning of
plore the possibility of learning word meanings in a naturalis-            words, such as the principle of contrast and the principle of
tic environment, and to account for many general patterns ob-              inclusivity. The model is tested on artificially generated input
served in child experimental data. These patterns include the              consisting of highly controlled referential uncertainty. It is
vocabulary spurt (i.e., a slow stage of learning, followed by              shown that under these circumstances the meaning of words
a sudden increase in the learning rate), fast mapping (i.e., the           can be learned, and certain types of noise can be handled by
ability to map a novel word to a novel object in a familiar con-           detecting and ruling out the inconsistent input. However, the
text), and the effect of the age of acquisition of words on their          rule-based nature of the model limits its adaptability to natu-
processing speed. Computational modeling is a powerful tool                ral data. For example, it is not possible to revise the meaning
for precise investigation of the hypothesized mechanisms of                of a word once it is considered as ‘learned’, which prevents
word learning, and for studying the suggested patterns.                    the model from handling highly noisy or ambiguous data.
                                                                       703

   Another computational model that uses cross-situational           The Learning Algorithm
inference is proposed by Yu (2005), which is also used to ex-        Our model combines probabilistic interpretations of cross-
amine the role of various factors, such as syntax (Yu, 2006),        situational learning (Quine, 1960) and a variation of the prin-
in word learning. However, the model uses the original form          ciple of contrast (Clark, 1990),1 through an interaction be-
of the automatic translation learning algorithm of Brown et          tween two types of probabilistic knowledge acquired and re-
al. (1993), which lacks cognitive plausibility: It is non-           fined over time. Given an utterance–scene pair received at
incremental and learns through an intensive batch processing         time t, i.e., (U(t) , M(t) ), the model first calculates an align-
of a whole training data. Moreover, it is tested on limited ex-      ment probability a for each w ∈ U(t) and each f ∈ M(t) , using
perimental data containing a very small vocabulary, and with         the meaning p(.|w) of all the words in the utterance prior to
no referential uncertainty.                                          this time. The model then revises the meaning of the words in
   Most of the existing models rely on a pairing of a semantic       U(t) by incorporating the alignment probabilities for the cur-
representation with a single word form (or its phonological          rent input pair. This process is repeated for all the input pairs,
representation)—as opposed to full utterances—as training            one at a time.
data. Connectionist models have been proposed for learn-
ing such associations, and investigating various patterns in         Step 1: Calculating the alignment probabilities: For a
the process of learning. For example, Li et al. (2004) simu-         feature f ∈ M(t) and a word w ∈ U(t) , the higher the prob-
late vocabulary spurt and age of acquisition effects, whereas        ability of f being part of the meaning of w (according to
Horst et al. (2006) examine the role of fast mapping. Regier         p( f |w)), the more likely it is that f is aligned with w in the
(2005) proposes an associative exemplar-based model that ac-         current input. In other words, a(w|f , U(t) , M(t) ) is propor-
counts for the changes observed in children’s word learning          tional to p(t−1) (f |w). In addition, if there is strong evidence
pattern, such as fast mapping and learning synonymy, with-           that f is part of the meaning of another word in U(t) —i.e.,
out a change in the underlying learning mechanism. The               if p(t−1) ( f |wk ) is high for some wk ∈ U(t) other than w—the
Bayesian model of Xu and Tenenbaum (2007), on the other              likelihood of aligning f to w should decrease (principle of
hand, focuses on how humans generalize and learn category            contrast). Combining these two requirements:
meanings from examples of word usages.
                                                                                                           p(t−1) (f |w)
      Overview of the Computational Model                                      a(w|f , U(t) , M(t) ) =                                 (1)
Our word learning model is an adaptation of a model of auto-
                                                                                                         ∑      p(t−1) (f |wk )
                                                                                                       wk ∈U(t)
matic translation between two languages, proposed by Brown
et al. (1993). Unlike the original model (as used by Yu,                General features such as ARTIFACT or ENTITY are part of
2005), ours is incremental and does not require a batch pro-         the meaning of, and thus co-occur with, many words in lan-
cess over the entire data. We explain the details of our model       guage. Therefore, in an input pair, they are usually aligned
in the following subsections.                                        with more than one word in the utterance. Over time, the
                                                                     model correctly learns a relatively strong association between
Utterance and Meaning Representations                                such features and the appropriate words, although their asso-
The input to our word learning model consists of a set of            ciation is less strong than those of more specific features.
utterance–scene pairs that link an observed scene (what the
child perceives) to the utterance that describes it (what the        Step 2: Updating the word meanings: We need to update
child hears). We represent each utterance as a sequence of           the probabilities p(.|w) for all words w ∈ U(t) , based on the
words, and the corresponding scene as a set of semantic fea-         evidence from the current input pair reflected in the alignment
tures (including features irrelevant to the utterance), e.g.:        probabilities. We thus add the current alignment probabilities
   Utterance: Joe rolled the ball                                    for w and the features f ∈ M(t) to the accumulated evidence
   Scene: { ANIMATE , JOE , ACT , MOTION, ROLL , ARTIFACT ,
                                                                     from prior co-occurrences of w and f . We summarize this
   OBJECT , GAME EQUIPMENT , MOTHER , HAND , TALK }
                                                                     cross-situational evidence in the form of an association score,
                                                                     which is updated incrementally:
In the Experimental Setup section, we explain how the utter-
ances and the corresponding semantic features are selected,            assoc(t) (w, f ) = assoc(t−1) (w, f ) + a(w|f , U(t) , M(t) )   (2)
and how we add referential uncertainty.
   Given a corpus of such utterance–scene pairs, our model           where assoc(t−1) (w, f ) is zero if w and f have not co-occurred
learns the meaning of each word w as a probability distribu-         before. The association score of a word and a feature is ba-
tion, p(.|w), over the semantic features appearing in the cor-       sically a weighted sum of their co-occurrence counts: In-
pus. In this representation, p( f |w) is the probability of fea-     stead of adding one each time the two have appeared in a
ture f being part of the meaning of word w. In the absence of        pair together, we add a probability (a value between zero and
any prior knowledge, all features can potentially be part of the
                                                                         1 We assume that a feature in a scene is highly associated with
meaning of all words. Hence, prior to receiving any usages of
                                                                     only one of the words in the corresponding utterance. This differs
a given word, the model assumes a uniform distribution over          from what is widely known as principle of contrast, in that the latter
semantic features as its meaning.                                    assumes contrast across the entire vocabulary.
                                                                 704

one) that reflects the confidence of the model that their co-                ball
                                                                                   → GAME EQUIPMENT #1
occurrence is indeed because f is part of the meaning of w.                           → EQUIPMENT #1
                                                                                         → INSTRUMENTALITY #3, INSTRUMENTATION #1
                                                                                            → ARTIFACT #1, ARTEFACT #1
   The model then uses these association scores to update the                                   → WHOLE #2, UNIT #6
                                                                                                   → OBJECT #1, PHYSICAL OBJECT #1
meaning of the words in the current input, as in:                                                     → PHYSICAL ENTITY #1
                                                                                                          → ENTITY #1
                                assoc(t) (f , w) + λ
          p(t) (f |w) =                                        (3)           ball: { GAME EQUIPMENT #1, EQUIPMENT #1, INSTRUMENTALITY #3, ARTIFACT #1,
                           ∑ assoc(t) (fj , w) + β × λ
                                                                                    WHOLE #2, OBJECT #1, PHYSICAL ENTITY #1, ENTITY #1 }
                          fj ∈F                                               Figure 1: Semantic features for ball from WordNet.
where F is the set of all features seen so far, λ is a smoothing
factor for allowing a small probability for unseen features,                                    Experimental Set-UP
and β is the expected number of feature types. The denomi-             The Input Corpora
nator is a normalization factor to get proper probabilities.           The training data for our model consists of a sequence of ut-
Handling Referential Uncertainty                                       terances, each paired with a set of semantic features. We ex-
                                                                       tract utterances from the Manchester corpus (Theakston et al.,
Our model updates the meaning of a word every time it is               2001) in the CHILDES database (MacWhinney, 1995). The
heard in an utterance. This flexibility, in addition to the prob-      Manchester corpus contains transcripts of conversations with
abilistic nature of the learning, allows the model to handle           children between the ages of 1;8 and 3;0. We use the mother’s
referential uncertainty. Recall that we simulate referential un-       speech from transcripts of 6 children, remove punctuation and
certainty in the form of additional semantic features that are         lemmatize the words, and concatenate the corresponding ses-
irrelevant to the meanings of the words in an utterance. We            sions as our test data.
expect that the irrelevant features do not regularly co-occur             There is no semantic representation of the correspond-
with a given word (in contrast to the relevant features). Thus         ing scenes available from CHILDES. Therefore, we auto-
the overall association score between an irrelevant feature and        matically construct a scene representation for each utterance
the word is expected to be lower than that of a relevant fea-          based on the semantic features of the words in that utterance.
ture. This in turn will lower the probability of the irrelevant        For nouns and verbs, we extract the semantic features from
features in p(.|w).                                                    WordNet3 as follows: We take all the hypernyms (ancestors)
   Another strategy we adopt for handling referential uncer-           for the first sense of the word, where each hypernym is a set of
tainty is the addition of a dummy word to every utterance              synonym words (or synsets), tagged with their sense number.
when updating the alignment probabilities. This is to allow            For each hypernym, we add the first word in its synset to the
the possibility of aligning the irrelevant semantic features to        set of the semantic features of the target word (see Figure 1
the dummy word, hence lowering their alignment probability             for an example). For adjectives and closed class words (e.g.,
with the words in the utterance. Note, however, that nothing           pronouns), we extract the semantic features using the system
indicates to the model a priori which features are relevant and        of Harm (2002). Other words not found in either of the two
which are irrelevant.                                                  resources (e.g., adverbs) are removed from the utterances.
                                                                          We need to evaluate our model on input that includes ref-
Word Comprehension Score
                                                                       erential uncertainty. That is, the representation of the scene
To evaluate our model, we need to verify how accurately the            must contain semantic features that do not come from the per-
model learns the meaning of words. We thus define a com-               ceived utterance. To simulate such data, we use every other
prehension score, c(t) (w), for each word w at time t, which           sentence from the original corpus (preserving their chrono-
compares the learned meaning of w, or p(.|w), to the word’s            logical order), paired with its own scene representation as
correct meaning, T w . The correct meaning of a word is the            well as that of the following sentence. The extra semantic
set of semantic features for that word in our input-generation         features that are added to each utterance thus correspond to
lexicon.2 The comprehension score is calculated as in:                 meaningful semantic representations, as opposed to randomly
                                                                       selected features. The resulting corpus has a high rate of ref-
                    c(t) (w)     =     ∑     p(t) (fj |w)      (4)     erential uncertainty, where on average an utterance is paired
                                     fj ∈T w
                                                                       with twice as many semantic features as there are in its origi-
where 0 ≤ c(t) (w) ≤ 1. Ideally, a word is accurately learned          nal meaning set.
when most of its probability mass p(.|w) is concentrated
around its true features (those in T w ). We thus consider a           Parameters
word as learned when its comprehension score exceeds a pre-            We set the parameters of our learning algorithm using a de-
defined threshold, θc . In our experiments reported in the fol-        velopment data set which is similar to our test data, but is se-
lowing sections, we examine the behaviour of our model by              lected from a non-overlapping portion of the Manchester cor-
looking into the comprehension scores.                                 pus. The expected number of semantic features, β in Eqn. (3),
                                                                       is set to 7000 based on the total number of distinct features
    2 Note that the model does not have access to this lexicon for
learning; it is used only for input generation and evaluation.             3 http://wordnet.princeton.edu/
                                                                   705

                                                                                                                                 1                                                                           1                                                                           1                                                                            1
                                                                                                                                0.9                                                                         0.9                                                                         0.9                                                                          0.9
                                           1                                                                                    0.8                                                                         0.8                                                                         0.8                                                                          0.8
                                                                                                          Comprehension score                                                         Comprehension score                                                         Comprehension score                                                          Comprehension score
                                                                                                                                0.7                                                                         0.7                                                                         0.7                                                                          0.7
                                                                                                                                0.6                                                                         0.6                                                                         0.6                                                                          0.6
                                          0.9                                                                                   0.5                                                                         0.5                                                                         0.5                                                                          0.5
                                                                                                                                0.4                                                                         0.4                                                                         0.4                                                                          0.4
            Proportion of learned words
                                                                                                                                0.3                                                                         0.3                                                                         0.3                                                                          0.3
                                          0.8                                                                                   0.2                                                                         0.2                                                                         0.2                                                                          0.2
                                                                                                                                0.1                                                                         0.1                                                                         0.1                                                                          0.1
                                                                                                                                 0                                                                           0                                                                           0                                                                            0
                                          0.7                                                                                         0   2000   4000   6000
                                                                                                                                                        Time
                                                                                                                                                               8000   10000   12000                               0   2000   4000   6000
                                                                                                                                                                                                                                    Time
                                                                                                                                                                                                                                           8000   10000   12000                               0    2000   4000   6000
                                                                                                                                                                                                                                                                                                                 Time
                                                                                                                                                                                                                                                                                                                        8000   10000   12000                               0    2000   4000   6000
                                                                                                                                                                                                                                                                                                                                                                                              Time
                                                                                                                                                                                                                                                                                                                                                                                                     8000   10000   12000
                                          0.6
                                                                                                                                          book (f=54)                                                        car (f=148)                                                                          fish (f=31)                                                                  kiss (f=14)
                                          0.5
                                          0.4
                                                                                                                                          Figure 3: Comprehension scores of four words over time.
                                          0.3
                                                                                   no RU
                                          0.2
                                          0.1
                                                                                   RU (f >= 5)
                                                                                   RU (f >= 3)            Figure 2 displays three learning curves including only words
                                                                                   RU (f >= 2)
                                           0
                                                                                   RU (all)               which are heard at least twice, three times or five times.
                                                0    2000   4000   6000   8000   10000      12000
                                                                   Time                                      A comparison of the curves shows that the more frequent
      Figure 2: Proportion of learned words over time.                                                    a word is, the more likely it is to be learned. These results
                                                                                                          confirm our hypotheses that in the presence of RU, the model
extracted for the development data. The smoothing parame-                                                 needs more instances of a word usage to learn it with high
ter λ in Eqn. (3) is set to a very small value, 10−7 . We set                                             confidence. Nonetheless, even for words with a minimum
the comprehension threshold, θc in Eqn. (4), to a reasonably                                              frequency of 5, the learning is still more difficult when there
high value, 0.7 (recall that comprehension scores are between                                             is referential uncertainty.
0 and 1). This was a value with which our model showed rea-                                               Convergence and Learning Stability
sonable performance on development corpus. Moreover, 0.7
                                                                                                          Our learning algorithm revises the meaning of a word every
is a reasonably large portion of the probability mass, given
                                                                                                          time it is heard in an utterance (in contrast, e.g., to Siskind,
that only a small fraction of the semantic features appear as
                                                                                                          1996’s model). This is a key property that makes our model
part of the correct meaning of a word.
                                                                                                          flexible so it can handle noise by revising an incorrectly
                                                    Experimental Results                                  learned meaning. It is however important to ensure that the
                                                                                                          learning is stable despite this constant revision—that is, the
In the following sections, we provide a qualitative analysis of                                           meaning of earlier-learned words is not corrupted as a result
our model through examination of its learning patterns. We                                                of learning new words (the problem of catastrophic interfer-
train the model on the input corpus with referential uncer-                                               ence often observed in connectionist models). If the learning
tainty, as explained in the previous section. In order to exam-                                           is stable, we expect the comprehension scores for words gen-
ine how adding referential uncertainty affects word learning,                                             erally to increase over time as more and more examples of the
we repeat most of the reported experiments on the input with                                              word usages are encountered in the input.
no referential uncertainty, and compare the results.                                                         Figure 3 shows the change in the comprehension scores
                                                                                                          of four sample words over time. As expected, the compre-
Effects of Referential Uncertainty
                                                                                                          hension scores show some fluctuation at the beginning, but
As noted before, one of the main challenges of word learn-                                                they converge on a high value as more examples are ob-
ing is the uncertainty inherent in the children’s learning envi-                                          served. We also examine the average comprehension score
ronment. To better understand the effect of referential uncer-                                            of all words, as well as of those which have been learned
tainty (RU) in learning, here we compare the behaviour of our                                             at some point (i.e., their comprehension score has surpassed
model in two conditions, without RU and with RU. Figure 2                                                 the threshold θc ). The average comprehension score of all
shows the change in the proportion of learned words (those                                                words increases rapidly and becomes stable around 0.7 after
whose comprehension scores exceed the specified threshold)                                                processing almost 4, 000 input pairs, reflecting the stability
over time, where time is measured as the number of input                                                  in learning. As expected, the average comprehension score
utterance–scene pairs processed. The bottom curve shows the                                               of the learned words increases more quickly and reaches a
learning pattern for input with RU, and the top one shows the                                             higher value (around 0.8). With no RU, the average com-
results for data without RU. As expected, in both cases, the                                              prehension scores show similar increasing patterns, but are
proportion of learned words increases over time, with a rapid                                             generally higher, reflecting easier learning.
pace at early stages of learning, and a more gradual pace later.
In addition, Figure 2 shows that the task of word learning is                                             Vocabulary Growth and Fast Mapping
much easier in the absence of RU, reflected in the sharp vo-                                              Longitudinal studies of early vocabulary growth in children
cabulary growth, as well as in the high proportion of learned                                             have shown that vocabulary learning is slow at earlier stages
words (above 90%) in this condition.                                                                      of learning, then proceeds to a rapid pace, and finally be-
   To further elucidate the notable drop in the proportion of                                             comes less active (Carey, 1978)—a phenomenon often re-
learned words when there is RU, we look into the relation be-                                             ferred to as “vocabulary spurt”. Here, we look at the change
tween a word’s frequency and how easily the model learns it.                                              in the learning rate over time to see whether the pattern of
We examine the learning curves when low frequency words                                                   vocabulary growth in our model matches this observation.
are removed. (Note that low frequency words are only re-                                                     Figure 4 depicts the proportion of learned words against
moved from the evaluations, and not from the input data.)                                                 the number of word types heard at each time, both without
                                                                                                    706

                                              1                                                                                                           20
                                                                                                                       Number of usages needed to learn
                                             0.9                                                                                                          18
               Proportion of words learned
                                             0.8                                                                                                          16
                                             0.7                                                                                                          14
                                             0.6                                                                                                          12
                                             0.5                                                                                                          10
                                             0.4                                                                                                           8
                                             0.3                                                                                                           6
                                             0.2                                                                                                           4
                                             0.1                                         No RU                                                             2
                                                                                         RU
                                              0                                                                                                            0
                                                   0   200   400     600    800   1000                                                                         0   2000   4000     6000     8000   10000   12000
                                                              Word types heard                                                                                            Time of first exposure
Figure 4: Rate of vocabulary growth as new words are heard.                                                                                               (a) No referential uncertainty.
                                                                                                                                                          20
                                                                                                                       Number of usages needed to learn
                                                                                                                                                          18
and with referential uncertainty. Without RU, the learning                                                                                                16
rate is immediately high, rather than a period of slow growth                                                                                             14
followed by a spurt as in children. The expected vocabulary                                                                                               12
                                                                                                                                                          10
growth pattern is more pronounced with RU: There is little                                                                                                 8
learning prior to hearing about 150 words. This can be at-                                                                                                 6
                                                                                                                                                           4
tributed to the property of our model that uses its own learned                                                                                            2
knowledge of word meaning to facilitate the learning of new                                                                                                0
                                                                                                                                                               0   2000   4000     6000     8000
                                                                                                                                                                          Time of first exposure
                                                                                                                                                                                                   10000   12000
words. After this sudden increase in the number of learned                                                                                  (b) With referential uncertainty.
words, the learning proceeds with a nearly constant rate. It
                                                                                                       Figure 5: Number of usages needed to learn a word vs. the
is important to note that since we test our model on realistic
                                                                                                       time of first exposure for that word.
data, we do not have a fixed vocabulary, and therefore, new
words are heard continually. The learning thus does not stop,                                             Studying the AoA effect in a computational model such as
but it gradually becomes slower, perhaps mainly due to a cor-                                          ours offers the advantage of having direct access to the exact
responding decrease in the rate of hearing new words.                                                  age at which the model has acquired each word. More impor-
   The observed shift from slow to fast word learning is some-                                         tantly, different interpretations of the age of acquisition can be
times tied with a phenomenon referred to as fast mapping                                               investigated: Whether AoA refers to the time of the first ex-
(Carey, 1978). Fast mapping states that once children have                                             posure to a word, or the age at which the model/child can cor-
learned a repository of words, they can easily link novel                                              rectly comprehend or produce the word. Moreover, most of
words to novel objects in a familiar context based only on                                             the reported studies on AoA effects in humans, as well as the
a single (or few) exposures. Many researchers believe that                                             computational modeling of these effects, have been focused
the delay in the onset of fast mapping in children is not due to                                       either on the association between the phonological form of
a change in the underlying learning mechanisms, but is a re-                                           a word and the corresponding written form (as in the word
sult of processing more input data (Regier, 2005; Horst et al.,                                        naming task), or on the familiarity of a written form (as in
2006). To examine this hypothesis in our model, we look at                                             the lexical decision task). Few studies have been performed
the interaction between the number of usages that the model                                            on whether similar AoA effects can be observed in tasks that
needs to learn a word, and the word’s age of exposure, de-                                             rely on the association between a word form and its meaning
fined as the first time the word is heard. Figure 5 depicts the                                        (but see Li et al., 2004). Our model provides an appropri-
plots (for the words that are learned at some point in time),                                          ate testbed for investigating whether AoA effects can be ob-
both without and with RU. In both cases, the model shows                                               served in the context of learning word meaning, especially in
clear fast mapping behaviour: Words received later in time,                                            the presence of referential uncertainty.
on average, require fewer usages to be learned. With refer-                                               To simulate AoA effects in our model, we need to estimate
ential uncertainty, fast mapping occurs much more gradually.                                           two factors. We estimate the processing speed of a word w
These results show that our model exhibits fast mapping pat-                                           at a time t as its comprehension score c(t) (w), as given in
terns once it has been exposed to enough word usages, and                                              Eqn. (4) (assuming that words that have a higher comprehen-
that no change in the learning mechanism is needed.                                                    sion score can be accessed and processed faster). We consider
                                                                                                       two different estimations for the age of acquisition of a word:
Age of Acquisition Effect
                                                                                                       First, the onset of the word in the training data, or its age
Recent studies have suggested that age of acquisition (AoA),                                           of ‘Exposure’; and second, the first time the model correctly
independently of word frequency, affects the speed of pro-                                             learns the word, or its age of ‘Learning’.
cessing a word. For example, AoA is shown to be a good                                                    For each AoA condition, we compile two sets of words
predictor of the adult’s speed in word naming (Tamminen &                                              from the training data, an ‘Early’ set, which contains words
Gaskell, 2006) or lexical decision (Nazir et al., 2003; Tam-                                           acquired at an earlier stage of learning, and a ‘Late’ set, con-
minen & Gaskell, 2006). One problem with these studies is                                              taining words acquired at a later stage. We consider the time
that they cannot accurately estimate the age of acquisition of                                         span 5,000–10,000 as the earlier stage, and the time span
a word, and mostly rely on the subjective adult AoA ratings.                                           10,000–15,000 as the later stage (we skip over the time span
                                                                                                 707

                    Low Frequency             High Frequency         ings acquired by our model. Such word categories can in turn
  Condition      Early        Late          Early       Late         be used as feedback to our word learning model through a
  Exposure       0.65 (70) 0.70 (35)        0.84 (8)    0.82 (1)     bi-directional bootstrapping process. In future work, we will
  Learning       0.82 (38) 0.85 (32)        0.85 (18) 0.79 (4)       explore these options, and examine the impact of new factors
Table 1: Average comprehension score for Early and Late              on the learning pattern of the model.
word sets for the Exposure and Learning conditions, con-
                                                                                              References
trolled for frequency. Size of the sets is shown in parentheses.
                                                                     Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., & Mer-
1–5,000 in order for the model to stabilize). To control for           cer, R. L. (1993). The mathematics of statistical machine
frequency, we only include words whose frequency after pro-            translation: Parameter estimation. Computational Linguis-
cessing all 15,000 input pairs falls into a certain range.             tics, 19(2), 263–311.
   Table 1 shows the average comprehension scores of the             Carey, S. (1978). The child as word learner. In M. Halle,
words in the Early and Late sets after processing 15,000 pairs.        J. Bresnan, & G. A. Miller (Eds.), Linguistic theory and
The scores are calculated for two conditions, Exposure and             psychological reality. The MIT Press.
Learning, and for two different frequency ranges, Low Fre-           Clark, E. (1990). On the pragmatics of contrast. Journal of
quency (between 2 and 4) and High Frequency (between 6                 Child Language, 17, 417–431.
and 10). The results show an interesting pattern: for more           Gleitman, L. (1990). The structural sources of verb meanings.
frequent words, an AoA effect can be observed, i.e., words             Language Acquisition, 1, 135–176.
acquired earlier are, on average, easier to comprehend (and          Harm, M. W. (2002). Building large scale distributed seman-
therefore easier to process). In contrast, for low frequency           tic feature sets with WordNet. Carnegie Mellon University.
words, an opposite effect can be observed for both condi-            Horst, J. S., McMurray, B., & Samuelson, L. K. (2006). On-
tions, i.e., words acquired later are easier to comprehend.            line processing is essential for learning: Understanding fast
This suggests that age of acquisition mainly affects the pro-          mapping and word learning in a dynamic connectionist ar-
cessing speed of words that are well-entrenched, and that the          chitecture. In Proc. of CogSci’06.
infrequent words can be remembered only if they have been            Li, P., Farkas, I., & MacWhinney, B. (2004). Early lexical
acquired later. However, this prediction may not be reliable           development in a self-organizing neural network. Neural
due to the small number of the high frequency words in each            Networks, 17, 1345–1362.
set, and further research is needed to confirm it.                   MacWhinney, B. (1995). The CHILDES project: Tools for
                                                                       analyzing talk (2nd ed.). Lawrence Erlbaum Associates.
          Conclusion and Future Directions                           Nazir, T. A., Decoppet, N., & Aghababian, V. (2003). On
                                                                       the origins of age-of-acquisition effects in the perception
We have presented a computational model of word learn-                 of printed words. Developmental Science, 6(2), 143–150.
ing that draws on cognitively plausible mechanisms, such             Pinker, S. (1989). Learnability and cognition: The acquisi-
as cross-situational observation and the principle of contrast.        tion of argument structure. The MIT Press.
The model employs a probabilistic learning algorithm that in-        Quine, W. (1960). Word and object. The MIT Press.
crementally updates word meanings based on the observed              Regier, T. (2005). The emergence of words: Attentional
pairings of utterances and scene representations. Our exper-           learning in form and meaning. Cognitive Science, 29, 819–
imental results show that the model can successfully handle            865.
referential uncertainty, and many general patterns in child ex-      Siskind, J. M. (1996). A computational study of cross-
perimental data can be observed and accounted for in our               situational techniques for learning word-to-meaning map-
model. A key property of the proposed model is that its pre-           pings. Cognition, 61, 39–91.
viously acquired knowledge is not corrupted by processing            Tamminen, J., & Gaskell, M. G. (2006). Learning new words:
more input. This makes the model suitable for handling syn-            Effects of lexical competition and age of acquisition. In
onymy and homonymy, which we plan to explore in the fu-                Proc. of CogSci’06.
ture. Also, we have shown that the model is robust against           Theakston, A. L., Lieven, E. V., Pine, J. M., & Rowland, C. F.
noisy data. In the future, we need to add new types of noise           (2001). The role of performance limitations in the acqui-
to the training data (e.g., having words in the utterance whose        sition of verb-argument structure: An alternative account.
meanings do not appear in the scene representation), and eval-         Journal of Child Language, 28, 127–152.
uate the the model under these conditions.                           Xu, F., & Tenenbaum, J. B. (2007). Word learning as
   Our model processes words in the context of a sentence, in          Bayesian inference. Psych. Review, 114(2), 245–272.
contrast to the majority of the existing computational models        Yu, C. (2005). The emergence of links between lexical ac-
that study words in isolation. Having access to the context of         quisition and object categorization: A computational study.
words would enable us to embed additional cues in our learn-           Connection Science, 17(3–4), 381–397.
ing algorithm, such as the word order of the utterance that a        Yu, C. (2006). Learning syntax–semantics mappings to boot-
word appears in. Moreover, we intend to explore syntactic              strap word learning. In Proc. of CogSci’06.
and semantic categorization of words, using the word mean-
                                                                 708

