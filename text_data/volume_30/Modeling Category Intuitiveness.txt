UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Category Intuitiveness
Permalink
https://escholarship.org/uc/item/7q85f0x8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Pothos, Emmanuel M.
Perlman, Amotz
Edwards, Darren J.
et al.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                          Modeling Category Intuitiveness
Emmanuel M. Pothos (e.m.pothos@swansea.ac.uk), Amotz Perlman (amotz@bgu.ac.il), Darren J. Edwards
                                                    (225088@swansea.ac.uk)
                              Department of Psychology, Swansea University, Swansea SA2 8PP, UK
                                        Todd M. Gureckis (todd.gureckis@nyu.edu)
                          Department of Psychology, New York University, New York, NY 10003, USA
                                         Peter M. Hines (peter.hines@cs.york.ac.uk)
                           Department of Computer Science, University of York, York YO10 5DD, UK
                                                Nick Chater (n.chater@ucl.ac.uk)
                         Division of Psychology and Language Sciences, UCL, London WC1E 6BT, UK
                            Abstract                                 of mostly perceptual considerations, whereas category
   We asked 169 participants to spontaneously categorize nine
                                                                     coherence is usually theory-laden.
   sets of items. A category structure was assumed to be more           Several frameworks have been proposed for modeling
   intuitive if a large number of participants consistently          category intuitiveness and an exhaustive comparison would
   produced the same classification. Our results provide a rich      be impractical. We consider SUSTAIN (Love, Medin, &
   empirical framework for examining models of unsupervised          Gureckis, 2004) and the simplicity model (Pothos & Chater,
   categorization—and illustrate a corresponding profound            2002). There are some reasons why this comparison is
   modeling challenge. We provide a preliminary examination          interesting. First, while drawing from radically different
   comparing two models of unsupervised categorization:
   SUSTAIN (Love, Medin, & Gureckis, 2004) and the                   formal specifications, both models suggest that a simplicity
   simplicity model (Pothos & Chater, 2002), and identify some       principle may guide category intuitiveness. In SUSTAIN,
   ways in which the models have to be extended.                     this is achieved via an incremental coverage principle,
                                                                     whereby new knowledge structures are created when items
   Keywords:       unsupervised    categorization;   simplicity;
   SUSTAIN.                                                          are encountered which do not fit well into any existing
                                                                     structure. In the simplicity model, a categorization is
                        Introduction                                 favored to the extent that it provides a ‘simplification’ (in a
                                                                     formal, algorithmic sense) of the similarity structure of the
In unsupervised categorization, there is no pre-determined           presented items. Classifications that are highly complex or
assignment of objects to categories. The participant is free         irregular for a set of items, would not allow much
to decide which classification makes more sense, typically           simplification and are discouraged. In addition, both models
with no or minimal constraints. Research into unsupervised           incorporate a similarity constraint: categories should be
categorization involves several themes, for example, the             more intuitive and easier to learn if they respect the
circumstances under which unidimensional classification              similarity structure of the items to be categorized.
might be observed and the role of general knowledge in                  Despite these similarities, there are important differences
category coherence (e.g., Milton & Wills, 2004; Yang &               as well: SUSTAIN is parametric, in that it assumes that
Lewandowsky, 2004). The focus of the present work is                 items have certain positions in psychological space. In
category intuitiveness, i.e. our ability to recognize certain        contrast, the input to the simplicity model is the set of
groupings of objects as intuitively natural. To pick a trivial       relative similarities, and so its operation is independent of
example, most people consider the grouping of all instances          item representation. Also, SUSTAIN’s operation is guided
of cats into one category as very intuitive. However, a              by a number of free parameters while the simplicity model
category consisting of dolphins, babies born on Tuesdays,            has typically no parameters; for a given input, it produces a
and the Eiffel Tower would be considered nonsensical.                prediction of what should be the most intuitive
Ideally, we would be able to express these intuitions in             classification. Finally, SUSTAIN is a process model of trial-
mathematical terms.                                                  by-trial learning, whereas simplicity assumes all items are
      Category intuitiveness is central in the study of              presented concurrently (although note that both models can
unsupervised categorization, as the spontaneous formation            be adapted to carry out their “non-native” form of
of categories must be guided by a sense in which certain             categorization). In a way, the distinction between these two
groupings are more intuitive than others. However, its study         models reflects the difference between theories developed at
has been problematic because of the very large number of             Marr’s algorithmic level (SUSTAIN) and at the
possible classifications for a set of items. Category                computational/normative level (the simplicity model).
intuitiveness is closely related to Murphy and Medin’s                  The goal of the present article is two-fold. First, we
(1985) notion of category coherence. A possible difference           present results examining the types of category structures
is that category intuitiveness can be established on the basis
                                                                 415

human participants prefer, when asked to spontaneously               fits are derived by running the model thousands of times on
categorize sets of stimuli. Second, we examine SUSTAIN               different stimulus orderings in order to create a distribution
and simplicity as formal accounts of the empirical results.          of plausible classifications: more psychologically intuitive
                                                                     classifications are considered to be the ones more frequently
       Incremental Coverage vs. Information-                         generated.
   Theoretic Simplicity: Comparing SUSTAIN
               and the Simplicity Model                              The Simplicity Model
                                                                     The simplicity model is effectively an implementation of
                                                                     Rosch and Mervis’s (1975) intuition about categorization,
SUSTAIN                                                              within an information-theoretic framework. Rosch and
SUSTAIN is a trial-by-trial clustering model of category
                                                                     Mervis (1975) suggested that basic level categories
acquisition, aiming to capture the full continuum between
                                                                     maximize within- and minimize between-category
supervised and unsupervised categorization. Clusters in the
                                                                     similarity. In Pothos and Chater’s (2002) information-
model correspond to psychologically meaningful groups of
                                                                     theoretic instantiation of this idea, classifications are
items. For example, when learning about categories of birds,
                                                                     considered descriptions of the similarity structure of a set of
a single cluster in the model might represent highly similar
                                                                     items. Where these descriptions afford an economical
species such as robins and blue-jays, as distinct from highly
                                                                     encoding of the similarity structure, they should be
dissimilar examples such as ostriches. SUSTAIN is initially
                                                                     preferred. This is Occam’s razor (the simplicity principle),
directed towards classifications involving as few clusters as
                                                                     which has been argued to have psychological relevance
possible, and only adds complexity as needed to explain the
                                                                     (Chater, 1999; Feldman, 2000) and is congruent with
structure of a category. Two key aspects of SUSTAIN’s
                                                                     Bayesian approaches in cognitive science (Tenenbaum et
account are the role of similarity and surprise in directing
                                                                     al., 2006).
category discovery. First, SUSTAIN favors clusters
                                                                        The simplicity model first computes the information
organized around perceptually or psychologically similar
                                                                     content of all the similarity relations between a set of items,
items. Second, new clusters are created in memory when the
                                                                     by assuming that the similarity for each possible pair of
existing ones do a poor job of accommodating a new
                                                                     items is compared to the similarity of every other pair. For
instance. Thus, SUSTAIN adjusts its category
                                                                     example, are a banana and an apple more/less or similar to a
representations in a trial-by-trial fashion to accommodate
                                                                     banana and an orange? Each such comparison is worth one
the similarity structure of the items it has experienced.
                                                                     bit of information (ignoring equalities). A classification for
   When a to-be-categorized item is first presented to the
                                                                     the items is defined as imposing constraints on the similarity
model, it activates each existing cluster in memory, in a way
                                                                     relations: all similarities between objects in the same
based on the similarity of the item to each cluster. In
                                                                     category are defined to be greater than all similarities
addition, learned attention weights in the model can bias this
                                                                     between objects in different categories. Thus, a
activation in favor of dimensions which are more predictive
                                                                     classification can be evaluated in terms of how many correct
for categorization. Clusters that are more activated are more
                                                                     constraints it provides—erroneous constraints need to be
likely to be selected as the “winner” for the item. If there are
                                                                     identified and corrected. Overall, taking into account the
many highly activated clusters for a particular item, then
                                                                     constraints imposed by a classification, the (information-
confidence in the winning cluster is reduced—i.e., there is
                                                                     theoretic) cost of correcting errors, and another cost term for
cluster competition (regulated by a parameter). In the
                                                                     specifying the classification, we compute the simplification
unsupervised learning situations considered here, if the
                                                                     provided by a particular classification. The prediction is that
current input item fails to activate any existing cluster above
                                                                     the greater this simplification, the lower the codelength of
some threshold level, then a new cluster is created for the
                                                                     the similarity information of the items (when described with
item. This is the key mechanism of ‘surprise’ in SUSTAIN:
                                                                     the classification), and the more psychologically intuitive
new clusters are created in response to surprisingly novel
                                                                     the classification should be.
stimuli that do not fit with existing knowledge structures.
                                                                        The above approach has proved adequate for small
The threshold parameter (τ) controls what level of activation        datasets (Pothos & Chater, 2002). For larger datasets,
is considered ‘surprising’ enough to require a new cluster,          additional assumptions are required. First, some
so that this parameter effectively determines the number of          subclustering may occur. Following Rosch and Mervis
clusters the model creates (τ is analogous to the coupling           (1975), we considered the initial prediction of the simplicity
parameter in the rational model).                                    model as a basic level categorization. Categories in this
   Quantitative fits of SUSTAIN have shown that the                  basic level categorization can be broken down into
model’s operation is not too dependent on exact parameter            subordinates, by considering the items in each cluster as a
values (Love et al., 2004). As a result, in the simulations          new dataset and examining whether their classification
reported here, we reuse a single set of global parameters            affords additional simplification. (This process of
from previous studies and only manipulate the setting of the         subclustering is equivalent to deciding that, e.g., in the
threshold parameter (τ). Given that SUSTAIN is a trial-by-           category of birds there are crows, robins etc.) Subclustering
trial learning model, in modeling a free sorting task where          may corroborate or compete with the final (see later)
multiple items are simultaneously presented, SUSTAIN’s               classification. Second, for stimuli composed of more than
                                                                 416

one dimension, classification may proceed on the basis of                Simplicity model predictions are specified as a
one dimension or both. The simplicity model has no                    percentage. We consider the codelength for the similarity
parameters for attentional weighting, therefore dimensional           structure of the items, without any clusters. Since we have
selection has to take place automatically. Dimensional                16 items in each dataset, this is 7140 bits (in each case).
selection depends on whether the classification along either          Note that this codelength does not take into account any
dimension (classification(x) or classification(y); for                regularity in the similarity structure of the items at all. We
simplicity, call these x and y) is more intuitive than                then consider the final codelength for a particular
classification on the basis of both dimensions (call this xy;         classification; this final codelength would take into account
Pothos & Close, in press). In the present study, participants         any non-competing subclustering. In other words, this is the
were asked to produce a two dimensional classification (i.e.,         codelength of the similarity structure of the items, when
an xy classification). Therefore, the final classification (i.e.,     encoded using categories. For example, in the case of the
the basic level classification plus any further subclustering)        dataset labeled as 3585 (Figure 1), the best possible
has to be an xy one; ‘final classification’ will denote the           classification is associated with a codelength of 3585 bits.
classification the simplicity model predicts for a dataset.           Therefore, the simplicity prediction for this dataset would
This can be achieved in two ways. Participants (or the                be expressed as 3585/7140*100 or 50.2%, indicating that
model) may produce an xy classification straightaway. Or              only about 50% of the original codelength is required for
participants may first produce an x or a y classification, and        describing the similarity structure of the items of this dataset
then produce an xy one, by stable (i.e., not susceptible to           with categories, compared to the situation where no
noise; see later) subclustering (we assume this is the only           categories are used. The lower this percentage, the greater
way in which subclustering can affect the form of the final           the simplification afforded by the classification, and the
classification).                                                      more intuitive the corresponding classification is
   In sum, in describing the results with the simplicity              considered. The qualification to this conclusion relates to
model, an assumption is that the final classification is xy,          the ‘competition’ term: competition terms are computed in a
i.e., most participants will look for an xy classification.           way analogous to the above, and they correspond to how
However, not all participants will produce the optimal xy             intuitive ‘competing’ subclusters are. Accordingly, the
classification. Why would they not do this? Because there             lower the competition term, the more intuitive competing
might be competition from salient subclusters along either x          subclusters are, and the less frequently the optimal
or y. A subclustering is considered to compete with the final         classification should be produced.
classification if it occurs either along x or y. Because the
final classification is assumed to be xy, x or y subclusters                        Experimental investigation
are considered to inhibit the salience of the xy classification.
The more salient these subclusters, the more the                      Materials
corresponding inhibition. Alternatively, there might be xy
subclusters which are susceptible to noise. Susceptibility to         We created nine datasets of 16 items each. Our approach
noise means that by introducing a little bit of noise in the          was exploratory, i.e., we chose datasets reflecting a range of
similarity structure of the items the classification changes.         intuitions about unsupervised categorization (Figure 1),
This is rarely the case with a basic level categorization, i.e.,      rather than attempt to motivate predictions in an a priori
such classifications are typically very stable against noise.         way. Such an approach was deemed appropriate both
However, when subclustering a cluster, introducing a little           because of the complexity of the models and the lack of
bit of noise (not more than 10% in psychological space                other relevant data. Each dataset is indexed by its
positions), often leads to alternative classifications, if the        codelength (with no subclustering or dimensional selection).
items in the cluster are close to each other. When a                  Items were instantiated as spider-like images (but with six
(sub)clustering is susceptible to noise, we consider it as            instead of eight legs; Figure 2), so that length of body
competing with the final classification.                              corresponded to the horizontal dimension in Figure 1 and
   Overall, subclustering and noise may lead to competition,          length of ‘legs’ (after the joint) to the vertical dimension. By
which increases classification variability. A competition             choosing such stimuli, both dimensions of physical variation
term is computed as the best codelength of the competing              were lengths, and so a Weber fraction in mapping the Figure
subclusters. Finally, competition may also arise if there is          1 values to physical values could be safely assumed (8%).
more than one salient xy final categorization (this only              By collecting similarity ratings and doing multidimensional
happens in the 5202 dataset below).                                   scaling, we verified that our representational assumptions
                                                                      are valid. Stimuli were printed individually and laminated.
                                                                  417

   Figure 1: The datasets in the present study, labeled according to the codelength of the best ‘basic level’ classification (the
  actual prediction of the simplicity model will depend on subclustering and dimensional selection as well). In parentheses is
                                     shown the frequency of the most popular classification.
                                                                      variation in classification strategy). Moreover, it is not
                                                                      clear how category intuitiveness can be modeled by
                                                                      considering individual classifications. Therefore, we
                                                                      examined classification variability (diversity): why in
                                                                      some datasets there were as few as 84 distinct
                                                                      classifications, while in others 160? Lower classification
                                                                      variability in a dataset means that more participants agree
                                                                      on how to classify the dataset, so that the corresponding
                                                                      classification(s) must be more intuitive. Alternatively, we
          Figure 2: An example of the stimuli used.                   could count the frequency of the most popular
                                                                      classification for a dataset. If the most popular
Participants and procedure                                            classification has a high frequency, then it should be the
                                                                      case that this classification is considered more obvious. In
Participants were 169 students at Swansea University,                 our results, the two measures are equivalent (correlation:
who took part for a small payment. They received each                 0.99), therefore, we shall consider only frequency of most
set of items in a pile. They were asked to spread the items           popular (of course, in general this may not be the case).
in front of them, and classify the items in a way that                Table 1 shows the empirical results and illustrates the
seemed natural and intuitive, using as many groups as                 complexity of research into unsupervised categorization.
they wanted, but not more than necessary. The two
dimensions of variation were described and presented as                   Table 1: ‘Fr of most popular’ refers to the number of
equally important. There was an alternative set of                    participants who produced the most popular classification,
instructions, where the stimuli were described as spiders                 ‘Distinct’ to the number of distinct classifications.
in the Amazon; this was a ‘general knowledge’                           Codelength, competition values refer to the simplicity
manipulation, which, however, had no effect; data were                                           model fit.
pooled. Participants indicated their classification by
arranging the stimuli into piles. Each participant went                       Dataset       Fr. most       Codelength (%) -
through all nine datasets, in a random order.                                               popular -      competition (%)
                                                                                            distinct
Results                                                                       3585          31 - 124       50.2 – 66
In an experiment of this sort there is clearly a wealth of                    3569          33 - 116       50 - 62.4
data. Analyzing actual categorizations does not appear a                      3585s         8 - 152        50.2 – 52.9
fruitful approach, since there were over 1100 unique                          4128          17 - 141       57.8 – 61
classifications (many of which appear to reflect random                       4201          55 - 104       43.5 – 68.2
                                                               418

        4244          3 - 160      59.4 – 52.9                    Furthermore, since no further subclustering is possible in
        5150          3 - 159      72 - 60.2                      xy (subclusters have very poor codelength, 92.5%), the
        5202          2 - 164      60.6 – 69.7                    final, predicted, classification for this dataset is (0 1 2 3 4
        5347          58 - 84      57 – 100                       5 6 7) (8 9 10 11 12 13 14 15). With respect to
                                                                  competition for this solution, there are very good
SUSTAIN Results                                                   subclusters along x or y, each one of which is associated
Following previous simulations of unsupervised sorting            with a codelength of 66%; so, the competition term for
tasks with SUSTAIN (Gureckis & Love, 2002; Love,                  this dataset is 66% (recall, the competition value is the
Medin, & Gureckis, 2004), the model was applied to the            lowest codelength corresponding to any subclustering;
sorting task in a trial-by-trial fashion. In order to             these subclusters are competing, rather than part of the
approximate the free-sorting task with SUSTAIN, we                predicted classification, because we assume that the
make the assumption that subjects consider each stimulus          predicted classification is xy). So, even though, in this
one at a time but that the order of item consideration is         case, we have a very intuitive final classification
idiosyncratic (averaging results across different                 (codelength of only 50.2%), there is considerable
presentation orders is equivalent to assuming concurrent          competition, suggesting there would be some noise.
presentation of all the stimuli). Accordingly, SUSTAIN               The simplicity model can account for the superiority of
was given 5 blocks of training, each block consisting of a        the 4201 and 5347 datasets because in both cases the
different random ordering of all stimuli. Stimuli were            basic level categorization is initially 1D. Therefore, xy
represented to the model as coordinate pairs. Input values        subclustering provides additional simplification, rather
along each dimension were scaled between 0.0 and 1.0.             than competition (recall, the final classification has to be
Attention for both dimensions was set to an initial value         xy). For the 5202 dataset the basic level categorization is
of λ= 1.0, but during the learning phase SUSTAIN could            also one dimensional and xy subclustering provides
adjust this value. Since subjects were encouraged to use          additional simplification; however, there is also an
both dimensions while sorting we assumed attention was            alternative, competing final xy classification with
equally allocated to both x and y (akin to the xy bias in         comparable codelength. Table 1 provides a list of
the simplicity model). After the learning phase, we               codelengths and competition values for the datasets. A
examined the structure of SUSTAIN’s clusters by probing           linear regression analysis with codelength and
which items the model assigned to the same clusters in            competition as the independent variables, and frequency
memory (i.e., items that strongly activated the same              of the most popular classification as the dependent
cluster were considered to be psychologically grouped).           variable, was significant (F(2, 6) = 14.5, p = .005,
   Figure 3 shows the results of SUSTAIN’s basic                  R2=.83); however, the correlation is not perfect, indicating
predictions (scaled by multiplying probability of                 that there is room for improvement in the simplicity
classification by 196). In order to account for the               approach. Also, the balance between competition and gain
variability of responses by subjects, in the simulations          was governed by two (regression) parameters, but in the
reported here we assumed that the τ parameter varied              future it would be desirable to specify it automatically.
from person to person following a roughly normal                     Note that it is important to check that the classifications
distribution (mean and SD were treated as free parameters         predicted by SUSTAIN and the simplicity model are
for each dataset). Remember that the τ parameter                  indeed the ones preferred by participants. This is indeed
determines how dissimilar an item has to be from an               the case for the datasets for which there was a strong
existing cluster in order to warrant creating a new cluster       preference for a particular classification (3585, 3569,
in memory. All other free parameters in the model were            4201, 5347, 4128). In general, the use of an index of
recycled from a single global set of parameters used in           classification similarity (such as the Rand Index; see also
previous studies. SUSTAIN provides a good account of              Haslam, Wills et al., 2007) further confirms that the
the results (Figure 3). For example, SUSTAIN (like                model predictions are consistent with empirical data.
simplicity) correctly predicts that dataset 4201 and 5347
should have the most agreement while also predicting                                  Discussion
little consistency in responding for problems 4244, 5150,         With 16 items there are well over 100,000 potential
and 5202.                                                         classifications. The immense size of this space, along with
                                                                  the fact that few constraints were given to participants in
Simplicity Model Results                                          our spontaneous classification task, suggests that
We illustrate the simplicity model fit with the 3585              idiosyncratic variation (assumptions about the stimuli,
dataset and highlight aspects of its account for the other        processing biases etc.) have plausibly played a significant
datasets. Observing Figure 1 for item id numbers, the             role in determining the distribution of classifications.
basic level categorization in xy is (0 1 2 3 4 5 6 7) (8 9 10     However, despite this variability, there were datasets for
11 12 13 14 15), with a codelength of 50.2%; the x, y             which more than 30% of participants agreed on which
basic level categorizations are the same, so we select as         classification is the best and datasets for which no more
xy the basic level categorization (since it is assumed that       than three participants agreed on an optimal
participants are biased to produce xy classifications).
                                                              419

                                                     70
                                                     60
                       Frequency of 'most popular'
                                                     50
                                                     40
                                                                                                                                       Empirical data
                                                     30
                                                                                                                                       Simplicity Model
                                                     20
                                                                                                                                       SUSTAIN
                                                     10
                                                      0
                                                     -10   3585   3569   3585s   4201    4244     4128     5347    5150   5202
                                                     -20
                                                                                        Dataset
 Figure 3: The y-axis shows the frequency of the most popular classification in different datasets. The frequencies correspond either to the
                              observed participant results or to the predictions of the computational models.
classification. We consider this variability extremely
interesting and an exciting, novel, and important challenge                                                                      References
for models of unsupervised categorization.                                                              Chater, N. (1999). The Search for Simplicity: A Fundamental
   In our preliminary analysis, we examined two models of                                                 Cognitive Principle? Quarterly Journal of Experimental
unsupervised category construction that draw from                                                         Psychology, 52A, 273-302.
somewhat different formalisms. While both SUSTAIN and                                                   Feldman, J. (2000). Minimization of Boolean complexity in human
the simplicity models have broad empirical support, neither                                               concept learning. Nature, 407, 630-633.
has been tested against such an extensive range of                                                      Gureckis, T.M., Love, B.C. (2002). Who says models can only do
unsupervised categorization data. The fact that both models                                               what you tell them? Unsupervised category learning data, fits,
provide a reasonable account of the classification behavior                                               and predictions. In Proceedings of the 24th Annual Conference
                                                                                                          of the Cognitive Science Society. Lawrence Erlbaum: Hillsdale,
of human participants in our task is encouraging and argues
                                                                                                          NJ.
favorably for the relevance of simplicity and similarity as                                             Haslam, C., Wills, A.J., Haslam, S.A., Kay, J., Baron, R. and
appropriate constraints in unsupervised categorization.                                                   McNab, F. (in press). Does maintenance of colour categories
Moreover, both models appear to have some difficulty over                                                 rely on language? Evidence to the contrary from a case of
the same range of datasets (5347, 5150, 5202). It is possible                                             semantic dementia. Brain and Language.
that SUSTAIN and the simplicity model reflect different                                                 Love, B. C., Medin, D. L., & Gureckis, T. M. (2004). SUSTAIN:
ways of computationally implementing (at the algorithmic                                                  A network model of category learning. Psychological Review,
and computational level respectively) simplicity/similarity                                               111, 309-332.
in unsupervised categorization. Much work remains before                                                Milton, F. & Wills, A. J. (2004). The influence of stimulus
                                                                                                          properties on category construction. Journal of Experimental
this potentially important conclusion can be confirmed.
                                                                                                          Psychology: Learning, Memory, and Cognition, 30, 407-415.
   With respect to the simplicity model, the roles of                                                   Murphy, G. L. & Medin, D. L. (1985). The Role of Theories in
subclustering and stability against noise need be better                                                  Conceptual Coherence. Psychological Review, 92, 289-316.
integrated with the main foundation of the model. Likewise,                                             Pothos, E. M. & Chater, N. (2002). A Simplicity Principle in
regarding SUSTAIN, more work is needed to understand                                                      Unsupervised Human Categorization. Cognitive Science, 26,
the full distribution of preferred groupings. For example, in                                             303-343.
some cases, SUSTAIN correctly predicted the relative                                                    Rosch, E. & Mervis, B. C. (1975). Family Resemblances: Studies
prevalence of the most popular solution generated by human                                                in the Internal Structure of Categories. Cognitive Psychology, 7,
participants, however on a few occasions the model showed                                                 573-605.
                                                                                                        Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-
a bias towards alternative groupings that participants did
                                                                                                          based Bayesian models of inductive learning and reasoning.
often not select. Extending the models in this way is a                                                   Trends in Cognitive Sciences, 10, 309-318.
prerequisite for more detailed comparisons between them.                                                Yang, L. & Lewandowsky, S. (2004). Knowledge partitioning in
   As has been the case in supervised categorization, we                                                  categorization: Constraints on exemplar models. Journal of
hope that comparative studies like the present one will also                                              Experimental Psychology: Learning, Memory, and Cognition,
help guide the development of computational models in                                                     30, 1045-1064.
unsupervised categorization.
                   Acknowledgments
This research was supported by ESRC grant R000222655 to
EMP and NIMH training grant T32 MH019879-12 to TMG.
We would like to thank Brad Love for his help.
                                                                                                  420

