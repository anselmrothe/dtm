UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Connectionist Simulation of Structural Rule Learning in Language Acquisition
Permalink
https://escholarship.org/uc/item/5jt6r2n2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Laakso, Aarre
Calvo, Paco
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

   A Connectionist Simulation of Structural Rule Learning in Language Acquisition
                                               Aarre Laakso (alaakso@indiana.edu)
                                 Department of Psychological & Brain Sciences, 1101 E. 10th Street
                                                       Bloomington, IN 47405 USA
                                                     Paco Calvo (fjcalvo@um.es)
                                          Departamento de Filosofía, Universidad de Murcia
                                                            Murcia, 30100 Spain
                             Abstract                                    nonadjacent syllables are matched, such that the transitional
  According to a dual-mechanism hypothesis, although                     probability between an Ai and the following Ci is 1.0. There
  statistical computations based on nonadjacent transitional             are three X syllables, so the transitional probabilities
  probabilities may suffice for speech segmentation, an                  between an Ai and an intermediate X and between an X and
  additional rule-following mechanism is required in order to            the final Ci are each 0.33. There are three word classes, and
  extract structural information out of the linguistic stream. We        no two adjacent words in the speech stream may be from the
  present a neural network study that shows how statistics alone
  can support the discovery of structural regularities, beyond           same class, so the transitional probability between the final
  the segmentation of speech, disconfirming the dual-                    syllable of one word Ci and the first syllable of the next
  mechanism hypothesis.                                                  word Aj is 0.5. The three word classes are pu…ki, be…ga
                                                                         and ta…du. The three filler syllables are li, ra and fo. Thus,
   Keywords: language acquisition; statistical            learning;      the A1XC1 family consists of the words puliki, puraki and
   nonadjacent dependencies; neural networks.                            pufoki, the A2XC2 family consists of the words beliga,
                                                                         beraga and befoga, and the A3XC3 family consists of the
                         Introduction                                    words talidu, taradu and tafodu. Ten-minute and two-
Language acquisition is a central component of human                     minute familiarization streams were produced by
development. A key question is whether language can be                   concatenating tokens of these nine words, randomly selected
acquired solely by domain-general (e.g., statistical) learning           subject to the constraints that (a) a word of a given family
mechanisms or whether domain-specific (e.g., algebraic)                  could not be immediately followed by another word of the
learning mechanisms are required. Peña, Bonatti, Nespor, &               same family, and (b) a word with a given intermediate
Mehler (2002) reported experimental evidence from French-                syllable could not be immediately followed by another word
speaking adults that they argued shows that humans use                   with the same intermediate syllable. In human experiments,
both statistical learning (to segment speech) and algebraic              the streams were converted to synthesized speech.
computations (to induce structural regularities such as                     In the experiments reported in Peña et al. (2002),
grammatical rules). Subsequently, Endress & Bonatti (2007)               participants were asked to choose, after familiarization,
replicated and extended the Peña et al. results with Italian-            between pairs of stimuli that could belong to three kinds of
speaking adults and attempted to model them using                        test items: words, part words and rule words. (In their
connectionist networks. Endress & Bonatti argued that their              Experiment 1, subjects had to choose between a word and a
failure to model the experimental results with connectionist             part word, after having been familiarized for 10 minutes to a
networks       demonstrated       that    associative       learning     continuous stream. In Experiments 2 and 3, participants had
mechanisms were insufficient for language learning. In this              to choose between a part word and a rule word, after 10
paper, we report a set of connectionist simulations that does            minutes of familiarization on either a continuous or a
model the experimental results. We conclude that Peña et al.             segmented stream, respectively.) The “words” were simply
and Endress & Bonatti have not demonstrated that rule-                   items of the form AiXCi, that is, words that had appeared in
governed structure learning mechanisms are necessary for                 the familiarization stream. The “part words” were also items
language acquisition.                                                    that had appeared in the familiarization stream, but ones that
                                                                         straddled a word boundary. As Endress & Bonatti point out,
Peña et al.’s experiments                                                these part words are of two types: “type 12” part words
The experiments in question test adult speakers’ ability to              consist of items having the form CiAjX, whereas “type 21”
(1) segment speech based on non-adjacent dependencies,                   part words consist of items having the form XCiAj. The type
and (2) generalize beyond the familiarization corpus. The                12 part words are dubefo, dubeli, dubera, dupufo, dupuli,
experiments were based on roughly the same method as                     dupura, gapufo, gapuli, gapura, gatafo, gatali, gatara,
Newport & Aslin (2000). The artificial language consists of              kibefo, kibeli, kibera, kitafo, kitali and kitara. The type 21
“words” that have the form AiXCi, where Ai, X and Ci are                 part words are foduga, foduki, fogapu, fogata, fokidu,
syllables. The subscripts on A and C indicate that the                   fokiga, lidube, lidupu, ligapu, ligata, likibe, likita, radube,
                                                                     709

radupu, ragapu, ragata, rakibe and rakita. The “rule words”          mechanism for segmenting the familiarization corpus
have the form AiX′Ci, where X′ indicates a syllable that had         (Experiment 1), and a rule-governed mechanism that
appeared in the speech stream but never in the middle of a           accounts for the induction of the rule that prefers rule words
word. The rule words are beduga, bekiga, bepuga, betaga,             over part words (Experiments 3 and 5).
pubeki, puduki, pugaki, putaki, tabedu, tagadu, takidu and
tapudu. These are called “rule words” because participants               Table 1: Summary of Peña et al.’s experimental results
who learned rules of the form “if the first syllable is Ai, then              (w = word; pw = part word; rw = rule word).
the last syllable is Ci” should find them familiar. The rule is
not merely a description of the statistical regularities in the      Exp.      Stream           Duration famil    Test choice
familiarization stream because none of the rule words                1         Continuous       10’               w over pw
actually appeared in the familiarization stream. Thus, testing       2         Continuous       10’               no pref rw/pw
participants on part words versus rule words provides a              3         Segmented        10’               rw over pw
means of testing whether they prefer sequences that had              4         Continuous       30’               pw over rw
occurred in the familiarization stream but did not conform           5         Segmented        2’                rw over pw
to word boundaries or words that had not occurred in the
familiarization stream but conformed to a rule.
   Peña et al.’s Experiment 1 supports the hypothesis that           Endress & Bonatti’s experiments
statistics allow for speech segmentation (subjects preferred         Endress and Bonatti (2007) go a step further and argue that
words over part words). They claim that their Experiment 2,          subjects may not prefer rule-words themselves, but so-called
however, shows that statistics are not sufficient to extract         “class words”, which involve a higher level of abstraction.
structural information in a continuous familiarization corpus        Class words have the form AiX′Cj, that is, an A syllable
(subjects preferred part words over rule words). In their            from one class, followed by a syllable that had appeared in
Experiment 3, a “subliminal” 25ms pause was inserted                 the speech stream but never in the middle of a word,
between each pair of words in order to relieve subjects of           followed by a C syllable from a different class. They are
some of the burden of segmenting the speech stream,                  called “class words” because they would be preferred if
potentially allowing them to find the structural regularity as       participants learned rules of the form “if the first syllable is
well. Indeed, although participants reported no awareness of         from the A class, then the last syllable is from the C class”
the gaps, they did affect the results. Specifically, Peña et al.     (where the A class comprises syllables A1, A2 and A3, and
found that, when participants were trained on a speech               the C class comprises syllables C1, C2 and C3). The class
stream with gaps, the participants subsequently preferred            words are beduki, bekidu, bepudu, bepuki, betadu, betaki,
rule words to part words at test, supporting the claim that          pubedu, pubega, puduga, pugadu, putadu, putaga, tabega,
segmentation may be accomplished statistically, but                  tabeki, tagaki, takiga, tapuga and tapuki.
identifying structural regularities requires a separate                 As Endress & Bonatti note, the experimental results from
mechanism. In an important footnote (#27), Peña et al.               Peña et al. (2002) highlight a negative correlation between
report a control experiment intended to dismiss the                  structural generalization and familiarization duration.
possibility that a single statistical mechanism could be             Likewise, in the case of class words, Endress & Bonatti
responsible for the preference for rule words found in their         assume that following algebraic computations results in a
Experiment 3. In this control, participants were not only            preference for generalization in the case of shorter
familiarized with words with gaps, but also tested with              familiarization durations, whereas a statistical mechanism
words with gaps. The presence of the pause at test makes             should take longer to generalize. So, they predict that
the transitional probabilities of part words higher than that        preference for class words will decrease for longer
of rule words. Nevertheless, participants preferred rule             familiarization durations. The following table summarizes
words to part words. Experiments 4 and 5 tested for                  some of Endress & Bonatti’s experimental results.
preference between part words and rule words after
familiarization on a continuous and a segmented stream for                  Table 2: Summary of Endress & Bonatti’s results
30 and 2 minutes, respectively (see Table 1). In the first                                 (cw = class word).
case, subjects preferred part words over rule words. In the
second case, they preferred rule words over part words.              Exp.      Stream           Duration famil    Test choice
Peña et al. suggest that, for a short exposure, the rule-            1         Segmented        10’               cw over pw
governed mechanism for extracting structural regularities            2         Continuous       10’               no pref cw/pw
dominates, whereas for a longer exposure, memory traces              3         Segmented        2’                cw over pw
for particular sequences in the familiarization stream are           4         Segmented        30’               no pref cw/pw
strong enough to override the rule-based mechanism.                  5         Segmented        60’               pw over cw
   In summary, Peña et al. interpret the results of Table 1 as       8         Segmented        2’                w over rw
evidence for a dual-mechanism hypothesis: a statistical              12        Segmented        2’                rw over cw
                                                                 710

   Endress and Bonatti next report a set of studies with             activations are close to zero. Moreover, because the delta
artificial neural networks that they claim shows that a              term in backpropagating sum-squared error involves a
Simple Recurrent Network, or SRN (Elman, 1990) cannot                multiplication by the derivative of the activation function
account for the preference for class words exhibited by              (the “sigma prime term”), training slows down dramatically
humans in their experiments. In what follows, we report a            whenever the output approaches 0 or 1, regardless of the
set of SRN studies that does model the experimental results.         target value (because the derivative of the sigmoid
                                                                     approaches 0 in both cases). The usual procedure for
                            Study 1                                  problems using a 1-of-c encoding is to use the softmax
The first simulation study was designed to find a set of             activation function at the output units combined with the
network parameters that could learn the familiarization              cross-entropy objective function (see Bishop, 1995 for
sequence quickly. Like Endress & Bonatti, we used an SRN.            justification and additional details). The softmax activation
(Space is too limited here to introduce the basic principles         function causes the activations of the output units to always
of connectionist modeling; the interested reader may consult         sum to unity, which is correct in the case of a 1-of-c
Bishop, 1995.) The syllables were coded as nine- or ten-bit          encoding; a side effect is that one may treat output
pairwise orthonormal binary vectors (a “1-of-c” encoding).           activations as the network’s subjective assessments of the
Networks trained without gaps in the input stream had nine           probability that each output unit codes for the right category
input units. Those trained with gaps had ten input units, the        on a given input pattern. Using the cross-entropy objective
tenth representing the gap. Presenting a word to the network         function causes the sigma prime term to drop out of the
consisted of sequentially presenting each of its three               calculation of delta values, ensuring that weight updates
syllables. Networks had the same number of output units as           approach zero only as the activation value approaches the
input units and were trained to predict the next syllable from       target value.
each syllable presented as input.                                       Thus, we ran a second set of simulations using the
   Endress & Bonatti do not report the activation functions          softmax activation function at the output layer and the cross
or objective function used in their simulations. In our first        entropy objective function. For networks with nine input
set of simulations, we used the standard sigmoid activation          units (those trained without gaps in the input), we stopped
function at both hidden-layer units and output-layer units,          training when networks got at least 33% of the training
together with the sum-squared error objective function. In           patterns right, because only ⅓ of the syllables are
an effort to follow Endress & Bonatti as closely as possible,        deterministically predictable (by the nonadjacent
we trained 20 different “subjects” (networks starting with           dependency). Networks with 27 hidden units reached this
different initial random weights) with five hidden units at          criterion in fewer than 300 epochs on average (N=20,
each combination of the following learning parameters:               M=259.3, SD=79.2). Even at 8000 epochs of training,
epochs ∈ (10, 50, 90, 100, 500), learning rate ∈ (0.00001,           networks with five hidden units had learned only about 10%
0.00005, 0.00009, 0.0001, 0.0005, 0.0009, 0.001, 0.005,              of the patterns on average, and none of them had reached
0.009, 0.01, 0.05, 0.09, 0.1, 0.5, 0.9) and momentum                 criterion (N=5, M=10.22, SD=4.69).
∈ (0.1, 0.5, 0.9). In this and all subsequent studies reported          Trained networks were tested on five item types: training
here, the weights were held constant during testing. Not one         words (N=9), part words of type 12 (N=18), part words of
of these networks learned the problem well enough to get             type 21 (N=18), rule words (N=12) and class words (N=18).
even a single output pattern correct within a tolerance of 0.2       The cosine similarity measure was recorded between the
(i.e., all units with target 0 having activations of 0.2 or less     third syllable of the test item and the network output
and the unit with target 1 having an activation of 0.8 or            activation in response to the second syllable of the test item.
greater). In addition to the parameters we sampled, Endress          We then performed an ANOVA on the cosine values, with
& Bonatti also trained networks with five hidden units for           item type (training, part word type 12, part word type 21,
900, 1000 and 5000 epochs and networks with 27 hidden                rule word and class word) as a between subjects factor.
units on all combinations. It is possible that, had we tried
networks with 27 hidden units or trained for a larger number         Results
of epochs, we would have found networks that could                   The ANOVA showed a significant effect of item type
perform the task. However, we suspect that the problem lies          (F(4,19)=96.014, p<0.001). Bonferroni-adjusted post-hoc
elsewhere.                                                           comparisons showed that the differences between part
                                                                     words of type 12, part words of type 21 and rule words were
                            Study 2                                  not significant (p>0.05) but all other differences were
There is a well-known issue with using sigmoid output units          significant (p<0.001). See Figure 1.
and the sum-squared error function to train networks on
problems where the target patterns are mostly zeros. Such            Discussion
networks easily find a local minimum of the sum-squared              The results of our Study 2 accurately model the behavior of
error function by adjusting weights so that all output unit          human participants in Experiments 1 and 2 of Peña et al.
                                                                 711

(2002), listed in Table 1. Specifically, Peña et al. found that                            than that of part words. This means that participants in the
human participants preferred training words to part words                                  control experiment may be computing statistical information
(their Experiment 1) but exhibited no preference for rule                                  about segmentation gaps. The prediction would be that they
words over part words (their Experiment 2).                                                should favor rule words over part words, which is exactly
                                                                                           what happens in Peña et al.’s control experiment.
                                        1.0
                                        0.8
                                                                                           Results
                Mean cosine to target
                                                                                              For networks tested without any gaps in the test items
                                        0.6
                                                                                           (Figure 2), the ANOVA showed a significant effect of item
                                        0.4
                                                                                           type (F(4,19)=370.49, p<0.001). Bonferroni-adjusted post-
                                                                                           hoc comparisons showed that all differences were
                                        0.2
                                                                                           significant (p<0.001), except for the difference between
                                        0.0
                                                                                           class words and part words of type 12. For networks tested
                                              class   part12 part21   rule   train
                                                                                           with a gap before every test item (Figure 3), the ANOVA
                                                                                           again showed a significant effect of item type
                                                        Test word type
                                                                                           (F(4,19)=1123.9, p<0.001). Bonferroni-adjusted post-hoc
                                                                                           comparisons showed that all differences were significant
  Figure 1: Mean cosine similarity between network outputs
                                                                                           (p<0.001), except for the difference between class words
and target syllables for networks trained without gaps. Error
                                                                                           and part words of type 21 (p=0.058). For networks tested
           bars in all figures show standard error.
                                                                                           with gaps within part words (Figure 4), the ANOVA again
                                                Study 3                                    showed a significant effect of item type (F(4,19)=468.6,
                                                                                           p<0.001). Bonferroni-adjusted post-hoc comparisons
In this study, we aimed to determine whether SRNs can
                                                                                           showed that all differences were significant (p<0.001).
exhibit a preference for rule words over part words, when
trained on a corpus that contains subliminal gaps (Peña et al.                                                                     1.0
Experiment 3; see Table 1). For networks with ten input
units (those trained with gaps in the input), we stopped                                                                           0.8
                                                                                                           Mean cosine to target
training when they got at least 50% of the training patterns                                                                       0.6
right, because the gaps, which followed every C syllable (¼
of the input patterns) were deterministically predictable                                                                          0.4
from the preceding syllable, and ⅓ of the remaining                                                                                0.2
syllables (the C syllables themselves) were deterministically
predictable by the nonadjacent dependency. The networks                                                                            0.0
trained with gaps learned the problem about twice as                                                                                     class   part12 part21   rule   train
quickly as those without gaps, achieving criterion in about                                                                                        Test word type
150 epochs on average (N=20, M=153.5, SD=31.39).
   The trained networks were tested in several ways. First,                                     Figure 2: Results for networks trained with gaps and
they were tested in exactly the same way as those in Study                                                     tested without gaps.
2; in particular, no gaps were used before or within the test
items. Second, they were tested with a gap at the beginning                                Discussion
of every test item. Third, they were tested with a gap before                              The results of our Study 3 accurately model the behavior of
every reliable A syllable. In this third case, at test, the                                human participants in Experiment 3 of Peña et al. (2002). In
training words, rule words and class words began with a                                    our simulations, even networks tested with gaps within part
gap, whereas the part words contained gaps between the                                     words exhibited a preference for rule words over part words,
first and second syllables (in the case of part words of type                              modeling the human behavior in the control experiment
12) or between the second and third syllables (in the case of                              reported in Footnote 27 of Peña et al. (2002). Thus, it is not
part words of type 21). The third testing regime was                                       necessary to suppose that non-statistical computations,
designed to emulate the task reported by Peña et al. (2002)                                “possibly of an algebraic or rule-governed nature…are
in their Footnote 27 and also simulated by Endress &                                       responsible for the observed behavior” (Peña et al., 2002, p.
Bonatti (2007) in a similar way. In test items with                                        606).
segmentation gaps, transitional probabilities for part words                                  Moreover, networks trained with gaps by our technique
become higher than those for rule words, considering only                                  also exhibit a reliable preference for class words over part
adjacent transitional probabilities. However, once                                         words of one or the other type. Networks tested without
nonadjacent transitional probabilities are taken into account,                             gaps prefer class words over part words of type 21, a result
the transitional probability of rule words becomes higher                                  that Endress & Bonatti also reported for some of their
                                                                                     712

networks trained with gaps and tested without. Endress &                                    indicate preferences for words over rule words, for rule
Bonatti dismissed this result because it predicted that,                                    words over class words, and for class words over part words
although human beings might prefer class words to part                                      of types 12 and 21. To demonstrate that an SRN can model
words of type 21, they would not prefer class words to part                                 this pattern of preferences, we trained 20 networks with ten
words of type 12, a result not observed in any of their                                     input units each for 30, 60, 90, 120, and 150 epochs of
experiments. However, our networks tested with gaps before                                  training, and tested them with a gap at the beginning of
test items prefer class words over part words of type 12, a                                 every test item. The goal was to determine, first, if class
reversal in the type of part words to which class words were                                words are preferred over both types of part words, and,
preferred. The distinction between testing networks on part                                 second, if the rank-order preference found by Endress &
words that are preceded by gaps and testing networks on                                     Bonatti (words > rule words > class words > part words) can
part words that are not preceded by gaps cannot be                                          be modeled statistically.
reproduced in the experimental procedure used with human
                                                                                                                                     1.0
beings – because the procedure involves comparing two
different words presented separately, it is indeterminate                                                                            0.8
                                                                                                             Mean cosine to target
whether the test words are “preceded by a gap” in the
relevant sense. Some participants may even subconsciously                                                                            0.6
align their calculations of transitional probabilities on the                                                                        0.4
initial syllables of test words (which according to our
simulations would lead to a preference for class words over                                                                          0.2
part words of type 21), whereas others subconsciously align
                                                                                                                                     0.0
their calculations of transitional probabilities on the silences                                                                           class   part12 part21   rule   train
that precede test words (which according to our simulations
                                                                                                                                                     Test word type
would lead to a preference for class words over part words
of type 12). Although Endress & Bonatti report no
                                                                                                 Figure 4: Results for networks trained with gaps and
difference in the population mean responses to tests of class
                                                                                             tested with gaps within part words and before other items.
words versus the two types of part words, they do not report
whether there are individual differences in preferences for
class words over part words of type 12 versus part words of                                 Results
type 21. Finally, although networks tested with a gap before
every test item (which clearly prefer class words to part                                      The mean performance for each test item type is plotted
words of type 12) do not prefer class words to part words of                                as a function of the number of epochs of training in Figure
type 21 at the alpha=0.05 significance level, there is                                      5. For networks trained for 120 epochs, an ANOVA showed
definitely a trend in that direction. It may be that networks                               a significant effect of item type (F(4,19)=586.66, p<0.001)
trained somewhat less or more would exhibit a reliable                                      and Bonferroni-adjusted post-hoc comparisons showed that
preference for class words over both types of part words, a                                 all differences were significant (p<0.001), except the
possibility that we explore in Study 4.                                                     difference between training words and rule words (p=0.18).
                                         1.0                                                Discussion
                                         0.8
                                                                                              Overall, the results of our Study 4 model the behavior of
                                                                                            human participants in Experiments 3, 8 and 12 of Endress &
                 Mean cosine to target
                                         0.6                                                Bonatti (2007). Networks trained for 30 epochs prefer class
                                         0.4
                                                                                            words over part words of both types, modeling the human
                                                                                            behavior in their Experiment 3. However, no preference is
                                         0.2                                                observed between class words, rule words and training
                                                                                            words. Thus, networks trained for only 30 epochs fail to
                                         0.0
                                                                                            match Endress & Bonatti’s rank-order preference, because
                                               class   part12 part21   rule   train
                                                                                            there are no differences between performance on class
                                                         Test word type
                                                                                            words, rule words and training words (Experiments 8 and
                                                                                            12). However, as the networks are probed after 60, 90, 120
     Figure 3: Results for networks trained with gaps and                                   and 150 epochs of training, performance on class words
          tested with a gap before every test item.                                         declines, while performance on training words improves
                                                                                            more quickly than performance on rule words. Although the
                                                 Study 4                                    differences between training words and rule words in Figure
Endress & Bonatti’s (2007) results on segmented 2-minute                                    5 are not statistically significant, it is clear that the trend is
familiarization streams (Experiments 3, 8 and 12 in Table 2)                                toward better performance on training words than rule
                                                                                      713

words. Moreover, the results of our Study 3 demonstrate                 Endress & Bonatti for their experiments 3, 8 and 12. The
that, in networks that have learned to predict the training             manipulation of training epochs, and the presentation of the
patterns, performance on rule words is reliably lower.                  results over time in Figure 5, was merely to find the right
                                                                        amount of training for a network to model the human
                                                                        behavioral data. It is true, however, that the performance of
                                                                        our networks over time does not completely model the
                       0.6
                                                                        pattern of performance that Endress & Bonatti report (in
                                                                        their experiments 1, 3, 4 and 5) when participants are
                             class                                      exposed to segmented familiarization streams of different
              Cosine
                       0.4
                             part12
                             part21                                     lengths. Demonstrating that, with sufficient training,
                             rule
                             train                                      networks can also show a reversal (coming to prefer part
                                                                        words over class words) remains a goal for future work.
                       0.2
                                                                                               Conclusions
                       0.0                                              According to a dual-mechanism hypothesis (Peña et al.,
                                                                        2002; Endress & Bonatti, 2007), language learning involves
                               40     60   80   100   120   140
                                           Epochs
                                                                        two mechanisms: a statistical mechanism that permits the
                                                                        learner to extract words from the speech stream, together
   Figure 5: Results for networks trained with gaps for 30,             with a non-statistical mechanism that is necessary for
  60, 90, 120 & 150 epochs, tested with gaps before items.              extracting higher-level structure. Our simulations show that
                                                                        a single statistical mechanism can account for the data that
   It may be argued that, whereas in Experiments 3, 8 and 12            has been used to motivate the dual-mechanism hypothesis.
of Endress & Bonatti, subjects were always familiarized for             We therefore conclude that Peña et al. and Endress &
2 minutes, we have probed our over several intervals of                 Bonatti have not demonstrated that rule-governed language-
training. In what sense, then, does Study 4 model the                   learning mechanisms are necessary for the extraction of
behavior of human participants? We chose to start with 30               structural information. In addition, we believe that these
epochs because the networks in our Study 3 trained for 150              modeling results go beyond the idiosyncrasies of SRNs. Our
epochs on average and the “short” streams in the human                  work shows that a primitive statistical learning mechanism
experiments were 2 minutes versus 10. There is no reason to             can learn linguistic preferences that appear to be governed
expect, however, that there should be a linear relation                 by abstract, structural rules. There is no reason to think that
between epochs of training in artificial neural networks and            the powerful statistical learning machinery that is the human
familiarization duration with human subjects. The networks              brain could not do the same.
do reproduce the preference for class-words over part words                                Acknowledgments
of both types after just 30 epochs of training, and that such a
preference does not decay in subsequent epoch intervals as a            Preparation of this manuscript was supported by DGICYT
result of a potential over-learning of the prediction task.             Project HUM2006-11603-C02-01 (Spanish Ministry of
That is to say, the networks retain the acquired knowledge              Science and Education and Feder Funds) to A.L. and P.C.
of the structural regularities inherent to class words.                                         References
   Second, the fact that the networks trained for only 30
                                                                        Bishop, C. M. (1995). Neural Networks for Pattern
epochs do not distinguish between class words, rule words
                                                                          Recognition. Oxford: Oxford University Press.
and training words suggests perfect learning of the most
                                                                        Elman, J. L. (1990). Finding structure in time. Cognitive
abstract “rule”, the one defining class words. The dual-
                                                                          Science, 14(2), 179-211.
mechanism hypothesis capitalizes on an observed negative
                                                                        Endress, A. D., & Bonatti, L. L. (2007). Rapid learning of
correlation between the extraction of structural regularities
                                                                          syllable classes from a perceptually continuous speech
and familiarization duration (Experiments 4 and 5, Table 1).
                                                                          stream. Cognition, 105(2), 247-299.
The longer the duration of the continuous familiarization
                                                                        Newport, E. L., & Aslin, R. N. (2000). Innately constrained
stream, the stronger the preference for part words over rule
                                                                          learning: Blending old and new approaches to language
words. On the contrary, a very short familiarization with a
                                                                          acquisition. In S. C. Howell, S. A. Fish & T. Keith-Lucas
segmented stream allows for the induction of the rule
                                                                          (Eds.), BUCLD 24: Proceedings of the 24th annual
(preference of class and rule words). However, the dual-
                                                                          Boston University Conference on Language Development.
mechanism hypothesis ignores the possibility that
                                                                          Boston: Cascalla Press.
subliminal segmentation gaps can be exploited statistically,
                                                                        Peña, M., Bonatti, L. L., Nespor, M., & Mehler, J. (2002).
as the present results with SRNs illustrate.
                                                                          Signal-driven computations in speech processing.
   The goal of this study was to demonstrate that networks
                                                                          Science, 298(5593), 604-607.
can replicate the rank order of preferences reported by
                                                                  714

