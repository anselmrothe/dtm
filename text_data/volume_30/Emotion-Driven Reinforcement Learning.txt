UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Emotion-Driven Reinforcement Learning
Permalink
https://escholarship.org/uc/item/9jk839mw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Marinier, Robert P.
Laird, John E.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                       Emotion-Driven Reinforcement Learning
                            Robert P. Marinier III, John E. Laird ({rmarinie,laird}@umich.edu)
                               Electrical Engineering and Computer Science Department, 2260 Hayward
                                                       Ann Arbor, MI 48109 USA
                              Abstract                                 augmented Soar with a new module, our emotion system,
   Existing computational models of emotion are primarily
                                                                       described below.
   concerned with creating more realistic agents, with recent
   efforts looking into matching human data, including qualitative     Integrating Appraisal Theories and Cognition
   emotional responses and dynamics. In this paper, our work              Our work is grounded in appraisal theories (Roseman &
   focuses on the functional benefits of emotion in a cognitive        Smith, 2001, for an overview) and Newell’s (1990)
   system where emotional feedback helps drive reinforcement
   learning. Our system is an integration of our emotion theory
                                                                       PEACTIDM (pronounced PEE-ACK-TEH-DIM). Appraisal
   with Soar, an independently-motivated cognitive architecture.       theories hypothesize that an emotional reaction to a stimulus
                                                                       is the result of an evaluation of that stimulus along a number
   Keywords: Emotion, reinforcement learning, intrinsic reward,        of dimensions, most of which relate it to current goals. The
   cognitive architecture, appraisal theories.
                                                                       particular appraisals that our system uses is a subset of the
                                                                       appraisals described by Scherer (2001) (see Table 1). The
                           Introduction                                subset our system uses can be split into two main groups:
Folk psychology often casts emotions in a negative light. For          appraisals that help the agent decide which stimulus attend to
example, in Star Trek, Vulcans are portrayed as superior to            (Suddenness, Unpredictability, Intrinsic Pleasantness,
humans because they are not distracted by emotions, and thus           Relevance) and those appraisals that help the agent decide
can make purely logical decisions. As far back as Phineas              what do in response to an attended stimulus (causal agent and
Gage, however, it has been clear that emotions play a critical         motive, outcome probability, discrepancy from expectation,
role in proper functioning in humans, and over the last several        conduciveness, control, power). Appraisal theories generally
decades psychological research has explored how emotions               do not give much detail about how appraisals are generated or
influence behavior.                                                    why. That is, the details of the process are left unspecified.
   We are interested in exploring how some of the functional           Thus, computational models must fill in those details, but
capabilities of emotion can be utilized in computational               with little or no direction from appraisal theory, the details are
agents; that is, we want to bring the functionality of emotions        often arbitrary. PEACTIDM, on the other hand, describes
to artificial intelligence. This is in contrast to most existing       necessary and sufficient processes for immediate behavior
computational models of emotion, which focused primarily               (see
on creating believable agents (Gratch & Marsella, 2004;                   Table 2). The PEACTIDM hypothesis is that stimuli are
Hudlicka, 2004), modeling human data (Marsella & Gratch                Perceived and Encoded so cognition can work with them.
2006; Gratch, Marsella, and Mao, 2006), or entertainment               Then Attend focuses cognition on one stimulus to process,
(Loyall, Neal Reilly, Bates, and Weyhrauch, 2004).                     which is then Comprehended. Tasking is managing tasks and
   In this paper, we present work in which reinforcement               goals (e.g., in response to a change in the situation), whereas
learning is driven by emotion. Intuitively, feelings serve as a        Intend is determining what actions to take. Decode and Motor
reward signal. The agent learns to behave in a way that makes          are translating cognitive choices into physical actions.
it feel good while avoiding feeling bad. Coupled with a task           PEACTIDM, however, does not describe the data upon which
that the agent wants to complete, the agent learns that                these processes operate. The basis of our theory is that
completing the task makes it feel good. This work contributes          appraisals are the information upon which the PEACTIDM
not only to research on emotion in providing a functional              theory operates (Marinier & Laird, 2006) (see
computational grounding for feelings, but it also contributes             Table 3). For example, the attend process determines what
to research in reinforcement learning by providing a detailed          to process next based on appraisal information generated by
theory of the origin and basis of intrinsically-motivated              the perceive and encode processes (i.e., suddenness,
reward.                                                                unpredictability, intrinsic pleasantness, and relevance). Thus,
                                                                       appraisals not only determine the information that
                           Background                                  PEACTIDM processes, PEACTIDM also imposes
Our system in implemented in the Soar cognitive architecture           dependencies between the appraisals (e.g., the appraisals for
(Newell, 1990). Soar is a complete agent framework                     Comprehend cannot occur until after appraisals for Attend
composed of interacting, task-independent memory and                   have been generated).
processing modules that include short- and long-term
memory, decision making, learning, and perception. We have
                                                                   115

Table 1: Subset of Scherer’s (2001) appraisals used by system     Emotion, Mood and Feeling
                                                                  Most existing models of emotion do not explicitly
  Suddenness              Extent to which stimulus is             differentiate between emotion, mood and feeling. In our
                          characterized by abrupt onset or        system, emotion is what the appraisals generate, and is thus a
                          high intensity                          set of values – one value for each appraisal. Mood plays the
  Unpredictability        Extent to which the stimulus            role of a moving history of emotion. Its value is pulled
                          could not have been predicted           towards the current emotion, and it decays toward a neutral
  Intrinsic               Pleasantness of stimulus                value a little each cycle. Thus, it acts like an average over
  pleasantness            independent of goal                     recent emotions. Mood provides some historical information,
  Relevance               Importance of stimulus with             so that an agent’s emotion, which might change wildly from
                          respect to goal                         one moment to the next, does not dominate the agent’s
  Causal agent            Who caused the stimulus                 interpretation of the situation. Feeling, then, is the
  Causal motive           Motivation of causal agent              combination of emotion and mood, and is what the agent
  Outcome                 Probability of stimulus occurring       actually perceives, and thus can respond to. Feeling and mood
  probability                                                     are represented as the same type of structure as emotion: as a
  Discrepancy from        Extent to which stimulus did not        set of appraisal values (Marinier & Laird, 2007).
  Expectation             match prediction                           Feeling intensity summarizes the feeling. The feeling is
  Conduciveness           How good or bad the stimulus is         composed of many dimensions (one for each appraisal),
                          for the goal                            whereas the intensity is a single number, valenced by the
  Control                 Extent to which anyone can              feeling’s conduciveness (Marinier & Laird, 2007). It is the
                          influence the stimulus                  feeling intensity that will act as a reward signal for
  Power                   Extend to which agent can               reinforcement learning.
                          influence the stimulus
                                                                  Reinforcement Learning and Soar-RL
      Table 2: Newell’s Abstract Functional Operations            In reinforcement learning (Sutton & Barto, 1998) an agent
                                                                  receives reward as it executes actions in its environment. The
   Perceive       Obtain raw perception                           agent attempts to maximize its future reward by maintaining a
   Encode         Create domain-independent                       value function that encodes the agent’s expected reward for
                  representation                                  each state-action pair. For a given state, the agent then selects
   Attend         Choose stimulus to process                      the best action based on the stored values for the available
   Compre-        Generate structures that relate stimulus        actions. The value function is updated based on external
   hend           to tasks and can be used to inform              rewards and the agent’s own prediction of future reward.
                  behavior                                           In Soar-RL (Nason & Laird 2004), the value function is
   Task           Perform task maintenance                        encoded as rules that associate expected rewards with state
   Intend         Choose and action, create a prediction          descriptions and operators. For a given state, all of the
   Decode         Decompose action into motor                     relevant rules fire, generating expected values, which are then
                  commands                                        combined for each operator to provide a single expected
   Motor          Execute motor commands                          reward. An epsilon-greedy based decision procedure then
                                                                  selects the next operator. The expected rewards associated
      Table 3: Integration of PEACTIDM and Appraisal              with rules are updated using the SARSA equation.
                                                                  Intrinsically Motivated Reinforcement Learning
   Appraisals               Generated by     Required by
   Suddenness               Perceive                              In traditional reinforcement learning, an agent perceives states
   Unpredictability                                               in an environment and takes actions. A critic, located in the
   Intrinsic                                 Attend               environment, provides a rewards and punishments in response
                            Encode                                to the choices being made. The agent learns to maximize the
   pleasantness
   Relevance                                                      reward signal (Sutton & Barto, 1998). This model is highly
   Causal agent                                                   abstract and assumes a source of reward that is specific to
   Causal motive                                                  every task.
   Outcome                                                           In intrinsically motivated reinforcement learning, the
   probability                                                    environment is split into internal and external parts. The
                                             Comprehend,          organism is composed of the internal environment together
   Discrepancy from         Comprehend
                                             Task, Intend         with the agent (Singh, Barto, and Chentanez, 2004). The critic
   Expectation
   Conduciveness                                                  resides in the internal environment, and thus the organism
   Control                                                        generates its own rewards.
   Power
                                                              116

                                                                     to a categorical view of emotions and thus requires a separate
                                                                     equation for each emotion. In our approach, emotions emerge
                                                                     from the interaction of the appraisals, providing a continuum
                                                                     of emotions (on top of which a categorical view could be
                                                                     imposed if desired).
                                                                                          Experimental Task
                                                                     To test the system, we created a maze task for the agent
                                                                     (Figure 2). The agent had to learn to navigate the maze,
                                                                     starting in the upper left and going to the far right. While the
                                                                     maze may look simple, the task is actually very difficult
                                                                     because the agent has essentially no knowledge about the
                                                                     environment.
                                                                        In this environment, the agent’s sensing is limited: it can
Figure 1: Our system viewed as an intrinsically motivated            only see the cells immediately adjacent to it in the four
reinforcement learner. (Adapted from Singh et al., 2004.)            cardinal directions. The agent has a sensor that tells it its
                                                                     Manhattan distance to the goal. However, the agent has no
   In our system, the appraisal process acts as the critic, and      knowledge as to the effects of its actions, and thus cannot
the resulting valenced feeling intensity provides the reward         evaluate possible actions relative to the goal until it has
signal over which the agent learns (Figure 1). Appraisal             actually performed them. Even then, it cannot always blindly
values are generated by rules, which match patterns in               move closer to the goal because given the shape of the maze,
perception and internal state. As the situation changes, the         it must sometimes increase its Manhattan distance to the goal
appraisal values change with it. The values are then detected        in order to make progress in the maze.
by a module that updates the current emotion, mood and                  Thus, the agent must learn such simple things as sometimes
feeling states of the agent.                                         moving in directions that reduce the distance to the goal, not
                                                                     walking into walls, and avoiding backtracking. At the lower
Previous Work                                                        level, it is actually learning about which PEACTIDM steps to
The idea that emotion influences reinforcement learning is           take and when. For example, it learns which direction to
not new. Grossberg (1982) describes a connectionist system           attend to, so it can take actions that bring it closer to the goal.
theory in which drives influence learning. Damasio (1994)            When the agent cannot take any actions that bring it closer to
describes experiments in which subjects with emotional               the goal, it must learn to do internal actions; specifically, to
impairments have difficulty learning.                                create subtasks to make progress in the maze while moving
   More recently, there have been other attempts to integrate        away from the goal.
emotion-like processes with reinforcement learning.                     The agent appraises situations in the following manner. The
Hogewoning, Broekens, Eggermont, and Bovenkamp (2007)                place an agent has just been has low suddenness, while other
describe a system developed in Soar that adjusts its                 places have higher suddenness. Directions leading to walls
exploration rate based on short- and long-term reward                have low intrinsic pleasantness. Directions leading away from
trajectories. They consider the reward histories to be a kind of     the goal have low relevance. Getting closer to the goal has
affect representation. This work is differs from our own in          positive conduciveness, while getting further away has
that it is not based on appraisal theories and rewards are not       negative conduciveness. The agent makes simple predictions
intrinsically generated.                                             that it will make progress with medium probability; if it does
   Salichs & Malfaz (2006) describe a system that is capable         not, discrepancy from expectation is high. When the agent is
of happiness, sadness and fear. Happiness and sadness serve          about to accomplish a task or a subtask, discrepancy from
as positive and negative rewards, while fear affects the             expectation is also high (it is pleasantly surprised). If the
selection of “dangerous” actions. Happiness is generated             agent is attending to a wall, control and power are low (since
when an external stimulus is present that is related to current      the agent cannot walk through walls); otherwise, they are
internal drives (e.g., if hungry and food is present, the agent      high. Causal agent and motive do not play much of a role in
will be happy). Sadness is when the desired external stimulus        this domain, since there is only one agent. Unpredictability
is not present. Fear is when the state values have a large           also does not play much of a role.
variance (even if positive overall). This work connects
physiology to goals and thus emotion. However, it commits
                                                                 117

                                                                     unfair comparison; however, creating a standard
                                                                     reinforcement learning agent with this capability but without
                                                                     the other appraisal information is difficult, since the appraisal
                                                                     representations comprise part of the state representation. If we
                                                                     were to remove the appraisal information, then the standard
                                                                     reinforcement learning agent would really be solving a
                                                                     different problem. If we leave the appraisal information, the
                                                                     agent is not really standard. However, the agent without
                                                                     mood can be viewed as a very rough approximation of an
                                                                     agent that would take advantage of this information to
                                                                     generate more frequent rewards. This approximation includes
                                                                     appraisal information, but without mood it is not the complete
                                                                     emotion system. Thus, we have two extremes and an
                                                                     intermediate agent: an agent with no emotion information at
                                                                     all, an agent with emotion but no mood, and an agent with
                                                                     both emotion and mood.
                                                                        The agents learned across 15 episodes. This was repeated
                                                                     in 50 trials. Each episode and trial took place in the same
                                                                     maze (shown in Figure 2). We recorded the amount of time it
Figure 2: The experimental task. The agent had to learn to
                                                                     took each agent to complete the task (measured in Soar
navigate this maze.
                                                                     decision cycles). Because of the task difficulty, the agents
                                                                     would sometimes get hopelessly lost; thus, to limit processing
                        Methodology                                  time, episodes were cut off at 10000 decision cycles. We
Three agent types were tested: a standard reinforcement              report the median to avoid skewing the data.
learning agent, which only received reward at the end when it
accomplished the goal, an agent that had no mood (so its                                        Results
feelings were its emotions) and a full agent that included
                                                                     The results are shown in Figure 3 and Figure 4. The
mood.
                                                                     horizontal axis is the episode number, while the vertical axis
   We expect the standard reinforcement learning agent to
                                                                     is the median amount of time it took the agent to complete the
have difficulty since it does not have access to the sensor that
                                                                     task.
tells it how far it is from the goal. This may seem like an
                                        Figure 3: Learning results for three different agents.
                                                                 118

 Figure 4: Close-up of last several episodes for agent with just emotion and agent with emotion and mood. "Error" bars show
 first and third quartiles.
   First, consider Figure 3. The standard reinforcement              This is because they get frequent reward signals (on every
learning agent never made any significant progress. This is          decision cycle) and thus get intermediate feedback on how
expected because 15 training episodes do not provide the             they are doing. The standard reinforcement learning agent
agent with enough experience when it only gets a single              only gets reward feedback at the end, and thus it takes a long
reward for each. In reinforcement learning, the reward “backs        time for that information to propagate back to earlier actions.
up” only one state per episode, and there are many more than         This result is not unexpected since part of the agent’s feeling
15 states in this domain.                                            is related to whether it is getting closer to the goal or not
   The agent whose feeling is just its emotion (without mood)        (albeit with the caveats mentioned earlier, it was not clear that
does not appear to be learning at first, but the values              the agent would be able to learn at all).
eventually converge. The agent whose feelings are composed              Next, the agent with mood learns faster. The reason is
of both emotion and mood does much better earlier on,                because, sometimes when the agent is doing some internal
learning much faster.                                                bookkeeping kinds of processing, it is not experiencing an
    Figure 4 shows a close-up of the last several episodes for       emotion. Thus, the agent without mood will get zero reward
the two agents with emotions. The “error” bars show the first        for those states, and later reward has to propagate back
and third quartiles, which gives an indication of the amount of      through those states. Propagation takes time (this is why the
variability in the agents’ behavior at that point. As we can         standard reinforcement learning agent takes so long to learn).
see, both agents reach optimality at the same time, but the             The agent with mood, however, carries a summary of its
variability of the agent with mood is much lower. In fact, the       recent emotions forward into those states (with some decay).
variability of the moodless agent reaches all the way to 10000       Thus, these states get reasonable value estimates, which
even in the final episode, implying that fewer agents did well       speeds up the propagation immensely.
on the task. In contrast, by the final episode, the agent with          In this experiment, a key factor in the success of the agent’s
mood has minimal variance.                                           using emotion is the availability of the knowledge about the
                                                                     Manhattan distance to the goal, which acts as an intermediate
                           Discussion                                reward. What we have presented is a theory about the origin
The first thing to note is that the agents with emotion learn        of those rewards and how they are applied to an integrated
very fast relative to the standard reinforcement learning agent.     system centered on abstract functional operations, and a
                                                                 119

demonstration that the rewards generated by that theory do, in     Nason, S. & Laird, J. (2005). Soar-RL, Integration
fact, speed learning.                                                Reinforcement Learning with Soar. Cognitive Systems
                                                                     Research, 6:51-59.
             Future Work and Conclusion                            Hogewoning, E., Broekens, J., Eggermont, J., & Bovenkamp,
Much work remains to be done. We are currently working on            E. (2007). Strategies for Affect-Controlled Action-
getting an agent learning in a more complex domain. We also          Selection in Soar RL. (pp. 501-510). In J. Mira and J.R.
plan to explore which subset of appraisals actually influences       Alvarez, editors, IWINAC 2007 Part II, LNCS 4528, Berlin
the learning. For example, we don’t expect that the causal           Heidelberg: Spinger-Verlag.
agent has much influence, because there is only one agent in       Hudlicka, E. (2004). Beyond Cognition: Modeling Emotion
the domain presented. However, we should be able to                  in Cognitive Architectures. In Proc. of the Sixth
construct domains in which the causal agent plays a critical         International Conference on Cognitive Modeling (pp. 118-
role.                                                                123). Mahwah, NJ: Lawrence Earlbaum.
   In conclusion, we have developed a computational model          Loyall, A. B., Neal Reilly, W. S., Bates, J. & Weyhrauch, P.
of emotion that integrates with a theory of cognition                (2004). System for Authoring Highly Interactive,
(PEACTIDM) and a complete agent framework (Soar). We                 Personality-Rich Interactive Characters. (pp. 59-68). In R.
have also confirmed a functional advantage of that integration       Boulic and D. K. Pai, editors, Eurographics/ACM
that had been proposed by other models; namely, that feelings        SIGGRAPH Symposium on Computer Animation.
can drive reinforcement learning. Finally, the system is           Marinier, R. & Laird, J. (2006). A Cognitive Architecture
learning how and when to execute various steps in the                Theory of Comprehension and Appraisal. In Robert Trappl,
PEACTIDM process which, unlike typical reinforcement                 editor, Cybernetics and Systems 2006 (Volume 2, pp. 589-
learning systems, includes learning both external actions and        594). Vienna: Austrian Society for Cybernetic Studies.
internal actions                                                   Marinier, R. & Laird, J. (2007). Computational Modeling of
                                                                     Mood and Feeling from Emotion. In Danielle S.
                                                                     McNamara and J. Gregory Trafton, editors, Proc. of the
                    Acknowledgments
                                                                     29th Meeting of the Cognitive Science Society (CogSci
The authors acknowledge the funding support of the DARPA             2007) (pp. 461-466). Nashville, Tennessee.
“Biologically Inspired Cognitive Architecture” program             Newell, A. (1990). Unified Theories of Cognition.
under the Air Force Research Laboratory “Extending the Soar          Cambridge, MA: Harvard University Press.
Cognitive Architecture” project award number FA8650-05-C-          Roseman, I. & Smith, C. (2001). Appraisal Theory:
7253.                                                                Overview, Assumptions, Varieties, Controversies. In Klaus
                                                                     Scherer, Angela Schorr, and Tom Johnstone, editors,
                        References                                   Appraisal Processes in Emotion: Theory, Methods,
Damasio, A. (1994). Descartes’ Error: Emotion, Reason, and           Research. New York and Oxford: Oxford University Press,
   the Human Brain. New York: Avon Books.                            pp. 3-19.
Gratch, J. & Marsella, M. (2004). A Domain-independent             Salichs, M. & Malfaz, M. (2006). Using Emotions on
   Framework for Modeling Emotion. Cognitive Systems                 Autonomous Agents. The Role of Happiness, Sadness, and
   Research, 5:269-306.                                              Fear. (pp. 157-164). In Adaptation in Artificial and
Gratch, J., Marsella, S., & Mao, W. (2006). Towards a                Biological Systems (AISB'06). Bristol. England.
   Validated Model of “Emotional Intelligence”. (pp. 1613-         Scherer, K. (2001). Appraisal considered as a process of
   1616). In 21st National Conference on Artificial                  multi-level sequential checking. In Klaus Scherer, Angela
   Intelligence (AAAI06), Boston, Massachusetts.                     Schorr, and Tom Johnstone, editors, Appraisal Processes in
Grossberg, S. (1982). A Psychological Theory of                      Emotion: Theory, Methods, Research. New York and
   Reinforcement, Drive, Motivation and Attention. Journal of        Oxford: Oxford University Press, pp. 92-120.
   Theoretical Neurobiology, 1:286-369.                            Singh, S., Barto, A, & Chentanez, N. (2004). Intrinsically
Marsella, S. & Gratch, J. (2006). EMA: A computational               Motivated Reinforcement Learning. In Proc. of Advances
   model of appraisal dynamics. In Robert Trappl, editor,            in Neural Information Processing Systems 17 (NIPS).
   Cybernetics and Systems 2006 (Volume 2, pp. 601-606).           Sutton, R. & Barto, A. (1998). Reinforcement Learning: An
   Vienna: Austrian Society for Cybernetic Studies.                  Introduction. Cambridge, MA: MIT Press.
                                                               120

