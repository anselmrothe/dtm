UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
How Features Create Knowledge of Kinds
Permalink
https://escholarship.org/uc/item/9xk0z1r3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Hidaka, Shohei
Smith, Linda B.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    How Features Create Knowledge of Kinds
                                               Shohei Hidaka (shhidaka@indiana.edu)
                                                Linda B. Smith (smith4@indiana.edu)
                                  Department of Psychological and Brain Sciences, Indiana University;
                                       1101 East Tenth Street, Bloomington, IN 47405-7007, USA
                               Abstract                                   name, the children systematically extend the name to new in-
                                                                          stances by different features for different kinds (Imai & Gen-
   Given a single instance of a novel category, two- and three-           tner, 1997, etc.) Specifically, they extend the names for things
   year-old children systematically generalize its name to other
   novel things based on appropriate feature dimensions. We               with features indicative of animates by multiple similarities,
   explain this in terms of a prediction of the probabilistic den-        for solid things with features typical of artifacts by shape,
   sity (category likelihood) in feature space from a single novel        and for nonsolid substances by material. For these different
   instance. In principle, observing more instances from a par-
   ticular probabilistic density, one can estimate the probabilistic      kinds of things, young children have clearly solved the fea-
   density more accurately. In this sense, children’s success in          ture selection problem and seem to know that different kinds
   generalization from a single instance seems to go beyond the           of features matter for different kinds of things. They know
   theoretical limit. We provide a theoretical account for the phe-
   nomenon. In our theory, these kind of kind specific generaliza-        what kinds of categories need to be formed.
   tions, a fast mapping from a single instance to a whole category
   is due to the structure of the system of learned categories and        Category likelihood and feature selection
   a sort of optimization of the category organization.
   Keywords: Fast mapping; Probability density estimation;
                                                                          Children’s use of different features to form different kinds
   Smooth feature space                                                   of categories in these tasks appears to directly reflect the
                                                                          category likelihoods of those features for known categories.
   Different categories are structured in different ways. For             Samuelson and Smith (see also Colunga & Smith, 2005) ex-
example, colors are relevant to categorizing foods but not to             amined the category structure of the first 312 nouns typi-
categorizing trucks. Further, some categories are decidedly               cally known by children learning English (and in other stud-
incoherent and not formed by people. For example, people                  ies the first 300 nouns learned by children learning Japanese).
do not form categories that include fish and elephants but                They measured category structure by asking adults to judge
not lions (see also Murphy & Medin, 1985). A key ques-                    the characteristic within-category similarities of typical in-
tion for a theory of categories, then, is how different features          stances of individual noun categories on four dimensions,
are selected for different kinds of categories, and how some              shape, color, texture, and material. They found that individ-
categories but not others are selected. An understanding of               ual artifact categories (e.g., chairs, forks, spoons, cups) were
very young children’s novel word generalizations may pro-                 judged to have instances that were highly similar in shape
vide an answer. Two and 3-year-old children generalize novel              but variable in other properties, that animal categories were
names for novel things in the “right” way given just a single             judged to have instances that were similar in all properties,
instance of the category: generalizing names for novel arti-              and that substance categories were judged to have instances
facts by shape, for novel animates by multiple features, and              that were similar in material (and color). Thus, the impor-
for substances by material. This fast mapping of a name for a             tance of features to different kinds of categories for children
single thing to a whole category surely facilitates early word            may reflect the expected distributions of those features for
learning.                                                                 nearby categories, a point we expand on below.
Kind specific generalizations                                             Distribution of category likelihoods
The phenomenon of interest derives from a widely used ex-                 Several recent studies further indicate that children’s differ-
perimental task of novel noun generalization (NNG) (see                   ent patterns of category generalization for different kinds of
Carey & Bartlett, 1978). In these tasks, children are shown               features may be geometrically organized in some larger fea-
a single novel thing and are then asked to generalize that                ture space (Imai & Gentner, 1997; Colunga & Smith, 2005).
name to other things. One experimental variable in these                  For example, Colunga & Smith (2005) showed that chil-
studies is the properties of the objects themselves, for exam-            dren’s generalizations of novel names by shape versus mate-
ple, whether they have features typical of animates (e.g., eyes,          rial shifted gradually as the presented novel instances varied
legs, hands), features typical of artifacts (e.g, solid with an-          incrementally from shapes typical of artifacts (complex, lots
gular parts, straight edges), or features typical of substances           of angles) to shapes typical of substances (simple rounded
(e.g., nonsolid, rounded, flat forms, with irregular shapes). In          shapes). Similarly, Colunga and Smith (under review, see
general, a large literature indicates that when 2- and 3-year-            also Yoshida & Smith, 2003) showed that children’s general-
old children are given a novel never-seen-before thing, told its          ization by shape versus material shifted gradually as (identi-
name (“This is a dax”), and asked what other things have that             cally shaped) instances were incrementally varied from solid
                                                                     1029

(brick like), to perturbable (play dough like), to nonsolid (ap-
                                                                             (a)                                                           (b)                                                                            (c)
plesauce like).
                                                                                                                                           Shape from constructed to animal like to simple
                                                                                                                                                                                                                            Feature 2
                                                                             likelihood
   We illustrate this idea in Figure 1 which represents cate-
gory generalization as a likelihood estimation problem in a
set of feature dimensions. Figure 1a shows a category like-
lihood (i.e., relative probability density of category member-
ship is shown on the z axis) and its contour plot projected in
a 2-dimensional feature space. Individual contours of cate-
gories are represented as ellipses in a 2-dimensional feature
space (Figure 1b). The contours of the category likelihoods–
that is the distribution of features across the two dimen-                                                                                                                                       Textures and materials                           Feature 1
sions of shape and texture/material–varies systematically as
a function of location in that space. This idealized repre-
sentation illustrates the structure that appears to characterize       Figure 1: Schematic category organizations having the same
the nouns that are learned early by young children and also            likelihoods contours: (a) likelihood pattern is represented as
to characterize children’s generalizations of a newly learned          ellipsis (b) ”smooth” and psychologically likely organization
noun to new instances. That is, instances of categories with           and (c) randomly distributed organization.
highly constructed and angular shapes vary little in shape
but vary greatly in texture and material whereas categories
of animal-like shapes vary little in shape but are also con-                                                                  
                                                                                   FKUVCPEGQHRCKTGFEQTTGNCVKQPOCVTKEGU
strained in their variation in texture/material. Finally, the un-
                                                                                                                             
constructed simple shapes of substances are correlated with
category distributions of relatively variable shapes but limited                                                              
texture/materials.
   The key insight is this: Similar categories, those categories                                                             
close in the feature space, have similar patterns of category
                                                                                                                              
likelihoods for different features. Put another way, the cat-
egories in the same region of conceptual space have similar                                                                  
shapes (their generalization patterns) and there is a gradient
of category shapes (category likelihoods) across the space as                                                                 
                                                                                                                                                                                                                                      
a whole. We will call a space of categories with these proper-                                                                         FKUVCPEGQHRCKTGFOGCPXGEVQTU
ties smooth: near (or categories with similar instances) have
similar generalization patterns and far (or categories with dis-       Figure 2: Scatter plot of distances in mean vectors and those
similar instances) have dissimilar generalization patterns.            in covariance matrices of pairs of categories from adult judg-
   Hidaka, Saiki and Smith (2006) analyzed the relation be-            ments.
tween the central tendencies and generalization patterns of
48 early-learned noun categories in a 16-dimensional fea-
ture space (See also the later simulation section). At issue           eralization pattern shown by the broken ellipsis. Because
was whether near categories would have similar variance pat-           nearby categories have similar patterns of likelihoods, the
terns and far categories would have dissimilar ones. Thus,             system (through the available space or competition among
smoothness was defined as a correlation between similarities           categories) can predict the likelihood of the unknown cate-
of paired categories in central tendencies and those in vari-          gory, a likelihood that would also be similar to other known
ance patterns. They found a positive correlation (R = 0.537,           and nearby categories in the feature space. If categories did
Figure 2) which indicates a smooth space of categories.                not have this property of smoothness, if they were distributed
                                                                       like that in Figure 1c, where each category has a variance pat-
Fast mapping with smooth categories                                    tern unrelated to those of nearby categories, the learner has no
Smoothness may not only be a descriptive property of early             basis on which to predict the generalization pattern. In sum,
learned categories but might also explain children’s kind-             the relationship between consistency in category distribution
specific generalizations from a single instance. Smooth cat-           and predictability implies that smooth categories may un-
egories provide an advantage because the category to be                derlie young children’s ability to generalize names for novel
learned is more predictable in a smooth space. To see the rela-        things in the right way given just one instance.
tionship between predictability and category organization, let
us assume that one knows some categories (shown as solid               Category packing
ellipses in Figure 1b) and observes the first instance (a black        But why should categories be smooth? The consistency and
star) of a novel category (a broken ellipsis). In the case of          predictability of categories could derive from the dense inter-
Figure 1b, the learner might easily predict the unknown gen-           action among adjacent categories. Starting with this idea, we
                                                                1030

propose a theoretical account, the packing model, so-called
because the category configuration is formed by competition
                                                                                                                                                          Optimal boundary
among categories for feature space with the result being that
                                                                                                                                                          Mean
categories are organized in feature space like things are orga-                                                                                           Discrimination error
nized in a well-packed suitcase. The main ideas of the pack-
                                                                                                   Probabilistic density
                                                                                                                                       The optimal decision boundary
ing model are (1) probabilistic densities of categories should
not “overlap” and (2) there should be no “gaps” in the fea-
                                                                                                                           A                         B
ture space in which no category is likely but in which some
uncategorized instances do occur. In this sense, Figure 1c,                                                                     μA           μB
which has many gaps (blanks among categories) and over-                                                                        σA                    σB
laps (intersection among categories), is not well packed. On
the other hand, Figure 1b, which has fewer gaps and over-
                                                                                                                                     Feature space   θ
laps, is well packed. More formally, (1) as joint probabili-
ties of paired categories indicate overlap probability, the total
                                                                                              Figure 3: Category likelihoods of two categories
sum over feature space of joint probabilities of all paired cat-
egories should be smaller , and (2) the probability distribution
of all categories should be well fitted to given instances’ prob-
                                                                                       Stork, 2000). In the following derivations, we assume each
ability distribution. We call computational condition (1) and
                                                                                       probabilistic density is a normal distribution, and utilize the
(2) discriminability and generalizability respectively. In gen-
                                                                                       Bhattarcheryya bound as discriminability of categories. In
eral, discriminability and generalizability are in a trade-off re-
                                                                                       the more general case of N categories in D dimensional space
lationship: more discriminable categories tend to have more
                                                                                       Ω ⊃ θ, we assume a likelihood P(θ|ci ) of category ci with fea-
gaps but less overlap, and more generalizable categories tend
                                                                                       ture θ defined as a normal distribution having mean µi vector
to have more overlap but less gaps. The optimally packed
                                                                                       and covariance matrix σi :
category configuration would be the middle of these two ex-
tremes. Next we give a formal description of the packing                                                           1      1
                                                                                         P(θ|ci ) = ((2π)D |σi |)− 2 exp(− (θ − µi )t σ−1
                                                                                                                                       i (θ − µi )) (1)
model and show that an optimal solution for the model has                                                                 2
smooth categories as a general trend.
                                                                                       where superscript t indicates transposition. In the case
                                                                                       of N categories, the discriminability is defined as the
          Theoretical Formulation of Packing                                           sum of Bhattercherrya       bound of all category pairs: F̄N =
                                                                                                            
We define discriminability as the probability of a discrimina-                         log ∑i ∑ j exp(Fi j ) .
tion error among categories and we define generalizability as                             Next we define generalizability that is, the likelihood of
the likelihood of instances given categories. Next we define                           instances given a category. The logarithm of the likelihood
the packing cost function as the sum of discriminability and                           of category ci (Equation 1) without constant terms can be
generalizability, and then derive the category distribution that                       simply written as Gi = log(σi ) + ∑Kk i (xik − µi )t σ−1
                                                                                                                                             i (xik − µi )
minimizes the packing cost function.                                                   where xik (k = 1, 2, . . . , Ki ) are observed instances of novel
    Consider first a simple case that includes only two cate-                          category ci . The packing cost Ln consists of combination
gories A and B in one feature dimension (Figure 3). The                                among discriminability Fn and fitting to given instances Gi .
likelihood of Category A (B) has a single central tendency at                          This is mathematically formulated as the minimization of the
mean µA (µB ) and varies with variance σA (σB ). An optimal                            Lagrange equation Ln with a constant λ as follows: LN =
category discrimination is to judge an instance as the most                            4F̄N + ∑i λ(Gi − log(C)), where C is a constant.
likely category. That is the probability of discrimination error                       Optimally-packed categories are smooth Next we show
probability over feature space is the minimum probability in                           a structural property of optimally-packed categories by solv-
Category A and B (i.e., colored    R
                                         area in Figure 3). That is for-               ing the differential of the packing cost with respective to
mally described as εAB = Ω min{P(θ|A), P(θ|B)}dθ, where                                statistical parameters of categories. The differential
min x, y is the minimum value in x and y, θ and Ω are re-                                                                                               of F̄N
                                                                                                                                           ∂F̄N         ∂F
                                                                                       with respective to a parameter X is ∂X = EF̄ ∂Xi j =
spectively a particular feature value and feature space. We                                                                   
                                                                                                          ∂F                    ∂F
define discriminablity as the total error probability between                          F̄N−1 ∑i, j Qi j ∂Xi j where EF̄ ∂Xi j is the expectation for
category A and B (the colored area in the figure) when cat-                            Qi j = P(ci )P(c j ) exp(Fi j ) (i, j = 1, 2, . . . , n) as a probabilis-
egory discrimination is optimal. Because our goal is min-                              tic density. Since the differential of LN with respect to σi
imizing εAB , hereafter we use the upper bound FAB , where                             (i = 1, 2, . . . , n) is zero, we obtain the following equation:
                 R                       1
exp(FAB ) = Ω {P(θ|A)P(θ|B)} 2 dθ ≥ εAB , instead of the er-                            ∂LN
                                                                                          −1 = EF̄ [(µi − µ̄i j )(µi − µ̄i j ) + σ̄i j − σi ] + λ{Si − σi } = 0
                                                                                                                              t
ror per se. In particular, when P(θ|A) and P(θ|B) are nor-                             ∂σi
mal distributions, the upper bound of error is Fi j = − 14 (µi −                       where σ̄i j = 2σi (σi + σ j )−1 σ j , µ̄i j = 12 σ̄i j (σ−1      −1
                                                                                                                                                i µi + σ j µ j )
µ j )t (σi + σ j )−1 (µi − µ j ) − 12 log( 12 |σi + σ j |) + 12 log(|σi ||σ j |),      and Si = ∑Kk i (xk − µi )(xk − µi )t is the scatter matrix. By solv-
which is called the Bhattarcheryya bound (Duda, Hart, and                              ing the above equation (i = 1, 2, . . . , N), we obtain the follow-
                                                                                1031

ing relationship 1 .                                                      Communicative Development Inventory (MCDI; Fenson et
                                                                        al., 1994), which is a vocabulary list of words normatively
                           ∑ j Qi j Ŝi j + σ̄−1
                                               ij   + λSi                 known by 50% of 30-month-olds, and 16-dimensional fea-
                 σi  =                                             (2)    tures provided by adults in a judgment task. The model’s pre-
                                   ∑ j Qi j + λKi
                                                                          diction of the probability density of novel categories is com-
                                                                          pared to actual statistics of natural categories. Because the
where Ŝi j = (µi − µ̄i j )(µi − µ̄i j )t . Equation (2) indicates co-
                                                                          structure of known categories has been shown to play a major
variance σi consists of the weighted average of three com-
                                                                          role in children’s kind specific generalizations (e.g., Colunga
ponents (i.e., Qi j and λ as its probabilistic density), the scat-
                                                                          & Smith, 2005), we manipulate the number of samples from
ter matrix of categories Ŝi j , the harmonic mean of covariance
                                                                          each category the model knows, using norms of the acquisi-
matrices σ̄−1i j and the scatter matrix of observed instances Si .        tion age of the 48 nouns from 16 to 30-month-olds (from the
Note that Qi j exponentially decays in proportion to the dis-
                                                                          MCDI). The goodness of fast mapping is discussed in light of
tance between category ci and c j . Thus, the scatter matrix
                                                                          this simulated word development.
of categories Ŝi j reflects the distribution of nearby categories
c j around category ci . Conceptually, it means that nearby
categories constrain the “niche” in the feature space for cat-            Procedure
egory ci to spread out (The broken ellipsis in Figure 1a or
b). The harmonic means of covariance matrices σ̄i j indicate              In each trial of the simulation, one category is assigned as un-
that σi would be similar to those of other “closer” categories.           known, and the other 47 categories are known. Each category
Therefore, Equation (2) implies that the general structure of             is assumed as a normal distribution, and the model predicts
optimally-packed categories is smooth, i.e., closer categories            the unknown covariance matrix based on the given means
in feature space have similar covariance patterns.                        and covariances of known categories. This process is simu-
                                                                          lated for each noun category as unknown and for each acqui-
Estimation of a novel category from the first instance
                                                                          sition rate of nouns corresponding from hypothetical sixteen
Here we derive the covariance estimation for “novel word
                                                                          to thirty month of age. In sum, novel word generalization was
generalization” that one only knows the first instance. In
                                                                          simulated 50 times for 15 ages by 48 categories.
this case, we approximately obtain Si ≈ 0 by assuming the
first instance is close to the true mean (Ki =1 and xk = µi ). In         Prediction We used Equation (3) to calculate a covariance
addition, we can obtain λ by solving the constraint equation              matrix of a novel category σi from an instance sampled from
∂LN                                                                       the category (µi = xi1 ) and other known categories (µ j and
 ∂λ = Gi − log(C) = 0. Then the optimal covariance of the                 σ j , j = i). The calculated σi minimizes the packing cost in
novel category is given as follows.
                                                                          terms of adjacent other categories. The coefficient λ in each
                                       
              N                   −1 N                              simulation is calculated by assuming the scaling constant as
                                −1 
      σi = C ∑ Qi j Ŝi j + σ̄i j  ∑ Qi j Ŝi j + σ̄−1           (3)    the determinant of the covariance matrix of the unknown cat-
               j                           j
                                                           ij
                                                                          egory (C = |(N − 1)−1 Si |).
                                                                          Word development To mimic the likely growth in knowl-
The EM algorithm (Dempster et al, 1977) is available for it-
                                                                          edge about known categories over 16 to 30 month old, we
erative minimizing the packing cost with Equation (2) or (3).
                                                                          assumed that the number of instances observed by the model
       Simulation of novel word generalization                            increases in proportion to these acquisition rates. The logic
                                                                          behind this manupulation is this: Statistical properties in the
In this simulation, we demonstrate that the model can predict
                                                                          adult judgments on feature of categories are products of de-
the likelihoods (feature distribution patterns) of natural cat-           velopment, and that children learn a subset of instances adults
egories which were unknown to the model. This is similar
                                                                          know. Specifically, we generated 2000 random instances for
to the problem of how children can predict the right gener-               each category which has the same mean and covariance ma-
alization pattern from a single instance. We formulated fast
                                                                          trix as that of adult judgments of these categories on the 16
mapping in this sense as a prediction of an unknown prob-                 dimensions (i.e., adults’ knowledge as a parent population).
abilistic density (category likelihood) in feature dimensions
                                                                          Then we assumed 50 out of 2000 instances were “known”
from a single novel instance. Specifically, in this simulation,           (samples) at the age the norms indicated the word was in
the model “knows” 47 natural noun categories and observe
                                                                          100% of the children’s productive vocabulary. In this learning
the first instance of the 48th novel category. Then the task              scheme, the accuracy of estimation of statistics for hypothet-
of the model is to predict the probabilistic density pattern of
                                                                          ical known categories increases in proportion to the acquisi-
this new category. The model’s prediction of the probabilistic            tion rates in the MCDI list. Figure 4 shows the mean acqui-
density was calculated by an optimal solution for the packing
                                                                          sition rates in the MCDI (solid line) and smoothness index
cost with respect to the configuration of surrounding known               (broken line) over 48 nouns. The smoothness index in the hy-
categories (See also the theoretical formulation). We used
                                                                          pothetical known categories gradually increases from 0.1 to
48 natural categories sampled from the Mac Arthur-Bates                   0.4 along increment of known instances (from approximately
    1 The precise solution is given as a quadratic eigenvalue problem.    10 to 45 instances per category).
                                                                     1032

                                                 
                                                                                                                                        generalization pattern from the adult judgments. Consider-
                                                
                                                                5OQQVJPGUU
                                                                /GCPCESWKUKVKQPTCVGU
                                                                                                                                        ing all the categories (and thus a hypothetical 30-month-old),
                                                
                                                                                                                                        the correlations were 0.58, 0.56 and 0.88 respectively. To as-
                                                                                                                                        sess whether knowing the whole organization of categories
         5OQQVJPGUUCPFOGCPESWKUKVKQPTCVGU
                                                
                                                
                                                                                                                                        enabled better category learning than merely learning single
                                                
                                                                                                                                        categories independently of the structure of the whole, we
                                                
                                                                                                                                        also compared the three measures of generalization for cate-
                                                
                                                                                                                                        gories defined by 3 randomly selected instances with the mea-
                                                
                                                                                                                                        sures from the adult judgments. These correlations are low:
                                                
                                                                                                                                        that for variances, covariances, and joint variance/covariance
                                                 
                                                                                                                                        are 0.23, 0.23 and 0.45 respectively. Moreover, for a series of
                                                                                                
                                                                          *[RQVJGVKECNCIG OQPVJ                                       vocabularies (at monthly intervals from 16 to 30 months by
                                                                                                                                        the MCDI), the covariance matrices predicted with an only in-
Figure 4: Average word acquisition rates and the smoothness                                                                             stance by the packing optimization have significantly higher
index of 48 categories                                                                                                                  correlation than those estimated from three instances. These
                                                                                                                                        finding illustrate the main idea of the packing account: the
                                                                                                                                        location of a to-be-formed category in a geometry of cate-
Categories and features                                                                                                                 gories –because of the local interactions of nearby categories–
                                                                                                                                        constrains the possible shape of the category in the feature
We used adult judgment data on early acquired noun cat-
                                                                                                                                        space. The packing model, however, depends on these be-
egories using adjectives as feature dimensions (Hidaka &
                                                                                                                                        ing a densly packed set of known categories. Figure 5 shows
Saiki, 2004). The survey includes 48 nouns selected ran-
                                                                                                                                        the correlation changing over the hypothetical development
domly from 312 nouns in the MCDI. They also span a variety
                                                                                                                                        trajectory. Each point and bar shows mean and standard de-
of kinds including food, animals, vehicles, tools, furniture,
                                                                                                                                        viation of 50 simulations with different random values. Since
and so forth. In the survey, 104 Japanese undergraduates rated
                                                                                                                                        the increase in the accuracy of known category estimations is
noun’s typical features using sixteen five-point-scale pairs of
                                                                                                                                        due to increase in the number of known instances along with
adjectives. For example, subjects rated the noun category
                                                                                                                                        the hypothetical age (Figure 4), we analyzed the correlation
”cat” as either ”very large”, ”large”, ”neither”, ”small”, and
                                                                                                                                        among these variables. The accuracy in prediction of novel
”very small”. The sixteen adjective pairs having larger vari-
                                                                                                                                        categories (for covariance matrices) increases in proportion
ance across noun categories were selected out of the initial 41
                                                                                                                                        to the number of instances (correlation to acquisition rate is
pairs, thus these adjectives would characterize the current set
                                                                                                                                        0.911, p < 0.01) and smoothness (0.816, p < 0.01) of known
of noun categories (see Hidaka & Saiki, 2004 for the detail).
                                                                                                                                        categories. It suggests that novel word prediction gets more
• Adjective pairs (feature dimensions)                                                                                                  accurate along with the growth of known categories. One
  dynamic-static, wet-dry, light-heavy, large-small, complex-                                                                           reason of this increment may be due to the smoothness of
  simple, slow-quick, quiet-noisy, stable-unstable, cool-warm,                                                                          known categories, because smoother categories would have
  natural-artificial, round-square, weak-strong, rough hewn-finely                                                                      more predictability. In sum, the prediction by packing op-
  crafted, straight-curved, smooth-bumpy, hard-soft.                                                                                    timization succeeded in a novel word generalization as ac-
                                                                                                                                        curate as known categories. The accuracy in prediction of
• Noun categories                                                                                                                       novel words increases as accuracy in estimation of known
  butterfly, cat, fish, frog, horse, monkey, tiger, arm, eye, hand, knee,                                                               categories increases. These results suggest that the packing
  tongue, boots, gloves, jeans, shirt, banana, egg, ice cream, milk,                                                                    optimization is powerful enough for earlier word learning.
  pizza, salt, toast, bed, chair, door, refrigerator, table, rain, snow,
  stone, tree, water, camera, cup, key, money, paper, scissors, plant,
                                                                                                                                                                Discussion
  balloon, book, doll, glue, airplane, train, car, bicycle
                                                                                                                                        These results show how children’s solution to the feature se-
                                                                               Results                                                  lection problem –to selecting the right features for a given
The logic of our analysis of the simulations is this: We gave                                                                           but as yet unknown category– may be a geometrical property
the model 47 of the 48 categories (whose generalization gra-                                                                            of the system of known categories in feature space. For this
dients were generated by adult judgments) and then asked,                                                                               to work, however, two computational conditions seem nec-
given the packing cost, if it could generate from a single                                                                              essary. First, the feature space itself must be organized in a
instance the generalization of the gradient of the 48th cate-                                                                           predictable way. There are hints -though more is needed -
gory, thus a simulation of fast mapping. To assess how well                                                                             that this is so (Hidaka et al., 2006; Colunga & Smith, 2005).
the model generated the generalization pattern of the miss-                                                                             If natural categories comprise a smooth space –for whatever
ing category, we examined the correlations, category by cate-                                                                           reason– it means that a learner of a new category can pre-
gory, between the predicted variances, covariances, and joint                                                                           dict the likelihood pattern of a novel category. This can
variance/covariance of the predicted category with its actual                                                                           be done because of a correlation between similarity of cat-
                                                                                                                                 1033

                       
                                                                                                              children’s smart feature selection and point in new directions
                                                                                                              for research - how dense known categories are in a given area
                      
                                                                                                              of feature space and how well the known instances of those
                      
                                                                                                              categories portray the generalization pattern for that category.
                      
        %QTTGNCVKQP
                      
                                                                                                                                      References
                      
                                                                                                              Carey, S. and Bartlett, E. (1978) Acquiring a single new word.
                                                                              2TGFKEVGF$QVJ                    Papers reports on child language development. 15, 17-29.
                                                                           2TGFKEVGF8CTKCPEG
                                                                              2TGFKEVGF%QXCTKCPEG
                                                                              1DUGTXGF$QVJ
                                                                                                              Colunga, E. and Smith, L. B. (2005). From the lexicon to
                      
                                                                              1DUGTXGF8CTKCPEG
                                                                              1DUGTXGF%QXCTKCPEG
                                                                                                                expectations about kinds: A role for associative learning.
                      
                                                                                  Psychological Review, 112, 347-382.
                                       *[RQVJGVKECNCIG OQPVJ
                                                                                                              Dempster, A., Laird, N., and & Rubin, D. (1977). Maxi-
                                                                                                                mum likelihood from incomplete data via the EM algo-
Figure 5: Correlation of the predicted variance patterns to the
                                                                                                                rithm. Journal of the Royal Statistical Society, 39 (1, Series
real data and that of estimation by samples
                                                                                                                B), 1-387.
                                                                                                              Duda, R. O., Hart, P. E., & Stork, D. G. (2000) Pattern Clas-
                                                                                                                sification (2nd ed) , New York: John Wiley & Sons.
egories and similarity of category likelihoods. This idea is
supported by the simulation results that show a relation be-                                                  Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal, D. J.,
tween the accuracy of known categories and generalization                                                       & Pethick, S. J. (1994). Variability in early communicative
of a novel category. The second computational requirement                                                       development. Monographs of the Society for Research in
is that the learning system needs to utilize not only the ob-                                                   Child Development, 59 (5, Serial No. 242) Chicago: Uni-
served instances of any given category, but also the consis-                                                    versity of Chicago Press.
tency of the distribution of those instances in the whole con-                                                Hidaka, S. & Saiki, J. (2004). A mechanism of ontologi-
figuration of categories. The core mechanism of the packing                                                     cal boundary shifting. In Proceedings of the Twenty Sixth
model is an optimization of category configuration in terms                                                     Annual Conference of the Cognitive Science Society, 565–
of discriminability and generalizability. The simulation sug-                                                   570.
gests that the prediction based on this structural consistency                                                Hidaka, S., Saiki, J. & Smith, L. B. (2006). Semantic packing
was enough powerful. Meanwhile, fast mapping based on the                                                       as a core mechanism of category coherence, fast mapping
category packing would not work very well when a learner                                                        and basic level categories. In Proceedings of the Twenty
has only a small number of inaccurately estimated categories.                                                   Eighth Annual Conference of the Cognitive Science Soci-
These two points raise a number of testable questions about                                                     ety, 1500–1505.
children’s novel noun generalizations. We know these be-
come more systematic with the number of known nouns. But                                                      Imai, M. & Gentner, D. (1997). A cross-linguistic study of
the present results suggest that it is not just number of known                                                 early word meaning: universal ontology and linguistic in-
nouns but how representative the known instances of those                                                       fluence., Cognition, 62, 169-200
nouns are with respect to that kind of category. This in turn                                                 Kemp, C., Perfors, A. & Tenenbaum, J. B. (2006) Learning
suggests that one might be able to speed up noun learning                                                       overhypotheses. In Proceedings of the Twenty Eighth An-
by presenting very young children -not just with many cate-                                                     nual Conference of Cognitive Society, 417-422.
gories in a region of feature space -but with a few well packed                                               Murphy, G.L. & Medin, D.L. (1985). The role of theories
instances of those categories, a result that has been empiri-                                                  in conceptual coherence. Psychological Review, 92, 289–
cally demonstrated in the case of artifacts (Smith et al, 2002),                                               316.
within a region of a feature space in the earliest developmen-
tal stage.                                                                                                    Samuelson, L. & Smith, L. (1999). Early noun vocabularies:
                                                                                                                do ontology, category structure and syntax correspond?,
   The explanation offered here is consistent with several
                                                                                                                Cognition, 73, 1–33.
other approaches to this phenomenon (Kemp et al., 2006;
Colunga & Smith, 2005) which also suggest that these kind                                                     Smith, L. B., Jones, S. S., Landau, B., Gershkoff-Stowe, L. &
specific generalizations are based on higher order correlations                                                 Samuelson, L. (2002). Object name learning provides on-
among feature dimensions. Colunga & Smith (2005) showed                                                         the-job training for attention. Psychological Science, 13,
that a connectionist model, fed the feature regularities char-                                                  13-19.
acteristic of early learned nouns, could generalize names for                                                 Yoshida, H. & Smith, L. B. (2003). Shifting ontological
things with different features by different properties. There                                                   boundaries: how Japanese- and English- speaking children
are a number of similarities and differences across the mod-                                                    generalize names for animals and artifacts. Developmental
els that merit investigation. This analysis however, points to                                                  Science, 6, 1–34.
new aspects of the system of knowledge that may underlie
                                                                                                       1034

