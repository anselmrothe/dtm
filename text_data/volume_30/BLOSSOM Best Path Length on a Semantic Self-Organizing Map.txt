UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
BLOSSOM: Best Path Length on a Semantic Self-Organizing Map
Permalink
https://escholarship.org/uc/item/8hj9p82v
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Lindsey, Robert V.
Stipicevic, Micheal J.
Veksler, Vladislav D.
et al.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              BLOSSOM: Best path Length on a Semantic Self-Organizing Map
           Robert V. Lindsey           Michael J. Stipicevic          Vladislav D. Veksler           Wayne D. Gray
            (lindsr@rpi.edu)             (stipim@rpi.edu)               (vekslv@rpi.edu)            (grayw@rpi.edu)
                                                    Rensselaer Polytechnic Institute
                                            Cognitive Science Department, 110 8th Street
                                                         Troy, NY 12180 USA
                            Abstract                                  unlimited vocabularies. These measures are not as
                                                                      computationally intensive as vector-based MSRs, but suffer
  We describe Vector Generation from Explicitly-defined               from a number of deficits. One issue is that statistical MSRs
  Multidimensional semantic Space (VGEM), a method for                cannot measure the relatedness of large multi-word terms
  converting a measure of semantic relatedness (MSR) into
  vector form. We also describe Best path Length on a                 (e.g. documents). Another issue relates to second-order
  Semantic Self-Organizing Map (BLOSSOM), a semantic                  word co-occurrence (words that do not occur together, but
  relatedness technique employing VGEM and a connectionist,           are often found in similar contexts). There is evidence in the
  nonlinear dimensionality reduction technique. The                   literature that vector-based measures provide better
  psychological validity of BLOSSOM is evaluated using test           performance over statistical measures due to context-based
  cases from a large free-association norms dataset; we find that     word processing (Budiu, Royer, & Pirolli, 2007; Landauer,
  BLOSSOM consistently shows improvement over VGEM.
                                                                      et al., 2007).
  BLOSSOM matches the performance of its base-MSR using a
  21 dimensional vector-space and shows promise to                       In this paper, we describe VGEM (Veksler, Govostes, &
  outperform its base-MSR with a more rigorous exploration of         Gray, 2008), a technique combining the benefits of
  the parameter space. In addition, BLOSSOM provides                  statistical MSRs with the power of vector-based MSRs. We
  benefits such as document relatedness, concept-path                 then propose BLOSSOM, an MSR that utilizes a Self-
  formation, intuitive visualizations, and unsupervised text          Organizing Map to reduce noise in vector-space semantic
  clustering.                                                         models. The psychological validity of the proposed
  Keywords: Measures of Semantic Relatedness, Self-
                                                                      technique is then evaluated using human word-association
  Organizing Maps, nonlinear dimensionality reduction,                norms. BLOSSOM is shown to exhibit consistent
  computational linguistics, natural language processing,             improvement over VGEM. Finally, we describe the unique
  VGEM, BLOSSOM, Dijkstra’s algorithm, SOM traversal                  benefits of using BLOSSOM over other MSRs, including
                                                                      intuitive visualizations and concept-path formations.
                        Introduction
Measures of Semantic Relatedness (MSRs) are a class of                                           VGEM
computational methods that calculate association strengths            To convert a statistical MSR M into vector form, we use
between terms in order to quantify the meaning of text.               Vector Generation from Explicitly-defined Multi-
MSRs have a wide variety of applications derived from their           dimensional semantic space (VGEM). VGEM's semantic
mathematical modeling of text meanings. These applications            space is explicitly defined by a set of dimensions d = {d1,
range from the computational modeling of human text                   d2, ..., dn}, where each word defines a single dimension. To
comprehension (Lemaire, Denhière, Bellissens, Jhean-                  compute the vector representation of a word in this semantic
Larose, 2006) to automated essay-grading algorithms                   space, VGEM uses its base-MSR M to calculate the
(Landauer & Dumais, 1997).                                            semantic relatedness between the target word w and each
  Most MSRs employ either statistical or vector-based                 dimension in d as follows:
techniques. Vector-based MSRs such as Latent Semantic                            v(M,w,d) = [ M(w,d1), M(w,d2), ..., M(w,dn) ]
Analysis (LSA; Landauer & Dumais, 1997) and Generalized                  For example, if d = {"animal", "friend"} and the word in
Latent Semantic Analysis (GLSA; Matveeva, Levow,                      question is "dog", then the vector for "dog" would be
Farahat, & Royer, 2005) generally model term-to-term                  [M("dog", "animal"), M("dog", "friend")]. If M("dog",
association strengths well, but require significant                   "animal") is 0.81 and M("dog", "friend") is 0.84, then the
computational resources to manipulate vector-space models             vector is [0.81, 0.84] as demonstrated in Table 1 and Figure
of text. For example, LSA involves the difficult task of              1. This example uses only two dimensions, though typically
solving a “large, sparse symmetric eigenproblem”                      many more are used.
(Landauer, McNamara, Dennis, & Kintsch, 2007, p. 45).                    Like other vector-based measures (e.g. LSA, GLSA),
  Statistical MSRs such as Pointwise Mutual Information               VGEM defines similarity between two words to be the
(PMI; Turney, 2001) and Normalised Google Distance                    cosine of the angle between the vectors that represent those
(NGD; Cilibrasi & Vitanyi, 2007) are often based on search            words. As the angle becomes smaller and the cosine
engine technology (e.g. Google search) and have virtually             approaches 1.0, the words are considered more related. A
                                                                  481

value of 1.0 means the two words are identical in meaning.         Dimensions
For example, in Figure 1, θ, the angle between “dog” and           Choosing a good set of dimensions is critical to VGEM's
“cat”, is relatively small. Consequently, the cosine of θ is       performance in tests of psychological validity. Although
close to 1.0 (.994) and the two words are considered to be         genetic algorithms may be used to derive a good set of
closely related.                                                   dimensions (Veksler et al., 2008), this process is
                                                                   computationally intensive, especially for a large set of
             Table 1: Sample VGEM Computations                     dimensions. BLOSSOM, as described below, achieves high
          Words                     Dimensions                     performance even from suboptimal VGEM dimensions.
                             Animal              Friend
           Dog                0.81                0.84
                                                                                             BLOSSOM
           Cat                0.81                0.67
          Tiger               0.79                0.13             Best path Length on a Semantic Self-Organizing Map
          Robot               0.02                0.60             (BLOSSOM) is an MSR that calculates semantic relatedness as
                                                                   the inverse of the distance one would have to “travel” from one
                                                                   term to another on a Self-Organizing Map (SOM). This SOM
                                                                   must be trained on an input semantic vector-space. In this paper
                                                                   we concentrate exclusively on the VGEM vector-space; future
                                                                   work will explore other vector-spaces. BLOSSOM uses a SOM
                                                                   to reduce the dimensionality of its input vector-space. To find
                                                                   the relatedness between two terms, BLOSSOM calculates the
                                                                   cost associated with the shortest path between the terms in the
                                                                   SOM’s internal representation of semantic space.
                                                                      Self-Organizing Maps afford a variety of benefits. Critical to
                                                                   BLOSSOM is the ability to create a weighted graph
                                                                   representation of the input space. The graph enables the
                                                                   calculation of the shortest path between any two nodes. This
                                                                   shortest path represents the distance between two points in
                                                                   semantic space.
          Figure 1: Sample VGEM semantic space.                       We are not the first to consider this type of path-analysis on a
                                                                   SOM. As Bui and Takatsuka (2007) describe, “we envisage
   This vector-based approach allows VGEM to represent a           that if an abstract knowledge space can be represented in the
group of words as the vector sum of the words that make up         form of a map, transitions of knowledge can be defined as a
the group. To compute the vector representation of a               path on this map.” Because our abstract knowledge space is the
paragraph, VGEM creates a vector for each word in that             English language (in vector form), “transitions of knowledge”
paragraph and adds them component by component. This               are conceptual differences in words. Thus, BLOSSOM
vector sum represents the meaning of the whole paragraph;          measures similarity as the shortest path through the knowledge
its relatedness to other vectors may be measured as the            space.
cosine of the angle between those vectors. Continuing with
the example in Table 1 and Figure 1, the vector representing       Self-Organizing Maps
the words "dog cat tiger" is the sum of first three vectors in
Table 1, [2.41, 1.64].                                             A Self-Organizing Map is an artificial neural network
                                                                   employing unsupervised, competitive learning techniques that
                                                                   performs nonlinear dimensionality reduction. It provides a
Advantages of VGEM                                                 method to represent high-dimensional data in low-dimensional
One of the advantages VGEM has over statistical MSRs is            spaces, often in 2-d or 3-d visualizations. The mappings
that it can compute relatedness between large, multi-word          between high- and low-dimensional spaces preserve
terms. A statistical MSR cannot find the relatedness               topological properties, meaning that high-dimensional
between two paragraphs because the probability of any two          structures are represented as independent clusters on the low-
paragraphs co-occurring (word for word) in any context is          dimensional map (Ultsch, 1995). Topological preservation is
virtually zero. VGEM, like other vector-based measures,            crucial to BLOSSOM because its method of calculating
can represent a paragraph or a document as a vector, and           semantic relatedness relies on an accurate low-dimensional
then compare that vector to other vectors within its semantic      mapping of semantic space.
space.                                                               The nodes of the SOM used by BLOSSOM are arranged in a
   The main advantage of VGEM over other vector-based              lattice structure, though many other possibilities exist (Wu &
MSRs is that VGEM does not require extensive                       Takatsuka, 2005). Associated with each node is a feature vector
preprocessing. This affords VGEM a larger dynamic                  consisting of the same dimensionality as the VGEM input
lexicon. Other vector-based MSRs cannot handle corpora             space. As with most neural networks, a SOM has two modes of
that are very large or dynamic.                                    operation, a learning mode and an operational mode.
                                                               482

Learning Mode When an input vector is given to a SOM,
the Euclidian distance between the input vector and each
node’s feature vector is calculated. The node with the
smallest Euclidian distance from the input vector, known as
the Best-Matching Unit (BMU), has its feature vector
adjusted according to the update formula:
         w k (t + 1) = wk (t )+μΧ(i, k )(x − wk (t ))
where i denotes the BMU, k denotes another node, t
represents a time index, µ is the learning rate, x is the input
vector, and Χ (i, k) is a time-decaying neighborhood
function. All nodes within the neighborhood of the BMU on
the lattice also have their feature vectors adjusted. A node’s
adjustment is proportional to its distance from the BMU.
                                                                    semantic distance (see Results). The solid path avoids
Operational Mode When a SOM is given an input vector,               directly crossing the semantic “mountains”, and represents a
the BMU is found. The coordinates of the BMU on the node            less costly path through the space because no major
lattice are the low-dimensional representation of the input         conceptual boundaries are crossed. BLOSSOM defines
vector. Even when presented with a novel input vector, a            semantic distance as the best path through the semantic
SOM is able to reduce the vector to a point on the lattice.         space, which is the path that minimizes the total toll cost.
BLOSSOM Setup                                                       SOM Training The SOM employed by BLOSSOM must
Words, as represented by their VGEM vectors, are clustered          learn to represent the VGEM semantic space. This is
on the SOM. Similar words are grouped within the same               accomplished by providing it with a large, representative
cluster. Figure 2b shows a 2-d SOM trained on the VGEM              sample of training data. For the purposes of this paper, we
semantic space. In this figure, the colors range on a gradient      used VGEM vectors representing words randomly selected
from black to white; dark areas represent clusters of similar       from the Factiva database (Factiva, 2004). The SOM
text and light areas represent cluster boundaries. Figure 2c        training algorithm repeatedly selects a random vector from
represents Figure 2b in three dimensions, where clusters are        this training set. It then performs the steps of the Learning
represented as “valleys” and cluster boundaries are                 Mode by adjusting the BMU and its neighboring nodes to
represented as “mountains” in this semantic landscape.              more accurately represent the training word. After a great
This representation illustrates the intuitive visual appeal of      many samplings, the nodes of the SOM will have
computing semantic relatedness as the traversal of a                sufficiently adapted to the VGEM representation of the
semantic landscape. There are two example paths traced              English language.
through the semantic landscapes of Figures 2a and 2b. The
traversal cost associated with a path is defined as the sum of      Undirected Graph Representation Once the SOM is
a series of “tolls” associated with the transition from one         trained, an undirected graph is constructed to facilitate the
node to another. Figure 2c represents the inter-node tolls as       calculation of shortest paths between nodes. The undirected
height; a high inter-node toll signifies a large conceptual         graph represents the relations between feature vectors, but
difference between the two nodes. The dotted path in                does not represent the feature vectors themselves. Edges in
Figures 2a and 2b is a straight line from one point to another      this graph are created only between neighboring nodes.
in semantic space. This straight line crosses several major         Each edge corresponds to the “toll” of traveling from a node
cluster boundaries and may not be a good representation of          to one of its neighbors (as discussed above). More precisely,
                                                                483

inter-node traversal cost is the angle between the two nodes'                        max (log f (w1), log f (w2 )) − log f (w1, w2 )
                                                                   NGD ( w1, w2) =
feature vectors. The angle represents the conceptual                                      logN − min (log f (w1), log f (w2 ))
dissimilarity between the two nodes. Because of this,
subsequent shortest-path calculations performed on this            NSS ( w1, w2) = 1 − NGD ( w1, w2)
graph are forced to take into consideration the complex           where f(w) is the number of documents w occurs in, f(w1,w2)
topological landscape encoded in the trained SOM (as              is the number of documents w1 and w2 appear together in,
demonstrated in Figure 2c). This ensures that sharp semantic      and N is the number of documents in the corpus.
jumps are avoided as much as possible.                               We used a subset (≈2.3 million paragraphs) of the Factiva
                                                                  database (Factiva, 2004) as a training corpus for NSS. This
SOM Traversal                                                     subset should be sufficient to reveal the power of
                                                                  BLOSSOM because it represents a sizeable sampling of the
Once the SOM has adapted to the VGEM semantic space               English language.
and a corresponding undirected graph has been created,
BLOSSOM is able to calculate the degree of association
between two query words. This involves node selection and         Evaluation To test the performance of the proposed
shortest path-cost calculation. For a pair of terms,              technique, we used an evaluation function based on human
BLOSSOM selects two nodes on the SOM best representing            word-association norms, previously used by Lindsey et al.
the terms. It then finds the shortest path between these two      (2007). Specifically, we used the Nelson-McEvoy free-
nodes on the undirected graph generated during setup.             association norms (Nelson, McEvoy, & Schreiber, 1998) to
                                                                  find out which target words were free-associated from cue
Node Selection The two terms are converted into VGEM              words (e.g. when the cue is 'old', targets might be 'new',
vectors based on the same explicitly-defined dimensions           'young', 'ancient', 'man', 'wrinkle', 'age'). For each set of cue-
that the SOM was trained on. A Best Matching Unit is then         targets we picked n random distracters, where n is the
found for each query word’s VGEM vector.                          number of targets. Distracter words were chosen randomly
                                                                  from the Factiva corpus, such that a third of the words were
Shortest Path Next, BLOSSOM finds the shortest path               rare (e.g. 'manatees', 'videocassette'), a third were common
from one node to another on the graph representation of the       (e.g. 'days', 'to'), and the rest were in between (e.g.
SOM. We use Dijkstra’s algorithm, though other shortest           'specifically', 'external'). We took a random sample of 100
path algorithms (e.g. Floyd’s algorithm) can be used. This        cue-targets-distracters test cases to evaluate a given MSR,
shortest path represents semantic distance as measured by         M.
BLOSSOM. It is then converted to semantic relatedness by             To evaluate each cue-targets-distracters test case, each of
applying a monotonically decreasing function. For                 the targets-distracters lists is sorted in descending order of
simplicity’s sake, we chose this function to be f(x) = -x.        M-derived relatedness values, M(cue, word), where
   There are at least two alternatives to the above method of     word∈{targets, distracters}. The score for each cue-targets-
computing semantic relatedness: (1) use the Euclidean             distracters test case for M is calculated as follows:
distance on the SOM between the BMUs of the two terms,                                  Number of targets in top n words
                                                                       Scorecase =
as represented by the straight, dotted line in Figures 2a and                                            n
2b or (2) take the cosine between the feature vectors of the      where "top n words" is the top half of the ordered targets-
BMUs. These potential techniques are considered and               distracters list. If, according to M, all target words are more
analyzed in the Results section.                                  related to cue than any of the distracter words, the score for
                                                                  that test case is 100%. If none of the target words are picked
                                                                  by M to be more related to the cue than any of the distracter
       Evaluating BLOSSOM: Methodology                            words, the score is 0%. The overall score for M is the
BLOSSOM has a large number of free parameters affecting           average of all test case scores.
performance (the ability to model human free-association
data). In the Results section, we examine the impact of                                        Results
several of these parameters. One of the major variables that
                                                                  To evaluate BLOSSOM, we examined BLOSSOMNSS-Factiva
we kept static for testing purposes is the base-MSR used in
                                                                  using different SOM sizes and training set sizes. The
VGEM vectors (denoted by M in the VGEM section). We
                                                                  dimensions we picked varied from random sets of 10, 100,
chose to use an MSR known as Normalised Similarity Score
                                                                  200, and 300 words from the Factiva corpus to a set of 21
(NSS) and utilized the Factiva text database. In the future,
                                                                  words from a general ontology. For the purposes of this
we will examine other MSRs (e.g. Pointwise Mutual
                                                                  paper, we tried a relatively small variation of these settings
Information), other corpora (e.g. Google, Wikipedia, Project
                                                                  until we could match the base-measure (NSS-Factiva)
Gutenberg), and other performance evaluation methods (e.g.
                                                                  performance. Future work will involve a more rigorous
Cilibrasi & Vitanyi, 2007).
                                                                  exploration of these settings.
   NSS is based on Normalised Google Distance (NGD,
                                                                     Our best results for BLOSSOM came from the set of 21
Cilibrasi & Vitanyi, 2007). To be more precise,
                                                                  ontology-based dimensions, on a 5x15 SOM using 320,000
                                                              484

training words. In this case, BLOSSOM showed no                   border effect (Kohonen, 2001). Nodes near the border of a
significant difference from the base-MSR, t(99)two-tail=.405,     2-d SOM are less likely to be updated during the learning
p=.68. BLOSSOM showed significant improvements over               process because they do not have many neighbors. This
VGEM by 6.27%, t(99)two-tail=3.96, p<.01.                         results in unreliable clustering in the outlying regions of the
   BLOSSOM consistently outperformed VGEM, BMU-                   SOM, which may prevent BLOSSOM from fully realizing
Euclidean-distance, and BMU-vector-cosine measures (the           its potential. In the future, we plan to implement a Geodesic
latter two measures are defined in the Shortest Path section,     Self-Organizing Map to remove the Border Effect problem.
points (1) and (2) respectively). Using random dimensions,        Geodesic SOMs use a spherical lattice to eliminate borders
all tested SOM sizes improved on average compared to              entirely (Wu & Takatsuka, 2005).
VGEM, BMU-Euclidean-distance, and BMU-vector-cosine
measures by 11.67%, 7.50% and 3.07%, respectively.                Linear vs. Nonlinear Dimensionality Reduction
                                                                  As previously discussed, Self-Organizing Maps are a
                                                                  technique for nonlinear dimensionality reduction. LSA, a
                                                                  well-established MSR (Landauer et al., 2007), employs a
                                                                  linear dimensionality reduction technique known as
                                                                  Singular Value Decomposition (SVD). Nonlinear reduction
                                                                  techniques have significant advantages over linear
                                                                  techniques for feature extraction (Backer, Naud, &
                                                                  Scheunders, 1998). If the VGEM semantic space contains
                                                                  nonlinear manifolds, some points in the low-dimensional
                                                                  mapping will be incorrectly mapped when using a linear
                                                                  technique. Self-Organizing Maps are able to properly cluster
                                                                  complex topological structures that linear statistical methods
 Figure 3. NSS-Factiva vs. VGEM vs. BLOSSOM using 21              cannot (Ultsch, 1995). We believe that with further
         dimensions. Error bars signify standard error.           development, BLOSSOM, due to its nonlinear nature, may
                                                                  exceed the capabilities of SVD-based MSRs.
                        Discussion
                                                                  Concept-Path Formation
The presented results are bounded by the small scope of the
parameter space we explored. It is premature to conclude          The shortest path between two words in BLOSSOM
that BLOSSOM is limited by the performance of its base-           represents a gentle transition from one point to another in
MSR, NSS-Factiva. Evidence exists in the literature that          semantic space. Because the feature vectors of the SOM
dimensionality reduction techniques can be used to improve        represent points in semantic space, each node traversed can
the performance of a base-MSR (Budiu, Royer, & Pirolli,           be assigned a term whose VGEM representation lies nearby.
2007).                                                            The nodes on a path from one term to another represent the
   The conversion of NSS into vector form with randomly           intermediate connecting concepts. For example, the path
picked dimensions favors the BLOSSOM method over                  between “foot” and “shirt” may pass through nodes
VGEM. Moreover, BLOSSOM's performance was better                  matching “shoe” and “clothes”, respectively. BLOSSOM is
when using ontology-based dimension words rather than             fully capable of creating conceptual paths through books,
much larger sets of randomly picked dimension words. This         web pages, and other documents. In the future, we will be
may be due to the fact that Self-Organizing Maps cluster the      exploring possible applications of this, the foremost being
semantic space; perhaps clusters are better formed when           the modeling of human free-association processes.
input vectors are based on categorical knowledge, rather
than randomly sampled words.
                                                                  Plasticity
  One of the reasons that BLOSSOM, BMU-Euclidean-                 When used with VGEM, one of BLOSSOM's advantages is
distance, and BMU-vector-cosine measures all perform              its ability to incorporate new vocabulary and word
better than VGEM given the same random dimensions is              relationships without significant processing costs. When
that using a SOM affords the ability to handle noisy, high-       presented with a new vocabulary word, BLOSSOM only
dimensional data (Pascual-Montano, Taylor, Winkler,               needs to calculate the word’s VGEM vector and proceed
Pascual-Marqui, & Carazo, 2001). A vector-space                   with the SOM Traversal steps.
representation of human semantics gleaned from any text
corpus is subject to a certain amount of error. The corpus        Future Work
may not lend itself toward a proper representation of human       In the future, we plan to test BLOSSOM’s performance
lexical knowledge, or the MSR used to generate the vector-        using different base-MSRs (e.g. WordNet). It may even be
space may simply be lacking. A SOM appears to                     the case that using a combination of base MSRs will result
compensate for these deficiencies.                                in higher performance. BLOSSOM may be able to reduce
   However, there are some problems associated with using         the noise in the high-dimensional data produced by using
SOMs. One major problem we encountered is known as the            multiple base MSRs in VGEM vectors.
                                                              485

   We also plan to use BLOSSOM for measuring relatedness           Cilibrasi, R., Vitanyi, P. (2007). The Google Similarity
between paragraphs and documents. VGEM word vectors                  Distance. IEEE Transactions on Knowledge and Data
can be summed up to produce paragraph and document                   Engineering, Vol. 19, No. 3, p. 370-383.
vectors that can be represented in the SOM. This sort of           Factiva. (2004). 2007, from http://www.factiva.com
document clustering can be used to improve information-            Kohonen, T. (2001). Self-Organizing Maps, Third Edition,
retrieval systems.                                                   Springer.
   Another potential application of BLOSSOM is automatic           Landauer, T. K., & Dumais, S. T. (1997). A solution to
generation of a lexical ontology from a document. After              Plato's problem: The latent semantic analysis theory of
BLOSSOM clusters words into a SOM, each cluster could                acquisition, induction, and representation of knowledge.
be labeled by a word that best describes the cluster. The            Psychological Review, 104(2), 211-240.
cluster descriptions could be put into another SOM and this        Landauer, T., McNamara, D., Dennis, S., Kintsch, W.
process could be repeated to recursively create a lexical            (2007). Handbook of Latent Semantic Analysis. Lawrence
ontology.                                                            Erlbaum Associates, Publishers. Mahwah, New Jersey.
   Yet another direction left for future exploration is            Lemaire, B., Denhière, G., Bellissens, C., Jhean-Larose, S.
augmentation of web-search queries. Searches could include           (2006). A computational model for simulating text
additional terms found within the original search term’s             comprehension. Behavioral Research Methods, Volume
cluster. This would help search engines provide results with         38, Number 4, pp. 628-637 (10).
subtle, indirect relationships to search terms.                    Lindsey, R., Veksler, V. D., Grintsvayg, A., & Gray, W. D.
                                                                     (2007). Effects of Corpus Selection on Semantic
                          Summary                                    Relatedness. 8th International Conference of Cognitive
In this paper, we briefly outlined VGEM, an MSR that                 Modeling, ICCM2007, Ann Arbor, MI.
integrates the benefits of vector-based and statistical MSRs.      Matveeva, I., Levow, G., Farahat, A., & Royer, C. (2005).
BLOSSOM was then introduced as a new MSR that reduces                Term representation with generalized latent semantic
noise in vector-space models like VGEM. Results indicate             analysis. Paper presented at the 2005 Conference on
that BLOSSOM offers a significant improvement over                   Recent Advances in Natural Language Processing.
VGEM and can match the performance of VGEM’s base-                 Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (1998).
MSR with just a small set of dimensions. BLOSSOM also                The University of South Florida word association, rhyme,
provides additional benefits, such as the visual appeal of the       and word fragment norms. :
semantic landscape model and automatic concept-path                  http://www.usf.edu/FreeAssociation/.
formation. In the near future, we will attempt to increase         Pascual-Montano, A., Taylor, K. A., Winkler, H., Pascual-
BLOSSOM's performance with a more rigorous exploration               Marqui, R.D., Carazo, J.M. (2002). Quantitative Self-
of the parameter space. We believe that the proposed                 Organizing Maps for Clustering Electron Tomograms.
technology has a wide range of applications in cognitive             Journal of Structural Biology. Volume 138, Issues 1-2,
modeling and cognitive engineering.                                  114-122.
                                                                   Turney, P. (2001). Mining the Web for Synonyms: PMI-IR
                    Acknowledgements                                 versus LSA on TOEFL. In L. De Raedt & P. Flach (Eds.),
                                                                     Proceedings of the Twelfth European Conference on
This work was supported in part by grants from the Office            Machine Learning (ECML-2001) (pp. 491-502). Freiburg,
of Naval Research (N000140710033) and the Air Force                  Germany.
Office of Scientific Research (FA9550-06-1-0074).                  Ultsch, A. (1995). Self Organizing Neural Networks
                                                                     perform different from statistical k-means clustering.
                         References                                  Gesellscahft fur Klassifikation, Basel.
Backer, S.D., Naud, A., & Scheunders, P. (1998). Nonlinear         Veksler, V. D., Govostes, R., & Gray, W. D. (2008).
  dimensionality reduction techniques for unsupervised               Defining the Dimensions of the Human Semantic Space.
  feature extraction. Pattern Recognition Letters. 19:711-           Accepted to the 30th Annual Meeting of the Cognitive
  720.                                                               Science Society.
Budiu, R., Royer, C., & Pirolli, P. (2007). Modeling               Wu, Y. and Takatsuka, M. (2005). The Geodesic Self-
  Informattion Scent: A Comparison of LSA, PMI-IR, and               Organizing Map and its error analysis. In Proceedings of
  GLSA Similarity Measures on Common Test and                        the Twenty-Eighth Australasian Conference on Computer
  Corpora. Paper presented at the RIAO'07, Pittsburgh, PA.           Science - Volume 38 (Newcastle, Australia). V. Estivill-
Bui, Michael; Takatsuka, Masahiro (2007). Path finding on            Castro, Ed. ACM International Conference Proceeding
  a spherical SOM using the distance transform and                   Series, vol. 102. Australian Computer Society,
  floodplain analysis. The 6th International Workshop on             Darlinghurst, Australia, 343-351.
  Self-Organizing Maps (WSOM), 2007, ISBN: 978-3-00-
  022473-7
                                                               486

