UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Optimal Processing Times in Reading: A Formal Model and Empirical Investigation
Permalink
https://escholarship.org/uc/item/3mr8m3rf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Smith, Nathaniel J.
Levy, Roger
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

Optimal Processing Times in Reading: a Formal Model and Empirical Investigation
                                                 Nathaniel J. Smith (njs@pobox.com)
                                       Department of Cognitive Science, 9500 Gilman Drive #515
                                                         La Jolla, CA 92093-0515 USA
                                                   Roger Levy (rlevy@ling.ucsd.edu)
                                            Department of Linguistics, 9500 Gilman Drive #108
                                                         La Jolla, CA 92093-0108 USA
                               Abstract                                   noisy samples from which to (Bayes-optimally) average out
    It is widely known that humans can respond to events they ex-         the noise and extract the signal; if we are optimal perceptual
    pect more quickly than to unexpected events, but we still have a      discriminators then the number of samples we must gather
    poor understanding of why. Models exist that derive a relation        (and thus how long we must wait) depends on the form of the
    between subjective probability and response time on the basis
    of optimal perceptual discrimination, but these models rely on        signal, of the noise, and of our prior beliefs. (For one exam-
    the ability of the responder control over perceptual sampling         ple of this approach in the context of word recognition, see
    of the environment, rendering them problematic for some do-           Norris, 2006.) It seems unlikely, however, that the sensory
    mains, such as auditory language processing, in which there
    are nevertheless clear dependencies between probability and           system is to blame for all response delays; we are reminded
    response time. We present a new model deriving the relation-          every time we start up our computers that computation qua
    ship between probability and reaction time as a consequence           computation takes time.
    of optimal preparation. This model is valid under very gen-
    eral conditions, requiring only that the results of optimization         Furthermore, while such models seem plausible for visual
    are invariant across scale of input stimulus granularity. The         perception, it is unclear how they might apply to, for instance,
    model makes the strong prediction that response times should          audition. We have reasonable control over how long we look
    scale linearly with the negative conditional log-probability of
    the stimulus. We present evidence for this prediction in an           at a scene, but very little control over how long we listen to an
    analysis of an existing database of eye movements in the read-        utterance. Yet, tasks using single auditorily presented words
    ing of naturalistic texts.                                            as stimuli find systematic variation in reaction time — and,
    Keywords: Optimal behavior; Language; Response time mod-              in fact, these variations are similar to those observed in cor-
    eling; Surprisal; Sentence comprehension; Eye movements;
    Reading                                                               responding visual presentation paradigms (Goldinger, 1996).
                                                                             This is one reason that we believe language to be a fruitful
                            Introduction                                  area in which to investigate alternative approaches to model-
It takes time to perform computation using a physical device,             ing processing time as an optimal behavior. While language
and the human brain is such a device. While obvious, this                 can be presented in the written modality, which is very con-
point is worth revisiting in light of the recent surge of interest        venient for experimentation, the bulk of our exposure is to
in rational models of optimal behavior (Chater et al., 2006;              spoken language, and the spoken modality has primacy both
Todorov, 2004). Such models have provided elegant explana-                evolutionarily and developmentally. To the extent, then, that
tions for many aspects of behavior, but processing time pro-              sensory sampling approaches couched in the framework of
vides a particular challenge for this approach. In general, hu-           optimal perceptual discrimination are implausible for spoken
mans respond to different stimuli within any given class with             language comprehension, these approaches are unlikely to
different speeds, and response times are a large part of the              provide the full story for general language processing. On the
stock and trade of experimental cognitive psychology. From                other hand, language processing is highly practiced and very
an optimality perspective, the difficulty is that it is unclear           efficient, which suggests that some other kind of optimality
why the time to spend on performing a computation should                  approach would still be valuable.
ever be larger than the physical minimum. Yet, from a theo-                  In this paper, we present a new model of optimal response
retical point of view, response times seem like the perfect can-          time couched in a framework of optimal preparation that
didate for an optimality approach, because they are so clearly            we believe may be more appropriate to domains such lan-
relevant to evolutionary fitness. We are all real-time organ-             guage processing in which we cannot always control time
isms who must react quickly and correctly in a wide variety               of sensory exposure. This model is motivated by the well-
of circumstances. So why are we still so much slower than                 established fact that processing times in language compre-
we could be?                                                              hension are probability-sensitive: in a given context, words
    The primary approach to these problems deployed within                which are more predictable are also read more quickly (e.g.
the Bayes-optimal framework has been to ascribe reaction                  Ehrlich & Rayner, 1981). This is intuitively sensible — cer-
times and other such delays to the sensory system. The argu-              tainly we would prefer it to the reverse! — but it is, as yet,
ment is that we require accurate information about the world              inadequately theorized. Our model explains this result as op-
to act, but our sensory system is noisy. Therefore, to ac-                timal behavior under a cost function which trades off prepa-
quire accurate information, we must wait and gather multiple              ration costs versus processing time; one would like to pro-
                                                                      595

cess quickly, but this requires preparation, and preparation                In any case, we are done as soon as we work out what form
is expensive in its own right. This model makes strong pre-              r(t) takes. This function summarizes the costs involved in
dictions about the relationship between probability, optimal             many kinds of preparation occurring over many time-scales.
preparation costs, and optimal reading times. We then test the           For instance, these might over the short term involve the at-
model’s prediction about probability and reading time against            tentional resources required to speculatively pre-compute re-
a corpus of naturalistic language processing data.                       sponses to stimuli that are especially likely in this context;
                                                                         over the medium term, the metabolic costs of maintaining
                                  Model                                  more or less precise cortical circuit tuning; and over the long
How much preparation is too much?                                        term, the allocation of limited cortical area to items which
                                                                         prove themselves to be reliably common. We therefore do
Our main idea is that in general, the nervous system does not
                                                                         not assume or attempt to derive any particular functional form
operate at the fastest possible speed, and that the reason for
                                                                         for r(t) from first principles, and limit ourselves to two sim-
this is that operating at the limits of efficiency is very expen-
                                                                         ple assumptions: (i) that it depends only on the chosen time
sive. Instead, it adjusts its performance on particular tasks
                                                                         ti and not on any other specific properties of the context or
to optimize a composite cost function that balances the speed
                                                                         stimulus; and (ii) that it is some smooth and monotonic de-
achieved against the costs of achieving that speed.
                                                                         creasing function. We turn instead to the particular attributes
   We further assume that this optimization occurs before
                                                                         of our system of interest, language.
each stimulus is actually encountered, because once the stim-
ulus is encountered it is too late to reallocate resources — one         Scale-free assumption
has whatever resources one has, and all there is to do is to pro-        One of language’s most celebrated properties is that it has
cess the stimulus as fast as possible given those constraints.           hierarchical structure, with regularities occurring at all lev-
Thus there are two stages, each with a cost: the pre-stimulus            els of granularity from, e.g., sentences to clauses to words to
or “preparatory” period, where optimization occurs, and the              morphemes to letters or phonemes. In our experiments we
post-stimulus or “processing” period, lasting from stimulus              may choose to measure processing time at word granularity,
onset until an appropriate response can be made. The dura-               but we have no reason to believe that this is a uniquely pre-
tion of the latter is evolutionarily relevant and what we com-           ferred scale for the brain. Spoken language is essentially a
monly measure experimentally, but the costs incurred in the              continuous auditory stream, and clearly there are many op-
former are presumably just as important to the brain.                    tions for how to break it into discrete, enumerable ‘stimuli’
   Formally, assume that for a given context there are n possi-          as required by our model. Therefore, instead of trying to de-
ble stimuli that we may possibly encounter (in the case of               rive r(t) directly, let us require that our model give the same
reading, these stimuli could be the words that may occur                 answer regardless of the temporal granularity we use to divide
next), and we may freely choose how long each will take us               our stimuli.
to process, subject to an additive global cost function:                    Formally, suppose we have some item i (e.g., a word)
                              n                                          which we can partition into smaller items i1 , . . . , im (e.g., the
                 C(t) = ∑ r(ti ) + E(tI |context).              (1)      phonetic segments in that word). Let pi j denote the condi-
                            i=1                                          tional probability that i j appears given that i1 , . . . , i j−1 have
                                                                         appeared previously — i.e., pi j = P(i j |i1 , . . . , i j−1 , context)
Here ti is the time we will require to process stimulus i if it
                                                                         — while pi as defined above can be rewritten as
appears, t = ht1 ,t2 , . . .i is the vector of all such times. Our
                                                                         P(i1 , . . . , im |context). Applying the chain rule to these for-
goal is to select t in such a way that we minimize our over-
                                                                         mulas shows that probability of the larger item is sim-
all cost C(t). The cost is composed of two parts. The first
                                                                         ply the product of the probabilities of the smaller items:
term, ∑i r(ti ), corresponds to the preparation cost we incur
                                                                         pi = ∏mj=1 pi j . On the other hand, the time taken to process
before encountering the stimulus; r(ti ) denotes the cost of in-
                                                                         the larger item is the sum of times taken to process its parts
vesting resources to prepare for stimulus i, and we prepare
                                                                         ti = ∑ j ti j . By (2′ ), ti j = f (pi j ). Substituting into (2′ ), we find
for all possible stimuli (though, at our option, in differing
amounts). The second term, E(tI |context), corresponds to the                                       ∑ f (pi j ) = f (∏ pi j ).
time it will take to process the stimulus that we do, in fact, en-                                    j                 j
counter; since the stimulus has not yet occurred, its identity is        That is, the function f turns products into sums. The only
a random variable, I, and we can only optimize the expected              non-trivial functions with this property are logarithms. Work-
time for processing it. Simple calculus then shows that (1) is           ing backwards and minding the appropriate monotonicity
minimized when                                                           conditions, we conclude
                            ti = (r′ )−1 (−pi )                 (2)
                                                                                                            ti = − logk pi                        (3)
where pi = P(I = i|context), and r′ denotes the derivative.
                                                                                                                    k−t
This unwieldy formula becomes clearer if define f (x) =                                                  r(t) =                                   (4)
(r′ )−1 (−x):                                                                                                     loge k
                                ti = f (pi ).                  (2′ )     where k > 1 is a free parameter.
                                                                     596

                                                                                  Methods
                                         Preparation cost curve                   Data The main technical challenge in measuring the shape
                                                                                  of a human response curve is obtaining enough data points
                                                                                  to estimate it reliably. Therefore, rather than attempt to con-
                               1.5
  Preparation cost (real ms)
                                                                                  struct a small set of balanced stimuli, we chose to analyze the
                                                                                  Dundee eye-movement corpus (Kennedy et al., 2003), which
                                                                                  consists of all eye-movements made by 10 subjects while
                               1.0
                                                                                  reading a collection of newspaper articles totaling approxi-
                                                                                  mately 50,000 words. In this paper we report results for first
                                                                                  fixation times, a standard reading time measure correspond-
                               0.5                                                ing to the durations of the first fixation to land on each word
                                                                                  in the text.1 This is not a perfect measure of processing time,
                                                                                  and it is not a perfect match to our model (which does not
                               0.0                                                assume that during a fixation centered on some word, sub-
                                                                                  jects will process only that word); these facts will tend to in-
                                     0        5      10       15       20
                                                                                  crease the noise in our data. Noise, however, can be overcome
                                                                                  through statistical means, and in return we are able to make
                                          Chosen time (potential ms)
                                                                                  use of existing methods of word probability estimation, and
                                                                                  achieve greater comparability with the existing reading liter-
                                                                                  ature.
Figure 1: Shape of preparation cost curve r(t) derived in (4),                    Probability estimation Probabilities were estimated by a
for k = 1.89 (chosen to match reading time data). X axis gives                    trigram language model trained on the 100 million word
the different achievable processing times that we select from                     British National Corpus (BNC) . We estimated the model us-
before encountering the stimulus; Y axis gives the cost for                       ing SRI Language Modeling Toolkit (Stolcke, 2002), with
selecting any given time, which in equation (1) is then added                     modified Kneser-Ney smoothing (Kneser & Ney, 1995).2 A
to the average actual processing time to produce the total cost.                  trigram model approximates the probability of a word in con-
                                                                                  text P(wordi |context) as the probability of the word given two
                                                                                  previous words, P(wordi |wordi−1 wordi−2 ); modified Kneser-
Predictions                                                                       Ney is a standard method of smoothing these trigram proba-
                                                                                  bilities, and a standard technology for broad-coverage lan-
Our model therefore makes two strong predictions about the                        guage modeling (Chen & Goodman, 1998). However, it
processing of stimuli which are continuous and have scale-                        should be noted that Kneser-Ney trigram probability (hence-
free hierarchical structure in time: that preparation costs drop                  forth, KN3-probability) is still a very noisy estimate of true
off exponentially as the chosen processing time increases (see                    conditional probability.
Figure 1), and that ultimately processing time of a linguistic                    Data selection The Dundee corpus contains 307,656 first
unit should be proportional to the negative log of that unit’s                    fixations; of these, we eliminated all fixations on words that
probability (its surprisal or self-information in information-                    occurred at the beginning or end of a line, which preceded or
theoretic parlance). The latter prediction can be tested using                    followed punctuation, that did not occur in the BNC (i.e., un-
existing data sources.                                                            known words), or that occurred in the BNC but in segmented
                                                                                  form (e.g., the BNC codes don’t as two words, do followed by
                                         Empirical validation                     n’t). This left N = 197, 503 fixations for our analysis, spread
                                                                                  roughly evenly across 10 subjects (range: 16666–22390 fixa-
                                                                                  tions per subject).
While it is generally agreed that more predictable words are
read more quickly, previous work on reading time and prob-                        Confounds A number of other linguistic measures are cor-
ability has suggested many functional forms for this relation-                    related with probability, and also known to be correlated with
ship: logarithmic (Hale, 2001; Levy, 2008), linear (Reichle et                    reading time; in particular, these include word length and
al., 1998; Engbert et al., 2005), or even reciprocal (Narayanan                   word frequency. Since we are using naturalistic data, we must
& Jurafsky, 2004). None have been empirically verified;                           control for such confounds retrospectively. Word length is
empirical work has been restricted to factorial comparisons
(Rayner & Well, 1996), which provide limited insight into                            1 Analyses   of first-pass reading times led to substantially similar
curve shape. In this section, we investigate the relationship                     results.
                                                                                     2 Traditionally, such probabilities are estimated via a cloze norm-
directly, using multiple regression techniques on an existing
                                                                                  ing task, but such behavioral measures are impractical for large num-
database of eye movements performed during the reading of                         bers of data points or low probability events, both of which are major
naturalistic text.                                                                considerations for our data-set.
                                                                            597

                                                                                                   Potential relationships between
                              Spread of frequency vs. probability                                   probability and reading time
                             0
                                                                                                                            Linear
  KN3−probability (log 10)
                             −2
                                                                                    Reading time
                             −4                                                                                      Logarithmic
                             −6
                             −8
                                                                                                       Reciprocal
                                  −8      −6      −4        −2
                                       Frequency (log 10)
                                                                                                           Probability (log scale)
Figure 2: Scatter-plot of log frequency versus estimated log                    Figure 3: Three possible relationships between probability
probability for the words in our data.                                          and reading time as proposed by different authors, illustrated
                                                                                in log-probability space. Our model predicts that the middle
                                                                                curve is correct.
simply the length of each word in letters. Word frequency
is the unconditional probability of a word, P(word), mea-
sured simply as the number of times a word appears in a large                   vided by generalized additive models (GAMs), a spline-based
body of text, divided by the total number of words in that                      extension of the standard linear regression framework (Hastie
text. For each fixated word in our corpus, we calculated word                   & Tibshirani, 1990). In our case, we fit a model of the form:4
frequency from the BNC. Unsurprisingly given their closely                            Timei = α + β · WordLengthi +
related theoretical definitions, frequency is highly correlated
with the conditional probability of a word (ρ = 0.80), and the                                       f (log KN3-probabilityi ) + g(log Frequencyi )
log of word frequency is reported to be correlated to process-                  where α and β are arbitrary constants, and f and g are arbi-
ing time on a wide variety of tasks. Our model, of course,                      trary smooth functions; all are chosen by the fitting process.
suggests that such effects are not driven by frequency per se,                  Such an approach, of course, is prone to overfitting; to com-
but rather by probability;3 however, testing this prediction re-                bat this (and make the problem well-posed), the fit penalizes
quires that we analyze our data with respect to both frequency                  functions based on how ‘wiggly’ they are, so that there is a
and probability together. Distinguishing such correlated vari-                  trade-off between following the data and avoiding extraneous
ables relies on what spread does exist; fortunately, this is non-               bends. The relative weight placed on these goals is deter-
negligible (see Figure 2).                                                      mined by cross-validation.
Analysis Analysis was carried out in R (R Develop-                                 The end result of this process are two functions, f and g
ment Core Team, 2007), using the package mgcv for non-                          above. Plotting f will show us how fixation duration varies in
parametric multiple regression (Wood, 2006) and lme4 for                        response to changes in KN3-probability, after accounting for
mixed-effect linear regression (Bates & Sarkar, 2007).                          confounds. Above, three possibilities from the literature were
                                                                                mentioned; the corresponding plots in log-space are shown in
Results                                                                         schematic form in Figure 3.
Curve shape Extracting the shape of an unknown func-                               The function g is less immediately relevant, but interest-
tional relationship from noisy data requires some form of                       ing nonetheless; there is a long tradition of frequency effects
non-parametric regression; doing so while simultaneously                        in psycholinguistics, but these results are usually confounded
controlling for confounds requires multiple non-parametric                      with any potential effect of probability (though see Rayner et
regression. An elegant framework for such analysis is pro-                      al., 2004). Examining g will show us the residual effect of
                                                                                frequency after accounting for the effects of probability.
    3 Note in particular that in single-word paradigms such as the lex-
ical decision task, there is effectively no context, which means that              4 In this model, logs are taken of KN3-probability and frequency
word probability, P(word|context), and word frequency, P(word),                 purely for convenience; invertible transformations of predictor vari-
give identical predictions.                                                     ables have a minimal effect on non-parametric regression.
                                                                          598

   One fit was performed to the data from each subject indi-
vidually, with results illustrated in Figure 4. As predicted by
                                                                                                Human response curves
our model, the curves in the left column (KN3-probability)                                       Probability   Frequency
are very close to linear (in log space), and this pattern holds                                                      A
                                                                                                 15
across at least five orders of magnitude in probability. Nine
out of ten subjects show this pattern; the exception is subject
                                                                                                 0
G, whose effect appears to be minimal, if any. The curves on
the right (frequency) are also roughly linear, and of a similar                                                      B
order of magnitude, though they appear to be less reliable —                                     15
only seven out of ten subjects show a clear effect.
   Now that we have established the shape of these effects,                                      0
we can better quantify their strength and significance using
                                                                                                 15                  C
traditional parametric techniques.
Significance To analyze significance, we used linear re-
                                                                                                 0
gression of fixation duration on word length, log frequency,
and log KN3-probability, with subject as a random effect.                                                            D
   After controlling for word length and frequency, KN3-                                         15
probability remains highly significant (χ2 (2) = 246.12, p ≪
0.001) as a predictor of first-fixation times. After controlling                                 0
for word length and KN3-probability, frequency also remains
                                                                                                                     E
significant (χ2 (2) = 182.81, p ≪ 0.001).                                                        15
   We can also investigate the relative magnitude and relia-
                                                                                    Time (ms)
bility of these effects by fitting a model including both fre-                                   0
quency and KN3-probability simultaneously, after standard-
izing them both to ensure comparability. In such a model, the                                    15                  F
response coefficient for KN3-probability is both larger than
that for frequency (−4.1 vs. −3.7 in arbitrary units), and less                                  0
variable across subjects (standard deviation 1.5 vs. 2.4).
                                                                                                 15                  G
                         Discussion
We have presented a model which predicts that in language-                                       0
like tasks — those where processing is skilled, speed is im-
portant, and stimuli are arranged through time in a continu-                                                         H
                                                                                                 15
ous manner with no preferred scale — the processing time for
an individual unit should be proportional to the negative log-
                                                                                                 0
probability of that unit occurring. Further, we have for the
first time performed a broad-coverage analysis of the func-                                                              I
tional relationship between probability and reading times, and                                   15
have found that over a wide range of probabilities, this effect
is both significant and takes the predicted form. Although we                                    0
have validated this prediction of the model on language pro-
                                                                                                 15                  J
cessing, this model could in principle apply to any cognitive
or perceptual domain in which our assumption that there is no
preferred granularity scale of processing is reasonable.                                         0
   These results suggest that a substantial portion of what
have previously been understood as frequency may, in fact, be
                                                                                                      −4 −2    −4   −2
context-sensitive probability effects — which may be prob-
lematic for theories that explain log-frequency effects as a              Figure 4: Estimated response curves for probability (left col-
result of the (static, context-insensitive) structure of the lex-         umn) and frequency (right column), both in log space, plotted
icon, such as the classic logogen theory of (Morton, 1969)                individually by subject. The X axis in each case ranges from
and its descendants in this respect, such as READER (Just                 −5 to the maximum value that occurred in the data set. This
& Carpenter, 1992). It still remains to compare to the other              range was chosen to include ≈ 90% of all data points (see Fig-
covariates that have been proposed besides word frequency                 ure 2); outside of this range the fit becomes extremely unre-
(e.g., Gernsbacher, 1984; Morrison & Ellis, 1995; McDonald                liable. X = −5 corresponds to a 1-in-100,000 event. Dashed
& Shillcock, 2001; Murray & Forster, 2004).                               lines are bootstrapped 95% confidence intervals.
                                                                    599

   On the other hand, frequency does remain significant, and              Chater, N., Tenenbaum, J. B., & Yuille, A. (2006). Probabilistic
it is not entirely clear how to interpret this. (No extant the-              models of cognition: Conceptual foundations. Trends in Cogni-
                                                                             tive Sciences, 10, 287–291.
ories have proposed explanations for why we would expect                  Chen, S. F., & Goodman, J.(1998). An empirical study of smoothing
probability and frequency to have separate, independent ef-                  techniques for language modeling (Tech. Rep. No. TR-10-98).
fects.) One possibility is that this result arises from noise                Computer Science Group, Harvard University.
                                                                          Ehrlich, S. F., & Rayner, K. (1981). Contextual effects on word
in the probability estimation process: trigram models have                   perception and eye movements during reading. Journal of Verbal
far more parameters than unigram (word frequency) models,                    Learning and Verbal Behavior, 20(6), 641–655.
                                                                          Engbert, R., Nuthmann, A., Richter, E. M., & Kliegl, R. (2005).
which increases estimation error; furthermore, the theoreti-                 SWIFT: A dynamical model of saccade generation in reading.
cal quantity of interest is not trigram probability at all, but              Psychological Review, 112(4), 777–813.
‘full’ conditional probability, and this approximation intro-             Gernsbacher, M. A. (1984). Resolving 20 years of inconsistent in-
                                                                             teractions between lexical familiarity and orthography, concrete-
duces additional error. Taken together, this means that our                  ness, and polysemy. Journal of Experimental Psychology: Gen-
probability estimates should be understood as having a much                  eral, 113, 256–281.
higher degree of noise than our frequency estimates. Despite              Goldinger, S. D. (1996). Auditory lexical decision. Language and
                                                                             Cognitive Processes, 11(6), 559-567.
this noise, however, KN3-probability still marginally outper-             Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic
forms frequency in explaining fixation times. This suggests                  model. In Proceedings of NAAACL-2001 (pp. 159–166).
the possibility that frequency’s role could diminish or con-              Hastie, T. J., & Tibshirani, R. J.(1990). Generalized additive models.
                                                                             New York: Chapman and Hall.
ceivably disappear with improvements in our technical ability             Just, M. A., & Carpenter, P. A. (1992). A capacity theory of com-
to estimate subjective probability.                                          prehension: Individual differences in working memory. Psycho-
                                                                             logical Review, 99(1), 122–149.
   However, it is also possible that frequency’s role will not            Kennedy, A., Hill, R., & Pynte, J. (2003). The Dundee corpus. In
disappear, and is in fact real. Our model makes the strong                   Proceedings of the 12th European conference on eye movement.
claim that subjective probability is the only determinant of              Kneser, R., & Ney, H. (1995). Improved backing-off for m-gram
                                                                             language modeling. In Proceedings of the IEEE international
reading time, so if frequency’s role is validated, there are two             conference on acoustics, speech and signal processing (Vol. 1,
possibilities: either our model is essentially correct but hu-               pp. 181–184).
mans are sub-optimal with regards to estimating conditional               Levy, R. (2008). Expectation-based syntactic comprehension. Cog-
                                                                             nition, 106, 1126–1177.
probabilities of words in text — perhaps their estimates are              McDonald, S. A., & Shillcock, R. C. (2001). Rethinking the word
partially biased and smoothed by word frequency, as a quick,                 frequency effect: The neglected role of distributional information
                                                                             in lexical processing. Language and Speech, 44(3), 295–323.
dirty, and low-variance approximation — or our model is in-               Morrison, C., & Ellis, A.(1995). Roles of word frequency and age of
complete.                                                                    acquisition in word naming and lexical decision. Journal of Ex-
   Finally, we should note that the juxtaposition of our                     perimental Psychology: Learning, Memory, and Cognition, 21,
                                                                             116–133.
optimal-preparation model against optimal perceptual dis-                 Morton, J. (1969). Interaction of information in word recognition.
crimination models such as Norris’s (2006) Bayesian Reader                   Psychological Review, 76(2), 165–178.
opens up a typology of optimal response time theories.5                   Murray, W. S., & Forster, K. I. (2004). Serial mechanisms in lexical
                                                                             access: The rank hypothesis. Psychological Review, 111(3), 721–
Optimal-discrimination and optimal-preparation accounts                      756.
make different predictions — notably, the perceptual confus-              Narayanan, S., & Jurafsky, D.(2004, November). A Bayesian model
                                                                             of human sentence processing. (Unpublished manuscript, http:
ability of the stimulus should have a huge effect on response                //www.icsi.berkeley.edu/˜snarayan/newcog.pdf)
times in optimal-discrimination accounts, whereas it does not             Norris, D. (2006). The Bayesian reader: Explaining word recog-
play a role in our account. It is also logically possible that               nition as an optimal Bayesian decision process. Psychological
                                                                             Review, 113(2), 327–357.
the truth lies in a combination of both accounts, or in a third           Norris, D. (submitted). Why the lexical decision task really does tell
account that lies elsewhere in this typology, in a theory of                 us a lot about word recognition: Modeling reaction-time distri-
optimal response times yet to be constructed.                                butions with the Bayesian Reader.
                                                                          R Development Core Team. (2007). R: A language and environ-
                                                                             ment for statistical computing [Computer software and manual].
                      Acknowledgments                                        Vienna, Austria. (ISBN 3-900051-07-0)
                                                                          Rayner, K., Ashby, J., Pollatsek, A., & Reichle, E. D. (2004). The
This work was supported by an NSF graduate fellowship to                     effects of frequency and predictability on eye fixations in reading:
NS.                                                                          Implications for the E-Z Reader model. Journal of Experimental
                                                                             Psychology: Human Perception and Performance, 30(4), 720–
                                                                             732.
                          References                                      Rayner, K., & Well, A. D.(1996). Effects of contextual constraint on
                                                                             eye movements in reading: A further examination. Psychonomic
Bates, D., & Sarkar, D. (2007). lme4: Linear mixed-effects mod-              Bulletin & Review, 3(4), 504–509.
   els using S4 classes [Computer software]. (R package version           Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K.(1998). To-
   0.99875-9.1)                                                              ward a model of eye movement control in reading. Psychological
The British National Corpus, version 3 (BNC XML edition). (2007).            Review, 105(1), 125–157.
   (Distributed by Oxford University Computing Services on behalf         Stolcke, A. (2002). SRILM—an extensible language modeling
   of the BNC Consortium. URL: http://www.natcorp.ox.ac.                     toolkit. In Proceedings of the international conference on spo-
   uk/)                                                                      ken language processing (Vol. 2, pp. 901–904).
                                                                          Todorov, E. (2004). Optimality principles in sensorimotor control.
    5 Norris’s Bayesian Reader also predicts a response-time effect          Nature Neuroscience, 7(9), 907–915.
                                                                          Wood, S. N. (2006). Generalized additive models: An introduction
linear in negative log-probability, for reasons explained in (Norris,        with R. Boca Raton: Chapman and Hall/CRC.
submitted).
                                                                      600

