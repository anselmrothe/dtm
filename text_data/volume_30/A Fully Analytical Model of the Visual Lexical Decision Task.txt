UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Fully Analytical Model of the Visual Lexical Decision Task
Permalink
https://escholarship.org/uc/item/5vk2z7qh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Author
Martin, Fermin Moscoso Del Prado
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                     University of California

                     A Fully Analytical Model of the Visual Lexical Decision Task
                    Fermı́n Moscoso del Prado Martı́n (fermin.moscoso-del-prado@univ-provence.fr)
                                            Laboratoire de Psychologie Cognitive (UMR 6146)
                                Aix-Marseille Université & Centre National de la Recherche Scientifique
                                                                Marseilles, France
                               Abstract                                     where the additional conditionings on (H ) represent the set of
                                                                            modeling assumptions and previous knowledge under which
   This study describes an analytical model of the visual lex-
   ical decision (VLD) task. The task is modeled as a prob-                 we provide the estimates of the posteriors.
   lem of hypothesis testing given noisy evidence using a gen-                 The input I corresponds to a sequence of samples I =
   eral Bayesian framework, similar to several previously pub-              {x1 , . . . , xT } from a noisy distribution. Therefore we can view
   lished models. The Bayesian formulation is then shown to
   reduce to a Geometric Brownian Motion with a drift and an                the OR in (1) as a function of time, whose value changes with
   infinitesimal variance (a Drift-Diffusion Model). In turn, this          each new sample that is received. In this way, W would be
   reduction enables the use of direct analytical techniques – in-          chosen over NW at the first time T when the value of the OR
   stead of simulations – to understand the different factors that
   influence response latencies. We demonstrate the power of                between them exceeds some threshold value ΘW > 1:
   this technique by analyzing the individual response latencies                                        P(W |x1 , . . . , xT , H )
   to a realistic size vocabulary covering virtually the full English                         B(T ) =                               ≥ ΘW .   (2)
   lexicon. The model achieves an accurate prediction of the re-                                       P(NW |x1 , . . . , xT , H )
   sponse latencies and error scores to thousands of individual
   words in relation to previously published VLD data. Crucially,           This amounts to ensuring that the probability of W given the
   this approach enables a direct explanation of several known              stimulus (and our assumptions) is at least ΘW times greater
   non-linear effects, directly addressing their underlying math-           than the probability of NW given the stimulus. Symmet-
   ematical explanation in a level of detail that is not attainable
   using traditional simulation-based approaches.                           rically, we can use another threshold 0 < ΘNW < ΘW that
                                                                            would enable us to choose to respond that the input corre-
   Keywords: Visual Lexical Decision; Bayesian; Analytical;
   Brownian Motion; English; Distributed Representations                    sponds to a non-word, whenever B(T ) ≤ ΘNW . These thresh-
                                                                            old parameters could take very different values, depending
        A Bayesian Model of Lexical Decision                                on how much easier we would expect one hypothesis to be
                                                                            recognized over another. However, for simplicity we will
In line with some current models of the VLD task (e.g., Adel-               work under the assumptions that the thresholds are symmet-
man & Brown, 2008; Norris, 2006; Ratcliff, Gómez, & McK-                   rical (ΘW = Θ, ΘNW = Θ1 ).
oon, 2004; Wagenmakers, Steyvers, Raaijmakers, Shiffrin,                       The calculation of the OR’s is simplified when one works
van Rijn, & Zeelenberg, 2004) we can view lexical decision                  in a logarithmic scale, the Log Odds Ratio (LOR). If we de-
as a problem of optimally taking a decision by accummulat-                  fine θ = log Θ, then the condition to choose a word over a
ing evidence coming from a noisy input. In VLD the input                    non-word stated in (2) is fully equivalent to:
corresponds to the visual information provided by the eyes as
time passes. As more samples of the input are accumulated                                                  P(W |x1 , . . . , xT , H )
                                                                                             b(T ) = log                               ≥ θ.  (3)
the evidence supporting a ‘yes’ or a ‘no’ response grows until                                            P(NW |x1 , . . . , xT , H )
a certain level of certainty is attained. As noted by Adel-                 The time T at which the condition in (2) is first satisfied is the
man and Brown (2008), Norris (2006), or Wagenmakers et                      same at which (3) becomes true.
al. (2004), from a Bayesian perspective, the problem of de-                    By applying Bayes’ Theorem on both terms of this ratio,
ciding whether a certain visual input corresponds to a word                 we obtain the ratio of the likelihoods times the ratio of the
or not can be characterized as a general problem of hypoth-                 prior probabilities. However, in a typical lexical decision
esis choice. We can view word versus non-word decision as                   experiment, the prior probability of observing a word or a
two hypotheses from which participants have to choose one,                  pseudo-word are balanced and would normally cancel out.
and assume that the participants respond as soon as they have               Thus we are left with the ratio between the likelihoods of the
gathered a certain level of evidence in favor of either.                    input sample {x1 , . . . , xt } under a word or non-word hypoth-
                                                                            esis:
Odds Ratios                                                                                                  P(x1 , . . . , xt |W, H )
                                                                                                 b(t) = log                            .     (4)
The Odds Ratio (OR) between two hypotheses is the ratio of                                                  P(x1 , . . . , xt |NW, H )
their posterior probabilities given the available information.                 As in most models of this task, we assume that the samples
In our case the two hypotheses being word (W ) and non-word                 from the visual input are independent of each other. Thus,
(NW ), the decision on presentation of a certain visual input               we can expand this log-likelihood as the sum of the log-
(I) could be made using the OR:                                             likelihoods of the independent samples (xt ) from the input:
                                                                                               t                          t
                                P(W |I, H )
                          B=
                               P(NW |I, H )
                                              ,                      (1)         b(t) =      ∑ log P(xk |W, H ) − ∑ log P(xk |NW, H ).       (5)
                                                                                             k=1                        k=1
                                                                        1035

This assumption of temporal independence of the samples               driven by the likelihood of the target word, with a minor con-
could suffer if samples came from different eye fixations.            tribution of relatively high frequency orthographic neighbors.
However, on the lack of information of the specific fixations         Therefore this will give rise to a facilitatory effect of word
we can safely assume that their average can be described by           frequency, as most of the value of the numerator to the LOR
a single distribution of independent samples.                         will come from the target word itself, and this contribution is
                                                                      frequency-weighted. In addition, at least at the initial stages
Likelihood for Words                                                  of processing, the LOR will also receive a facilitatory con-
In an optimal decision process, it is not the likelihood of a par-    tribution from the orthographic neighborhood, which will in-
ticular word that drives the decision, but rather the combined        crease with the number, relative proximity, and frequency of
likelihoods of all possible words, weighted by their individual       existing neighbors.
prior probabilities:
                                                                      Likelihood for pseudo-words
                              Nw                                      The problem is to decide whether the input has been sampled
          P(I|W, H ) = ∑ P(I|Wi ,W, H )P(Wi |W, H ).           (6)    from a Gaussian centered in the mean of one of the existing
                             i=1
                                                                      words, or whether it is more likely to have been sampled from
where Nw is the number of words in the lexicon. The first             another Gaussian distribution with a different unknown mean,
component of each of the terms of this sum, the prior for a           corresponding to a non-word.
particular word (Wi ), is its overall probability of occurrence          A simple way to represent this is that the probability of
in the experiment. On the lack of additional contextual con-          a pseudo-word corresponds to the sum of the probabilities
straints, we can estimate it as being proportional to its relative    of possible non-words located at all points in the representa-
frequency of occurrence in a linguistic corpus:                       tional space, weighted by our prior expectation of finding a
                                                                      pseudo-word at that point in space:
                                             F(Wi )                                           Z ∞
                   P(Wi |W, H ) '          Nw
                                                       .       (7)        P(xk |NW, H ) =          p(xk |m, NW, H )p(m|NW, H )dm,
                                         ∑ j=1 F(W j )                                         −∞
                                                                                                                                          (10)
   In turn, the combined likelihoods given a word Wi for a            where m are the possible locations in the representational
sequence of t independent input samples x1 , . . . , xt sampled       where the pseudo-word could be located.1
from a multidimensional Gaussian with diagonal covariance                The two components in this integral require assumptions
(N (µi , σ2 )) is the product of the individual likelihoods for       on the corresponding distributions. As was done for the
each of the {x1 . . . xt }:                                           words, we can assume that the likelihood of observing a par-
                                                                      ticular sample from the input x given that the presented stim-
                                             T
                                                                      ulus was a non-word with orthographic representation m fol-
             P(x1 , . . . , xT |Wi , H ) = ∏ P(xk |Wi , H )    (8)
                                            k=1
                                                                      lows a multidimensional Gaussian centered on the represen-
                                                                      tation of the non-word, and a diagonal covariance matrix with
   Each of the individual likelihoods in (8) is a Gaussian cen-       determiner σ2 :
tered on µi and with a variance σ2 :                                                                                      kx −mk 2
                                                                                                                 1      − k
                                                                                     p(xk |m, NW, H ) = √              e 2σ2              (11)
                                          1      kx −µ k
                                                − k i
                                                         2                                                      2πσ2
                P(xk |Wi , H ) = √             e 2σ2       .   (9)
                                         2πσ2                            The non-words in a typical visual lexical decision exper-
                                                                      iment are constructed to be similar to the existing words,
For simplicity, in line with previous models, we will assume          therefore their distribution in the representation space should
that the sampling variance (σ2 ), is uniform across experimen-        be similar to that of the words themselves. On the lack of
tal stimuli. This is a simplification of the actual process where     additional knowledge on the shape of distribution of words in
different types of stimuli could give rise to different noisy         the representational space, by the Maximum Entropy Princi-
variances (consider for instance the effect on the visual input       ple we can assume it is a multidimensional Gaussian. We can
to varying word lengths).                                             estimate the frequency weighted mean of the representations
   At this stage, we can already notice two factors that will af-     of all words in our lexicon (µw ) and the corresponding vari-
fect the LOR. On the one hand, from (7) we can infer that the         ance (σ2w ). Therefore, the prior for the location of the possible
contribution that each word’s contribution to the LOR will            pseudo-word means is:
be proportional to its frequency of occurrence in a corpus.
                                                                                                                        km−µw k2
On the other hand, the likelihood in (9) decreases exponen-                                                    1      −
                                                                                                                           2σ2
                                                                                      p(m|NW, H ) = p                e       w            (12)
tially with the distance between each sampled input and the                                                   2πσ2w
centroid of the representation of the target word. As the in-
                                                                          1 This integral includes all locations in the representational space,
put itself will follow a normal distribution centered on this
                                                                      also those of the words themselves. Note however, that in relation to
particular centroid, due to the product in (8) we can be cer-         the whole space, the combined cumulative probability of the points
tain that with time the contribution to the LOR will be mostly        corresponding to words is zero, and thus negligible.
                                                                  1036

   The integral in (10) can be reduced to a convolution be-         a Gaussian distribution to simulate the probability of finding a
tween two Gaussians, thus it is itself also a Gaussian distribu-    pseudo-word in the different parts of the lexicon. This Gaus-
tion with mean µw and variance σ2w + σ2 :                           sian was chosen to replicate as closely as possible the distri-
                                                                    bution of words in the lexicon. Therefore it is reasonable to
                                                kx−µw k2
                                  1          −                      employ this same distribution to approximate the contribution
                                               2 σ2w +σ2
          P(I|NW, H ) = p                   e (          ). (13)    of the other words in the lexicon. The contribution of those
                             2π (σ2w + σ2 )
                                                                    others should be relatively smaller than that of the pseudo-
   In practice, the variance of the words over the whole lexi-      words because the pseudo-words could be located anywhere
con is much greater than the variance of the input, σ2w  σ2 .      in the space, while the existing words only occupy a few of
This implies that the expression in (13) will be very close to      those infinite possible locations. We therefore account for the
the original prior on the pseudo-word mean expressed in (12).       contribution of the other words using a parameter 0 ≤ α ≤ 1.
The likelihood of the input given a pseudo-word changes lit-        The approximated OR becomes:
tle with time, and depends only on the centrality of the input
                                                                                           P(x1 , . . . , xt |Wi , H )
in the lexical representational space. In general, the closer         B̂(t) = P(Wi |H )                                  + (1 − P(Wi |H )) α.
the input is to the center of the representational space (µw ),                           P(x1 , . . . , xt |NW, H )
                                                                                                                                              (14)
the greater that the likelihood for the pseudo-word will be,
                                                                    Note that the term corresponding to the non-word likelihood
and the smaller the value of the LOR. The center of the rep-
                                                                    cancels out.
resentational space is the area where a greater number of ex-
                                                                       With this simplification, the condition in (2) can be restated
isting words can be expected: a denser orthographic neigh-
                                                                    in terms or the LOR between one particular word (the one
borhood. Thus, when the input is coming from a dense or-
                                                                    presented) and the non-words:
thographic neighborhood, the recognition of a word will be
more difficult. It will become slower. This implies that the                                                   P(x1 , . . . , xt |Wi , H )
orthographic neighborhood size effect (and neighborhood fre-                 b0 (t) = log P(Wi |H )                                         , (15)
                                                                                                              P(x1 , . . . , xt |NW, H )
quency as well, as µw was computed as by weighting the
contribution of words by their frequency) will have an in-                   b0 (t) ≥ log [Θ − (1 − P(Wi |H )) α] .                           (16)
hibitory component in visual lexical decision, at least when
the pseudo-words used in the experiment were designed to            Note that we have now a case of an asymmetrical thresh-
                                                                    old. The ‘yes’ decisions will be take with the threshold θW               0 =
be similar to the words. This inhibitory component will in
turn be combined with the facilitatory neighborhood density         log [Θ − (1 −  P(Wi |H )) α], and the        ‘no’ will use a threshold
component that was discussed above.                                 θ0NW = log Θ1 − (1 − P(Wi |H )) α . Furthermore, the equa-
                                                                    tions give us an additional constraint on the possible values
                    Response Latencies                              of the α parameter. In order to be useful, the OR in (16)
                                                                    should never have a negative value. Therefore α ≤ Θ1 . To en-
Distribution of the LOR in time                                     sure the maximum possible contribution of the other words to
As discussed above, the time taken to decide whether the in-        the decision process, it is safest to assume that the parameter
put corresponds to a word or to a non-word will be the first        gets its maximum possible value, α = Θ1 .
time T when the LOR in (5) reaches a value greater then θ              The LOR b0 (t) between two possible hypotheses follows a
(‘yes’ response) or lower than −θ (‘no’ response). There-           normal distribution (c.f., Kass & Raftery, 1995). If we now
fore the average RT to a particular word should correspond          integrate to calculate the expectation of b0 (t), we find that the
to the expected time that it takes the LOR to cross the θ bar-      expected value of the LOR at any time t is a linear expression
rier, without having previously crossed the −θ barrier (which       of t:
would have already led to a ‘no’ response).                                                 E(b0 (t)) = K + υ · t,                            (17)
   Closed-form expressions for the mean and variance of the
ORs between the word and non-word hypotheses at each                where with K and υ have the values:
point in time can be obtained by integration. Unfortunately,
the equations that one obtains in this way are computation-                                   K = log P(Wi |H ),                              (18)
                                                                                           σ2w +σ2                                di2
                                                                                                             2       
ally intractable. Using a motivated approximation, we can                       υ = 12 log    σ2
                                                                                                    + 12 σσ2 − 1 + 2(σ2 +σ             2) ,   (19)
                                                                                                               w                  w
obtain an estimate of the reaction times. The value of the
likelihood for the word hypothesis is mostly driven by the          and di is the Euclidean distance between the prototypical rep-
likelihood of the particular word that was presented, with a        resentation of the presented word (µi ) and the center of the
rather small contribution of the other words in the lexicon,        representational space (µw ):
which will be stronger at the earlier stages of processing. We
can thus consider individually the contribution to the likeli-                                   di = kµi − µw k                              (20)
hood of the target word (Wi ), and summarize in a single term
the contribution of all other words (which is in any case mi-        Furthermore, integration also reveals that the variance of the
nor). Our estimation of the likelihood of a pseudo-word used        LOR also follows a (very similar) linear function of time.
                                                                1037

Reaction Times                                                      Moscoso del Prado Martı́n, Schreuder & Baayen, 2004). This
As described above, the value of the LOR between two words          technique enables us to automatically build distributed vec-
follows a Gaussian distribution, whose mean and standard de-        tors representing all orthographic forms in a given language.
viation are a linear function of time. Instead of considering a     These vectors have a fixed dimensionality, and do not require
discrete intake of samples from the distribution at a fixed rate    alignment of the words at their beginnings, or endings (40 di-
– as was done by Adelman & Brown (2008) or Norris (2006)            mensions per vector, for all words). The AoE vectors have
– we can consider the equivalent limiting process in which          successfully been used in large-scale connectionist models of
samples from the input are collected continuously in time.          the processing of Dutch and English words. We used the En-
This limit continuous process is a Brownian Motion with a           glish vectors of Moscoso del Prado and colleagues to esti-
starting value (18), a drift (19), and an infinitesimal variance    mate the distribution of words in the English lexicon. For
(the expected variance of the LOR). If we momentarily ignore        this, we employed the vectors corresponding to all English
the times when the negative threshold is reached first (i.e.,       words appearing in the CELEX database (Baayen, Piepen-
the errors), the distribution of the times for such a process       brock & Gulikers, 1995) with a frequency greater than one.
to reach a particular positive threshold value corresponds to       In order to adapt them to the needs of our model, several
the distribution of first-passage times of the Brownian motion      modifications were done on these vectors. First, to ensure
through a fixed positive barrier. This distribution is known in     that the similarity space is defined by the Euclidean distance
its closed form: First-passage times of a Brownian motion           (the vectors were originally developed for use with angular
follow an Inverse Gaussian distribution (IG). If we have a          measures), we normalized them to modulus one. Second, as
Brownian motion with a starting value K and a positive drift        reported by Moscoso del Prado Martı́n, (2003), these vectors
υ, the expected first passage time of the process through a         tend to represent longer words in central areas of the represen-
positive level θW0 is given by:                                     tational space, which can lead to reversed word length effect.
                                                                    We overcome this problem by linearly scaling the vectors by
                                   θ0 − K                           their word length. Finally, in order to ensure the required
                         E(TW ) =         .                 (21)
                                      υ                             diagonality of the covariance matrix, the vectors were ro-
                                                                    tated using a Principal Component Analysis2 . We used these
This expresses the intuitive notion that the average time to
                                                                    vectors to compute the frequency-weighted mean (µw ' 0)
reach a preset level of certainty starting at time zero from an
                                                                    and the determiner of the corresponding covariance matrix
offset equal to our prior expectations, is equal to the differ-
                                                                    (σ2w = 14.74) to use with the equations defined above.
ence between the desired level to be attained and the initial
offset, divided by the average accumulation of evidence per         Dataset and Model Fit
unit of time.
                                                                    We investigated how accurately would our model predict
   The IG distribution describes the first-passage times
                                                                    VLD RT’s of a previously published dataset. For simplicity,
through the positive threshold. However, it does not con-
                                                                    we chose a subset of the data described by Balota, Cortese
sider whether at that moment the negative threshold has al-
                                                                    and Pilotti (1999) for which a highly detailed analysis of
ready been crossed, in which case an error would have hap-
                                                                    the RTs was provided by Baayen et al., (2006). This subset
pened and the time would not affect our distribution. What
                                                                    contained the average young participants’ VLD responses to
we are interested in is in the distribution of the time taken
                                                                    2, 088 monosyllabic mono-morphemic English words. Using
to cross the positive threshold, provided that the positive one
                                                                    the formulation from the previous section, we computed the
is crossed before the negative. This is expressed by the con-
                                                                    predicted average VLD RT using a geometric Brownian Mo-
ditional probability function p(TW |CorrectResponse,Wi , H ).
                                                                    tion with an absorbing barrier. The drifts, infinitesimal vari-
We can use Bayes’ theorem to calculate this distribution, and
                                                                    ances, and biases were computed directly. The values of the
then integrate to find the corrected distribution of latencies
                                                                    two free parameters of the model the threshold Θ and the vari-
(see Dixit, 1993 for a detailed discussion of these issues).
                                                                    ance of the input error (σ) were set in different ways. On the
Fortunately, in our particular case, introducing this correc-
                                                                    one hand, the value of Θ was set using a Gauss-Newton non-
tion did not produce significantly different results than those
                                                                    linear least-squares regression from the theoretical to the ac-
produced by just applying (21), so for simplicity we do not
                                                                    tual RTs. The value of σ was chosen to be small (σ = .1) rel-
consider it in the remaining discussion.
                                                                    ative to the variance of the words in the lexicon (σw = 3.84).
                  Model Implementation                              We chose this value because it is the point were the parame-
                                                                    ter seemed to reach an asymptote in the prediction of reaction
Orthographic Representations                                        times (in general, the smaller this parameter, the better the
In order to obtain estimates of the reaction times us-              prediction)3 .
ing our method, we need a distributed representation
                                                                        2 These transformed orthographic vectors can be obtained by con-
of the orthographic forms of all English words. For
                                                                    tacting the author.
this purpose, we used the Accumulation of Expecta-                      3 We excluded σ from the non-linear regression because includ-
tions (AoE) technique (Moscoso del Prado Martı́n, 2003;             ing it led to non-convergence of the regression algorithm, as the per-
Moscoso del Prado Martı́n, Ernestus & Baayen, 2004;                 formance keeps improving infinitesimally as its value decreases. In
                                                                1038

                                                                       r^2=.41                                                                                                                                                                 700
                                                                                                                                                                          700
                                                                                                                                                Balota et al. RTs (ms.)
                                                                                                                                                                                                                             Model RTs (ms.)
                                                                                                                                                                                                                                               650
                                                                                                                                                                          650
                                                                                                   ++
                            900
 Balota et al. RT's (ms.)
                                                                                                                                                                                                                                               600
                                                                                                                                                                          600
                                                                                                +                 ++++ ++
                                                                                               ++ ++        +++         ++ + +      +
                            800                                                                ++++++++++        + +++++
                                                                                                                   +       +
                                                                                                                           + + +++                                        550
                                                                                                                                                                                                                                               550
                                                                                         +++++ +          +++
                                                                                                            ++  + + ++++++
                                                                                                                         ++++++
                                                                                                              +  +++
                                                                                                                                                                                −16    −14       −12        −10    −8   −6                           −16    −14       −12        −10    −8   −6
                                                                        ++ + ++         ++  ++++++         ++++++  +   ++++ ++ + +
                                                                             +         + +++  +++ +
                                                                                                  ++++
                                                                                                     + +++ ++ +++
                                                                                                                +  ++
                                                                                                                   +
                                                                                                                   + +++ +++++++
                            700                                              +                   +
                                                                                 +++++++++++++++++++  +  +++++
                                                                                                             ++ ++
                                                                                                                ++ ++ +
                                                                                                                      +
                                                                                                                      ++
                                                                                                                    +++++  +++++++++
                                                                                                                                                                                             log CELEX Frequency                                                  log CELEX Frequency
                                                                + +++       ++++++ +++++++++ ++++++
                                                                                                + ++
                                                                                                   + +++
                                                                                                      +      +
                                                                                                           +++
                                                                                                        ++++
                                                                                                           +  ++
                                                                                                               ++ ++
                                                                                                                   +     +         +
                                               ++ ++++++++++                 +  +++++++  +++++  +  ++
                                                                                                  +++   ++++  ++++   ++++++
                                                                                                                          ++
                                                                                                                          +++ + +
                                                                                                                             +
                                             +   +     +++ ++++++++++    +
                                                                         +
                                                                          ++
                                                                          +
                                                                            ++
                                                                             ++
                                                                              +
                                                                               +
                                                                               + +++
                                                                                 +  ++
                                                                                     +
                                                                                       ++
                                                                                       + ++
                                                                                        ++
                                                                                        +  ++
                                                                                           ++ +
                                                                                             +++
                                                                                               ++
                                                                                                +
                                                                                                 ++
                                                                                                  +
                                                                                                  +
                                                                                                  +++
                                                                                                    +
                                                                                                     +++
                                                                                                     + +++++++
                                                                                                             +++
                                                                                                             +++
                                                                                                               +
                                                                                                                  ++++++
                                                                                                                +++
                                                                                                                  ++ ++ ++
                                                                                                                         ++       +
                                    + ++++++++++                       +  ++++   + +   +  +   + ++++  +
                                                                                                   ++++   +++++++   + +++++
                                                         ++++++
                                                              ++ +++++ ++++  ++
                                                                             +
                                                                             + +++ +++++++
                                                                                         +++
                                                                                          + +++ + ++    ++        ++
                            600               ++++++  ++ +  +++
                                                             ++ ++   ++++    + +++
                                                                                 +++
                                                                                   + +
                                                                                     +  ++
                                                                                        +   +++++++++
                                                                                                   +   ++ +    ++
                                                                                                            ++++ +++++                                                    640
                                       +++ +++
                                             +        ++
                                                       ++      +++  ++++
                                                                       + +
                                                                         ++
                                                                          ++ +
                                                                             + ++++++++  ++
                                                                                          ++ ++++ ++ ++   +++    +       +           +
                                      ++   ++
                                            +++ ++++++++
                                                       ++++
                                                         ++ +
                                                            +
                                                            ++++
                                                               +
                                                               +
                                                              +++
                                                                + +
                                                                  +
                                                                 ++ ++++
                                                                       +
                                                                       +
                                                                    ++++++++++
                                                                            +++++
                                                                                ++
                                                                                 +
                                                                             +++++++
                                                                                  ++
                                                                                   +
                                                                                    +
                                                                                    ++ +
                                                                                      +++
                                                                                        ++++++
                                                                                             ++
                                                                                         ++++++ +++
                                                                                                  ++
                                                                                                   +++++ +++ + ++
                                                                                                      +  ++  + +  ++                                                                                                                           631
                                   ++++ ++++
                                           +++
                                            ++ +
                                               ++ +  +
                                                     ++++
                                                       +++ ++++
                                                              +++
                                                               +
                                                               +
                                                             +++
                                                                ++
                                                                +  ++
                                                                    ++++
                                                                       + +
                                                                         +
                                                                         +
                                                                         +
                                                                        ++ +
                                                                           + + ++++
                                                                                  +  +
                                                                                     ++  +
                                                                                  ++++++++  +   +
                                                                                                +
                                                                                              +++ +
                                                                                                  +
                                                                                                  +        +                 +
                                                                                                                                                                          635
                                     +++++++   ++++++++  ++ +  ++++  ++  +++++ +++
                                                                                                                                                Balota et al. RTs (ms.)
                                                  + ++++   ++
                                                            ++     ++ ++
                                                                      ++++++++     ++++ +
                                                                                                                                                                                                                             Model RTs (ms.)
                                                                                                                                                                                                                                               630
                                                ++ ++++++++         ++                                                                                                    630
                            500                                     + ++ +                                                                                                625
                                                                                                                                                                                                                                               629
                                                                                                                                                                          620
                                  500          550                600                650                 700                750                                                                                                                628
                                                                                                                                                                          615
                                                       Simulated RT's (ms.)                                                                                                      0        5            10         15    20                            0        5            10         15    20
                                                                                                                                                                                      Orthographic Neighborhood Size                                       Orthographic Neighborhood Size
Figure 1: Comparison of the theoretical RT’s predicted by the                                                                               Figure 2: Comparison of the effects of word frequency (top
model (horizontal axis), with the average VLD RT’s of young                                                                                 panels) and orthographic neighborhood size (bottom panels)
participants to the same items in the Balota et al. (1999) study                                                                            on the Balota et al. (1999) RT’s (left panels), and on the RTs
(vertical axis). The dashed line plots the identity relation.                                                                               predicted by the model (right panels). The effects were es-
The solid line is a non-parametric regression between both                                                                                  timated using a least-squares regression analysis on the log
measures.                                                                                                                                   RT’s including non-linear terms considered using restricted
                                                                                                                                            cubic splines. The rugs at the bottom of each panel illustrate
                                        Results and Discussion                                                                              the densities of the counts.
Figure 1 shows the relationship between the predicted aver-
age response latencies using our model (horizontal axis), and                                                                               the diagonal), irrespective of the direction in which we per-
the actual average response latencies of subjects performing                                                                                formed the regression. In fact, rather than pointing to a de-
VLD, taken from the Balota et al. (1999) dataset. The first                                                                                 viation between the model’s predictions and the participants’
thing to notice is that, despite the very large number of items,                                                                            responses, this is an intrinsic property of the reaction times
the model achieves a fairly good prediction of each individual                                                                              distribution. If we were to plot the RTs of individual sub-
latency, with an overall explained variance of over 40%. As                                                                                 jects against the others, we would again find the same non-
illustrated by the overlap between the non-parametric regres-                                                                               linearity. This is no more than a form of regression towards
sion (solid line) and the identity line (dashed line), the relation                                                                         the mean, which in very left-skewed distributions (as that
between both measures is remarkably linear. This indicates                                                                                  of the RTs), is more marked in the lower than in the up-
that the model captures the detailed distribution of response                                                                               per range (Baayen, Moscoso del Prado Martı́n, Schreuder, &
latencies, with enough detail to make item-level predictions.                                                                               Wurm, 2003).
   In addition, one can observe a slight nonlinearity for the
                                                                                                                                               We now turn to examine the role of frequency and neigh-
fastest reaction times. Although relatively small, this devia-
                                                                                                                                            borhood size of the experimental response latencies and
tion from linearity is very robust, and will show up consis-
                                                                                                                                            model predictions. We performed a least-squares regression
tently in re-sampling analyses of this dataset. Interestingly,
                                                                                                                                            on both the experimental and the theoretical RT’s, including
if we were to swap the axes, and perform the non-parametric
                                                                                                                                            log frequency and the orthographic neighborhood size vari-
regression in the opposite direction (i.e., predicting the model
                                                                                                                                            able, and including the possibility of a restricted cubic splines
responses from the experimental RTs), we would find the
                                                                                                                                            for the effects for which the non-linear term reached signifi-
same deviation from linearity in the low range. Counter-
                                                                                                                                            cance. Figure 2 summarizes the effects that we observed in
intuitively, this deviation goes in the same direction (above
                                                                                                                                            these regressions.
addition, a general linear scaling factor was added to speed up the                                                                            First, with respect to frequency (top two panels), we ob-
convergence. Finally and additional intercept fixed to 447ms. was
added to the model based on a separate theoretical study on the (rel-                                                                       serve that the both in the human RT’s (left panel) and model
atively) constant portion of the VLD RT distributions.                                                                                      responses (right panel) it played a clearly linear effect. In-
                                                                                                                                         1039

deed it is not surprising that frequency has a linear effect on                               References
the model predictions. From (21) we can see that log fre-            Adelman, J. S., & Brown, G. D. A. (2008). Modeling lexi-
quency will have a direct linear effect. Both Adelman and              cal decision: The form of frequency and diversity effects.
Brown (2008) and Norris (2006) defended the crucial role of            Psychological Review, 115, 214–227.
log frequency on lexical decision latencies. In the case of          Baayen, R. H., Feldman, L. B., & Schreuder, R. (2006).
Adelman and Brown they could infer that the evidence for log           Morphological influences on the recognition of monosyl-
frequency was superior to the evidence for other counts like           labic monomorphemic words. Journal of Memory and Lan-
rank frequency (Forster, 1976). In Norris’ study, the author           guage, 55, 290–313.
had to conclude that both these counts could be equally good         Baayen, R. H., Moscoso del Prado Martı́n, F., Schreuder, R.,
predictors for a Bayesian model of lexical decision. Notice            L., & Wurm. (2003). When word frequencies may NOT
that our analytical approach enables to conclude for certain           regress towards the mean. In R. H. Baayen & R. Schreuder
that – at least on this type of models – it is log frequency           (Eds.), Morphological structure in language processing.
rather than rank frequency that will drive the decision.               Berlin: Mouton de Gruyter.
   Finally, we come to the issue of orthographic neighborhood        Baayen, R. H., Piepenbrock, R., & Gulikers, L. (1995). The
size effects. On the reaction times, we observed a non-linear          CELEX lexical database (CD-ROM). Philadelphia, PA:
contribution of this variable. Notice that the model, on the           Linguistic Data Consortium, University of Pennsylvania.
other hand, does not show this non-linearity, but rather shows       Balota, D., Cortese, M., & Pilotti, M. (1999). Item-level
a constant inhibitory effect of the variable on the predicted          analyses of lexical decision performance: Results from a
latencies.As we discussed when introducing the likelihoods             mega-study. Abstracts of the 40th Annual Meeting of the
for words and non-words, both of these will contribute in op-          Psychonomics Society, Psychonomic Society, Los Angeles,
posite directions to the neighborhood size effect, with the fa-        44.
cilitation provided by the words being stronger at the earlier       Dixit, A. K. (1993). The art of smooth pasting. Chur, Switzer-
stages of processing. This is indeed the pattern that we ob-           land: Harwood Academic Publishers.
serve on the human reaction times. In order to approximate           Forster, K. I. (1976). Accessing the mental lexicon. In R. J.
the word likelihood, we made the contribution of the ‘other’           Wales & E. C. T. Walker (Eds.), New approaches to lan-
words constant in time (i.e., the α parameter), thus elimi-            guage mechanisms. Amsterdam: North Holland.
nating the non-linearity on the reaction times, which passes         Kass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal
to be dominated just by the inhibition provided by the non-            of the American Statistical Association, 90, 773–795.
word likelihood. In turn, the direction of this effect is op-        Moscoso del Prado Martı́n, F. (2003). Paradigmatic effects
posite to the effect reported by Norris (2006) on his model.           in morphological processing: Computational and cross-
As he assumed non-words to be uniformly distributed in the             linguistic experimental studies. Unpublished doctoral dis-
lexical space, there would be no reason in his model to ex-            sertation, Max Planck Institute for Psycholinguistics &
pect that centrality and non-word density are correlated, thus         University of Nijmegen.
eliminating the inhibitory contribution of the pseudo-words.         Moscoso del Prado Martı́n, F., Ernestus, M., & Baayen, R. H.
Although Norris claims that the actual distribution of pseudo-         (2004). Do type and token effects reflect different mech-
words in the model matters little, in fact we can see that it is       anisms: Connectionist modelling of Dutch past-tense for-
crucial for issues like neighborhood size. In fact, we can pre-        mation and final devoicing. Brain and Language, 90, 287–
dict that this variable will also be affected by the distribution      298.
of the non-words in a real experiment.                               Moscoso del Prado Martı́n, F., Schreuder, R., & Baayen, R.
                                                                       (2004). Using the structure found in time: Building real-
   In sum, we have introduced a fully analytical model of the          scale orthographic and phonetic representations by Accu-
VLD task (that can be run in seconds on a mid-range laptop).           mulation of Expectations. In H. Bowman & C. Labiouse
Although further work on this model is clearly necessary, as           (Eds.), Models of Cognition, Perception and Emotion. Pro-
far as we are aware, this model outperforms any published              ceedings of the VIII Neural Computation and Psychology
model both in terms of coverage (around 40K words vocabu-              Workshop. Singapore: World Scientific.
lary), and item-level performance. Furthermore, by eschew-           Norris, D. (2006). The Bayesian Reader: Explaining word
ing simulations, we can arrive at full-fledge analytical expla-        recognition as an optimal Bayesian decision process. Psy-
nations of effects, rather than relying simply on goodness of          chological Review, 113, 327–357.
fit statistics. The model described here has made use of a           Ratcliff, R., Gómez, P., & McKoon, G. (2004). A diffusion
particular representational technique to represent the ortho-          model account of the lexical decision task. Psychological
graphic variation. Note however that, while the simulated re-          Review, 111, 159–182.
sults might depend on this particular representational scheme,       Wagenmakers, E. J., Steyvers, M., Raaijmakers, J. G.,
the theoretical analysis holds for any representation of the vi-       Shiffrin, R. M., Rijn, H. van, & Zeelenberg, R. (2004).
sual form of words for which a vector-space can be defined,            A model for evidence accumulation in the lexical decision
as long sampling is made based on the distance measure that            task. Cognitive Psychology, 48, 332-367.
defines the space.
                                                                 1040

