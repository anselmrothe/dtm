UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Semantic Cognition as Logical Dimensionality Reduction
Permalink
https://escholarship.org/uc/item/50r1c7qh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 30(30)
Authors
Katz, Yarden
Goodman, Noah D.
Kersting, Kristian
et al.
Publication Date
2008-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Modeling Semantic Cognition as Logical Dimensionality Reduction
             Yarden Katz, Noah D. Goodman, Kristian Kersting, Charles Kemp, Joshua B. Tenenbaum
                                    {yarden, ndg, kersting, ckemp, jbt}@mit.edu
              Department of Brain and Cognitive Sciences & Computer Science and Artificial Intelligence Laboratory
                                                   Massachusetts Institute of Technology
                                                          Cambridge, MA 02139
                              Abstract                                  perspective. While these models can handle noise and un-
   Semantic knowledge is often expressed in the form of intuitive
                                                                        certainty, they also have important limitations, complemen-
   theories, which organize, predict and explain our observations       tary to those of symbolic approaches. They do not naturally
   of the world. How are these powerful knowledge structures            display the systematicity and compositionality that character-
   represented and acquired? We present a framework, logical            ize people‚Äôs intuitive theories and common-sense reasoning
   dimensionality reduction, that treats theories as compressive
   probabilistic models, attempting to express observed data as a       (Fodor & Pylyshyn, 1988). This limitation shows up most
   sample from the logical consequences of the theory‚Äôs under-          clearly when making inferences about novel entities that are
   lying laws and a small number of core facts. By performing           only sparsely observed. Suppose that we encounter two new
   Bayesian learning and inference on these models we combine
   important features of more familiar connectionist and symbolic       kinds of organisms, tufas and kibos, and we are told that kibos
   approaches to semantic cognition: an ability to handle graded,       have some novel property (e.g., they have omulums). If we
   uncertain inferences, together with systematicity and compo-         then learn that tufas are kibos, it is likely that tufas also have
   sitionality that support appropriate inferences from sparse ob-
   servations in novel contexts.                                        omulums. The CQ model makes this prediction, via property
                                                                        inheritance down through the taxonomy of categories. The
             Problems of semantic cognition                             RM network, however, does not automatically yield this in-
A person‚Äôs store of common-sense knowledge about the                    ference, if it is trained on the two facts is a(tufa, kibo) and
world is vast, but it is more than just a vast collection of            has a(kibo, omulum). Because the concept ‚Äòkibo‚Äô plays dif-
facts. In many domains, the mind organizes what it knows                ferent roles in these two propositions‚Äîit is the object of is a
into large-scale systems, with structure at multiple levels of          and the subject of has a‚Äîit is represented in different popu-
abstraction. These abstract systems of representation‚Äîcalled            lations of units, and the effects of training on these two facts
schemas, or theories‚Äîare crucial for our ability to make in-            appear in non-overlapping sets of weights. The network does
ferences that go beyond the sparse and noisy data of percep-            not equate ‚Äòkibo‚Äô in the first proposition with ‚Äòkibo‚Äô in the
tual experience.                                                        second proposition, and so fails to draw the obvious infer-
   How are such powerful knowledge representations ac-                  ence.
quired, structured, and used? These problems lie at the heart              This example does not imply that it would be impossible to
of semantic cognition. Early proposals focused on symbolic              design a connectionist semantic model whose inferences did
structures, as in the semantic networks of Collins and Quillian         respect basic principles of systematicity and compositional-
(1969) (hereafter CQ) for organizing categories of objects and          ity. Our point is only that the connectionist approach does
their properties. Categories are placed in a tree-structured            not naturally capture these aspects of human inference, which
taxonomy, with properties located at nodes of the tree and              are no less essential than the statistical capacities it does cap-
assumed to inherit down to all categories below them. Each              ture well. While it is possible that either the connectionist
property needs to be stored only at the highest node of the tree        approach or the structured symbolic approach could be ex-
where it holds generically, leading to a compact encoding of            tended in some way that makes for a satisfactory solution, our
objects‚Äô properties and the ability to project known properties         aim here is to explore new alternatives for modeling abstract
to novel objects.                                                       semantic knowledge.
   Later work emphasized the limitations of symbolic repre-                We describe an approach that combines valuable capaci-
sentations in handling noisy data or exceptions, and in ac-             ties of both traditional paradigms: an ability to represent ab-
counting for graded effects of similarity on people‚Äôs inductive         stract knowledge respecting systematicity and composition-
judgments. Symbolic approaches were also criticized for not             ality, and hence to make appropriate inferences from sparse
providing a working account of how their abstract represen-             data in novel situations; and an ability to learn from noisy
tations could be learned from experience.                               data and generalize in graded fashion based on the statistics
   An alternative approach to modeling abstract semantic                of observed data. Our approach is based on a hierarchical
knowledge using connectionist networks emerged in the                   Bayesian model over logical representations. This is similar
1980‚Äôs. Hinton proposed a connectionist network that could              in spirit to proposals in inductive logic programming (Mug-
learn abstract systems of kinship relations (Hinton, 1986),             gleton & De Raedt, 1994), although these are not typically
while RM explored a similar architecture for learning about             formalized explicitly as hierarchical Bayesian models (for a
categories and properties (Rogers & McClelland, 2004)‚Äî                  review of this approach see, Tenenbaum, Griffiths, and Kemp
essentially the same problem that CQ treated from a symbolic            (2006)). A close relative of our approach is (Conklin & Wit-
                                                                    71

ten, 1994), where logical theories are learned using a com-           the theory.) Laws in our theories take the form of typed Horn
plexity prior. However, that approach does not attempt to             clauses, commonly used in the logic and inductive logic pro-
construct theories with novel unobservable predicates, which          gramming literature. A proposal for Horn clauses as a psy-
is crucial to understanding many real-world domains and to            chologically plausible representation of theories is found in
our work here.                                                        (Kemp, Goodman, & Tenenbaum, 2007).
   Our framework can be viewed as a kind of dimensional-                 A feature of our framework is that the values of derivative
ity reduction for structured logical theories. We observe data        relations (such as mother, in kinship) can be compressed into
in the form of relations and attributes over a set of objects,        a particular assignment of values for the core relations (such
and we infer a representation of the abstract structure under-        as parent), via the theory‚Äôs laws. The probabilistic genera-
lying these data, expressed in terms of a subset of first-order       tive model we define favors theories that adopt as few core
predicate logic. The observations are high dimensional, and           relations as possible, seeking the optimal compression of the
the inferred underlying abstract structure can be seen as a low       given set of observations.
dimensional ‚Äòspace‚Äô into which the observed objects are em-
bedded.                                                                                                                                          =   spouse
                                                                           (a)                                                                       parent
   While the model instantiates a structured domain theory,
                                                                                           Christopher = Penelope Andrew = Christine
it supports statistical inference via the probabilistic genera-
tive process linking the domain theory to the observed data.
                                                                                  Margaret = Arthur        Victoria = James         Jennifer = Charles
Bayesian inversion of this generative model allows us to in-
fer the low-dimensional abstract structure underlying the ob-                                              Colin       Charlotte
served data.
                                                                                                                                                      is_a
                                                                                                                       has_skin
   We illustrate our approach on simple versions of the kin-                (b)                                            eats
                                                                                                                                                      has_a
ship and taxonomic categorization domains where previous                        has_wings                  animal          breathes
                                                                                                                                            has_fins
                                                                                                                                               can_swim
connectionist approaches have been developed. We show                     has_feathers
that given an appropriate probabilistic domain theory, we                     can_fly       bird                                   Ô¨Åsh           has_gills
can make successful inferences from sparse data in novel                      canary                 eagle            shark                     salmon
contexts‚Äîa setting which has proven challenging for connec-
                                                                         can_sing is_yellow has_claws is_strong can_bite is_dangerous swims_upstream is_pink
tionist models. The problem of learning an abstract domain
theory remains more difficult for our approach than for con-
                                                                      Figure 1: (a) A segment of a family tree for the kinship theory
necionist models, but we show that at least in some simple
                                                                      used in Hinton (1986). (b) A Collins & Quillian-like taxon-
cases, our hierarchical Bayesian formulation allows the cor-
                                                                      omy. Categories in bold red, properties in italics blue.
rect abstract domain theory to be inferred from observations.
             Theories: structure and form
We use the term theory formally as a specification of a set of           (a)                                           (b)
                                                                          Core rels: femaleC : Person √ó Person             Core rels: is aCF : Cat √ó Cat
relations and an associated set of laws and types that govern
                                                                                      spouseCF : Person √ó Person                      has aC : Cat √ó Prop
them. Theories are instantiated by models, or possible worlds,                        childCF : Person √ó Person
corresponding to ways in which a theory may hold for a par-                   Types: Person                                    Types: Cat, Prop
ticular set of objects. For example, we can apply the theory                   Laws:                                            Laws:
                                                                           female(X) ‚Üê femaleC (X)                          is a(X, Y) ‚Üê is aCF (X, Y)
of kinship to reason about a set of individuals in a family, and
                                                                        spouse(X, Y) ‚Üê spouseCF (X, Y)                    has a(X, Y) ‚Üê has aC (X, Y)
later apply it to an entirely different family. We will refer each
                                                                        spouse(X, Y) ‚Üê spouse(Y, X)                         is a(X, Y) ‚Üê is a(X, Z) ‚àß is a(Z, Y)
of these collections of objects that a theory can apply to, and           child(X, Y) ‚Üê childCF (X, Y)                    has a(X, Y) ‚Üê is a(X, Z) ‚àß has a(Z, Y)
their associated abstract structures, as contexts. Relations in-          child(X, Y) ‚Üê child(X, Z) ‚àß spouse(Z, Y)
clude ‚Äòfather‚Äô, ‚Äòchild‚Äô, or ‚Äòspouse‚Äô, while a law might be that         mother(X, Y) ‚Üê female(X) ‚àß child(Y, X)              son(X, Y) ‚Üê ¬¨female(X) ‚àß child(X,Y)
‚Äòthe child of an individual‚Äôs spouse is also their child.‚Äô Every         father(X,Y) ‚Üê ¬¨female(X) ‚àß child(Y, X)             wife(X,Y) ‚Üê female(X) ‚àß spouse(X,Y)
                                                                      daughter(X, Y) ‚Üê female(X) ‚àß child(X, Y)          husband(X,Y) ‚Üê ¬¨female(X) ‚àß spouse(X,Y)
family forms a context, and two models might be one where
Alice is the spouse of Bob, and another in which Alice is the
spouse of Carroll.
   The general framework we follow is shown in Figure 2(a).           Table 1: Logical representations of (a) portion of the kinship
In a generative fashion, our theories produce models, each of         theory, and (b) taxonomy theory. Core relations and functions
which in turn generates observations. Each theory specifies           are denoted with C and F subscripts, respectively.
a set of core relations, whose values are not directly observ-
able. These are analogous to the lower-dimensional space in
numerical dimensionality reduction. The laws of the theory                            A generative model for theories
then relate the core relations to an observable set of deriva-        In this section we expand the basic generative model (Figure
tive or observable relations (the ‚Äòhigh-dimensional space‚Äô of         2(a)), describing in more detail how theories are represented,
                                                                   72

                                          Œª            Œ±                   (d) Sample a set of positive observations, Obsk , from M :
   (a)        Theory                (b) T             Œ∏                                                 Y
                                                                                           P (Obsk ) =       P (oi ‚àà Obsk )
                                                                                                          i
                                                                                                             (
                                               C          Obj                                           Y        1
                                                                                                                     oi ‚àà M,
              Model                                                                                   =        size
                                                                                                          i
                                                                                                                    oi 6‚àà M.
                                               M
                                                                                where size is the number of true facts in M . A small
          Observations
                                                                                non-zero value of  allows the theory to tolerate noise in
                                              Obs
                       Contexts
                                                               K                the observed data.
                                                                             Note that while we focus here on the problem of learning
 Figure 2: (a) The generic structure of a theory learning frame-          from positive observations only, the model can easily be ex-
 work based on dimensionality reduction. (b) An instantiation             tended to learn from negative observations as well. Learn-
 of the generic framework in K contexts, for theories where               ing from positive data is often considerably more difficult
 the core relations are generated independently. Shaded nodes             than learning from both positive and negative data, and we
 denote observed variables for inference given an existing the-           show that promising generalization is possible even in this
 ory.                                                                     less richly observed setting.
                                                                                                Model inference
 how a theory generates a model of the relations over a partic-
 ular set of objects, and how observations are generated from             Given this generative process, we can compute the probabil-
 a model.                                                                 ity that a query q‚Äîa logical atom, such as female(mary) or
    Formally, a theory T is a triple hCore, Laws, œÑ i of core             spouse(mary, jon)‚Äîis true given a set of contexts, a theory,
 relations1 , laws, and types, respectively. The core relations           and a setting of the hyperparameters Œ±=(Œ±, Œ≤). Summing
 specify an unobservable, compressed representation of the                over models of the theory (and restricting to K = 1 for clar-
 domain, while the laws are a set of rules for recovering the             ity), we see that:
 observable properties of the domain from this core repre-
 sentation. Because the laws determine the observable rela-                     P (q | Obs, Obj, T, Œ±) =
 tions given the core relations, fixing the extension of all core
                                                                                     X
                                                                                         P (q | M )P (M | C)P (C | Obs, Obj, T ).
 relations uniquely determines a model. The core relations                           C,M
 themselves are generated independently according to exten-
 sion weights Œ∏i (each core relation Ri has its own extension             The laws of T uniquely determine M given C, so the sum
 weight determining the fraction of ‚Äúcore facts‚Äù that are ex-             over M and the term P (M | C) can be dropped. The remain-
 pected to be true). As a further compression, we allow that              ing sum can be expressed via Bayes‚Äô rule as follows:
 some core relations are functions: if R is a functional rela-
 tion, then for each i, R(i, j) holds for at most one j (note                  P (q | Obs, Obj, T ) ‚àù
 that this reduces the number of independent core facts, since                        X
 columns of R are no longer independent). More formally, the                             P (q | C)P (Obs | C, T )P (C | Obj, T ).      (1)
                                                                                       C
 core relations are generated as follows:
1. For each Ri ‚àà Core, draw a Œ∏i ‚àº Beta(Œ±, Œ≤).                            When there are few objects, the sum over the extensions of
                                                                          the core relations can be computed exactly. In practice, we
2. For every context k ‚â§ K,                                               must often approximate the sum using Gibbs sampling or
  (a) Choose a group of objects Ot ‚äÜ Obj k that belong to                 other Monte Carlo methods; these methods can also be used
        each type t ‚àà œÑ .                                                 to search for single most probable model.
  (b) Generate the core extension Cki for Ri : for every a, b ‚àà
                                                                          Inference in sparse contexts
        Obj k , Ri holds with probability P (Ri (a, b)) = Œ∏i when
        the types of a, b match the type signature of Ri (and             Once a learner acquires a systematic theory, such as kin-
        probability 0 otherwise).                                         ship or taxonomy (as shown in Table 1), a number of infer-
  (c) Complete the model M for Ck , by iterative application              ences about novel, sparsely observed objects can be made.
        of every L ‚àà Laws to Ck , until no additional inferences          When we place probabilities over these logical representa-
        are made.                                                         tions, three general classes of inferences are possible: deduc-
     1                                                                    tive, inductive, and deductive consequences of inductive in-
       For simplicity, we describe the generative process only for the
 case of binary relations; similar processes describe unary or higher-    ferences. We show examples of all three for the theories of
 arity relations.                                                         taxonomy and kinship.
                                                                       73

           (a) a b c                                                  ing to direct edges in the hierarchy, along with the properties
                   mother        father      daughter
            a                                                         true of the leaf-node categories. For instance, we observe
            b
            c                                                         that canaries (a leaf-node category) can sing, are yellow, have
                                                                      wings, and have skin, that eagles also have wings and skin,
                                                                      and that canaries and eagles are both birds and are animals
                    son          wife        husband
                                                                      (along with many other facts). We make no direct observa-
                                                                      tions about the properties of birds, fish or animals, though.
                                                                      If we then search for the best-scoring model, we recover the
                                                                      configuration of core relations shown in Figure 1(b): the ex-
           (b)
                                  e= f                                tension of is aCF and has aC includes only the minimal set
                                                                      of is a and has a links needed to capture all observations un-
                          a=b           g=h
                                                                      der the theory‚Äôs laws. Each property is attached to only one
                                                                      category in the is a hierarchy, the lowest superordinate of all
                        c         d i         j
                                                                      categories with that property. We compress out the correla-
                                                                      tions in the observed properties of leaf-node categories, by
Figure 3: (a) (a . . . j √ó a . . . j) matrices of observable rela-    positing properties true of abstract superordinates which are
tions in the kinship domain. Black and grey entries indicate          not themselves ever directly observed.
observed and inferred relations, respectively. White entries
                                                                         Now suppose that we learn of two new objects, a and b,
indicate relations inferred to be false. (b) Inferred family tree,
                                                                      by making the following minimal observations: is a(a, fish),
compactly representing all inferences in (a). Females shown
                                                                      is a(b, animal). The classic CQ approach would infer many
in bold italics, males in ordinary font (c‚Äôs gender is unknown,
                                                                      common-sense inferences about a, b, e.g. that both breathe,
indicated by a circle.)
                                                                      that a can swim, and so on. Our best-scoring model does
                                                                      as well. Conditioned on the inferences described in the pre-
Kinship Consider the family relations shown in Figure 3.              vious paragraph, that familiar properties like swimming and
In light of the logical theory of kinship, many inferences            breathing are probably true of the unobserved superordinate
could be made. For example, the gender of the family mem-             categories fish and animal, we now infer that these properties
bers can be deduced with certainty, since mothers are always          also hold for the new species a and b, for which we have ob-
female, sons are always males, and so forth.                          served only a single is a relation each. This example shows
   In addition to deductive inferences, the observations invite       how we capture a general feature of common-sense reasoning
several plausible inductive inferences. Since a and b are both        by combining the power of induction and deduction: an in-
observed to be parents of c, it is plausible that a is the spouse     ductive leap to the likely properties of unobserved superordi-
of b. In light of this inductive inference, one can infer‚Äîby          nate categories, with deductive inference of the consequences
application the logical laws of kinship‚Äîthat d is the daughter        that follow.
of b, since d was already observed to be the daughter of a.           Property induction We now show how intuitive patterns of
This is an example of a deductive consequence of an inductive         graded, uncertain inference, usually thought to weigh against
inference. Note how our initial inductive leap, that a and b are      symbolic representations of human semantic knowledge, can
married, led to a deductive inference that allowed us to make         also be captured in our probabilistic logical framework. We
efficient use of our observations. The best model found for           consider a simple case of property induction. For tractability
this context via greedy stochastic search contains all of these       and ease of presentation, we explore these phenomena us-
inferences, deductive and inductive. The core relations for           ing a pared-down version of the Collins & Quillian example,
this best-scoring model correspond to the family tree shown           shown in Figure 4(a). We call this structure a balanced taxon-
in Figure 3(b); the observable relations that it predicts will be     omy, where both properties and nodes are distributed evenly
true (or false) are indicated by the squares colored gray (or         along all branches of the tree. In this case, each node has
white, respectively) in Figure 3(a).                                  a single unique property, in addition to properties it inherits
   Essential to these inferences is the fact that the identity of     from its parent nodes. For instance, a‚Äôs unique property is p3,
objects remains the same even if they play different ‚Äòroles‚Äô          while f‚Äôs unique property is p5, and both inherit p1 from their
in distinct scenarios. In our observations, we see two roles          parent node g.
for a: one in which it is the mother of c (as first argument to          Given observations of direct is a links and properties of
the mother predicate), and another in which it is the parent          leaf node category, we can then query for the probability that
of d (as the the second argument to daughter.) The effective          each property is true of each category in the hierarchy. Fig-
integration of information from both the deductive and induc-         ure 4(c) shows the probabilities of all queries for the property
tive inferences just shown relies heavily on the identity of a        p3, which is observed only at one leaf node category, a. As
in these two distinct roles.                                          expected, the probability that a has p3 is near certain. How-
Taxonomy Consider the full taxonomy given in Figure                   ever, other less certain generalizations are found. It is plausi-
1(b). Suppose that we observe only the is a links correspond-         ble but not likely that e has p3, and very unlikely that g or f
                                                                   74

         (a)                                 p1                  is_a          omy and then predict the unobserved propositions this model
                                                                 has_a
                                                                               entails.
                             p2              g             p5                     We evaluated performance by computing the number of
                                                                               incorrect observable propositions entailed by the recovered
                                    e                  f                       model. This number includes two types of errors: ‚Äúfalse
                                                                               alarms‚Äù (false propositions predicted to be true) and ‚Äúmisses‚Äù
                             a          b         c          d                 (true propositions predicted to be false). Figure 4(b) shows
                                                                               the results, averaged across five trials at every level of spar-
                             p3         p4        p6       p7                  sity. Generalization is perfect when all 27 true propositions
         (b)
                        10
                                                                               are observed. This means that none of the 71 false propo-
          # Errors
                                                                               sitions were incorrectly predicted to be true. Generalization
                        5
                                                                               decreases gradually as the data become sparser, with errors
                        0                                                      distributed among both misses and false alarms. Even at the
                             100%        80%           70%          60%
         (c)                                                                   highest levels of sparsity, generalization is quite good: we
                        1
          Probability
                                                                               observe only 16 (‚âà 60% of 27) of the 98 observable propo-
                   0.5                                                         sitions and we make about 7 errors on average, inferring the
                        0                                                      correct truth values for (on average) 91/98 observables.
                                      )
                         p3) p3) p3 p3) p3) p3) p3)
                  _ a (a, a(b, _a(c, a(d, _a(e, _a(f, a(g,
                         _   s      _          s     _
               has has ha has has ha has                                                      Learning a logical theory
                                                                               We now address the problem of learning the theories we‚Äôve
Figure 4: (a) Balanced taxonomy. (b) Simulations of CQ con-                    described. We do so by defining a prior distribution P (T )
texts with different sparsity levels. (c) The probabilities of                 over theories. Following Kemp et. al., we take a represen-
various queries under full observations of leaf node proper-                   tation length (RL) approach . Intuitively, given the choice
ties. Results obtained by Gibbs sampling core extensions of                    between two theories, a RL prior will favor the one that is
the theory (using 6000 samples.)                                               less complicated to write. The precise definition of RL is tied
                                                                               to a choice of language, which in our case is the language of
                                                                               Horn clauses.
                                    1                                             As before, the distribution is described as a generative pro-
has it. This is explained by the size  factor in our likelihood.
If g had p3, all nodes in the hierarchy would inherit p3, mak-                 cess. Given an assignment of values to the hyperparameters
ing the fact of our first and only observation of the property                 Œ±, Œª, theories are generated as follows:
at one specific node a relatively less likely. The same size
                                                                               1. Generate a number Œ≥ of core relations in T : Œ≥ ‚àº
principle weighs against generalization to e, but less so, since
                                                                                  Poisson(Œª).
e is not so far up in the taxonomy. We also see a pattern of
similarity-based generalization along the leaf node categories                  (a) For every generated core relation R, choose its arity,
of the is a hierarchy. The closer a category is in the tree to a,                   where P (R is binary) ‚àº Bern(Œ∏a ) and whether it is
the more likely it is to have p3.                                                   functional, P (R is functional) ‚àº Bern(Œ∏f ). We assume
                                                                                    Œ∏a , Œ∏f are given.
Systematic study of generalization
                                                                               2. Draw a set of laws, scored according to their RL. The RL
We now study more systematically how well our approach                            in our case is a count of the number of total predicates,
generalizes beyond the observed data, as a function of the                        variables and clauses that appear in the laws:
sparsity of the data it receives. Inspired by the simulations of
Hinton (1986) for the theory of kinship, we ran simulations in                                     P (Laws) ‚àù 2‚àírl(Laws)
which we observed randomly sampled sets of the observable                         where rl(Laws) = #cp + #vars + #clauses . We assume
facts in the balanced taxonomy domain and searched for the                        every L ‚àà Laws is syntactically well-formed (otherwise,
best scoring model under the taxonomic theory. In this do-                        P (Laws) = 0.)
main there are a total of 98 observable propositions (7 √ó 7 =
49 is a propositions, and the same number of has a proposi-                       We now consider a case of competing theories, by eval-
tions), of which 27 are true and 71 are false. For instance,                   uating the true taxonomy theory discussed earlier against
has a(a, p2) is true, while has a(b, p5) is false. In keeping                  seven variants, shown in Table 2. These were generated
with our focus on learning from positive examples, we ob-                      by considering all theories that can be constructed by inclu-
served a fraction of the true observable propositions‚Äî100%,                    sion/omission of the following features: (1) the is a transi-
80%, 70%, or 60% of the 27 possible positive examples‚Äîand                      tivity law (L1), (2) the property has a inheritance law (L2),
searched for the best scoring model under the taxonomic the-                   and (3) having the is a relation be a function. L1 and L2
ory. Rather than merely memorizing the given true facts, we                    are the third and fourth laws in Table 1(b), respectively. To
seek to compress the data via inferring the underlying taxon-                  see which theory is favored on a given data set, we first
                                                                          75

              T1      T2    T3     T4     T5     T6    T7      T8        And finally, (3) Does it compress and generalize information
              ‚àö       ‚àö                   ‚àö      ‚àö
  L1                        √ó      √ó                    √ó      √ó
              ‚àö       ‚àö     ‚àö      ‚àö                                     from distinct but isomorphic context?
  L2                                      √ó      √ó      √ó      √ó
              ‚àö             ‚àö             ‚àö             ‚àö
  is a func.          √ó            √ó             √ó             √ó            We believe that our proposed framework fares well on all
  log score  ‚àí150   ‚àí180   ‚àí163   ‚àí185   ‚àí171   ‚àí200  ‚àí164   ‚àí206        three. First, the use of a logical framework makes our rep-
                                                                         resentation transparent, allowing for direct and interpretable
Table 2: Log scores for true taxonomy theory (shown in bold)             comparisons of the internal structure of two learned repre-
and its variants.                                                        sentations. It is not obvious how to make such comparisons
                                                                         in a connectionist setting. At the same time, Bayesian infer-
look for the best scoring model of each theory using greedy              ence over these structured representations allows us to make
stochastic search. We then score each theory together with               inductive leaps from sparse and noisy data, overcoming a ma-
its best model according to the joint posterior probability              jor flaw of traditional symbolic approaches. An advantage of
P (T, M, C | Obs, Obj, Œª, Œ±). Table 2 shows the log scores               our framework is that the engine of deduction (logical repre-
of the eight theories, taking as data all true observable propo-         sentations), and the engine of induction (Bayesian inference),
sitions in the balanced taxonomy domain.                                 cooperate and supplement each other to reason in sparse con-
    Note that all the variants of the true theory are favored by         texts. We have shown the benefits of this approach for the
our prior, as they are less complex. We can also see that the            theories of kinship and taxonomy.
score ordering for these theories is sometimes structured and               In future work, we would like to develop more scalable in-
monotonically decreasing as we go from more to less com-                 ference algorithms and a more expressive language, allowing
plex theories. While the prior favors simplicity, the posterior          unobserved entities and probabilistic generative mechanisms.
ought to favor the more complex of the variants we consider,             This should let us explore more realistic theories, such as the
since these have greater predictive power. For example, T2               dynamic theory of Mendelian genetics. A second direction is
is penalized for lacking is a as a function, and T3 for lacking          to study how well our approach captures people‚Äôs reasoning
L1, but T4 is penalized more heavily than either for lacking             in the laboratory, following (Kemp, Goodman, & Tenenbaum,
both.                                                                    2008).
    Similarly, theories having is a as a function are preferred             Acknowledgements We thank Vikash Mansinghka, Brian
to those that do not. A learner who believes is a is only a              Milch, and Daniel Roy for helpful discussions. This work was
relation and not a function might believe that a shark is both           funded in part by AFOSR grant FA9550-07-1-0075 and the James
a fish and a bird, leading to a penalty in the likelihood of our         S. McDonnell Foundation Causal Learning Research Collaborative.
model. Fixing is a as a function rules out these cases and so
                                                                                                   References
gives a more compact encoding of the observations.
    This monotonicity property does not hold of all theories             Collins, A., & Quillian, M. (1969). Retrieval time from se-
(compare T3, T5, and T7.) This can be explained by the fact                 mantic memory. Journal of Verbal Learning and Verbal
that L1 and L2 are not independent. A learner who does not                  Behavior.
know about the inheritance of properties (T7) would gain lit-            Conklin, D., & Witten, I. H. (1994). Complexity-based in-
tle by adopting L1, in the current data set, and vice versa,                duction. Machine Learning, 16(3), 203-225.
depending on the statistics of the observations.                         Fodor, J., & Pylyshyn, Z. (1988). Connectionism and cogni-
                                                                            tive architecture. a critical analysis. Cognition, 3-71.
    Our purpose was to demonstrate that our generative model
                                                                         Hinton, G. E. (1986). Learning distributed representations of
can be used as a evaluation metric for theories. We leave
                                                                            concepts. In Proceedings of the Eights Annual Meeting of
open the orthogonal question of how actual learners select
                                                                            the Cognitive Science Society.
hypotheses to evaluate from this vast space. Though we used
                                                                         Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2007).
a stochastic search for hypothesis selection here, more work
                                                                            Learning and using relational theories. In Advances in Neu-
is needed to see if any search-based account could provide
                                                                            ral Information Processing Systems 20.
a solid foundation for theory learning. For any search-based
                                                                         Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2008). The-
proposal to work, the probability landscape of theories must
                                                                            ory acquisition and the language of thought. In Proceedings
have tractable properties, such as the monotonicity property
                                                                            of Thirtieth Annual Meeting of the Cognitive Science Soci-
we considered above. An area of future work is determining
                                                                            ety.
when these conditions hold for theories in general.
                                                                         Muggleton, S., & De Raedt, L. (1994). Inductive logic pro-
             Conclusions and future work                                    gramming: Theory and methods. Journal of Logic Pro-
                                                                            gramming, 19/20, 629-679.
In a classic paper on distributed representations, Hinton out-           Rogers, T., & McClelland, J. (2004). Semantic cognition: A
lined three questions to ask of all systems of knowledge rep-               parallel distributed processing approach. MIT Press.
resentation: (1) Does the representation create ‚Äúsensible in-            Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006).
ternal representations‚Äù for the entities it processes, in a way             Theory-based Bayesian models of inductive learning and
that is sensitive to the (potentially latent) regularities in its in-       reasoning. Trends in Cognitive Sciences, 10, 309‚Äì318.
put? (2) Does it generalize to unobserved truths of a domain?
                                                                      76

