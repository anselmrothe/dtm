UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Domain Structures
Permalink
https://escholarship.org/uc/item/0hk260pj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Kemp, Charles
Perfors, Amy
Tenenbaum, Joshua B.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                         Learning Domain Structures
                            Charles Kemp, Amy Perfors & Joshua B. Tenenbaum
                                               {ckemp, perfors, jbt}@mit.edu
                                          Department of Brain and Cognitive Sciences
                                             Massachusetts Institute of Technology
                           Abstract                                structure, although with repeated training, its hidden
                                                                   unit representations may implicitly come to approximate
   How do people acquire and use knowledge about do-               the taxonomic relations between biological species.
   main structures, such as the tree-structured taxonomy              This paper proposes an alternative approach – struc-
   of folk biology? These structures are typically seen ei-
   ther as consequences of innate domain-speci c knowl-            ture learning – that combines important insights from
   edge or as epiphenomena of domain-general associative           both of these traditions. Our key contribution is to
   learning. We present an alternative: a framework for            show how structured domain representations can be ac-
   statistical inference that discovers the structural princi-     quired within a domain-general framework for Bayesian
   ples that best account for di erent domains of objects
   and their properties. Our approach infers that a tree           inference. Like nativists, we suggest that different do-
   structure is best for a biological dataset, and a linear        mains are represented with qualitatively different struc-
   structure (“left”–“right”) is best for a dataset of peo-        tures, and we show how these structured representations
   ple and their political views. We compare our proposal          serve as critical constraints on inductive generalization.
   with unstructured associative learning and argue that           Like empiricists, though, we emphasize the importance
   our structured approach gives the better account of in-
   ductive generalization in the domain of folk biology.           of learning, and attempt to show how domain structures
                                                                   can be acquired through domain-general statistical in-
                                                                   ference. This is not only more parsimonious than the
   Psychologists have argued that cognition in differ-             nativist position, but allows us to explain the origin
ent domains draws on qualitatively different mental                of structured representations in novel domains, where
representations. Tree structures appear well-suited to             the prior existence of domain-specific innate structure is
representing relationships between animal species [1,              highly implausible.
2, 10], while a one-dimensional structure (the liberal-
                                                                      After describing our structure learning framework, we
conservative spectrum) seems better for representing
                                                                   present two empirical tests of its performance. First, we
people’s political views. The possibility of different
                                                                   show that it chooses the appropriate domain structure
structures raises a fundamental question: how do peo-
                                                                   for both synthetic and real-world data sets. It correctly
ple learn what kind of structure is appropriate in each
                                                                   chooses a tree structure for a biological domain (animal
domain?
                                                                   feature judgments), and a linear structure for a politi-
   The standard approach to this question is to reject             cal domain (US Supreme Court decisions). Second, we
one of its assumptions. Nativists deny that core struc-            model two classic data sets of inductive judgments in bi-
tures are learned, at least for evolutionarily important           ology [13] and show that our framework performs better
domains like folkbiology. Instead, infants come equipped           than an unstructured connectionist approach.
with innate knowledge about which structures are appro-
priate for which domains. Atran [1], for example, argues
that folkbiology is a core domain of human knowledge,
                                                                             Bayesian structure learning
and that the tendency to group living kinds into hier-             Our proposal takes the form of a rational analysis. We
archies re ects an “innately determined cognitive struc-           aim to demonstrate the computational plausibility and
ture.” More generally, Keil [8] has argued that ontologi-          explanatory value of Bayesian structure learning, but
cal knowledge obeys an innate “M-constraint”, requiring            leave for future work the question of how these com-
the extensions of predicates to conform to rigidly tree-           putations might be implemented or approximated by
structured hierarchies of objects.                                 cognitive processes. Assume the learner’s data consist
   Alternatively, empiricists generally deny that struc-           of a binary-valued object-feature matrix D specifying
tured representations are present at all. Domain-specific          the features of each object in a given domain. In bi-
representations are merely emergent properties of un-              ology, for instance, the rows of D might correspond to
structured, domain-general associative learning architec-          species, and the columns to anatomical and behavioral
tures. McClelland and Rogers [12], for example, have re-           attributes. The entry in row i and column j would then
cently suggested that the acquisition of semantic knowl-           specify the value of feature j for species i. Structure-
edge in domains such as intuitive biology can be ex-               learning includes computational problems at two levels.
plained as learning in a generic connectionist network.            First, which structure class is most appropriate for the
Their architecture never explicitly represents any tree            domain? Second, given a structure class, which structure
                                                               672

in that class provides the best account of the data?             from the distribution p(S|D, Ci ). We then approximate
   For instance, suppose that a learner exposed to bi-           p(D|Ci ) by the harmonic mean estimator [7]:
ological data ends up organizing animal species into a
taxonomic tree. The first problem asks how she knew to                                                      −1
                                                                                             m
use a tree rather than some other kind of structure. The                                  1 X        1        .
                                                                           p(D|Ci ) =                                    (2)
second problem asks why she settled on one specific tree                                 m j=1 p(D|Sj , Ci )
instead of the many other trees she might have chosen.
Our focus here is on the first problem – the problem of
inferring the right structure class for a domain. A so-          This estimator does not satisfy a central limit theorem,
lution to the second problem, however, falls out of our          and can be thrown off by a sample with very low like-
probabilistic approach.                                          lihood. Despite its limitations, it is often sufficient to
   We assume that learners come to a domain equipped             identify a model that is very much better than its com-
with a hypothesis space of structure classes, either             petitors. In future work we plan to estimate these inte-
constructed from innate primitives or based on analo-            grals more accurately using path sampling [4].
gies with previously learned domains. For simplicity,
this paper considers a hypothesis space of just three              From structures to probabilistic models
canonical classes: taxonomic trees, one-dimensional (lin-        We will work with three probabilistic models, each ap-
ear) spaces, and independent feature models. People              propriate for a different structure class, and show how
surely have access to other classes, including higher-           to compute the likelihoods p(D|S, Ci ) for structures in
dimensional spaces, at (non-hierarchical) clusterings,           each class. For simplicity we assume here that all fea-
and causal networks. We leave it to future work to               tures are binary, but our framework extends naturally to
characterize the full range of structure classes accessible      multi-valued or continuous features.
to human cognition. In particular, it is an open ques-
tion whether this space is small enough to be explicitly         CT : Taxonomic trees
enumerated as we do here, or is so large (perhaps infi-
                                                                 Class CT is the set of taxonomic trees — rooted trees
nite or uncountable) that it can be specified only implic-
                                                                 with the objects in D as their leaves. This is a natural
itly through some generating mechanism. Future work
                                                                 representation when the objects are the outcome of an
should also consider the possibility that multiple struc-
                                                                 evolutionary process. We restrict ourselves to ultramet-
tures may apply within a single domain.
                                                                 ric trees — trees where each leaf node is at the same
   Given a set of probabilistic models, Bayesian tech-
                                                                 distance from the root.
niques can be used to evaluate which of the models is
most likely to have generated some data [7]. Before these           Assume that each feature is generated by a mutation
techniques can be applied to inferring domain structures,        process over the tree. We formalize the mutation process
we need to associate each structure class in our hypoth-         using a simple biological model [11]. Suppose that a fea-
esis space with a probabilistic generative model for the         ture F is defined at every point along every branch, not
features of objects. The next section defines these mod-         just at the leaf nodes where the data points lie. Imag-
els, but here we show how Bayesian inference can be used         ine F spreading out over the tree from root to leaves
to choose between them.                                          — it starts out at the root with some value and could
   Let D be an object-feature matrix generated from one          switch values at any point along any branch. Whenever
of several structure classes. The posterior probability          a branch splits, both lower branches inherit the value of
of each class Ci is proportional to the product of the           F at the point immediately before the split. Figure 1(a)
likelihood p(D|Ci ) and the prior probability p(Ci ). If we      shows one mutation history for a binary feature on a tree
assign equal prior probabilities to each class (as we do         with four objects.
throughout this paper), the best class is the class that
makes the data most likely.                                          (a)                         (b)
   Computing the likelihood p(D|Ci ) requires integrating
over all structures S belonging to structure class Ci :
                       Z
            p(D|Ci ) = p(D|S, Ci )p(S|Ci )dS,            (1)
                                                                      A     B       C    D        A     B        C     D
Intuitively, this means that a structure class Ci provides       Figure 1: (a) A tree with four objects (A, B, C and
a good account of object-feature data D if the data are          D) and three internal nodes. A mutation history for a
highly probable under a range of structures S in class           single feature is shown. The feature is off at the root, but
Ci , and if these structures themselves have high prior
                                                                 switches on at two places in the tree. Shaded nodes have
probability within Ci . The following section explains
how the fit of each structure to the data, p(D|S, Ci ), is       value 1, clear nodes have value 0, and crosses indicate
computed for several structure classes.                          mutations. (b) A line with four objects.
   We estimate the integral in Equation 1 using stochas-
tic approximations. First we run a Markov chain Monte               We formalize this model of mutation using a Poisson
Carlo simulation to draw a sample of m structures, {Sj },        arrival process. Under this process, the probability that
                                                             673

F switches values between the beginning and end of any           that θi is the weight of the coin for feature i, and our
branch b is                                                      prior on θi is θi ∼ Beta( , ) (for each of our experi-
                                                                 ments we use       =     = 1). If column i of matrix D
                                        1  e−2λ|b|               contains miQones and ni zeros, it can be shown that
          p(switch along branch b) =               ,     (3)
                                            2                    p(D|C0 ) = i B(mi + , ni + )/B( , ), where B(·, ·)
                                                                 is the beta function.
where |b| denotes the length of b, and λ is the mutation
rate. Note that the mutation process is symmetric: mu-           Model complexity and Occam’s razor
tations from 0 to 1 are just as likely as mutations in the
other direction. Asymmetric mutation processes may be            The three models CT , CL , and C0 vary significantly in
more appropriate in some contexts.                               their complexity. Both the tree model CT and the linear
   Assume that the features are conditionally indepen-           model CL include the independent feature model C0 as
dent given the tree (i.e., their mutation histories are          a special case: when each object in CT or CL is a long
independent). We can then compute p(D|T , CT ), the              way from its neighbors, feature values at adjacent object
probability of the data given tree T by multiplying prob-        nodes are generated in effect by tosses of a fair coin. CT
abilities for each feature vector taken individually. The        is also more complex than CL : in a domain with n ob-
necessary calculations can be organized efficiently using        jects, there are roughly 2n more distinct tree structures
a Bayes net with the same topology as T [9].                     than distinct linear structures, and the mutation process
   Computing the total likelihood p(D|CT ) requires inte-        operating over each tree involves roughly twice as many
grating over the space of all trees (including variations in     potential mutation events.
branch length and topology), as in Equation 1. We used              A key feature of Bayesian model selection is that it au-
the MrBayes [6] program for Bayesian phylogenetic infer-         tomatically penalizes unnecessarily complex structures.
ence to draw a sample of trees {Ti } from the distribution       Some form of Occam’s razor is essential when comparing
p(T |D, CT ). We then estimated the likelihood p(D|CT )          candidate domain structures of different complexities,
using the harmonic mean estimator (Equation 2).                  where the more complex structure (e.g., trees) can more
                                                                 easily mimic the simpler structure (e.g., linear orders)
CL : One-dimensional (linear) spaces                             than vice versa. A more naive approach to structure
Although trees seem appropriate for representing bio-            learning, such as choosing the structure that accounts for
logical species and their properties, other domains will         the most variance in the object-feature matrix D, would
have other kinds of structures. Euclidean spaces figure          be biased against choosing the simpler model class, even
prominently in mathematical models of similarity com-            when it really generated the observed data.
parison, judgment, and choice, and probably should ap-
pear in any canonical list of structure classes. Let class           Empirical tests of structure learning
CL indicate the set of one-dimensional linear structures.
Extensions to higher dimensions are easy in principle, if        Synthetic Data
computationally more demanding.                                  We created three synthetic datasets (unconstrained,
   A line L ∈ CL is a one-dimensional structure where            tree-structured and linear) with 16 objects and 120 fea-
every node corresponds to an object in the domain. A             tures each. The unconstrained set was constructed using
line is a degenerate tree, but unlike the trees of the pre-      model C0 . The tree-structured set was built by running
vious section, lines have no latent nodes. A four-object         the mutation process of CT over a balanced tree with 16
line is shown in Figure 1(b).                                    leaf nodes. The linear set was built similarly by running
   Features are generated over a line according to the           the mutation process over a line with 16 nodes.
mutation model of the previous section. Imagine that                Table 1 shows log likelihoods computed for each
Feature F starts at the leftmost node with some value            dataset and structure class. The first row shows that
and spreads to the right with the possibility of switching       the linear model CL is better than the tree model CT on
value at any point. Again, the probability that adjacent         the unconstrained data, but that both are worse than
nodes separated by a branch of length |b| have different         the independent features model C0 . Similarly, the lin-
                    −2λ|b|
values of F is 1−e 2       .                                     ear model is preferred for the synthetic linear data. The
   As with CT , we estimate the likelihood p(D|CL ) with         results for the synthetic tree data are more interesting.
an approximate (MCMC) sum over all linear structures.            Even though the data were generated over a tree, the
                                                                 structure class of choice is CL .
C0 : Independent Features                                           To see why a linear order is a good hypothesis when
Class C0 is similar to a null hypothesis. Unlike the pre-        a tree-structured domain is first encountered, imagine a
vious models, it assumes no underlying relationships be-         picture of the true tree, then remove all the branches
tween objects in the domain. Each feature is distributed         and internal nodes, leaving behind only the leaves in
over objects independently of all other features. The            some linear order. Now join each leaf node to its imme-
pattern of overlap in feature extensions is thus com-            diate neighbors. This linear order is a better hypothesis
pletely unconstrained. More formally, C0 assumes that            than the true tree at first. The linear model CL is sim-
feature vectors (columns of D) are generated by ipping           pler than the tree model CT , and if the mutation rate is
weighted coins. Unlike the previous two cases, the like-         small, most concepts generated over the tree will be con-
lihood p(D|C0 ) can be computed analytically. Suppose            nected subsets of the linear order. Only as more features
                                                             674

                 Data                         C0     CL     CT                   Of the three classes in our hypothesis space, Table 1
                 Synthetic Unconstrained      59     31      0                confirms that trees provide the best account of the bi-
                 Synthetic Linear              0    544     300               ological data and linear structures are best for the vot-
                 Synthetic Tree                0    210     168               ing data. Note that a more naive approach to structure
                 Biology                       0    230     339               learning fails here. An additive tree model accounts
                 Political                     0    1312    883               for more of the variance of the Supreme Court data
                                                                              than a one-dimensional metric scaling solution. Choos-
Table 1: Scaled log-likelihoods for three synthetic and                       ing the model that accounts for the greatest proportion
two real-world datasets. Each row has been scaled addi-                       of the variance incorrectly favors trees, since it ignores
tively so that its smallest entry is zero.                                    the greater complexity of the tree model.
                                                                                 Once the structure class is known, we can identify the
accumulate should a rational learner conclude that the                        member of that class that makes the data most likely.
extra complexity of a tree-structured model is necessary.                     For the animal data, we took our MCMC sample from
   To confirm that the true domain structure will eventu-                     the posterior over tree structures, and identified the most
ally win out, we generated a tree-structured set with 32                      representative tree using the consense program in the
objects and 240 features and computed log likelihoods as                      PHYLIP package [3]. The resulting tree is shown in
more and more features were observed. Figure 2 shows                          Figure 3(a). Similarly, the best linear structure for the
that the linear structure is preferred while the number                       Supreme Court data is shown in Figure 3(b).
of observed features is small, but that the correct tree                         The ultimate reason why trees are appropriate for bi-
structure dominates in the end. This transition suggests                      ological data is that evolution is a branching process.
that our Bayesian model may offer some insight into the                       It is harder to say a priori why the voting data should
dynamics of development. Piaget and others have ar-                           be one-dimensional, but the political spectrum (“left”–
gued that children move from simple to relatively com-                        “right”) is an extremely common notion, and others have
plex conceptual structures as they mature. Our model                          analyzed Supreme Court data and found that the first
shows an analogous shift in tree-structured domains.                          dimension of a multidimensional linear model explains
                                                                              almost all of the variance [15]. Our results may explain
                                           Figure 2: Differences be-          in part why people represent these domains as they do,
                200                                                           but the analysis is mute with respect to the precise mech-
LL(CT)−LL(CL)
                                           tween the log likelihoods
                                                                              anisms that give rise to these cognitive structures. Multi-
                100                        of trees (CT ) and lin-            ple learning mechanisms probably operate in both these
                                           ear structures (CL ) on            domains. Likely mechanisms include inferences drawn
                  0                        synthetic tree-structured          from feature observations, as modeled explicitly by our
                                           data. Linear structures            Bayesian learning algorithm, as well as cultural trans-
       −100                                are preferred at first but         mission of knowledge, which surely occurs for structures
                        30 60 120 240      the true structure be-             like the “left”–“right” metaphor.
                      Number of Features   comes clear as more fea-
                                           tures are seen.                       Structure learning versus empiricism
Biological and Political Data                                                 The conventional empiricist critique of structured do-
We used our framework to infer the structure of a bio-                        main representations has three lines of attack, well ar-
logical data set (expected to be tree-structured), and a                      ticulated recently by McClelland and Rogers [12]: (1)
political data set (expected to be linear). The biological                    structured representations such as taxonomic trees are
set was constructed from human feature judgments col-                         too rigid to deal naturally with exceptions or gradients
lected by Osherson et al. (1991). Subjects were given 48                      of typicality; (2) it is not clear how structured repre-
animals and 85 features (eg ‘lives in water’, ‘has a tail’)                   sentations can be induced from raw data; (3) unstruc-
and asked to rate the “relative strength of association”                      tured associative learning architectures can match all of
between each animal and feature. Subjects gave ratings                        the supposed advantages that structured representations
on a scale that started at zero and had no upper bound.                       claim. Our work challenges all of these critiques. Pre-
Ratings were linearly transformed to values between 0                         viously [10], we showed that robustness to exceptions
and 100, then averaged. We created a binary dataset by                        and sensitivity to typicality fall out naturally from defin-
thresholding all values at the global mean.                                   ing a probabilistic generative model of object features in
   The political dataset was taken from the Supreme                           terms of a mutation process over a taxonomic tree (or
Court database collected by Harold Spaeth (1998). We                          other domain structure). Point (2) was addressed in the
looked at the Burger court which served from 1981 to                          previous section, and now we turn to point (3). We
1985. Spaeth records 8 possible types of voting behavior:                     show that learning explicitly structured domain repre-
we considered only the cases where every judge either                         sentations provides a powerful source of inductive bias
joined the majority, dissented, or cast a regular concur-                     for reasoning about novel properties, and that this power
rence (which we treated the same as a majority vote).                         is not easily matched by a generic connectionist architec-
This left a binary dataset containing votes for 9 judges                      ture.
on 637 cases.                                                                    We compared our tree-structured model for the
                                                                        675

         (a)
                        gorilla
               chimpanzee
             spider monkey  bat
                grizzly bear
                   polar bear
                       beaver
                          otter
                        skunk
                         mole
                     hamster
                        rabbit
                       mouse
                      squirrel
                       weaselrat
                     raccoon
                   dalmatian
                         collie
                 persian cat
                  chihuahua
               siamese cat
          german shepherd tiger
                           lion
                      leopard
                           wolf
                            fox
                       bobcat
                     antelope
                          deer
                        horse
                        zebra
                        giraffe
                       moose
                       buffalo
                             ox
                            pig
                           cow
                        sheep
             hippopotamus
                     elephant
         (b)      rhinoceros
                giant panda
                 killer whale
                 blue whale
           humpback whale
                       walrus
                          seal
                      dolphin
               Marshall   Brennan                   Stevens                 Blackmun                Burger   Rehnquist   OConnor   Powell
                                                                                           White
Figure 3: Structures (found via Bayesian structure learning) that best characterize two domains: (a) Mammal species
and their properties, and (b) Supreme Court Judges and their decisions.
animal-feature data described above1 with a connection-                guments, and the conclusion species is always “horse”.
ist model inspired by the work of McClelland and Rogers                The general set contains 45 three-example arguments,
[12]. The network includes one input unit for each an-                 and the conclusion category is “all mammals.” Unfa-
imal species and one output unit for each feature. We                  miliar (blank) predicates – e.g., “have biotinic acid in
explored a wide range of network parameters in an at-                  their blood” – were used for all these arguments. The
tempt to achieve the best possible performance (see be-                tree-based Bayesian model rates the strength of general
low). Following McClelland and Rogers, we trained each                 arguments by computing the probability that all ten ani-
network on the full matrix D of object-feature associa-                mals in the domain have the property. The connectionist
tions, then tested how well the hidden-unit representa-                model rates general arguments by computing projections
tions supported inductive projections for novel features.              to each animal separately and adding these ten scores.
   In the inductive projection task, a new feature is in-                 Table 2 shows correlations between model predictions
troduced, and one or more examples of species with that                and human judgments of argument strength. The first
feature are provided to the learner. The learner’s task                column summarizes the performance of two separate
is to infer which other species have this novel property.              neural networks, re ecting the best performance we ever
Like Rogers and McClelland, we modeled this task by in-                observed on each data set over a thorough two-stage ex-
troducing a new output unit for the novel feature, freez-              ploration of the space of possible networks2 . In the first
ing all weights except those connected to the new unit,                stage, we tested many different network topologies and
and training the new unit’s weights until it reliably pro-             varied the learning rate, the number of training and test-
duced the correct feature values for the given examples.               ing epochs, and the presence or absence of momentum
We then tested the new unit’s output when other species                and bias. We then took the best-performing networks
were presented as inputs.                                              from the first stage and ran every possible combination
   We modeled this same induction task using our tree-                 of the two best architectures, three best learning rates,
based Bayesian framework, as described in [10]. Given a                two best numbers of testing epochs, and three best num-
tree T inferred for the domain, the mutation process in                bers of training epochs. The best networks were trained
model CT induces a prior distribution over all possible                for 20,000 epochs, tested after 250 epochs of training
labellings of the species (i.e., the leaves of T ). Given one          on each testing example, and had no momentum and a
or more examples of a novel property, this prior together              bias of -2. They had two hidden layers, typically with
with the machinery of Bayesian concept learning allows                 10-30 units each, and a learning rate between 0.005 and
us to infer the most likely value of that property for all             0.01. Even allowing different neural networks for the
other species in the tree [10]. We used the tree shown                 two datasets, we were unable to match the performance
in Figure 3(a), and set the mutation rate for the novel                of the tree-based Bayesian model.
property to the value that best fit the 85 features in the                Our model differs from these connectionist models
biological data set. The resulting tree-based model has                along at least two important dimensions, either or both
no free parameters.                                                    of which could account for its superior performance.
   The inductive projections of each model were com-                   First, it uses explicit taxonomic structure and second,
pared with human argument ratings collected by Osh-                    it uses Bayesian statistical inference. To isolate the ef-
erson et al. [13]. Osherson used a ten-animal domain:
horse, cow, chimp, gorilla, mouse, squirrel, dolphin, seal                2
                                                                            The majority of these tests were conducted with the orig-
and rhino. The specific set contains 36 two-example ar-                inal 48-animal feature ratings (substituting ox for cow and
                                                                       blue whale for dolphin), before we collected feature ratings
   1
    In order to model the behavioral judgments described               for cow and dolphin. Qualitatively similar results were ob-
below, we supplemented these data with feature ratings for             served with the 50-animal dataset. The results reported in
two additional species, cow and dolphin, to give a total of 50         Table 2 re ect the best performance observed across either
species.                                                               dataset.
                                                                 676

                NN     NN      Bayes     Tree-    Sim          domain structure may be valuable for guiding inductive
                       (T)      (U)     Bayes     Cov.         projections from sparse data. Structured domain repre-
    Specific    0.62   0.86     0.16      0.95    0.75         sentations and domain-general statistical learning thus
    General     0.41   0.68     0.38      0.91    0.77         need not exclude each other, and indeed are comple-
                                                               mentary. Statistical learning suggests how novel domain
Table 2: Correlations between human judgments and              structures can be acquired, and these structures provide
five models for the specific (row 1) and general (row 2)       a powerful inductive bias for future statistical learning.
inductive projection tasks described in the text.
                                                               Acknowledgments We thank S. Sloman for providing
                                                               the biological dataset (originally collected by Osherson and
                                                               Wilkie), Doug Rohde for his neural net package, and Tom
fect of structure we implemented models that incorpo-          Griffiths and Sean Stromsten for helpful suggestions. Sup-
rate only one of these factors. NN(T) is a neural net-         ported by NTT Communication Sciences Lab, the DARPA
work that uses an explicitly taxonomic representation          CALO program, and the Paul E. Newton Chair (JBT).
but not Bayesian inference. The network has 19 input
units and a single output unit for the novel property. In-
put features are derived from the ten-animal tree — the                               References
subtree of Figure 3 that includes the ten animals used          [1] S. Atran. Folk biology and the anthropology of science:
                                                                    Cognitive universals and cultural particulars. Behavioral
in this task. Each input node corresponds to a node in              and Brain Sciences, 21:547–609, 1998.
the tree, and a species is represented by switching on          [2] A. Collins and M. R. Quillian. Retrieval time from se-
an input unit for each of its parent nodes in the tree              mantic memory. Jn of Verbal Learning and Verbal Be-
(including a distinctive feature for itself). Species that          havior, 8:240–247, 1969.
appear nearby in the tree will share a relatively large         [3] J. Felsenstein. PHYLIP – Phylogeny inference package
number of ancestors and will therefore have similar rep-            (version 3.2). Cladistics, 5:164–166, 1989.
resentations. Bayes(U) is a model that uses Bayesian            [4] A. Gelman and X. L. Meng. Simulating normalizing
inference but without any explicit structural represen-             constants: from importance sampling to bridge sampling
                                                                    to path sampling. Statistical Science, 13:163–185, 1998.
tation constraining hypotheses. The model is inspired
                                                                [5] E. Heit. A Bayesian analysis of some forms of inductive
by Heit’s (1998) suggestion that priors for Bayesian in-            reasoning. In M. Oaksford and N. Chater, editors, Ra-
duction could be derived from familiar features stored in           tional models of cognition, pages 248–274. Oxford Uni-
memory [5]. Each of the 85 observed feature vectors is              versity Press, New York NY, 1998.
identified with a candidate hypothesis for generalization,      [6] J. P. Huelsenbeck and F. Ronquist.           MRBAYES:
e.g., the feature “nocturnal” gives rise to the hypothesis          Bayesian inference of phylogenetic trees. Bioinformatics,
                                                                    17(8):754–755, 2001.
that the new property is true of all and only the noc-
                                                      1         [7] R. E. Kass and A. E. Raftery. Bayes factors. Technical
turnal species. We assigned a prior probability of 86   to          Report 254, University of Washington, 1993. Revision
                                                     1
each of these hypotheses and reserved a further 86 for              3: July 6, 1994.
the hypothesis including all mammals.                           [8] F. Keil. Semantic and Conceptual Development. Har-
   Table 2 shows that NN(T) performed better than                   vard University Press, 1979.
all of the networks explored previously. The tree-              [9] C. Kemp, T. L. Griffiths, S. Stromsten, and J. B. Tenen-
based Bayesian model performed better than Bayes(U)                 baum. Semi-supervised learning with trees. In Advances
                                                                    in Neural Information Processing Systems, 2003.
or a feature-based version of Osherson et al.’s (1990)
                                                               [10] C. Kemp and J. B. Tenenbaum. Theory-based induc-
similarity-coverage model (which also assumes no do-                tion. In Proceedings of the 25th Annual Conference of
main structure). These results suggest that generic ap-             the Cognitive Science Society, 2003.
proaches to biological induction may be improved by            [11] P. O. Lewis. A likelihood approach to estimating phy-
adding explicit representations of taxonomic structure.             logeny from discrete morphological character data. Sys-
The tree-based Bayesian approach also performed bet-                tematic Biology, 19(6):913–925, 2001.
ter than the tree-based neural network, suggesting that        [12] J. McClelland and T. Rogers. The parallel distributed
                                                                    processing approach to semantic cognition. Nature Re-
both rational statistical inference and structured domain           views Neuroscience, 4:310–322, 2003.
representations play important roles in guiding people’s       [13] D. N. Osherson, E. E. Smith, O. Wilkie, A. Lopez, and
generalizations.                                                    E. Sha r. Category-based induction. Psychological Re-
                                                                    view, 97(2):185–200, 1990.
                      Conclusion                               [14] D. N. Osherson, J. Stern, O. Wilkie, M. Stob, and E. E.
                                                                    Smith. Default probability. Cognitive Science, 15:251–
Our results are preliminary, with a focus on the domain             269, 1991.
of biology and just the taxonomic aspect of knowledge          [15] L. Sirovich. A pattern analysis of the second Rehnquist
in that domain. No strong general claims can be made                U.S. Supreme Court. PNAS, 100:7432–7437, 2003.
until we push this inquiry more deeply in the domain           [16] H. J. Spaeth. United States Supreme Court judicial
of biology, and more broadly into other domains. Even               database, 1953-1996 terms. 1998. 8th ICPSR version.
so, our work suggests a viable alternative to traditional
nativist and empiricist accounts of domain knowledge.
Contrary to a strong nativist view, the organizing struc-
tural principles of a domain may be learned. Contrary
to a strong empiricist view, explicit representations of
                                                           677

