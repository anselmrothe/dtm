UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Inferring knowledge of properties from judgments of similarity and argument strength
Permalink
https://escholarship.org/uc/item/1cj0n0nd
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Author
Stromsten, Sean
Publication Date
2004-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

    Inferring knowledge of properties from judgments of similarity
                                         and argument strength
                                      Sean Stromsten (sean s@mit.edu)
                                     Department of Brain and Cognitive Sciences
                                         Massachusetts Institute of Technology
                                           Cambridge, MA 02139-4307 USA
                          Abstract                            in all models discussed here, by the set of ten mam-
                                                              mals used in the arguments)1 .
   Psychological similarity has been invoked to ex-              In order to study argument strength, rather than
   plain many phenomena, including judgments of               particular knowledge of predicates, the premises
   the strength of inductive arguments (Osherson et
   al., 1990). The present work follows the sugges-           and conclusion assert so-called ‘blank’ predicates of
   tion of Tenenbaum and Griffiths (2001) that judg-          species, about which experimental participants will
   ments of similarity and judgments of argument              not have direct knowledge. The biological sound of
   strength cohere because they are essentially judg-
   ments of the same kind, which consult the same             the predicates, and the fact that they are asserted
   knowledge of properties of objects or classes. I           to be true of all members of one or more species, are
   work backward from people’s judgments of argu-             clues that they are biological properties. The inten-
   ment strength and similarity to the knowledge of
   properties–specifically, knowledge of probable prop-       tion, then, is that participants have no choice but to
   erty extensions–that might explain the coherence           fall back on categorical biological knowledge.
   among those judgments. I show that the knowl-                 Osherson et al. propose the similarity-coverage
   edge inferred can be used to predict other such judg-
   ments. I then examine this knowledge for structural        model, which predicts the judged strength of these
   properties such as taxonomic organization.                 arguments as a function of judgments of pairwise
                                                              similarity among the species of animals in them. The
   Induction, or generalization from examples, is a           strength g(X, Y ) of a conclusion, according to this
central cognitive capacity in need of two kinds of ex-        model, is a weighted sum of (1) the similarity of
planation: (1) What representations and processes             the premise categories X to the conclusion category
underly induction? (2) Why do we have those repre-            Y , and (2) the degree to which the diversity of the
sentations, and carry out those processes? That is,           premise categories ‘covers’ the lowest superordinate
to the degree that they work, what relation to right          category S including both the premise categories and
reason explains their success? I focus here on the            the conclusion category:
second question, and with respect to just one much-
studied inductive task, category-based induction.
   To illustrate this task, consider the following in-                   g(X, Y ) =
ductive argument (after Osherson, et al. , 1990)                              α max sim(Xi , Y ) +
                                                                                  i
   Chimpanzees require biotin for hemoglobin synthesis.                               X
   Gorillas require biotin for hemoglobin synthesis.                          (1 − α)     max sim(Xi , Sj ).
                                                                                            i
                                                                                        j
   Mammals require biotin for hemoglobin synthesis. (1)
                                                                  1
                                                                    In what follows, in addition to the 81 judgments
                                                              studied by Osherson et al. , I use data on 28 addi-
   Horizontal lines separate conclusions from their           tional judgments, collected by Sanjana and Tenenbaum
premises. The premises assert facts about categories          (2003). They designed these additional generalization
of objects, and the conclusions do not (in general)           judgments to demonstrate effects which their Bayesian
                                                              model could explain, but which the Osherson et al. model
follow deductively.                                           could not. Again, ‘horse’ was the only species in the con-
   Osherson et al. collected extensive judgments of           clusions. The innovation was repeated examples of the
the strength of such arguments–that is, the sub-              same species, which required a cover story that makes
                                                              such examples reasonable. Participants observed a set
jective probability of the conclusions, given the             of example animals–individual animals–with a particu-
premises. The arguments contained various mix-                lar disease, and were then asked to judge the probability
tures of ten species of mammals in the premises,              that horses could get the disease. Trusting that par-
                                                              ticipants assume that disease susceptibility is a species
but all conclusions were about either ‘horses’ or ‘all        property, I aggregate these data with the Osherson et al.
mammals’ (the set of all mammals is approximated,             data.
                                                         1273

   Osherson et al. test their model against a number         Bayesian generalization
of robust qualitative patterns in the way the plau-          Before discussing the details of particular proposals,
sibilities people assign to such arguments relate to         I will briefly review the notion of category-based in-
the similarities of the categories used. A few exam-         duction as Bayesian generalization, as formulated by
ples of these patterns will illustrate the utility of the    Tenenbaum and colleagues. We assume that:
similarity and coverage terms. The argument
   Chimpanzees require biotin for hemoglobin synthesis.
                                                             • The premise categories are random samples from
                                                                the set c of categories having the target ‘blank’
   Gorillas require biotin for hemoglobin synthesis. (2)        property.
is stronger than the argument                                • Prior to receipt of any examples, the generalizer
   Chimpanzees require biotin for hemoglobin synthesis.         has a hypothesis space H, where each hypothesis
                                                                h ∈ H is a possible extension for the target prop-
   Dolphins require biotin for hemoglobin synthesis. (3)        erty. The generalizer also has a probability distri-
because gorillas are more like chimpanzees than dol-            bution over H, which represents the prior degree
phins are. The argument                                         of belief that each candidate is the extension of
                                                                the target property. This prior distribution may
   Chimpanzees require biotin for hemoglobin synthesis.         itself be sensitive to (conditional on) other infor-
   Dolphins require biotin for hemoglobin synthesis.            mation, for instance, about the kind of property
   Mammals require biotin for hemoglobin synthesis. (4)         being generalized.
is stronger than argument (1), which may be ex-                 The probability that a category y is a member
plained by the greater ‘coverage’ of the set of mam-         of the set c, given a set of n examples x drawn at
mals by ‘chimpanzees and dolphins’ than by ‘chim-            random from c , can be found by summing over hy-
panzees and gorillas’.                                       potheses:
   It may strike the reader that these intuitions re-
quire more than purely psychological, ad hoc expla-
nations, for surely they are correct. If so, they re-               P (y ∈ c|x ∼ c, ξ) =
                                                                         X
quire normative (Bayesian) explanation. This point                           P (y ∈ c|c = h)P (c = h|x ∼ c, ξ).
has been addressed by several authors, beginning                           h
with Heit (1998).
   There are a number of other reasons for dissat-              Here x ∼ c means that the examples x are random
isfaction with an explanation of judgments of argu-          draws from c, and ξ represents background informa-
ment strength in terms of judgments of similarity,           tion. The first term is 1 if y ∈ h, and 0 otherwise.
having nothing to do with the degree of predictive           The second term can be re-written in an enlighten-
success of the similarity-coverage model. The most           ing form by Bayes rule:
obvious, perhaps, is that similarity and argument
strength are judgments of equal status, equally in
need of explanation. Another objection is that the                    P (y ∈ c|x ∼ c, ξ) =
                                                                            P
judged similarity of x to y is not a stable, context-                             P (x ∼ c|c = h)P (c = h|ξ)
                                                                            Ph3y                             .
free property of the pair (Tversky, 1977). If judg-                                            0        0
                                                                              h0 (x ∼ c|c = h )P (c = h |ξ)
                                                                                P
ments of similarity must be computed on-the-fly,
as judgments of the strength of arguments presum-               The terms P (x ∼ c|c = h) represent the prob-
ably are, then whatever knowledge is consulted when          ability of seeing just the examples x in n draws
computing similarities could be consulted when com-          from h. Assuming that items in h are drawn with
puting argument strengths, without computing sim-            equal probability, then the probability of drawing
ilarity as an intermediary. This is, in essence, the         any particular item in a single draw is 1/|h|. Then
kind of explanation proposed in the Bayesian mod-            P (x ∼ c|c = h) is |h|−n , if h contains all the exam-
els of Sanjana and Tenenbaum (2003) and Kemp and             ples in x, and zero otherwise. The likelihood term
Tenenbaum (2003). For purposes of direct compar-             P (x ∼ c|c = h) depends only on the examples and
ison, they predicted argument strengths from simi-           the contents of h, so we see now that ξ represents
larities, just as Osherson, et al. did, but did so by        information we may have, prior to seeing the exam-
way of inferring taxonomic knowledge presumed to             ples, about the probability of the various possible
underly both similarity and argument strength judg-          extensions. In what follows, I suppress this term to
ments.                                                       make the notation simpler.
                                                         1274

   Further abbreviating P (c = h) to P (h), we can          bilities determine similarities. I assume, as Osherson
re-write the above as                                       et al. do, that similarity is symmetrical, and, further,
                                                            that it has this particularly simple form:
                                         −n
                           P
                              h⊃y∪x |h|     P (h)
         P (y ∈ c|x ∼ c) = P          0 −n
                                                   . (1)
                              h0 ⊃x |h |   P (h0 )                   sim(x, y) ≡                                 (2)
                                                                          P (y ∈ c|x ∼ c) + P (x ∈ c|y ∼ c)
   Note that the sum in the denominator can be bro-                                                         .
ken into two sums: one is the same as that in the                                          2
numerator, and the other is over those hypotheses              Intuitively, this definition says that two items are
that contain the x but not y. Generalization, then,         similar to the degree that one is likely to have a
depends on two weighted sums: one over the prop-            property that the other exemplifies.
erties common to both x and y, and another over             Previous work on Bayesian modeling of
those distinctive to x. Each summand is weighted
                                                            category-based induction
by both its prior plausibility and its likelihood or
‘fit’ to the examples.                                      Various restrictions on the form of the prior could be
                                                            entertained. For instance, each species might corre-
   The two terms have different jobs to do. The fit
                                                            spond to a location in a low-dimensional Euclidean
for extension h–that is, |h|−n –gives an advantage to
                                                            ‘psychological space’, with higher priors assigned to
smaller extensions, which is exponential in the num-
                                                            sets contained by convex or connected regions. The
ber of examples. Without a likelihood term sensi-
                                                            restricted families of priors investigated by Tenen-
tive to the number of examples, we miss an impor-
                                                            baum and colleagues are based on binary trees, with
tant phenomenon: given that examples are consis-
                                                            species at the leaves. The sets with highest priors
tent with two extensions, increasing the number of
                                                            are those corresponding to single subtrees, but some
examples ought to shift weight to the more specific
                                                            probability is assigned to sets picked out by multi-
extension. For instance, suppose our prior gives high
                                                            ple subtrees. Sanjana and Tenenbaum use a generic
weight to the classes ‘mammal’ and ‘rodent’. Then,
                                                            method for assigning probabilities to disjunctions of
given ‘mouse’ as an example of a species with prop-
                                                            a basis set of hypotheses (in this case, single sub-
erty P , either class is quite plausible. But adding
                                                            trees), while Kemp and Tenenbaum define a simple
the further examples ‘gerbil’ and ‘hamster’ ought,
                                                            ‘mutation’ process that can generate arbitrary hy-
intuitively, to give a strong advantage to ‘rodent’,
                                                            potheses, but assigns lower probability to those that
because the selection of three rodents from the larger
                                                            require many mutations, or mutations over short
class is highly coincidental. The likelihood term cap-
                                                            branches.
tures this focusing effect.
                                                               The proponents of these tree-based priors stress
   Without prior preferences for some extensions
                                                            that taxonomic trees are not just another restricted
over others, the likelihood or ‘fit’ term will always
                                                            family of priors; they are also an independently-
favor the extension consisting of just the examples,
                                                            motivated theory of the domain. People around
and will have no preference among larger extensions
                                                            the world seem to organize creatures into ‘folk tax-
of the same size. For example, given ‘mouse’ and
                                                            onomies’ (Atran, 1995), and the geneology of species
‘gerbil’ as examples of species with some property,
                                                            does, indeed, form a tree. This kind of theory may
generalization to ‘turtle’ will be just as strong as
                                                            be applicable in domains besides biology: even arti-
that to ‘hamster’. A prior favoring the natural class
                                                            fact kinds are often the result of a process of copying
‘rodents’ over ‘rodents minus hamsters, plus turtles’
                                                            and modifying earlier designs.
prevents this bizarre behavior.
                                                               One obvious way to compare various proposed
Similarity as a function of generalization                  families of priors is to compare predictive accura-
probabilities Tenenbaum and Griffiths (2001)                cies: fit the parameters (for instance, the locations
have argued that the similarity of x to y is a func-        of the points in a metric-space model, or the topol-
tion of the probability of generalizing from x to y, or     ogy and branch lengths of a tree) to subsets of the
vice-versa, or both. This move gives the infamously         judgments and see how well each model predicts the
slippery notion of similarity some solid footing on         rest.
the ground of reason, because generalization has a             Rather than competing with previous models on
normative foundation in Bayesian statistics. They           data fit, I take a complementary, ‘empirical Bayes’
also show how this view rationalizes earlier work on        approach (see, for instance, Gelman, et al. , 1995):
formalizing similarity and generalization.                  I place no constraints on the form of the prior, find
   For present purposes, we need not delve deeply           priors that do a good job predicting the data, and
into the question of just how generalization proba-         then examine those priors for structural properties.
                                                        1275

   This strategy has an obvious pitfall: an unre-                     proportion        correlations of model
stricted search for a prior that makes the data prob-              used in training    and data on remaining
able may over-fit the accidental properties of the                 args sims          arguments similarities
training data, especially, as in this case, when there             0      1           .50±.026      n. a.
are many more parameters than data points. Before                  1      0           n. a.         .88±.006
examining the prior for interesting structural prop-               0.5    0.5         .61±.029      .77±.033
erties, therefore, I demonstrate that the model is not             0.9    0.9         .80±.026      .72±.104
over-fitting so badly as to be uninformative.                      0      0.5         .29±.046      .60±.064
                                                                   0.5    0           .54±.030      .67±.043
   Computing a prior from judgments                                0      0.9         .41±.033      .79±.084
For any given hypothesis space and prior, Bayesian                 0.9    0           .67±.038      .82±.018
generalization yields point estimates for a set of sim-
ilarities and/or argument strengths. To accommo-                 Table 1: Predictions of held-out data given various
date noisy human data, I take these point estimates              training data. All rows show averages of ten runs,
to be central tendencies.                                        with associated standard errors.
   In what follows, I refer to the model’s prediction
of the ith judgment, given a prior, θ, as jim (θ) (this                Predicting held-out judgments
is given by either equation 1 or equation 2, above).
                                                                 Remarkably, this rather lavishly parameterized
The actual human judgment I denote jih . A simple
                                                                 model does a reasonable job of predicting randomly
noise model that respects the constraint that both
                                                                 held-out judgments when fit to the rest.
generalization probabilities and similarities must be
                                                                    Tuned to the judgments of argument strength,
between 0 and 1 assumes that
                                                                 the model’s predictions of pair-wise similarity agree
             h                 h 
                                                                 strongly with the actual judgments, approaching a
                                                      
                ji                       ji         2
       log               ∼   N  log             , σ     .        correlation of 0.9. A number of experiments, us-
              1 − jih                  1 − jih
                                                                 ing various proportions of each kind of judgment as
   In words, we apply a transform to each model pre-             training data, are reported in table 1.
diction that may (conveniently) take on any real                    This model does relatively poorly on the task that
value, and assume that the similarly-transformed                 has been the focus of the previous work–predicting
human judgment is normally distributed around this               the argument strengths, given the similarities. A
transformed prediction.                                          possible explanation for the deficit relative to the
   A bit of work (omitted here) reveals that the log-            other published fits is that the assumptions about
likelihood (up to an additive constant) of a set of              the form of the prior made explicitly by using a
judgments j is                                                   tree with mutations (and perhaps implicitly in the
                                                                 similarity-coverage model) are essentially correct, in
                                                                 which case opening up the space of priors, as I have
   P (j|θ) =                                              (3)    done, can only reduce predictive accuracy. As fur-
                  h                 h                           ther evidence of over-fitting, early stopping would
                                            
        X             ji       1 − ji
             log           h
                             +          +2 +                     usually have yielded better predictions, although I
          i
                   1  −  ji      jih
                h                     m           2        could not find a single stopping rule that consistently
          1               ji                ji (θ)               did so.
                log              −  log                   .
         2σ 2          1 − jih            1 − jim (θ)               Given these results, we can expect that the pri-
                                                                 ors converged to will reflect both the underlying
   The log likelihood of a set of judgments has a com-           structure of people’s knowledge and, to some de-
plicated but readily-computed gradient with respect              gree, peculiarities of the data set fit by the over-
to the prior, involving only the second term in equa-            parameterized model. In the next section, I examine
tion 3, which can therefore be optimized by off-the-             the priors converged on for taxonomic structure.
shelf techniques. I used the method of conjugate
gradients, stopping whenever several iterations pro-                        The ‘shape’ of the prior
duced less than a set increase in the log likelihood of          For the purpose of examining the structure of the
the training data. The model was parameterized by                prior that best explains the data, I focus on results
‘soft-max’ parameters z, where thez prior probability            obtained by optimizing the prior over the entire set
                                          i
of extension i is given by θi = Pe ezj . On each run,            of judgments.
                                        j
the z were randomly initialized such that the θ were                If we examine the hypotheses with highest priors,
nearly uniform.                                                  certain patterns can be found by eye or statistical
                                                             1276

test. Table 2 lists the 10 sets with the highest aver-                                           seal
                                                                                                       dolphin
age prior probability in a typical optimization run.
   If the most probable sets are those correspond-
ing to sub-trees of a taxonomic tree, then we should              rhino
                                                                     elephant
expect that most pairs of such sets will obey taxo-
                                                                                     gorilla
nomic constraints: either one will contain the other,
                                                                                           chimp
or they will be disjoint. There are a suspiciously                   cow
large number of these containment relations among                       horse
the top-ranked sets–randomly generated collections
of sets have as many containment relations between
pairs as the top-ranked 100 sets only about 40 out
of 1000 times. There is an even more extreme num-                                              squirrel
                                                                                              mouse
ber of disjoint pairs–exceeded not even once in 1000
random sets. Forcing the random sets to match the
top-ranked 100 in number of members makes no dif-           Figure 1: A two-dimensional MDS solution for the
ference to these results.                                   similarities of the ten mammals (Euclidean metric,
   However, there are also quite a few partially over-      variance accounted for = .81)
lapping sets, which is not what we would expect
from a single, strictly-observed tree. The overlap
is notably non-arbitrary, however. For instance, the        distributed according to a particular tree/mutation
sets ‘chimp, gorilla, mouse, squirrel’, ‘chimp, gorilla,    process, others are likely not to be. This is true
dolphin, seal’, and ‘mouse, squirrel, dolphin, seal’        even if we restrict attention to biological properties
are composed of just the three pair ‘dolphin, seal’,        of the kind that are likely to be universal across a
‘chimp, gorilla’, and ‘mouse, squirrel’ (‘Mouse, squir-     species (and which therefore are sensible fodder for
rel’ is not shown here, but ranked 14th in this solu-       the kinds of judgments we consider here). ‘Deep’
tion. ‘Horse, cow’, another pair one might expect, is       biological properties, such as having a certain or-
not far behind.).                                           gan or metabolic process, are quite likely to respect
                                                            the ‘tree of life’–that representing the genealogy of
   What this might point to is a ‘mutation’ pro-
                                                            species. The distribution of other species properties,
cess, as suggested by Kemp and Tenenbaum (2003).
                                                            such as what and how members eat, may be quite
While there are sets above that could only be ex-
                                                            random with respect to this tree, but might still re-
plained by mutations, if a single tree is assumed,
                                                            spect a different tree.
they seem to be restricted to cases where the mu-
tations could occur over relatively long branches;
                                                            How might people come to have these
members of the very short subtrees, such as ‘dol-
phin, seal’, seem to be present or absent in tandem,                               priors?
as predicted by the mutation process.                       I proceeded above with no constraints on the form of
   Another possibility is that the prior reflects uncer-    the prior over possible extensions of a new predicate.
tainty over several taxonomies. Uncertainty about           People or machines asked to make these judgments,
just which taxonomy to consult may be of two kinds:         however, have no such luxury. They must assume
uncertainty about which taxonomy is correct; and            that the extension of the new predicate is systemat-
uncertainty about which taxonomy is relevant to the         ically related to some known predicate or predicates
property under consideration. The first is a com-           (and, more generally, that predicates are likely to
monplace of probabilistic modeling, and quite intu-         have systematically related extensions), or have no
itively understandable, in this case. If I perform          basis for generalization.
bottom-up, agglomerative clustering by eye, using              In addition to positing coherence among new
the two-dimensional multidimensional scaling solu-          properties and old ones, real learners must learn
tion in figure 1, I come up with the tree topology          from the kind of data available in the real world.
used in both the Sanjana and Tenenbaum and the              Similarity-like data may sometimes be available, but
Kemp and Tenenbaum papers. But only the lowest-             they are not necessary; people can observe objects
level clusterings are obvious. Is the ‘seal, dolphin’       and their properties–for instance, that cows, horses,
cluster closer to the ‘gorilla, chimp’ cluster than the     elephants and rhinos all eat grass. Lists of such
‘mouse, squirrel’ cluster is? It is hard to tell.           properties are standard fodder for machine-learning
   The second kind of uncertainty is about which of         methods, including agglomerative clustering or more
several trees is relevant. Even if some properties are      sophisticated tree-finding methods. Several strate-
                                                        1277

  rank                                            contents
  1       horse   cow     chimp   gorilla   mouse squirrel       dolphin   seal   elephant   rhino
  2                                                              dolphin   seal
  3                       chimp   gorilla   mouse    squirrel
  4                                         mouse    squirrel    dolphin
  5                       chimp   gorilla                        dolphin   seal
  6       horse   cow             gorilla            squirrel                     elephant   rhino
  7                                         mouse    squirrel    dolphin   seal
  8       horse   cow     chimp   gorilla   mouse    squirrel                     elephant   rhino
  9                       chimp   gorilla
  10      horse   cow                                                                        rhino
Table 2: The 10 sets with the highest prior probability, on a single optimization over all judgments. There
are many instances of nesting, but they are not strictly compatible with any single taxonomic tree.
gies of tree-learning from such data have been ap-         thank Amy Hoff, Steven Sloman, and David Sobel.
plied to a number of standard machine-learning             This work was supported in part by NSF IGERT
datasets in Kemp et al. (2003).                            grant 9870676 at Brown University.
           Summary and discussion                                               References
I have suggested a novel technique of general utility      Atran, S. (1995). Classifying nature across cultures.
for fitting a Bayesian model to a set of judgments. I         In Smith, E. E. and Osherson, D. N., eds., An
applied this technique to a large collection of human         Introduction to Cognitive Science, volume 3. MIT.
judgments. Without imposing a taxonomic form on            Gelman, A., Carlin, J. B., Stern, H. S., and Rubin,
the prior, the prior of a Bayesian model optimized to         D. B. (1995). Bayesian Data Analysis. Chapman
fit human judgments nevertheless shows significant            and Hall.
conformity to taxonomic constraints. It seems that
                                                           Heit, E. (1998). A Bayesian analysis of some forms
either participants have a bias, in the domain of ani-
                                                              of inductive reasoning. In Oaksford, M. and
mals, toward priors that respect the taxonomic con-
                                                              Chater, N., eds., Rational Models of Cognition,
straints, or the raw facts about mammals have this
                                                              248-274. Oxford.
structure (which would, in turn, justify a taxonomic
bias).                                                     Kemp, C. and Tenenbaum, J. B. (2003). Theory-
   The technique is not limited to the case of a struc-       based induction. In Proceedings of the Twenty-
tureless prior over a small set of possible extensions.       fifth Annual Meeting of the Cognitive Science So-
Any prior that has tractable derivatives with respect         ciety.
to its parameters could be so optimized. In the case       Kemp, C., Griffiths, T. L., Stromsten, S., and
of a larger number of categories, whose power set is          Tenenbaum, J. B. (2003). Semi-supervised learn-
too large for enumeration, an approximate gradient            ing with trees. Advances in Neural Information
could be computed using a sample from the current             Processing 16.
estimate of the prior.
                                                           Osherson, D., Smith, E. E., Wilkie, O., López, A.,
   A principled alternative to using held-out data
                                                              and Shafir, E. (1990). Category-based induction.
to check models, and to using null-distribution hy-
                                                              Psychological Review, 97(2), 185-200.
pothesis tests to look for structure in the prior, is
Bayesian model comparison: compare the marginal            Tversky, A. (1977). Features of similarity. Psycho-
likelihoods of various structures. For most interest-         logical Review, 84, 327-352.
ing structure classes, the sums or integrals involved      Sanjana, N. and Tenenbaum, J. (2003). Bayesian
are intractable, but they can be approximated by              models of inductive generalization. In Becker, S.,
Markov Chain Monte Carlo or other methods.                    Thrun., S., and Obermayer, K., eds., Advances in
                                                              Neural Information Processing Systems 15. MIT.
               Acknowledgments
                                                           Tenenbaum, J. B. and Griffiths, T. L. (2001). Gener-
This work grows out of many conversations with sev-           alization, similarity, and Bayesian Inference. Be-
eral members of the Computational Cognitive Sci-              havioral and Brain Sciences, 24, 629-641.
ence lab at MIT: Charles Kemp, Tom Griffiths, and
Joshua Tenenbaum. For helpful discussion, I also
                                                       1278

