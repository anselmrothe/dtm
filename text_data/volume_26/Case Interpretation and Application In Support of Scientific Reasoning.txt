UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Case Interpretation and Application In Support of Scientific Reasoning

Permalink
https://escholarship.org/uc/item/3bv0r2qs

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)

Authors
Owensby, Jakita N.
Kolodner, Janet L.

Publication Date
2004-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Case Interpretation and Application In Support of Scientific Reasoning
Jakita N. Owensby, jowensby@cc.gatech.edu

Georgia Institute of Technology, College of Computing
801 Atlantic Drive Atlanta, GA 30332-0280

Janet L. Kolodner, jlk@cc.gatech.edu
Georgia Institute of Technology, College of Computing
801 Atlantic Drive Atlanta, GA 30332-0280
Abstract
Scientific reasoning involves the use of scientific skills, practices, and domain
knowledge to solve science problems. A little emphasized tool that experts
use to help them reason is to refer back to previous problem solving
experiences, interpreting and applying those experiences as they solve
problems. Results from a pilot study conducted Fall Semester 2002 suggest
that improvement in interpreting and applying expert cases to solve a problem
may also lead to improvement in certain scientific reasoning skills. In this
paper, we seek to explore the connection between case application and
scientific reasoning skills, namely, using evidence to justify a claim,
generating hypotheses, making predictions, and explaining scientific
phenomena.

Introduction
Scientific reasoning involves the use of scientific skills,
practices and domain knowledge to solve science problems.
Much research has been done to understand how students can
develop more expert-like scientific reasoning skills (e.g.
Kuhn, 1993; Schauble et. al., 1995), and much research has
been done to promote more expert use of scientific reasoning
skills in educational settings (e.g. Bell & Davis, 2000; Reiser,
et. al., 2000). However, little attention has been given to the
role case interpretation and application might play in learning
to reason scientifically. There is evidence that scientists use
cases extensively in their reasoning. For example, when
trying to analyze a series of unexpected results, scientists will
refer to cases that may seem unrelated but that have similar in
order to explain why those unexpected results may have
occurred (Blanchette & Dunbar, 2001).
In educational settings, it is often difficult to support
students as they attempt to acquire and carry out expertreasoning processes. In many cases, the expert-reasoning
process may be too complex to pare down in such a way that
students can engage in it without getting lost in all of the
complexity (Reiser, 2000). In other cases, because the expertreasoning process is not fully understood, it becomes difficult
to assess where students may experience difficulty, and when
they do, it is difficult to know what kind of help to provide.
We have sought to address these difficulties for one
complex skill: case application. Fall semester 2002, we
conducted a study to understand the effectiveness of the Case
Application Suite (CAS) (Owensby & Kolodner, 2004), a set
of tools designed to support middle-school students in projectbased inquiry classrooms as they interpret and apply the
experiences of experts to solve design problems. In particular,
we were interested in understanding how effective our system
of scaffolds was at supporting students as they interpreted and

1065

applied expert cases, whether the distribution of scaffolding
responsibilities across teacher and software was effective, how
well students were able to use case application skills in the
absence of the scaffolding, and whether the distribution of
scaffolding responsibilities could be articulated in a cognitive
apprenticeship (Collins, Brown & Newman, 1989) framework.
Analysis of the data showed that CAS was effective at
supporting students in case application, showing significant
differences for interpretation and trends for application. In
addition, the trends in the data suggested an unexpected
finding—that case application supports the learning of
scientific reasoning skills. Our analysis of this phenomenon
suggests that this is because case application and scientific
reasoning share foundational skills, namely using evidence to
support a claim, generating hypotheses, making predictions,
and explaining phenomena scientifically. This paper seeks to
explore the connection between case application and scientific
reasoning skills to suggest that improvement of certain case
application skills will promote improvement in these aspects
of scientific reasoning. As part of our exploration, we will
show how we’ve used software-realized scaffolding (Guzdial,
1994) to support the acquisition of case application skills
among middle-school students in project-based inquiry
science (Blumenfeld, et al., 1991).

Case Application and Scientific Reasoning
Case application is the process of interpreting, analyzing, and
applying experiences to address challenges or solve problems
(Owensby & Kolodner, 2003; the CBR literature, e.g.,
Kolodner, 1993). It involves three high-level steps:
interpretation, application, and assessment. Interpretation
involves, at the time of encountering the case, it, focusing on
extracting the connections between its criteria and constraints
and the solution chosen to address its challenge, making
connections between the solution chosen and the outcomes
that happened, and identifying what can be learned from the
experience, and at the time of working toward applying it,
making connections between the case (acting as a source case)
and the new situation (target). Application involves applying
those lessons to the new situation or target case, either directly
or via adaptation. Assessment involves analyzing the
applicability and quality of the proposed solution either by
making predictions about the target case’s solution or by
testing the target case’s solution and analyzing the outcomes
that result.

Case application is integral to the practices of experts.
Medical experts use cases to diagnose as well as to refine
treatments for patients. Architects keep file cabinets of cases
to go back to when working on new projects. Lawyers refer to
previous cases and decisions when constructing a strategy to
prosecute someone or to defend a client.
Analogical reasoning has long been recognized as an
important aspect of scientific reasoning (e.g. Gentner, 1999;
Anderson, 2000, Blanchette & Dunbar, 2001). Case
application extends standard analogical reasoning. In addition
to mapping the solution for one problem onto the solution for
another problem, we include in case application the analysis
and interpretation of a case at the time it is encountered that
allows its application. We also include in case application the
identification of those nuggets of an encountered case that
might apply in a new situation. When the cases being used are
those of others, this interpretation process involves significant
reading for understanding. While reading is taught in schools,
rarely does science class focus on helping learners read. Yet
real science practice is impossible without the skills involved
in reading a scientific case for understanding and reasoning
through its application.
Understanding requires identifying claims, the evidence used
to support its claims, and the quality of explanations put forth,
while applying what is in a science case requires making
predictions based on those claims and finding particularly
useful information in a big document.
In order to use evidence to support a claim, one must
interpret the experience from which the claim arose in such a
way that he/she recognizes that the evidence applies. Then,
one must interpret the evidence in such a way that the aspects
that apply to the claim can be identified. Next, one must be
able to articulate how the relevant aspects of the evidence
support the claim and make predictions for future use of the
concept, skill, or claim. Understanding the experience from
which the claim and evidence put forth involves interpreting
the experience and drawing out the lessons that can be learned
from the experience. Articulating how the evidence supports
the claim involves articulating the lessons learned from the
evidence and the experience that the claim rises from and then
applying those lessons to explain how the evidence supports
the claim and then making predictions about how the claim
might be useful in the future. It does make sense, then, that
supporting students as they learn how to interpret and apply
cases illustrating the evidence of scientific phenomena and the
application of scientific principles could help those same
students become better scientific reasoners.

Our approach to supporting the development of
case application skills
To help middle schoolers interpret cases and apply them in
new situations, we have designed a suite of software tools
called the Case Application Suite (CAS) to play the role of
coach within a cognitive apprenticeship framework (Collins,
Newman & Brown, 1989). In a cognitive apprenticeship
approach to learning complex skills, the teacher models the
skills and explains his/her reasoning to the students and then
coaches and hints as students begin to carry out parts of that
reasoning. As students become more capable, they, in turn,
1066

model for their peers and coach them to their next levels of
capability. But when students work in small groups, there
may not always be a group member expert enough to be able
to apply that coaching to the rest of the group. CAS supports
students as they work in small groups by asking the kinds of
questions and making the kinds of suggestions that a teacher
or more able student might make if he/she were available.
The design of CAS was informed by suggestions
made by the skills acquisition, case-based reasoning, transfer,
and cognitive apprenticeship literatures (Anderson, et. al,
1981; Anderson, 2000; Kolodner 1993; Branford, Brown &
Cocking, 1999; Collins, Brown & Newman, 1989,
respectively). CAS contains three tools. The Case
Interpretation Tool helps students identify problems the
experts encountered in achieving their goals, solutions they
attempted and why they chose those, criteria and constraints
that informed those solutions, results they accomplished and
explanations of those, and any lessons learned, or rules of
thumb, that can be extracted from the experience. The Case
Application Tool guides students through attempting to apply
the rules of thumb gleaned from the case, prompting them to
consider whether a rule of thumb is applicable and then
helping them explore ways they can apply it to their solution.
The Solution Assessment Tool helps students make
predictions about the success of their solution, analyzing the
impacts they expect their solution to make as well as where
they expect their solution to fall short.
The system of scaffolds in CAS includes five different
types of scaffolds: (1) the structure of the suite serves as a
scaffold as each tool corresponds to a major step in the case
application process; (2) the prompts in each tool’s center
frame focus students’ attention on important aspects of the
case; (3) hints are provided with each prompt to give more
specific help; (4) examples are provided with each prompt to
help students see what they need to be accomplishing; and (5)
charts and templates serve as organizers to help students with
creating an analyzing the applicability of the rules of thumb
they have gleaned.
Each tool is divided into three frames (Owensby &
Kolodner, 2003; Owensby & Kolodner, 2004). In the left
frame is the expert case and interpretations that have already
been done of it. The middle-frame shows the prompts for the
tool the group is currently working on. The right frame shows
hints and examples (Figure 1).

Use of CAS in the Classroom
We’ve tried CAS out in classrooms engaging in the Learning
by Design (LBD; Kolodner et al., 2003) project-based inquiry
unit called Tunneling Through Georgia. In this challenge,
student teams serve as consultants for the design of several
tunnels needed for a transportation system that will run across
the state of Georgia. Four tunnels need to be designed, each
for a different geological area of the state—mountainous,
sandy, and so on. Students need to address several issues—at
what depth to dig the tunnel, what methods to use for the
digging, and what support systems are needed in the tunnel’s
infrastructure. Cases are used extensively in the unit to
suggest which geological characteristics of the tunnel location
they need to learn more about to address the challenge, to

introduce students to different kinds of tunneling technologies,
and to give them an appreciation of the complexity of tunnel
design. For example, the story about the design and
construction of the Lotchberg Tunnel in Switzerland, shows

read another set of four cases. Later, they sometimes use the
Solution Assessment Tool to make predictions about how well
their proposed designs might work, what they might have
overlooked, and what they would do differently if given
another chance

Our Study

We were interested in learning how to help students learn to
interpret and apply cases to project challenges and in
understanding the effects of adding software designed to
augment the teacher’s modeling and coaching to a cognitive
apprenticeship. Our study collected data to answer three
questions: (1) How are students’ abilities to interpret and
apply cases to their project challenge affected by such
scaffolding? (2) To what extent would students’ ability to
apply cases in the absence of the suite be influenced by its use
during a project? (3) To what extent does the suite enable
students to articulate the processes involved in case
application? When we noticed that some students’ scientific
reasoning capabilities were improved, we also analyzed to
answer a fourth question: To what extent does case
application capability predict scientific reasoning capability?

Figure 1: Case Application Tool
some of the problems the experts faced trying to tunnel
through the summit of a mountain that has two peaks
separated by a river and suggests understanding the
composition of a mountain by using test shafts and core
sampling can help to identify and possibly avoid problems like
crumbling rock, flooding, and cave-ins.
The Tunneling unit is preceded by another unit that requires
case application. In that unit, students learn about earth’s
surface processes as they engage in the challenge of designing
and constructing (in a stream table) a way of managing
erosion around a basketball court. They read two cases during
this unit, one about the dustbowl and another about landslides
on the U.S.’s West Coast. The teacher helps them read and
understand the cases together as a class and moves around the
room coaching them as they work in small groups to apply
what they’ve learned to their challenge. In addition, students
use a template to keep track of important aspects of the cases
they are reading about. The template, created by the teacher
and based on the My Case Summary Design Diary page
(Puntambekar & Kolodner, 1998), organizes a page into
columns representing Case Summary, Problems, Ideas,
Learning Issues, and Questions.
As they get started with the Tunneling unit, the
teacher again models case application for students as they
analyze the Lotschberg Case together as a class. After
analyzing the Lotschberg Case, student groups are assigned
one of four tunnel cases to interpret and present to the rest of
the class. They are introduced to CAS‘s Case Interpretation
Tool to support them as they interpret the case on their own in
small groups. This is followed by presentation of their
interpretations of their cases to the class and discussion of the
lessons that can be pulled from them. When it is time to apply
what’s been learned from the cases to their own tunnel
challenge, students use the Case Application Tool to create a
solution. This sequence is repeated a second time as groups
1067

Methods
Procedures
We report here on a study where we used CAS in the
classrooms of an 8th grade teacher (Mrs. K) (Owensby &
Kolodner, 2003) who had only 4 computers available for her
class. Because of this, only a subset of the students were able
to use the software; the rest engaged in all of the same
activities but had available only the template as scaffolding as
they were interpreting and applying cases. All students in the
study engaged together in solving the erosion challenge and in
doing Tunneling Through Georgia activities, and all were
exposed to the same teacher modeling. Overall, students
engaged in case interpretation and application activities five
times – twice during the erosion challenge, once with the
teacher at the beginning of the Tunneling unit (the Lotchberg
Case), and twice more in small groups. Each time, groups
work together to interpret a case and draw out the lessons it
teaches; they present their case interpretations to the class, and
they lead discussion about their case. Comparison students
(n=33 students; 9 groups) used the template to scaffold their
case interpretation and application as they interpreted and
applied cases after the Lotschberg Case, while experimental
students used CAS (n=14 students; 4 groups). We compared
the capabilities of students who had the software available to
those who did not as students engaged in the unit and after its
completion.
Software groups were videotaped as they used the
software, and software and non-software groups were
videotaped as they presented their interpretations to the class.
In addition, templates and logs of CAS use were collected for
analysis.
At the end of the unit, a performance assessment was
given. Called the Bald Head Island Challenge, students
worked in their Tunneling Through Georgia groups to make
recommendations about the design of two subdivisions on an
island off the coast of Georgia. They read a case about Bald

Head Island and used it to give advice. They were asked to
identify the risks involved with the project, identify possible
management methods, create rules of thumb (Part 1), design a
plan for designing and constructing the subdivisions, and
make final recommendations about whether the project should
move forward with the given time and budget constraints (Part
2). Groups were videotaped as they discussed their ideas. All
groups had only template scaffolding available as they
engaged in this activity, organized into columns representing
Risk, Why Is This A Risk, Ways To Manage The Risk, Pros,
and Cons.

Analysis

Video data was analyzed using a coding scheme that described
the data for specific interpretation and application dimensions.
Two coders analyzed video-recorded group performance for
interpretation on dimensions shown in Table 1 and for
application and assessment on dimensions shown in Table 2,
treating each of the two parts of the performance assessment
as an episode. A five-point Likert scale was used for each,
with one representing no evidence of presence of the quality
being rated and 5 representing that the group fully displayed
the quality being rated. Differences in ratings were negotiated
by discussion, and inter-rater reliability was calculated.

Results

The results that follow provide evidence that case application
can be supported in educational settings despite its difficulties,
that distribution of scaffolding responsibilities across teacher
and software in a cognitive apprenticeship framework seems
to be a viable approach for promoting case application, and
that particular scientific reasoning skills among students who
used the software tools seem to be more sophisticated. We
first discuss the differences between students who used the
software and those who did not as they were engaging in
classroom activities of the Tunneling challenge. We then
discuss student capabilities while engaging in the performance
assessment, completed by all students after the Tunneling unit
was completed and without software scaffolding. The results
are discussed with respect to using evidence to justify a claim,
generating hypotheses, making predictions, and explaining
scientific phenomena.

Case Application During Class Activities

Examination of student artifacts and presentations of case
interpretations for groups using CAS vs. the case study
template showed three major differences. First, the software
groups better identified the reasons for positive and negative
outcomes. For example, in learning about the Queens
Midtown Tunnel, one software group told us: “They wanted to
build [the tunnel] straight [through the city] but couldn’t, so
they continued it further underground in an S-shape under
First Avenue and they took different core samples”. This
group was specific about the goals of the experts, the
constraints that kept them from achieving those goals if they
tried the obvious solution, what they did instead, and the
activities they had to engage in to do that successfully. The
typical non-software group, on the other hand, provided
general descriptions about the experts’ goals, neither
mentioning the constraints’ impact on the outcomes nor
alternatives. For example, one non-software group told us:
“The Manhattan side [was] on a large bluff higher than
Queens[, so they ] continued tunnels underground in a slope
under First [Avenue].”
Second, the groups who used the software included more
sophisticated causality in their rules of thumb. For example,
the non-software groups’ rules of thumb are in the form of
simple imperative statements (e.g., “Control water problem”,
“Take core samples”), while the software groups’ rules of
thumb explain why (e.g., “Take core samples—they can save
your life because if you hit the wrong kind of rock, you can
get hurt”, “You should always have an oxygen pass so the
toxic fumes can get out.”
Case Application at Completion of the Unit
In the performance assessment, groups discussed their answers
in preparation for writing individual recommendations. We
analyzed the video for interpretation and application
capabilities.
Table 1 shows results for case interpretation
(reliability 89%). Software groups tended to be better at all
case interpretation capabilities and significantly better at
specifying expert problems, identifying relevant aspects of the
case to apply, and using the case to understand the context in
which the risks/problems arose.

Table 1. Performance Assessment Results for Part 1 - Interpretation

0.25
0.25

NonSoftware
group
2.66
2.33

Standard Dev.
(non-software
group)
0.71
0.82

3.00
2.63
3.88

0.00
0.75
0.25

2.42
2.00
1.83

0.61
1.17
0.98

1.88

1.44

1.33

0.41

3.38
2.88
1.00

1.11
0.25
0.00

1.58
1.67
1.00

0.66
0.61
0.00

Coding Characteristic (bold denotes significant difference,
p<0.05)

Software
group

Standard Dev.
(software group

Recognizes that the case should be used to solve the challenge
Makes direct reference to the case to justify an argument or
position
Able to identify expert problems
Able to identify expert “mistakes”
Able to identify relevant aspects of the case that can be applied
to the challenge
Identifies risks based on prior experience with another
LBD/software case
Able to identify criteria and constraints
Uses the case to understand the context of the risks
Identifies rules of thumb

3.88
3.135

Software groups tended to describe expert problems
on a finer-grained level than non-software groups
1068

(3.00 vs. 2.42, p<0.05). For example, non-software
groups identified “sand” as a risk, while software
groups identified the “incompatibility of the old sand
and the beach with the new sand dug when the
channel was deepened” as a risk, or expert problem.
In the case, there are a number of risks or problems
that involve sand, so being able to distinguish
between those problems is important.
Software groups tended to discuss whether a
management method made sense for their challenge,
analyzing how the management method would play
out in their challenge and questioning each other
about the feasibility of a proposed management
method (3.88 vs. 1.83). Non-software groups tended
to discuss management methods only if they were
different from what they expected.
Software groups tended to use the case not only to
identify the problems the experts encountered, but
also to understand the context in which those
problems arose (2.88 vs. 1.67, p<0.05). They sought

to understand what was happening in the
environment that caused the problems to occur or to
grow worse. Non-software groups tended to look for
keywords that they were familiar with when
identifying problems and management methods. For
example, while flipping through the case, one nonsoftware student declared , “Oh!! I see erosion
here—erosion is a problem.” In a similar incident in
which one software group member stated that erosion
was a problem, another member of that group
declared, “but it says here that the problem is the
shoreline eroding.” This discussion resulted in the
software group providing more detail about the
erosion problem. In addition, for interpretation, we
looked specifically at how well software students
used evidence (the case) to justify a claim, and found
that software students tended to do a better job than
non-software students.

Table 2. Performance Assessment Results for Part 2 – Application and Assessment

2.88
2.50
2.50
1.63

Stadard Dev.
(software
group)
0.25
1.08
0.58
0.58

2.00
1.67
1.92
2.08

Standard Dev.
(non-software
group)
1.02
1.03
0.92
0.88

2.38

0.95

2.33

0.68

2.75

0.25

2.25

0.76

1.75
1.25
1.00

1.03
0.50
0.00

1.33
1.25
1.00

0.82
0.61
0.00

Coding Characteristic (bold denotes significant difference, p<0.05)

Software
group

Identifies issues or problems not explicitly stated in the case
Able to identify relevant aspects that can be applied to the challenge
Suggests incorporating a solution found in the case
Notices that a management method used by the experts cannot be
applied as is but must be adapted
Notices that a solution used by the experts cannot be applied as is
but must be adapted
Justifies use, modification, or abandonment of an expert solution
based on criteria and constraints of group’s challenge
Applies a solution used by the experts directly to their challenge
Suggests that an expert solution should be abandoned
Applies the case to the challenge using rules of thumb

Non-Software
group

another material. Few non-software groups even
mentioned criteria and constraints when deciding
whether an expert solution or management method
should be used. Again, justification of a claim using

For the video-recorded data for Part 2, application
and assessment, reliability was 86% and results show
trends toward better performance by software groups
on several dimensions. First, software groups tended
to suggest that a solution from the case would be
good to incorporate into their challenge solution
(2.50 vs. 1.92). This seems to result from the fact
that software groups tended to refer back to the risks
and solutions they identified in the expert case in Part
1. They would discuss those solutions to figure out
whether they made sense to use in their challenge
solution.
Second, software groups tended to justify the use,
modification, or abandonment of an expert solution
based on the criteria and constraints of the group’s
challenge (2.75 vs. 2.25). For example, one software
group member suggested that the group build a sea
wall out of an expensive material. His fellow group
member pointed out that that particular material
would be very expensive and given that they only had
2 million dollars to work with, they should consider

evidence was analyzed directly and software groups
showed better performance than non-software
groups.

Discussion

The goals of this paper are two-fold: (1) to show that
through repeated use of scaffolding that supports case
interpretation and application students do indeed
become better users of cases and (2) to point out the
connection between interpretation and application of
expert cases and scientific reasoning. The first is
shown in the data that has been reported. The second
can be seen by connecting what students did while
interpreting and applying cases to scientific
reasoning.
It seems that using evidence to support a claim and
explaining scientific phenomena is important in both
case application and scientific reasoning, while
1069

analysis of the data suggests that certain case
application skills (i.e. understanding the context of
problems, understanding criteria/ constraints,
identifying relevant aspects of the case to apply) may
be important in generating hypotheses and making
predictions. For example, understanding the
connection between addressing criteria/constraints
and the outcomes that result seems to involve the
same reasoning as generating a hypothesis and
analyzing the results to determine whether the
hypothesis is supported or rejected. This seems to
suggest several things:
1. Understanding how to better support case
application may lead to understanding how to
better support certain scientific reasoning skills.
2. Students can be supported in case application
despite its complexity, and students can improve
case application skills. As such, support that
leads to improvement in case application skills
may also lead to improvement in certain
scientific reasoning skills.
3. Using a cognitive apprenticeship framework and
distributing scaffolding responsibilities across
teacher and software seems to be effective at
supporting case application skills that seem to be
connected to certain scientific reasoning skills.
As such, this same approach may be useful in
supporting other scientific reasoning skills.
To make these suggestions stronger or to make
stronger claims about the connection between case
application and certain scientific reasoning skills, the
data would need to be coded using dimensions to
describe more specifically what is happening with
students’ scientific reasoning skills as their case
application skills are improving. Though this was not
the focus of this study, the trends that emerged and
the suggestions that arose certainly suggest that this
connection between case application and scientific
reasoning is worthy of further exploration.

naturalistic settings: The influence of audience,
emotion, and goals. Memory & Cognition, 29, pp. 730735.
Mahwah, NJ: Lawrence Erlbaum Associates.
Blumenfeld, P.C., Soloway, E., Marx, R. W., Krajcik, J. S.,
Guzdial, M. & Palincsar, A. (1991). Motivating
project-based learning: Sustaining the doing,
supporting the learning. Educational Psychologist, Vol.
26 (Nos. 3 & 4), pp. 369-398.
Bransford, J.D., Brown, A.L. & Cocking, R.R. (1999).
How People Learn, Washington, D.C. National
Academy Press, 41-66.
Collins, A., Brown, J.S., & Newman, S.E. (1989).
Cognitive Apprenticeship: Teaching the crafts of
reading, writing, and mathematics. In L.B. Resnick
(Ed.) Knowing, Learning, and Instruction: Essays in
Honor of Robert Glaiser. Hillsdale, NJ: Erlbaum.
Gentner, D. (1999). Analogy. In W. Bechtel & G. Graham
(Eds.), A companion to cognitive science (pp. 107-113).
Oxford: Blackwell.
Guzdial, M. (1995). Software-realized scaffolding to
facilitate programming for science learning. Interactive
Learning Environments, 4(1), 1-44.
Kolodner, J.L. (1993). Case-Based Reasoning, Morgan
Kaufmann Publishers, San Mateo, CA, 1993.
Kolodner, J.L., Crismond, D., Fasse, B. B., Gray, J.,
Holbrook, J., Puntembakar, S. (2003). Putting a
Student-Centered Learning By DesignTM Curriculum
into Practice: Lessons Learned. Journal of the Learning
Sciences, Vol. 12 No. 4..
Kuhn, D. (1993). Science as argument: implications for
teaching and learning scientific thinking. In Science
Education, 77(3), 319-337.
Owensby, J.N. & Kolodner, J.L. (2004). Case
Interpretation Tool: Collaboratively Coaching Students’
Understanding of Second-hand Experiences in Learning
by Design Classrooms. To be presented as a paper at
International Conference of the Learning Sciences 2004
(ICLS 2004). Santa Monica, California, June 2004.
Owensby, J.N. & Kolodner, J.L. (2003). Case
Application Suite: A Study of Teacher Use in Learning
By DesignTM Classrooms. Paper presented at the
American Educational Researchers’ Association
(AERA) 2003 Conference, Chicago, IL.
Puntembekar, S. & Kolodner, J. L. (1998). The Design
Diary: Development of a Tool to Support Students
Learning Science By Design. Proceedings
International Conference of the Learning Sciences ’98,
pp. 230-236.
Reiser, B.J., Tabak, I., Sandoval, W.A., Smith, B.,
Steinmuller, F., L & Leone, T.J. (2001). Bguile:
Strategic and Conceptual Scaffolds for Scientific
Inquiry in Biology Classrooms. In S.M. Carver & D.
Klahr (Eds.) Cognition and Instruction: Twenty five
years of progress. Mahwah, NJ: Earlbaum.
Schauble, L. Glaser, R., Duschl, R., Schulze, S. & John, J.
(1995). Students’ Understanding of the Objectives and
Procedures of Experimentation in the Science \
Classroom. In The Journal of the Learning Sciences
4(2), 131-166.

Acknowledgements

We would like to thank the National Science
Foundation and the National Physical Science
Consortium for their support in this research effort.

References
Anderson, J.R., Greeno, J.G., Kline, P.K. & Neves, D.M.
(1981). Acquisition of problem solving skill. In J.R.
Anderson (Ed.) Cognitive skills and their acquisition.
Hillsdale, NJ: Erlbaum.
Anderson, J.R. (2000). Cognitive Psychology and Its
Implications: Fifth Edition. New York: Worth
Publishing.
Bell, P. & Davis, E.A. (2000). Designing Mildred:
Scaffolding Students’ Reflection and Argumentation
Using a Cognitive Software Guide. In B. Fishman
(Ed.), Proceedings of ICLS ’00: The Fourth
International Conference on the Learning Sciences.
Blanchette, I. & Dunbar, K. (2001). Analogy use in

1070

