UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
How do I Know how much I don’t Know? A cognitive approach about Uncertainty and
Ignorance
Permalink
https://escholarship.org/uc/item/2d36c141
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Pezzulo, Giovanni
Lorini, Emiliano
Calvi, Gianguglielmo
Publication Date
2004-01-01
Peer reviewed
  eScholarship.org                             Powered by the California Digital Library
                                                                University of California

                                    How do I Know how much I don’t Know?
                          A cognitive approach about Uncertainty and Ignorance
                                             Giovanni Pezzulo (pezzulo@ip.rm.cnr.it)
                   Istituto di Scienze e Tecnologie della Cognizione - CNR, viale Marx 15 – 00137 Roma, Italy
                 and Università degli Studi di Roma “La Sapienza” – piazzale Aldo Moro, 9 – 00185 Roma, Italy
                                               Emiliano Lorini (e.lorini@istc.cnr.it)
                               Università degli Studi di Siena – via Banchi di Sotto, 5 - 3100 Siena, Italy
                                               Gianguglielmo Calvi (calvi@noze.it)
                   Istituto di Scienze e Tecnologie della Cognizione - CNR, viale Marx 15 – 00137 Roma, Italy
                              Abstract                                 information from the world (from witnesses) and that leads
                                                                       the agents to be “ready” to decide. We will claim that this
   We propose a general framework for reasoning and deciding           process involves strength of beliefs that are relevant for
   in uncertain scenarios, with possibly infinite source of            deciding, as well as uncertainty and ignorance. The results
   information (open world). This involves representing
                                                                       of the current work are suitable e.g. for MAS environments,
   ignorance, uncertainty and contradiction; we present and
   analyze those concepts, integrating them in the notion of lack
                                                                       where an agent has to take decisions in open worlds.
   of confidence or perplexity. We introduce and quantify the
   strength of the beliefs of an agent and investigate how he can              The Red-or-Blue Card Game (RBG)
   do explicit epistemic actions in order to supply information        We introduce a simple distributed game that is suitable for
   lacks. Next we introduce a simple distributed game (RBG)            Multi-agent system simulations as well as for human
   and we use it as a testbed for comparing the performance of
                                                                       experiments: the agents (players) have to bid on the color of
   agents using the (classical) “expected utility maximization”
   and the “perplexity minimization“ strategies.                       a card (red or blue) and they have many sources of
                                                                       information (their perception and potentially infinite
                          Introduction                                 witnesses); the game can last an indefinite number of turns.
                                                                       The bidding game is the following: a card is shown (very
In an “open world” uncertainty and ignorance are difficult             quickly) to the player; it can be either red or blue and the
categories to deal with; how much can I be certain of a                player has to bid on the right color (he starts with 1000
belief of mines? how much information there is that I have             Credits). We assume that he cannot be totally sure of his
not considered and I should?                                           own perception (e.g. it is shown very quickly, or the lights
The first aim of the present work is to provide an analysis of         in the room are low), but he is able to provide a degree of
epistemic dimensions such as strength of belief, uncertainty,          certainty about the color. Before bidding he can ask for help
contradiction and ignorance (or ambiguity). A special focus            to a (potentially) infinite number of witnesses that have
will be given to the third dimension. In Economical                    observed the scene and provide the answer “red” or “blue”
literature the notion of ignorance has been extensively                (without degrees of certainty); those new information can
investigated (Shackle, 1972) and ways to quantify it have              lead the agent to confirm or revise his beliefs. Asking a
been proposed (Shafer, 1976). In those approaches “lack of             witness has not a cost in Credits but it costs 1 Time. Credits
information” has been shown to affect the decision process             and Times can be aggregated in different ways.
and ambiguity aversion in subject has been identified; see             When he decides that he is “ready”, he can bid from 0 to 10
Camerer & Weber (1992) for a review of the literature on               Credits on the color he wants. The true card color is shown:
decisions under ambiguity. We will argue in the following              if he was right, he gains two times the bid; otherwise he
analysis that Ignorance is a subjective evaluation of actual           loses the bid. The game lasts an indefinite number of turns;
lack of information on the basis of cognitive evidential               between the turns, depending on the result, the agent can
models. The agent has a model (script) of his sources that             revise the reliability he attributes to his sources: his
allows him to evaluate that a certain type and a certain               perception and the witnesses (depending for example on the
number of sources can provide sufficient information for               number of correct answers they provided) as well as his
reducing ignorance close to zero. In this way the strength of          SCAI. Besides, his perception and the witnesses have true
the belief and the (perceived) ignorance are two different             reliability values that determine the average correctness of
measures, the second belonging to the meta-level. The                  their answers. True values are not known by the player; at
second aim of the present work is to investigate the                   the first game round they are initialized and they do not
decision dynamics in an open world, with conflicting                   change during the game. At the end of the game the agent
beliefs and multiple sources of information. We will                   will collect a certain amount of Credits; a set of reliability
formalize the process that leads the agent to acquire new              values for his sources; a SCAI and he will have spent a
                                                                  1095

certain amount of Time. Using this game as a testbed, we                Classes of Acceptable Ignorance
compare different kinds of agents having many possible                  Each agent has a SCAI that includes several Classes of
heuristics in order to collect information and Credits1.                Acceptable Ignorance (CAI) that include one or more
                                                                        sources (e.g. Witnesses), each having its reliability value.
                 Theoretical Foundations                                For instance, CAI_1 = (witness 1, witness 2, witness 3)
We present first an evidential notion of ignorance:                     could be one of those classes. Classes of acceptable
ignorance is determined by the lack of belief sources. In our           ignorance can be intersected and unified (see Fig. 1): they
approach, following the cognitive approach of Castelfranchi             have the normal properties of sets in set theory.
(1995), the strength of a belief (i.e. how much I rely on one
of my beliefs) depends on the reliability of its sources (i.e.
the beliefs it is grounded on). Sources include: direct
experience (such as perceptive evidences); information
provided by other agents; reasoning (about other beliefs)
and categorization (reasoning about classes and
similarities). Since in an open world in principle there are
infinite sources to take into account, the agent can never
conclude that his own ignorance is zero. We propose in the
present model a solution to the problem of ignorance
quantification by identifying Classes of Ignorance                      Fig. 1 Structure of Classes of Acceptable Ignorance (SCAI).
Acceptance that reduce ignorance to finite values.
Uncertainty and contradiction have the same status of the               The agent knows that testing all witnesses in a given class is
notion of ignorance: they are meta-cognitive notions, i.e.              enough for making the ignorance acceptably close to value
agent’s evaluations about his own “epistemic state”. We are             zero. Imagine for example that the agent wants to know if
interested in this paper in investigating how those different           tomorrow will rain or will be sunny. He has several classes
epistemic states affect the decision process, and especially            of acceptable ignorance. For instance he can believe that by
how they affect the way the agent decides to execute a                  acquiring information about tomorrow’s weather from
pragmatic action (e.g. bet) or an epistemic one (e.g. query).           source 1 = “New York Times” and source 2 = “CNN” is
For example, if the agent feels to be too much ignorant or              enough for making ignorance acceptable. Moreover, the
uncertain he can decide to query, to bid a little amount, or            following points are crucial for understanding how the
not to bid at all. Here we describe the epistemic dimensions.           relation between SCAI and agents works.
                                                                        A. The agent has explicit models (meta-level) of Classes of
Ignorance                                                               Acceptable Ignorance as shown in Fig.1. There are
Intuitively ignorance depends on how much information I                 witnesses who are included in classes of acceptance but also
have with respect to how much it exists; in an open world               witnesses who are not included in any class.
there is a potentially infinite number of witnesses that have           B. The agent can also make “queries” to witnesses who are
not been questioned; so if we calculate ignorance in this               not in the SCAI. Indeed the number of classes is finite even
way the agent has always the maximum degree of                          if, according to the agent, the set of witnesses he can make a
ignorance. The agent does not know how many witnesses he                query is indefinite. The agent can make a query to whatever
can consider at most or better he does not know how he can
                                                                        witness even if this witness is not included in the structure
reduce his ignorance close to zero. A qualitative and
                                                                        of ignorance. In principle the agent can ask witness_10000
cognitive analysis is here required. Here we shift the issue to
an evidential and subjective level2. We introduce the notion            and he will always get an answer. Witnesses that are not in
of Structure of Classes of Acceptable Ignorance (SCAI).                 the structure do not have a value of reliability. A default
                                                                        value is assigned to them (through feature value assignment)
                                                                        whenever a query is made to them. After the query the
1
  Since it is an “open world” (there are an infinite number of          witness belongs to the SCAI as a witness who is not
witnesses and an indefinite number of turns) it is not possible to      included in any CAI (for example witness 8 in Fig.1).
perform full search. More, it is not possible to perform a long-term    C. The value of Ignorance is calculated at a meta-level
maximization because the agents don’t know when the game will           whereas the value of reliability of a witness is calculated at a
end (this condition is called “shadow of the future”).                  base-level.
2
   Our notion of ignorance is very close to the notion of ambiguity
identified in some recent economical and psychological literature
where is stressed that decision making is affected by the decision      is not explicitly evaluating that evidences concerning a certain
maker’s evaluation of his or her actual available information and       event e1 are also evidences concerning another event e2. Indeed it
competence to make judgments in specific domains (Heat &                has been shown that unpacking (making information available for
Tversky, 1991). Instead, our notion of ignorance is quite far from      explicit evaluation) a compound event into disjoint components
Sample Space Ignorance in Support Theory (Tversky & Koehler,            tends to increase the perceived likelihood of that event. An
1994) where it is claimed that people do not follow the extensional     immediate implication is that unpacking an hypothesis and/or
logic of conventional probability theory. In Support Theory an          repacking its complement will increase the judged likelihood of
agent can actually “ignore” actual information in the sense that he     that hypothesis.
                                                                    1096

Quantifying Ignorance through SCAIs                                             different ways, depending on some more cognitive biases
Class-Ignorance is given for each class at a certain point of                   (e.g. Agents that are biased to consider ignorance, or
a query sequence (q1,…, qn) and is defined as the total                         contradiction, or uncertainty). The basic heuristic is
number of witnesses in the class minus the number of tested                     summing them (and normalizing).
witnesses in that class, weighted for the inverse of the total
number of witnesses in the class.                                               Value of Information: Epistemic Actions
Absolute-Ignorance is defined as the minimal value of                           An Epistemic Action (EpA) is any action aimed at
Class-Ignorance among all CAIs.                                                 acquiring knowledge from the world; any act of active
          Class-Ignorance (Class n, qi) =                                       perception, monitoring, checking, testing, ascertaining,
          (n.wit. (Class n, qi) - n.queried.wit. (Class n, qi) Agent x, qi )/   verifying, experimenting, exploring, enquiring, give a look
          n.wit. (Class n, qi) Agent x, qi                                      to, etc. (Castelfranchi & Lorini, 1998). The notion of
          Absolute-Ignorance (qi) =                                             epistemic action has been extensively considered both in
          Min Class x (Class-Ignorance (Class x, qi) Agent x, qi)               psychology and in economics. The centrality of this notion
We have already pointed out that after a query is made to a                     comes from the fact that epistemic actions have a role in
witness who does not belong to SCAI, the witness will be                        different cognitive functions. In the present model an
included in SCAI as a witness who is not included in any                        Epistemic Action is always towards a witness (i.e. making a
class (such as witness 8 and 9 in Fig.1). The measure of                        query). Epistemic Actions are directed either to reduce
Absolute-Ignorance is not fixed: it depends on the single                       perplexity (or one of its dimensions) given a certain
agent categorization and classes organization. That measure                     “perplexity aversion” threshold of the agent (first function);
varies through learning, as new witnesses are added.                            or to acquire new information in order to make a better
                                                                                decision (second function).
Uncertainty                                                                     In both cases a value is assigned to epistemic actions. The
Uncertainty is a measure of the difference between the                          first value is a measure of the capacity of a given witness of
value of strength of the belief “the card is red” and the value                 reducing perplexity: we call it informativeness. The second
of strength of the belief “the card is blue”. When the                          value is called value of information and has been
difference is 0 the value of uncertainty is maximal, when the                   extensively investigated in economical literature in the sense
difference is 1 the value uncertainty is minimum. This                          of “how much the agent is disposed to pay for obtaining that
dimension takes into account the difficulty of deciding when                    information?” In that approach a possible way to calculate
the two strengths of beliefs are too close.                                     the value of information is given with respect to utility
                                                                                functions. These two notions can lead to different decision
Contradiction                                                                   strategies; in order to compare them, we have designed the
                                                                                simulative testbed “Red-or-Blue Card Game” (see above).
Contradiction is a (logical) inconsistency in a belief set; for
example I can not believe consistently that (in the previous
example) the ball in an urn is both red and black. In a                                          Considering the Sources
normal (statistical) analysis there is contradiction if the sum                 Strength of beliefs depends on its sources (perception; more
of the two strengths of beliefs is more than 1. In the                          or less reliable witnesses). Those sources are not all equal:
evidential approach the threshold of perceived contradiction                    in order to represent their relative contribute, we aggregate
(α) can be fixed at different values depending on cognitive                     them using Fuzzy Cognitive Maps (Kosko, 1986). In order
biases (e.g. more or less contradiction tolerant).                              to represent the fact that there are diverging sources (and
                                                                                they aggregate in a different way with respect to converging
          If(Strength.belief(CardRed, qi) +                                     ones) our FCMs have two “competing” branches for
          Strength. Belief(CardBlue, qi)) ≤ α then                              representing the competing beliefs “the card is red” and “the
          Perceived.contradiction(qi) = 0                                       card is blue”. FCMs are additive fuzzy systems with
                                                                                feedback, having nodes and edges. The weight of the nodes
          If(Strength.belief(CardRed,qi) +                                      represents the strength of a belief (e.g. “I am pretty sure that
          Strength.Belief(CardBlue,qi)) > α then                                the card is red”); the edges are weighted and they represent
          Perceived.contradiction(qi) =                                         the impact of a belief over another. The FCM that we use
          (Strength.belief(CardRed,qi) +                                        can be seen as divided into two branches, each aggregating
          Strength.Belief(CardBlue,qi)) – α                                     the values either for “red” or “blue”. These nodes receive
                                                                                input from intermediate nodes (“perception for red” and
Perplexity                                                                      “witnesses for red” the first; “perception for blue” and
Ignorance, Uncertainty and Contradiction are three meta-                        “witnesses for blue” the second); these edges are weighted
level epistemic information that an agent can take into                         by two fixed factors κ, λ representing the relative impact of
account in order to “decide if he is ready to decide”. In order                 perception and witnesses. The nodes “perception for red”
to model this kind of decisions we propose to integrate                         and “perception for blue” assume either the value 0 or 1
ignorance, uncertainty and contradiction in a single measure                    depending on the perceptual input; their edges have the
called Perplexity (i.e. lack of confidence). In calculating                     value of perception reliability (according to the agent). The
Perplexity, the three dimensions can be aggregated in                           “witnesses for red” and “witnesses for blue” nodes receive
                                                                                as input the information of the queried witnesses (either 0 or
                                                                            1097

1); the edges between each witness and “witnesses for                         speech (wit.z, CardBlue, qi+1)) credits(y, qi)))/2 –
<color>” have the value of the witnesses’ reliability. There                  Max x,y (Strength.belief (x, qi) credits (y, qi))
are also negative-weighted edges between the “red” and
“blue” nodes, as well as for each source. In this way the                     Effective-Choice (qi) =
contribute of diverging sources is modeled, because each                      1. If Max wit. z (Value-Information (wit.z, qi)) > 0 then
positive evidence in a branch counts also as a negative one                   Effective-choice (qi) = QUERY.wit. z such that
in the other branch. So, starting from the input values (the                  Max wit. z (Value-Information (wit.z, qi))
contributes of perception and the queried witnesses) the                      2. If Max wit. z (Value-Information (wit.z, qi)) ≤ 0 then
FCM calculates the final values for the strength of belief in                 Effective-choice (qi) = BET.yONx such that
“the card is red” and “the card is blue”. There is not fixed                  Max x,y (Strength.belief (x, qi) credits (y, qi))
“sum 1” between the two final values, so it is possible to
model contradictory beliefs (that the agent can reduce             Satisficing Agent
performing epistemic actions). The FCM structure is the
same for all the agents, but at each step it can be updated        The Satisficing Agent makes sequential search through the
(e.g. modifying the impact of the edges, i.e. reliability          witnesses in his SCAI. He starts with a given threshold γ for
values).                                                           expected utility. At each step, he randomly calculates either
                                                                   the expected utility value associated with BET.yONx or the
                                                                   expected utility value associated with BET.yONx after that
       Player Agents and Decision Strategies                       a given witness will be questioned. This value is the average
Here we describe three classes of decision strategies,             of the expected utility value associated with BET.yONx in
implemented into three Agents. Normative Agent and                 case the witness will say “Red Card” and the expected
Satisficing Agent do not use the notions of ignorance,             utility value associated with BET.yONx in case the witness
uncertainty and contradiction. Perplexity Reducing Agent           will say “Blue Card”. The first option during the sequential
uses them in order to select the witness to be tested.             search that overcomes threshold γ is chosen. If no suitable
                                                                   option is found after n (fixed value) steps, the agent lowers
Normative Agent                                                    the threshold of a certain value ∆δ. With respect to the
A normative agent decides either to bet a certain amount of        Normative Agent, the Satisficing Agent makes less queries
credits on a given option (either “the card is red” or “the        and it is better suited for open worlds (Simon, 1990).
card is blue”) or to make a query to a specific witness as
follows (this agent is not affected by perplexity).                Perplexity Reducing Agent
The agent calculates the value of information obtainable           The Perplexity Reducing agent has the goal to reduce the
from a given witness for all witnesses in the Structure of         level of perplexity below a given threshold δ before betting.
Classes of Ignorance Acceptance. The value of information          Since the only way to reduce perplexity is through queries,
obtainable from witness z is determined as: the average of         the agent starts choosing the witness to test: he makes a
the max value of expected utility given the information “the       sequential search on witnesses and takes the first witness
card is red” given by wit. z (which impacts on the agent’s         whose information is able to reduce perplexity under the
beliefs) and the max value of expected utility given the           threshold. If not suitable witness is found, the agent reduces
information “the card is blue” given by wit. z minus the max       the value of the threshold of a certain value ∆δ and restarts
value of expected utility given the actual information.            with the same strategy. The expected capacity of a witness
Afterwards the agent is able to decide. If the max value of        of reducing (or augmenting) perplexity represents the
information obtainable from witnesses is more than 0 then          expected informative contribute of the epistemic act of
the agent decides to make a query to the witness who               querying him. This value is called expected informativeness
maximizes that value; otherwise he decides to bet a quantity       and it is calculated as the actual value of perplexity minus
y of credits on “the card is x” that maximizes his actual          the average of the value of perplexity after that witness z
expected utility. We have not included the costs of making a       says “the card is red” and the value of perplexity after that
query in the utility function (we assume only the cost in          witness z says “the card is blue”3.
Time).                                                                        Expected-Informativeness (wit.z, qi) =
          Potential-Chosen-Bet (qi) = BET.yONx                                (Subj.unconfidence(qi)) –
          such that                                                           ((Subj.unconfidence(qi+1) ← speech (wit.z, CardRed,
          Max x,y (Strength.belief (x, qi) credits (y, qi))                   qi+1))+ (Subj.unconfidence(qi+1) ← speech (wit.z,
                                                                              CardBlue, qi+1)))/2
where x is either “the card is blue” or “the card is red” and y
is whatever sub-amount of the total amount of credits at a         Expected informativeness is quite different from the value of
given point in the query sequence (q1,…, qn). This agent has       information as defined for the Normative Agent. The
a very time consuming policy (minimizing the lack of               difference between those two definitions indicates two
information) and is not well suited for real time situations.      different theoretical perspectives: while the Normative
Another agent can be introduced that limits Time spent.            Agent maximizes utility values (for bidding) the Perplexity
          Value-Information (wit.z, qi) =                          Reducing Agent uses a cognitive theory of sources in order
          (Max x,y ((Strength.belief (x, qi+1) ←
                                                                   3
          speech (wit.z, CardRed, qi+1)) credits(y, qi)) +           It follows from the definition that there could be negative values
          Max x,y ((Strength.belief (x, qi+1) ←                    of expected informativeness.
                                                               1098

to consider the contribute of the witnesses in the cognitive                     1.   Given a previous sequence of queries (q1,…, qn), if
dimensions of uncertainty, ignorance and contradiction. The                           during that sequence there were two queries qi and
Perplexity Reducing Agent is implicitly biased to make                                qi+1 for witness A ← qi and witness B ← qi+1 and
queries to witnesses that are in the CAIs, since by definition                        witness A and witness B belong to the same CAI x
they lower the value of absolute ignorance more than                                  and the degree of perplexity did not vary so much
witnesses that are not in any CAIs. The Perplexity Reducing                           (in absolute value given threshold α) from qi to qi+1
Agent should be combined with the two others agents                                   then the witness B is taken out from CAI x.
(Normative or Satisficing). Once the level of perplexity is                      2.   Given a previous sequence of queries (q1,…, qn), if
under the threshold, he could decide either which color and                           during that sequence there were two queries qi and
how much to bid or decide to make a query to another                                  qi+1 for witness A ← qi and witness B ← qi+1 and
witness using his optimization methods. However, in order                             witness A belongs to CAI x whereas witness B
to simplify our experiments we did not allow perplexity                               belongs to the SCAI but not to CAI x, and the
reducing agents to carry on making queries to witnesses                               degree of perplexity varied a lot (in absolute value
once the degree of perplexity was reduced under the                                   given threshold β) from qi to qi+1 then witness B is
threshold δ. This simplification is plausible for maintaining                         inserted into CAI x.
completely distinct the 2 different functions of epistemic
actions: the function of perplexity reduction and the                                Experimental Setting and Variables
function of “increase” of expected utility.                                Here we show the comparison between three players:
                                                                           Normative (N), Satisficing (S), Perplexity Reducing (E).
               Learning During the Game                                    There are also two baselines: Random Bidder (B1) that
The RBG game has many turns, so it is possible to learn                    chooses at random to test or to bid (and how much); and
between them. In the epistemic perspective, it is interesting              Perceptive Bidder (B2) that bids only according to his
to model how agents revise information about sources of                    perceptive input.
beliefs.                                                                   The three independent variables we use are: perception
                                                                           reliability (PR); average witnesses reliability (AWR);
Updating Reliability Values                                                witnesses’ convergence (WC). The first one describes how
All the agents have a representation of the witnesses                      reliable in absolute is the perception of the agent; the second
reliability and are able to update these values depending on               one indicates how reliable are in average the witnesses
past interactions. Since reliability updating strategies are               answers. They reflect also the “difficulty” of the task. The
outside the scope of this paper, we used a linear statistical              third one describes how convergent are the answers of the
heuristic for all players: witnesses' reliability is lowered if            witnesses; this influences the final uncertainty value. We
they furnished a wrong advice, augmented otherwise, of a                   have built three scenarios: good perception (where PR is
fixed amount ∆φ.                                                           higher than AWR); good witnesses (the inverse); high
                                                                           uncertainty (where WC is set to a low value, and PR and
Updating Classes of Acceptable Ignorance                                   AWR have the same value)5.
The Perplexity Reducing Agent is also able to change its
SCAI adding or removing the witnesses in the Classes of                                         Results and Discussion
Acceptable Ignorance. At the beginning of the game the                     In the following tables we present the preliminary results of
SCAI is set randomly (e.g. the one shown in Fig.1) and it                  our experiments (for Credits and Time) of the three
can be updated after each turn extending or contracting its                Scenarios (250 simulations, 100 bid turns)6. As an indirect
CAIs. Imagine that the agent has queried in sequence w1,
w2, w3, w4, w8 before deciding. Imagine he has verified
that after the second test the value of perplexity has not                 relatively high values of α and relatively low values of β and φ the
changed so much (i.e. less than a threshold α). Since w1 and               agent is relatively closed-minded and conservative (he is less
w2 belong to the same Class1, Class1 can be contracted                     biased to revise the structure of classes of acceptance and the
eliminating w2 (that resulted not very informative). Imagine               reliability values). But for relatively low values of α and relatively
also he has verified that after the fifth test the value of                high values of β and φ the agent is relatively open-minded. This
perplexity has changed quite a lot (over a threshold β).                   distinction is very close to the typology of cognitive epistemic
Since w4 and w8 do not belong to the same Class, the class                 styles in (Sorrentino et al., 1986).
                                                                           5
                                                                             We use many thresholds and variables in our model: Close Mind
of w4 can be extended adding w8, that proved to be so
                                                                           agents vs. Open Mind agents in SCAI revision strategies
informative. We do not describe here the full algorithm for                (thresholds α and β); strong vs. weak need for low degree of
CAIs contraction and extension4. We want only to present                   perplexity (threshold δ); degree of satisfaction in expected utility
verbally its structure.                                                    (threshold γ); different way to weight different kinds of sources
                                                                           (bias towards perception or witnesses). In order to eliminate their
                                                                           effects we have randomly varied them through the experiments
4
  The variable φ for reliability updating, as well as thresholds α e β    (three dimensions for each variable on average).
                                                                           6
in classes of Acceptable Ignorance updating depend from cognitive            The simulations were performed using the cognitive architecture
biases towards belief revision. It is relevant to notice that for          AKIRA, developed at ISTC-CNR (http://www.akira-project.org/).
                                                                       1099

measure of “algorithm performance”, we introduced also              open world. We have introduced a MAS game (RBG) as a
Hypothesis Time: it measures how many witnesses an                  simulation setting in order to compare many agents that take
Agent has considered (but not questioned) before deciding.          or do not take into account epistemic dimensions. Our
                                                                    preliminary results show that perplexity reduction is a good
             Table 1: Good Perception Scenario.                     heuristic for dealing with open world scenarios, and the
                                                                    Structure of Classes of Acceptable Ignorance can be used in
      Agent       Credits         Time    H. Time                   order to quantify ignorance and reasoning about it. It would
      B1                  981       102              0              be interesting to test mixed decision strategies (e.g.
      B2                 1202          0             0              considering the perplexity in the utility function; or using
      N                  1641      6112       112453                the Perplexity Reducing Agent as a filter). Another
      S                  1388       987        13936                interesting direction is comparing simulation data with data
      E                  1622       409        10681                from human experiments; actually the RBG game is being
                                                                    used as an experimental setting in order to collect such data.
             Table 2: Good Witnesses Scenario.
                                                                                        Acknowledgments
      Agent       Credits         Time    H. Time
                                                                    We are indebted to Cristiano Castelfranchi for many
      B1                 1009       101              0
                                                                    insightful discussions about Ignorance and Uncertainty.
      B2                  799          0             0
      N                  1306      9207       144582
      S                  1102       997        19103                                         References
      E                  1298       603        13190                Camerer, C., Weber, M. (1992). Recent Developments in
                                                                      Modelling Preferences: Uncertainty and Ambiguity.
            Table 3: High Uncertainty Scenario.                       Journal of Risk and Uncertainty, vol. 5, pp. 325-370.
                                                                    Castelfranchi, C. (1995). Representation and integration of
      Agent       Credits         Time    H. Time                     multiple knowledge sources: issues and questions. In
      B1                 1007        99              0                Cantoni, Di Gesu', Setti e Tegolo (Eds.), Human &
      B2                  999          0             0                Machine Perception: Information Fusion, Plenum Press.
      N                  1803      8834       137866                Castelfranchi, C, Falcone, R. & Pezzulo, G. (2003). Trust in
      S                  1551      1156        21033                  information sources as a source for trust: a fuzzy
      E                  1563       673        15943                  approach. AAMAS 2003: 89-96.
                                                                    Castelfranchi, C., Lorini, E. (2003). Cognitive Anatomy and
In the first and second Scenarios the Perplexity Reducing             Functions of Expectations. IJCAI ’03 Workshop on
Agent performs very well with respect both to gained                  Cognitive modeling of agents and multi-agent interaction,
Credits and temporal measures (Time and Hypothesis                    Acapulco, Mexico.
Time): it performs at the same level of Normative agent             Ellsberg, D. (1961). Risk, Ambiguity and the Savage
with respect the final amount of credits but his temporal             axioms. Quarterly Journal of Economics, 75, pp. 643-
measures are much better. The comparison with the                     669.
Satisficing agent is even better. Not surprisingly, in the third    Heath, C., Tversky, A. (1991). Preference and belief:
Scenario he needs to query more witnesses and it is not able          Ambiguity and competence in choice under uncertainty.
to perform as the Normative. Results in bold are significant          Journal of Risk and Uncertainty, 4, pp. 5-28.
with respect to the Perplexity Reducing Agent. These results        Kirsh, D., Maglio, P. (1994). On distinguishing epistemic
                                                                      from pragmatic action. Cognitive Science, 18, pp. 513-
show that Perplexity Reducing Agents are very suited in
                                                                      549.
open world conditions where search of new information is
                                                                    Kosko, B. (1986). Fuzzy Cognitive Maps. International
in general very costly.                                               Journal Man-Machine Studies, vol. 24, pp. 65-75.
Moreover, a qualitative analysis allows to get a nice result        Shackle, G. L. S. (1972). Epistemics and Economics.
about SCAIs updating: the final SCAIs are in average                  Cambridge: Cambridge University Press.
populated with small CAIs of very reliable witnesses: the           Shafer, G. (1976). A Mathematical Theory of Evidence.
average reliability changes from 0.5 to 0.7 in the three              Princeton: Princeton University Press.
scenarios and the number of witnesses remains less to 20 in         Simon, H. (1990). Invariants of human behavior. Annual
all simulations. The fact that final CAIs are small and               Review of Psychology, 41, pp.1-19.
include reliable witnesses is in accordance with the way we         Sorrentino, R. M., Short, J. C. (1986). Uncertainty
learn about belief sources. The more you know an                      orientation, motivation, and cognition. In R. M.
environment, the less you need to question. Moreover, you             Sorrentino, E. T. Higgins (Eds.), Handbook of motivation
prefer to question very reliable sources.                             and cognition: Foundations of social behaviour, vol. 1,
                                                                      Guilford Press, New York, pp. 379-403.
             Conclusions and Future Work                            Tversky, A., Koehler, D. J. (1994). Support theory: A
We have proposed a theoretical foundation of some                     nonextensional representation of subjective probability.
cognitive categories such as ignorance, uncertainty and               Psychological Review, 101, pp. 547-567.
contradiction that are generally difficult to quantify in an
                                                                1100

