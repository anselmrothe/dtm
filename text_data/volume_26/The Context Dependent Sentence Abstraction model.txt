UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Context Dependent Sentence Abstraction model
Permalink
https://escholarship.org/uc/item/8tv0j3k8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Ventura, Matthew
Hu, Xiangen
Graesser, Art
et al.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            The Context Dependent Sentence Abstraction model
                                       Matthew Ventura (mventura@memphis.edu)
                          Department of Psychology/ Institute for Intelligent Systems, 365 Innovation Drive
                                                 Memphis, TN 38152-3115 USA
                                              Xiangen Hu (xhu@memphis.edu)
                          Department of Psychology/ Institute for Intelligent Systems, 365 Innovation Drive
                                                 Memphis, TN 38152-3115 USA
                                          Art Graesser (a-graesser@memphis.edu)
                          Department of Psychology/ Institute for Intelligent Systems, 365 Innovation Drive
                                                 Memphis, TN 38152-3115 USA
                                         Max Louwerse (mlouwers@memphis.edu)
                          Department of Psychology/ Institute for Intelligent Systems, 365 Innovation Drive
                                                 Memphis, TN 38152-3115 USA
                                           Andrew Olney (aolney@memphis.edu)
                     Department of Computer Science/ Institute for Intelligent Systems, 365 Innovation Drive
                                                 Memphis, TN 38152-3115 USA
                              Abstract                               has been shown to be impressive at the paragraph level
                                                                     (Foltz, Gilliam, & Kendall, 2000; Landauer, Laham,
    The Context Dependent Sentence Abstraction (CDSA)                Rehder, & Schreiner, 1997), other research has found
    model and Latent Semantic Analysis (LSA) were                    limitations of LSA at the sentence level (Kintsch, 2001). In
    compared in their ability to predict sentence similarity.        this paper we will present the Context Dependent Sentence
    Evidence supports the conclusion that the CDSA model             Abstraction (CDSA) model, a corpus-based model that
    better predicts human ratings for short phrases and
    sentences than does LSA. Alternative theoretical reasons
                                                                     builds sentence meanings based on combinations of pooled
    are given for this finding.                                      adjacent neighbors of individual words.          We will first
                                                                     discuss a weakness with vector representational systems
                                                                     (e.g., LSA) in handling sentence comprehension and then
                          Introduction
                                                                     turn to a description of the CDSA model, with evidence
Researchers in many disciplines within cognitive science             supporting it.
have proposed and tested theoretical claims about the
meaning of natural language expressions. One of the                  A weakness with LSA
contemporary models is Latent Semantic Analysis (LSA;
Landauer & Dumais, 1997). LSA is a statistical, corpus               One major strength of LSA is its versatility and simplicity in
based technique for representing world knowledge. It                 handling word meaning and sentence meaning by the use of
computes similarity comparisons between words or                     vector representations. It could be argued, however, that
documents by capitalizing on the fact that words are similar         there are potential theoretical problems with combining
when they are surrounded by similar words (i.e., the                 word vectors to form sentences. For example, the meaning
company a word keeps).                                               created from a sentence in LSA is a linear combination of
   LSA takes quantitative information about co-occurrences           word vectors, without eliminating information for any word.
of words in documents (paragraphs and sentences) and                 Consider the sentence the cow ate in the field. In LSA all
translates this into a K-dimensional space. The input of LSA         information about cows (e.g., animal, milk, burger), ate
is a large co-occurrence matrix that specifies the frequency         (e.g., food, grocery, digest), and field (e.g., grass, baseball,
of words in documents. LSA reduces each document and                 football) may be included in the sentence representation. It
word into a lower dimensional space by using singular value          could be argued that this assumption is not theoretically
decomposition. This way, the initially extremely large word-         plausible because much of this associated information is not
by-document co-occurrence matrix is typically reduced to             relevant to the word in context. There must be constraints
about 300 dimensions. Each word ends up being a K-                   that narrow down the vast array of information that may be
dimensional vector. The semantic relationship between                “primed” in the first stages of sentence comprehension.
words can be estimated by taking the cosine (normalized dot          Indeed, Kintsch’s construction-integration model (1998) has
product) between two vectors. Although LSA performance               attempted to explain this convergence of activated
                                                                1387

information by principles that guide the integration              environment. We pursued a corpus-based model of word
mechanisms.                                                       and sentence meaning, called the Context Dependent
   Whereas the standard use of LSA is based on the                Sentence Abstraction (CDSA) model. In the CDSA model,
assumption that a sentence’s meaning is the sum of all the        semantic information within any word w is the pooled
individual word meanings, there are extensions. Kintsch’s         words that co-occur with word w in every context. One of
predication algorithm (2001) tries to build meaning of a          the goals of this model is to try and capture the associations
sentence by using syntactical information and LSA to create       between words under a new level of specificity that
dependencies between subjects, predicates, and objects.           considers the pool of their surrounding words.
For example, consider the sentences the horse ran and the            In order to implement this model, it was necessary to
color ran. The context established by ran has different           make decisions about the learning rule and training set to be
meanings in these two sentences. Therefore, in the                used. For this model, the deciding factor in each of these
predication algorithm, constraints are made on what ran           cases was psychological plausibility. That is, this model
means in these sentences. The first step is to find the near      considers a corpus of prior experiences with words in
neighbors of the word ran (i.e., words that give the highest      context and the theoretical weights between words that
cosine to ran). For the horse example, all the neighbors of       change with experience, as opposed to a priori sets of
ran are compared to the word horse. This provides words           features that are dictated by a brittle, symbolic model. The
like walk, gallop, crawl, rode, etc. These neighbors of ran       central question is how these weights change with
that are closest to horse (i.e., highest cosine) are then         experience. The proposed CDSA claims that they change
included into the vector for the sentence the horse ran. The      by accumulating specific sentence exemplars.
same is done for the color example, resulting in different           Consider two words chair and table.             The central
overall meanings. Including this additional information has       question to be asked is what are all the possible relevant or
been shown to more accurately capture the meaning of a            useful relations that can exist between these two concepts?
sentence when we consider metaphor and causal inferences          Each word has a neighborhood set that includes all words
(Kintsch, 2001).                                                  that co-occur with the target word. These words are the
   Kintsch’s predication algorithm (2001) therefore imposes       extensional meaning of the target word and serve as the
augmentations and constraints on the standard use of LSA.         basis for all associations.    The neighborhood intersection
However, this algorithm still may not go the distance in          is the relation that occurs when two words share similar co-
solving the problem of information overload mentioned             occurrences with other words.         Much like LSA, words
earlier. That is, predicating the verb ate to cow does give       become associated by their occurrence with many of the
relevant information like graze, but all information about        same words. For example, food and eat may become
cows and ate are also included. To successfully implement         associated because they both occur with words such as
context in the given example, we would want to include            hungry and table. Therefore the neighborhood set N for
only information about “cows eating”, not about “cows and         any word w is all the information we have in the exemplars
ate and graze and field and pasture”. While the predication       for a word.
algorithm solves some problems by adding information, it
also may be limited by not taking any information away.           Neighbor weights
                                                                  The neighborhood set for any word is intended to represent
The need for contextual constraints                               the meaning of a word from a corpus. But there were
Computational representations like LSA go beyond general          several theoretical challenges that arose when we developed
word meanings, but may not adequately handle contextual           the model. One dealt with how to differentially weight
constraints. LSA may go some distance in handling                 neighborhood words. We assigned neighborhood weights
proposition meanings that constrain words in context              to each neighborhood word n of word w according to
(Kintsch, 1998), but there still is a large landscape of          Equation (1).
representations and algorithms for combining information
from words. We propose a new way of implementing
                                                                                              f (n | w)
contextual constraints. These contextual constraints are first                  λn w =
                                                                                              f (w) f (n )
built from simple individual word meanings that get                                                              (1)
established over time from their occurrences in the
environment. But as sentences are constructed, similarities
between the words in the constrained construction build a         The expression f(n|w) designates the frequency of
new meaning different from the sum of its parts.                  occurrence of the neighbor word n to target word w,
                                                                  whereas f(n) is the total frequency of the neighbor word n,
                    The CDSA Model                                and f(w) is the total frequency of the target word w. This
Associationist frameworks (Landauer, 2002; Louwerse &             formula essentially restricts the weights for the neighbor
Ventura, in press; Smith, Jones, & Landau, 1992) assume           words as being between 0 and 1 in most cases. We adopted
that it is critically important to measure and model the          this simple assumption but we acknowledge that there are
correlations between occurrences or events in the
                                                              1388

other ways to guarantee the range of the weights being              experiments were made by 10 undergraduate psychology
within 0 and 1.                                                     students who were instructed to rate the similarity of various
   Therefore, the weighting function was aimed at giving            pairs of words (i.e., primarily from words from Spellman,
more importance to words that consistently co-occur and             Holyoak, & Morrison, 2001) on a 6-point scale that varied
less importance to words that occur frequently in the corpus.       from 1 (very unrelated) to 6 (very related). A rating of 1 or
Additionally, rare co-occurrences may be given low weights          2 meant the rater could not easily find a functional or
because they do not consistently co-occur with the target           physical relationship between the word pairs (e.g. fish-
word.                                                               office). The mean among the raters for each pair was taken
   Some important assumptions had to be made in order to            as the basic data to test the models.
build relevant associations to target words most effectively.
The next section will explain the procedures of the                                         Experiment 1
algorithm written to perform these operations.
                                                                    Word Pairs A total of 64 word pairs was constructed that
                                                                    had a frequency over 10 in the TASA corpus. Some of the
Neighborhood Intersection Algorithm                                 words were expected to be unrelated (e.g., chair-hear) and
In order to construct the neighborhood set for any word, an         some related (e.g., chair-sit) in order to provide a sensitive
algorithm was written that pooled all words N that co-              range of values.
occurred with the target word w. We used the Touchstone
Applied Science Associates (TASA) corpus because of its             Results and Discussion
size (750,000 sentences) and diversity of topics (reading a
                                                                    Human ratings (M = 3.57, SD = 2.20) were significantly
diversity of texts up to college level). Each sentence in the
                                                                    correlated with the values produced by the CDSA model, r
corpus served as the context for direct co-occurrence. So for
                                                                    = .71, p < .001, and with LSA cosines, r = .78, p < .001. So
entire set of sentence sentences (s1...sC) that target word w
                                                                    both models fared quite well in accounting for the ratings of
occurs in, every unique word in (s1...sC) is pooled into the
                                                                    word pairs.
neighborhood set N. For example the neighborhood of
                                                                       Neighborhood intersection estimation shared a relation to
chair may consist of: table, sit, leg, baby, kitchen, talk, etc.
                                                                    human ratings, so we might conclude that this type of
This represents the neighborhood N of each target word w.
                                                                    association between words is used in human judgments.
Each word in the set (n1...nK) of N is weighted by the
                                                                    That is, by using all the co-occurrence information about a
function described in equation (1). To evaluate the relation
                                                                    word, one can capture the meaning of a word. As can be
between any two words w1 and w2, we follow the following
                                                                    seen, LSA was slightly more predictive of word relations
algorithmic procedure:
                                                                    than the CDSA model, although the difference was not
                                                                    statistically significant.
     1.   Pool neighborhood sets for w1 and w2 (N1 and N2
                                                                        The lack of difference between models may be due to
          respectively), computing the weights for all the
                                                                    the construction of neighborhood sets for a single word in
          neighbor words using Equation (1).
                                                                    the CDSA model. Since there are many neighbors that exist
     2.   Calculate neighborhood intersection as follows:
                                                                    for any particular word, there are many degrees of freedom
               ∑ (λ                    )                            that exist for determining the meaning for a single word.
                         n w1
                              + λn w2                               For instance, if one is asked to give an association to the
            n∈ N 1 ∩ N 2
                                                                    word cow, there are many possible associations (e.g.,
               ∑ (λ                    )                            animal, milk, burger, etc), which will lead to a very general
                         n w1
                              + λn w2         (2)                   non-specific representation of a single word.
                                                                       The purpose of Experiment 2 is to try to use the model to
            n∈ N 1 ∪ N 2
                                                                    represent the meaning of word-pairs.           This involves
                                                                    imposing constraints on the neighbors for each pair in order
The numerator is the summation of weights over the                  to more accurately represent the contextual meaning of the
intersection of the neighborhood sets (N1 and N2) whereas           pair. For instance, cow-graze should give a more specific
the denominator is the summation of weights over the union          representation of cow than cow without a context because
of the two neighborhood sets. This formula produces a               constraints are built on the meaning of cow. These
value between 0 and 1.                                              constraints initially involve measuring the neighborhood
   In the next section we will discuss how the CDSA model           overlap between the neighbors of cow and the neighbors of
was evaluated.                                                      graze, which then are used to compare to another set of
                                                                    information (e.g., word, sentence).
CDSA Model Evaluation
In four experiments we evaluated the CDSA model against                                     Experiment 2
LSA and human raters. The estimations of word and
sentence meanings in the CDSA model and LSA were                    A central theoretical assumption in Experiment 1 was the
trained on the TASA corpus.              Ratings in all four        idea that neighborhood intersection plays a prominent role
                                                                    in the relation between words. But how can the current
                                                                1389

model account for conceptual relationships beyond the word          Additionally the union of the neighborhood weights (i.e.,
level? Figure 1 gives an illustration of how this could be       the entire neighbor weights of all words in each pair) was
done. If two pairs are being compared, the neighborhood          calculated for F to compare the effectiveness of intersecting
overlap of each pair is pooled into F1 and F2. Then the          the neighborhoods.
intersection (Equation 2) is calculated to access the
similarity between the two pairs. This constrains the            Word Pairs We constructed 53 word pairs that had a
degrees of freedom for the pair, which eliminates any            frequency over 10 in the TASA corpus. Separate sets of
information that is not mutually shared by both words in the     pairs were intended to be unrelated (e.g., bear/cave—
pair (i.e., the problem found in Kintsch’s predication           pen/write), related by analogy (e.g., bear/cave—fish/pond),
algorithm). Therefore, each word is always dependent on          or related by both analogy and semantic relation (e.g.,
the context in which it appears. As the context for a word       teeth/bite—leg/kick).
becomes more specific (i.e., as reflected by the number of
unique words it appears with), the less likely that the same     Results and Discussion
context will be associated with any random word. For             Human ratings (M = 3.46, SD = 1.62) were significantly
instance, chair-sit has a smaller neighborhood set than the      correlated with CDSA intersection, r = .60, p < .001, and
sum of neighbors for chair and the neighbors for sit. This       union, r = .51, p < .001. LSA cosines were also related to
assumption therefore states that word pairs, or even             human similarity ratings, r = .64, p < .001.
sentences, are different than the sum of its parts, an              It appears that imposing context reduced the correlation
assumption quite different from current models of                with rated similarity of 2-word pairs, compared with single-
associative learning like LSA.                                   word pairs. As can be seen LSA performance also drops.
                                                                 Most notably, the union of neighbor sets does not perform
                                                                 as well as the intersection version of the CDSA model.
                                                                    The purpose of Experiment 3 was to examine how
                                                                 performance would be affected by implementing more
                                                                 context through comparison of 3-word phrases.
                                                                                        Experiment 3
                                                                 The process we used in building constraints on three-word
                                                                 combinations involves a multinomial neighborhood overlap
                                                                 (N-O) among all neighborhood pairs. Each neighbor that is
                                                                 shared by at least two neighborhoods is then pooled into F.
                                                                 Figure 2 gives an illustration of how this can be achieved..
Figure 1: The recursive nature of neighborhood overlap.
The neighborhood overlap (F1) of chair (N1) and sit (N2) is
intersected with to the neighborhood overlap (F2) of bed
(N3) and lay (N4 ).
Model modifications
Neighborhoods were first built on words within the pair as
described in equation 1. As described in Figure 1, the
neighborhood N1 of chair is intersected with the
neighborhood N2 of sit to yield a new neighborhood F1 that
represents the “chair sit” neighborhood. Since any shared
neighbor in N1 and N2 each have a separate weight, the
average of the two weights (Equation (3)) is calculated to
represent the new weight for each neighbor in F1.
                                                                 Figure 2: 3-word neighborhood overlap for two 3-word
           λn w w
                 1  2
                        1
                        2
                           [
                      = λn w1 + λn w2      ]    (3)
                                                                 combinations. The neighbors of each combination are
                                                                 compared for neighborhood overlap, which are then pooled
                                                                 into a neighborhood F for each pair.
In the same manner, we obtain F2. Once F1 and F2 have            Model Modifications
been calculated for both word-pairs, the neighborhood
intersection is calculated (as described in Equation (2)), to    Equation 3 is used to compute weights for the words that are
access the relationship between the 2-word pairs.                in the intersection of any two neighborhoods (o1, o2, and o3).
                                                             1390

By making all possible intersections between each                   performance to 5-word and 7-word contexts in syntactic
neighborhood N1, N2, and N3, a select number of words may           tagging. So if a sentence had five words, a multinomial
be counted three times. Therefore neighbors shared by all           N-O calculation between the 1st 2nd and 3rd word
three sets (o4; see figure 2) will be averaged (i.e., divided by    neighborhoods would produce P1 (i.e., as described in
3) and eliminated from any other N-O to avoid multiple              Experiment 3), then N-O would be calculated between the
counts. The computation for the weights in the intersection         4th and 5th neighborhood words to produce P2 (as described
of the three neighborhoods is simply an extension of                in Experiment 2). Then the N-O between P1 and P2 would
equation 3, where the average is taken with three weights           be calculated to produce the final neighborhood F for the
instead of two. Additionally each neighbor in o4 is a special       sentence. The intersection between F1 and F2 (Equation 2)
neighbor because it is shared by all neighbor sets. Therefore       will give the final similarity between the two sentences.
these neighbors are multiplied by a constant of 3 (i.e., since      This word-chunking hypothesis is consistent with the
there are three sets) to give greater importance to these           intuition that adjacent words in a sentence constrain
context bound neighbors. Once F1 and F2 have been pooled            meaning more than nonadjacent words in a sentence.
together by all the N-O for each pair, the neighborhood                Finally, the union of the neighborhood weights (i.e., the
intersection is calculated (Equation 2), to access the              entire neighbor weights of all words in each pair) was
relationship between the 3-word pairs.                              calculated for F to compare the effectiveness of intersecting
  Additionally the union of the neighborhood weights (i.e.,         the neighborhoods.
the entire neighbor weights of all words in each pair) was
calculated for F to compare the effectiveness of intersecting       Sentences We constructed 42 sentences whose words had a
the neighborhoods.                                                  frequency over 10 in the TASA corpus. Sentences pairs
                                                                    were constructed of varying length (e.g., blue bird fed
Short phrases We constructed 58 three-word phrases that             babies nest tree -- bear protected cubs den; articles,
had a frequency over 10 in the TASA corpus. Some pairs              pronouns and prepositions were removed). Sentences were
were intended to be unrelated (e.g., bird/nest/fly—                 constructed so that about half were considered related and
brush/paint/art), related by analogy-like relations (e.g.,          half unrelated.
gun/shot/bullet — axe/chop/wood), and related by both
analogy and semantic relation (e.g., dog/loud/bark—                 Results and Discussion
cat/quiet/meow).                                                    Human ratings (M = 2.12, SD = 1.48) were significantly
                                                                    correlated to CDSA model 3 word chunking intersection, r
Results and Discussion                                              = .69, p < .001, CDSA union, r = .65, p < .001, and the
Human ratings (M = 3.10, SD = 1.78) were significantly              CDSA multinomial intersection, r = .56, p < .001. LSA
related to the CDSA intersection, r = .63, p < .001, and            cosines were also related to human similarity ratings, r =
union, r = .44, p < .001.        LSA cosines were related to        .50, p < .001.
human similarity ratings, r = .47, p < .001.         The results       The results give evidence that imposing context may be
give evidence that imposing context improves performance            important when calculating sentence similarity.            By
in calculating similarity. Furthermore, LSA performance             applying an arbitrary rule set to sentences of varying lengths
continues to drop as more word context is introduced. This          seems to yield better performance than just making all
in part could be due to the lack of constraints that are put in     possible intersections among neighbors. Alternatively, the
the sentence representation in LSA.                                 union of all the neighbors seems to perform just as well as a
                                                                    rule based intersection procedure. Possible reasons for this
                        Experiment 4                                will be discussed next.
The purpose of the present experiment is to test the CDSA
model to sentences of varying lengths (i.e., sentences                                 General Discussion
ranging from 4 to 6 words). One challenge that arises in            In sentence comprehension, comprehenders must
calculating sentence similarities is how to handle all the          understand the nature of word context and the constraints
possible intersections between word neighbors within one            one word places on another (Kintsch, 1998). In other
sentence. Therefore three conditions were tested on how to          words, comprehenders will have to ask themselves: how
calculate the final sentence neighbor set F. First, a               does the meaning of one word affect the meaning of another
multinomial approach entailing N-O among all                        word? The most straightforward relationship among words
neighborhood sets was pooled to get F. Weightings were              is an additive one, where the meaning of one word has no
computed between any N-O words as an extension of                   influence on the meaning of another word. In contrast, in
Equation 3, where the neighborhood intersections could              the case of sentence comprehension, the levels of a one
entail 2-6 neighborhoods.                                           word can dramatically change the effects of another word.
  Second, a word-chunking maximum likelihood approach               In this model, context refers to N-O among words in a
was used that calculated a set P for every three words in a         sentence. That is, changing levels of one word can
sentence (Johansson, 2000). This chunking approach using            dramatically affect the meaning of another word. Thus,
a 3-word context to any target word was found to give equal         without structural constraints involving processes similar to
                                                                1391

N-O, sentence meanings proceed in a radically different          time. Since the measures derived are computed on-line on
manner. Many relations shared between the pairs in the 4         the corpus, dynamically adding text to the corpus is not a
experiments were abstract relations, ones that were only         problem. Essentially, weights are changed between words
clearly established by filtering the individual word meanings    as soon as text is added.
and keeping shared information among words.
   The word-chunking N-O approach appears to perform                                 Acknowledgments
better than the multinomial N-O approach among all
                                                                 This research was supported by the National Science
neighbors.      Making all possible intersections among
                                                                 Foundation (REC 0106965, ITR 0325428) and the DoD
neighbors does not seem to be very psychologically
                                                                 Multidisciplinary University Research Initiative (MURI)
plausible since it would involve making many comparisons
                                                                 administered by ONR under grant N00014-00-1-0600. Any
between words that may not be relevant. For instance,
                                                                 opinions, findings, and conclusions or recommendations
comparing the first word to the last word in a sentence may
                                                                 expressed in this material are those of the authors and do not
not be important in evaluating the meaning of a sentence.
                                                                 necessarily reflect the views of ONR or NSF.
                                                                   The TASA corpus used was generously provided by
Possible improvements                                            Touchtone Applied Science Associates, Newburg, NY, who
As can be seen in experiment 4, N-O did not seem to help         developed it for data on which to base their Educators Word
predict sentence similarity to a great extent over the union     Frequency Guide.
of all the neighborhoods in a sentence. This may be due to
the arbitrariness of the rules used to calculate N-O for
varying sentence lengths. For instance, if the sentence was
                                                                                          References
6 words long, N-O would be calculated for the three words        Foltz, P.W., Gilliam, S., & Kendall, S. (2000). Supporting
and the last three words. With these two pools we would             content-based feedback in on-line writing evaluation with
then calculate F. This type of rule makes the assumption            LSA. Interactive Learning Environments, 8, 111-127.
that all 6-word sentences follow the same syntactic              Johansson, C. (2000). A Context Sensitive Maximum
structure. This obviously will not do for all 6-word               Likelihood Approach to Chunking. In: Proceedings of
sentences. Therefore, it seems likely that if the CDSA             CoNLL-2000 and LLL-2000, Lisbon, Portugal.
model was implemented with a syntactic parsing                   Kintsch, W. (1998) Comprehension: A paradigm for
mechanism, it could give the correct word pairs to calculate       cognition. New York: Cambridge University Press.
N-O for any sentence.                                            Kintsch, W. (2001) Predication. Cognitive Science 25, 173-
                                                                   202.
                        Conclusion                               Landauer, T. K., & Dumais, S. T. (1997) A solution to
                                                                   Plato’s problem: The latent semantic analysis theory of
The computational model presented here captures both word          acquisition, induction, and representation of knowledge.
and sentence meaning. There are several reasons why using          Psychological Review, 104, 211-240.
the CDSA model is advantageous. First, it uses simple            Landauer, T. K., Laham, D., Rehder, B., & Schreiner, M. E.,
mechanisms that are psychologically plausible. Second, it          (1997). How well can passage meaning be derived
gives the freedom to add more information to the corpus at         without using word order? A comparison of Latent
any time. Since the measures derived are computed on-line          Semantic Analysis and humans. In M. G. Shafto & P.
on the corpus, dynamically adding text to the corpus is not a      Langley (Eds.), Proceedings of the 19th annual meeting
problem. Essentially, many weights are changed between             of the Cognitive Science Society (pp. 412-417). Mahwah,
words as soon as text is added.                                    NJ: Erlbaum.
   The proposed computational model captures word and            Landauer, T. K. (2002). On the computational basis of
sentence meaning by appealing to constraints reflected in a        learning and cognition: Arguments from LSA. In N. Ross
corpus analysis. Embodiment theorists (Glenberg &                  (Ed.), The psychology of learning and motivation, 41, 43-
Robertson, 2000) may claim that there is no meaning                84.
derived from a corpus analysis because the words are not         Louwerse, M.M. & Ventura, M. (in press). How children
grounded in sensory-motor experience. In principle, one            learn the meaning of words and how computers do it
could have a more grounded corpus with units extensively           (too). Journal of the Learning Sciences.
embedded in sensory and motor experience. The TASA               Smith, L. B., Jones, S. S., & Landau, B. (1996). Naming in
corpus was simply readily available. Whether the episodic          young children: A dumb attentional mechanism?
experiences are reflected in TASA or in sensory-motor              Cognition, 60, 143-171.
experience, the theoretical assumptions of the CDSA model        Spellman, B. A., Holyoak, K. J., & Morrison, R. G. (2001).
are that, specific exemplars and associative processes are         Analogical priming via semantic relations. Memory &
sufficient to account for the judgments of meaning                 Cognition, 29, 383-393.
similarity. The CDSA model uses simple mechanisms that
rely on co-occurrences of words in exemplars.
   One additional advantage of the CDSA model is that it
allows more information to be added to the corpus at any
                                                             1392

