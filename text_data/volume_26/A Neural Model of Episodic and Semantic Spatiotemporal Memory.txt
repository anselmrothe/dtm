UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Neural Model of Episodic and Semantic Spatiotemporal Memory
Permalink
https://escholarship.org/uc/item/3rs718ts
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Author
Rinkus, Gerard J.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               A Neural Model of Episodic and Semantic Spatiotemporal Memory
                                              Gerard J. Rinkus (rinkus@comcast.net)
                                                                468 Waltham St.
                                                               Newton, MA USA
                             Abstract                                      allowing those IR cells having maximal input via
                                                                           previously modified weights to be reactivated (i.e., fully
  A neural network model is proposed that forms sparse                     deterministic recall). When they completely mismatch,
  spatiotemporal memory traces of spatiotemporal events                    enough noise is added to completely drown out the
  given single occurrences of the events. The traces are
                                                                           learned, deterministic inputs, resulting in activation of an
  distributed in that each individual cell and synapse
  participates in numerous traces. This sharing of
                                                                           IR having little overlap with preexisting traces.
  representational substrate provides the basis for similarity-               The opposing purposes of episodic memory and pattern
  based generalization and thus semantic memory.                           recognition (i.e., semantic memory)—i.e., remembering
  Simulation results are provided demonstrating that similar               what is unique about individual instances vs. learning the
  spatiotemporal patterns map to similar traces. The model                 similarities between instances—has led other researchers
  achieves this property by measuring the degree of match,                 to propose that the brain uses two complementary
  G, between the current input pattern on each time slice and              systems. McClelland et al (1995) and O’Reilly & Rudy
  the expected input given the preceding time slices (i.e.,                (1999) propose that the purpose of the hippocampus is to
  temporal context) and then adding an amount of noise,
                                                                           rapidly learn new specific information whereas the
  inversely proportional to G, to the process of choosing the
  internal representation for the current time slice. Thus, if G
                                                                           purpose of neocortex is to slowly integrate information
  is small, indicating novelty, we add much noise and the                  across individual instances thus coming to reflect the
  resulting internal representation of the current input pattern           higher-order statistics of the environment. The
  has low overlap with any preexisting representations of                  hippocampus then repeatedly presents its newly acquired
  time slices. If G is large, indicating a familiar event, we add          memory traces to neocortex, acting as trainer facilitating
  very little noise resulting in reactivation of all or most of            the gradual transfer of information to neocortex during the
  the preexisting representation of the input pattern.                     period of memory consolidation. We point out that
                                                                           TESMECOR is not such a two-component model. Rather,
                         Introduction                                      it is a monolithic model, i.e., it has a single local circuit
Any realistic cognitive model must exhibit both episodic                   architecture and processing algorithm (envisioned as an
and semantic memory. And, as emphasized by Ans,                            analog of the cortical mini-column) that satisfies the
Rousset, French, & Musca (2002), it must demonstrate                       opposing needs.
these properties for the spatiotemporal (or, sequential)
pattern domain. Thus, the model must be able to recall,                            Episodic Spatiotemporal Memory
without significant interference, large numbers of                         Rinkus (1995) introduced a neural network model,
spatiotemporal patterns, which we will call episodes,                      TEMECOR, of episodic memory for spatiotemporal
given only single presentations of those episodes.                         patterns. As shown in Figure 1, the model’s. Layer 1 (L1)
Furthermore, it must exhibit human-like similarity-based                   consists of binary feature detectors and its layer 2 (L2)
generalization and categorization properties that underlie                 consists of competitive modules (CMs). The L2 cells are
many of those phenomena classed as semantic memory.                        nearly completely connected via a horizontal matrix
  We propose a sparse, distributed neural network model,                   (H-matrix) of binary weights.
TESMECOR (Temporal Episodic and Semantic Memory                               The model operates in the following way. On each time
using Combinatorial Representations), that performs                        step, a pattern is presented to L1. On that same time step,
single-trial learning of episodes. The degree of overlap                   one L2 cell is chosen at random to become active in each
between its distributed memory traces increases with the                   CM corresponding to an active L1 cell. In addition, the
similarity of the episodes that they represent. This latter                horizontal weights from the L2 cells active on the prior
property provides a basis for generalization and                           time slice to those that become active on the current time
categorization and thus, semantic memory. The model                        are increased to their maximal value of one. In this way,
achieves this property by computing, on each time slice,                   spatiotemporal memory traces are embedded in the
the similarity, G, between the expected and actual input                   H-matrix. Later on, if we reinstate a set of L2 cells that
                                                                           was coactive in the past while learning an episode, the
patterns and then adding an amount of noise inversely
                                                                           remainder of that episode will be read out in time. That is,
proportional to G into the process of choosing an internal
                                                                           the model recalls spatiotemporal memories.
representation (IR) for that time slice. When expected and
actual inputs match completely, no noise is added,
                                                                     1155

                                      Layer 2 (L2)
                                                                                  i
                                                                                                 j
                                                                                                                     k
                                                          e
                                                                              f                                                      l
                                a                                                            g                                                 t=3
                                                    b                                                            h
                                                                          c                                                t=2
                                            Layer 1 (L1)                                d
                                                                                                        t=1
Figure 1: TEMECOR architecture showing how spatiotemporal memory traces are laid down amongst the horizontal
connections of Layer 2. Features {b,c,d} are active at t = 1, {e,f,h} at t = 2, and {i,j,k} at t = 3. Each L2 cell has horizontal
connections to all other L2 cells except those in its own CM. Only the connections increased while processing this particular
spatiotemporal pattern (episode) are shown. Note that although this figure shows each time slice of the episode being handled
by a separate portion of the network, this is purely to keep the figure uncluttered. In fact, all L1 cells and all L2 CMs are
eligible to become active on every time slice.
  TEMECOR exhibits high capacity, as shown in Figure                                         of M = 100) active features, chosen at random. The
2, as well as other essential properties of episodic                                         bottom row of the table shows that a network containing
memory, e.g., single-trial learning. The model’s beneficial                                  4000 L2 cells, i.e., 100 CMs having K = 40 cells each,
properties derive principally from its use of a sparse                                       can store 5693 such episodes.
distributed, or combinatorial, representational framework,
a framework underlying many other models—Willshaw,                                                              Table 1: Capacity Test Results
Buneman & Longuet-Higgins, 1969; Lynch, 1986; Palm,
1980; Moll & Miikkulainen, 1995; Coultrip & Granger,                                               E     E/L       F      K     L         V    Rset     H
1994. The key to its high capacity is that by randomly                                           237     0.30    285     8    800        36    96.3   52.3
choosing winners in the CMs, it minimizes the average                                            943     0.59    1132    16   1600       71    97.0   52.1
overlap amongst the memory traces.                                                               2104    0.88    2524    24   2400       105   97.0   51.8
                                                                                                 3691    1.15    4430    32   3200       138   97.2   51.4
                                           Episodes vs. L2 Cells
                                                                                                 5693    1.42    6831    40   4000       171   97.4   50.9
                      10000
                                    u80            c80                                         Table 1 was generated as follows. For each K, the
 Number of Episodes
                      8000
                                    u120           c120                                      maximal number of episodes, E, which could be stored to
                                    u200           c200                                      criterion average recall accuracy, 96.3%, was determined.
                      6000
                                                                                             Recall accuracy, Re, for a given episode e, is defined as:
                      4000                                                                      Re = (Ce − De ) (Ce + I e )                          (1)
                      2000                                                                   where Ce is the number of L2 cells correctly active during
                                                                                             recall of eth episode, De is the number of deleted L2 cells,
                         0                                                                   and Ie is the number of intruding L2 cells. The table
                          800        1600       2400               3200       4000           reports Rset, the average of the Re values for a whole set of
                                        Number of L2 Cells                                   episodes. All episodes were presented only once.
                                                                                               The other statistics in Table 1 are as follows. E/L is the
                                                                                             ratio of stored episodes to the number of cells in L2,
                                    Figure 2: Capacity Results
                                                                                             which increases linearly. F is the average number of
   Table 1 provides the data for the bold curve in the                                       instances of each feature across the entire set of episodes.
                                                                                             V is the average number of times each L2 cell in a given
figure. It gives the maximal capacity, E, and other
                                                                                             CM became active to represent the corresponding feature.
statistics for networks of increasing size, L. All episodes
had T = 6 time slices and each time slice had S = 20 (out                                    H is the percentage of weights increased, which is nearly
                                                                                      1156

constant, at just over 50%, across rows. As we allow the        which L2 cells will become active. The smaller the match,
fraction of weights to increase beyond 50%, more                the more noise that is added and the greater the difference
episodes are stored, but with a lower average recall            between the internal representation (IR) that would have
accuracy due to the increase in intrusion errors resulting      become active purely on the basis of the deterministic
from saturation of the weights.                                 inputs reflecting prior learning and the IR that actually
   While TEMECOR exhibited the major properties of              does become active. The greater the match, the less noise
episodic memory it was not initially intended to model          added and the smaller the difference between the most
semantic memory and, due to its completely random               highly implicated IR (on the basis of prior learning) and
method of choosing sparse internal representations at L2,       the actually chosen IR.
it did not exhibit the generalization and categorization           Figure 4 illustrates the basic principles by which the
properties that underlie semantic memory. The successor         model computes, on each time slice, the degree of match,
version of the model, TESMECOR was developed to                 G, between its expected input and the actual input and
address this shortcoming (Rinkus, 1996).                        then uses G to determine how much noise to add to the
                                                                internal representation selection scheme. Figure 4a shows
Semantic Spatiotemporal Memory                                  a pattern, A, presenting at t = 1. The H-weights are
   TESMECOR is shown in Figure 3. It has some                   increased (represented by the dotted lines) from the active
architectural differences with the original version             L1 cells onto an internal representation, IRA, comprised of
(essentially, relaxations of some of the original’s             the three L2 cells that emerge as winners in their
structural constraints) and a greatly modified winner           respective CMs. For purposes of this example, these three
selection process. The H-matrix of L2 is as before but the      winners can be assumed to be chosen at random.
vertical projection is generalized. There is no longer a           Figure 4b shows another pattern, B, presenting at t = 2.
1-to-1 correspondence between L1 cells and L2 CMs.              As with IRA, IRB can be assumed to be chosen at random.
Rather; each L1 cell connects to a fraction of all the L2       Here, we see the both H- and F-weights being increased.
cells chosen at random in simulations. In TESMECOR,                Figure 4c shows another trial with pattern A presenting
all CMs are active on every time slice. In addition, the        at t = 1. This time, IRA becomes active due to the
bottom-up, or forward, connections (F-weights) and the          deterministic effects of the previously increased weights
top-down, or reverse, connections (R-weights) are now           (which are now shown as solid lines). The cells of IRA
modeled separately and are modifiable.                          now send out signals via the H-matrix which will arrive at
                                                                the other CMs at t = 2.
                                                                   At this point, it is convenient to portray the t = 2 time
                 Global           Noise                         slice in two steps. Figures 4d and 4e show these two
                 Match          Generation
                                                                steps. Figure 4d shows the signals arriving via the
                                                                H-matrix at the same time that that signals arrive via the
                                                                F-matrix from currently active L1 cells. Thus, the L2 cells
                                                                in the three CMs on the right simultaneously receive two
                                                                vectors each carrying possibly different expectations
                                                                about which IR should become active (or equivalently,
                                                                different hypotheses about what the current state of the
                                                                world is). It is these two vectors that TESMECOR
                                                                compares. In this particular case, the three cells of IRB are
                                                                receiving full support via the H-matrix. In other words,
                                                                the temporal context says that IRB should become active.
                                                                However, these cells are receiving only partial support
                                                                (two out of four L1 cells) via the F-matrix. Indeed, this is
                                                                a novel input, pattern C, which has presented. Thus, the
                                                                current spatial context does not contain sufficient
            Figure 3: TESMECOR architecture.                    information (given this network’s history of inputs) to
                                                                clearly determine what IR should become active. We
   The most significant change between TEMECOR and              represent this less-than-maximal support for IRB by the
TESMECOR however is in the processing algorithm.                gray shading of its cells. Because of this mismatch, i.e., G
Specifically, TESMECOR adds circuitry implementing              < 1.0, we add some noise into the winner selection
spatiotemporal matching operations, both locally within         process. The final result is that a different L2 cell than the
each CM and globally over the entire L2. On each time           one most strongly implicated by the deterministic inputs
slice, the global degree of match between the actual            ends up winning the competition in one of the three CMs
current input and the expected input, given the                 (the bottom right-hand one) active at t = 2. Thus, Figure
spatiotemporal context of the current input, modulates the      4e shows a new IR, IRC, representing the novel pattern, C.
amount of noise injected into the process of selecting
                                                           1157

                                a)
                                a) t=1
                                   t=1                                                   b)
                                                                                         b) t=2
                                                                                             t=2
                c) t=1
                    t=1                                d)
                                                        d) t=2a
                                                            t=2a                                           e) t=2b
                                                                                                          e)   t=2b
                f) t=1                                g)
                                                       g) t=2a
                                                           t=2a                                         h)
                                                                                                         h) t=2b
                                                                                                             t=2b
Figure 4: Sketch of TESMECOR’s spatiotemporal pattern comparison and noise-modulated internal representation selection
scheme. See text for explanation. As in Figure 2, the division of the L2 CMs into separate groups for different time slices is
purely to avoid clutter. In the model’s actual operation, all CMs are active on every time slice.
   Figures, 4f, 4g, and 4h, show another possible scenario.
This time, we will again present pattern B at t = 2. However            3.  φi ,t = ∑ j∈∆ w ji , t > 0                                (4)
                                                                                               t −1
a novel pattern, D, having only two features in common
with A, presents at t = 1. As this is the first time slice of this                                         φi ,t
new trial, there is no prior context vector active in the               4.  Φ i ,t =                                          ,t > 0  (5)
H-matrix. For concreteness, let’s assume that this degree of                             max(max j∈CM (φ j ,t ), H Θt )
mismatch causes a new winner to be chosen in two of the
three CMs active at t = 1, resulting in a new IR, IRD. When                           Ψ u Φ v , t > 0
B presents at t = 2, the F-vector lends maximal support for             5.  χ i ,t =  i ,t wi ,t                                     (6)
IRB but the H-vector has great uncertainty; only 1/3 of the                            î Ψi , t             ,t = 0
maximal possible horizontal input arrives at the cells of IRB.
This seems like even a worse match than in Figure 4d                                                       χi ,t
                                                                        6.  Χi,t =                                                    (7)
(shown by an even lighter shading of the IRB cells than in                               max(max j∈CM ( χ j , t ), χ Θ)
Figure 4d). Consequently, more noise is added to the winner
selection process. Let’s assume that this degree of mismatch
leads to a new winner in two of the three CMs active at t =
                                                                        7.  π k ,t = max j∈CM Χ j , t k
                                                                                                                   ,1 ≤ k ≤ Q         (8)
2, resulting in a new IR, IRB*, for pattern B.
                                                                            Gt = ∑ k =1π k , t Q
                                                                                          Q
   With this example of the desired behavior in mind, we                8.                                                            (9)
now give TESMECOR’s processing algorithm, which is
computed on each time slice for each L2 cell.                                                 f ( Χ i ,t , Gt )
                                                                             pi , t =
                                                                                         ∑ j∈CM f ( Χi,t , Gt )
                                                                        9.                                                           (10)
 1.  ψ i ,t = ∑ j∈Γ w ji                                       (2)
                     t
                                                                          In step 1, each L2 cell, i, computes its total weighted
                          ψ i,t                                        input, ψi,t, from the set, Γt, of currently active L1 cells. In
 2.  Ψi , t =                                                  (3)
              max(max j∈CM (ψ j , t ), F Θt )                          step 2, the ψ values are normalized within each CM. That is,
                                                                       we find the maximum ψ value, in each CM and divide all
                                                                       the individual values by the greater of that value and
                                                                  1158

F-matrix threshold, FΘt. FΘt is needed to ensure that small         values by an expansive nonlinearity, f, so that the cell with
feedforward signals are not amplified in subsequent                 the maximal Χ value maps to a probability, pi,t, of 1.0 and
normalization steps. FΘt is a parameter that can vary from          the rest of the cells end up mapping to pi,t = 0.0. On the
one time slice to the next but we omit discussion of this           other hand, if Gt = 0, indicating that the actual input is
detail in this paper due to space limitations.                      completely unexpected in the current temporal context
  Steps 3 and 4 perform analogous operations for the                given all of the model’s past experience, then we want to
horizontal inputs. In step 3, i, computes its total weighted        make all the cells, in any given CM, be equally likely to be
input, φi,t, from the set, ∆t-1, of L2 cells active on the prior    chosen winner. Thus, in this case, f should be a compressive
time slice. In step 4, the φ values are normalized within each      nonlinearity that maps all cells in the CM to p= 1/K, where
CM. That is, we find the maximum φ value, in each CM and            K is the number of cells in the CM. Without going into
divide all the individual values by the greater of that value       details, the function, f, is a sigmoid that meets the above
and an H-matrix threshold, HΘt. HΘt is needed to ensure that        goals. In the last stage of step 9, we simply choose the
small H values are not amplified in subsequent                      winner in each CM according to the resulting distribution.
normalization steps. HΘt also varies from one time slice to           To summarize, on each time slice, every L2 cell compares
the next but again, space limitations force us to omit              two evidence vectors, the H-vector, reflecting the sequence
discussion of this detail. Note that steps 3 and 4 are only         of patterns leading up to the present time slice (temporal
applicable on non-initial time slices (t > 0) of episodes.          context), and the F-vector, reflecting the current spatial
  Step 5 works differently on the first time slices of              pattern (spatial context). These vectors are separately
episodes than on the rest. When t > 0, we multiply the two          nonlinearly filtered and then multiplicatively combined. The
pieces of evidence, Ψi,t and Φi,t, that cell i should become        combined evidence vector is then renormalized and
                                                                    nonlinearly filtered before being turned into a probability
active but we do this after passing them through separate
                                                                    distribution that governs the final selection of L2 cells to
exponential filters. Since Ψi,t and Φi,t, are both between 0
                                                                    become active. Note that this basic scheme can be extended
and 1, the final χi,t values output from this step are also
                                                                    to simultaneously compare other evidence vectors as well.
between 0 and 1. The exponential filters effect a                   This is one of our intended lines of future research:
generalization gradient: the higher the exponents, u and v,         specifically, we will examine incorporating a hippocampal
the sharper the gradient and the more sensitive the model is        component to the model, which will provide a third
to differences between inputs (i.e., the finer the                  evidence vector to the L2 cells.
spatiotemporal categories it would form) and the less                 The concept of controlling the embedding of internal
overlap between the internal representations chosen by the          representations (IRs) based on comparing the expected and
model. When t = 0, we do not have two vectors to compare.           actual inputs is common to other cognitive models, e.g.,
Instead, we simply pass the Ψ values through an exponential         Grossberg (1987). However, TESMECOR’s use of
gradient-controlling filter. The three different exponent           distributed IRs, rather than singleton IRs, requires a
parameters, u, v, and w, simply let us fine-tune the model’s        generalized comparison scheme. Specifically, with
generalization gradients. For example, we might want the            distributed IRs, there exists a range of possible degrees of
model’s sensitivity to featural similarity to be stricter at the    overlap between IRs. We want to use that range to represent
beginning of episodes than on the successive time slices of         the spatiotemporal similarity structure of the environment to
episodes; thus we would set w higher than u.                        which the model has been exposed. Therefore, rather than
  In step 6, we normalize the combined evidence vector,             having a single threshold for judging the similarity of the
again subject to a threshold parameter, χΘt, that prevents          current input and expected inputs (e.g., ART's vigilance
small values from erroneously being amplified. In step 7,           parameter), TESMECOR's continuous-valued similarity
we simply determine the maximum value, πi,t, of the Χi,t            measure, G, is used to inject a variable amount of noise into
values in each CM. These π values constitute local, i.e.,           the IR-selection process, which in turn allows for selecting
within each CM, comparisons between the model’s                     IRs whose degrees of overlap are correlated with their
expected and actual inputs. In step 8, we compute the               spatiotemporal similarities.
average of these local comparison results across the Q CMs
of L2, resulting in the model’s global comparison, Gt, of its                           Simulation Results
expected and actual inputs.
                                                                    In this section, we provide the results of preliminary
  In step 9, we convert the Χi,t values back into a
                                                                    investigations of the model demonstrating that it performs
probability distribution whose shape depends on Gt. We              similarity-based generalization and categorization in the
want to achieve the following: if Gt is 1.0, indicating that        spatiotemporal pattern domain.
the actual input has perfectly matched the model’s expected           The four simulations described in Table 2 were performed
input, then, in each CM, we want to choose, with probability        as follows. In the learning phase, E episodes were presented,
1.0, the cell belonging to the IR representing that expected        once each. Each episode consisted of 5 time slices, each
input. That cell, in each CM, is the one having the highest Χ       having 20 (out of 100) randomly selected features present.
value. Since, in general, other cells in that cell’s CM could       Then, perturbed versions, differing by d = 2, 4, 6, or 8 (out
have non-zero or even high Χ values, we need to filter the          of 20) features per time slice from the original episodes
                                                                1159

were generated. The model was then tested by presenting           specifically, 4 out of 20 features were substituted on each
the first Z time slices of the perturbed episodes as prompts.     time slice (for a total of 8 featural differences). In all but
Following the prompt time slices, the model entered a free-       two cases, the model ‘locks into’ the L2 trace corresponding
running mode (i.e. cutting off any further input) and             to the most-closely-matching original episode (i.e., the
processing continued from that point merely on the basis of       episode from which the degraded prompt was created.
signals propagating in the H-projection.                            These simulations provide preliminary evidence that
                                                                  TESMECOR exhibits generalization, and in fact
        Table 2: Generalization/Categorization Results            categorization, in the spatiotemporal domain, while at the
                                                                  same time exhibiting episodic memory since the episodes
                                                                  are learned with single trials.
    Simulation         E         d     Z           Rset
          1            27        2     1          92.3%                              Acknowledgments
          2            13        4     1          98.0%           Thanks to Daniel H. Bullock, Michael E. Hasselmo,
          3             7        6     1          98.3%           Michael A. Cohen and Frank Guenther, for discussions that
          4            13        8     2          82.7%           provided many helpful ideas during the formulation of this
                                                                  theory and preparation of the doctoral dissertation from
   These results indicate that the model was extremely good       which this work was derived.
at locking into the trace corresponding to the most-closely-
matching original episode. The accuracy measure, Rset (eq.                                References
1) measures how close the recall L2 trace is to the L2 trace      Ans, B., Rousset, S., French, R.M., & Musca, S. (2002)
of the most-closely-matching original episode. The accuracy         Preventing Catastrophic Interference in Multiple-
for simulation 4 (82.7%) may seem low. However, if the              Sequence Learning Using Coupled Reverberating Elman
accuracy measure is taken only for the final time slice of          Networks. Proc. of the 24th Annual Conf. of the Cognitive
each episode then it is close to 100% for all four                  Science Society. LEA, NJ.
simulations. The view taken herein is that given that the         Carpenter, G. & Grossberg, S. (1987) Massively parallel
pattern to be recalled are spatiotemporal, the most relevant        architectures for a self-organizing neural pattern
measure of performance is the measure of accuracy on the            recognition machine. Computer Vision, Graphics and
last time slice of the test episode. If the model can “lock         Image Processing. 37, 54-115.
into'' the correct memory trace by the end of the recalled        Coultrip, R. L. & Granger, R. H. (1994) Sparse random
trace, then that should be sufficient evidence that model has       networks with LTP learning rules approximate Bayes
recognized the input as an instance of a familiar episode.          classifiers via Parzen’s method. Neural Networks, 7(3),
                                                                    463-476.
           Table 3: Per-Time-Slice L2 Accuracy for                Lynch, G. (1986) Synapses, Circuits, and the Beginnings of
           the Test Trials of Simulation 4 of Table 2               Memory. The MIT Press, Cambridge, MA.
                                                                  McClelland, J. L., McNaughton, B. L., & O’Reilly, R. C.
                                                                    (1995) Why there are complementary learning systems in
    Episode       T=1      T=2     T=3      T=4      T=5            the hippocampus and neocortex: Insights from the
                                                                    successes and failures of connectionist models of learning
         1         0.9       0.9     1.0      1.0       1.0         and memory. Psychological Review, 102, 419-457.
         2        0.82       1.0     1.0      1.0       1.0       Moll, M. & Miikkulainen, R. (1995) Convergence-Zone
         3        0.67       1.0     1.0      1.0       1.0         Episodic Memory: Analysis and Simulations. Tech.
         4        0.82       0.9     1.0      1.0       1.0         Report AI95-227. The University of Texas at Austin,
         5        0.67      0.82     1.0      1.0       1.0         Dept. of Computer Sciences.
                                                                  O'Reilly, R. C. & Rudy, J. W. (1999) Conjunctive
         6        0.67       0.9     1.0      1.0       1.0
                                                                    Representations in Learning and Memory: Principles of
         7         0.9       1.0     1.0      1.0       1.0         Cortical and Hippocampal Function. TR 99-01. Institute
         8        0.74       1.0     1.0      1.0       1.0         of Cognitive Science, U. of Colorado, Boulder, CO
         9        0.74       1.0     1.0      1.0       1.0       Palm, G. (1980) On Associative Memory. Biological
         10       0.67      0.82     1.0      1.0       1.0         Cybernetics, 36. 19-31.
         11       0.54      0.67    0.22      0.0       0.0       Rinkus, G. J. (1995) TEMECOR: An Associative,
         12       0.48      0.21     0.0      0.0       0.0         Spatiotemporal Pattern Memory for Complex State
         13       0.82       0.9     1.0      1.0       1.0         Sequences. Proc. of the 1995 World Congress on Neural
                                                                    Networks. LEA and INNS Press. 442-448.
   Table 3 shows the details of the simulation 4 in Table 2.      Rinkus, G. J. (1996) A Combinatorial Neural Network
                                                                    Exhibiting both Episodic Memory and Generalization for
Specifically, it shows the L2 accuracy on each time slice of
                                                                    Spatio-Temporal Patterns. Ph.D. Thesis, Graduate School
each episode during the recall test. For each recall trial the
                                                                    of Arts and Sciences, Boston University.
model received a prompt consisting of degraded versions of        Willshaw, D., Buneman, O., & Longuet-Higgins, H. (1969)
the first two time slices of the original episode—                  Non-holographic associative memory. Nature, 222, 960-962
                                                              1160

