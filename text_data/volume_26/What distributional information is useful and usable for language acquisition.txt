UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
What distributional information is useful and usable for language acquisition?
Permalink
https://escholarship.org/uc/item/0776c074
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Monaghan, Padraic
Christiansen, Morten H.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                     University of California

     What distributional information is useful and usable for language acquisition?
                                             Padraic Monaghan (pjm21@york.ac.uk)
                                              Department of Psychology, University of York
                                                            York, YO23 1ED, UK
                                         Morten H. Christiansen (mhc27@cornell.edu)
                                              Department of Psychology, Cornell University
                                                           Ithaca, NY 14853, USA
                             Abstract                                     for syntactic categorization, and questions what type of
                                                                          information is most useful and thus usable by the child.
  Numerous theories of language acquisition have indicated that           Theories of the use of distributional information in language
  distributional information is extremely valuable for assisting          acquisition have suggested different analyses of the context
  the child to learn syntactic categories, yet these theories differ
                                                                          in which a word (category) occurs, but no empirical
  over the type of information that is proposed as useful in
  acquisition. Mintz (2003) has proposed that children utilize
                                                                          comparisons of these competing accounts have been made.
  the previous word and the following word (AxB frames) for               We present a series of computational models that compare
  acquiring categories, whereas Monaghan, Chater, and                     the extent to which accurate syntactic categorization of
  Christiansen (submitted) have suggested that information                language directed to the child can be made on the basis of
  about the previous word alone provides a rich source of data            different sources of distributional information.
  for categorization. In three modeling experiments we found
  that bigrams were better than fixed AxB frames for learning                    Sources of distributional information
  syntactic categories in a corpus of child-directed speech.
  However, presentation of the preceding and succeeding words                Theories of distributional information in language
  when these can be processed separately resulted in better               acquisition have tended to focus on demonstrating that such
  learning than presenting the preceding word alone, and also             information     can      contribute     significantly  toward
  improved performance over presenting the previous two                   categorization, rather than proposing that the particular
  words.                                                                  implementation is psychologically realistic. Redington,
                                                                          Chater, and Finch (1998) produced context vectors based on
                         Introduction                                     the two preceding words and the two words following the
  What sort of information does the child use to develop an               target word from the CHILDES (MacWhinney, 2000)
understanding of their language? The rational analysis                    database of child-directed speech. The resulting vectors for
approach answers this question by assessing what sort of                  the most frequent 1000 words in the database clustered
information is useful for learning the language. If a                     together with a high correspondence to syntactic categories.
particular source of information proves to be rich and                    Redington et al. (1998) also assessed vectors resulting from
reliable then a computational system (of which the child is a             using different context words. They found that good results
very special case) will exploit it. The child learns a sense of           were also obtained for the one preceding and one following
syntactic categories early in language development. In order              word, and also for the two preceding words, and for the two
to understand speech and relate it to the world, the child                succeeding words (with better performance for preceding
must know which part of speech refers to an action, and                   words than succeeding words). Yet, using only the
which to objects, and which words modify relations between                immediately preceding word also resulted in good
objects. “Look at the cow mooing” elicits many possibilities              performance, though addition of richer contextual
for relations between words and the world, for example,                   information improved performance.
whether the animal in question is referred to by the word                    An alternative approach is the proposal that particular
“cow”, “look”, or “mooing”. Constraints within the                        sequences of words are useful for determining syntactic
language, restricting which words in the sentence can refer               category. Fries (1952) produced a set of “frames” in which
to objects, for example, greatly limit the number of                      only words of a certain category could appear. For example,
possibilities for relating words to the world.                            only a noun could appear in “The __ is/was/are good”.
  But what sort of information is useful for constructing                 Similarly, Maratsos and Chalkley (1980) proposed that there
syntactic categories? A variety of different types of                     were local constraints on the occurrence of particular word
information have been proposed as useful for categorization,              categories, such as that only a verb can occur before the
including      gestural,      semantic,      phonological,        and     inflection –ed.
distributional information. Combining more than one type                     Mintz (2003) provided an empirical test of this local
of information has indicated improvements in categorization               source of information, by analyzing corpora of child-
(Reali, Christiansen, & Monaghan, 2003), and it may indeed                directed speech for the occurrence of frames of the
be the case that combining multiple sources is necessary for              preceding and the succeeding words. We refer to these as
categorization to take place (Braine, 1987).                              AxB frames, where A and B are fixed, and x indicates the
  This paper focuses on distributional information as a cue               intervening word. For example, for the frame “you __ to”,
                                                                          “go” and “have” both occur as “x” words in the frame.
                                                                      963

Mintz selected the 45 most frequent frames involving the            relationship between A and x and B and x can also
preceding and succeeding word, and then grouped the words           contribute separately towards categorization, and compared
that occurred within each of these frames. In the above             performance to a model with information about the two
example, “go” and “have” would be grouped together in the           preceding words.
analysis. Accuracy was assessed by counting the number of
times that words of the same category were grouped                                         Experiment 1
together, and dividing this by the number of pairings of all
words within the groups. Completeness was determined by             Method
counting the number of pairings of words of the same
                                                                    Corpus preparation From the CHILDES database, we
category within the group, and dividing this by the number
of pairings of words of the same category occurring in any          selected a corpus of speech directed towards a child of age
                                                                    0-2;6 years (anne01a-anne23b, Theakston, Lieven, Pine, &
of the groupings.
                                                                    Rowland, 2001). This was one of the corpora used by Mintz
   The 45 most frequent frames resulted in high accuracy but
                                                                    (2003). We replaced all pauses and turn-taking with
low completeness, indicating that these frequent AxB
frames grouped together words of the same category, but             utterance boundary markers, and the resulting corpus
                                                                    contained 93,269 word tokens in 30,365 utterances (mean
that many words of the same category tended to occur in
                                                                    utterance length = 3.072 words). There were 2,760 word
different groups. Relatedly, Mintz (2002) found that people
                                                                    types, and the syntactic category for these words was taken
categorized words together when they occurred in AxB
frames in an artificial language learning task, and                 from the CELEX database (Baayen, Pipenbrock, &
                                                                    Gulikers, 1995), according to the most frequent category
consequently claimed that such AxB frames were a source
                                                                    usage for each word. Some interjections, alternative
of distributional information that children used to acquire
                                                                    spellings, and proper nouns were hand-coded. There were
syntactic categories.
   An alternative proposal is that a frame involving only the       12 syntactic categories: noun, adjective, numeral, verb,
                                                                    article, pronoun, adverb, conjunction, preposition,
preceding word – an Ax frame – is required in order to
                                                                    interjection, wh-words (e.g., why, who), and proper noun.
produce effective categorization (e.g., Valian & Coulson,
                                                                    Analysis In accordance with Mintz (2003), we selected the
1988). Monaghan, Chater, and Christiansen (submitted)
found that categorizations of child-directed speech based on        45 most frequent AxB frames from the corpus, and
                                                                    determined the words that occurred in the x position within
the association between the 20 most frequent preceding
                                                                    each frame. Each AxB frame thus resulted in a cluster of
words and the target word resulted in accurate classification
                                                                    words. Accuracy and completeness were assessed in the
of words of different categories, but critically, also resulted
in a large proportion of words being classified. Additionally,      same way as for Mintz (2003), described above. An
                                                                    additional method for assessing completeness was taken as
Monaghan et al. showed that, in an artificial language
                                                                    the total number of word types that were classified in (at
learning task, participants could group words on the basis of
                                                                    least) one frame.
Ax frame information alone.
   Both AxB and Ax frames can therefore be exploited in                 For the Ax analysis, the 45 most frequent words were
                                                                    selected from the corpus, and co-occurrence with these
learning artificial languages, but which source of
                                                                    frequent words formed the clusters in the bigram analysis.
information is most useful to the child learning their
                                                                    Accuracy and completeness were assessed in the same way
language? AxB frames result in high accuracy, but low
completeness, whereas Ax frames produce high                        as for the AxB co-occurrence analysis.
completeness at the expense of some accuracy. Should a
learning system select accuracy over completeness, or vice
                                                                    Results
versa?                                                              As an example of the resulting classification, Table 1 shows
   A comparison of different sources of distributional              a summary of the words that were classified into the 5 most
information requires that alternative methods are subjected         frequent AxB and Ax frames. For these most frequent AxB
to the same analyses. In addition, an empirical test of             frames, two frames clustered verbs together, and two
whether accuracy or completeness is a priority in acquisition       clustered only pronouns. For the Ax classifications, the
is necessary. We now present a series of modeling                   results are noisier, but have far higher numbers of words
experiments that test the extent to which different types of        classified. The most frequent Ax frame – “the x” – classifies
distributional information lead to successful categorization        623 nouns, and very few verbs, whereas the next most
of words in child-directed language. Experiment 1                   frequent Ax frame – “you x” – classifies 210 verbs, and
replicated Mintz’s (2003) analysis of AxB frames in child-          only 26 nouns. The accuracy and completeness results are
directed speech, and directly compared the resulting                shown in Table 2, together with those from Mintz (2003)1.
classification to an Ax analysis. Experiment 2 assessed             In parentheses are the random baseline values. We closely
whether a neural network model learned to categorise words          replicated Mintz’s (2003) results indicating the high
more accurately on the basis of AxB information or Ax               accuracy of the AxB frames, though, as noted in the
information alone. Finally, Experiment 3 tested a neural
network model learning from AxB information when the                1
                                                                      Data are shown from Mintz’s analysis of the anne corpus, with
                                                                    standard labeling and word-type analyses.
                                                                964

    Table 1. Classifications based on the 5 most frequent Ax        proportion of the words in the environment, but with the
                        and AxB frames.                             possibility that such classifications may contain more errors.
                                 AX                                     One way to test this issue is to train a neural network to
                                                                    base predictions of the syntactic category of words based on
      AX       noun verb pronoun adjective preposition other
         a
                                                                    either AxB frames, or Ax frames. After training, the neural
                335 33        2      56           0        11
        it                                                          network model’s error on the predicted classifications
                 37 69       12      29          13        43
        to       76 107      16       6           1         9
                                                                    reflects the extent to which the given source of information
      you        26 210      15      27           8        39       is beneficial for learning the syntactic categories of the
       the      623 23        9      38           5        14       language. If the model trained on AxB frames has lower
                                                                    error then learning is more effective when based on high
                                AXB                                 accuracy but low completeness, whereas if the model
     AXB noun verb pronoun adjective preposition other              trained on the Ax frames has lower error then high
    do_think 0        0       1       0           0         0       completeness at the expense of high accuracy is a better
    do_want 0         0       6       0           0         0       source of information for learning.
   are_going 0        0       5       0           0         0           We were concerned with how effective the frame is in
    what_you 0       10       0       0           1         0
     you_to                                                         predicting the category of the x word, so we trained the
                  0  19       2       1           1         1       models to predict the category of x without entering the
                                                                    identity of the x word at the input. In addition, we did not
   Table 2. Completeness and accuracy of classifications for        preselect the frames that were input into the model: the
           the Ax and the AxB co-occurrence models.                 entire corpus was used for training and not just the 45 most
                           CO-OCCURRENCE MODEL                      frequent frames, as we were interested in whether the model
                        MINTZ           AX            AXB           would be able to pick up which frames were useful for
   Accuracy            0.94 (0.41)   0.57 (0.22)    0.88 (0.26)     categorisation. From Mintz’s (2003) analysis, it is not clear
   Completeness        0.09 (0.04)   0.07 (0.04)    0.06 (0.03)     whether the AxB frames are to be interpreted as non-
   Words classified    405, 14.7%   1930, 69.9% 394, 14.3%          compositional, or whether the relationship between A and x
                                                                    and between x and B may also contribute to categorization.
Introduction, there was very low completeness for this              Experiment 2 tests the non-compositional interpretation,
classification. The Ax analysis also resulted in high               whereas Experiment 3 assesses the compositional version of
accuracy, and slightly higher completeness according to             the AxB frames.
Mintz’s definition. However, a striking difference between
the AxB and the Ax analyses is the overall number of words                                 Experiment 2
from the corpus that were categorized. Clustering based on          We trained two neural network models to learn to predict
bigrams resulted in a classification of almost 5 times as           the category of the target (x) word using the same corpus of
many words as the trigram analysis. The small differences           child-directed speech as in Experiment 1. We compared the
in completeness between the two analyses is therefore               learning of models that were given either Ax or AxB
misleading, as this only considered words that were                 information. The AxB model was designed to test whether
clustered – in the AxB case, completeness was assessed              the AxB frame was useful for learning when the frame is
over only a fraction of the corpus considered in the Ax             interpreted as a whole, i.e., the “A” and the “B” do not
analysis.                                                           contribute separately toward classification.
Discussion                                                          Architecture
We successfully replicated Mintz’s (2003) demonstration             Ax model The model was a feed-forward network with a set
that classifications of syntactic category based on                 of input units fully-connected to a hidden layer, which was
occurrence within the most frequent AxB frames resulted in          fully-connected to an output layer. The model is shown in
impressively high accuracy. However, our prediction that            Figure 1. Each unit in the input layer represented one word
high accuracy could also be achieved by the smaller, less           type in the child-directed speech corpus (so there were
specific Ax frame was supported. The Ax analysis had the            2,760 input units), and there was also a unit representing the
additional advantage of enabling a classification of far more       utterance boundary, in accordance with other connectionist
words from the child’s environment than was possible using          models of syntax learning (e.g., Elman, 1990) that provide
AxB frames. There is a pay-off between accuracy and                 this additional information to the simulated child learner.
completeness: a specific context will result in high accuracy,      There were 10 units in the hidden layer. The output layer
but low completeness, whereas a general context will result         contained units representing the syntactic category of the
in lower accuracy but high completeness.                            next word in the corpus. The model was trained on all Ax
    This raises the question as to whether categorization is        bigrams in the corpus, with the first word in the bigram
best based on information that renders highly reliable              occurring in the input layer, and the category of the second
classifications of only a few words, or whether learning            word in the bigram as the target at the output layer.
would benefit from using information that classifies a larger
                                                                965

                                                                       Table 3. Percent correctly classified and MSE for the Ax
                                                                      and AxB models for each syntactic category in the corpus,
                                                                    with number of tokens (n) and t-test on MSE (all p < 0.001).
                                                                                         % CORRECT                 MSE
                                                                     CATEGORY N           AX AXB AX AxB                     t
                                                                    Nouns         12458    66.3    0     0.533  1.000  -116.316
                                                                    Adjectives     4125     1.9    0     1.116  1.035    21.373
                                                                    Numerals       1087      0     0     1.128  1.040    20.304
                                                                    Verbs          23182   83.9    0     0.511  0.851  -145.602
                                                                    Articles       7996    31.0    0     0.848  1.025   -52.371
                                                                    Pronouns       18932   47.6    0     0.675  0.869   -71.369
                                                                    Adverbs        5456      0     0     1.150  1.040    46.221
                                                                    Prepositions 9491      31.3    0     0.865  1.016   -34.894
                                                                    Conjunctions 1955        0     0     1.147  1.032    29.448
                                                                    Interjections 3762       0     0     0.984  1.026   -24.608
                                                                    Proper nouns 2104        0     0     1.149  1.032    28.642
                                                                    Wh-words        3500     0     0     1.041  1.024    7.510
                                                                    Boundary       30365   79.6 100      0.446  0.793  -147.391
                                                                    TOTAL         123634   52.4 22.9     0.680  0.911  -205.957
       Figure 1. The feedforward neural network model of
   syntactic categorization. The active input unit represents       Results
 either the A-word in the Ax model, or the AxB frame in the
  AxB model. The active output unit is the category of the x        The Ax model performed better than the random baseline,
  word, or the utterance boundary if x represents the end of        MSE was 0.680 compared to 0.920, t(247266) = -189.808, p
  the utterance. In the Figure, the output verb unit is active.     < 0.001. The model also classified more words correctly
                                                                    than the random baseline: 52.4% compared to 22.9%, 2 =
AxB model The AxB model was identical to that of the Ax             75,014.859, p < 0.001.
model, except that in the input layer each unit represented             The AxB model performed at a level similar to the
one of the possible AxB frames. There were 36,607 such              random baseline. MSE was 0.911 which was slightly higher
AxB frames, and so there were 36,607 input units in the             than the randomized version of 0.910, t(247264) = 4.418, p
model. The model was trained on all AxB frames in the               < 0.001. Classification was poor, with the model classifying
corpus, with the A_B frame activating the appropriate unit          all words as the utterance boundary, which was the single
in the input layer, and the syntactic category of the x word        most frequent token in the input, This behavior was
as the output layer target.                                         identical to the performance of the AxB model on the
                                                                    randomized corpus.
Training and testing                                                    Table 3 shows the comparison between the Ax and the
                                                                    AxB models, for all words, and for each syntactic category.
The models were trained using backpropagation with
                                                                    In terms of MSE, performance was better for the Ax model
gradient descent with learning rate 0.01, and momentum
                                                                    than the AxB model on all categories apart from adjectives,
0.95. Before training, the weights between connections were
                                                                    numerals, adverbs, conjunctions, proper nouns, and wh-
randomized with mean 0 and standard deviation 0.1. We
                                                                    words. However, performance was better for the large
imposed a 0.1 error tolerance on the output units to prevent
                                                                    closed-class categories – pronouns and articles – and for
the development of very large weights on the connections.
                                                                    nouns and verbs. Overall, the Ax model classified more
The models were trained on all Ax or AxB frames in the
                                                                    words correctly than the AxB model, 2 = 75,014.011, p <
corpus, with each epoch being one pass through the corpus,
                                                                    0.001.
and training was halted after 5 epochs, which was over
600,000 training events. As a baseline, we trained and tested
                                                                    Discussion
the Ax model and the AxB model on a corpus where the
frequency of words was maintained, but word-order was               The Ax model performed significantly better than chance in
randomized. In the AxB randomized control model, there              predicting the category of the x word from the preceding
were 44,786 AxB frames and thus 44,786 input units in the           word. The AxB model performed at a chance level, and did
model.                                                              not discriminate any word category. The better performance
    The models were tested after each epoch on the whole            of the AxB model in terms of MSE on adjectives, numerals,
corpus, with the mean square error (MSE) across the output          adverbs, conjunctions, proper nouns and wh-words may
units taken as a measure of the ability of the model to learn       have been due to a broader context serving these categories
to categorize words in the corpus on the basis of either the        better: adverbs often occur after nouns in positions normally
Ax or the AxB information. As an additional measure, we             taken by verbs, and adjectives intervene between
assessed whether the target unit – that is, the appropriate         determiners and nouns. An enriched context would
category of the x word – was the most highly activated for          undoubtedly assist the categorization of these types.
each pattern presentation.                                          However, the better performance may merely have been due
                                                                966

to a lack of discrimination between any of the word types in        Table 4. Percent correctly classified and MSE for the AxB-c
the AxB model.                                                         and ABx models. T-tests are computed on MSE (all p <
    These simulations demonstrated that categorization of a                              0.001, except † p < 0.1).
large, entire corpus of child-directed speech was best                                % CORRECT                    MSE
achieved using information about the preceding word, rather          CATEGORY AXB-C           ABX       AXB-C      ABX        t
than information about set frames comprised of the                  Nouns           73.7       68.0      0.408     0.509  -43.808
preceding and the following word. Greater coverage of the           Adjectives      25.8         0       0.878     1.167  -44.306
set of words, rather than greater accuracy in categorization,       Numerals          0          0       1.185     1.149    5.969
resulted in better performance.                                     Verbs           85.4       86.6      0.289     0.466  -77.029
    The next experiment assessed whether a compositional            Articles        67.6       38.7      0.490     0.827  -72.861
treatment of the AxB frame may provide better information           Pronouns        80.5       53.5      0.361     0.585  -81.153
about the syntactic category of the target x word than the Ax       Adverbs         20.8         0       0.976     1.151  -33.207
                                                                    Prepositions    59.0       37.8      0.592     0.807  -50.213
frame alone, and compared it to a model with information            Conjunctions     0.5         0       1.140     1.148  -1.409†
about the two preceding words.                                      Interjections   80.8         0       0.671     0.957  -71.643
                                                                    Proper nouns     0.1         0       1.214     1.155   11.694
                       Experiment 3                                 Wh-words        38.6         0       0.817     1.006  -23.613
We trained neural network models to learn to predict the            Boundary        84.7       85.8      0.283     0.350  -26.769
                                                                    TOTAL           69.4       56.3      0.480     0.628 -147.470
category of the next word from the same corpus of child-
directed speech as used in Experiments 1 and 2. We
                                                                    performed with greater accuracy than the non-compositional
compared the learning of a model that was given
information about the preceding and the following word in           AxB model from Experiment 2 for all syntactic categories:
order to predict the category of the intervening word, but          overall, t(123633) < -300, p < 0.001, for each individual
could operate on this information separately and combined.          syntactic category, all t < -50, all p < 0.001.
We call this the AxB-compositional (AxB-c) model. We                    Compared to the Ax model in Experiment 2, the
also tested a model where information was given about the           additional word information in the AxB-c and ABx models
two preceding words: the ABx model. Note that these                 resulted in an increase in accurate classifications. For both
models embed the bigram information from the Ax model in            models, classification was more accurate (p < 0.001), and
the input. We predicted that both models would perform              resulted in lower error, both t < -300, p < 0.001. For the
better than both the Ax model and the non-compositional             individual syntactic categories, the AxB-c and the ABx
AxB model from Experiment 2. We also predicted that the             model performed better for all syntactic categories apart
AxB-c model would outperform the ABx model, as                      from numerals, all t < -50, all p < 0.001, though the
proximity to the target word is most informative.                   difference for conjunctions was non-significant.
                                                                        Table 4 compares the AxB-c model to the ABx model,
Architecture and training                                           indicating that accuracy was lower and MSE higher in the
                                                                    ABx model. The AxB-c model performed better on all
The AxB-c model had the same architecture as the Ax                 syntactic categories apart from numerals and proper nouns.
model in Experiment 2, except that it had two banks of input
units. In the first bank of units the unit corresponding to the     Discussion
A-word was activated, and in the second bank of units the
B-word unit was activated. At the output layer, the model           Providing decomposable information about the preceding
had to learn to predict the category of the x word. The same        and following word resulted in increased accuracy of
architecture was used for the ABx model, but it had as input        performance in the model. The AxB-c model classified
the two words preceding the target word.                            words of all syntactic categories better than the non-
    Training and testing was identical to that for the models       compositional AxB and the Ax models of Experiment 2.
in Experiment 2. Baselines for learning were determined by          Accuracy across all the categories was high, though
training and testing the models on the randomized corpus.           classifications of adjectives and adverbs was still inaccurate
                                                                    – these tended to be classified as nouns/pronouns and verbs,
Results                                                             respectively. Adding information about the two preceding
                                                                    words also assisted in increasingly accurate classifications,
For both models, performance was better than the random             though not to the same degree as providing the preceding
baseline in terms of accurate classifications and MSE. For          and succeeding word.
the AxB-c model, accuracy was 69.4% (baseline 22.9%), 2
= 82422.148, p < 0.001, and MSE was 0.480 (baseline                                     General Discussion
0.920), t(247266) = -329.487, p < 0.001. For the ABx
model, accuracy was 56.3% (22.9%), 2 = 60841.166, p <               Experiment 1 demonstrated, as predicted, that AxB
0.001, and MSE was 0.628 (0.920), t(247266) =                       information provides high accuracy at the expense of
-221.728, p < 0.001.                                                completeness, whereas Ax information results in slightly
    As predicted, both the AxB-c and the ABx model                  lower accuracy but much higher coverage of the language.
                                                                967

     Experiment 2 tested the extent to which a computational                          Acknowledgments
model could utilize AxB frame information in categorizing
                                                                   This research was supported in part by a Human Frontiers
the intervening word. The model trained on AxB frames              Science Program Grant (RGP0177/2001-B).
performed at slightly below chance level, and well below
the accuracy that could be achieved from categorizing on
the basis of Ax information alone. The high completeness of
                                                                                            References
Ax frames resulted in significantly better learning than the       Baayen, R.H., Pipenbrock, R. & Gulikers, L. (1995). The
high accuracy but low-coverage of AxB information.                  CELEX Lexical Database (CD-ROM). Linguistic Data
     However, when the model is able to learn on the basis of       Consortium, University of Pennsylvania, Philadelphia,
AxB information when this information is compositional,             PA.
i.e., the relationship between the preceding word and the          Braine, M.D.S. (1987). What is learned in acquiring word
target word and between the succeeding word and the target          classes: A step toward an acquisition theory. In B.
word can be computed separately, then a different picture           MacWhinney (Ed.), Mechanisms of Language Acquisition
emerges. The AxB-c model of Experiment 3 was more                   (pp.65-87). Hillsdale, NJ: Lawrence Erlbaum Associates.
accurate than the Ax model of Experiment 2. Furthermore,           Elman, J.L. (1990). Finding structure in time. Cognitive
this provided better classification results than the two            Science, 14, 179-211.
preceding words (the ABx model), though this latter model          Fries, C.C. (1952). The Structure of English: An
also improved performance over a non-compositional AxB               Introduction to the Construction of English Sentences.
frame or just the single preceding word.                             New York: Harcourt, Brace & Co.
     The simulations presented here suggest that learning is       MacWhinney, B. (2000). The CHILDES Project: Tools for
most effective when information about the preceding word             Analyzing Talk, Third Edition. Mahwah, NJ: Lawrence
and the succeeding word is available. However, this is only          Erlbaum Associates.
the case when the AxB frame is not computed as a whole.            Maratsos, M.P. & Chalkley, M.A. (1980). The internal
Learning must also be based in part on the relationship              language of children’s syntax: The ontogenesis and
between A and x and between x and B. In the experiments              representation of syntactic categories. In K.E. Nelson
presented in Mintz (2002), such a distinction is not made –          (Ed.), Children’s Language Volume 2, pp.127-214. New
the learning situation resembles that of the AxB-c model,            York: Gardner Press.
where the participant has access not only to the AxB frame,        Mintz, T.H. (2002). Category induction from distributional
but also to the Ax and the xB bigrams. Therefore, it is not         cues in an artificial language. Memory and Cognition, 30,
yet possible to distinguish the contribution of bigram and          678-686.
trigram information in adult learning situations (though see       Mintz, T.H. (2003). Frequent frames as a cue for
Onnis et al., 2003).                                                grammatical categories in child directed speech.
     The possibility remains that the requirement for category      Cognition, 90, 91-117.
learning depends on establishing distinctions and                  Monaghan, P., Chater, N., & Christiansen, M.H.
similarities between only a few words in the language: it is        (submitted). The differential contribution of phonological
not realistic or feasible to attempt to learn the whole             and distributional cues in grammatical categorization.
language simultaneously. However, performance for the              Onnis, L., Christiansen, M.H., Chater, N., & Gómez, R.
most frequent 100 words was poorer in the non-                      (2003). Reduction of uncertainty in human sequential
compositional AxB model than the Ax model, and even                 learning: Evidence from artificial grammar learning.
taking only those words that occurred in the most frequent          Proceedings of the 25th Cognitive Science Society
45 AxB frames resulted in poorer performance than for the           Conference (pp. 887-891). Mahwah, NJ: Lawrence
45 most frequent Ax frames.                                         Erlbaum.
     The experiments presented in this paper require the           Reali, F., Christiansen, M.H., & Monaghan, P. (2003).
models to learn pre-ordained syntactic categories. The task         Phonological and distributional cues in syntax acquisition:
facing the child is more difficult: the child must also             Scaling-up the connectionist approach to multiple-cue
construct the categories. Yet, both tasks concern learning          integration. Proceedings of the 25th Cognitive Science
about which words co-occur. When the relationship between           Society Conference (pp. 970-975). Mahwah, NJ: Lawrence
the occurrence of certain categories in particular                  Erlbaum.
distributional contexts is easy to learn then this                 Redington, M., Chater, N. & Finch, S. (1998). Distributional
demonstrates that the category itself is more clearly defined.      information: A powerful cue for acquiring syntactic
     We have shown that AxB frames provide poor                     categories. Cognitive Science, 22, 425-469.
information about categorization unless this information is        Theakston, A.L., Lieven, E.V.M., Pine, J.M., & Rowland,
componential, such that Ax information is also available.             C.F. (2001). The role of performance limitations in the
We suggest that the distributional information that a neural          acquisition of verb-argument structure: an alternative
network model finds most useful is more likely to be used             account. Journal of Child Language, 28, 127-152.
by the child in acquiring syntactic categories.                    Valian, V. & Coulson, S. (1988). Anchor points in language
                                                                     learning: The role of marker frequency. Journal of
                                                                     Memory and Language, 27, 71-86.
                                                               968

