UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Biased stochastic learning in computational model of category learning
Permalink
https://escholarship.org/uc/item/48x8x7vc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Author
Matsuka, Toshihiko
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

            Biased stochastic learning in computational model of category learning
                                   Toshihiko Matsuka (matsuka@psychology.rutgers.edu)
                                                  RUMBA, Rutgers University – Newark
                                        101 Warren St., Smith Hall 327, Newark, NJ 07102 USA
                               Abstract                                 with two diagnostic but perfectly correlated dimensions, the
                                                                        proportion of human participants who paid attention
   Matsuka and Corter (2003b) presented evidence that people            primarily to only one of the two correlated dimensions was
   tend to utilize only the minimally necessary information for         higher than that of those who paid attention to both of the
   classification tasks. This approach for categorization was
                                                                        two correlated dimensions approximately equally (see
   efficient and valid for the stimulus set used in the experiment,
   but might be considered a statistically or mathematically non-
                                                                        Figure 2, top row, third column). In other words, many
   normative approach. In the present paper, I hypothesized             participants utilized only the minimal necessary information
   human category learning processes are biased toward simpler          for this task. In contrast, the stochastic learning model
   representation and/or conception rather than complex but             inadequately predicted that a higher proportion of
   normative ones. In particular, a few variants of “biased”            participants would pay attention to the two correlated
   learning algorithms are introduced and applied to Matsuka            dimensions approximately equally.
   and Corter’s stochastic learning algorithm (2003a, 2004). The            The strategy of using minimal information may be a very
   result of a simulation study showed that the biased learning         natural and efficient usage of limited mental resources for
   models account for empirical results successfully.
                                                                        humans. This would be particularly true for real world
                                                                        categorization tasks, where the number of feature
                          Introduction                                  dimensions could easily exceed a manageable number, in
In their recent work, Matsuka and Corter (2003b & 2004)                 which many are not necessary or crucial (e.g., irrelevant and
investigated the possibility of using stochastic learning               or highly correlated) for successful categorization. There
rather than gradient-based methods in neural network                    are several ways that could lead people to use a lesser
models of human category learning. They introduced                      amount of information, resulting in simple conception of
stochastic learning models to more accurately account for               categories. One possible explanation is that there may be an
human category learning. The gradient based learning                    implicit or explicit penalizing mechanism in human
algorithm used in many neural network models may be                     cognition that encourages less complete but simpler
considered to have a normative justification (i.e., it models           concepts than more complete but more complex concepts.
how people “should” learn or process information), but may              Another possible explanation is that there may be a
not be descriptively valid at the individual level. Models              mechanism in human cognition that leads to a more
utilizing a gradient method for learning seem to require a              thorough search for simple concepts.
high degree of mental effort and assume that optimal                       In the present research, based on these remarks, I
adjustments are made to the vector of parameters on each                hypothesize and model human category learning as being
trial. In contrast, Matsuka & Corter’s stochastic learning              biased toward simpler and heuristic concepts 1 (or
model (2003a, 2004) does not assume that learning is                    representation) than complex and complete ones.
associated with monotonic increases in accuracy (and
attention) or continuous search for better categorization                               Biased Stochastic Learning
processes by humans.                Rather, it models random            The proposed algorithm is based on a simulated annealing
fluctuations or “errors” in people’s memory and learning
                                                                        algorithm (Kirkpatrick, Gelatt, & Vecchi, 1983; Metropolis,
processes, and how people utilize and “misutilize” such                 Rosenbluth, Rosenbluth, Teller, & Teller, 1958) and
errors.                                                                 somewhat resembles Boltzmann Machine (Hinton &
   In their simulation studies (Matsuka & Corter 2004a), the            Sejnowski, 1986).           In the present algorithm, initial
effectiveness of stochastic learning methods applied to an
                                                                        association weights are randomly selected from a uniform
ALCOVE-like model (Kruschke, 1992) was evaluated in                     distribution centered at 0, and initial dimension attention
several settings. The modified models were shown to be                  weights are equally distributed across all dimensions. This
satisfactory in replicating two phenomena observed in                   equal attention allocation in the early stages of learning is
empirical studies on categorization; namely, rapid change in
                                                                        motivated by the results of empirical studies (Matsuka,
attention processes (Macho 1997; Rehder and Hoffman                     2002; Rehder & Hoffman, 2003) that showed many
2003), and individual differences in distribution of attention          participants initially tended to evenly allocate attention to
(Matsuka & Corter 2003b).                                               the feature dimensions. In the present algorithm, at the
   Although the stochastic learning model reproduced more
                                                                        beginning of each training epoch, a hypothetical “move” in
realistic individual differences than models with a gradient
type learning algorithm, it did not replicate one tendency
                                                                        1
observed in the empirical study of Matsuka and Corter                     In the present paper, the concepts of categories correspond to the
(2003b). They found that for four dimensional stimulus sets             configurations of the association weights and dimensional attention
                                                                        attractiveness.
                                                                    915

the parameter space is computed by adjusting each                     Accept all weight and attention parameters at the
parameter by an independently sampled term. These                     probability of:
adjustment terms are drawn from a prespecified distribution.                                                         (       ) (            )
                                                                                                                                                −1
                                                                                                                   F wt , α t − F w s , α s
The move (i.e., the set of new parameter values) is then            P ( w t , α t | w s , α s , T t ) = 1 + exp
accepted or rejected, based on the computed relative fit or                                                                   Tt
utility (defined below) of the new values. Specifically, if                                                                                   (SL-3)
the new parameter values result in a better fit/utility, they         if F(wt, αt) > F(ws, α s), or 1 otherwise, where F(ws, α s) is
are accepted. If they result in a poorer fit/utility, they are        the fit index for the previously accepted parameter set,
accepted with some probability P. This probability is a               and Tt is temperature at time t.
function of a parameter called the “temperature”, which            STEP 4: Reduce temperature:
decreases across blocks according to the annealing schedule.           T t = T o δ (υ , t )                                                   (SL-4)
   Because of the human’s biased cognitive processes,                 where δ is the temperature decreasing function that take
possibly as a consequence of our implicit or explicit biased          temperature decreasing rate, υ, and time t as inputs.
processes and/or preference toward simpler but less                STEP 5: Generate new wkj and i.
complete concept (these processes are discussed in detail in
                                                                       wkjt = wkjs + r w , r w ~ Φ w (⋅)                                      (SL-5)
the model section), the learned concepts of categories, thus
the configuration of the association weights and attention             α it = α is + r α , r α ~ Φ α (⋅)                                      (SL-6)
strengths, are inclined toward simpler ones. Note that in the                       w            α
                                                                      where r and r are random numbers generated from
present algorithm the notion of simplicity (or complexity) is         prespecified distributions Φw and Φα.
directly related to the number of effective (non-zero, or non-     REPEAT STEPS 1~5 until stopping criterion is met.
subzero) association weights and attention strengths.
   The proposed models would not require computation                           Biased Stochastic Learning Models
intensive (back) propagations of classification errors.
Rather, in the present biased stochastic learning model            There are several approaches to model biased learning
framework, a very simple operation (e.g., comparison of            processes using stochastic learning model. Here, two
two values) along with the operation of stochastic processes       simple approaches are introduced. The first biased learning
are assumed to be the key mechanisms in category learning.         model is based on the parameter regularization in which
These learning algorithms can be applied to virtually any          complex parameter configurations are penalized. The
feed-forward NN model of human category learning                   second model based on asymmetric random distributions,
                                                                   searches simpler parameter configurations more thoroughly.
    General Algorithm for Stochastic Learning                      Model 1: Bias via penalizing fitness function
A general framework for the stochastic learning algorithm is
                                                                   In the present algorithm the utility index rather than the fit
discussed in this section. Here, the stochastic learning
                                                                   index is used for the decision on acceptance and rejection of
algorithm is embedded into ALCOVE, which is one of the
                                                                   the current parameter set. The utility of a particular
most studied and applied computational models of category
                                                                   parameter configuration is defined as a weighted sum of the
learning incorporating a selective attention mechanism
                                                                   accuracy in classification and the mental effort required by
(Kruschke, 1992). Again, it should be noted that this
                                                                   the parameter configuration. Thus, the utility index consists
learning algorithm is very general and can be applied to
                                                                   of two independent indices, namely “classification
virtually any NN model of category learning.
                                                                   accuracy”, L and “mental effort”, Q, both dependent on
                                                                   learnable parameters w and α at time t.
STEP 0: Initialization:
      Problem specific parameters: (T0,υ)                                  U (w t , α t ) = L(w t ,α t ) + Q (w t , α t )                  (M1-1)
           T0 : initial temperature.                               The L function can be the same function for the fitness
           υ : temperature decreasing rate                         index (i.e., Eq. SL-2). Here, the Q function may be
      Association weights wkj,, Attention strengths αi,,           considered as a penalty function, penalizing “complex”
                                                                   parameter configurations that are believed to require more
      Exemplar ψji
                                                                   mental effort. The general form of Q function is given as
STEP 1: Calculate ALCOVE output activations:
                                                                   follows:
   Ok =       w kj exp − c (       α i | ψ ji − x i |)  (SL-1)             Q(w t , α t ) = γ wφ w (wmt ) + γ α φ α (a mt )                  (M1-2)
                                                                   where φw and φα are functions calculating mental effort
           J                     I
STEP 2: Calculate fit index for the current parameter set:         required for specific parameter configurations at time t (i.e.,
                    N    K
    F (wt ,α t ) =           (d k − Okt ) 2             (SL-2)     wt and αt), and γw and γα are coefficients weighting these
                   n =1 k =1                                       mental efforts. Note that γw and γα also control relative
   where K = # categories, N = # input in one block, dk is a       importance of L and Q functions (i.e., accuracy vs.
   desired output for category node k. Here, the superscript t     simplicity). That is the hypothetical coefficient, γQ,
   indicates time.                                                 weighting importance of Q function relative to L function is
STEP 3: Accept or reject of parameter set, α & w:
                                                               916

included in γw and γα. I.e., γ w = γ Qγ w* and γ α = γ Q γ α* . Thus                    number which will lead its updated value toward zero is
                                                                                        higher than that of a random number that leads to the
Equations M1-1 and M1-2 may be rewritten as:
                                                                                        opposite direction. In other words, when the association
      U (w t , α t ) = L(w t ,α t ) + γ Q Q * (w t ,α t )                 (M1-1R)
                                                                                        weight value, wkj is negative, then the probability of drawing
      γ Q Q * (w t , α t ) = γ Q γ w* φ w (wmt ) + γ Q γ α* φ α (a mt )   (M1-2R)       a positive number is greater than a negative number; when
   There are several functions applicable for φ:                                        the weight is positive, then the opposite is true, or
                               (M1-3a);                                    (M1-3b)            P(r w > 0 | wkj > 0 ) < P(r w < 0 | wkj > 0)
   φw =            wkj2                              φ α = α i2
           j    k                                               i                               (                    ) (
                                                                                              P r w > 0 | wkj < 0 > P r w < 0 | wkj < 0   )     (M2-2)
   φ =
     w
                     (
                   I wkj > ζ     w
                                    )                                      (M1-4a)      For the attention strength parameter αi the probability of
           j    k
                                                                                        drawing a negative random move is larger than for a
   φα =          (
               I αi > ζ α    )                                             (M1-4b)      positive move, assuming that αi is constrained to be
           i
                                                                                        positive, thus,
where ζw and ζα are threshold values, and I(expression) is                                    P (r α > 0 ) < P (r α < 0 ) .                     (M2-3)
the indicator function that returns 1 if the expression is
                                                                                          Parameter updates are accomplished by the following
satisfied. Equations M1-3a and M1-3b, often referred to as
                                                                                        functions:
ridge penalty function or weight decay, encourage
                                                                                              wkjt +1 = wkjt + rkjw                             (M2-4)
parameter settings that have small parameter values,
whereas Equations M1-4a and M1-4b encourage parameter                                         α it +1 = α it + riα                              (M2-5)
settings that have large number of parameters with less than                            where rkjw ~ sgn (wkjt )⋅ Φ w (⋅) and riα ~ Φ α (⋅)
the threshold values ζs. More general φ function is given as
follows:                                                                                The random movement rm is drawn from the negatively
                              2                                      2                  skewed distributions for αi and wkj if wkj is positive, and
                     wkj                                   αi                           from the positively skewed distributions for negative wkj.
                          q        ,                           q             (M1-5)
φw =                                       φα =                                         Thus, the expected value of the distance of the random
                                2                                       2
         j   k          w                          i
                                                      1+ i
                                                              α                         movement leading the learnable parameters to zero is
                1 + kj                                            q                     greater than that of the opposite direction. This makes the
                            q
where q, which can be either time dependent or                                          model to decrease values of “irrelevant” parameters quickly.
independent, controls types of penalization or                                            There are several asymmetric distributions, and the χ2
encouragement. That is, Equations M1-5 approach                                         (Eq. M2-6, Figure 1, left panel) and Rayleigh (M2-7 &
Equations M1-3s as q → ∞, and approach Equations M1-4s                                  Figure 1, right panel) distributions are examples of
as q → 0 (Cherkassky & Mulier, 1997).                                                   asymmetrical distributions.
                                                                                                                           v  v
   In many simulation studies, relative, but not absolute                                                        1       1 2 2 −1    −x         (M2-6)
                                                                                              f ( x | v) =                   x exp
predicted attention allocation strengths are analyzed and                                                    Γ(v / 2 ) 2              2
compared (e.g. Matsuka, 2002). In such cases, the relative                              where Γ(·) is a gamma function, v is the degree of freedom.
attention strengths ai = αi/Σ(αm) should be used as inputs
                                                                                                          x         − x2
for the penalty function. In addition, the penalization                                    f (x | b ) = 2 exp                                   (M2-7)
functions do not have to be in the same form for association                                             b          2b 2
weights and attention strengths. For example, in order to                               where b is the Rayleigh distribution parameter.
pay attention to a smaller number of feature dimensions it
seems more sensible to use M1-4b or M1-5 with small q
values for the attention parameters, because the relative but
not absolute attention strength values are usually
considered. In contrast, either choice seems appropriate for
the association weight parameters where raw values are
usually used.
Model 2: Bias via asymmetric distribution.                                                Figure 1. Example asymmetric distributions. Left panel:
In the present model, random numbers are drawn from an                                  χ2 distributions with several different distribution
asymmetric distribution with its mode equal to zero. Thus,                              parameters. Right panel: Rayleigh distributions with
as in the previous model, the probability of drawing a                                  several different distribution parameters.
random number r from the vicinity of current values (i.e.,
vicinity of zero) is still the highest                                                    Since the modes of these asymmetric non-negative
      P (0 − ε < r < 0 + ε ) > P(M − ε < r < M + ε )                        (M2-1)      distributions are not zero, and the distribution parameters
     for all M ≠ 0.                                                                     affect both central tendencies and spreads of the
However, unlike the previous model, for a particular                                    distributions, the random numbers should be transformed as:
parameter value, the probability of drawing a random                                        r = − s t ( f ( x ) − MODE ( f ( x) ))              (M2-8)
                                                                                    917

where st is a time-dependent scalar controlling the width of      attention allocation is initialized equally (Matsuka & Corter,
the search areas. This ensures that the mode of the               2003a, 2004).
transformed random variable is zero and thus satisfies M2-           All three models were run in a simulated training
1. Note that the distribution parameter v or b may be             procedure to learn the correct classification responses for
selected a priori and held constant throughout the training,      the stimuli of the experiment. ARAY was run for 300
or they can be time dependent so that the model starts with a     blocks of training, where each block consisted of a complete
highly skewed distribution and terminates with a near             set of the training instances, while ASL and ARSL were run
normal distribution, or vice versa.                               for 500 training blocks. For each model, the final results are
   While the present biased learning model (bias via              based on 50 replications.
thorough searches around zero) may be interpreted as active          The model configurations (e.g., type of distribution,
bias, actively trying to reduce the effective number of           temperature decreasing rate & function, search ranges) for
parameters or simplifying concepts, the bias via                  ASL and ARSL were the same except for the additional
regularization (Model 1) may be interpreted as passive bias,      parameter-penalization functions incorporated in RSL to
involuntary resulting in simpler concept because of the           model biased processes in category learning. The random
limitation of mental capacity.                                    numbers for these two models were drawn from the Cauchy
                                                                  distribution, and its random number generation algorithm
                        Simulations                               was based on Ingber (1989). For ARSL, the ridge penalty
Here, I examined how the two new biased stochastic                (Equation M1-3a) was imposed on the association weights,
learning models account for individual differences in             and a subset selection method (M1-4b with ζ = 0.1) was
attention learning. To do this, I simulated the results of an     used for the relative attention strengths.
empirical study on classification learning, Study 2 of               For ARAY, a (pseudo) random number generator function
Matsuka (2002). In this study, there were two perfectly           from MATLAB Statistical Toolbox (MathWorks, 2001) was
redundant feature dimensions, Dimension 1 & Dimension 2           used to generate random numbers, and its transforming
(see Table 1), and those two dimensions are also perfectly        scalar s (see Eq. M2-8) was exponentially decreased during
correlated with category membership. Thus, information            the learning. For all models, an exponential function was
from only one of the two correlated dimensions was                used as the temperature decreasing function. Models’ user-
necessary and sufficient for perfect categorization               definable parameters (e.g., initial temperature, temperature
performance. Besides classification accuracy, data on the         decreasing rate, ζ, and etc…) were selected arbitrarily.
amount of attention allocated to each feature dimension
were collected in the empirical study. The measures of            Table 1: Stimulus structure used in Study 2 of Matsuka
attention used were based on feature viewing time, as
measured in a MouseLab-type interface (Bettman, Johnson,                      Category Dim1 Dim2          Dim3   Dim4
Luce, & Payne, 1993).                                                             A        1*       1*      3      4
   The empirical results that I am trying to simulate                             A        1*       1*      4      1
                                                                                  A        1*       1*      1      2
indicated that 13 out of 14 subjects were able to categorize                      B        2*       2*      2      1
the stimuli almost perfectly (Figure 2, top left panel). The                      B        2*       2*      3      2
aggregated results suggest that on average subjects paid                          B        2*       2*      4      3
attention to both of the correlated dimensions approximately                      C        3*       3*      1      3
equally (Figure 3, top middle panel). However, more                               C        3*       3*      2      4
interestingly when the attention data were analyzed per                           C        3*       3*      3      1
individual, it was found that many subjects tended to pay                        D         4*       4*      4      2
attention primarily to only one of the two correlated                            D         4*       4*      2      3
dimensions, particularly in the late learning blocks as shown                    D         4*       4*      1      4
                                                                              *Diagnostic feature
in Figure 2, top row third column (Matsuka & Corter,
2003). This suggests that subjects used only the minimal
                                                                  Results: All three models correctly replicated aggregated or
necessary information for this task.
                                                                  averaged relative attention allocations to the four feature
                                                                  dimensions (Figure 2, second column). However, there are
Simulation method: There were three ALCOVE-type
                                                                  some minor differences in their predictions; ARAY paid
models in the present simulation study, namely ALCOVE
                                                                  less attention to non-diagnostic dimensions than ASL,
with stochastic learning (ASL; Matsuka & Corter, 2003a,
                                                                  which in turn paid less attention to those dimensions as
2004); ALCOVE with a regularized stochastic learning
                                                                  compared with ARSL. Qualitatively, ARSL appears to be
(ARSL); and ALCOVE with the Rayleigh distribution-
                                                                  the most successful in replicating not paying attention to
based stochastic learning (ARAY). The standard ALCOVE
                                                                  both Dimension 1 and 2 equally, while ASL appears to be
will not be evaluated in the present simulation study,
                                                                  least successful in this regard. ARAY was similarly
because its standard gradient learning method was shown to
                                                                  unsuccessful, overestimating the proportion of people who
be unsuccessful in replicating individual difference when
                                                                  would attend to both of the correlated dimensions equally.
                                                                  A noticeable difference between ARAY and other two
                                                              918

models is that ARAY virtually ignored non-diagnostic               resulted in simpler configuration. Note that the model
feature dimensions and paid attention exclusively to either        configurations and settings for ASL and ARSL were the
or both Dimensions 1 and 2.                                        same expect for the regularization process incorporated in
   Among all three models, the proportion of sub-zero              ARSL. Thus, the straightforward comparison of ASL and
association weights for ARAY was the largest (Figure 2,            ARSL seems reasonable. However, because ARAY and
fourth column), indicating it yielded simpler category             ARSL had different parameter settings, interpreting the
conceptions than the other two models. Here, the notion of         comparisons of distributions of the weights for ARAY and
simplicity (or complexity) is directly related to numbers of       ARSL or ASL should be done with care.
effective (i.e., non-zero, or non-subzero) association weights        In sum, the stochastic learning model with the
and attention parameters. When compared with the                   regularizing processes penalizing mentally-expensive
distribution of the association weights of ASL, the                complex category conceptions (i.e. ARSL) appears to be the
proportion of sub-zero weights for ARSL was larger,                most successful model capturing human category learning
indicating penalizing processes incorporated in ARSL               trends that appeared biased, heuristic, and/or less optimal.
Figure 2. Results of the simulation study. Top row: Observed empirical results of Matsuka & Corter (2003b). The graphs on
the first column show observed and predicted classification accuracy, second column shows relative attention allocation for
the four feature dimensions; third column compares relative attention allocated to Dimensions 1 and 2 for the last four
blocks, where each dot represents an observation. Fourth column shows histograms for the final association weights. Second
row shows results of ALCOVE-SL; Third row, ALCOVE-RSL; Fourth Row, ALCOVE-RAY.
Discussion: Although there are 12 unique exemplars in the          learner, who utilizes a lesser amount of information. In
stimulus set, there are only four exemplars (one from each         contrast, ARSL predicts that people would utilize more than
category) needed for a perfect categorization. Then, one           necessary information. In terms of attention allocation, the
might wonder if people would utilize all the exemplars or          empirical results indicate that some people do try utilizing
not. The distribution of ARAY’s association weights may            irrelevant information, suggesting that ARSL is more
suggest that there are several “dead” or inactive exemplars        descriptive than other models. This suggests that people
whose association weights are all zero or near-zero, not           may not actively being biased, searching for simpler
being utilized for categorization. This characteristic along       concepts (i.e., Model 2). Rather it suggests that biases may
with not paying attention to irrelevant feature dimensions         be caused by the limited mental capacity, involuntarily
may suggest that ARAY replicates learning of an efficient          resulting in simpler concepts.
                                                               919

                         Discussion                                dimensions and having smaller numbers of effective
                                                                   association weights (proficient-like concepts).
RULEX vs. Stochastic Learning: The stochastic learning’s             Although the present study supports biased stochastic
take-all-or–none parameter updating strategy may be                learning’s descriptive validity, more comprehensive
considered as a type of hypothesis testing learning model,         simulation studies would be useful in evaluating the present
which makes it similar to the RULEX model (Nosofsky,               learning models.
Palmeri, & McKinley, 1994). However, its random search
method, interpreted as unstructured hypothesis generation                                  References
and search, is very distinct from RULEX whose hypothesis
generation algorithm is very strategic and well-structured.        Bettman, J.R., Johnson, E.J., Luce, M.F., Payne, J.W.
Thus, for Matsuka’s (2002) stimuli set, RULEX would                  (1993). Correlation, conflict, and Choice. Journal of
predict that everyone would allocate his/her attention               Experimental Psychology: Learning, Memory, and
                                                                     Cognition, 19, 931-951.
exclusively to the one of the two diagnostic dimensions.
                                                                   Cherkassky, V. & Mulier, F. (1997). Learning from data:
Whereas the stochastic learning would predict some paying
                                                                     Concepts, Theory, and Methods. New York: Wiley
attention to either one of the two dimensions, another             Hinton, G E., & Sejnowski, T. J. (1986). Learning and
paying attention to both, and others distributing attention in       relearning in Boltzmann machines. In D.E. Rumelhart &
some other combinations, since, as an exemplar-based                 J.L. McClelland (Eds.) Parallel distributed processing:
model, it can minimize classification error with several             Explorations in microstructure of cognition. Cambridge,
different attention allocation patterns (i.e., it can learn to       MA: MIT Press.
classify stimuli without “optimal” or “rational” attention         Ingber, L. (1998). Very fast simulated annealing. Journal of
distribution).     In other words, when there are several            Mathematical Modelling, 12: 967-973.
minima, which is probably true for real world category             Kruschke, J. E. (1992). ALCOVE: An exemplar-based
learning task, stochastic learning can result in several             connectionist model of category learning, Psychological
different learning trajectories and parameter (i.e.,                 Review, 99. 22-44.
association weight & attention allocation) configurations,         Kirkpatrick, S., Gelatt Jr., C. D., & Vecchi, M. P. (1983).
corresponding to possible individual differences.           In       Optimization by simulated annealing. Science, 220, 671-
contrast, RULEX would always predict that people pay                 680.
attention to the least number of dimensions, which may be a        Macho, S. (1997). Effect of relevance shifts in category
too normative prediction.                                            acquisition: A test of neural networks. Journal of
                                                                     Experimental Psychology: Learning, Memory, and
Gradient-type vs. Stochastic Learning: For two perfectly             Cognition, 23, 30-53.
redundant feature dimensions, a gradient-type learning             MathWorks. (2001).         MATLAB [Computer Software].
algorithm in general would allocate the same amounts of              Natick, MA: Author.
attention to the two dimensions, or its attention learning         Matsuka, T. (2002). Attention processes in computational
curves for the two dimensions would be parallel. In                  models of category learning. Unpublished doctoral
                                                                     dissertation. Columbia University, NY.
contrast, (biased) stochastic learning could result in
                                                                   Matsuka, T. (2003). Generalized exploratory model of
asymmetric attention allocation to the two dimensions, and
                                                                     human category learning. Accepted for publication.
its attention learning curves are not necessarily parallel. In     Matsuka, T & Corter, J. E. (2003a). Stochastic learning in
these regards, stochastic learning’s predictions appear more         neural network models of category learning. Proceedings
realistic than those of gradient-type learning. However, this        of the 25th Annual Meeting of the Cognitive Science
point alone does not necessarily indicate stochastic learning        Society.
is what people would do. Perhaps, a gradient-type learning         Matsuka, T. & Corter, J. E. (2003b). Empirical studies on
with some stochastic elements or errors might, as well,              attention processes in category learning. Poster presented
result in more “realistic” predictions.                              at 44th Annual Meeting of the Psychonomic Society.
                                                                     Vancouver, BC, Canada.
                         Conclusion                                Matsuka, T. & Corter, J.E (2004). Stochastic learning
Biased stochastic learning is a descriptive model of heuristic       algorithm for modeling human category learning.
learning that prefers a simpler conception of categories in          Accepted for publication.
which less mental effort seems to be needed. Although the          Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,
present two stochastic learning algorithms are intended to           Teller, A. H., & Teller, E. (1953). Equation of state
model such bias, the algorithms appear to be modeling two            calculations by fast computing machines. Journal of
different types of learners, namely “ordinary people” and            Chemical Physics, 21, 1087-1092.
“proficients”. The simulation study indicates that modeling        Nosofsky, R.M., Palmeri, T.J., & McKinley, S.C. (1994).
biased learning via parameter-configuration regularization           Rule-plus-exception model of classification learning.
was the most successful in replicating the empirical results         Psychological Review, 101, 53-79 .
(i.e., ordinary people). In contrast, biased learning via          Rehder, B. & Hoffman, A. B. (2003). Eyetracking and
asymmetric distributions appears to be more optimal or               selective attention in category learning. Proceedings of
rational model, paying attention to only diagnostic feature          the 25th Annual Meeting of the Cognitive Science Society,
                                                                     Boston, 2003.
                                                               920

