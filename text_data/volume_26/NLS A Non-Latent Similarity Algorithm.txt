UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
NLS: A Non-Latent Similarity Algorithm
Permalink
https://escholarship.org/uc/item/0sc5p977
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Cai, Zhinqiang
McNamara, Danielle S.
Louwerse, Max
et al.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    NLS: A Non-Latent Similarity Algorithm
                                               Zhiqiang Cai (zcai@memphis.edu)
                                     Danielle S. McNamara (dsmcnamr@memphis.edu)
                                          Max Louwerse (mlouwers@memphis.edu)
                                                Xiangen Hu (xhu@memphis.edu)
                                             Mike Rowe (mprowe@memphis.edu)
                                       Arthur C. Graesser (a-graesser@memphis.edu)
                         Department of Psychology/Institute for Intelligent Systems, 365 Innovation Drive
                                                     Memphis, TN 38152 USA
                             Abstract                                ways. However, here we used a method modified from Lin
                                                                     (1998). In the following sections, we describe the general
  This paper introduces a new algorithm for calculating              concept behind vector space models, describe the
  semantic similarity within and between texts. We refer to this     differences between the metrics examined, and present an
  algorithm as NLS, for Non-Latent Similarity. This algorithm
                                                                     evaluation of these metrics’ ability to predict word
  makes use of a second-order similarity matrix (SOM) based
  on the cosine of the vectors from a first-order (non-latent)       associates.
  matrix. This first-order matrix (FOM) could be generated in
  any number of ways; here we used a method modified from                              Vector Space Models
  Lin (1998). Our question regarded the ability of NLS to            The basic assumption behind vector space models is that
  predict word associations. We compared NLS to both Latent
                                                                     words that share similar contexts will have similar vector
  Semantic Analysis (LSA) and the FOM. Across two sets of
  norms, we found that LSA, NLS, and FOM were equally                representations. Since texts consist of words, similar words
  predictive of associates to modifiers and verbs. However, the      will form similar texts. Therefore, the meaning of a text is
  NLS and FOM algorithms better predicted associates to nouns        represented by the sum of the vectors corresponding to the
  than did LSA.                                                      words that form the text. Furthermore, the similarity of two
                                                                     texts can be measured by the cosine of the angle between
                         Introduction                                two vectors representing the two texts (see Figure 1).
Computationally determining the semantic similarity
between textual units (words, sentences, chapters, etc.) has           Corpus                              Word
become essential in a variety of applications, including web                                               Representation
searches and question answering systems. One specific
example is AutoTutor, an intelligent tutoring system in
which the meaning of a student answer is compared with the
meaning of an expert answer (Graesser, P. Wiemer-                      Text                                Text
Hastings, K. Wiemer-Hastings, Harter, Person, & the TRG,               Similarity                          Representation
2000). In another application, called Coh-Metrix, semantic
similarity is used to calculate the cohesion in text by                        Figure 1. From Corpus to Text Similarity.
determining the extent of overlap between sentences and
paragraphs (Graesser, McNamara, Louwerse & Cai, in                   The four items of Figure 1 can be described as follows.
press; McNamara, Louwerse, & Graesser, 2002).                        First, the corpus is the collection of words comprising the
  Semantic similarity measures can be classified into                target texts. Second, word representation is a matrix G used
Boolean systems, vector space models, and probabilistic              to represent all words. Each word is represented by a row
models (Baeza-Yates & Ribeiro-Neto, 1999; Manning &                  vector g of the matrix G. Each column of G is considered a
Schütze, 2002). This paper focuses on vector space models.           “feature”. However, it is not always clear what these
Our specific goal is to compare Latent Semantic Analysis             features are. Third, text representation is the vector v = GTa
(LSA, Landauer & Dumais, 1997) to an alternative                     representing a given text, where each entry of a is the
algorithm called Non-Latent Similarity (NLS). This NLS               number of occurrences of the corresponding word in the
algorithm makes use of a second-order similarity matrix              text. Fourth, text similarity is represented by a cosine value
(SOM). Essentially, a SOM is created using the cosine of             between two vectors.
the vectors from a first-order (non-latent) matrix. This first-         More specifically, Equation 1 can be used to measure the
order matrix (FOM) could be generated in any number of               similarity between two texts represented by a and b,
                                                                 180

respectively. For reasons of clarity, we do not include word           (1) For each word base, form a feature vector.
weighting in this formula.                                             (2) For each pair of word bases, find the similarity of
                                 a T GG T b                                   two word bases from the corresponding two feature
         sim(a, b) =                                        (1)               vectors.
                           a T GG T a b T GG T b                    In Lin’s algorithm, the similarity is calculated according to
                                                                    Equation 2.
Latent Semantic Analysis (LSA)                                                                          2 × I ( F ( w1 ) ∩ F ( w2 ))
                                                                                       sim( w1 , w2 ) =                               (2)
LSA is one type of vector-space model that is used to                                                   I ( F ( w1 )) + I ( F ( w2 ))
represent world knowledge (Landauer & Dumais, 1997).                F(w) is the set of features possessed by the word w and I(F)
LSA extracts quantitative information about the co-                 is the “information” contained in the feature set F: I(F) =
occurrences of words in documents (paragraphs and
sentences) and translates this into an N-dimensional space.          ∑   f ∈F
                                                                              u ( f ) . u is the weight function of the feature f.
The input of LSA is a large co-occurrence matrix that
specifies the frequency of each word in a document. Using           First-Order Matrix LSA is referred to as latent because
singular value decomposition (SVD), LSA maps each                   the content is not explicit or extractable after SVD. Thus,
document and word into a lower dimensional space. In this           the features that two similar words share are “latent.” In
way, the extremely large co-occurrence matrix is typically          contrast, every feature is explicit and directly extractable
reduced to about 300 dimensions. Each word then becomes             from the matrix using Lin’s (1998) algorithm. Hence, it is
a weighted vector on K dimensions. The semantic                     non-latent, and can be used as a first-order similarity matrix
relationship between words can be estimated by taking the           (FOM).
cosine between two vectors. This algorithm can be briefly              We created the FOM using a modification of Lin’s
described as follows.                                               algorithm with cosines rather than proportions. First, we
                                                                    parsed all of the sentences (about 2 million) in the TASA
  (1) Find the word-document occurrence matrix A from a             corpus using Lin’s MINIPAR parser (Lin, 1998). This
      corpus1.                                                      provided about 9 million word-relation-word triplets. Table
  (2) Apply SVD: A = UΣV T .                                        1 shows the triplets extracted for the sentence People did
  (3) Take the row vectors of the matrix U as the vector            live in Asia, however.
      representations of words.
                                                                        Table 1: An example with word-relation-word triplets.
Non-Latent Similarity (NLS) Model
                                                                    Word1                        Relation                    Word2
NLS is proposed here as an alternative to latent similarity
                                                                    Live                         V:s:N                       people
models such as LSA. NLS relies on a first order, non-latent
matrix that represents the non-latent associations between          Live                         V:aux:Aux                   do
                                                                    Live                         V:mod:Prep                  in
words. The similarity between words (and documents) is
calculated based on a second-order matrix. The second               In                           Prep:pcomp-n:N              Asia
                                                                    Live                         V:mod:A                     however
order matrix is created from the cosines between the vectors
for each word drawn from the FOM. Hence, for NLS, the
cosines are calculated based on the non-latent similarities            A “feature” consists of a word (e.g., Word1 or Word2)
between the words, whereas for LSA, the similarities are            and a relation that contains a verb (V), noun (N), or modifier
based on the cosines between the latent vector                      (A). For example, the association between the word live
representations of the words. The following section                 and its relation to people, which is “V:s:N”, comprises two
describes the components and algorithms used in NLS.                features (live - V:s:N; people - V:s:N). About 400,000 such
                                                                    features were obtained. Each feature was assigned a weight,
Lin’s (1998) Algorithm Our starting point for NLS is Lin’s          using Lin’s formula. We adopted an occurrence frequency
(1998) algorithm for extracting the similarity of words.            threshold, which yielded 10363 nouns (occurrence > 50),
Similarity is based upon the syntactic roles words play in          5687 verbs (occurrence > 5), and 6890 modifiers
the corpus. A syntactic role is designated here as a feature.       (occurrence > 10). For each of the selected words, a feature
For example, “the Modifier of the NP man” is a feature. A           vector was formed according to the features it involved.
word has this feature if and only if it is used as the modifier        We modified Lin’s method in the last step. Specifically,
of man when man is part of an NP in the corpus. For                 rather than applying Equation 2 to the feature vectors, the
example, if the corpus contains the phrase the rich man,            cosine between any two feature vectors was calculated. This
then rich has the (adjectival) feature of modifying man.            provided a FOM containing the similarity between all word
Each feature is assigned a weight to indicate the feature’s         pairs. In addition, the FOM guarantees a property called
importance. This algorithm is briefly described as follows.         “decomposability”, which will be addressed in the next
                                                                    section.
1
  Hu et al. (2003, theorem 2) proved that the LSA similarity
measure is a special case of (1)
                                                                181

Non-Latent Similarity (NLS) Algorithm The logic behind                       The decomposability therefore raises a new question: Is
the use of a second order matrix to represent textual                     there a straightforward way to guarantee both
similarity relies on a reformulation of the algorithm used in             decomposability and validity of the similarity matrix S? An
general vector models. Specifically, Equation 1 can be                    easy way of guaranteeing these criteria is by using a word
rewritten as Equation 3.                                                  similarity matrix to act as a word representation matrix.
                                            a T Sb                           Suppose S is a word similarity matrix regardless of its
                        sim(a, b) =                               (3)     creation method. Then each column vector in S contains the
                                        a T Sa b T Sb                     similarities of a particular word to all other words.
When the columns of G are normalized to be unit vectors, S                Therefore, each column vector can represent the
becomes a word-similarity matrix 2. In other words, each                  corresponding word.
entry of S, sij = g i g j , is the similarity of two words
                         T
                                                                                  Table 2. A small section of a first order matrix.
represented by row vectors gi and gj, respectively.
Essentially, a word-similarity matrix (S) is used rather than                                   chair           table           strength
word representation vectors (G).                                            desk                 0.16            0.17                  0
   From Equation 3 we can see that the similarity of two                    bed                  0.14            0.13                  0
texts is determined by two factors: the word occurrences in                 speed                   0               0               0.14
each text and the similarity between words. Since we can                    success                 0               0               0.11
do little to the occurrence vectors (other than applying word
weighting), the word similarity matrix will determine the                    Table 2 is a small section of our FOM. It can be seen that
validity of the measure of text similarity. In other words,               the column vectors for chair and table are very similar to
Equation 3 provides a good measure if and only if similar                 each other, but quite different from that of “strength”. In the
words have similar vector representations. If similar words               complete matrix, desk is the 4th nearest neighbor of (i.e.,
have dissimilar vector representations or dissimilar words                most similar to) chair and the 1st nearest neighbor of table.
have similar representations, then the measure provided by                In addition, bed is the 2nd nearest neighbor of chair and the
Equation 3 is unreliable. Therefore, the verification of the              5th       nearest       neighbor          of        table       (see
validity of the word representation, at least in terms of text            http://cohmetrix.memphis.edu/wordsim/wf1.aspx).
similarity comparison, is equivalent to the verification of the              If we believe that similar words should share most nearest
validity of the word similarity matrix (or FOM in this case).             neighbors (a group of words that are most similar to a given
   While it is not possible to directly judge the quality of a            word), then similar words should have similar column
vector representation, it is possible to judge the validity of            vectors in S. Therefore, we can create a new word similarity
word similarity. Provisions for such a judgment will be                   matrix by the cosine between the column vectors of S,
made in the next section of this paper.                                    ~
                                                                           S = D T S T SD , where D is a diagonal matrix formed by the
   Equation 3 raises an important question: Instead of
                                                                          reciprocal of the norms of the column vectors of S. We call
creating the similarity matrix S by the word representation                ~
matrix G, can we find the similarity matrix by any other                   S the second-order word similarity matrix (SOM) and S the
                                                                                                                                         ~
method that provides a better word similarity measure? One                first-order similarity matrix (FOM). This new matrix S is
of the conditions under which this question may be                        obviously decomposable and should maintain the validity of
answered is that the similarity matrix S, no matter how it is             the original word similarity matrix.
created, must be decomposable. That is, there exists a                       If the SOM is valid, then we can form a measure based on
matrix G (we do not have to find it) such that S = GGT. This              the FOM:
condition is necessary to guarantee that the value calculated                                                  a T D T S T SDb
from Equation 3 ranges from -1 to 1.                                                    sim(a, b) =                                        (4)
   The FOM that we generated by the modified Lin’s                                                    a T D T S T SDa b T D T S T SDb
method is decomposable and can therefore be used in
Equation 3 for text comparison. However, that matrix is                      Corpus          Word Similarity                    Text
high-dimensional (N by N, where N is the total number of                                                                        Similarity
words). This will cause some computational complexity.
To reduce the number of dimensions, we kept only the 400                         Figure 2. From Corpus to Text Similarity (SOM).
largest similarity values for each word and set the other
smaller values to be zero. Thus, the similarity matrix                    Equation 4 provides a new algorithm for text comparison,
became sparse and the computational complexity was                        which relies solely on the similarity matrix. We call this
reduced. However, this made the similarity matrix un-                     algorithm the Non-Latent Similarity (NLS) algorithm,
decomposable and invalid for Equation 3.                                  assuming that the FOM is non-latent. Figure 2 shows the
                                                                          difference between NLS and the general vector-space
                                                                          model. When compared with Figure 1, we can see that the
2
  The normalization guarantees that the similarity between any two        “representations” are replaced by the similarity matrix.
words will not exceed the similarity of a word to itself and that the
values are in a known range [-1,1].
                                                                      182

                        Evaluation                                 similarities are extracted not only from the similar semantic
                                                                   context but also from the similar syntactic roles that the
In this section, we compare NLS to LSA to examine the
                                                                   words play. That is, the FOM includes syntactic relations as
differences between the latent analytic method exemplified
                                                                   features, whereas word order and the relations between
by LSA and the non-latent method of NLS. We examine
                                                                   words are ignored in LSA. Thus, we expected LSA to be
the validity of these two methods by examining their
                                                                   less successful in identifying the associates of nouns as
ability to predict word associates obtained from two
                                                                   compared to modifiers and verbs. We did not expect this
sources of free association norms. We also examine the
                                                                   factor to affect the performance of NLS. We expected that
ability of the FOM to predict these word associates. The
                                                                   FOM and NLS would be sensitive to both context based and
ability of FOM and NLS to predict word associates should
                                                                   non-context based associations.
be reflective of the overall validity of NLS to predict
                                                                      To examine these factors, we randomly selected 135
similarity of text corpora, which is crucial to our new
                                                                   common words, composed of 45 modifiers (including
algorithm shown in Equation 4.
                                                                   adjectives and adverbs), 45 nouns, and 45 verbs. We then
   We have two concerns. First, is our FOM valid? Second,
                                                                   determined the first most commonly listed and the second
if our FOM is valid, then will the second order similarity
                                                                   most commonly listed associate to those words, based on
matrix (SOM) be valid as well? To answer these questions,
                                                                   the association norms provided by EAT and the USFFAN.
we compared the validity of the following three similarity
                                                                   Finally, we determined whether each of the three similarity
matrices generated by three different methods.
                                                                   metrics listed the first and second most commonly listed
   - LSA: The similarity matrix created from TASA
                                                                   associate from the respective norm database. A criterion
        corpus by LSA.
                                                                   was set in the following analyses: A metric identifies an
   - FOM: The similarity matrix created from TASA
                                                                   associate of a word if, according to the metric, the associate
        corpus using the modified version of Lin’s method.
                                                                   is among the top five nearest neighbors of the word. While
   - NLS: The second order similarity matrix based on
                                                                   not extremely strict, the cutoff was intended to be relatively
        the above FOM.
                                                                   conservative compared to setting the cutoff at 20 words.
   Our overall question addressed the ability of the three
similarity metrics (LSA, FOM, and NLS) to correctly list
                                                                   Results
word associates. We were also interested in examining how
this ability varied as a function of several variables. First,     Table 3 shows the proportion associates identified by each
we were interested in whether the results remained stable          metric. A 3 x 2 x 2 analysis of variance (ANOVA) was
across norming databases. We chose to use two sets of free         conducted that included the between-words variable of word
association norms: the Edinburgh Associative Thesaurus             type (noun, verb, adjectival/adverbial modifier) and the
(EAT; Kiss, Armstrong, Milroy, & Piper, 1973) and the              within-words variables of associate (first, second) and
University of South Florida Free Association Norms                 database (EAT, USFFAN).
(USFFAN; Nelson, McEvoy, & Schreiber, 1998).
   We were also interested in how the results differed across         Table 3: Proportion of correctly identified associates
word types (i.e., nouns, verbs, vs. adjectival/adverbial           listed in the top five nearest neighbors provided by LSA,
modifiers). One difference between the three classes of            FOM, and NLS as a function of the free association norms
words is the amount of semantic contextualization.                 and word types.
Specifically, the meaning of verbs and modifiers is usually
context dependent, whereas the meaning of nouns is less                                   EAT                  USFFAN
dependent on the context (e. g., Graesser, Hopkinson &                            Mod     Noun    Verb    Mod    Noun Verb
Schmid, 1987). For example, in the phrase a big house, the       Associate 1
size of the adjectival modifier big depends on the noun                   LSA     0.40    0.11    0.16    0.31    0.07     0.13
house. It could be argued, moreover, that words that are
                                                                         FOM      0.40    0.36    0.13    0.31    0.31     0.16
more concrete are less context-dependent. Adjectives are
less concrete than nouns so they would be more context-                   NLS     0.38    0.36    0.11    0.27    0.36     0.13
dependent. A similar argument could be made for verbs,           Associate 2
which are more context dependent than nouns.                              LSA     0.07    0.04    0.09    0.13    0.04     0.18
    We expected the context-dependency factor to most                    FOM      0.18    0.11    0.11    0.16    0.16     0.11
affect the performance of LSA, because the success of LSA                 NLS     0.16    0.11    0.11    0.13    0.13     0.16
relies heavily on the occurrence of words in similar
contexts, and essentially taps into that factor to assess word
                                                                      There was a main effect of word type, F(2, 132) = 3.4,
similarity. The basic assumption behind LSA is that words
                                                                   MSE = .471, p < .05. Bonferroni Means tests indicated that
used in similar context have similar representations. Thus,
                                                                   the proportion of associates identified for modifiers (M =
if a word is less context-dependent, LSA may be less able to
                                                                   .243) was significantly greater than for verbs (M = .122),
tap into associations.
                                                                   but not significantly greater than for nouns (M = .187).
   While NLS similarly uses semantic context to compute
                                                                   There was an effect of associate, F(1, 131) = 19.5, MSE =
similarity, it also uses syntactic context. The word
                                                                   .330, p<.001, reflecting a greater proportion of first
                                                               183

associates identified (M = .250) than second associates (M =                              ways. We used a modified form of Lin’s (1998) algorithm
0.120). There was also an interaction between word type                                   to extract non-latent word similarity from corpora. Our
and associate, F(2, 131) = 4.2, p < .05. This interaction                                 evaluation of NLS compared its ability to predict word
reflected an effect of word type for first associates, F(2,132)                           associates to the predictions made by the FOM and LSA.
= 5.5, MSE = .533, p<.01 (Mmodifier = .34 Mnoun = .26 Mverb =                             The critical difference between the algorithms addressed the
.14), compared to no differences between word types for                                   latency of the word representations. The use of SVD results
second associates, F<1, (Mmodifier =.14, Mnoun = .11, Mverb =                             in latent word representations in LSA, whereas the use of
.12). Thus, the metrics were unable to identify the second                                the syntax parser in NLS results in a non-latent
associates, regardless of word type.                                                      representation. We found that NLS, using the similarity
  Finally, there was significant effect of similarity metric,                             matrix that we generated, identified the associates to
F(2,264) = 4.6, MSE = .139, p <.05, and an interaction                                    modifiers and nouns relatively well. Both LSA and NLS
between metric and word type, F(4,264) = 4.1, p<.01. This                                 were equally able to identify the associations to the
interaction is depicted in Figure 3. The interaction reflects                             modifiers. In contrast, none of the metrics successfully
the finding that the three metrics were equally successful in                             identified the associates to the verbs.
identifying the associates to modifiers and verbs, whereas
                                                                                          FOM versus NLS
FOM and NLS were significantly more successful in
identifying the associates to nouns than was LSA, F(2,88)=                                There were two motivations for examining the results from
4.1, MSE = .052, p < .05.                                                                 the FOM as well as NLS. The first was to examine the
                                                                                          validity of using a FOM. The second was to examine the
                                                                                          correspondence in results between FOM and NLS. That is,
                                       0.30                                               if the FOM is valid, is the SOM valid as well? We found
  Proportion of Associates Identifed
                                       0.25
                                                                                          that NLS and FOM were equally successful in identifying
                                                                                          all types of associations. This result indicates that SOM
                                       0.20
                                                                              LSA
                                                                                          maintains the validity of FOM. The result supports the
                                       0.15                                   FOM
                                                                                          validity of using the NLS algorithm.
                                                                              NLS
                                                                                             One consideration is that the second order similarity
                                       0.10                                               matrix may reveal new similarity relations which do not
                                       0.05
                                                                                          exist or are weak in the FOM. It is not hard to imagine that
                                                                                          two words that have weak similarity in FOM may share
                                       0.00                                               some nearest neighbors and thus reveal a stronger relation
                                              Modifier       Noun      Verb               between the two words in SOM. Nonetheless, we found
                                                                                          here that the second-order matrix maintains the validity of
Figure 3. Proportion of associates identified (in the top 5 of                            FOM as much as possible, assuming the FOM is valid.
the list) by the three similarity metrics.                                                When the FOM is decomposable, it can be directly used in
                                                                                          NLS. The SOM is used when FOM is computationally
  These results did not depend on where the cutoff was                                    heavy or is not decomposable. Our future investigations will
drawn, (e.g., top 5 vs. top 20). Of course, the means                                     work toward a better understanding of the situations that
increased with a more lax cutoff. For example, the overall                                require a SOM as opposed to a FOM, or vice versa.
accuracy of associate identification for LSA increased from
20% to 28% when the cutoff was set at 20 (i.e., when 20 of                                LSA versus NLS and FOM
the words output by LSA were considered). Similarly, the                                  We confirmed our predicted results that LSA would be less
overall accuracy for NLS increased from 27% to 42% when                                   accurate in identifying the associates to context-independent
the cutoff was set at 20 words. Thus, there was a 140% and                                nouns than to adjectival or adverbial modifiers, which have
157% increase respectively for LSA and NLS. The results                                   greater context dependency. We further predicted that this
also remained the same when word frequency was entered                                    difference would not occur for NLS and the FOM. Indeed,
as a covariate. Essentially, these trends emerged regardless                              NLS and FOM were equally predictive of noun and
of how we examined the data.                                                              modifier associates. Thus, one advantage of NLS is that it
  There were no differences as a function of norming                                      makes use of both semantic and syntactic information
database. This indicates that the results we have reported                                within the text corpora. Specifically, the FOM includes
should remain stable across norming databases.                                            both syntactic and semantic relations as features. Here, we
                                                                                          have documented this advantage solely with respect to word
                                                         Conclusions                      similarities. However, we expect that this advantage will
In summary, we have provided an alternative algorithm,                                    also improve the detection of similarity across larger bodies
NLS, which makes it possible to use any non-latent                                        of text.
similarity matrix to compare text similarity. This algorithm                              Verbs versus Nouns and Modifiers
uses a second-order similarity matrix (SOM) that is created
using the cosine of the vectors from a first-order (non-latent)                           One result that has baffled us is why NLS and LSA are both
matrix. This FOM could be generated in any number of                                      unable to pick up on the associates to the verbs. We
                                                                                    184

considered several explanations. First, one might think that       Education Sciences (IES R3056020018-02). Any opinions,
the number of forms of the word would be a factor to               findings, and conclusions or recommendations expressed in
consider. Since verbs tend to have more forms than do              this material are those of the authors and do not necessarily
modifiers (e.g., add has four forms: add, added, adding,           reflect the views of the DOD, NSF or IES. The TASA
adds), a typical vector space model would contain relatively       corpus used was generously provided by Touchtone Applied
less information about any one form of the verb. This factor       Science Associates, Newburg, NY, who developed it for
may explain the inability of LSA to identify the associates        data on which to base their Educators Word Frequency
to verbs. However, it cannot do so for NLS because we              Guide.
used the word base, not the word itself, when forming the
matrix.                                                                                     References
  We further considered that humans may have produced a            Baeza-Yates, R., Ribeiro-Neto, B. (Eds.) (1999). Modern
greater variety of associates to verbs than to nouns or               Information Retrieval. New York , ACM Press.
modifiers. If so, then across the two databases (i.e., EAT         Graesser, A. C., Hopkinson, P. L. & Schmid, C. (1987).
and USFFAN), the match between the associates in one                  Differences in interconcept organization between nouns
database to another should vary as a function of word type.           and verbs. Journal of Memory and Language, 26, 242-
However, this was not the case. The two databases matched             253.
the first associate for 69% of the words, with no differences      Graesser, A.C., McNamara, D.S., Louwerse, M.M., & Cai,
across word types. There was lower agreement (40%) and                Z. (in press). Coh-Metrix: Analysis of text on cohesion
greater variance for the second associate, but not in the             and language. Behavioral Research Methods, Instruments,
expected direction.                                                   and Computers.
  An alternative explanation regards the contextualization         Graesser, A.C., Wiemer-Hastings, P., Wiemer-Hastings, K.,
of verbs as compared to nouns. As we stated earlier, the                Harter, D., Person, N., & the TRG (2000). Using latent
meaning of verbs is more dependent on semantic context                  semantic analysis to evaluate the contributions of
than are nouns. In addition, verbs seem to be used in a                 students in AutoTutor.            Interactive Learning
wider variety of contexts. Whereas a person can do only so              Environments, 8, 129-148.
much with a chair, the person can sit just about anywhere          Hu, X., Cai, Z., Franceschetti, D., Penumatsa, P., Graesser,
and anyhow. One can imagine eating, walking, and                      A.C., Louwerse, M.M., McNamara, D.S., & TRG (2003).
thinking in any number of contexts, whereas the contexts for          LSA: The first dimension and dimensional weighting.
chairs and cars are more constrained. Hence, semantic                 Proceedings of the 25th Annual Conference of the
context is more variable for verbs than for nouns. This               Cognitive Science Society (pp. 587-592). Boston, MA:
variability may render models such as NLS or LSA unable               Cognitive Science Society.
to determine the ‘meaning’ of verbs.                               Kiss, G.R., Armstrong, C., Milroy, R. & Piper, J. (1973). An
  This idea is in line with notions of how verbs are                  associative thesaurus of English and its computer
represented with semantic representations. Generally, verbs           analysis. In A.J. Aitkin, R.W. Bailey & N. Hamilton-
are treated as the links between the concepts. Verbs                  Smith (Eds.), The computer and literary studies.
constitute the relations or links between nodes. Essentially,         Edinburgh : University Press.
we see here that vector space models are less able to abstract     Landauer, T. K., & Dumais, S. T. (1997). A solution to
meanings of relations than the meanings of concepts.                  Plato’s problem: The Latent Semantic Analysis.
  This notion gains clarity when we examine the associates            Psychological Review, 104, 211-240.
to verbs that were provided by LSA and NLS. The EAT                Lin, D. (1998). An information-theoretic definition of
associates to try are attempt and again. LSA’s top five               similarity. Proceedings of International Conference on
predictions were do, if, you, can, and way. FOM’s                     Machine Learning (pp. 296-304), Madison, Wisconsin.
predictions were think, say, go, know, and ask. We can             Manning, C.D. & Schütze, H. (1999). Foundations of
provide many examples such as these where the associates              statistical natural language processing. Cambridge, MA:
produced by the metric make little sense. The associations            MIT press.
predicted for nouns and modifiers, in contrast, showed             McNamara, D.S., Louwerse, M.M. & Graesser, A.C.
obvious relationships to the target word. This observation            (2002). Coh-Metrix: Automated cohesion and coherence
leads us to conclude that these metrics are not able to use           scores to predict text readability and facilitate
contextual information of verbs, perhaps because that                 comprehension. Technical report, Institute for Intelligent
information is not available.                                         Systems, University of Memphis, Memphis, TN.
                                                                   Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (1998).
                    Acknowledgments                                   The University of South Florida word association, rhyme,
The research was supported by grants from DoD                         and word fragment norms.
Multidisciplinary University Research Initiative (MURI)               http://www.usf.edu/FreeAssociation/.
program administered by the Office of Naval Research
(N00014-00-1-0600), National Science Foundation (SBR
9720314, REC0106965, ITR0325428), and the Institute of
                                                               185

