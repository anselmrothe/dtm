UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Processing Ambiguous Words: Are Blends Necessary for Lexical Decision?
Permalink
https://escholarship.org/uc/item/9bn3k2rr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Medler, David A.
Piercey, C. Darren
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

           Processing Ambiguous Words: Are Blends Necessary for Lexical Decision?
                                                David A. Medler (dmedler@mcw.edu)
                                                        Language Imaging Laboratory
                                          Department of Neurology, Medical College of Wisconsin
                                                                Milwaukee, WI
                                                 C. Darren Piercey (piercey@unb.ca)
                                          Department of Psychology, University of New Brunswick
                                                                Fredericton, NB
                                Abstract                                correspond to one pronunciation, which would correspond
                                                                        to one meaning. Unfortunately, one-to-one mappings are far
       A previous computational model (Joordens & Besner,               from the norm in English. That is, words that sound the
  1994) has suggested that during lexical access, ambiguous             same (homophones) can have different semantic
  words tend toward a blend state; that is, network activations         representations (/flaI/: fly [insect]; fly [zipper]), different
  settle into an incorrect state that is a mixture of the multiple
  representations of the ambiguous item. It has been suggested
                                                                        orthographic representations (/laIt/; light [fewer calories];
  that this blend state actually aids lexical decision (LD) for         lite [fewer calories]), or different semantic and orthographic
  ambiguous items as the blend state creates a larger “feeling of       representations (/be/: bear[furry animal]; bare[naked]).
  familiarity” which lexical decision may exploit. This theory,         Similarly, words that are spelled the same (have the same
  however, is based on the results of a computational model (a          orthographic representation) can have different phonological
  simple Hopfield network) in which multiple representations            representations (either: /aI.D/ [one or the other], /ID/
  cannot be learned. Here we use a Symmetric Diffusion
                                                                        [one or the other]), or phonological and semantic
  Network (SDN) to effectively learn and retrieve multiple
  mappings for a single input (i.e., ambiguous items). The              representations (wind: /waInd/ [twist]; /wInd/ [moving air]).
  model consists of three main processing regions–                           In fact, many words in English have polysemous or
  orthographics, phonology, and semantics–and is trained on a           ambiguous semantics. For example, WordNet® (Fellbaum,
  corpus of unambiguous items and ambiguous items that range            1998) lists a total of 146,350 noun, verbs, adjectives, and
  in their degree of balance (probability distribution) between         adverbs. Table 1 shows the percentage of unique and
  the multiple meanings. Following training, the SDN is able to         ambiguous words, as well as sense data. Whereas ambiguity
  reproduce the correct probability distributions for the               is often defined as a word having multiple meanings across
  ambiguous items; that is, it does not produce blend states.           semantic categories or word classes, a word’s sense is
  Furthermore, the model qualitatively captures the processing          defined as it’s meaning within a semantic category and can
  advantage for ambiguous items. Consequently, the notion of a
  blend state being used for LD is re-evaluated, and further
                                                                        vary dramatically from the prior definition of ambiguity. For
  assumptions about semantic processing are explored.                   example, although Borowsky & Masson (1996) consider
                                                                        “deep” to be an unambiguous word, WordNet lists “deep”
                           Introduction                                 with 3 noun senses, 15 adjective senses, and 3 adverb
                                                                        senses. It is clear that ambiguity is prevalent in English, and
                                                                        there is evidence that it has an effect on how we process
From a computational perspective, we can break basic
                                                                        words.
language processing into three main components: the
semantic representation (what a word means), a
                                                                             Table 1. Percentage of words having unique, ambiguous,
phonological representation (the sound of a word), and an
                                                                        and multisense meanings.
orthographic representation (the written form of a word)
(e.g., Seidenberg & McClelland, 1989; Plaut, McClelland,
                                                                           Word Class        Unique        Ambiguous        Senses
Seidenberg, & Patterson, 1996; Harm & Seidenberg, 1999).
The relationship between the phonological and semantic                         Noun            86.7            13.3           29.7
representations is initially established in early childhood,                   Verb            53.4            46.6           75.4
and then the mapping between the orthographic                                  Adjective       74.5            25.5           48.7
representation and the phonological representation (spelling-                  Adverb          82.9            17.1           33.1
to-sound conversion) along with the mapping between the
orthographic representation and the semantic representation
                                                                             For example, the behavioral data from word ambiguity
(spelling-to-meaning conversion) is learned later in life
                                                                        studies produces a paradox. In a lexical decision (LD)
(e.g.,Harm & Seidenberg, in press).
                                                                        paradigm, ambiguity aids in word identification; that is,
    Ideally, there would be a one-to-one mapping between
                                                                        ambiguous words are identified as words more quickly and
any of the representations, such that one spelling would
                                                                        more accurately than unambiguous words (Gernsbacher,
                                                                    944

1984; Borowsky & Masson, 1996).                In contrast, in      also settled into blend states in the meaning units (Kello,
connected text studies (Rayner & Duffy, 1986; Rayner &              2003, Personal Communication).
Duffy, 1987; Duffy, Morris, & Rayner, 1988), ambiguous                   Although both of these models produced an ambiguity
words are processed more slowly than unambiguous words.             advantage (albeit in different processing regions), the
In other words, when semantic decisions (SD) (decisions on          networks failed to differentiate between the ambiguous items
word meaning) are required, words with multiple meanings            and produced blended representations. However, in later
pose more difficulty than words with single meanings. This          commentaries (Masson & Borowsky, 1995; Rueckl, 1995;
ambiguity paradox was illustrated in a single experiment in         Besner & Joordens, 1995), it was concluded that it may be
which participants first made a lexical decision, and then          possible for lexical decisions to be made prior to the
had to make a relatedness judgement on a subsequently               network settling into these blend states. In other words,
presented word (Piercey & Joordens, 2000). In this study,           correct lexical decisions could be based on the “blend”
participants showed an ambiguity advantage for lexical              states for ambiguous words resulting in a greater feeling of
decision, and an ambiguity disadvantage on the subsequent           familiarity which could then be used to produce LD.
relatedness decision. The importance of the ambiguity                    Using the model of Joordens and Besner (1994) as a
paradox lies in the fact that it leads directly to the question     basis for their theory, Piercey and Joordens (2000)
of how words are represented in the brain, and how we get           developed the “efficient then inefficient” hypothesis for the
access to these words. Any model of language will have to           processing of ambiguous words. They concluded that a
account for the ambiguity paradox if it is to be successful.        lexical decision is made based on early processing and that a
    Previous models of the ambiguity advantage in LD,               blend state (i.e., when all meanings of a word are
however, have shown mixed results. For example, Joordens            simultaneously activated) produces an advantage for lexical
& Besner (1994) trained a two layer Hopfield network                decision but a disadvantage for the relatedness decision.
consisting of 125 binary nodes (75 perceptual nodes and 50          That is, lexical decisions are made based on a feeling of
conceptual nodes; activations of either +1 or -1). The              familiarity that occurs during the early stages of processing,
perceptual nodes represented perceptual features and were           before a complete representation of the current item forms
never updated during retrieval (that is, they were clamped to       (i.e., efficient processing). Therefore, these decisions could
a specific pattern). The conceptual nodes represented               be made regardless of an eventual blend state. However,
semantics, and the network was effectively fully connected.         when the participants need to determine which meaning of
Learning was via a Hebbian learning algorithm.                      the word is appropriate to a particular context, processing
    They had two criteria for deciding if a PDP model could         slows down. The participant continues to process the
successfully account for the ambiguity effect; (a) the              ambiguous word and each of the word’s meanings compete
network had to retrieve one of the semantic patterns                with each other. That is, the participant needs to leave the
associated with the ambiguous words, and (b) the network            blend state and choose a meaning for the item so that further
had to retrieve ambiguous words faster than unambiguous             semantic processing can occur. This disambiguation of the
words. Joordens and Besner (1994) were able to produce an           blend state is an inefficient process that unambiguous words
ambiguity advantage within the conceptual nodes of their            do not share. It should be noted that this theory is based
network when it into a stable pattern. This only occurred,          specifically on the fact that the model of Joordens and
however, when the network was relatively small and when             Besner (1994) produced blended states for ambiguous
the ambiguous meanings had equal probability. Most of the           words.
time (over 50% of the trials), their networks failed to settle           In this paper, we readdress the ambiguity advantage for
into a correct pattern and formed a “blend” of the two              lexical decision using a computational model that is able to
learned meanings of the words over the conceptual units.            learn multiple mappings for a single input. These models do
Their initial conclusion from these simulations was that            not produce blend states; therefore, if the ambiguity
distributed models trained with Hebbian learning rule may           advantage can be reproduced, then the notion of blend states
not be suitable for capturing ambiguity effect.                     existing should be questioned.
    In a different computational model, Kawamoto, Farrar &
Kello (1994) trained a recurrent neural network with the                          Symmetric Diffusion Networks
Least Mean Square learning algorithm. Their model                   Symmetric Diffusion Networks (SDNs) are a class of
contained both “spelling” nodes and “meaning” nodes using           computational models based upon the principles of
a distributed representational coding scheme. During recall,        continuous, stochastic, adaptive, and interactive processing
the “spelling” nodes were given environmental activation,           (Movellan & McClelland, 1993). From a computational
and the network was allowed to settle into a stable state.          perspective, SDNs can be viewed as a continuous version of
They found that they could produce an ambiguity advantage           the Boltzmann machine; that is, time is intrinsic to the
within the units representing “spelling”, but showed the            dynamics of the network. Furthermore, SDNs embody
opposite effect in units representing “meaning” (an                 Bayesian principles in that they develop internal
ambiguity disadvantage in semantics?). It has been                  representations based upon the statistics of the environment.
suggested, however, that Kawamoto et al.’s (1994) network           One of the main advantages of SDNs is that they are able to
                                                                945

learn multiple mappings for a single concept, something
previous models often have difficulties with. In other                                            Semantics
words, SDNs are able to learn ambiguous mappings.
    Recent work (Medler & McClelland, 2001) has shown
that when biologically inspired constraints (i.e, activations
within the range [0,1], positive between layer projections,
lateral inhibition) are applied to SDN’s, their effective                        Phonology                         Orthograph
                                                                                                                       y
performance is increased substantially in terms of the
number of patterns they can be trained on, the rate at which            Figure 1. Network architecture showing the three main
patterns are learned, and their ability to separate out                 processing layers and connecting hidden layers.
independent sources in an unsupervised manner.
                                                                       which adjusts weights only after all patterns have been
    Network Dynamics and Learning                                      presented (Movellan & McClelland, 1993).
Network dynamics are based upon continuous activations
that develop over time, and are governed by the following                  Network Architecture, Stimuli, & Training
equation:                                                              In keeping with previous models of language (e.g., Harm &
                                                                       Seidenberg, 1999), the network consisted of three main
     ai (t )  t[neti (t )  neˆti (t )]    t Z i (t ) Eq. 1     processing layers: an “orthographic”, a “phonological”, and
where,                                                                 a “semantic processing” layer. To capture the gross
is the summed activation of all the activities coming into the         relationship between semantics and the orthographic and
unitincluding its biaspassed through a squashing function            phonology representation of words, there were twice as
                                                                       many units (10) in the semantic layer as in the orthographic
         neti  h( j 1 a j wij )
                         n
                                                                       and the phonology layers (5 units each). Each layer was
                                                                       connected to the other via a set of hidden layers (5 units).
such as the logistic, h(u) = 1-exp(-u), and                            Between layer connections were excitatory, while within
         neˆt i  1/g i  f ( ai )                                     layer connections were inhibitory (Medler & McClelland,
                                                                       2001).
                 1/g i  log[( ai  min) / (max  ai )]                   Stimuli were arbitrary, distributed binary patterns [0,1]
represents the net input required to maintain an activation            that encoded the orthography, phonology, and semantics of a
value of ai. Here we use the inverse logistic, where min and           given “word”. It is recognized that the abstract, distributed
max are the minimum and maximum activation bounds                      codes used in this simulation are not true representations of
respectively. gi is a gain function, and Zi(t) is the standard         semantics, phonology, and orthography; however, future
Gaussian variable with zero mean and unit variance. The                simulations using the same architecture will use more
last term in the equation adds stochasticity to the network,           systematic encodings for these representations. Half of the
which allows it to learn multiple meanings for a single input.         training patterns (20) were unambiguous words, and half
    SDNs are trained with the Contrastive Hebbian                      (20) were ambiguous words. In this model, only semantics
Learning (CHL) algorithm, which performs both supervised               had ambiguous patterns (as opposed to ambiguous
and unsupervised learning depending on the environmental               orthography or phonology). Hence, ambiguous words had
inputs to the network. Basically, learning occurs by                   two possible meanings, and were selected with either a
presenting a pattern to the network and letting it settle for a        70/30 distribution or a 50/50 distribution.                  Two
set number of cycles. During this positive phase, co-                  representational training patterns are shown in Table 2; the
occurrence statistics are computed for all the units. A                presentation probability is the likelihood of that specific
negative phase then follows where the pattern is removed,              pattern being selected during the positive phase. Nonwords
the network is allowed to re-settle, and co-occurrence                 were simply random patterns across the orthographic and
statistics are collected once again. Weights are then                  phonology units that had not been previously trained1.
adjusted using the difference between the negative phase                   During training, the orthography and phonology units
statistics and the positive phase statistics.                          were clamped on, and the semantic and hidden units were
                                                                       modified during the positive and negative phases. Following
             wij      a a    a
                                 
                                 i
                                   
                                   j          i
                                               
                                                 a j      Eq. 2     training, the network was able to correctly produce the
    In essence, the CHL algorithm makes weight                         probability structure of the training stimuli. That is, the
adjustments based upon subtracting out the statistics of the           network was able to successfully recall the semantic patterns
base activity of the network (negative phase) from the                 with the same probabilities that it was trained on. The
statistics of the environment plus base activity (positive                 1
                                                                             As previous results have suggested that non-word foils need
phase). Weight adjustments in this model were computed                 to be word-like for the ambiguity advantage to be stable
after each pattern presentation, as opposed to batch learning          (Borowsky & Masson, 1996), and we are assessing LD over the
                                                                       semantic units, we clamped both the orthographic and phonology
                                                                       units for the non-words.
                                                                   946

                                                                                                                   Meaning 1
                                                                                                                   Meaning 2
                                                                                                                   Meaning 3
                                                                                                                   Meaning 4
    Figure 2. Sample differentiation scores for a subset of unambiguous, ambiguous, and non-words. Note that the first non-
word is mistaken for a word at a criterion of 0.25.
network did not produce blend states for the ambiguous             where Gi is the generated pattern, and Ti is a target. If a
items.                                                             generated pattern does not match a target pattern, then the
                                                                   differentiated score should approach zero. A matched
    Table 2. Sample Patterns Showing Positive and                  pattern, on the other hand, should produce a score that
Negative Training Phases for Unambiguous and Ambiguous             approaches one. If multiple patterns are partially activated
Words                                                              (i.e., a blend), then several words should show a
                                                                   differentiation score that approaches a middle value.
Present.                       Unambiguous                              When diffk exceeds a threshold (in this case, an arbitrary
   Prob. Orthography        Phonology          Semantics           value of 0.25), a decision of “word” is made. If a word (or
    +1.0      10011            01101       1010111010              non-word) fails to reach the threshold within a certain time
     -1.0     10011            01101                     limit (an arbitrary point such as 20 time steps plus or minus
                                 Ambiguous                         some random time to introduce stochasticity in the response
            Orthography     Phonology          Semantics           times), then a nonword decision is made. This nonword
    +0.7      01110            11000       0110011001              time limit can be adjusted to reflect task instructions (e.g.,
    +0.3      01110            11000       1011000010              “respond as quickly as possible” vs. “respond as quickly and
     -1.0     01110            11000                     accurately as possible”). Consequently, we can produce both
                                                                   accuracy and reaction time measurements from our model.
    During testing, the orthography and phonology units
were clamped on, and the semantic units were allowed to                                         Results
settle. Previous models waited for the networks to settle into     Figure 2 shows some representative differentiation scores
a stable state, and took this measure as a reaction time. In       for a sub-sample of the testing stimuli. As can be seen, no
our model, we assume a speeded decision based on a                 blends were formed (a single score tended towards one,
differentiation measure (McClelland & Chappell, 1998)              whereas all other scores tended towards zero). Furthermore,
computed over the known words, k:                                  the figure shows how using a threshold criterion of 0.25 for
                              n
                                                                   a speeded decision leads to the first non-word being
                  diff k   1  (Ti )   (Gi )                  misclassified as a word. Finally, it should also be noted that
                            i 1                                   although the second ambiguous word looks like it is initially
                                                                   activating two word meanings (heading towards a blend
                                                               947

perhaps?), the second word meaning (i.e., the dashed line) is      decisions. If we assume that the network has settled into a
actually associated with the second unambiguous word.              stable state following the lexical decision (processing is
    In terms of reaction times, the network showed an              automatic and continues even after the decision process),
ambiguity advantage. The network made a lexical decision           then both the unambiguous and ambiguous items will have
for unambiguous items in an average of 13.0 time steps,            activated a meaning in semantics. For unambiguous items,
whereas ambiguous items took 11.5 time steps. In contrast to       the semantic comparison would be relatively easy as there
previous empirical work (Piercey & Joordens, 2000),                would only be one meaning to compare. For ambiguous
however, there was not a clear advantage of ambiguous              items, however, the comparison becomes more unsettling.
items in terms of accuracy.             Lexical decision for       On some trials, the semantic decision would be relatively
unambiguous items was approximately 97% correct, while             quick2 as the network would be in the correct semantic
ambiguous items were only 95% correct within the speeded           attractor. On other trials, however, the network would be in
decision.                                                          an incorrect attractor, and would have to switch attractor
    One last note to make is that the final differentiation        states. Therefore, when trials are averaged, ambiguous
score for ambiguous items was often lower and more                 items should show a disadvantage for semantic decisions.
variable than for unambiguous words. The differentiation           Consequently, semantic decisions are inefficient for
score averaged over the last ten time steps for the                ambiguous items because of the need to visit multiple
unambiguous items was 0.92 (var = 3.7x10-4) whereas                attractor basins. Hence, this theory predicts that if we prime
ambiguous items had an average differentiation score of            an ambiguous item towards one meaning or another, then the
0.87 (var = 5.1x10-4). This suggests that, for ambiguous           disadvantage should be lessened. Indeed, preliminary
items, the final settled state for the networks was more           behavioral results show this to be the case (Piercey, Medler,
unstable than unambiguous items, and that if speeded               & Hebert, 2003).
decisions were not made, then the ambiguity advantage in               One area of potential criticism for the current model is
reaction times may disappear.                                      that although it showed an ambiguity advantage for reaction
                                                                   times, it did not show an ambiguity advantage for accuracy.
                           Discussion                              This discrepancy may be due to the choice of the
We have shown how a network trained with the CHL                   differentiation score to evaluate network performance. This
produces the ambiguity advantage over the semantic nodes           scoring mechanism assumes that the currently presented
based on speeded decision. Furthermore, the model was able         pattern is simultaneously compared to all learned words
to produce the approximate correct probability distributions       (thus, assuming that the learned patterns are stored
of the training corpus, thereby avoiding “blend” states.           somewhere exterior to the current model). Consequently, as
Consequently, the theory of blend states having to exist to        unambiguous and ambiguous words are learned to criteria in
aid in LD for ambiguous items may have to be re-evaluated.         the model, a decision based on the learned representations
Furthermore, the efficient-then-inefficient hypothesis of          should show equal performance (where failure to recognize
Piercy and Joordens (2000) may have to be recast.                  a word is based on a combination of the threshold criterion
    The results from this network stimulation suggest an           and the nonword decision time limit). One possible solution
alternative theory as to why ambiguous items show an               to this would be to use a different type of LD process, such
advantage for lexical decision. Given that there are multiple      as the harmony/referent model (Piercey, 2002; Joordens,
distinct attractor states in semantics for ambiguous items,        Piercey, & Azarbehi, 2003) of lexical decision.
and given a random start state, then the probability of                Future models will focus on training all processing levels
starting near an attractor is greater for ambiguous items than     (orthographic, phonological, and semantic) to address the
unambiguous items. Consequently, if a decision is based on         theory of non-word background driving the ambiguity
traveling toward an attractor basin, then ambiguous items          advantage in LD. As well, we will explicitly address the
shouldon averagereach a basin sooner than unambiguous            semantic relatedness decision issue to evaluate the theory
items. This is similar to the attractor basin theory proposed      predicted by the current simulations.
by Plaut and Booth (2000). Consequently, lexical decisions
are efficient for ambiguous items because they have a higher                                 Reference List
probability of starting near an attractor basin.                   Besner, D., & Joordens, S. (1995). Wrestling with
    Note that this theory would require lexical decisions to             ambiguity--further reflections: Reply to Masson and
be made at the semantic level. That is, if LD could be                   Borowsky (1995) and Rueckl (1995). Journal of
completed at the orthographic level or at the phonological               Experimental Psychology: Learning Memory and
level (say by having non-words that either violated the                  Cognition, 21, 515-519.
orthographic rules or the phonological rules of English),
then the ambiguity advantage would disappear (cf.,                     2
                                                                          It is unclear whether the decision would be as fast as the
Borowsky & Masson, 1996).                                          unambiguous trials, as simulation results show the final state for
    Interestingly, this theory would also explain the              the ambiguous words to be less robust and more variable.
disadvantage seen for ambiguous items during semantic              Consequently, one might expect this variability to slightly slow
                                                                   decision processes.
                                                               948

Borowsky, R., & Masson, M. E. J. (1996). Semantic                  Medler, D. A. & McClelland, J. L. (2001). Improving the
     ambiguity effects in word identification. Journal of              performance of Symmetric Diffusion Networks via
     Experimental Psychology: Learning Memory and                      biologically inspired constraints. In K. Marko & P.
     Cognition, 22, 63-85.                                             Werbos (Eds.), IJCNN'01: Proceedings of the INNS-
                                                                       IEEE International Joint Conference on Neural
Duffy, S. A., Morris, R. K., & Rayner, K. (1988). Lexical              Networks (pp. 400-405). Washington, DC: IEEE Press.
     ambiguity and fixation times in reading. Journal of
     Memory & Language, 27, 429-446.                               Movellan, J. R., & McClelland, J. L. (1993). Learning
                                                                       continuous probability distributions with Symmetric
Fellbaum, C. (1998). WordNet: An Electronic Lexical                    Diffusion Networks. Cognitive Science, 17, 463-496.
     Database. Cambridge, MA: MIT Press.
                                                                   Piercey, C. D. (2002). The Referent Model of Lexical
Gernsbacher, M. A. (1984). Resolving 20 years of                       Decision. Unpublished doctoral dissertation, University
     inconsistent interactions between lexical familiarity and         of Alberta, Edmonton, Alberta, Canada,
     orthography, concreteness, and polysemy. Journal of
     Experimental Psychology: General, 113, 256-281.               Piercey, C. D., & Joordens, S. (2000). Turning and
                                                                       advantage into a disadvantage: Ambiguity effects in
Harm, M. W., & Seidenberg, M. S. (1999). Phonology,                    lexical decision versus reading tasks. Memory &
     reading acquisition, and dyslexia: Insights from                  Cognition, 28, 657-666.
     connectionist models. Psychological Review, 106, 491-
     528.                                                          Piercey, C. D., Medler, D. A., & Hebert, B. E. (Eds.).
                                                                       (2003). Ambiguity Effects in Lexical Access: Do
Harm, M. W. & Seidenberg, M. S. Computing the Meanings                 Blends Exist? (Vol. 8).
     of Words in Reading: Cooperative Division of Labor
     Between Visual and Phonological Processes.                    Plaut, D. C., & Booth, J. R. (2000). Individual and
     Psychological Review, (in press).                                 developmental differences in semantic priming:
                                                                       Empirical and computational support for a single-
Joordens, S., & Besner, D. (1994). When banking on                     mechanism account of lexical processing.
     meaning is not (yet) money in the bank: Explorations in           Psychological Review, 107, 786-823.
     connectionist modeling. Journal of Experimental
     Psychology: Learning Memory and Cognition, 20,                Plaut, D. C., McClelland, J. L., Seidenberg, M. S., &
     1051-1062.                                                        Patterson, K. (1996). Understanding normal and
                                                                       impaired word reading: Computational principles in
Joordens, S., Piercey, C. D., & Azarbehi, R. (2003). From              quasi-regular domains. Psychological Review, 103, 56-
     word recognition to lexical decision: A random walk               115.
     along the road of harmony. In International Conference
     on Cognitive Modeling.                                        Rayner, K., & Duffy, S. A. (1986). Lexical complexity and
                                                                       fixation times in reading: Effects of word frequency,
Kawamoto, A. H., Farrar, W. T., & Kello, C. T. (1994).                 verb complexity, and lexical ambiguity. Memory &
     When two meanings are better than one: Modeling the               Cognition, 14, 191-201.
     ambiguity advantage using a recurrent distributed
     network. Journal of Experimental Psychology: Human            Rayner, K. & Duffy, S. A. (1987). Eye movements and
     Perception & Performance, 20, 1233-1247.                          lexical ambiguity. In J.K.O'Regan & A. Levy-Schoen
                                                                       (Eds.), (pp. 521-529). Amersterdam: North-Holland:
Masson, M. E. J., & Borowsky, R. (1995). Unsettling                    Elsevier Science Publications.
     questions about semantic ambiguity in connectionist
     models: Comment on Joordens and Besner (1994).                Rueckl, J. G. (1995). Ambiguity and connectionist networks:
     Journal of Experimental Psychology: Learning                      Still settling into a solution: Comment on Joordens and
     Memory and Cognition, 21, 509-514.                                Besner (1994). Journal of Experimental Psychology:
                                                                       Learning Memory and Cognition, 21, 501-508.
McClelland, J. L., & Chappell, M. (1998). Familiarity
     breeds differentiation: A subjective-likelihood approach      Seidenberg, M. S., & McClelland, J. L. (1989). A
     to the effects of experience in recognition memory.               distributed, developmental model of word recognition
     Psychological Review, 105, 724-760.                               and naming. Psychological Review, 96, 523-568.
                                                               949

