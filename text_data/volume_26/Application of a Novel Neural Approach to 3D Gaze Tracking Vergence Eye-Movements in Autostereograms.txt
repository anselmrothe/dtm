UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Application of a Novel Neural Approach to 3D Gaze Tracking: Vergence Eye-Movements
in Autostereograms
Permalink
https://escholarship.org/uc/item/31p9f05t
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Essig, Kai
Pomplun, Marc
Ritter, Helge
Publication Date
2004-01-01
Peer reviewed
  eScholarship.org                             Powered by the California Digital Library
                                                                University of California

         Application of a Novel Neural Approach to 3D Gaze Tracking:
                       Vergence Eye-Movements in Autostereograms
                                    Kai Essig1 , Marc Pomplun2 and Helge Ritter1
                          1
                            Neuroinformatics Group, Faculty of Technology, Bielefeld University,
                                        P.O.-Box 10 01 31, 33501 Bielefeld, Germany
                                        Email: (kessig, helge)@techfak.uni-bielefeld.de
                       2
                         Department of Computer Science, University of Massachusetts at Boston,
                                  100 Morrissey Boulevard, Boston, MA 02125-3393, USA
                                                    Email: marc@cs.umb.edu
                          Abstract                                 reaches zero, showing the anticipatory behavior of the
                                                                   eye-vergence system.
   Vergence eye-movements occur not only in real environ-             For our experiment, however, instead of RDS we used
   ments, but also in virtual 3D environments. Autostere-          Single Image Random Dot Stereograms (SIRDS), also
   ograms can cover large visual angles without requiring
   vergence beyond natural parameters and are thus well-           called autostereograms. The difference between RDS and
   suited for the investigation of vergence movements in           SIRDS is that in SIRDS the two slightly different im-
   virtual 3D images. We developed an anaglyph-based               ages of the RDS are combined into a single image. No
   3D calibration procedure and used a parametrized self-          stereoscope is necessary to perceive the 3D information
   organizing map (PSOM) to approximate the 3D gaze
   point from a subject’s binocular eye-movement data.             in a SIRDS. To achieve the 3D perception of a SIRDS,
   Besides analyzing the general pattern of vergence eye-          observers have to combine the information of the two
   movements in autostereogram images, the present study           images for the left and the right eye to one depth per-
   examined the influence of image granularity on these            ception by focusing a point before (cross-eyed method)
   movements. Unlike previous research on random-dot               or behind (wide-eyed method) the image plane, whereby
   stereograms, we found substantially overshooting con-
   vergence eye-movements, especially for medium gran-             our preliminary studies (Essig, 1998) demonstrated that
   ularities. Moreover, divergence movements were com-             the later method leads to a more stable perception of
   pleted more quickly for coarse than for fine granularities.     the depth information and was therefore used for the
   Results are discussed in the context of granularity effects     reported experiments. Once the observers perceive the
   on autostereogram perception and the dissociation be-
   tween convergence and divergence eye-movements.                 3D scene, they may move their gaze to any position in
                                                                   the image without losing the 3D impression. Most im-
                                                                   portant in the present context, SIRDS can cover large
                                                                   visual angles without requiring vergence beyond natural
           Vergence Eye-Movements in                               parameters, which makes them appropriate stimuli for
                   Autostereograms                                 studying vergence movements in virtual 3D images.
In everyday life, we employ vergence eye-movements to                 In our natural environment, and when looking at RDS,
successively fixate objects at different distances. It is in-      we move our eyes inward (convergence) to inspect near
teresting to note that these movements are also produced           objects and outward (divergence) to inspect distant ob-
in virtual 3D environments. Some work has been done                jects. An obvious question is: Do these vergence move-
on the analysis of vergence movements occurring while              ments also occur during the observation of autostereo-
viewing Random-Dot Stereograms (RDS). For RDS two                  gram images? Belopolskii and Logvinenko (1994) found
slightly different images for the left and the right eye seen      that vergence movements are not necessary to get the
through a special device, a so-called stereoscope, lead to         3D perception of an autostereogram. However, in recent
the perception of 3D information. Mowforth, Mayhew                 experiments (Essig, 1998) we found that in fact vergence
and Frisby (1981) found that RDS are perceptually fil-             eye-movements are executed during the examination of
tered by spatial frequencies. Low frequencies enable the           autostereogram images. These results encouraged us
3D perception of RDS with larger disparities and lead              to conduct further experiments investigating stereogram
to faster vergence movements than do high frequencies.             parameters that are likely to influence vergence move-
Furthermore, vergence velocities were found to be faster           ments.
for convergent than for divergent eye movements.                      A distinguishing feature of autostereogram images is
   In another study, Rashbass and Westheimer (1961)                their grain size (granularity). Figure 1 shows examples
found reaction times of about 160 ms for both conver-              for autostereogram images with small (upper panel) and
gent and divergent eye movements in response to sud-               big grain sizes (lower panel). The reported experiment
denly introduced target vergence changes. The authors              used autostereogram images with different grain sizes to
reported start velocity varying with stimulus amplitude,           systematically investigate the influence of this parame-
and vergence reaching asymptotically its final level af-           ter on the vergence movements. According to Mowforth
ter approximately 800 msec without any overshoots or a             et al. (1981), overshoots and oscillations should be more
prolonged period of oscillations. They also found turn-            likely for autostereogram images with coarser granulari-
abouts in the response before the target vergence change           ties, i.e. lower spatial frequencies, which, however, is in-
                                                               357

                                                              these individual parameters. Additionally, we have to
                                                              take into consideration that the mapping from pupil to
                                                              screen coordinates is non-linear. Hence neural nets are
                                                              suitable for the solution of these problems, because they
                                                              are able to “learn” nonlinear functions. Kohonen (1990)
                                                              developed the so-called self-organizing maps (SOMs),
                                                              which could learn the correct mapping from the pupil
                                                              to the screen coordinates. However, SOMs have two
                                                              disadvantages: They supply only the position of the
                                                              most stimulated neuron in a “neuron lattice” instead
                                                              of a continuous output, and they usually require thou-
                                                              sands of training examples. Therefore, we use a variant
                                                              of SOMs, namely a so-called parametrized self-organizing
                                                              map (PSOM) (Ritter, 1993). This variant does not have
                                                              the disadvantages mentioned above, because it provides
                                                              the demanded continuous output and gets only some se-
                                                              lected input/output pairs as parameters, i.e. the coordi-
                                                              nates of the calibration points and the related positions
                                                              of the pupils measured by the eye tracker.
                                                                 A 2D version of the PSOM was already used in (Pom-
                                                              plun, Velichkovsky & Ritter, 1994), reducing the average
                                                              calibration error to about 30% of its previous value. In
                                                              opposition, our “new” PSOM does not only enhance the
                                                              accuracy of measurement, it also approximates the sub-
                                                              ject’s 3D gaze position from the two 2D coordinates of
                                                              the eye tracker system.
                                                                 A PSOM can be considered as a recursive neural net
Figure 1: Autostereogram images with granularities 1          that realizes a distinct, mostly multi-dimensional pro-
(upper panel) and 12 (lower panel).                           jection f. The input data of this projection consist of
                                                              the “correct” coordinates of 27 calibration points k ∈ A
                                                              (standardized in the interval from 0 to 2 by the PSOM),
consistent with the findings of Rashbass and Westheimer       where A = {kxyz |kxyz = xeˆx + y eˆy + z eˆz ; x, y, z = 0...2}
(1961). For a clarifying analysis of this issue, we mea-
sured vergence eye-movements with a modern binocular
eye tracker with a sampling rate of 250 Hertz. For in-
creased precision of vergence and 3D gaze-position mea-
surement, we developed and applied a novel neural-net
based calibration interface, which is described in the fol-
lowing section.
   A Neural Approach to 3D Gaze-Point
                     Calculation
During the examination of autostereograms shown on
the computer screen, the intersection of the viewing axes
is in general in front of or behind the screen plane, de-
pending on the 3D gaze position. As a consequence, the
correct coordinates for the actual gaze position are dif-
ferent from the screen coordinates which the eye tracker      Figure 2: The calibration points in the virtual 3 x 3 x 3
provides, because the system uses a 2D calibration to cal-    cuboid. For the PSOM the coordinates are standardized
culate the relation between the pupil positions and the       in such a way that they only have the values 0, 1, and 2.
gaze point on the screen. We tackled this problem by de-
veloping a 3D calibration that precedes the experiment        (see Figure 2) arranged in a 3 x 3 x 3 grid and presented
and uses a parametrized self-organizing map (PSOM) to         to the subjects during the calibration procedure. In or-
approximate the 3D gaze point from a subject’s binocu-        der to create a virtual 3D perception, the calibration grid
lar eye-movement data.                                        was presented as an anaglyph image, viewed through
   Since the experimental conditions change from sub-         red-green glasses. The calibration points were drawn on
ject to subject (e.g. subjects have physical differences,     the planes of a cuboid, where the planes before and be-
their gaze characteristics are individually different, and    hind the screen plane formed its surfaces. This made it
the eye-tracker setup varies across sessions), it is clearly  perceptually easier for the subjects to locate the stimuli
advantageous to use a method which is able to “learn”         on one of the three planes of the cuboid. For each of these
                                                          358

27 calibration markers, we have the associated gaze coor-          Because n has only three possible values (0, 1, and
dinates measured by the eye tracker and the x-divergence        2), it is sufficient to choose an account of three basis
between the gaze positions of the left and right eye on         functions IR → IR. For this purpose, polynomials of sec-
the screen calculated from this data. Thus, the reference       ond degree are especially suitable, because they have no
vector is wk = (xplk , yplk , xprk , yprk , xdk ) ∈ IR5 . xplk  redundant degrees of freedom.
is the x-coordinate for the left eye, and yplk the corre-          Now we have built a function which projects 3D coor-
sponding y-coordinate for the left eye belonging to the         dinates onto 2D gaze-positions for both eyes. Our final
vector wk . For the right eye the corresponding values          aim is to find a function which does just the opposite
are xprk and yprk . The fifth element of wk results from        though, namely to approximate the 3D gaze-position of
xdk := xprk − xplk .                                            the subject from the system’s 2D measurements. There-
   We introduce the divergence (xdk ) as the fifth dimen-       fore, we have to calculate the inverse function f −1 of f.
sion of wk because the z-coordinate of the 3D gaze-             The non-linearity of f forces us to use a numerical pro-
position mainly depends on this divergence. Since the           cedure. Thus we have to create an error function E(s),
differences in the divergence are smaller than those in the     which is defined as:
x- and y-directions, the divergence has to be weighted by
a specific factor. This method leads to a faster termi-                                     1
                                                                                    E(s) =     (f (s) − fet )2           (4)
nation of the PSOM-calculations. Hence, every fixation                                      2
point k ∈ A is associated with a corresponding refer-
                                                                i.e. the deviation of the pupil coordinates provided by
ence vector wk ∈ IR5 . With the reference vectors wk we
                                                                the eye tracker (fet ) from the eye tracker data calculated
could already construct a SOM which could enable the
                                                                by the PSOM f (s) for the actually assigned two screen
projection from the 2D coordinates of the system to the
                                                                points.
“correct” 3D coordinates. This SOM would only enable
                                                                   If this difference exceeds a specified threshold, the co-
a crude approximation to the real 3D coordinates, be-
                                                                ordinates of these points are modified in an iterative
cause it could only choose one of the calibration points
                                                                gradient-descent procedure until the results of Equation
as a possible output. In the present context, however,
                                                                (1) closely approximate the actual eye tracker data pro-
we are looking for a projection which can interpolate be-
                                                                vided by the system:
tween the vectors wk . This projection can be done by a
PSOM.                                                                                           δE(s)
   The desired interpolation function f(s) can be con-                   s(t + 1) = s(t) − ² ·             , with ² > 0  (5)
                                                                                                   δs
structed from the superposition of a suitable number of
simpler basis functions H(.,.) as follows:                         This means that the iteration process stops if E(s(t))
                              X                                 falls below the threshold, which we should adapt to the
                      f (s) =     H(s, k)wk                (1)  screen resolution. In this way, an exact 3D gaze position
                              k∈A                               of the subject during the examination of a 3D stimulus
                                                                is assigned to the 2D eye tracker data for the left and
   The values of the basis functions are within the in-
                                                                right eye.
terval [0,1] depending on different values s, so that the
coordinates of a calibration point, which is close to the          We conducted an experiment in which the accuracy
desired gaze position, is weighted strongly (near 1) and        of the PSOM gaze-point calculation is compared to one
points which are far away are weighted weakly (near 0).         provided by a geometrical solution. Subjects had to vi-
                                                                sually track a dot that appeared at positions of a 4 x 4 x
The basis functions H : IR3 × A → IR have to comply             4 grid in 3D space in a random sequence. In the geome-
with the requirement                                            trical method the equations for the right and left visual
                H(s, k) = δs,k        ∀ s, k ∈ A           (2)  axes are calculated on the basis of both the measured
                                                                gaze-positions on the screen and the subjects’ (assumed)
where δ represents
          ½              the Kronecker symbol. It is defined    constant head position. The gaze point is determined as
              1 : i=j                                           the point where the visual axes are closest to one an-
as: δij =                       ∀ i, j = 0, 1, 2
              0 : i 6= j                                        other. It results as the center of the shortest straight
(in this special case). This ensures that                       link between the visual axes.
                                                                   The results show that the neural net calculates gaze
                    f (s) = ws      ∀ s∈A
                                                                positions from the eye tracker data which are nearly 46%
   Thus, it is guaranteed that the interpolation function       closer to the actual gaze positions than those of the geo-
passes through the given points. But how can we choose          metrical solution. The average total error for all sub-
suitable functions H that obey equation (2), that are           jects, separated for the individual coordinates, is shown
smooth and simple to handle?                                    in Table 1 (the values for the geometrical method and the
   One convenient solution is to make a product ansatz          neural net show the average total error and the standard
and to derive the suitable function by combining three          error).
1D functions (one in each case for every coordinate di-            It is obvious that the z-error is always higher than the
rection x, y, and z). The new 1D functions must then            x− and y−errors, because the z-coordinate is much more
have the property H (1) : IR × {0, 1, 2} → IR:                  sensitive to small changes in the binocular gaze position
                                                                than the x− and y−coordinates. We also found that the
         H (1) (q, n) = δq,n ∀ q ∈ IR, n ∈ {0, 1, 2}       (3)  measurement errors decrease from the back to the front
                                                            359

Table 1: Average total error for individual coordinates.       Apparatus. We used an SMI EyeLink eye tracker for
                                                               the experiments. This system employs a headset with
                 both          geometrical     neural          two cameras to enable binocular eye-movement record-
                 methods       method          net             ing. Further features of the EyeLink system are a high
  coordinate average to-       average to-     average to-     sampling rate of 250 Hz and an average on-screen gaze
                 tal error     tal error       tal error       position error between 0.5o and 1.0o .
       x           0.97cm      1.41cm     ±    0.52cm ±        Procedure. Prior to the experiment, subjects were
                               0.04cm          0.05cm          presented with autostereogram images without using the
       y           1.03cm      1.24cm     ±    0.82cm ±        eye tracker. This way the subjects could practice the per-
                               0.05cm          0.05cm          ception of the depth information. A lamp fixed behind
       z           4.16cm      5.79cm     ±    2.53cm ±        the subjects to create reflections on the screen helped
                               0.36cm          0.11cm          the subjects to fixate a virtual point behind the screen
                                                               plane, facilitating the 3D perception of the autostereo-
                                                               gram image. When the subject got the 3D impression,
                                                               the light from behind was switched off.
plane. The neural net compensates the errors in the back          At the beginning of every trial the light was switched
plane better than the geometrical method, because small        on as soon as the autostereogram image appeared on
changes in the gaze position at the back plane lead to         the screen. When the subject got the 3D impression of
severe inaccuracies. Obviously, the precision of our novel     the image, the experimenter switched off the light and
method of 3D gaze-position measurement provided an             started the eye-movement recording. It was the subjects’
appropriate basis for the autostereogram study.                task to change their view from the front plane to the back
                                                               plane and vice versa (every few seconds). Between these
                                                               eye movements subjects kept their view on one plane for
       The Autostereogram Experiment                           a while before changing their view to the other one. The
Method                                                         vergence movements were recorded over a duration of 1
                                                               minute per image. If subjects “lost” the 3D impression
Subjects. Eight paid subjects (1 female, 7 male), all          during the recording interval, they had to press the right
of them were students at the University of Bielefeld,          mouse button, recover the 3D impression, and press the
participated in this experiment. They had normal or            left button to continue recording. Ten autostereogram
corrected-to-normal visual acuity and were experienced         images were shown to the subject in a random order so
in viewing autostereogram images.                              that every granularity appeared twice.
Stimuli. The stimuli in this experiment were auto-             Data Analysis. In general, the vergence angle α is
stereogram images whose depth impression arose from            used to make statements about a subject’s vergence. The
two horizontally divided half planes, which were recog-        bigger the vergence angle is, the stronger the eyes con-
nized at different virtual distances. The autostereogram       verge, i.e. the nearer the (virtual) surface fixated by the
images used in the experiment varied in their granulari-       subject. The time course of vergence movements was
ties. Using the autostereogram generation program rds-         the most important data to be analyzed in this experi-
gen V1.2b by Frederic N. Feucht1 , we produced different       ment, especially during the period right before and after
autostereogram images in which one dot of the auto-            subjects changed their gaze point from one plane to an-
stereogram image corresponded to one, two, four, eight,        other. We distinguished between eye movements from
or twelve screen pixels in height and width. The sub-          the near to the far plane and vice versa. For the evalua-
jects always got the spatial perception that the lower         tion process we summarized the data of all subjects and
plane was nearer than the upper one. The depth differ-         calculated the arithmetic mean and the standard error
ence between the two planes corresponded to a vergence         of the vergence angle as a function of time relative to
angle difference of 0.859o .                                   gaze transitions between the two depth planes.
   By using rdsgen we created two autostereogram im-              There were two criteria for the presence of a gaze tran-
ages for each of the granularities 1, 2, 4, 8, and 122 .       sition between planes: First, there had to be a change in
These 10 autostereogram images were presented to the           the measured y-value that indicated a crossing of the hor-
subjects in a random order on a 20-inch screen and a spa-      izontal boundary. Second, any such change from plane A
tial resolution of 640x480 pixels. The images consisted        to plane B had to be preceded by a contiguous sequence
of black and blue (luminance: 24 cd/m2 ) points, where         of 50 measured gaze positions (200 msec) on plane A,
one pixel corresponded to 4.17 min arc. The distance           and succeeded by a contiguous sequence of 250 gaze po-
from the subject to the screen was 50 cm.                      sitions (1000 ms) on plane B. To account for a small
                                                               fraction of measurement errors, we allowed a maximum
   1                                                           of 4 violations of these conditions among the 300 mea-
     http://www.bcc.cc.nc.us/graphics/g2c.html.
   2                                                           surements. After detecting a plane transition, the time
     We first changed the distance between the repeating pat-
terns of the autostereogram image and magnified it with the    course of vergence from 200 ms before to 1000 ms after
corresponding “enlargement factor” (grain size) to keep the    the transition was calculated.
distance between the repeating patterns (and therefore the
depth impression) constant.
                                                           360

Results and Discussion                                                                            2
                                                                                                                                        granularity 1
                                                                                                                                        granularity 2
Our first observation was that subjects had problems to                                                                                 granularity 4
achieve a stable 3D perception of autostereogram im-                                            1.5
                                                                                                                                        granularity 8
                                                                                                                                        granularity 12
ages with large grain size (granularities 8 and 12). Once
                                                                vergence angle (standardized)
achieved, it was difficult for them to maintain the sta-
ble depth impression when changing their gaze point
                                                                                                  1
from one plane to the other. Subjects classified the au-
tostereogram images with the granularities 2 and 4 as
most pleasant to view. Figure 3 shows the temporal
progress of the convergence and divergence movements                                            0.5
during the examination of autostereogram images across
granularities. The vergence angles are standardized, i.e.
0 corresponds to the back and 1 to the front plane. In                                            0
the figures the value 0 on the time axis signals the time
of plane transition. Each panel shows the vergence data
from 200 ms before to 1 s after an eye movement be-                                             −0.5
                                                                                                 −200   0    200       400        600    800             1000
tween the two planes. Figure 3 also illustrates that, for                                                          time (in ms)
all granularities, the convergence movements are obvi-                                           1.2
ously faster than the divergence movements. This is in                                                                                  granularity 1
                                                                                                                                        granularity 2
line with the observations in Mowforth, Mayhew and                                                1                                     granularity 4
Frisby (1981). The final values are reached after a pe-                                                                                 granularity 8
                                                                                                                                        granularity 12
riod of about 800 ms, as stated in Rashbass and West-
heimer (1961). As can clearly be seen, both convergence
                                                                vergence angle (standardized)
                                                                                                 0.8
and divergence movements already start before the onset
of an eye movement between planes, suggesting that the                                           0.6
imagination of the target distance is already sufficient for
the execution of vergence eye-movements (proximal ver-                                           0.4
gence). Similar results were already obtained by Yarbus
(1967).
                                                                                                 0.2
   The high overshoot of the convergence movements
(Figure 3, upper panel) for the granularities 2 and 4
is very obvious. Both show an overshoot of 1.8 and ap-                                            0
proach the end value at nearly 900 ms. The similarity
between those vergence movements is in line with the                                            −0.2
                                                                                                 −200   0    200       400        600    800             1000
subjects reporting that it is easier to get and stabilize                                                          time (in ms)
the 3D illusion of these two types of autostereogram im-
ages. One reason could be that the perception of the
depth planes is particularly stable for these granular-                                  Figure 3: Convergence (upper panel) and divergence
ities. Subjects can perceive the near plane very well,                                   movements (lower panel) occurring during the exami-
even if they fixate a point on the back plane. As a con-                                 nation of autostereogram images with different granu-
sequence, the visual system might perform a particularly                                 larities.
fast, overshooting vergence movement towards an angle
that would project the pattern of the near plane to corre-                               is much more difficult for the subjects to perceive both
sponding points on the retina. This mechanism allows us                                  planes simultaneously in those autostereogram images,
a fast focussing of near or approaching objects, which is                                which is consistent with the subjects’ impressions. An
an important capability to react in dangerous situations.                                important point is that the overshoot of the convergence
   The vergence movements during the observation of                                      movements is substantially weaker (1.4) than for those
the autostereogram image with granularity 1 are sim-                                     occurring in the images with finer granularities. The
ilar to those for granularities 2 and 4. It is obvious,                                  standard error for the convergence as well as for the di-
though, that the overshoot of the convergence move-                                      vergence movement is high. This can be interpreted as
ments for granularity 1 is smaller (1.5) than for gran-                                  indicating an instable perception of the depth planes.
ularities 2 and 4 (granularity 1 seems too fine for a sta-                               Furthermore, the progress of the divergence movements
ble extrafoveal perception). Also the approaching to the                                 is faster and finishes sooner than 600 ms after the gaze
end value is shorter (nearly 750 ms instead of approx.                                   shift.
900 ms). These results further support the finding that                                     The finding of overshoots for convergence movements
vergence movements can clearly be driven by relatively                                   is inconsistent with the results obtained by Rashbass and
high spatial frequencies (fine granularities), as stated in                              Westheimer (1961). It is possible that either the appa-
Mowforth, Mayhew and Frisby (1981).                                                      ratus used in their study did not allow the detection
   The data for the coarse granularities 8 and 12 are                                    of these overshoots, or that vergence eye-movements in
clearly different from those for 1, 2, and 4. Hence it                                   RDS – in contrast to those in SIRDS – do not show
                                                          361

overshoots. Rashbass and Westheimer describe the ver-                           Acknowledgments
gence system as a damped system, because it does not           This research was funded by the Deutsche Forschungs-
show any oscillations. In the present study, however, we       gemeinschaft (Sonderforschungsbereich 360, Situierte
found overshoots, but no following undershoots or any          Künstliche Kommunikatoren).
kind of oscillations. It seems therefore correct to speak
of a damped system but with a strong tendency towards                               References
overshoots for convergence movements in SIRDS.
                                                               Essig, K. (1998). Messung von binokularen Augenbewe-
   With regard to divergence movements (Figure 3, lower          gungen in realen und virtuellen 3D-Szenarien. Master
panel), we found faster approximation of the target an-          Thesis, Faculty of Technology, Bielefeld University.
gle for the autostereogram images with coarser granular-
ities than for those with finer granularities. This is just    Julesz, B. (1971). Foundations of Cyclopean Perception.
the opposite of the results we got for the convergence           Chicago: University of Chicago Press.
movements. A possible explanation is a fundamental             Kohonen, T. (1990). The Self-Organizing Map. Proceed-
difference in nature between convergence and divergence          ings of IEEE, 78, 1464–1480.
movements, as can be seen from Figure 3. Convergence           Littmann, E. (1989). Vergenz und Kontrast. Optome-
movements are fast and impulse-like, whereas divergence          trie, 3, 15–30.
movements are slower and smoother. From an evolu-
tionary standpoint, since distant or vanishing objects         Logvinenko, A. D. & Belopolskii, V. L. (1994). Conver-
are less dangerous, there is no need for fast divergence         gence as a Cue for Distance. Perception, 23, 207–217.
eye-movement mechanisms. Instead, this divergence re-          Mowforth, P., Mayhew, J. E. W. & Frisby J. P. (1981).
sembles more a relaxation process. And this relaxation           Vergence Eye Movements Made in Response to
might be facilitated by easily losing the perception of the      Spatial-Frequency-Filtered Random-Dot Stereograms.
near plane. In other words, for the finer granularities 1,       Perception, 10, 299–304.
2, and 4, the continuous stable perception of the front        Ning Qian (1997). Binocular Disparity and the Percep-
plane might impede the subjects’ effort to focus atten-          tion of Depth. Neuron, 18, 359-368.
tion – and consequently vergence – on the far plane.
                                                               Pomplun, M., Velichkovsky, B.M. & Ritter, H. (1994).
   All in all, some findings for the RDS were confirmed in
                                                                 An artificial neural network for high precision eye
our experiments with SIRDS, in particular that conver-
                                                                 movement tracking. In Nebel, B. & Dreschler-Fischer,
gence is faster than divergence, that the vergence mech-
                                                                 L. (Eds.), Lecture notes in artificial intelligence: AI-
anism can clearly be driven by relatively high spatial fre-
                                                                 94 Proceedings, 63-69. Berlin: Springer Verlag.
quencies, and that vergence response occurs even before
the disparity reaches zero. Inconsistent with previous         Rashbass C. & Westheimer G. (1961). Disjunctive Eye
studies, however, our experiment demonstrates substan-           Movements. J. Physiol., 159, 339-360.
tial differences in vergence movements between SIRDS           Reading, R.W. (1983). Binocular Vision. Foundations
and previously investigated RDS, especially with regard          and Applications. Boston: Butterworths.
to the overshoot for convergence movements. This differ-
ence, however, might have been caused by the possibly          Ritter, H. (1993). Parametrized Self-Organizing Maps.
low resolution of the apparatus used in the RDS study            ICANN93-Proceedings, 568–577, Berlin: Springer.
(Rashbass & Westheimer, 1961).                                 Schor, C. M. & Ciuffreda, K. J. (1983). Vergence Eye
   Furthermore, our experiments show a clear influence           Movements: Basic and Clinical Aspects. Boston: But-
of granularity on vergence movements. The pattern of             terworths.
influence seems to be more complex than described in           Yarbus, A. (1967). Eye Movements and Vision. New
Mowforth, Mayhew and Frisby (1981), because, accord-             York: Plenum Press.
ing to our results, lower frequencies (coarser granulari-
ties) do not always lead to faster vergence movements.
One reason might be the more difficult simultaneous per-
ception of different depth planes in the stimuli of very
low frequencies. At any rate, these complex granular-
ity effects on vergence eye-movements in autostereogram
images indicate that there may be rather non-intuitive
factors that can have a significant impact on the coordi-
nation of both eyes. Future research will have to address
further issues, such as the timing of coordinated binocu-
lar saccades and its dependence on various pattern fea-
tures. This is a research area that so far has been ex-
plored very little, and the rather recent possibility of fast
and accurate binocular eye tracking, combined with ap-
propriate techniques for 3D gaze measurement like the
approach presented here, will greatly contribute to its
advancement.
                                                           362

