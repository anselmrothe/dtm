UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Computationally Recognizing Wordplay in Jokes
Permalink
https://escholarship.org/uc/item/0v54b9jk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Taylor, Julia M.
Mazlack, Lawrence J.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 Computationally Recognizing Wordplay in Jokes
                                               Julia M. Taylor (tayloj8@email.uc.edu)
                                              Lawrence J. Mazlack (mazlack@uc.edu)
                                  Electrical & Computer Engineering and Computer Science Department
                                                           University of Cincinnati
                                Abstract                                 piece of literature in which the funniness culminates in the
                                                                         final sentence.” Most researchers agree that jokes can be
   In artificial intelligence, researchers have begun to look at ap-     broken into two parts, a setup and a punchline. The setup is
   proaches for computational humor. Although there appears to           the first part of the joke, usually consisting of most of the
   be no complete computational model for recognizing verbally           text, which establishes certain expectations. The punchline
   expressed humor, it may be possible to recognize jokes based          is a much shorter portion of the joke, and it causes some
   on statistical language recognition techniques. This is an in-        form of conflict. It can force another interpretation on the
   vestigation into computational humor recognition. It considers        text, violate an expectation, or both (Ritchie, 1998). As
   a restricted set of all possible jokes that have wordplay as a
                                                                         most jokes are relatively short, it may be possible to recog-
   component and examines the limited domain of “Knock
   Knock” jokes. The method uses Raskin's theory of humor for
                                                                         nize them computationally.
   its theoretical foundation. The original phrase and the                  Computational recognition of jokes may be possible, but
   complimentary wordplay have two different scripts that                it is not easy. An “intelligent” joke recognizer requires
   overlap in the setup of the joke. The algorithm deployed              world knowledge to “understand” most jokes.
   learns statistical patterns of text in N-grams and provides a
   heuristic focus for a location of where wordplay may or may                               Theories of Humor
   not occur. It uses a wordplay generator to produce an utter-          Raskin’s (1985) Semantic Theory of Verbal Humor has
   ance that is similar in pronunciation to a given word, and the        strongly influenced the study of verbally expressed humor.
   wordplay recognizer determines if the utterance is valid.             The theory is based on assumption that every joke is com-
   Once a possible wordplay is discovered, a joke recognizer de-         patible with two scripts, and those two scripts oppose each
   termines if a found wordplay transforms the text into a joke.         other in some part of the text, usually in the punch line,
                                                                         therefore generating humorous effect.
                            Introduction                                    Another approach is Suls’ (1972) two-stage model, which
Thinkers from the ancient time of Aristotle and Plato to the             is based on false expectation. The following algorithm is
present day have strived to discover and define the origins              used to process a joke using two-stage model (Ritchie,
of humor. Most commonly, early definitions of humor relied               1999):
on laughter: what makes people laugh is humorous. Recent
works on humor separate laughter and make it its own dis-                     • As a text is read, make predictions
tinct category of response. Today there are almost as many                    • While no conflict with prediction, keep going
                                                                              • If input conflicts with prediction:
definitions of humor as theories of humor; as in many cases,                       o If not ending – PUZZLEMENT
definitions are derived from theories (Latta, 1999). Some                          o If is ending, try to resolve:
researchers say that not only is there no definition that                                  No rules found – PUZZLEMENT
covers all aspects of humor, but also humor is impossible to                               Cognitive rules found –HUMOR
define (Attardo, 1994).
   Humor is an interesting subject to study not only because                There have been attempts at joke generation (Attardo,
it is difficult to define, but also because sense of humor               1996; Binsted, 1996; Lessard and Levison, 1992;
varies from person to person. The same person may find                   McDonough, 2001; McKay, 2002; Stock and Strapparava,
something funny one day, but not the next, depending on the              2002) and pun recognizers (Takizawa, et al. 1996;
person’s mood, or what has happened to him or her recently.              Yokogawa, 2002) for Japanese. However, there do not
These factors, among many others, make humor recognition                 appear to be any theory based computational humor efforts.
challenging.                                                             This may be partly due to the absence of a theory that can be
   Although most people are unaware of the complex steps                 expressed as an unambiguous computational algorithm. In
involved in humor recognition, a computational humor                     the cases of Raskin and Suls, the first does not offer any
recognizer has to consider all these steps in order to                   formal algorithm, and the second does not specify what a
approach the same ability as a human being.                              cognitive rule is, leaving one of the major steps open to
   A common form of humor is verbal, or “verbally ex-                    interpretation.
pressed, humor” (Ritchie 2000). Verbally expressed humor
involves reading and understanding texts. While
                                                                                               Wordplay Jokes
understating the meaning of a text may be difficult for a                Wordplay jokes, or jokes involving verbal play, are a class
computer, reading it is not.                                             of jokes depending on words that are similar in sound, but
   One of the subclasses of verbally expressed humor is the              are used in two different meanings. The difference between
joke. Hetzron (1991) defines a joke as “a short humorous                 the two meanings creates a conflict or breaks expectation,
                                                                     1315

and is humorous. The wordplay can be created between two         scripts that overlap in the phonetic representation of “wa-
words with the same pronunciation and spelling, with two         ter,” but also oppose each other. Following Suls’ approach,
words with different spelling but the same pronunciation,        “what are” conflicts with the prediction. In this approach, a
and with two words with different spelling and similar           cognitive rule can be described as a function that finds a
pronunciation. For example, in Joke1 the conflict is created     phrase that is similar in sound to the word “water,” and that
because the word has two meanings, while the pronun-             fits correctly in beginning of the final sentence’s structure.
ciation and the spelling stay the same. In Joke2 the wordplay    This phrase is “what are” for Joke3.
is between words that sound nearly alike.
   Joke1: “Cliford: The Postmaster General will be making                                   N-grams
           the TOAST.                                            A joke generator has to have an ability to construct mean-
           Woody: Wow, imagine a person like that helping        ingful sentences, while a joke recognizer has to recognize
           out in the kitchen!”                                  them. While joke generation involves limited world
   Joke2: “Diane: I want to go to Tibet on our honeymoon.        knowledge, joke recognition requires a much more
           Sam: Of course, we will go to bed.”1                  extensive world knowledge.
   Sometimes it takes world knowledge to recognize which            To be able to recognize or generate jokes, a computer
word is subject to wordplay. For example, in Joke2, there is     should be able to “process” sequences of words. A tool for
a wordplay between “Tibet” and “to bed.” However, to un-         this activity is the N-gram, “one of the oldest and most
derstand the joke, the wordplay by itself is not enough, a       broadly useful practical tools in language processing”
world knowledge is required to “link” honeymoon with             (Jurafsky and Martin, 2000). An N-gram is a model that
“Tibet” and “to bed.”                                            uses conditional probability to predict Nth word based on N-
   A focused form of wordplay jokes is the Knock Knock           1 previous words. N-grams can be used to store sequences
joke. In Knock Knock jokes, wordplay is what leads to the        of words for a joke generator or a recognizer.
humor. The structure of the Knock Knock joke provides               N-grams are typically constructed from statistics obtained
pointers to the wordplay.                                        from a large corpus of text using the co-occurrences of
   A typical Knock Knock (KK) joke is a dialog that uses         words in the corpus to determine word sequence probabili-
wordplay in the punchline. Recognizing humor in a KK             ties (Brown, 2001). As a text is processed, the probability
joke arises from recognizing the wordplay. A KK joke can         of the next word N is calculated, taking into account end of
be summarized using the following structure:                     sentences, if it occurs before the word N.
      Line1: “Knock, Knock”                                         “The probabilities in a statistical model like an N-gram
      Line2: “Who’s there?”                                      come from the corpus it is trained on. This training corpus
      Line3: any phrase                                          needs to be carefully designed. If the training corpus is too
      Line4: Line3 followed by “who?”                            specific to the task or domain, the probabilities may be too
      Line5: One or several sentences containing one of the      narrow and not generalize well to new sentences. If the
      following:                                                 training corpus is too general, the probabilities may not do a
           Type1: Line3                                          sufficient job of reflecting the task or domain” (Jurafsky and
           Type2: a wordplay on Line3                            Martin, 2000).
           Type3: a meaningful response to Line3.                   A bigram is an N-gram with N=2, a trigram is an N-gram
   Joke3 is an example of Type1, Joke4 is an example of          with N=3, etc. A bigram model will use one previous word
Type2, and Joke5 is an example of Type3.                         to predict the next word, and a trigram will use two previous
                                                                 words to predict the word.
   Joke3: Knock, Knock
           Who’s there?
           Water                                                                    Experimental Design
           Water who?                                            A further tightening of the focus was to attempt to recognize
           Water you doing tonight?                              only Type1 of KK jokes. The original phrase, in this case
   Joke4: Knock, Knock                                           Line3, is referred to as the keyword.
           Who’s there?                                             There are many ways of determining “sound alike” short
           Ashley                                                utterances. The only feasible method for this project was
           Ashley who?                                           computationally building up “sounds like” utterances as
           Actually, I don’t know.
   Joke5: Knock, Knock                                           needed.
           Who’s there?                                             The joke recognition process has four steps:
           Tank                                                       Step1: joke format validation
           Tank who?                                                  Step2: generation of wordplay sequences
           You are welcome.2                                          Step3: wordplay sequence validation
   From theoretical points of view, both Raskin’s (1985) and          Step4: last sentence validation
Suls’ (1972) approaches can explain why Joke3 is a joke.            Once Step1 is completed, the wordplay generator gener-
Following Raskin’s approach, the two belong to different         ates utterances, similar in pronunciation to Line3. Step3 only
                                                                 checks if the wordplay makes sense without touching the
1
                                                                 rest of the punchline. It uses a bigram table for validation.
  Joke1, Joke2 are taken from TV show “Cheers”                   Only meaningful wordplays are passed to Step4 from Step3.
2
  http://www.azkidsnet.com/JSknockjoke.htm
                                                             1316

   If the wordplay is not in the end of the punchline, Step4          from Frisch’s table or assigned a value close to the average
takes the last two words of the wordplay, and checks if they          of Frisch’s similarity values. The Similarity Table should be
make sense with the first two words of text following the             taken as a collection of heuristic satisficing values that
wordplay in the punchline, using two trigram sequences. If            might be refined through additional iteration.
the wordplay occurs in the end of the sentence, the last two
words before the wordplay and the first two words of the                 Table 1: Subset of entries of the Similarity Table, showing
wordplay are used for joke validation. If Step4 fails, go back            similarity of sounds in words between different letters
to Step3 or Step2, and continue the search for another
meaningful wordplay.                                                                        a     e        0.23
   It is possible that the first three steps return valid results,                          e     a        0.23
but Step4 fails; in which case a text is not considered a joke                              e     o        0.23
                                                                                            en    e        0.23
by the Joke Recognizer.                                                                     k     sh       0.11
   The punchline recognizer is designed so that it does not                                 l     r        0.56
have to validate the grammatical structure of the punchline.                                r     m        0.44
Moreover, it is assumed that the Line5 is meaningful when                                   r     re       0.23
the expected wordplay is found, if it is a joke; and, that                                  t     d        0.39
Line5 is meaningful as is, if the text is not a joke. In other                              t     z        0.17
                                                                                            w     m        0.44
words, a human expert should be able to either find a                                       w     r        0.42
wordplay so that the last sentence makes sense, or conclude                                 w     wh       0.23
that the last sentence is meaningful without any wordplay.
It is assumed that the last sentence is not a combination of             When an utterance A is “read” by the wordplay generator,
words without any meaning.                                            each letter in A is replaced with the corresponding replace-
   The joke recognizer is to be trained on a number of jokes;         ment letter from the Similarity Table. Each new string is
and, tested on jokes, twice the number of training jokes. The         assigned its similarity with the original word A.
jokes in the test set are previously “unseen” by the                     All new words are inserted into a heap, ordered according
computer. This means that any joke, identical to the joke in          to their similarity value, greatest on top. When only one
the set of training jokes, is not included in the test set.           letter in a word is replaced, its similarity value is being
                                                                      taken from the Similarity Table. The similarity value of the
         Generation of Wordplay Sequences                             strings is calculated using the following heuristic formula:
Given a spoken utterance A, it is possible to find an utter-
ance B that is similar in pronunciation by changing letters                similarity of string = number of unchanged letters +
from A to form B. Sometimes, the corresponding utterances                  sum of similarities of each replaced entry from the table
have different meanings. Sometimes, in some contexts, the                Note, that the similarity values of letters are taken from
differing meanings might be humorous if the words were                the Similarity table. These values differ from the similarity
interchanged.                                                         values of strings.
   A repetitive replacement process is used for generation of            Once all possible one-letter replacement strings are found,
wordplay sequences. Suppose, a letter a1 from A is replaced           and inserted into the heap, according to the string similarity,
with b1 to form B. For example, in Joke3 if a letter ‘w’ in a         the first step is complete.
word ‘water’ is replaced with ‘wh’, ‘e’ is replaced with ‘a’,            The next step is to remove the top element of the heap.
and ‘r’ is replaced with ‘re’, the new utterance, ‘what are’          This element has the highest similarity with the original
sounds similar to ‘water’.                                            word. If this element can be decomposed into an utterance
   A table, containing combinations of letters that sound             that makes sense, this step is complete. If the element can-
similar in some words, and their similarity value was used.           not be decomposed, each letter of the string, except for the
The purpose of the Similarity Table is to help computation-           letter that was replaced originally, is being replaced again.
ally develop “sound alike” utterances that have different             All newly constructed strings are inserted into the heap
spellings. In this paper, this table will be referred to as the       according to their similarity. Continue with the process until
Similarity Table. Table 1 is an example of the Similarity             the top element can be decomposed into a meaningful
Table. The Similarity Table was derived from a table devel-           phrase, or all elements are removed from the heap.
oped by Frisch (1996). Frisch’s table contained cross-refer-             Consider Joke3 as example. The joke fits a typical KK
enced English consonant pairs along with a similarity of the          joke pattern. The next step is to generate utterances similar
pairs based on the natural classes model. Frisch’s table was          in pronunciation to ‘water.’
heuristically modified and extended to the Similarity Table              Table 2 shows some of the strings received after one-letter
by “translating” phonemes to letters, and adding pairs of             replacements of ‘water’ in Joke3. The second column shows
vowels that are close in sound. Other phonemes, translated            the similarity of the string in the first table with the original
to combinations of letters, were added to the table as needed         word.
to recognize wordplay from a set of training jokes.                      Suppose, the top element of the heap is ‘watel,’ with the
   The resulting Similarity Table approximately shows the             similarity value of 4.56. Watel cannot be decomposed into
similarity of sounds between different letters or between             a meaningful utterance. This means that each letter of
letters and combination of letters. A heuristic metric indi-          ‘watel’ except for ‘l’ will be replace again. The newly
cating how closely they sound to each other was either taken          formed strings will be inserted into the heap, in the order of
                                                                  1317

their similarity value. The letter ‘l’ will not be replaced as it       The wordplay recognizer queries the bigram table. The
not the ‘original’ letter from ‘water.’ The string similarity        joke recognizer, discussed in section on Joke Recognition,
of newly constructed strings will be most likely less than 4.        queries the trigram table.
(The only way a similarity of a newly constructed string is             To construct the database several focused large texts were
greater than 4 is if the similarity of the replaced letter is        used. The focus was at the core of the training process.
above 0.44, which is unlikely.) This means that they will be         Each selected text contained a wordplay on the keyword
placed below ‘wazer.’ The next top string, ‘mater,’ is re-           (Line3) and two words from the punchline that follow the
moved. ‘Mater’ is a word. However, it does not work in the           keyword from at least one joke from the set of training
sentence ‘Mater you doing.’ (See Sections on Wordplay                jokes. If more than one text containing a given wordplay
Recognition and Joke Recognition for further discussion.)            was found, the text with the closest overall meaning to the
The process continues until ‘whater’ is the top string. The          punchline was selected. Arbitrary texts were not used, as
replacement of ‘e’ in ‘whater’ with ‘a’ will result in               they did not contain a desired combination of wordplay and
‘whatar’. Eventually, ‘whatar’ will become the top string, at        part of punchline.
which point ‘r’ will be replaced with ‘re’ to produce                   To construct the bigram table, every pair of words occur-
‘whatare’. ‘Whatare’ can be decomposed into ‘what are’ by            ring in the selected text was entered into the table.
inserting a space between ‘t’ and ‘a’. The next step will be            The concept of this wordplay recognizer is similar to an
to check if ‘what are’ is a valid word sequence.                     N-gram. For a wordplay recognizer, the bigram model is
                                                                     used.
    Table 2: Examples of strings received after replacing               The output from the wordplay generator was used as input
     one letter from the word ‘water’ and their similarity           for the wordplay recognizer. An utterance produced by the
                        value to ‘water’                             wordplay generator is decomposed into a string of words.
     New String         String Similarity to ‘Water’                 Each word, together with the following word, is checked
     watel              4.56                                         against the database.
     mater              4.44                                            An N-gram determines for each string the probability of
     watem              4.44                                         that string in relation to all other strings of the same length.
     rater              4.42
     wader              4.39
                                                                     As a text is examined, the probability of the next word is
     wather             4.32                                         calculated. The wordplay recognizer keeps the number of
     watar              4.23                                         occurrences of word sequence, which can be used to calcu-
     wator              4.23                                         late the probability. A sequence of words is considered valid
     whater             4.23                                         if there is at least one occurrence of the sequence anywhere
     wazer              4.17                                         in the text. The count and the probability are used if there is
                                                                     more than possible wordplay. In this case, the wordplay
  Generated wordplays that were successfully recognized              with the highest probability will be considered first.
by the wordplay recognizer, and their corresponding key-                For example, in Joke3 ‘what are’ is a valid combination if
words are stored for the future use of the program. When             ‘are’ occurs immediately after ‘what’ somewhere in the text.
the wordplay generator receives a new request, it first
checks if wordplays have been previously found for the re-                                 Joke Recognition
quested keyword. The new wordplays will be generated                 A text with valid wordplay is not a joke if the rest of the
only if there is no wordplay match for the requested key-            punchline does not make sense. For example, if the
word, or the already found wordplays do not make sense in            punchline of Joke3 is replaced with “Water a text with valid
the new joke.                                                        wordplay,” the resulting text is not a joke, even though the
                                                                     wordplay is still valid. Therefore, there has to be a
                  Wordplay Recognition                               mechanism that can validate that the found wordplay is
A wordplay sequence is generated by replacing letters in the         “compatible” with the rest of the punchline and makes it a
keyword. The keyword is examined because: if there is a              meaningful sentence.
joke, based on wordplay, a phrase that the wordplay is based            A concept similar to a trigram was used to validate the
on will be found in Line3. Line3 is the keyword. A                   last sentence. All three-word sequences are stored in the
wordplay generator generates a string that is similar in             trigram table.
pronunciation to the keyword. This string, however, may                 The same training set was used for both the wordplay and
contain real words that do not make sense together. A                joke recognizers. The difference between the wordplay
wordplay recognizer determines if the output of the                  recognizer and joke recognizer was that the wordplay
wordplay generator is meaningful.                                    recognizer used pairs of words for its validation while the
  A database with the bigram table was used to contain               joke recognizer used three words at a time. As the training
every discovered two-word sequence along with the number             text was read, the newly read word and the two following
of their occurrences, also referred to as count. Any sequence        words were inserted into the trigram table. If the newly read
of two words will be referred to as word-pair. Another               combination was in the table already, the count was
table in the database, the trigram table, contains each three-       incremented.
word sequence, and the count.                                           As the wordplay recognizer had already determined that
                                                                     the wordplay sequences existed, there was no reason to re-
                                                                     validate the wordplay.
                                                                 1318

   To check if wordplay makes sense in the punchline, the            expected to be recognized because the program is not
last two words of the wordplay, wwp1 and wwp2, are used, for         expected to recognize their structure.
the wordplay that is at least two words long. If the punch-             The program was able to find wordplay in 85 jokes, but
line is valid, the sequence of wwp1, wwp2, and the first word        recognized only seventeen jokes as such out of 122 that it
of the remainder of the sentence, ws, should be found in the         could potentially recognize. Twelve of these jokes have the
training text. If the sequence <wwp1 wwp2 ws> occurs in the          punchlines that matched the expected punchlines. Two
trigram table, this combination is found in the training set,        jokes have meaningful punchlines that were not expected.
and the three words together make sense. If the sequence is          Three jokes were identified as jokes by the computer, but
not in the table, either the training set is not accurate, or the    their punchlines do not make sense to the investigator.
wordplay does not make sense in the punchline. In either                Some of the jokes with found wordplay were not recog-
case, the computer does not recognize the joke. If the pre-          nized as jokes because the database did not contain the
vious check was successful, or if the wordplay has only one          needed sequences. When a wordplay was found, but the
word, the last check can be performed. The last step in-             needed sequences were not in the database, the program did
volves the last word of the word play, wwp, and the first two        not recognize the jokes as jokes.
words of the remainder of the sentence, ws1 and ws2. If the             In many cases, the found wordplay matched the intended
sequence <wwp ws1 ws2> occurs in the trigram table, the              wordplay. This suggests that the rate of successful joke
punchline is valid, and the wordplay fits with the rest of the       recognition would be much higher if the database contained
final sentences.                                                     all the needed word sequences.
   If the wordplay recognizer found several wordplays that              The program was also run with 65 non-jokes. The only
“produced” a joke, the wordplay resulting in the highest             difference between jokes and non-jokes was the punchline.
trigram sequence probability was used.                               The punchlines of non-jokes were intended to make sense
                                                                     with Line3, but not with the wordplay of Line3. The non-
                   Results and Analysis                              jokes were generated from the training joke set. The
A set of 65 jokes from the “111 Knock Knock Jokes” web-              punchline in each joke was substituted with a meaningful
site3 and one joke taken from “The Original 365 Jokes, Puns          sentence that starts with Line3. If the keyword was a name,
& Riddles Calendar” (Kostick, et al., 1998) was used as a            the rest of the sentence was taken from the texts in the
training set. The Similarity Table, discussed in the Section         training set. For example, Joke6 became Text1 by replacing
on Generation of Wordplay Sequences, was modified with               “time for dinner” with “awoke in the middle of the night.”
new entries until correct wordplay sequences could be                          Joke6: Knock, Knock
generated for all 66 jokes. The training texts inserted into                          Who’s there?
the bigram and trigram tables were chosen based on the                                Justin
punchlines of jokes from the set of training jokes.                                   Justin who?
   The program was run against a test set of 130 KK jokes,                            Justin time for dinner.
and a set of 65 non-jokes that have a similar structure to the                 Text1: Knock, Knock
KK jokes.                                                                             Who’s there?
                                                                                      Justin
   The test jokes were taken from “3650 Jokes, Puns & Rid-                            Justin who?
dles” (Kostick, et al. 1998). These jokes had the punchlines                          Justin awoke in the middle if the night.
corresponding to any of the three KK joke structures
discussed earlier.                                                      A segment “awoke in the middle of the night” was taken
   To test if the program finds the expected wordplay, each          from one of the training texts that was inserted into the
joke had an additional line, Line6, added after Line5. Line6         bigram and trigram tables.
is not a part of any joke. It only existed so that the wordplay         The program successfully recognized 62 non-jokes.
found by the joke recognizer could be compared against the
expected wordplay. Line6 consists of the punchline with the                              Possible Extensions
expected wordplay instead of the punchline with Line3.               The results suggest that most jokes were not recognized
   The jokes in the test set were previously “unseen” by the         either because the texts entered did not contain the neces-
computer. This means that if the book contained a joke,              sary information for the jokes to work; or because N-grams
identical to the joke in the set of training jokes, this joke        are not suitable for true “understanding” of text. One of the
was not included in the test set.                                    simpler experiments may be to test to see if more jokes are
   Some jokes, however, were very similar to the jokes in            recognized if the databases contain more sequences. This
the training set, but not identical. These jokes were in-            would require inserting a much larger text into the trigram
cluded in the test set, as they were not the same. As it             table. A larger text may contain more word sequences,
turned out, some jokes to a human may look very similar to           which would mean more data for N-grams to recognize
jokes in the training set, but treated as completely different       some jokes.
jokes by the computer.                                                  It is possible that no matter how large the inserted texts
   Out of 130 previously unseen jokes the program was not            are, the simple N-grams will not be able to “understand”
expected to recognize eight jokes. These jokes were not              jokes. The simple N-grams were used to understand or to
                                                                     analyze the punchline. Most jokes were not recognized due
                                                                     to failures in sentence understanding. A more sophisticated
3
  http://www.azkidsnet.com/JSknockjoke.htm
                                                                     tool for analyzing a sentence may be needed to improve the
                                                                 1319

joke recognizer. Some of the options for the sentence ana-       jokes contains (keyword, wordplay) pair that is needed for
lyzer are an N-gram with stemming or a sentence parser.          the new joke, but the two words that follow or precede the
   A simple parser that can recognize, for example, nouns        keyword in the punchline differ, the new joke may not be
and verbs; and analyze the sentence based on parts of            recognized regardless of how close the new joke and the
speech, rather than exact spelling, may significantly im-        previously recognized jokes are.
prove the results. On the other hand, giving N-grams the            The joke recognizer was trained on 66 KK jokes; and
stemming ability would make them treat, for example,             tested on 130 KK jokes and 66 non-jokes with a structure
“color” and “colors” as one entity, which may significantly      similar to KK jokes.
help too.                                                           The program successfully found and recognized wordplay
   The wordplay generator produced the desired wordplay in       in most of the jokes. It also successfully recognized texts
most jokes, but not all. After the steps are taken to improve    that are not jokes, but have the format of a KK joke. It was
the sentence understander, the next improvement should be        not successful in recognizing most punchlines in jokes. The
a more sophisticated wordplay generator. The existing            failure to recognize punchline is due to the limited size of
wordplay generator is unable to find wordplay that is based      texts used to build the trigram table of the N-gram database.
word longer than six characters, and requires more that three       While the program checks the format of the first four lines
substitutions. A better answer to letter substitution is pho-    of a joke, it assumes that all jokes that are entered have a
neme comparison and substitution. Using phonemes, the            grammatically correct punchline, or at least that the punch-
wordplay generator will be able to find matches that are         line is meaningful. It is unable to discard jokes with a
more accurate.                                                   poorly formed punchline. It may recognize a joke with a
   The joke recognizer may be able to recognize jokes other      poorly formed punchline as a meaningful joke because it
than KK jokes, if the new jokes are based on wordplay, and       only checks two words in the punchline that follow Line3.
their structure can be defined. However, it is unclear if           In conclusion, the method was reasonably successful in
recognizing jokes with other structures will be successful       recognizing wordplay. However, it was less successful in
with N-grams.                                                    recognizing when an utterance might be valid.
              Summary and Conclusion                                                          References
Computational work in natural language has a long history.       Attardo, S. (1994) Linguistic Theories of Humor. Berlin: Mouton de
Areas of interest have included: translation, understanding,        Gruyter
database queries, summarization, indexing, and retrieval.        Binsted, K. (1996) Machine Humour: An Implemented Model Of Puns.
                                                                    Doctoral dissertation, University of Edinburgh
There has been very limited success in achieving true com-       Frisch, S. (1996) Similarity And Frequency In Phonology. Doctoral
putational understanding.                                           dissertation, Northwestern University
   A focused area within natural language is verbally ex-        Hetzron, R. (1991) On The Structure Of Punchlines. HUMOR:
pressed humor. Some work has been achieved in computa-              International Journal of Humor Research, 4:1
tional generation of humor. Little has been accomplished in      Jurafsky, D., & Martin, J. (2000) Speech and Language Processing, New
                                                                    Jersey: Prentice-Hall
understanding. There are many linguistic descriptive tools       Kostick, A., Foxgrover, C., & Pellowski, M. (1998) 3650 Jokes, Puns &
such as formal grammars. But, so far, there are not robust          Riddles. New York: Black Dog & Leventhal Publishers
understanding tools and methodologies.                           Latta, R. (1999) The Basic Humor Process. Berlin: Mouton de Gruyter
   The KK joke recognizer is the first step towards compu-       Lessard, G., & Levison, M. (1992) Computational Modelling Of Linguistic
                                                                    Humour: Tom Swifties. ALLC/ACH Joint Annual Conference, Oxford
tational recognition of jokes. It is intended to recognize KK    McDonough, C. (2001) Mnemonic String Generator: Software To Aid
jokes that are based on wordplay. The recognizer’s                  Memory Of Random Passwords. CERIAS Technical report, West
theoretical foundation is based on Raskin’s Script-based            Lafayette, IN
Semantic Theory of Verbal Humor that states that each joke       McKay, J. (2002) Generation Of Idiom-based Witticisms To Aid Second
is compatible with two scripts that oppose each other. The          Language Learning. Proceedings of Twente Workshop on Language
                                                                    Technology 20, University of Twente
Line3 and the wordplay of Line3 are the two scripts. The         Raskin, V. (1985) The Semantic Mechanisms Of Humour, Dordrecht:
scripts overlap in pronunciation, but differ in meaning.            Reidel
   The joke recognition process can be summarized as:            Ritchie, G. (1999) Developing The Incongruity-Resolution Theory.
                                                                    Proceedings of AISB 99 Symposium on Creative Language: Humour
          Step1: joke format validation                             and Stories, Edinburgh
          Step2: generation of wordplay sequences                Ritchie, G. (2000) Describing Verbally Expressed Humour. Proceedings of
          Step3: wordplay sequence validation                       AISB Symposium on Creative and Cultural Aspects and Applications of
          Step4: last sentence validation                           AI and Cognitive Science, Birmingham
                                                                 Stock, O., & Strapparava, C. (2002) Humorous Agent For Humorous
   The result of KK joke recognizer heavily depends on the          Acronyms: The HAHAcronym Project. Proceedings of Twente
choice of appropriate letter-pairs for the Similarity Table         Workshop on Language Technology 20, University of Twente
and on the selection of training texts.                          Suls, J. (1972) A Two-Stage Model For The Appreciation Of Jokes And
                                                                    Cartoons: An Information-Processing Analysis. In J. H. Goldstein and P.
   The KK joke recognizer “learns” from the previously rec-         E. McGhee (Eds.) The Psychology Of Humor NY: Academic Press
ognized wordplays when it considers the next joke. Unfor-        Takizawa, O., Yanagida, M., Ito, A., & Isahara, H. (1996) On
tunately, unless the needed (keyword, wordplay) pair is an          Computational Processing Of Rhetorical Expressions - Puns, Ironies
exact match with one of the found (keyword, wordplay)               And Tautologies. Proceedings of Twente Workshop on Language
pairs, the previously found wordplays will not be used for          Technology 12, University of Twente
                                                                 Yokogawa, T. (2002) Japanese Pun Analyzer Using Articulation
the joke. Moreover, if one of the previously recognized             Similarities. Proceedings of FUZZ-IEEE, Honolulu
                                                             1320

