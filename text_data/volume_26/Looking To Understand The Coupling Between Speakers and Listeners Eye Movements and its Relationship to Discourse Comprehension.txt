UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Looking To Understand: The Coupling Between Speakers’ and Listeners’ Eye Movements
and its Relationship to Discourse Comprehension
Permalink
https://escholarship.org/uc/item/6pk1d8t6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Richardson, Daniel C.
Dale, Rick
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

           Looking To Understand: The Coupling Between Speakers’ and Listeners’
                   Eye Movements and its Relationship to Discourse Comprehension
                                    Daniel C. Richardson (richardson@psych.stanford.edu)
                                               Department of Psychology, Stanford University
                                                         Stanford, CA 94305, USA
                                                     Rick Dale (rad28@cornell.edu)
                                                Department of Psychology, Cornell University
                                                          Ithaca, NY 14853, USA
                             Abstract                                  discussing a diagram drawn on a whiteboard, figuring out
                                                                       together how to do something on a computer, or talking
While their eye movements were being recorded, participants            during a movie.
spoke extemporaneously about a TV show whose cast members                 Uniquely poised between perception and cognition, eye
they were viewing. Later, other participants listened to these
                                                                       movements can reveal cognitive processes such as speech
speeches while their eyes were tracked. Within this naturalistic
paradigm using spontaneous speech, a number of results linking
                                                                       planning, language comprehension, memory, mental
eye movements to speech comprehension, speech production               imagery and decision making. The current experiment
and memory were replicated. More importantly, a cross-                 investigates whether the eye movements of a speaker and a
recurrence analysis demonstrated that speaker and listener eye         listener to a visual common ground can provide insight into
movements were coupled, and that the strength of this                  a discourse.
relationship positively correlated with listeners’ comprehension.
Just as the mental state of a single person can be reflected in        Eye movement Research
patterns of eye movements, the commonality of mental states
that is brought about by successful communication is mirrored          Eye movements of a speaker
in a similarity between speaker and listener’s eye movements.          If a speaker is asked to describe a simple scene, they will
                                                                       fixate the objects in the order in which they are mentioned,
                         Introduction                                  around 900ms before naming them (Griffin & Bock, 2000;
                                                                       Meyer, Sleiderink, & Levelt, 1998). Since such pictures can
Imagine standing in front of a painting, discussing it with a
                                                                       be identified rapidly, it is argued that during this time
friend. As you talk, your eyes will scan across the image,             speakers are not just retrieving words but selecting and
moving approximately three times a second. They will be                planning which to use.
drawn by characteristics of the image itself, areas of contrast        Eye movements of a listener
or detail, as well as features of the objects or people                Eye movement research has shown that there is a tight
portrayed. Eye movements are driven both by properties of
                                                                       interdependence between speech recognition and visual
the visual world and processes in a person’s mind. Your
                                                                       perception. Eye movements to potential referents for a word
gaze might also be influenced by what your friend is saying,
                                                                       can provide evidence for a lexical item being recognized
what you say in reply, what is thought but not said, and
                                                                       before the word is finished being spoken. The link between
where you agree and disagree. If this is so, what is the
                                                                       visual and linguistic processing can also be seen in eye
relationship between your eye movements and those of your
                                                                       movements that disambiguate syntactic structures
friend? How is that relationship related to the flow of
                                                                       (Tanenhaus, Spivey Knowlton, Eberhard, & Sedivy, 1995)
conversation between you?                                              and anticipate the future agents of actions (Kamide,
   Language use often occurs within rich visual contexts               Altmann, & Haywood, 2003). Recent studies of the eye-
such as this, and the interplay between linguistic processes           movements of a participant engaged in a conversation with
and visual perception is of increasing interest to                     another naïve participant reveal a remarkable sensitivity to
psycholinguists and vision researchers (Henderson &
                                                                       the referential domains established by the task, the visual
Ferreira, 2004). As yet, however, such processes have been
                                                                       context and the preceding conversation (Brown-Schmidt,
limited to experiments that examine the eye movements of
                                                                       Campana, & Tanenhaus, 2004). Qualitatively, eye
the speaker or the listener in isolation. Language use, more
                                                                       movement research reveals a very close, time-locked
often than not, occurs within a richer social context as well.
                                                                       integration between visual and linguistic processing
   Direct eye contact between conversants plays an
                                                                       (Tanenhaus, Magnuson, Dahan, & Chambers, 2000).
interesting, crucial role in coordinating a conversation
                                                                       Although fixation times are heavily modulated by context,
(Bavelas, Coates, & Johnson, 2002), and in conveying
                                                                       as a very rough quantitative guide, research suggests that
various attitudes or social roles (Argyle & Cook, 1976). The           listeners will fixate an object around 400-800ms after the
focus of the current experiment, however, is cases such as             name onset.
those introduced at the outset, where conversants are not
looking at each other, but at some visual scene that is the
topic of the conversation. More common examples might be
                                                                   1143

Eye movements of a thinker                                          Participants
Since participants will make systematic eye movements to            40 Stanford undergraduates took part in the experiment in
entirely empty and uninformative regions of space when              exchange for course credit.
retrieving information from memory (Richardson &                    Apparatus
Kirkham, in press; Richardson & Spivey, 2000) or listening          An ASL 504 remote eye tracking camera was positioned at
to a story (Spivey & Geng, 2001) it is clear that they can be       the base of a 17” LCD stimulus display. Participants were
governed by cognitive as well as perceptual processes.              unrestrained, and sat approximately 30” from the screen.
Influencing how the eye moves across an image can have              The camera detected pupil and corneal reflection position
profound effects on mental processes. Researchers have              from the right eye, and the eye tracking PC calculated point-
recorded the eye movements of participants interpreting an          of-gaze in terms of co-ordinates on the stimulus display.
ambiguous picture in a particular way, or solving a difficult       This information was passed every 33ms to a PowerMac G4
deductive problem from a diagram. Using low level visual            which controlled the stimulus presentation and collected
cues, a second set of participants were then influenced to          looking time data. Prior to the experimental session, the
attend to the same regions of the picture. The second set of        participants went through a 9 point calibration routine,
participants were more like to form the same interpretation         which typically took between 2 and 5 minutes.
of the ambiguous picture (Pomplun, Ritter, & Velichkovsky,             Speakers’ voices were recorded by microphone, and
1996), and remarkably, were more likely to solve the                listeners made responses using the two buttons of a mouse
deductive problem (Grant & Spivey, 2003). If forced                 held in their lap.
similarity between participants’ eye movements can result in        Design – Speakers
similar cognitive states, then will the similar cognitive states    The intention was to record participants speaking
that are brought about by successful verbal communication           spontaneously about a TV show while looking at a picture
result in similar eye movement patterns between speaker             of the cast members. In the first case, a picture of the 6
and listener?                                                       principal characters of the cast of the TV sitcom Friends
                                                                    was used. The characters were shown individual in 6
                        Experiment                                  separate pictures. Potential speakers were asked if they
Speech production and speech comprehension have                     knew they show and would like to talk about it, and two
previously been studied in separate eye tracking paradigms.         speakers were selected who were knowledgeable and
Yet if both are indeed closely linked to eye movements, the         reasonably gregarious. Speakers were instructed to ‘Talk
eye movement patterns of two people engaged in a natural,           about the show for a couple of minutes. You could talk
unscripted conversation may bare some relationship to each          about the relationships between the characters, your opinion
other. Moreover, it raises the intriguing possibility that the      of them, or your favourite episode’. In the second case, two
strength of the relationship between conversants’ eye               participants were shown a 5 minute scene from T h e
movements will parallel the success of their linguistic             Simpsons during which they undergo family therapy. These
relationship.                                                       participants were then shown a picture of the five family
   The current experiment approximates a conversation               members and their therapist. The participants were asked to
between naïve conversants by asking participants to speak           ‘Describe what went on in the scene and what you thought
spontaneously, with neither a script nor a rehearsal for an         about it’.
extended period of time about a TV show, whose characters              As they spoke, the speakers’ eye movements were tracked
were displayed in front of them. These speeches were then           and their voices were recorded by microphone. These
played back to other participants who were looking at the           recordings were trimmed so that they were all roughly one
same display. Crucially, both the speakers’ and the listeners’      minute long, and the text was transcribed for later analysis.
eye movements were tracked throughout. The listeners’               Design - Listeners
comprehension was then measured by a series of content              Participants listened while looking at the same picture of the
questions. Thus in addition to extending various eye                six cast members that had been in front of the speaker. Since
movement-language results to natural, spontaneous speech,           there could not be systematic looks to the cast members if
the current experiment was able to investigate a number of          the participant did not recognize any of them, participants
entirely novel hypotheses regarding the linkage between             were first asked if they were familiar with either show. On
speaker and listener eye movements, and its relation to the         this basis, the listeners were presented with one or both of
listener’s comprehension.                                           the Friends and Simpsons stimuli, and were randomly
                                                                    assigned one of the two speakers.
Methods                                                                Listeners heard a minute of speech, and then a screen
                                                                    appeared warning them that the question period was about
The first four participants recruited to take part in this
                                                                    to start. In the four question trials, participants saw six solid
experiment were designated as speakers, and the remainder
                                                                    grey circles or squares in the locations where pictures of the
were listeners. The methods for both stages will be
described below.                                                    individual cast members had previously appeared. After a
                                                                    1000ms pause, they heard a question and responded yes or
                                                                1144

                                                                                                       EYE MOVEMENTS
no using the two mouse buttons. There followed a 2000ms                     TIME (s)    SPEECH
                                                                                                    SPEAKER    LISTENER
ISI during which the screen was blank.
   The questions were recorded by the experimenter and                                              Phoebe
                                                                                      uh
were of the form, “Did the speaker say…?”. The questions                                                        Chandler
were designed such that they could not be answered on the                             what
                                                                                47                  Monica
basis of knowledge about Friends or The Simpsons alone,                               else
but were specific to the information mentioned (or not) by                            oh
that particular speaker. The correct answer to half the                               Monica        Chandler
questions was yes and half no.                                                  48
                                                                                      and                       Phoebe
Data Coding                                                                           Chandler
Roughly half of our listeners were familiar with both TV                                            Monica
                                                                                                                Monica
shows and half knew the characters from only one. All                           49
                                                                                      are
analyses are based on 49 usable listener-speaker dyads. A
further 9 cases were dropped due to problems with the                                 sister
                                                                                                    Chandler
equipment or the calibration procedure.                                               and
                                                                                                                Chandler
                                                                                      brother
   The eye movements of the speaker and of the listener                         50
                                                                                                                Phoebe
during the minute of speech were analyzed in exactly the                                                        Monica
same way. The eye movement data relayed which, if any, of
                                                                                                                Ross
the six pictures were being fixated every 33ms. The data                        51
                                                                                                    Joey
were cleaned for blinks and saccades across a picture - only                          so                        Chandler
stable fixations longer than 99ms were analyzed – and then                                          Monica
                                                                                                                Monica
expressed in terms of a sequences of gaze onsets and offsets                                        Ross
                                                                                      tha'
in the six pictures.                                                            52
                                                                                      oh
                                                                                                                Chandler
                                                                                      no            Monica
   The speakers’ recordings were transcribed with onset                               R...
times for each word spoken. In addition, words were flagged                           Monica
                                                                                                                Monica
if they were names of any of the six characters pictured.                       53    and
                                                                                      Ross
Listener responses to the questions were coded for accuracy,                          are
                                                                                                    Chandler    Ross
                                                                                                    Ross
and their looking times to each of the pictures while                                 sister
answering were calculated.                                                            and
                                                                                      brother
                                                                                54
                                                                                                                Monica
Results and Discussion
                                                                                      nevermind                 Joey
This experiment provided precise timing information about                                           Chandler    Monica
speakers’ speech and gaze onsets, and listeners’ gaze onsets.                   55
                                                                                                    Chandler
This information can depicted graphically in what we call a                                         Phoebe      Ross
‘scarf plot’, which represents a transcript of the speech
together with the timing of word onsets and the eye                     Figure 1. Scarf Plot of a 9 second segment of one dyad.
movements of both speaker and listen. Figure 1 shows an                The speaker’s words are shown on the left, with nouns
nine second segment of a scarf plot for one speaker-listener          highlighted. The speaker’s and listener’s eye movements
dyad. Such eye movement data can be statistically analyzed            are shown in the middle and right columns respectively.
and compared with the objective measure of the listeners’                  Time is on the y axis, increasing down the page.
understanding of the speech provided by their performance          Speaker Fixations Prior To Naming
answering four comprehension questions.                            For each occasion that the speaker named character X, their
   Before the detailed inferential analyses begin, it is useful    eye movement data were consulted to find the point at
to get a rough sense of the behavior being studied. On             which X was last previously fixated. The difference between
average, speakers used 160 words, only 12 of which were            the gaze onset and the name onset was computed for every
the names of the characters depicted. It is important to note      name used by every speaker. On average, a character was
that the speeches were not edited for content, and include all     fixated 860 ms prior to being named.
the deviations, hesitations and repetitions that are typical of       This lag is exactly in the range reported by the speech
just a minute of normal, spontaneous speech.                       production literature (Griffin & Bock, 2000), where
   Speakers and listeners switched their gaze between              typically participants are explicitly instructed to describe a
pictures around 120 times. For each occasion, they spent           simple picture. We have found a lag of the same magnitude
about 500ms looking at the picture. Since the average eye          with spontaneous, natural speech, when participants are
fixation lasts 200-300ms, it is reasonable to assume that this     describing not what is front of them per se, but things that
represents two fixations within the same picture.                  are not depicted - stories, opinions, relationships – that
                                                                   relate to those characters.
                                                               1145

                                                                   were successively lagged by 330ms. On the line defined by
                                                                   i = j in the plot (the line of incidence), any point indicates
                                                                   that in the same temporal context fixations are recurrent.
                                                                   Thus, by lagging the listeners’ time series, and recording
                                                                   maximal recurrence along the line of incidence within each
                                                                   lag, we get a measure of the extent to which dyads’ eye
                                                                   movements are related. Though our chosen window size is
                                                                   small, the results are quite compelling.
                                                                      Figure 3 shows the degree of recurrence between speaker
                  Figure 2. Example CRPs                           and listener at different time lags, averaged across all 49
Relationship Between Speaker And Listener Eye                      dyads. We also randomized listeners’ eye movement data
Movements                                                          and calculated its recurrence with the speakers’. This
To what degree were speaker and listener looking at the            randomized series serves as a baseline of looking ‘at
same thing at the same time?                                       chance’ at any given point in time, but with the same overall
   We quantified this question by generating categorical           distribution of looks to each picture as the real listeners.
cross-recurrence plots between the speaker and listener time            A 2 (listeners/randomized listener) x 40 (lag times)
series of fixations (Dale & Spivey, in submission). These          ANOVA revealed a significant main effect of listener type
plots permit visualization and quantification of recurrent         (F(1,45)=785.5, p<.0001) and a main effect of lag
patterns of states between two time series (see Shockley,          (F(40,1800)=25.2, p<.001). Moreover, there was a
Santana & Fowler, 2003, for a fuller introduction; see             significant interaction between the factors
Eckmann, Kamphorst & Ruelle, 1987; Zbilut & Webber,                (F(40,1800)=24.7, p<.001).
1992 for foundational treatises). In our case, the cross-             Clearly, the real listeners are not looking around these
recurrence plot portrays the extent to which dyad fixations        displays randomly. Rather their eye movements are linked
are overlapping temporally.                                        to the speakers’, and this relationship has a temporal
   To begin, windows of a given length are moved along             character. More precisely, the maximum recurrence between
each time series, forming individual windows at every time         the speakers and listeners, the lag time at which their eye
index. The windows of each time series are then compared           movements overlap the most, is at 1650ms
to all those of the other time series (comparing every time           These results are exactly what one would expect from the
index). At time index i for the first time series and j for the    combination of the speech production and speech
second, if their windows are sufficiently similar, a point (i,     comprehension eye movement literature. Typically,
j) is recorded on a two-dimensional plot. By comparing             speakers will fixate an item 900ms before naming it and
every window in the first to the second time series, we can        listeners will fixate an object around 800ms after the name
generate a full plot of points in which the two time series are    onset. Very roughly this would suggest we would find a lag
close to each other – a cross-recurrence plot.                     of 900+800=1700ms between speaker gaze onsets and
   For simplicity, we used a window size of 1 for our              listeners’. This derived value corresponds both to the exact
analysis. By using a categorical metric (see Dale & Spivey,        lag that produces a maximum recurrence value, 1650ms,
in submission, for details), we have the criterion that dyad       and the generalCross
                                                                                      region  of higher recurrence in the 1000-
                                                                                          Recurrence at different time lags
fixations are recurrent if falling on the same object for          2000ms range.
                                                                                         22
33ms. We generated plots using this metric between every                                                                        Speaker - Listener
speaker-listener pair. Figure 2 shows example cross                                                                             Speaker - Randomized
recurrence plots between a speaker and (a) a listener who                                                                       listener
                                                                                         20
answered all comprehension questions correctly (b) a
listener who answered few correctly, and (c) a listener with
their eye movement data placed in a random order. There
                                                                                         18
are three things to notice here. Firstly, the good listener has
                                                                  %REC
higher density in their plot, indicating more points of
recurrence with the speaker. Secondly, both listeners have
                                                                                         16
more structured plots compared to the randomized series.
Lastly, one can see that for the two real listeners there is a
higher density in the region on and below the i=j diagonal.
                                                                                         14
This indicates that the speaker and listeners’ eye movements
overlapped more when the listeners’ eye movements lagged
behind the speakers.
   We employed a further analysis to find out exactly what                               12
                                                                         -4000   -2000        0   2000   4000   6000     8000   10000   12000    14000   16000
temporal lag between the listener and the speaker would                                                    time of lag (ms)
produce the greatest degree of recurrence, or overlap,
between their eye movement patterns. Listener time series                        Figure 3. Cross recurrence at different time lags
                                                              1146

   The speech production and comprehension literatures,                                              40
however, deal with cases where an object or person is                                                35
explicitly named. Perhaps it is the case that the differences
between critical and non-critical gaze onset lag distributions                                       30
observed here are due mainly to the occasions when the                                               25
speaker planned and spoke out loud a name of one of the
                                                                                              %REC
                                                                                                     20
characters pictured.
   This question was addressed by examining a subset of the                                          15
data. The name-subset includes only speaker fixations to
                                                                                                     10
person X that were immediately prior to the speech onset of
name X. As noted previously, since there were on average                                              5
12 cases of name use, this constituted about 10% of the 120                                           0
fixations the average speaker made.                                                                       0            1            2                 3             4            5
   Figure 4A plots the recurrence at different time lags for                                                     Number of comprehension questions answered correctly
the name subset of our data. The 2 (listeners/randomized                                         Figure 5. Correlation Between Speaker-Listener Eye
listener) x 40 (lag times) ANOVA revealed a significant                                          Movements Coupling and Listener Comprehension
main effect of listener type (F(1,45)=192.3, p<.0001) and a
                                                                                       not just the when the speaker names a character that speaker
main effect of lag (F(40,1800)=28.1, p<.001). As before,
                                                                                       and listener eye movements are linked. It must be other
there was a significant interaction between the factors
                                                                                       properties of the discourse (implicit reference, anaphor,
(F(40,1800)=27.5, p<.001).
                                                                                       topics, agents, for example) which drive the speakers eye
    For the subset of speaker fixations that precede a name,
                                                                                       movements while they are being planned, and a few seconds
there is a highly pronounced difference between the speaker
                                                                                       later, influence the listener’s eye movements once they are
and the listener and the speaker and randomized looking.
                                                                                       spoken.
Once more, the greatest extent of this difference is just
before 2000ms. Again, this would be predicted by the                                   Speaker-Listener Eye Movement Linkage and Listener
speech production and comprehension eye movement                                       Comprehension
literatures. Is it the case, then, that the current experiment                         The degree to which eye movements were linked in a given
has simply replicated these name-use results using                                     speaker-listener dyad were compared with the listener’s
spontaneous speech?                                                                    comprehension of what had been said. For each dyad, we
   To answer this question the data excluded from the name                             computed the degree of recurrence (REC%) at a lag of
analysis above were analyzed in isolation. Figure 4B plots                             1650ms between speaker and listener. This is the lag that
the ‘non name dataset’ that corresponds to the 90% of                                  produced the greatest recurrence across our whole data set,
speaker fixations to person X which were not immediately                               and hence serves as a baseline to compare the linkage
followed by X being named out loud. The ANOVA showed                                   between individual speaker-listener dyads. The performance
a similar pattern of results: main effect of listener type                             of listeners answering four comprehension questions was
(F(1,45)=559, p<.0001), a main effect of lag                                           taken as an objective measure of how well they had
(F(40,1800)=25.8, p<.001). and a significant interaction                               comprehended the one minute of speech.
between the factors (F(40,1800)=25.0, p<.001). Although                                   A regression analysis was performed on this data, and
subtracting the cases of name use from the full data set                               found that a linear fit had r 2=0.14. Although it may not
appeared to attenuate somewhat the differences between                                 account for a large portion of the variance in participants’
critical and non-critical gaze onset lags, it is certainly the                         behaviour, an ANOVA shows that this relationship is
case that theseNouns  - Cross Recurrence at different time lags
                  distributions   still differ. In other words, it is                               Non - Nouns: Cross Recurrence at different time lags
                                                                                       significant (F(1,47)=7.39,     p<.01).
                           8                                                                                          18
               B                                       Speaker - Listener                             A                                                   Speaker - Listener
               A                                       Speaker - Randomized                               B                                               Speaker - Ramdomized
                                                       Listener                                                                                           listener
                           6                                                                                          16
%REC                                                                                  %REC
                           4                                                                                          14
                           2                                                                                          12
                           0                                                                                          10
       -4000       -2000       0   2000         4000    6000         8000     10000          -4000            -2000        0      2000         4000         6000          8000       10000
                                    time of lag (ms)                                                                               time of lag (ms)
                           Figure 4. Cross recurrence at different time lags for (a) name fixations, (b) non-name fixations
                                                                                 1147

                     General Discussion                               In J. C. Trueswell & M. K. Tanenhaus (Eds.), World-
                                                                      situated language processing: Bridging the language as
The current experiment uses a naturalistic paradigm that
                                                                      product and language as action traditions. Cambridge:
elicits and presents spontaneous speech. The language-use
                                                                      MIT Press.
in this experiment is grounded in the visual items presented
                                                                    Clark, H. H. (1996). Using language. Cambridge:
on the display, but is not a description of them per se, or an
                                                                      Cambridge University Press.
explicit instruction relating to their presence or appearance.
                                                                    Dale, R. & Spivey, M. J. (submitted). Data visualization of
Nevertheless, this single paradigm replicates several results
                                                                      complex behavioral structure across time. Manuscript
obtained in more constrained circumstances concerning the
                                                                      submitted for publication.
relationship between eye movements, speech production,
                                                                    Eckmann, J.-P., Kamphorst, S.O., Ruelle, D. (1987).
and speech comprehension.
                                                                      Recurrence lots of dynamical systems. Europhysics
   More importantly, this experiment provides what could be
                                                                      Letters, 5, 973-977.
the first demonstration that during the production and
                                                                    Grant, E. R., & Spivey, M. J. (2003). Eye movements and
comprehension of a spontaneous discourse, the eye
                                                                      problem solving: Guiding attention guides thought.
movements of a speaker and a listener are coupled.
                                                                      Psychological Science, 14(5), 462-466.
Moreover, this relationship between eye movement patterns
                                                                    Griffin, Z. M., & Bock, K. (2000). What the eyes say about
is not driven by cases in which the speaker explicitly names
                                                                      speaking. Psychological Science, 11(4), 274-279.
people who are depicted. It seems to be that the planning of
                                                                    Henderson, J. M., & Ferreira, F. (Eds.). (2004). The
more diverse types of reference and foregrounding may be
                                                                      integration of language, vision, and action: Eye
influencing the speaker’s eye movements, and, a few
                                                                      movements and the visual world. New York: Psychology
seconds later via the speech stream, influencing the
                                                                      Press.
listener’s eye movements. Crucially, the strength of
                                                                    Kamide, Y., Altmann, G. T. M., & Haywood, S. L. (2003).
relationship between the speaker’s and the listener’s eye
                                                                      The time-course of prediction in incremental sentence
movements appears to predict the degree to which the
                                                                      processing: Evidence from anticipatory eye movements.
listener successfully comprehended the speech.
                                                                      Journal of Memory & Language, 49(1), 133-156.
   Instances of new paradigms such as this inevitably raise
                                                                    Meyer, A. S., Sleiderink, A. M., & Levelt, W. J. M. (1998).
many questions for future research. Is it the case that a tight
                                                                      Viewing and naming objects: Eye movements during
coupling between speaker and listener eye movements is an
                                                                      noun phrase production. Cognition, 66(2), B25-B33.
overall indication of listener attentiveness, which also
                                                                    Pomplun, M., Ritter, H., & Velichkovsky, B. (1996).
predicts listener comprehension? Or is it that by rapidly
                                                                      Disambiguating complex visual information: Towards
bringing their eyes to bear on the same item as the speaker,
                                                                      communication of personal views of a scene. Perception,
good listeners receive appropriate visual information that
                                                                      25(8), 931-948.
supports the verbal input? Or perhaps it is not so much that
                                                                    Richardson, D. C., & Kirkham, N. Z. (in press). Multi-
moving the eyes closely in step with a speaker brings in
                                                                      modal events and moving locations: evidence for dynamic
visual content, but rather it is an indication (or a cause) that
                                                                      spatial indexing in adults and six month olds. Journal of
the listener is using spatial information to cognitively
                                                                      Experimental Psychology: General.
structure the information in the same way as the speaker?
                                                                    Richardson, D. C., & Spivey, M. J. (2000). Representation,
   The close relationship between speaker and listener eye
                                                                      space and Hollywood Squares: looking at things that
movements and the success of the discourse clearly aligns
                                                                      aren’t there anymore. Cognition, 76, 269-295.
with a view of language use as a joint activity (Clark, 1996),
                                                                    Shockley, K., Santana, M. V. & Fowler, C.A. (2003).
in which successful communication is brought about by a
                                                                      Mutual interpersonal postural constraints are involved in
successful coordination of information in the common
                                                                      cooperative conversation. Journal of Experimental
ground. The human eye only receives detailed information
                                                                      Psychology: Human Perception and Performance, 29,
from 2º of its visual field: therefore, if the speaker and
                                                                      326-332.
listener are looking at exactly the same thing, then they are
                                                                    Spivey, M. J., & Geng, J. J. (2001). Oculomotor
certainly sharing a higher, common ground.
                                                                      mechanisms activated by imagery and memory: Eye
                                                                      movements to absent objects. Psychological
                    Acknowledgments                                   Research/Psychologische Forschung, 65(4), 235-241.
The authors are indebted to Natasha Kirkham, Herb Clark             Tanenhaus, M. K., Magnuson, J. S., Dahan, D., &
and Michael Spivey.                                                   Chambers, C. (2000). Eye movements and lexical access
                         References                                   in spoken-language comprehension: Evaluating a linking
                                                                      hypothesis between fixations and linguistic processing.
Argyle, M., & Cook, M. (1976). Gaze and Mutual Gaze.                  Journal of Psycholinguistic Research, 29(6), 557-580.
   Cambridge: Cambridge University Press.                           Tanenhaus, M. K., Spivey Knowlton, M. J., Eberhard, K.
Bavelas, J. B., Coates, L., & Johnson, T. (2002). Listener            M., & Sedivy, J. C. (1995). Integration of visual and
   responses as a collaborative process: The role of gaze.            linguistic information in spoken language comprehension.
   Journal of Communication, 52(3), 566-580.                          Science, 268(5217), 1632-1634.
Brown-Schmidt, S., Campana, E., & Tanenhaus, M. K.                  Zbilut, J. P. & Webber, C. L., Jr. (1992). Embeddings and
   (2004). Real-time reference resolution by naïve                    delays as derived from quantification of recurrence plots.
   participants during a task-based unscripted conversation.          Physics Letters A, 171, 199-203.
                                                                1148

