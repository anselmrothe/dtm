UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Attachment Decisions with a Probabilistic Parser: The Case of Head Final
Structures
Permalink
https://escholarship.org/uc/item/2mh5273c
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Baldewein, Ulrike
Keller, Frank
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Modeling Attachment Decisions with a Probabilistic Parser:
                                           The Case of Head Final Structures
               Ulrike Baldewein (ulrike@coli.uni-sb.de)                       Frank Keller (keller@inf.ed.ac.uk)
           Computational Psycholinguistics, Saarland University           School of Informatics, University of Edinburgh
                        D-66041 Saarbrücken, Germany                       2 Buccleuch Place, Edinburgh EH8 9LW, UK
                               Abstract                                 be attached either to a noun phrase or a verb. In German, PP
   We describe an incremental, two-stage probabilistic model of         attachment ambiguities can occur in two syntactic configura-
   human parsing for German. The model is broad coverage, i.e.,         tions: in verb second sentences, the verb precedes the NP and
   it assigns sentence structure to previously unseen text with         the PP as it does in English (see (1)).
   high accuracy. It also makes incremental predictions of the at-      (1) Iris tröstete den Jungen mit dem Lied.
   tachment decisions for PP attachment ambiguities. We test the
   model against reading time data from the literature and find               Iris comforted the boy        with the song.
   that it makes correct predictions for verb second sentences;               ‘Iris comforted the boy with the song.’
   however, the model is not able to account for reading times          (2) (daß) Iris den Jungen mit dem Lied tröstete.
   data for verb final structures because attachment preferences
   in our training data do not match those determined experimen-              (that) Iris the boy      with the song comforted.
   tally. We argue that this points to more general limitations with          ‘(that) Iris comforted the boy with the song.’
   our type of probabilistic model when it comes to realizing pro-      In verb final sentences (which occur as subordinate clauses),
   cessing strategies that are independent of the data the parsing      the NP and the PP precede the verb (see (2)). As sentence
   model is trained on.                                                 processing is incremental, this means that an attachment de-
                                                                        cision has to be made before parser reaches the verb (and the
                           Introduction                                 frequency information associated with it). These structures
Experimental results show that human sentence processing                therefore provide an interesting challenge for probabilistic
is sensitive to different types of frequency information, in-           models of sentence processing.
cluding verb frame frequencies (e.g., Garnsey et al. 1997),                Reading studies (e.g., Konieczny et al. 1997, whose mate-
frequencies of morphological forms (e.g., Trueswell 1996),              rials we use) have shown that in verb second sentences, the
and structural frequencies (e.g., Brysbaert & Mitchell 1996).           PP is preferredly attached according to the subcategorization
Probabilistic parsing models are an attractive way of account-          bias of the verb (as in English). In verb final sentences, where
ing for this fact, as they provide a theoretically sound way of         verb frame information cannot be accessed until the end of
combining different sources of frequency information into a             the sentence, the PP is preferentially attached to the NP site.
coherent model. Typically, these models are hybrid models,
combining symbolic knowledge (e.g., phrase structure rules)                                        The Model
with frequency information (e.g., rule probabilities gleaned            Our parsing model consists of two modules: one is a syntactic
from a corpus).                                                         module based on a probabilistic parser, which also has access
   In particular, probabilistic parsers have been used success-         to a probabilistic verb frame lexicon. This module guarantees
fully to model attachment decisions in human sentence pro-              broad coverage of language data and a high accuracy in pars-
cessing. Early models demonstrated the viability of the prob-           ing unseen text. The other module is a semantic module that
abilistic approach by focusing on a small selection of relevant         uses probabilistic information to estimate the plausibility of
syntactic constructions (Jurafsky 1996; Hale 2001). More re-            the analyses proposed by the syntactic module.
cently, broad coverage models have been proposed (Crocker                  The model uses a syntax-first processing strategy: The syn-
& Brants 2000; Sturt et al. 2003) that can deal with unre-              tactic module proposes a set of analyses for the input and
stricted text. These models are able to account for the ease            ranks them by probability. The semantic module then com-
with which humans understand the vast majority of sentences,            putes the semantic plausibility of the analyses and ranks them
while at at the same time making predictions for sentences              by plausibility score. If there is a conflict between the deci-
that trigger processing difficulties.                                   sions made by the two modules (i.e., the top-ranked analyses
   However, existing probabilistic models deal exclusively              differ), this is interpreted as a conflict between syntactic pref-
with English data, and thus fail to address the challenges              erence and semantic plausibility and increased processing ef-
posed by the processing of head final constructions in lan-             fort is predicted.
guages such as Japanese (e.g., Kamide & Mitchell 1999) or
German (e.g., Konieczny et al. 1997). In this paper, we ad-             Syntactic Module
dress this problem by presenting a probabilistic model of               Modeling Syntactic Preferences The syntactic module
human sentence processing in German. The model is broad                 consists of a probabilistic left-corner parser which relies on a
coverage, i.e., it generates accurate syntactic analyses for un-        probabilistic context free grammar (PCFG) as its backbone. A
restricted text. Furthermore, it makes predictions for PP at-           PCFG consists of a set of context-free rules, where each rule
tachment ambiguities for both head initial and head final sen-          LHS → RHS is annotated with a probability P(RHS|LHS).
tences. The model consists of two probabilistic modules: a              This probability represents the likelihood of expanding the
syntactic module that proposes an initial attachment, and a se-         category LHS to the categories RHS. In order to obtain a
mantic module that evaluates the plausibility of the proposed           mathematically sound model, the probabilities for all rules
attachment, and corrects it if necessary.                               with the same left hand side have to sum to one. The proba-
   We evaluate our model on reading time data for PP attach-            bility of a parse tree T is defined as the product of the proba-
ment, i.e., for structures in which a prepositional phrase can          bilities of all rules applied in generating T .
                                                                     73

                        S       →         NE VVFIN.n.p NP PP                  .3
                        S       →         NE VVFIN.n NP                       .7        and modifiers of the main verb are its sisters, and all are di-
                       NP       →         ART NN                              .4        rect daughters of the S node (see Figure 2). This means that
                       NP       →         ART NN PP                           .6
                       PP       →         APPR ART NP                        1.0        scrambling phenomena simply alter the sequence of sisters in
               VVFIN.n          →         tröstete                            .8        the tree, and do not involve movement and traces.
              VVFIN.n.p         →         tröstete                            .2
                    ART         →         den                                 .5           We checked the PP attachment preferences in Negra and
                    ART         →         dem                                 .5        found that in 60% of all sentences containing a verb and an
                       NE       →         Iris                               1.0
                      NN        →         Jungen                              .6        NP object followed by a PP, the PP is attached to the verb.
                      NN        →         Lied                                .4        The corpus therefore reflects a general attachment preference
                   APPR         →         mit                                1.0
                                                                                        for verb attachment. Additionally, we found that the subcate-
                   Figure 1: Example of a PCFG                                          gorization preferences for the verbs in our materials were re-
                                                                                        versed with regard to the preferences obtained by Konieczny
                               S                                                        et al. (1997) in a sentence completion task: the verbs that had
                                                                                        a bias towards the NP-PP frame in the corpus exhibited an
                                                                                        NP frame bias in the completion study, and vice versa.
  NE          VVFIN.n.p                     NP                          PP                 For all subsequent experiments, Negra was split into three
  Iris         tröstete              ART          NN
                                                                                        subsets: the first 90% of the corpus were used as training set,
                                                       APPR            ART       NN     the remainder was divided into a 5% test set and a 5% de-
                                      den      Jungen                                   velopment set (used during model development). Sentences
                                                         mit           dem      Lied
             P(T1 ) = .3 · 1.0 · .2 · .4 · .5 · .6 · 1.0 · 1.0 · .5 · .4 = .00144       with more than 40 words were removed from the data sets (to
                        S                                                               increase parsing efficiency).
                                                                                           The syntactic module was realized based on Lopar
                                                                                        (Schmid 2000), a probabilistic parser using a left-corner pars-
        NE          VVFIN.n                    NP                                       ing strategy. A grammar and a lexicon were read off the Ne-
        Iris         tröstete                                                           gra training set, after empty categories and function labels
                                                                                        had been removed from the trees. Then the parameters for the
                        ART                    NN                   PP                  model were estimated using maximum likelihood estimation.
                        den                 Jungen                                      This means that the probability of a rule LHS → RHS is esti-
                                                     APPR          ART        NN        mated as P(LHS → RHS) = f (LHS → RHS)/N, which is the
                                                       mit         dem        Lied      number of times the rule occurs in the training data over the
             P(T2 ) = .7 · 1.0 · .8 · .6 · .5 · .6 · 1.0 · 1.0 · .5 · .4 = .02016       total number of rules in the training data. Various smoothing
                                                                                        schemes are implemented in Lopar to address data sparse-
       Figure 2: Example of trees generated by a PCFG                                   ness, see Schmid (2000) for details. We also complemented
                                                                                        the Negra verb frame counts with frame probabilities from an
    An example for a PCFG is given in Figure 1. This grammar                            existing subcategorization lexicon (Schulte im Walde 2002),
contains the rules required to generate the two readings of (1).                        as the Negra counts were sparse.
The readings are displayed in Figure 2, which also lists the
parse probabilities, obtained by multiplying the probabilities                          Semantic Module
of the rules used to generated a given tree.                                            Modeling Semantic Plausibility The semantic module
    This examples illustrates how PCFGs can be used for dis-                            determines whether an attachment decision proposed by
ambiguation: the two readings involve different rules (and                              the syntactic module is semantically plausible by deciding
rule probabilities), and therefore differ in their overall prob-                        whether the PP is more likely to be semantically related to
abilities. In this example, reading T2 is predicted to be pre-                          the preceding verb or to the preceding noun.
ferred over T1 . Note that the grammar in Figure 1 incorporates                            Our semantic model rests on the assumption that “semantic
verb frame probabilities: tröstete ‘consoled’ can either be a                           plausibility” or “semantic relatedness” can be approximated
VVFIN.n (finite verb with an NP complement) or VVFIN.n.p                                by probabilistic measures estimated from corpus frequen-
(finite verb with an NP and a PP complement). The proba-                                cies. Previous work provided evidence for this assumption by
bilities attached to these lexical items correspond to the psy-                         demonstrating that co-occurrence frequencies obtained from
cholinguistic notion of verb frame bias, i.e., the probability of                       various corpora (including the web) are reliability correlated
the verb occurring with a given subcategorization frame. The                            with human plausibility judgments (Keller & Lapata 2003).
overall probability of an analysis is determined not only by                            Training and Test Data Ideally, the same training data
verb frame bias, but also by structural probabilities attached                          should be used for the syntactic and the semantic module;
to the phrase structure rules. This is a way of modeling struc-                         however, this was not possible, as the semantic module re-
tural disambiguation preferences (in this example, there is a                           quires vastly more training data. We therefore used the web to
bias for attachment to the NP). A PCFG therefore provides                               estimate the parameters of the frequency based measures (see
a principled way of integrating lexical preferences and struc-                          Keller & Lapata 2003 for a detailed evaluation of the reliabil-
tural preference, as argued by Jurafsky (1996).                                         ity of web counts). For the selectional preference method, we
Training and Test Data A PCFG is typically trained on a                                 used one year’s worth of text from the ECI Frankfurter Rund-
syntactically annotated corpus. For German, a suitable cor-                             schau corpus as training data. This unannotated corpus is the
pus is available in the form of Negra (Skut et al. 1997), a                             basis for the Negra corpus, but it is much larger (34 million
350,000 word corpus of newspaper text. The Negra annota-                                words). The corpus was parsed using a parser very similar
tion scheme assumes flat syntactic structures in order to ac-                           to the syntactic module. Tuples of verbs and head nouns of
count for free word order in German. For example, there is no                           modifying PPs were then extracted according to the structures
VP node dominating the main verb. Instead, subject, objects                             assigned by the parser.
                                                                                     74

   The development and test set for the semantic module were                 As will be explained below, we experimented with these mea-
taken from the set of 156 items from Experiments 1 and 2 of                  sures in isolation, but we also combined them with Clark &
Konieczny et al. (1997). The development set consists of 68                  Weir’s (2002) approach for computing selectional preference
randomly chosen sentences, the remaining 88 sentences are                    from corpora. This approach relies on a lexical data base to
used as a test set. The items from Experiment 1 vary word                    compute the semantic similarity between lexical items.
order (verb second and verb final), verb subcategorization
preference (bias for NP frame or for NP-PP frame), and at-
tachment (to the NP or verb), which is disambiguated by the                                              Results
semantic implausibility of one alternative. In Experiment 2,
verb subcategorization preference was not varied. The devel-                 Syntactic Module
opment set was used to compare the performance of different
semantic and syntactic models and to set the parameters for                  As mentioned in the introduction, the present modeling effort
one semantic models. The final performance will be reported                  was guided by the idea of building a broad coverage model,
on the unseen test set.1                                                     i.e., a model that explains why human sentence processing
                                                                             is effortless and highly accurate for the vast majority of sen-
Plausibility Measures In computational linguistics, a stan-                  tences; at the same time, the model should account for psy-
dard approach to PP attachment disambiguation is the use                     cholinguistically interesting phenomena such as processing
of configuration counts from corpora (e.g., Hindle & Rooth                   difficulties arising from attachment ambiguities. Incremental-
1991). To decide the attachment of nPP , the head noun of the                ity is crucial for predictions of this type. In its original form,
PP, to one of the attachment sites (the verb v or nNP , the noun             the Lopar parser used for the syntactic module is not incre-
phrase), we compare how probable each attachment is based                    mental and was therefore modified to achieve partial incre-
on previously seen configurations involving nPP and the at-                  mentality. It now outputs its ranking of the attachment alter-
tachment sites. In many approaches, the the preposition p is                 natives in two stages: after processing the PP and at the end of
also taken into account.                                                     the sentence. This provides a record of incremental changes
   As outlined above, we used web counts to mitigate the                     in the attachment preferences of the model when processing
data sparseness that such a model is faced with. In this ap-                 the critical region for which Konieczny et al. (1997) report
proach, corpus queries are replaced by queries to a search                   eye-movement data (the noun of the PP in Experiment 1 and
engine, based on the assumption that the number of hits that                 the PP object in Experiment 2).
the search engine returns is an approximation of the web fre-
                                                                                To evaluate the broad coverage of the model, we ran the
quency of the word in question (Keller & Lapata 2003). Of
                                                                             syntactic module on our unseen Negra corpus test set. The
course text on the web is not parsed, which makes it diffi-
                                                                             model was able to assign an analysis to 98% of the sentences.
cult to identify the correct syntactic configurations. We follow
                                                                             As is standard in computational linguistics, we tested the ac-
Volk (2001) in assuming that string adjacency approximates
                                                                             curacy of the model by measuring labeled bracketing: to score
syntactic attachment reasonably well, and simply use queries
                                                                             a hit, the parser has to predict both the bracket (the beginning
of the form "V PP" and "NP PP". The search engines used
                                                                             or end of a phrase) and the category label correctly. We report
were www.altavista.de and www.google.com (restricted
                                                                             labeled recall, the number of correctly labeled brackets found
to German data). Google generally outperformed AltaVista
                                                                             by the parser divided by the total number of labeled brack-
(presumably because it indexes more pages); the results re-
                                                                             ets in the test corpus, and labeled precision, the number of
ported below were obtained using Google counts.
                                                                             correctly labeled brackets found by the parser divided by the
   We experimented with a variety of plausibility measures                   total number of labeled brackets found by the parser.
(site ranges over the two attachment sites, v and nNP ):
                                                                                The model achieved a labeled recall of 66.65% and a la-
        f (site,p)
(a)       f (site) ,    the Lexical Association Score (LA), computes         beled precision of 63.92%. It is similar to the baseline model
       how likely the attachment site is to be modified by a PP              of Dubey & Keller (2003), who report a maximum labeled
       with the preposition p.                                               recall and precision of 71.32% and 70.93%.
(b)     f (site, p, nPP ), Model 1 of Volk (2001), relies on the raw            To further evaluate the syntactic model, we tested it on the
       trigram co-occurrence frequencies to decide attachment.               test set generated from Experiments 1 and 2 of Konieczny
        f (site,p,nPP )                                                      et al. (1997). This allows us to determine whether the syn-
(c)           f (site) , Model 2 of Volk (2001), takes into account
                                                                             tactic module is able to correctly resolve the PP attachment
       that high-frequency attachment sites are more likely to               ambiguities even without access to any semantic information.
       co-occur   with PPs.
                                                                                Table 1 shows the parser’s decisions at the PP for verb final
                                     
                      f (site,nPP )
(d)    log2 f (site)         f (nPP ) , Pointwise Mutual Information (MI)    and verb second sentences. We report the number and the per-
       measures how much information about one of the items                  centage of correct attachments per condition. In the verb final
       is gained when the other is seen. This measure has pre-               condition of Experiment 1, the parser always attached the PP
       viously been used for the related problem of identifying              to the verb. No verb frame information is available to guide
       collocations (words that appear together more often than              the decision when the PP is processed, so the baseline is ran-
       chance, Church & Hanks 1990).                                         dom guessing (50%). In verb second sentences, the parser can
        f (site,nPP ) f (site,nPP )                                          use the subcategorization preference of the verb, which leads
(e)         f (site) · f (nPP ) , Combined Conditional Probabilities
       (CCP) is similar to MI. It squares the joint probability              to the correct attachment in 50% of all cases. The parser in-
       term to give it more weight.                                          deed reaches this baseline. In Experiment 2, the parser again
                                                                             always attaches the PP to the unseen verb in verb final sen-
    1 Since for all models except one no parameters were set on the          tences. In the verb second condition, there is a marked prefer-
development set, we had to maintain a fixed development-test split           ence to attach according to verb bias, but only 42% of attach-
to ensure the test set remained truly unseen.                                ments are correct over both conditions.
                                                                          75

                                 Verb final  Verb second                             Prior                 Average
        Experiment 1                                                    Measure      CCP       CCP       MI     Volk1     Volk2
        NP frame, V bias         7 (100%)      2 (29%)                  Development Set
        NP frame, NP bias              0       5 (83%)                  # correct     12        11       8         9        11
        NP-PP frame, V bias      5 (100%)      3 (60%)                  % correct    60%       55%      40%      45%       55%
        NP-PP frame, NP bias           0       2 (33%)                  Test Set
        % correct                    50%         50%                    # correct     24        20       22       23        25
        Experiment 2                                                    % correct 57.1%       47.6%    52.4%    54.8%     59.5%
        NP frame, V bias         9 (100%)      1 (11%)                  Baseline     50%       50%      50%      50%       50%
        NP frame, NP bias              0       5 (56%)
        % correct                    50%         33%               Table 3: Semantic Module, verb final: Results on the devel-
        Baseline                     50%         50%               opment (Exp. 1) and test set (Exp. 1 and 2)
Table 1: Syntactic module: correct attachment decisions at the
PP for the test set from Experiments 1 and 2                       more suitable for our task, while a variation of α value had no
                                                                   noticeable effect.
     Measure      CCP        MI         LA  Volk 1   Volk 2
     Development Set                                                  We used the development set to estimate a threshold value
     # correct      23       22         17    22       21          for the attachment decision. Coverage on the test set was only
     % correct 67.6%       64.7%       50%  64.7%    61.7%         48% due to sparse data. Whenever the Clark & Weir method
     Test Set                                                      did not return a value, we backed off to the decision made
     # correct      21       23          –    23       27          by the Volk 2 model (which is the most consistently perform-
     % correct     50%     54.8%         –  54.8%    64.3%         ing model). Recall that this model has a 64% precision on
     Baseline      50%      50%        50%   50%      50%          the test data while the chance baseline is 50%. The combined
Table 2: Semantic module, verb second: results of the plausi-      model reaches 67% precision on the same data (precision for
bility measures on the development and test set                    the selectional preference model alone is 70% for 48% of the
                                                                   data). This model performs best numerically (though not sig-
                                                                   nificantly so) and was used in the final model.
Semantic Module
                                                                   Verb Final Sentences A particularly interesting case arises
Verb Second Sentences As a next step, we evaluated the             with respect to verb final sentences (see (2)): at the critical
semantic module, again on the data derived from Experi-            region (once the PP has been processed), the verb is not avail-
ments 1 and 2 of Konieczny et al. (1997). We again used the        able yet, which means that the plausibility of the combination
chance baseline (50%) that the syntactic module was unable         of the verb with the head noun of the PP cannot be computed
to outperform.                                                     at this point. Konieczny et al. (1997) found processing diffi-
   The verb second sentences arguably constitute the standard      culty in these cases when the PP was an implausible modifier
case for PP attachment: Both possible attachment sites have        of the noun, so apparently immediate semantic evaluation sets
been seen before the attachment has to be decided. In a first      in and has to be accounted for.
attempt, the five plausibility measures introduced above were
                                                                      In the verb final case, we therefore have to estimate the
tested on the development set. Table 2 shows that the CCP
                                                                   plausibility of the PP head noun modifying the NP as opposed
measure performed best, while the Lexical Association mea-
                                                                   to an unseen verb. One way of doing this is to average over the
sure failed to beat the baseline. The CCP measure should
                                                                   results for the PP head noun and every possible verb to obtain
therefore be chosen to model semantic attachment in verb
                                                                   a generic value for verb attachment. We restrict ourselves to
second sentences. However, on the test set (see Table 2), the
                                                                   just the verbs in the test and development set. This backoff
best and worst measure changed places. This time, the Volk 2
                                                                   approach was realized for four models.
measure performed best. No measure significantly outper-
formed the others or the baseline.                                    An alternative is to use the prior probability of the PP head
   As the performance of the CCP measure on the test set was       noun as an estimate of its conditional probability with every
disappointing, we experimented with a second approach that         possible verb. The prior probability of the PP head noun is
combines the Volk 2 model of PP attachment with a model            its frequency divided by the size of the corpus, f (nPP /N). In
of selectional restrictions. We used Clark & Weir’s (2002)         the case of web counts, N is the number of all documents
approach, which was extended to German by Brockmann &              searched. This figure was empirically estimated as proposed
Lapata (2003), whose implementation we used. Relying on a          by Keller & Lapata (2003). Note that this method of backoff is
semantic hierarchy (in our case: GermaNet, Hamp & Feldweg          possible only for the CCP measure, because the probabilities
(1997)), the Clark & Weir algorithm finds the statistically op-    to be estimated for the other methods are too complex.
timal superclass (concept) for input nouns given a verb and           Table 3 gives the results on the development set for the
the relation between noun and verb. The probability of a con-      items from Experiment 1. The items from Experiment 2 could
cept c given a verb v and relation rel is computed as:             not be tested as the averaging procedure is extremely costly
                                                                   in terms of web queries. The CCP model with simple backoff
(3) P(c|v, rel) = P(v|c, rel) P(c|rel)
                               P(v|rel)
                                                                   to the prior shows the best results at 60% correct attachments.
                                                                   We therefore used it for the final evaluation to predict attach-
To find the best concept for a hn, v, reli triple, at each step    ments for verb final sentences. Table 3 also lists the results
up the hierarchy, the probability estimate for the new concept     on the test set for items from Experiments 1 and 2. CCP with
is compared to that of the original concept. When the esti-        backoff to the prior performed better than most models that
mates differ significantly, the lower concept is assumed for       use averaging, and substantially outperforms CCP with av-
the noun. The parameters of this algorithm are the statistical     eraging. The best model is Volk 2 with averaging. Again, no
test used (χ2 or G2 ) and the α value which determines the         measure outperforms the baseline of 50% correct attachments
level of significance required for the test. The G2 test proved    or the other models.
                                                                76

                                   100                                                                     2000
                                                          NP bias                                                                   NP bias
                                                         Verb bias                                                                 Verb bias
           % Corrected Decisions
                                   80                                                                      1800
                                                                                       Total RPDs (msec)
                                   60
                                                                                                           1600
                                   40
                                                                                                           1400
                                   20
                                                                                                           1200
                                    0
                                         NP frame           NP-PP frame                                           NP frame           NP-PP frame
                                                (our) Verb Type                                                              Verb Type
 Figure 3: Exp. 1, verb second: Predictions of the combined model (left) compared to the Konieczny et al. (1997) data (right)
                                   100                                                                     550
                                                                                                                                    NP bias
                                                        NP bias                                                                    Verb bias
           % Corrected Decisions
                                    80                 Verb bias                                           500
                                                                                       Total RPDs (msec)
                                    60                                                                     450
                                    40                                                                     400
                                    20                                                                     350
                                     0                                                                     300
                                         NP frame           NP-PP frame                                           NP frame           NP-PP frame
                                                (our) Verb Type                                                              Verb Type
  Figure 4: Exp. 1, verb final: Predictions of the CCP/Prior model (left) compared to the Konieczny et al. (1997) data (right)
Combined Model                                                                    In verb final sentences (Figure 4), the syntactic module al-
In the previous sections, we evaluated the syntactic and se-                   ways predicts verb attachment, so correct decisions for NP
mantic module separately. We found that the syntactic mod-                     attachment by the semantic module always lead to a con-
ule performs at the level of the chance baseline of 50%,                       flict. This pattern does not correspond to the Konieczny et al.
while the semantic module achieves an accuracy of up to                        (1997) reading data, which show a general preference to at-
67% for verb initial sentences and 60% for the verb final sen-                 tach to the NP. Figure 5 shows a replication in principle of
tences. A more interesting question is how well the model                      the reading time data in the verb second case. In Konieczny
accounts for the processing difficulties that are evident in the               et al.’s (1997) pretests, all the verbs subcategorized for an NP
eye-movement data reported by Konieczny et al. (1997). As                      and a PP, while in our data, they preferredly subcategorize
mentioned at the beginning of the Results Section, our model                   for just an NP. Our model predicts longer reading times for
makes predictions for the critical region used by Konieczny                    the NP frame when subcategorization preference and seman-
et al. (1997) (the PP). Recall also that we assume that a con-                 tic disambiguation are mismatched, which is what Konieczny
flict between syntactic preference and semantic plausibility                   et al.’s (1997) show for the NP-PP frame. The verb final
predicts increased processing effort.                                          case again fails: Instead of predicting preferred attachment
   As explained in the section on Training and Test Data                       to the NP (Matched bias for our data, Mismatched bias for
above, the subcategorization variable was reversed for our                     Konieczny et al.’s data), the model predicts verb attachment.
data: where Konieczny et al. (1997) assume an NP-PP frame
bias, we found a preference for the NP frame in our cor-                                                             Discussion
pus (and vice versa). Below, our model’s predictions are la-
beled with the preferences found in our data, while data from                  While our model replicates Konieczny et al.’s (1997) read-
Konieczny et al. (1997) are labeled with the preferences they                  ing time results for PP attachment in the verb second case, it
found. Figure 3 compares the predictions of our model with                     fails to account for reading times of verb final sentences. This
Konieczny et al.’s results in Experiment 1 for verb second                     failure is caused by the syntactic module which always pre-
                                                                               dicts verb attachment in verb final sentences, while there is a
sentences.2 The graph for our model gives the percentage of                    human preference for NP attachment in these cases.
correct decisions by the semantic module that are in conflict
with the the decisions of the syntactic module. Such con-                         The behavior of the syntactic module is influenced by two
flicts predict longer reading times, and the more conflicts in a               factors. One is the probability of phrasal rules such as S →
condition, the higher we expect the average reading times to                   NE VVFIN.n.p NP PP. The second factor is a verb-specific
be. The figure shows that our model predicts the data pattern                  frame bias, which manifests itself as probabilities for lexi-
found by Konieczny et al. (1997) (who report regression path                   cal rules such as VVFIN.n.p → tröstete. In verb second sen-
durations, RPDs).                                                              tences, the verb’s frame probability together with the phrasal
                                                                               rule probability determines the analysis proposed by the syn-
  2 Note that our results are on the unseen subset of the items only,          tactic module. In verb final sentences, however, only the
while the reading times are on all items.                                      phrasal probabilities are used (as the verb is not yet avail-
                                                                          77

                                  100                                                                                800
                                            Verb 2nd
                                            Verb final
                                                                                                                     700
          % Corrected Decisions
                                  80
                                                                                                 First RPDs (msec)
                                                                                                                     600
                                  60
                                                                                                                     500
                                  40
                                                                                                                     400
                                  20                                                                                 300
                                                                                                                               Verb 2nd
                                                                                                                               Verb final
                                   0                                                                                 200
                                            Matched bias       Mismatched bias                                                 Matched bias       Mismatched bias
                                        Match between Syntactic and Semantic Bias                                          Match between Syntactic and Semantic Bias
Figure 5: Exp. 2: Predictions of the combined model (left) compared to the Konieczny et al. (1997) data (right). Verbs subcate-
gorize for an NP frame in our data and for an NP-PP frame in the Konieczny et al. data.
able), so the syntactic module makes the same prediction for                                                                        References
all verb final sentences. This prediction is incorrect because                           Brockmann, C., & Lapata, M. (2003). Evaluating and combining
the general PP attachment bias in the corpus is to the verb,                                approaches to selectional preference acquisition. In Proc. EACL,
rather than to the NP as in the reading time data.                                          (pp. 27–34), Budapest.
                                                                                         Brysbaert, M., & Mitchell, D. C. (1996). Modifier attachment in sen-
   This points to a more general problem with probabilistic                                 tence parsing: Evidence from Dutch. Quaterly J. of Experimental
models: They can only be as good as the training data. It                                   Psychology, 49A, 664–695.
is therefore vital to check relevant properties of the training                          Church, K., & Hanks, P. (1990). Word association norms, mutual
corpus in comparison to experimental data when developing                                   information, and lexicography. Computational Linguistics, 16,
probabilistic models. Balanced corpora that consist of lan-                                 22–29.
                                                                                         Clark, S., & Weir, D. (2002). Class-based probability estimation
guage data from different sources are more reliable in this                                 using a semantic hierarchy. Computational Linguistics, 28, 187–
respect than newspaper corpora such as the Negra corpus.                                    206.
   This means that the failure to model the verb final data                              Crocker, M. W., & Brants, T. (2000). Wide-coverage probabilistic
is not a failure of probabilistic models per se; our approach                               sentence processing. J. of Psycholinguistic Research, 29, 647–
                                                                                            669.
would be in principle capable of modeling the general attach-                            Dubey, A., & Keller, F. (2003). Probabilistic parsing for German
ment preference to the NP in verb final sentences, if the at-                               using sister-head dependencies. In Proc. ACL, (pp. 96–103), Sap-
tachment preference in the training data corresponded to that                               poro.
in the experimental results. Thus, our results strengthen the                            Garnsey, S. M., Pearlmutter, N. J., Myers, E. M., & Lotocky, M. A.
                                                                                            (1997). The contributions of verb bias and plausibility to the com-
case for probabilistic models by showing that they can be ap-                               prehension of temporarily ambiguous sentences. J. of Memory
plied even to head final constructions.                                                     and Language, 37, 58–93.
   It is important to note, however, that our explanation of the                         Hale, J. (2001). A probabilistic earley parser as a psycholinguistic
German PP attachment data in terms of biases in the train-                                  model. In Proc. NAACL, Pittsburgh, PA.
                                                                                         Hamp, B., & Feldweg, H. (1997). GermaNet: A lexical-semantic
ing corpus is at variance with explanations in the literature.                              net for German. In P. Vossen, G. Adriaens, N. Calzolari, A. San-
For instance, Konieczny et al. (1997) proposes a strategy of                                filippo, & Y. Wilks (eds.), Proc. ACL/EACL Workshop on Auto-
Parameterized Head Attachment to explain why the parser                                     matic Information Extraction and Building of Lexical Semantic
prefers to attach incoming material (such as the PP) to exist-                              Resources for NLP Applications, (pp. 9–15), Madrid.
ing sites (such as the verb). This strategy, which aims at the                           Hindle, D., & Rooth, M. (1991). Structural ambiguity and lexical
                                                                                            relations. In Proc. ACL, (pp. 229–236), Berkeley, CA.
immediate semantic evaluation of the input, is designed to                               Jurafsky, D. (1996). A probabilistic model of lexical and syntactic
cope with head final structures in general, not only in the case                            access and disambiguation. Cognitive Science, 20, 137–194.
of PP attachment. A basic PCFG model such as the one used                                Kamide, Y., & Mitchell, D. C. (1999). Incremental pre-head attach-
in this paper is not able to implement such a general strategy.                             ment in Japanese parsing. Language and Cognitive Processes,
                                                                                            14, 631–662.
                                                                                         Keller, F., & Lapata, M. (2003). Using the web to obtain frequencies
                                              Conclusions                                   for unseen bigrams. Computational Linguistics, 29, 459–484.
                                                                                         Konieczny, L., Hemforth, B., Scheepers, C., & Strube, G. (1997).
We have presented a two-stage model parsing model that ac-                                  The role of lexical heads in parsing: Evidence from German. Lan-
counts for PP attachment in German. The model is able to                                    guage and Cognitive Processes, 12, 307–348.
assign correct sentence structures to unseen text and predicts                           Schmid, H. (2000). LoPar: Design and implementation. Unpubl.
average reading times in verb second sentences. For verb fi-                                ms., IMS, University of Stuttgart.
nal sentences, the model fails to correctly predict the reading                          Schulte im Walde, S. (2002). A subcategorisation lexicon for Ger-
                                                                                            man verbs induced from a Lexicalised PCFG. In Proc. LREC,
time data. The reason is that our training corpus exhibits a                                vol. IV, (pp. 1351–1357), Las Palmas, Gran Canaria.
general bias for attaching PPs to the wrong attachment site (to                          Skut, W., Krenn, B., Brants, T., & Uszkoreit, H. (1997). An an-
the verb instead of the NP). In principle, however, our model                               notation scheme for free word order languages. In Proc. ANLP,
would be able to account for the data in the verb final case if                             Washington, DC.
                                                                                         Sturt, P., Costa, F., Lombardo, V., & Frasconi, P. (2003). Learning
the training data were consistent with experimental findings.                               first-pass structural attachment preferences with dynamic gram-
Our findings therefore strengthen the case for probabilistic                                mars and recursive neural nets. Cognition, 88, 133–169.
models of language processing by showing their applicability                             Trueswell, J. C. (1996). The role of lexical frequency in syntactic
to head final structures. At the same time, they demonstrate                                ambiguity resolution. J. of Memory and Language, 35, 566–585.
                                                                                         Volk, M. (2001). Exploiting the WWW as a corpus to resolve PP
that probabilistic models can be highly sensitive to idiosyn-                               attachment ambiguities. In Proc. Corpus Linguistics, Lancaster.
crasies in the training data.
                                                                                    78

