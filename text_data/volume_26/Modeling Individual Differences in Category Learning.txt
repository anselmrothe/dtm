UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Individual Differences in Category Learning
Permalink
https://escholarship.org/uc/item/75m038zq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Webb, Michael R.
Lee, Michael D.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                               Powered by the California Digital Library
                                                                University of California

              Modeling Individual Differences in Category Learning
                         Michael R. Webb (michael.webb@dsto.defence.gov.au)
                   Command and Control Division, Defence Science and Technology Organisation
                                    Edinburgh, South Australia, 5111, AUSTRALIA
                               Michael D. Lee (michael.lee@adelaide.edu.au)
                                   Department of Psychology, University of Adelaide
                                           South Australia, 5005, AUSTRALIA
                         Abstract                                 The potential benefit of averaging data is that, if the
                                                               performance of subjects really is the same except for
   Many evaluations of cognitive models rely on data           ‘noise’ (i.e., variation the model is not attempting to
   that have been averaged or aggregated across all ex-        explain), the averaging process will tend to remove the
   perimental subjects, and so fail to consider the possi-     noise, and the resultant data will more accurately re-
   bility that there are important individual differences
   between subjects. Other evaluations are done at the         flect the underlying psychological phenomenon. When
   single-subject level, and so fail to benefit from the       the performance of subjects has genuine differences,
   reduction of noise that data averaging or aggrega-          however, it is well known (e.g., Estes 1956; Myung,
   tion potentially provides. To overcome these weak-          Kim, & Pitt 2000) that averaging produces data that
   nesses, we develop a general approach to modeling
   individual differences using families of cognitive mod-     do not accurately represent the behavior of individuals,
   els, where different groups of subjects are identified      and provide a misleading basis for modeling.
   as having different psychological behavior. Separate           Even more fundamentally, the practice of averaging
   models with separate parameterizations are applied          data restricts the focus of cognitive modeling to issues
   to each group of subjects, and Bayesian model selec-        of how people are the same. While modeling invariants
   tion is used to determine the appropriate number of
   groups. We demonstrate the general approach in a            is fundamental, it is also important to ask how people
   concrete and detailed way using the ALCOVE model            are different. Experimental data reveal individual dif-
   of category learning and data from four previously          ferences in cognitive processes, and in the psychological
   analysed category learning experiments. Meaningful          variables that control those processes, that also need
   individual differences are found for three of the four
   experiments, and ALCOVE is able to account for this         to be modeled.
   variation through psychologically interpretable differ-        Cognitive modeling that attempts to accommodate
   ences in parameterization. The results highlight the        individual differences usually assumes that each sub-
   potential of extending cognitive models to consider in-     ject behaves in accordance with a different parame-
   dividual differences.                                       terization of the same basic model, and so the model
                                                               is evaluated against the data from each subject sep-
                                                               arately (e.g, Ashby, Maddox, & Lee 1994; Nosofsky
                      Introduction                             1986; Wixted & Ebbesen 1997). Although this avoids
Much of cognitive psychology, as with other empiri-            the problem of corrupting the underlying pattern of
cal sciences, involves the development and evaluation          the data, it also foregoes the potential benefits of aver-
of models. Models provide formal accounts of the ex-           aging, and guarantees that models are fit to all of the
planations proposed by theories, and have been de-             noise in the data.
veloped to address diverse cognitive phenomena rang-              Another problem with individual subject analysis,
ing from stimulus representation (e.g., Shepard 1980),         from a model theoretic perspective, is that fitting each
to memory retention (e.g., Anderson & Schooler 1991;           additional subject requires an extra set of free parame-
Estes 1997), to category learning (e.g., Ashby & Per-          ters, and so leads to a progressively more complicated
rin 1988; Berretty, Todd, & Martignon 1999; Kruschke           accounts of the data as a whole. As has been pointed
1992; Tenenbaum 1999). One recurrent shortcoming of            out repeatedly in the psychological literature recently
these models, however, is that (whether intentionally,         (e.g., Myung & Pitt 1997; Pitt, Myung, & Zhang
or as an unintended consequence of methodology) hu-            2002), it is important both to maximize goodness-of-
mans are usually modeled as ‘invariants’, and not as           fit and minimize model complexity to achieve the basic
‘individuals’. This occurs because, most often, mod-           goals of modeling. Unnecessarily complicated models
els are evaluated against data that have been averaged         that “over-fit” data often do not provide any insight
or aggregated across subjects, and so the modeling as-         or explanation of the cognitive processes they address,
sumes that there are no individual differences between         and are less capable of making accurate predictions
subjects.                                                      when generalizing to new or different situations.
                                                        1440

   A better approach, therefore, is to partition subjects  rameterization θi , predicts a correct response proba-
according to their individual differences, and model the   bility of γij at the ith group of subjects on the jth
averaged or aggregated data from each group. Under         block. Then the likelihood of the data arising un-
this approach, data are addressed by a set of models,      der the model is given by the binomial distribution:
                                                                                        d              b k −d
called a model family, where a different parameteriza-     p (dij | Mi , θi ) = bdjijki γijij (1 − γij ) j i ij . The like-
tion is applied to each group of subjects. Where av-       lihood of a model family simply extends this result to
eraging is appropriate, within groups of subjects, it is   consider every block of trials and every partition, so
applied. Where averaging is not appropriate, between       that
groups of subjects, it is not applied.
   In this paper, we apply these ideas to model individ-
ual differences in category learning, using Kruschke’s                        Y Y bj ki  dij
(1992) well known, empirically successful, and widely        p (D | M) =                       γij (1 − γij )bj ki −dij . (1)
                                                                               i j
                                                                                        d ij
used ALCOVE model. Our basic approach, however,
is applicable to any model of category learning or, in-
deed, models of other cognitive phenomena.                 The extension of this likelihood function to more gen-
                                                           eral category learning experiments with more than two
                                                           possible category responses, using a multinomial dis-
     Modeling Individual Differences in                    tribution, is straightforward.
                 Category Learning                            Having defined the likelihood function, the Bayesian
Formally, a model family M partitions the subjects         Information Criterion (BIC: Schwarz 1978) can be ap-
S into G groups S → {S1 , . . . , SG }, and so parti-      plied to balance goodness-of-fit with the complexity of
tions the complete data D into G averaged data sets        a model family. The BIC is given by:
D → {D1 , . . . , DG }. For the ith data set, a model fam-
ily also specifies a model parameterization θi . Any pos-                BIC = −2 ln p (D | θ∗ ) + P ln N,                (2)
sible partitioning of subjects can be considered, includ-
                                                           where P is the number of parameters in the model
ing the possibility that all subjects are in the same par-
                                                           family (i.e., the sum of all the parameters used by the
tition (corresponding to aggregating across subjects),
                                                           models for each group), N is the total number of data,
or that each has their own partition (corresponding to
                                                           and θ∗ is the maximum likelihood parameterization
a complete individual analysis). Differences in the cat-
                                                           over all the models. Different possible model families,
egory learning processes between groups are revealed
                                                           corresponding to different groupings of subjects, can be
by differences in the parameter values they use.
                                                           compared in terms of their BIC values, with the mini-
   Because of the enormous flexibility allowed by model    mum BIC corresponding to the most likely account of
families, they can be made almost arbitrarily compli-      the data.
cated, and could potentially fit any data set perfectly
by adding new models, with extra parameters, to ac-               Demonstration Using ALCOVE
count for any remaining unexplained variation in data.
It is necessary, therefore, for model fitting methods to   Kruschke’s (1993) Study
use model selection criteria that balance goodness-of-     ALCOVE is a model of category learning that uses
fit and model complexity. The application of Bayesian      an exemplar-based stimulus representation, similarity-
model selection criteria (e.g., Pitt et al. 2002) is most  based generalization that is mediated by selective at-
easily pursued by specifying a probabilistic account,      tention, and error-based learning from external feed-
in the form of a likelihood function, of the relationship  back. The standard ALCOVE model Kruschke (1992)
between a parameterized model family and empirical         uses four free parameters. These control the rate of
data.                                                      learning for attention weights (λa ), the rate of learning
   To develop a likelihood function for category learn-    for the associations between stimulus representations
ing, suppose, under a proposed partitioning of sub-        and category responses (λw ), the gradient of the gen-
jects, the ith partition has ki subjects, and that the     eralization function that measures stimulus similarity
n category learning trials are divided into blocks, with   (c), and the way in which different levels of evidence for
the jth block having bj trials. Choosing one block with    category alternatives are mapped onto response prob-
b1 = n corresponds to an analysis of the average re-       abilities (φ).
sponse probabilities over all trials. Choosing n blocks       Kruschke (1993) considered the ability of ALCOVE
with all bj = 1 corresponds to a trial-by-trial analysis.  to model human category learning for filtration and
   In a two category learning experiment, the data take    condensation Categorization tasks (Garner 1974). The
the form of counts, dij , of the number of correct re-     results of four separate experiments were reported,
sponses made by all of the subjects in the ith par-        covering two filtration tasks (called position-relevant
tition on the jth block of learning trials. Suppose        and height-relevant, due to the nature of the stimuli)
also that a category learning model M , with its pa-       and two condensation tasks (called condensation A and
                                                      1441

                                                                  450
                                                                            CB
                                                                  400
                                                                  350                                              FP
                                              12
First Component
                                                                                                                   CA
                                                                  300
                                                                                                                   FH
                           28
                                                            BIC   250
                                                                  200
                                                                  150
                                                                  100
                                                                   50
                                                                        1        2     3     4    5     6      7
                      Second Component                                               Number of ALCOVE Models
Figure 1: The application of the heuristic for partition-   Figure 2: The pattern of change in BIC values for
ing subjects to find two groups for the position-relevant   each clustering of the position-relevant filtration (FP),
filtration data.                                            heigh-relevant filtration (FH), condensation A (CA)
                                                            and condensation B (CB) category learning data.
condensation B). The data involved a total of 160 sub-
jects, with 40 completing each task. Kruschke (1993)
                                                            vided into two groups for the position-relevant filtra-
fit ALCOVE to all four sets of experimental results
                                                            tion task are shown in Figure 1. Each circle represents
simultaneously, using trial-by-trial data formed by av-
                                                            the learning curve of a subject, represented according
eraging across all 40 subjects. An examination of the
                                                            to their values along the first two component eigen-
individual learning curves in the raw data, however,
                                                            vectors. The two groups of subjects identified by k-
reveals a large degree of variation between subjects
                                                            mean clustering are superimposed using broken lines.
within each experiment, and raises the possibility that
                                                            One cluster on the left encompasses 28 of the subjects,
there are psychologically meaningful individual differ-
                                                            while a much tighter cluster on the right encompasses
ences in category learning.
                                                            the remaining 12 subjects.
Heuristic for Partitioning Subjects
                                                            Model Fitting and Evaluation
In classification and clustering, an essential require-
                                                            For each of the clusterings for each task, maximum
ment for the determination of homogenous classes is a
                                                            likelihood fits of ALCOVE were found using a different
calculable similarity or distance measure between ob-
                                                            parameterization for each group according to Eq. (1).
jects being compared (Gordon 1999). For category
                                                            BIC values were then calculated for each model family
learning, the objects are the individual experimen-
                                                            using Eq. (2), giving the results1 shown in Figure 2.
tal observations for each subject, (i.e., each subject’s
                                                            It is clear that the minimum BIC for three of the four
learning curve). A candidate measure for describing
                                                            tasks (position-relevant filtration, condensation A and
the similarities between these curves is the correlation
                                                            condensation B) is achieved when two separate groups
coefficient, which we used in a two-stage heuristic. In
                                                            of subjects are considered, while the height-relevant
the first stage, singular value decomposition is applied
                                                            filtration data are best modeled by considering all of
to produce an ordered eigenvector-based representa-
                                                            the subjects as learning in the same way.
tion of the similarities between the learning curves of
                                                               Figures 3 and 4 give more detailed results for, respec-
subjects. In the second stage, a simple k-means clus-
                                                            tively, the position-relevant filtration and condensation
tering algorithm is applied to this representation to
find clusters of subjects.                                      1
                                                                  The full range of BIC values for the CB task is not
   For each of Kruschke’s (1993) four category learning     shown because, when four or more groups are considered,
tasks, this heuristic was applied to produce a range of     at least one of the groups contains only subjects who be-
partitions of the data, from a single group with all        come less accurate as learning blocks progress. ALCOVE
                                                            is qualitatively unable to accommodate the decrease in the
40 subjects, to seven groups with differing numbers of      averaged learning curve for this type of group, leading to
subjects in each group. As a concrete example of this       very poor fit, and very large BIC values. We have omitted
process, the clusters found when the subjects were di-      these values.
                                                    1442

                1    All                                                          1    All
               0.8                                                               0.8
               0.6                                                               0.6
               0.4                                                               0.4
Pr (Correct)                                                      Pr (Correct)
                1    G1                                                           1    G1
               0.8                                                               0.8
               0.6                                                               0.6
               0.4                                                               0.4
                1    G2                                                           1    G2
               0.8                                                               0.8
               0.6                                                               0.6
               0.4                                                               0.4
                           1   2   3   4   5   6   7   8                                     1   2   3   4   5   6   7   8
                                       Block                                                             Block
Figure 3: The change in accuracy across learning                  Figure 4: The change in accuracy across learning
blocks for the subjects (broken lines) and ALCOVE                 blocks for the subjects (broken lines) and ALCOVE
(solid lines), for the one group (“All”) and two group            (solid lines), for the one group (“All”) and two group
(“G1” and “G2”) model families on the position-                   (“G1” and “G2”) model families on the condensation
relevant filtration task.                                         A task.
A tasks. In both of these figures, the top panel, labeled         as coming from two distinct groups of subjects. The
“All”, shows the average accuracy of all subjects across          first group exhibits almost no learning, while the sec-
the eight learning blocks, and the maximum likelihood             ond learns at a moderate rate. Once again, ALCOVE
fit of ALCOVE to these data. The middle and bottom                is able to model both of these patterns of learning.
panels show the first (G1) and second (G2) groups of              In fact, ALCOVE has more difficulty accommodating
subjects proposed by the two-group model family that              the learning data resulting from averaging across all of
is prefered by the complexity analysis. These panels              the subjects. What the individual differences analy-
show the average accuracy for both groups of subjects             sis developed here suggests is that this inability may
separately, together with the maximum likelihood AL-              not indicate a fundamental weakness in ALCOVE, but
COVE learning curve.                                              rather that the averaging process involved in summa-
   Figure 3 shows that the moderate learning evident              rizing human performance has masked important indi-
when treating the subjects as having no individual dif-           vidual differences, and corrupted the underlying learn-
ferences is better modeled as coming from two dis-                ing patterns in the original data.
tinct groups of subjects. Some subjects, in the first                Table 1 shows the maximum likelihood parameter
group, maintain near-perfect accuracy throughout the              values for each group of subjects in the model family
category learning task. Other subjects, in the sec-               with the lowest BIC value, for all four learning tasks.
ond group, learn more gradually, only achieving near-             These parameter values are generally interpretable in
perfect accuracy in the last few learning blocks. Figure          terms of the different learning behavior revealed by the
3 shows that, with the exception of the rapid achieve-            individual differences analysis. For example, for the
ment of accuracy in the first block for the first group           position-relevant filtration task, the first group of sub-
of subjects, ALCOVE is able to model both of these                jects have a greater λw value than the second group,
patterns of learning2 .                                           consistent with their more rapid learning. For this
   In a similar way, Figure 4 shows that the gradual in-          task, both groups have high φ values, consistent with
crease in accuracy, evident when treating the subjects            their decisiveness (or ‘confidence’) in mapping evidence
as having no individual differences, is better modeled            into response probabilities. Both groups of subjects in
                                                                  the condensation A task, however, have much lower φ
   2
     It is possible the application of one of ALCOVE’s de-        values, consistent with their inferior learning perfor-
scendents, such as RASHNL (Kruschke & Johansen 1999)              mance, and the first group in this task, who basically
or the unified mixture of experts model (Kruschke 2001),
which emphasize rule-oriented learning and incorporate a          fail to learn, have a very low φ value. Other com-
rapid attention shifting capability (Kruschke 1996), could        parisons of this type, both within and across tasks,
overcome the deficiency.                                          generally have meaningful and useful interpretations,
                                                           1443

                                                           highlights the possibility of extending individual differ-
Table 1: Maximum likelihood parameter values for           ence accounts to incorporate fundamentally different
each group of subjects in the model family with            models to capture between-subject variation, rather
the lowest BIC value, for all four learning tasks.         than relying solely on parametric variation within the
FP=position-relevant filtration, FH=height-relevant        same basic model. In memory retention, for exam-
filtration, CA=condensation A, CB=condensation B.          ple, one group of subjects could be modeled using a
                                                           power function while another group is modeled using
        Task    Group       λw     λa       c     φ
                                                           an exponential decay function. For stimulus represen-
         FP       G1       0.38   0.49   1.68    3.20      tation, some groups of subject could be modeled us-
                  G2       0.06   27.0   6.83    2.66      ing a featural representation while others use a dimen-
         FH       All      0.23   0.58   1.56    1.00      sional representation. In the category learning con-
         CA       G1       0.47   1.14   2.53    0.27      text considered here, it may make sense to model some
                  G2       0.24   0.38   7.52    0.93      subject groups using ALCOVE or its descendants, but
         CB       G1       0.41   0.32   0.79    0.31      apply a very different category learning model to oth-
                  G2       0.17   0 .02  3.37    1.09      ers, such as the fast and frugal account provided by
                                                           Categorization-By-Elimination (Berrety et al. 1999).
                                                              One of the weaknesses of the demonstration pre-
                                                           sented here is the reliance on the BIC to compare dif-
and highlight the ability of ALCOVE to represent psy-      ferent competing individual differences models. While
chologically important variations in category learning     the BIC is conceptually and computationally straight-
through its free parameters.                               forward, it is insensitive to the complexity effects aris-
                                                           ing from the functional form of parametric interaction
                        Discussion                         within the individual models (Myung & Pitt 1997).
There are at least two conclusions that can be drawn       This is a potentially important shortcoming, especially
from modeling individual differences in Kruschke’s         if fundamentally different models are used to explain
(1993) category learning data using ALCOVE. The            performance for different subject groups. There are,
first is that there is strong evidence for large and mean- for example, many competing models of retention that
ingful differences in the learning behavior of groups of   use two parameters (Rubin & Wenzel 1996), with dif-
subjects for three out of the four tasks. Previous analy-  ferent complexities that the BIC is unable to distin-
ses, adopting the standard cognitive modeling practice     guish. The obvious remedy for this problem is to use
of considering all of the subjects as a single group,      more sophisticated model selection criteria that are
are insensitive to these potentially important patterns    sensitive to all of the components of model complex-
of variation. The second conclusion is that, for these     ity. These include measures such as the Stochastic
data, the basic ALCOVE model is generally able to          Complexity Criterion (SCC: Rissanen 1996) and Nor-
capture the individual differences in learning, when       malized Maximum Likelihood (NML: Rissanen 2001).
asked to model appropriate groups of subjects. It does     For cognitive models that resist the formal analysis
this by applying different psychologically meaningful      needed to derive these measures, an alternative is to
parameterizations to accommodate variations in learn-      use numerical methods, such Markov Chain Monte
ing behavior. In this sense, what the results presented    Carlo (e.g., Gilks, Richards, & Spiegelhalter 1996) to
here demonstrate is that accounting for individual dif-    approximate the Bayesian posterior distributions that
ferences using model families has the potential to ex-     compare model families.
tend and increase the usefulness of existing cognitive        A final possibility for refining the approach demon-
models significantly.                                      strated here is to use a more principled optimization
   From this promising start, there are a number of        approach to determine the groupings of subjects. The
directions in which the basic approach described here      method used here, based on k-means clustering of cor-
can be refined and extended. Most generally, the ex-       relations, is a sensible heuristic one. It is particularly
tension to other cognitive phenomena provides a rich       well suited to a model like ALCOVE that requires con-
set of opportunities for future research. As with cat-     siderable computation effort when finding maximum
egory learning, there is evidence of individual differ-    likelihood parameter values. The clustering heuris-
ences in the similarity data used to model stimulus        tic is designed to identify good partitions of the sub-
representations (e.g., Ashby et al. 1994), and in the      jects into groups, and only requires parameter fitting
curves of forgetting used to model memory retention        to be done once for each possible number of subject
(e.g., Anderson & Tweney 1997; Heathcote, Brown,           groups. For other models, however, such as analytic
& Mewhort 2000; Myung, Kim, & Pitt 2000; Wixted            models of memory retention, finding maximum like-
& Ebbesen 1997), and in a range of other data from         lihood parameterizations is straightforward. In these
which cognitive models have been developed.                cases, a more explicit optimization approach to find-
   Considering a broader range of cognitive phenomena      ing partitions could be adopted, because repeated pa-
                                                      1444

rameter fitting is possible. For example, a stochastic     Gordon, A. D. (1999). Classification (Second ed.).
hill-climbing procedure could be used to find subject        London: Chapman & Hall/CRC Press.
groups that minimize the BIC, SCC or NML of the            Heathcote, A., Brown, S., & Mewhort, D. J. K. (2000).
model family.                                                The power law repealed: The case for an exponen-
   Collectively, these possibilities describe a principled   tial law of practice. Psychonomic Bulletin & Re-
and general approach for building and evaluating cog-        view 7 (2), 185–207.
nitive models, using a variety of basic models and num-    Kruschke, J. K. (1992). ALCOVE: An exemplar-based
bers of parameterizations, to accommodate individual         connectionist model of category learning. Psycho-
differences. It is a more general approach to cognitive      logical Review 99 (1), 22–44.
modeling than one that averages data, assuming there       Kruschke, J. K. (1993). Human category learning: Im-
are no individual differences. It is a more powerful         plications for backpropagation models. Connection
and succinct approach than one that uses subject-by-         Science 5, 3–36.
subject analysis. While much of the work to realize this   Kruschke, J. K. (1996). Base rates in category learn-
potential remains to be done, the demonstration pre-         ing. Journal of Experimental Psychology: Learning,
sented here, using multiple ALCOVE models to cap-            Memory & Cognition 22, 3–26.
ture differences in category learning, provides a good     Kruschke, J. K. (2001). Toward a unified model of
concrete example of its potential. It shows how using        attention in associative learning. Journal of Mathe-
model families, and relying on principled model selec-       matical Psychology 45, 812–863.
tion criteria, can be used to develop detailed and inter-  Kruschke, J. K., & Johansen, M. K. (1999). A model
pretable accounts of both how people are cognitively         of probabilistic category learning. Journal of Exper-
                                                             imental Psychology: Learning, Memory and Cogni-
the same, and how they are different.
                                                             tion 25 (5), 1083–1119.
                                                           Myung, I. J., Kim, C., & Pitt, M. A. (2000). Toward
                 Acknowledgments                             an explanation of the power law artifact: Insights
This research was supported by Australian Research           from response surface analysis. Memory & Cogni-
Council Grant DP0451793.                                     tion 28 (5), 832–840.
                                                           Myung, I. J., & Pitt, M. A. (1997). Applying Occam’s
                      References                             razor in modeling cognition: A Bayesian approach.
Anderson, J. R., & Schooler, L. J. (1991). Reflections       Psychonomic Bulletin & Review 4 (1), 79–95.
                                                           Nosofsky, R. M. (1986). Attention, similarity, and the
   of the environment in memory. Psychological Sci-
                                                             identification-categorization relationship. Journal
   ence 2 (6), 396–408.
                                                             of Experimental Psychology: General 115 (1), 39–57.
Anderson, J. R., & Tweney, R. D. (1997). Artifactual       Pitt, M. A., Myung, I. J., & Zhang, S. (2002). Toward
   power curves in forgetting.Memory & Cognition 25,         a method of selecting among computational models
   724–730.                                                  of cognition. Psychological Review 109 (3), 472–491.
Ashby, F. G., Maddox, W. T., & Lee, W. W. (1994).          Rissanen, J. (1996). Fisher information and stochas-
   On the dangers of averaging across subjects when us-      tic complexity. IEEE Transactions on Information
   ing multidimensional scaling or the similarity-choice     Theory 42 (1), 40–47.
   model. Psychological Science 5 (3), 144–151.            Rissanen, J. (2001). Strong optimality of the normal-
Ashby, F. G., & Perrin, N. A. (1988).Toward a unified        ized ML models as universal codes and information
   theory of similarity and recognition. Psychological       in data. IEEE Transactions on Information The-
   Review 95 (1), 124–150.                                   ory 47 (5), 1712–1717.
Berretty, P. M., Todd, P. M., & Martignon, L. (1999).      Rubin, D. C., & Wenzel, A. E. (1996). One hundred
   Categorization by elimination. In G. Gigerenzer &         years of forgetting: A quantitative description of re-
   P. M. Todd (Eds.), Simple Heuristics That Make Us         tention. Psychological Review 103 (4), 734–760.
   Smart, pp. 235–254. New York: Oxford University         Schwarz, G. (1978). Estimating the dimension of a
   Press.                                                    model. Annals of Statistics 6 (2), 461–464.
Estes, W. K. (1956). The problem of inference from         Shepard, R. N. (1980). Multidimensional scaling, tree-
   curves based on group data. Psychological Bul-            fitting, and clustering. Science 210, 390–398.
   letin 53 (2), 134–140.                                  Tenenbaum, J. B. (1999). Bayesian modeling of hu-
Estes, W. K. (1997). Processes of memory loss, recov-        man concept learning. In M. S. Kearns, S. A. Solla,
   ery, and distortion. Psychological Review 104 (1),        & D. A. Cohn (Eds.), Advances in Neural Infor-
   148–169.                                                  mation Processing Systems 11, Cambridge, MA, pp.
Garner, W. R. (1974). The Processing of Information          59–65. MIT Press.
                                                           Wixted, J. T., & Ebbesen, E. B. (1997). Genuine
   and Structure. Potomac, MD: Erlbaum.
                                                             power curves in forgetting: A quantitative analysis
Gilks, W. R., Richardson, S., & Spiegelhalter, D. J.         of individual subject forgetting functions. Memory
   (1996). Markov Chain Monte Carlo in Practice.             & Cognition 25 (5), 731–739.
   London: Chapman and Hall.
                                                      1445

