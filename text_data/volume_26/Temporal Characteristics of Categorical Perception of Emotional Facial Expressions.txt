UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Temporal Characteristics of Categorical Perception of Emotional Facial Expressions
Permalink
https://escholarship.org/uc/item/0nw113h7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Suzuki, Atsunobu
Shibui, Susumu
Shigemasu, Kazuo
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

Temporal Characteristics of Categorical Perception of Emotional Facial Expressions
                                        Atsunobu Suzuki (suzuki@bayes.c.u-tokyo.ac.jp)
                                             Department of Cognitive and Behavioral Science,
                                           3-8-1 Komaba, Meguro-ku, Tokyo 153-8902, Japan
                                          Susumu Shibui (shibui@bayes.c.u-tokyo.ac.jp)
                                             Department of Cognitive and Behavioral Science,
                                           3-8-1 Komaba, Meguro-ku, Tokyo 153-8902, Japan
                                        Kazuo Shigemasu (kshige@bayes.c.u-tokyo.ac.jp)
                                             Department of Cognitive and Behavioral Science,
                                           3-8-1 Komaba, Meguro-ku, Tokyo 153-8902, Japan
                              Abstract                                  been argued that the perceptual system transforms the
                                                                        information continuously received from a given facial
   Perceptual processing of emotional facial expressions occurs         expression into categorical information corresponding to the
   very quickly, even without awareness. To determine whether           most likely emotion (Etcoff & Magee, 1992).
   the fast processing of facial expressions is categorical, we
                                                                           In previous research, however, facial stimuli were
   studied temporal characteristics of categorical perception (CP)
   of facial expressions. We investigated the effect of shortening      presented for a relatively long period (≧ 750 ms), so it
   stimulus duration on participant performance with respect to         remains unclear whether the fast perceptual processing of
   identifying and discriminating morphed facial expressions.           facial expressions is categorical in nature. Our goal was to
   The results of two experiments showed that CP was                    investigate the effects of shortening stimulus duration on CP
   attenuated or even disappeared when facial stimuli were              of facial expressions and to examine whether the fast
   presented for as briefly as 50-75 milliseconds. These findings       perceptual processing is categorical.
   indicate that CP is irrelevant to the fast perceptual processing
   of facial expressions.
                                                                                                Experiment 1
                                                                        In Experiment 1, participants identified facial expressions
                                                                        from three continua extending from anger to happiness,
Facial expressions of emotion are processed very quickly,               from happiness to fear, and from fear to anger. We
even without awareness (Morris, Ohman, & Dolan, 1998;                   investigated how shortening stimulus exposure duration
Whalen et al., 1998). It has been proposed that this fast               (750 ms, 150 ms, 50 ms) affected the abrupt change in the
emotional processing provides a ‘dirty’ image of the                    rate of identification at the category boundary.
external world, enabling organisms to detect salient stimuli
immediately (Adolphs, 2002; LeDoux, 1996). What is still                Method
not understood, however, is how this fast and dirty
processing of facial expressions works.                                 Participants Twelve undergraduates and postgraduates
   A hallmark of facial expression recognition is its                   participated (8 men, 4 women; 20-24 years old).
categorical nature, that is, facial expressions are recognized
as belonging to discrete categories of emotion, so-called               Facial Stimuli Three sets of eight gray-scale images of
basic emotions (e.g., Ekman, 1992; Izard, 1992). Reports on             facial expressions were used. Each set consisted of a
categorical perception (CP) of facial expressions are thought           continuum extending either from anger to happiness, from
to provide strong evidence that people process facial                   happiness to fear, or from fear to anger. The original
expressions categorically (Calder et al., 1996; DeGelder,               (endpoint) facial expressions were posed photographs of a
Teunisse, & Benson, 1997; Etcoff & Magee, 1992; Young                   Japanese woman, and interpolated (morphed) expressions
et al., 1997). Previous studies have confirmed that                     between the two continuum endpoints were created with
recognizing facial expressions fulfills the following features          software for facial expression processing (Information-
of CP: (a) in identifying a stimulus within a continuum                 technology Promotion Agency, Japan (IPA), 1998).
extending from one category to another, the rate of                        The morph transformation started with the delineation of
categorizing the stimulus changes abruptly at a boundary                two endpoint expressions; landmarks were placed manually
(category boundary); and (b) in discriminating a pair of                on critical positions of each image. There were 759
stimuli that differ by a constant physical amount,                      landmarks in total, 88 points of which were placed manually
discrimination is superior for pairs straddling the category            on corresponding positions of each image: for the head, 4
boundary (between-category pairs), as compared to pairs                 points; for the outline, 28 points; for the eyes, 5 x 2 points;
falling within one category (within-category pairs). It has             for the eyebrows, 4 x 2 points; for the nose, 4 points; for the
                                                                    1303

mouth, 6 points; for the neck, 13 points; and finally for the    detect clear CP for the fear-anger continuum (Calder et al.,
hairline, 15 points. An intermediate face was then created       1996; DeGelder et al., 1997).
with linear interpolation between point-to-point pixel
intensity values, yielding a weighted blend of both facial
configuration and texture of the two endpoint faces. Six                                                     AH Session
intermediate faces were generated for each continuum, so
                                                                    Identification Rate (%)
that eight images from one endpoint to the other were                                         100
spaced with a 14.3% gap.                                                                       80
                                                                                               60
Procedure and Design Participants performed the                                                                           750ms
identification task (three sessions) by viewing stimuli                                        40
                                                                                                                          150ms
presented on a 17-inch CRT monitor. Each trial began with                                      20
                                                                                                                          50ms
an 800-ms presentation of a fixation point, followed by a                                       0
blank interval of 300 ms, and then a facial stimulus from
one continuum for 750 ms, 150 ms, or 50 ms. After a 300-                                            0   20    40     60   80      100
ms blank interval, participants were asked to decide which                                                    Morph (%)
endpoint category the face expressed. The continuum was
manipulated across sessions: from anger to happiness (AH
session), from happiness to fear (HF session), and from fear
to anger (FA session). The order of the three sessions was                                                   HF Session
counterbalanced across participants. Each session began
                                                                    Identification Rate (%)
                                                                                              100
with 24 training trials, followed by five blocks of 48
experimental trials, corresponding to two repetitions of the                                   80
24 combinations of faces (8) and stimulus durations (3). The                                   60
order of the face-duration sets was fully randomized in each                                                              750ms
                                                                                               40
repetition.                                                                                                               150ms
                                                                                               20
                                                                                                                          50ms
Results & Discussion                                                                            0
Figure 1 displays the overall percentage with which a given                                         0   20    40     60   80      100
endpoint expression was identified at each stimulus duration,                                                 Morph (%)
for each continuum. As an estimate of the category
boundary, the point at which two endpoints were identified
with equal probability was computed by applying a logit
model to each identification rate curve (Table 1). In general,                                               FA Session
the category boundary lay near the 50% morph. More
                                                                    Identification Rate (%)
                                                                                              100
importantly, Table 1 also indicates the slope of the logistic
curve at the category boundary as a measure of the                                             80
identification rate change at the boundary. Multiple                                           60
comparisons (z tests) with Bonferroni’s method (α=0.017)                                                                  750ms
                                                                                               40
revealed that the slope at 50 ms was significantly, or                                                                    150ms
marginally significantly, smaller than at 150 ms and at 750                                    20
                                                                                                                          50ms
ms for the anger-happiness and happiness-fear continua (AH                                      0
session, 50 ms vs. 150 ms, z=2.37, p=0.018, 50 ms vs. 750
ms, z=3.67, p<0.001, 150 ms vs. 750 ms, z=1.48, p=0.139;                                            0   20    40     60   80      100
HF session, 50 ms vs. 150 ms, z=2.69, p=0.007, 50 ms vs.                                                      Morph (%)
750 ms, z=4.07, p<0.001, 150 ms vs. 750 ms, z=1.74,
p=0.082). No slope change was observed for the fear-anger
                                                                 Figure 1: Overall percentage with which a given endpoint
continuum (all ps>0.35).
                                                                 expression (top: happiness, middle: fear, bottom: anger) was
   The decreased slopes of the identification rate curves for
                                                                 identified at each stimulus duration, for each session. Morph
the anger-happiness and happiness-fear continua at 50 ms
                                                                 represents a percentage of happiness (top), fear (middle), or
indicate that the CP of facial expressions is attenuated with
                                                                 anger (bottom) for a given face.
the briefest stimulus exposure duration. The lack of slope
change for the fear-anger continuum may be due to the
originally weak categorical perception between fear and
anger, which reflects their remarkable similarity with
respect to the dimensional information of pleasure and
arousal (Russell, 1997). Indeed, the slope for the fear-anger
continuum was the smallest of the three continua at 750 ms.
Likewise, there are previous reports that have failed to
                                                             1304

Table 1: Estimates of the position of the category boundary          XAB discrimination task The sequence of the stimuli
and the slope at the boundary.                                    presented in each trial was as follows: (1) a fixation point
                                                                  for 450 ms; (2) a blank interval for 300 ms; (3) a facial
         Session Duration      Boundary (%)      Slope            stimulus ‘X’ for 150 ms or 75 ms (target); (4) a black-and-
           AH                                                     white checker-pattern for 150 ms (backward mask); (5) a
                    750 ms          56.9         14.5
                                                                  blank interval for 750 ms; (6) and finally, two facial stimuli
                    150 ms          52.5         12.5             ‘A’ and ‘B’, positioned horizontally to the right and left of
                     50 ms          49.0         10.0             center, displayed until the participants responded (reference).
           HF       750 ms          43.3         18.1             Participants were asked to decide which reference was
                    150 ms          47.9         14.9             identical to the target.
                                                                     The backward mask was used to restrict visual access to
                     50 ms          51.2         11.3             the target over the controlled stimulus exposure duration
           FA       750 ms          53.9         13.3             (Enns & DiLollo, 2000). Because the backward mask might
                    150 ms          51.8         13.8             degrade target perception, we used 75 ms as the brief
                     50 ms          49.2         12.6             exposure duration, which is somewhat longer than the 50 ms
                                                                  tested in Experiment 1. We also used 150 ms as the sole
   Note Boundary represents a percentage of happiness (AH         longer exposure duration interval, because we speculated
Session), fear (HF Session), or anger (FA Session) for a          that task difficulty might differ too much between 75 ms
morphed face.                                                     and 750 ms. Reference stimuli were spaced with a 20% gap,
                                                                  resulting in nine possible pairs. For each pair, there were
                                                                  four presentation orders; (X,A,B) = (A,A,B), (A,B,A),
                       Experiment 2                               (B,A,B), (B,B,A). Target duration was manipulated across
                                                                  sessions, and the order of the two durations was
Experiment 1 revealed that the abrupt change in                   counterbalanced across participants. Each session contained
identification rate at the category boundary was attenuated       10 training trials and two blocks of 36 experimental trials
at the briefest stimulus exposure duration, indicating that CP    representing all combinations of pairs (9) and presentations
of facial expressions did not reflect fast perceptual             (4). The order of the pair-presentation sets was fully
processing. To further examine temporal characteristics of        randomized in each block.
CP, Experiment 2 investigated whether superior                       Identification task Stimulus sequence in each trial was
discrimination for between-category pairs could be observed       the same as in the XAB discrimination task, with the
with the brief stimulus exposure duration when compared to        exception that reference stimuli were removed. Participants
discrimination for within-category pairs. Because a more          were asked to decide whether the target stimulus expressed
accurate discrimination performance for between-category          fear or happiness. The order of the two sessions across
pairs is regarded as the key indicator of CP (Harnad, 1987),      which the target duration was manipulated was the same as
its disappearance at the brief stimulus duration provides         in the XAB discrimination task. Each session contained 10
crucial evidence that categorical perception does not occur       training trials and two blocks of 44 experimental trials
rapidly. In Experiment 2, participants engaged in both            corresponding to four repetitions of the 11 faces. The order
discrimination and identification tasks of facial expressions     of the faces was fully randomized in each repetition.
from the happiness-fear continuum. We selected the
happiness-fear continuum from the three continua used in          Results & Discussion
Experiment 1 because it did not incorporate any gross
changes in physical features (e.g., from open to closed
mouth), which might obscure CP (Calder et al., 1996).             Identification Task Figure 2 presents the overall
                                                                  percentage of trials in which “fear” was identified at each
                                                                  stimulus exposure duration in the identification task.
Method
                                                                  Application of a logit model to each identification rate curve
                                                                  revealed that the category boundary lay at 52.4% (150 ms)
Participants Twenty-three undergraduates participated (12         and 55.2% (75 ms). Contrary to the results of Experiment 1,
men, 11 women; 18-24 years old).                                  logit models also showed that the slope at 150 ms (17.9)
                                                                  was somewhat smaller than at 75 ms (19.4), though this
Facial Stimuli One set of 11 gray-scale images of facial          difference did not reach significance (z=1.03, p=0.303). As
expressions was used. The set consisted of a continuum            the slope was attenuated in proportion to the decreased
extending from happiness to fear. The endpoint expressions        duration in Experiment 1, these data suggest that the
and the morphing procedure to create interpolated faces           duration difference between 150 ms and 75 ms was
were the same as in Experiment 1. Nine intermediate faces         insufficient to cause a significant slope change.
were generated so that the 11 images from happiness to fear
were spaced with a 10% gap.                                       XAB Discrimination Task Figure 3 presents the overall
                                                                  correct response rate for each stimulus exposure duration in
Procedure and Design Participants performed two
successive tasks (two sessions each) by viewing stimuli
presented on a 17-inch CRT monitor.
                                                              1305

                                                                                                               the XAB discrimination task. The data from the
                                                                                                               identification task revealed that identifying between-
                              Identification Rate (%)
                                                        100
                                                                                                               category pairs was 40%-60% and 50%-70% at both
                                                         80                                                    durations. The peak in discrimination performance at the
                                                         60                                                    category boundary was found only at the 150-ms stimulus
                                                                                                               exposure duration.
                                                         40
                                                                                              150ms               We calculated the mean correct rate for between-category
                                                         20                                                    pairs (40%-60%, 50%-70%) and within-category pairs (the
                                                                                              75ms
                                                          0                                                    remaining seven pairs) for each duration, and conducted a 2
                                                                                                               x 2 ANOVA of the mean correct rate with the two factors of
                                                              0      20        40     60     80       100
                                                                                                               pair (between-category or within-category) and stimulus
                                                                               Morph (%)                       duration (150 ms or 75 ms). A significant main effect of pair
                                                                                                               was found (F(1,22)=7.79, p=0.011, MSE=0.008), indicating
Figure 2: Overall percentage with which “fear” was                                                             better discrimination for between-category pairs (72%) than
                                                                                                               for within-category pairs (67%). The main effect of duration
identified at each stimulus duration. Morph represents a
percentage of “fear” for a given face.                                                                         was also significant (F(1,22)=9.74, p=0.005, MSE=0.011),
                                                                                                               indicating worse performance at 75 ms (66%) than at 150
                                                                                                               ms (73%). There was no significant interaction between the
                                                                          75 ms                                two factors (p>0.25), suggesting that discrimination of
                                                                                                               between-category pairs was more accurate than that for
                                          90                                                                   within-category pairs, at both durations. However, post-hoc
  Correct Response Rate (%)
                                                                    Observed          Theoretical              paired t tests revealed that superior discrimination for
                                                                                                               between-category pairs was significant only at the 150-ms
                                          80                                                                   exposure duration (150 ms, M=8%, t(22)=2.42, p=0.024; 75
                                                                                                               ms, M=3%, t(22)=1.18, p=0.252). Post-hoc comparisons
                                          70                                                                   also revealed that poorer discrimination at 75 ms was
                                                                                                               significant for both between-category and within-category
                                          60                                                                   pairs (between-category, M=9%, t(22)=2.36, p=0.027;
                                                                                                               within-category, M=4%, t(22)=2.66, p=0.014).
                                          50                                                                      To indicate CP in the discrimination task, previous
                                                                                                               studies (Calder et al., 1996; Young et al., 1997) have
                                                          0       10 20 30 40 50 60 70 80                      reported correlations between observed and theoretical
                                                                          Morph (%)                            performances, predicted from CP. Calder and colleagues
                                                                                                               (Calder et al., 1996) assumed that discrimination between
                                                                                                               two facial expression stimuli depends on two cues: first, the
                                                                                                               physical difference between the stimuli, which is constant
                                                                          150 ms                               for any pair, regardless of their expressions; and second, the
                                                                                                               expression categories of the stimuli. To estimate the
                                          90
  Correct Response Rate (%)
                                                                                                               contribution of the first non-categorical cue, they computed
                                                                    Observed          Theoretical
                                                                                                               the mean observed discrimination for the two pairs, placed
                                          80                                                                   at both ends of the continuum. To estimate the contribution
                                                                                                               of the second categorical cue, they calculated the difference
                                          70                                                                   in identification rates for the two relevant stimuli observed
                                                                                                               in the identification task, and multiplied the obtained
                                          60                                                                   difference by 0.25 (a constant). Calder et al. (1996) argued
                                                                                                               that summing the two estimates measures theoretical
                                                                                                               performance in the discrimination task.
                                          50
                                                                                                                  Figure 3 also presents theoretical performance, based on
                                                          0       10 20 30 40 50 60 70 80                      the same formula as Calder et al. (1996), along with
                                                                                                               observed performance. The fit between observed and
                                                                          Morph (%)
                                                                                                               theoretical performance looks better at 150 ms than at 75
                                                                                                               ms; indeed, their correlation was significant only at the 150-
Figure 3: Overall correct response rate in the XAB                                                             ms exposure duration (150 ms, r=0.667, t(7)=2.37, p=0.050;
discrimination task for each duration. Observed=observed                                                       75 ms, r=0.362, t(7)=1.03, p=0.338).
rate. Theoretical=theoretical rate predicted from CP. Morph                                                       Analyses revealed that the superior discrimination
represents a percentage of “fear” for the less fearful face of                                                 performance for between-category pairs disappeared at 75
a given pair.                                                                                                  ms but was present at 150 ms, providing decisive evidence
                                                                                                               of null CP at the brief stimulus duration. In response to
                                                                                                               objections claiming that observing only one continuum is
                                                                                                               insufficient, studies using neuroimaging (Morris et al.,
                                                                                                            1306

1996; Morris, Friston, et al., 1998; Whalen et al., 1998;          (Beale & Keil, 1995; Levin & Beale, 2000), race (Levin &
Wright et al., 2001) and developmental data (Kotsoni,              Angelone, 2002), and even familiar objects (Newell &
DeHaan, & Johnson, 2001) have indicated that happiness             Bulthoff, 2002). Categorical perception of facial expressions
and fear are the most distinct categories, and that the lack of    may thus reflect visual information processing that is
CP between the two expressions is important. Critics might         common to general object recognition rather than specific to
also insist that the disappearance of CP at 75 ms is a floor       emotional content.
effect; however, discrimination for any stimulus pair at 75
ms was above chance (all ps<0.01), indicating that residual                                 References
perception of differences in some visual properties was
                                                                   Adolphs, R. (2002). Neural systems for recognizing emotion.
present. Moreover, the 75-ms duration was sufficient for
                                                                      Current Opinion in Neurobiology, 12, 169-177.
accurate identification of near-endpoint facial expressions
                                                                   Beale, J. M., & Keil, F. C. (1995). Categorical effects in the
(see Figure 2). Thus, categorical perception did not occur,
                                                                      perception of faces. Cognition, 57, 217-239.
even with such well-preserved visual processing.
                                                                   Calder, A. J., Young, A. W., Perrett, D. I., Etcoff, N. L., &
                                                                      Rowland, D. (1996). Categorical perception of morphed
                   General Discussion                                 facial expressions. Visual Cognition, 3, 81-117.
The present experiments revealed that categorical                  DeGelder, B., Teunisse, J. P., & Benson, P. J. (1997).
perception of facial expressions was attenuated and even              Categorical perception of facial expressions: categories
disappeared with brief stimulus exposure durations. The               and their internal structure. Cognition and Emotion, 11,
abrupt change in identifying facial expressions at the                1-23.
category boundary was weakened at the 50-ms stimulus               Ekman, P. (1992). Are there basic emotions? Psychological
duration (Experiment 1), and the more accurate                        Review, 99, 550-553.
discrimination observed for between-category pairs than for        Ekman, P. (1994). Strong evidence for universals in facial
within-category pairs was eliminated at the 75-ms stimulus            expressions: a reply to Russell’s mistaken critique.
duration (Experiment 2). Our findings indicate that the fast          Psychological Bulletin, 115, 268-287.
perceptual processing of emotional facial expressions is not       Enns, J. T., & DiLollo, V. (2000). What’s new in visual
categorical.                                                          masking? Trends in Cognitive Sciences, 4, 345-352.
   It has been postulated that the fast processing of facial       Etcoff, N. L., & Magee, J. J. (1992). Categorical perception
expressions involves subcortical structures, specialized in           of facial expressions. Cognition, 44, 227-240.
the detection of salient stimuli (Adolphs, 2002). However,         Harnad, S. (1987). Psychophysical and cognitive aspects of
categorical perception refers to the processing of ambiguous          categorical perception: a critical over view. In S. Harnad
stimuli or the transformation of an indistinctive facial              (Ed.), Categorical perception: the groundwork of
configuration into a distinctive emotion category. Such fine          cognition (pp. 1-25). Cambridge: Cambridge University
analysis of emotional information may involve cortical                Press.
structures, implying slower processing (LeDoux, 1996).             Information-technology Promotion Agency, Japan. (1998).
   Etcoff and Magee (1992) have claimed that their                    Software for Facial Image Processing System for Human-
observation of CP in facial expression recognition rejects            like ‘Kansei’ Agent [Computer Software]. Retrieved from
the possibility that categorical processing of facial                 http://www.tokyo.image-lab.or.jp/aa/ipa/
expressions is performed by higher conceptual and linguistic       Izard, C. E. (1992). Basic emotions, relations among
systems. There are data, however, demonstrating that verbal           emotions, and emotion-cognition relations. Psychological
interference eliminated CP of colors and facial expressions,          Review, 99, 561-565.
suggesting the essential role of verbal coding over visual         Kotsoni, E., DeHaan, M., & Johnson, M. H. (2001).
coding (Roberson & Davidoff, 2000). Medin and Barsalou                Categorical perception of facial expressions by 7-month-
(1987) have argued that it is important to distinguish                old infants. Perception, 30, 1115-1125.
between sensory perception categories (SP categories),             LeDoux, J. (1996). The emotional brain: the mysterious
categories related to low-level perceptual experiences, and           underpinnings of emotional life. New York: Simon &
generic knowledge categories (GK categories), categories              Schuster Inc.
related to high-level knowledge representation. Shibui and         Levin, D. T., & Angelone, B. L. (2002). Categorical
colleagues (Shibui, Yamada, Sato, & Shigemasu, 2001)                  perception of race. Perception, 31, 567-578.
found that discrimination accuracy for within-category pairs       Levin, D. T., & Beale, J. M. (2000). Categorical perception
of facial expressions was proportional to their semantic              occurs in newly learned faces, other-race faces, and
distance, and they inferred that facial expressions belong to         inverted faces. Perception and Psychophysics, 62, 386-
cognitive GK categories. Our findings are consistent with             401.
the view that categorical processing of facial expressions is      Medin, D. L., & Barsalou, L. W. (1987). Categorization
performed by higher cognitive systems.                                processes and categorical perception. In S. Harnad (Ed.),
   Some researchers have postulated that CP of facial                 Categorical perception: the groundwork of cognition (pp.
expressions supports the notion of basic emotions                     1-25). Cambridge: Cambridge University Press.
(DeGelder et al., 1996; Ekman, 1994), such that each               Morris, J. S., Friston, K. J., Buchel, C., Frith, C. D., Young,
emotion possesses an innate and specialized neuro-cognitive           A. W., Calder, A. J., & Dolan, R. J. (1998). A
system. However, categorical perception is also known to              neuromodulatory role for the human amygdala in
occur for non-emotional visual categories such as identity
                                                               1307

  processing emotional facial expressions. Brain, 121, 47-
  57.
Morris, J. S., Frith, C. D., Perrett, D. I., Rowland, D., Young,
  A. W., Calder, A. J., & Dolan, R. J. (1996). A differential
  neural response in the human amygdala to fearful and
  happy facial expressions. Nature, 383, 812-815.
Morris, J. S., Ohman, A., & Dolan, R. J. (1998). Conscious
  and unconscious emotional learning in the human
  amygdala. Nature, 393, 467-470.
Newell, F. N., & Bulthoff, H. H. (2002). Categorical
  perception of familiar objects. Cognition, 85, 113-143.
Roberson, D., & Davidoff, J. (2000). The categorical
  perception of colors and facial expressions: the effect of
  verbal interference. Memory and Cognition, 28, 977-986.
Russell, J. A. (1997). Reading emotions from and into faces:
  resurrecting a dimensional-contextual perspective. In J. A.
  Russell, & J. M. Fernandez-Dols (Ed.), The psychology of
  facial expression (pp. 295-320). Cambridge: Cambridge
  University Press.
Shibui, S., Yamada, H., Sato, T., & Shigemasu, K. (2001).
  The relationship between the categorical perception of
  facial expressions and semantic distances. The Japanese
  Journal of Psychology, 72, 219-226.
Whalen, P. J., Rauch, S. L., Etcoff, N. L., McInerney, S. C.,
  Lee, M. B., & Jenike, M. A. (1998). Masked
  presentations of emotional facial expressions modulate
  amygdala activity without explicit knowledge. Journal of
  Neuroscience, 18, 411-418.
Wright, C. I., Fischer, H., Whalen, P. J., McInerney, S.,
  Shin, L. M., & Rauch, S. L. (2001). Differential
  prefrontal cortex and amygdala habituation to repeatedly
  presented emotional stimuli. NeuroReport, 12, 379-383.
Young, A. W., Rowland, D., Calder, A. J., Etcoff, N. L.,
  Seth, A., & Perrett, D. I. (1997). Facial expression
  megamix: tests of dimensional and category accounts of
  emotion recognition. Cognition, 63, 271-313.
                                                               1308

