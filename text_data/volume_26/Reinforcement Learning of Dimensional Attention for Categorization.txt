UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Reinforcement Learning of Dimensional Attention for Categorization
Permalink
https://escholarship.org/uc/item/04q789kv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Philips, Joshua L.
Noelle, David C.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

       Reinforcement Learning of Dimensional Attention for Categorization
                                           Joshua L. Phillips & David C. Noelle
                                (  JOSHUA . L . PHILLIPS , DAVID . NOELLE    @ VANDERBILT. EDU)
                                                      Vanderbilt University
                                                    Nashville, TN 37235 USA
                           Abstract                                  make classification judgments for each. Immediately fol-
                                                                     lowing each judgment, feedback is provided, typically
   The ability to selectively focus attention on stimulus di-        informing the learner of the correct category label for the
   mensions appears to play an important role in human cat-          preceding stimulus. Once learning is complete, catego-
   egory learning. This insight is embodied by learned di-
   mensional attention weights in the ALCOVE model (Kr-              rization judgments on transfer stimuli, for which no feed-
   uschke, 1992). The success of this psychological model            back is provided, can provide a window into the structure
   suggests its use as a foundation for efforts to under-            of the learned category knowledge. The ALCOVE model
   stand the neural basis of category learning. One obsta-           uses the feedback provided during training to calculate
   cle to such an effort is ALCOVE‚Äôs use of the biologi-
   cally implausible backpropagation of error algorithm to           an ‚Äúerror signal‚Äù, which is simply the difference between
   adapt dimensional attention weights. This obstacle may            the category assignment made by the model and the spec-
   be overcome by replacing this attention mechanism with            ified ‚Äútrue‚Äù category. A variant of the backpropagation
   one grounded in the reinforcement learning processes              of error learning algorithm (Rumelhart et al., 1986) is
   of the brain‚Äôs dopamine system. In this paper, such a             used to communicate this error signal to an early stage
   biologically-based mechanism for dimensional attention
   is proposed, and the fit of this mechanism to human per-          of stimulus encoding, and this backpropagated error sig-
   formance is shown to be comparable to that of ALCOVE.             nal is used to adjust ALCOVE‚Äôs dimensional attention
                                                                     weights. Like the GCM, ALCOVE provides good fits to
                                                                     human performance data on learned categories. Unlike
                       Introduction                                  the GCM, ALCOVE provides a detailed account of how
Human category learning performance cannot be easily                 dimensional attention is shaped by experience.
explained without recourse to a mechanism for selec-                    ALCOVE has been proposed as a model of psycho-
tive dimensional attention (Shepard et al., 1961). Di-               logical processes, with virtually no aspiration to explain
mensional attention is the cognitive process which em-               the neural basis of human category learning. Despite this
phasizes task relevant stimulus dimensions while deem-               fact, the empirical successes of ALCOVE and its con-
phasizing others. Thus, contemporary formal models of                nectionist formalization make the model a tempting can-
categorization, such as the Generalized Context Model                didate for a coarse characterization of associated brain
(GCM) (Nosofsky, 1984), have incorporated adaptable                  mechanisms. Perhaps ALCOVE can be refined, with
dimensional attention parameters. By adjusting these pa-             each of its proposed psychological mechanisms mapped
rameters in a category-specific fashion, the GCM has re-             onto a corresponding detailed account of the underlying
peatedly provided excellent fits to human data reflecting            neural machinery. One feature of ALCOVE that stands
the frequency (or probability) with which each stimulus              in the way of such a theoretical reduction is its use of
is recognized as an instance of a target category. When              the backpropagation of error algorithm in order to learn
the GCM is applied to experimental results, dimensional              dimensional attention weights. This powerful learning
attention parameters are freely varied to optimize the               algorithm has long been criticized for its lack of biolog-
model fit. This means that, while the GCM provides a                 ical plausibility (Crick, 1989), suggesting that the brain
powerful account of learned categorization performance,              cannot be adapting dimensional attention based on such
it offers no explanation for how dimensional attention is            a gradient-based technique (c.f., O‚ÄôReilly (1996)).
adjusted over the course of learning.                                   As a first step toward a biological model of category
   This shortcoming of the GCM has been addressed by a               learning, we replaced the backpropagation-based dimen-
connectionist model called ALCOVE (Kruschke, 1992).                  sional attention mechanism used by ALCOVE with a re-
ALCOVE incorporates the GCM‚Äôs formalization of cat-                  inforcement learning mechanism intended to reflect the
egory knowledge, but it also provides a precise algo-                role of the brain‚Äôs dopamine (DA) system in learning.
rithm for modifying the attentional ‚Äúweight‚Äù assigned to             This role for dopamine has been formalized by other re-
each stimulus dimension, based on feedback provided                  searchers in terms of an algorithm called temporal dif-
to learners on their categorization judgments. In a typ-             ference (TD) learning (Sutton, 1988; Montague et al.,
ical category learning experiment, learners are presented            1996). Versions of ALCOVE which adapt dimensional
with stimulus objects, one at a time, and are asked to               attention weights using the biologically supported TD
                                                               1101

learning method, instead of the more computationally                     are standard linear units, with their activation levels, aout       k ,
powerful but biologically implausible backpropagation                    computed as the sum of exemplar unit activation levels,
method, were found to fit human performance data about                   ahid
                                                                           j , weighted by the corresponding association weights,
as well as the original ALCOVE. Thus, this work offers                   wk j . Output unit activations are mapped onto response
a more biologically realistic model of the adaptation of                 probabilities using an exponential Luce choice rule:
dimensional attention without sacrificing accuracy in ac-
counting for human categorization behavior. Also, the                               PK          exp œÜaoutK    ‚àë exp      œÜaout
                                                                                                                               k 
ability to capture human performance with the highly                                                              k
stochastic TD learning method suggests that cognitive                    where P K is the probability of selecting category K for
mechanisms for adapting dimensional attention may not                    the current stimulus, and œÜ is a gain term. These response
need to be particularly precise.                                         probabilities may be used to compare network responses
                                                                         with human performance data.
                         Background                                         After the presentation of each stimulus and the conse-
ALCOVE Architecture                                                      quent outputs are produced, the output unit correspond-
The ALCOVE (Kruschke, 1992) model of category                            ing to the
                                                                                   correct response is presented with a target acti-
learning is a feedforward connectionist model that in-                   vation level of  1, and other units are presented with tar-
volves three layers of processing units (see Figure 1(a)).               gets of 1. An error signal consisting of the difference
The input layer consists of a set of units that each cor-                between aoutk and these targets is used to adjust weight
respond to a single dimension in the stimulus psycho-                    values (though output units that ‚Äúovershoot‚Äù their target
logical space. Explaining the structure of this perceptual               values are assigned zero error). The association weights
representation is outside of ALCOVE‚Äôs scope. When fit-                   are then adjusted using this error signal directly (i.e., us-
ting ALCOVE to human data, multidimensional scaling                      ing the delta rule), but the selective attention weights are
(MDS) techniques are typically applied to collected stim-                adjusted based on a backpropagated error signal. The re-
ulus similarity ratings in order to discern the psychologi-              sulting weight update equations are:
cal space used by human learners (Shepard, 1962a; Shep-
                                                                                         ‚àÜwout
                                                                                             kj       Œª w tk     aout
                                                                                                                    k  aj
                                                                                                                          hid
ard, 1962b). Each input unit has its own dimensional at-                                                                           
tention weight, Œ±i . These weights are non-negative scalar
values that modulate the amount of attention paid to
the corresponding stimulus dimension. Higher Œ±i values
                                                                              ‚àÜŒ±i      ŒªŒ± ‚àë   ‚àë    tk    akout  wk j  ahid
                                                                                                                         j c  h ji
                                                                                                                               
                                                                                                                                      ain
                                                                                                                                       i 
                                                                                                                                           
                                                                                           j  k
magnify the differences between stimuli along the given
dimension, making them easier to discriminate based on                   where   ‚àÜwout
                                                                                    kj  is the adjustment value for the association
that dimension. As learning progresses, these weights                    weight from hidden unit j to output unit k, ‚àÜŒ±i is the ad-
are adjusted via the backpropagation of error algorithm.                 justment value for the attention weight for input unit i,
   The hidden layer in ALCOVE contains a set of units                    Œªw and ŒªŒ± are the learning rate parameters for the associ-
that are arranged in psychological space, one for each                   ation weights and attention weights, respectively, and tk
training exemplar. The activation level of each hidden                   is the target value for output unit k.
unit is determined by the following equation:
                                                                         Temporal Difference Learning
                                           
                                                           r q          Electrophysiological studies of the dopamine neurons
                                                        r
         ahid
            j     exp         c  ‚àë Œ±i  h ji   ain
                                                  i 
                                                      
                                                                         of the basal ganglia have suggested that the firing rates
                                                                         of these cells code for changes in expected future re-
                                   i
                                                                         ward (Shultz et al., 1997). This is particularly interesting
                                                                         because a measure of change in expected reward is the
where ahid j  is the activation of hidden unit j, c is the
                                                                         key variable of a reinforcement learning method called
specificity of the hidden units, Œ±i is the attention weight              temporal difference (TD) learning (Sutton, 1988). This
for input unit i, h ji is the preferred stimulus input for hid-          has led a number of researchers to develop TD learning
den unit j along stimulus dimension i, ain           i is the activa-    models of the role played by the midbrain dopamine sys-
tion value of input unit i, r is the psychological distance              tem in learning (Barto, 1994; Montague et al., 1996).
metric, and q is the similarity gradient. Hidden unit activ-                In the TD framework, a continuous reward value (r) is
ity is at a maximum when the inputs match the preferred                  delivered on each time step (t), with positive reward be-
stimulus of the unit (i.e., ain  i matches h ji ). This activation       ing desirable. A neural system called the adaptive critic
fades exponentially as the stimulus becomes more distant                 learns to predict expected future reward (V ), given fea-
from the preferred exemplar in psychological space, with                 tures of the current situation. When future rewards are
the c, r, and q parameters controlling exactly how activa-               exponentially discounted by a factor, Œ≥ (between 0 and 1),
tion decreases with psychological distance.                              with immediate rewards being valued more than tempo-
   Finally the output layer contains a set of units re-                  rally distant ones, the change in expected future reward
ceiving activation from the hidden layer via association                 between two consecutive time steps        is given
                                                                                                                                by:
weights. Each output unit corresponds to a category la-
bel that might be assigned to a stimulus. These units                                   Œ¥t       r t  Œ≥V t         V t     1
                                                                     1102

This Œ¥ value is called the temporal difference (TD) error.       (a)                                 (b)
The global TD error value can be used to drive learn-
ing in the adaptive critic, improving predictions of fu-
ture reward, and it can also be used to adapt connection
weights in neural networks which select actions, push-
ing those choices toward actions that regularly lead to
reward. Models of this kind have been used to explain
motor sequence learning in the striatum (Barto, 1994) as
well as other forms of learning. We propose that this
form of reinforcement learning may also be used to learn
dimensional attention weights that lead to correct cate-
gorization responses and, thus, reward.                         Figure 1: (a) ALCOVE Network Architecture. (b) Tile
                                                                Coding Of The Attention Map Layer ‚Äî A single unit is
                 Modeling Approach                              centered in each tile.
Applications of TD learning typically focus on choosing
an action from a discrete set. There is currently no clear
understanding of how to apply these methods to domains          ing weight update equation for     its single bias weight:
in which a continuous output is needed. Dimensional at-
tention weights are continuous parameters, however, so                            ‚àÜwi       Œªr r      ai f neti
some modification to standard TD learning is needed to
apply this technique to the adaptation of dimensional at-       where Œªr is the attention map learning rate, r is the re-
tention. We have devised two novel connectionist archi-         ward for the current trial, ai is the activation value of the
tectures to accomplish this. Our strategy encodes atten-        winning attention map unit, and f neti is the derivative
tional weight vectors (with one Œ±i weight per dimension)        of the unit‚Äôs activation function (which was the standard
across a single layer of standard connectionist process-        logistic sigmoid). Note that this is the standard method
ing units, called the attention map layer. Each unit in         for updating weights based on TD error, under the con-
this layer possesses a fixed preferred attentional weight       dition of absorbing
                                                                                      reward (i.e., we don‚Äôt predict reward
vector, and activation of a unit encourages the use of that     past the end of the trial). In this case, ai acts as our reward
unit‚Äôs preferred dimensional attention weights. The acti-       prediction (V t 1 ), and we do not predict beyond this
vation level of each unit is largely determined by its indi-    trial, so V t       0 and Œ¥ r ai . A reward value (r) of
vidual bias weight, and the TD learning method is used           1 was delivered to the network on trials in which AL-
to adapt these bias weights so as to optimize reward.           COVE selected the correct category label and produced
   At the start of each trial, each of these attention map      a confident response (i.e., all output units within 0 5 of  
units is activated, to some degree, by its bias weight.         their targets). A reward of 0 was delivered, otherwise.1
The units then compete to determine the set of dimen-              Our second attention map architecture used tile cod-
sional attention parameters to be used by ALCOVE, and           ing, resulting in a distributed representation of dimen-
the result of this competition is a set of such attention       sional attention. In this case, the attention map layer
weights. ALCOVE then processes the current stimulus             was partitioned into disjoint tilings, where each tiling
in its usual fashion, producing a categorization judgment.      contained a set of units with preferred dimensional at-
ALCOVE‚Äôs association weights are then modified in the           tention weight vectors that uniformly spanned the full
usual way, using the delta rule, but the dimensional atten-     weight space. The preferred weight vectors of the units
tion weights are handled differently. If ALCOVE confi-          in the various tilings were not identical, however, be-
dently chooses the correct category, it is rewarded. Oth-       cause each tiling was ‚Äúoffset‚Äù from the others, as shown
erwise, it is not. The TD error, Œ¥, is calculated based on      in Figure 1(b). To precisely represent a position in the
this reward signal, and this error is used to modify the        attention weight space, one unit in each tiling is acti-
bias weights of all active attention map units.                 vated, with the overlap in the tiles surrounding the po-
   Two different architectures for the attention map layer      sitions of these units determining the dimensional atten-
were investigated. The first of these used conjunctive          tion weights to be used. This kind of distributed rep-
coding, resulting in a localist representation of dimen-        resentation was originally used in the Cerebellar Model
sional attention. Under this scheme, the preferred atten-       Articulation Controller (CMAC) (Albus, 1975), and im-
tional weight vectors of processing units were distributed      proved generalization in TD learning systems has been
evenly throughout the weight vector space. Thus, each               1 An obvious alternative reward schedule involves stochas-
unit corresponded to a position in attention weight vec-                                                          
                                                                tically making category judgments based on P K and reward-
                                                                                                                     
tor space, and the positions of all of the units in the at-     ing any correct judgment. While we are currently investigat-
tention map layer formed a uniform grid in this space.          ing this approach, it is likely that it will produce behavior that
On each trial, a simple winner-take-all competition deter-      deviates substantially from that of standard ALCOVE. Dimen-
                                                                sional attention weights do not change much in ALCOVE until
mined the one unit whose preferred weight vector would          the network starts to make strong responses. This is part of
specify the distribution of attention for that trial. Learn-    the ‚Äúthree-stage learning‚Äù profile that ALCOVE exhibits. Our
ing occurred only for the winning unit, using the follow-       reward schedule encourages this pattern of learning.
                                                            1103

found to result from their use (Sutton, 1996). Previous           the Type 2 structure requires that only two of the di-
models have used such representational schemes to en-             mensions be attended, while Types 3, 4, 5, and 6 all
code network inputs, but here they have been used in a            require attention to all three, in order of increasing di-
novel way to select dimensional attention weights. As in          mensional significance. The speed with which humans
the conjunctive coding architecture, each attention map           learn these categories matches this ordering of tasks, but
unit is activated by a bias weight, and a competition en-         models that lack a dimensional attention mechanism fail
sues between units. In the tile coding scheme, the most           to learn Type 2 categories faster than some of the more
active unit across all tilings restricts activity in the other    difficult categories. Kruschke (1992) showed that AL-
tilings to those that are close to the winning unit (i.e.,        COVE, with its adaptive dimensional attention mecha-
units whose tiles overlap with that of the winning unit).         nism, learned Type 2 tasks at a relative rate comparable
This competitive process is recursively applied to tilings        to human learners. We have replicated these simulations
that do not contain the winning unit until one unit is ac-        (using bounded attention weights and learning after ev-
tive in each tiling, and the tiles corresponding to these         ery trial), and the results are shown in Figure 2.
units all overlap. The attention weight vector at the cen-            We applied our reinforcement learning version of AL-
ter of this overlapping region is then used by ALCOVE             COVE to these six categorization tasks. Since stimuli
to process the current stimulus. Once feedback is pro-            had three dimensions, the attention weight space was
vided, reward is calculated as in the conjunctive coding          three-dimensional. The conjunctive coding model used
case, and TD learning is used to adjust the bias weights          a 15 15 15 unit topology in its attention map layer
                                                                              
of all of the winning units in the attention map layer.           (3375 units total), while the tile coding model used five
     In standard ALCOVE, the initial attention weights are        tilings of 9 9 9 units each (3645 units total). The re-
                                                                                     
often set to be all equal and to sum to one. This effec-          sults of these simulations are shown in Figure 2. Note
tively emphasizes all dimensions equally at the start of          that our models learn Type 2 categories faster than the
training. We selected initial bias weights in the attention       higher numbered types, just as ALCOVE does. Model
map layer so as to form a similar initial bias in our mod-        parameter values were manually selected to produce per-
els. The unit in the attention map whose preferred at-            formance that matched the category learning times ex-
tention weight vector matched ALCOVE‚Äôs standard ini-              hibited by ALCOVE. These results demonstrate that TD
tial attention weights was given a maximum bias weight            learning can adapt dimensional attention weights so as to
(0 05), and the bias weights assigned to other units fell
   
                                                                  speed category learning.
off
       in a Gaussian fashion as the distance from this peak
increased (in attention weight space), bottoming out at           Categorization of Continuous Separable Stimuli
   0 05. A small  amount of uniformly sampled noise was
      
                                                                  In order to demonstrate the ability of our models to quan-
then injected into each bias weight, and the result was           titatively fit human performance on categorization tasks
clipped to the 0 05 0 05 range. The variance of the
                            
                                                                  involving stimuli with continuous and separable dimen-
Gaussian and the range of the injected noise were free            sions, we applied these models to an experiment con-
parameters of the model.                                          ducted by Nosofsky (1986) . The stimuli in this experi-
                                                                  ment consisted of semicircles that varied in size and con-
                            Results                               tained a radial line oriented at different angles. These
In order to assess the ability of our reinforcement-based         stimuli were to be categorized as members of one of two
dimensional attention mechanism to account for human              categories, and four different category structures were
performance, we applied our models to several previ-              explored (see Figure 3). The frequency with which each
ously reported category learning studies. The perfor-             of the sixteen possible stimuli were placed in a target cat-
mance of our modified version of ALCOVE was com-                  egory was measured after training, and the GCM was fit
pared to that of the standard version of ALCOVE and to            to these response probabilities.
the performance of the GCM. In all cases, the values of               We fit both standard ALCOVE and our reinforcement
dimensional attention weights were bound between zero             learning models to this data, as well. Since the stimu-
and one. (This was only a new upper bound for AL-                 lus space was two-dimensional, our models used a two-
COVE, which standardly forces these weights to be non-            dimensional attention map layer. In the conjunctive cod-
negative.) In all of the learning models, weights were            ing case, a 15 15 unit topology was used (225 units
                                                                                    
updated after every simulated trial.                              total), and the tile coding model used 9 tilings of 5 5 
                                                                  units each (225 units total). While both schemes used
Dimensional Attention & Learning Difficulty                       the same number of units, the tile coding model dis-
Shepard et al. (1961) examined the effect of category             cretized the space with a much greater resolution. Stim-
structure on the relative speed with which a category             uli were presented to the models using the MDS code
is learned. Stimuli were composed of three easily sep-            found by Nosofsky. Free parameters of the models were
arable binary dimensions, for a total of eight possible           fit to Nosofsky‚Äôs Subject 1 data for each category struc-
stimuli. Six category structures were examined, ordered           ture separately. A simple hill-climbing optimization al-
approximately by increasing number of relevant dimen-             gorithm on sum-squared error was used.
sions. Thus, the Type 1 category structure requires at-               The quality of the resulting fits are summarized in Ta-
tention to only one binary dimension to solve the task,           ble 1. While the original ALCOVE model provided the
                                                              1104

                                                ALCOVE                                Conjunctive Code                            Tile Code
                               1.0                                       1.0                                      1.0
                  P(correct)
                               0.8                            Type 1 0.8                                Type 1 0.8                            Type 1
                                                              Type 2                                    Type 2                                Type 2
                                                              Type 3                                    Type 3                                Type 3
                                                              Type 4                                    Type 4                                Type 4
                               0.6                            Type 5 0.6                                Type 5 0.6                            Type 5
                                                              Type 6                                    Type 6                                Type 6
                                     0   10      20    30     40    50         0     10     20     30   40   50         0   10     20   30    40   50
                                                  epoch                                      epoch                                  epoch
Figure 2: Model Learning Curves For Shepard‚Äôs 6 Tasks ‚Äî One epoch involves one trial with each distinct stimulus.
                                              Category Structure                                    The GCM provides the best fits to the data in this
       Model            1                        2         3                  4                  study. It seems that the ALCOVE model and our models
       GCM           99.93%                   94.73% 84.52%                98.31%                had trouble learning Category Structure 3. This is a dif-
   ALCOVE            99.65%                   96.45% 86.62%                98.61%                ficult category structure which benefits little from selec-
  Conj. Code         99.51%                   95.84% 86.01%                97.72%                tive attention to specific dimensions. Note, however, that
   Tile Code         99.54%                   95.62% 83.55%                97.72%                the fits of our reinforcement learning models are close to
                                                                                                 the standard ALCOVE fits, and our models continue to
Table 1: Model Fits To Nosofsky (1986) ‚Äì Percent Vari-                                           exhibit the same trends in learning as ALCOVE.
ance Accounted For
                                                                                                                                 Discussion
                                              Category Structure                                 Our results show that established computational models
      Model      1                  2            3         4          5               6
      GCM     99.10%             98.30%       97.20% 99.80%        98.20%          99.20%        of the brain‚Äôs dopamine system can provide an adequate
  ALCOVE      98.56%             99.29%       93.53% 99.79%        98.34%          98.71%        replacement for the biologically implausible backprop-
 Conj. Code   98.44%             98.16%       92.51% 99.57%        97.94%          98.53%        agation of error method for adapting dimensional atten-
  Tile Code   98.25%             97.27%       91.15% 99.15%        97.80%          97.30%
                                                                                                 tion during category learning. The new models were able
                                                                                                 to learn useful dimensional attention weights from their
Table 2: Model Fits To Nosofsky (1987) ‚Äì Percent Vari-
                                                                                                 less-informative global reinforcement signal. This sug-
ance Accounted For                                                                               gests that cognitive mechanisms for allocating dimen-
                                                                                                 sional attention may not be as precise as those posited
                                                                                                 by the original ALCOVE model.
best overall fits, our models matched the data almost as
                                                                                                    One noteworthy feature of our reinforcement learning
well, and all general trends in the ALCOVE and GCM
                                                                                                 models was their tendency to exhibit fluctuations in per-
fits are present in our models. This suggests that our
                                                                                                 formance over training, rather than smooth and mono-
mechanisms for learning dimensional attention can quan-
                                                                                                 tonic learning as displayed by the original ALCOVE
titatively capture human performance on learning tasks
                                                                                                 model. If each network model is to mirror the perfor-
that require selective attention to separable dimensions.
                                                                                                 mance of an individual learner, these performance fluc-
Categorization of Continuous Integral Stimuli                                                    tuations may reflect stochasticity commonly observed in
                                                                                                 individual behavior. Also, if performance is averaged
Integral stimulus dimensions often entail a difficulty in                                        across multiple ‚Äúsimulated individuals‚Äù, smooth learning
focusing attention on individual dimensions. Despite                                             curves, like those generated by ALCOVE, are produced.
this fact, Nosofsky (1987) showed that models equipped                                              Our models encoded dimensional attention weights in
with a dimensional attention mechanism fit human cat-                                            a fairly conjunctive fashion, with individual units in the
egorization performance on such stimuli slightly better                                          attention map layer specifying levels of attention for all
than models that lacked such a mechanism. This study                                             of the dimensions. This is needed because the appropri-
involved 12 different color chips which varied in satu-                                          ateness of attention to one dimension depends on how
ration and brightness. Six different category structures                                         attention is allocated to the other dimensions. Such a
were used, and these are shown in Figure 3. The fre-                                             conjunctive encoding requires very large attention map
quency with which each of the 12 stimuli were placed in                                          layers, however, and this may limit the scalability of this
a target category was measured after training, and, once                                         approach. In order to address this issue, we are currently
again, the GCM was fit to these response probabilities.                                          exploring more compact distributed representations for
   We applied both the original ALCOVE and our re-                                               dimensional attention weight vectors.
inforcement learning versions to this human data. The                                               Eventually we hope to modify ALCOVE to make use
same attention map layer sizes as used in the previous                                           of additional biologically plausible mechanisms of neu-
simulations were used here, and, as before, MDS repre-                                           ral computation. This work represents the first step in
sentations of the stimuli were presented to the models. A                                        this process, identifying a biologically realistic method
summary of the model fits is shown in Table 2.                                                   for governing dimensional attention.
                                                                                            1105

                     Figure 3: Category Structures Used In Nosofsky (1986) and Nosofsky (1987)
                  Acknowledgments                              Nosofsky, R. M. (1987). Attention and learning pro-
The authors extend their thanks for helpful comments on             cesses in the identification and categorization of in-
this work to Tom Palmeri, the members of the Vanderbilt             tegral stimuli. Journal of Experimental Psychology:
University Computational Cognitive Neuroscience Lab-                Learning, Memory, and Cognition, 13(1):87‚Äì108.
oratory, and three anonymous reviewers.                        O‚ÄôReilly, R. C. (1996). Biologically plausible error-
                                                                    driven learning using local activation differences:
                      References                                    The generalized recirculation algorithm. Neural
Albus, J. S. (1975). A new approach to manipulator con-             Computation, 8(5):895‚Äì938.
     trol: The cerebellar model articulation controller
     CMAC. Journal of Dynamic Systems, Meaasure-               Rumelhart, D. E., Hinton, G. E., and Williams,
     ment, and Control, 97(3):220‚Äì227.                              R. J. (1986). Learning representations by back-
                                                                    propagating errors. Nature, 323:533‚Äì536.
Barto, A. G. (1994). Adaptive critics and the basal gan-
     glia. In Houk, J. C., Davis, J. L., and Beiser, D. G.,    Shepard, R. N. (1962a). The analysis of proximities:
     editors, Models of Information Processing in the               Multidimensional scaling with an unknown dis-
     Basal Ganglia, pages 215‚Äì232. MIT Press.                       tance function. I. Psychometrika, 27:125‚Äì140.
                                                               Shepard, R. N. (1962b). The analysis of proximities:
Crick, F. M. C. (1989). The recent excitement about neu-
                                                                    Multidimensional scaling with an unknown dis-
     ral networks. Nature, 337:129‚Äì132.
                                                                    tance function. II. Psychometrika, 27:219‚Äì246.
Kruschke, J. K. (1992). ALCOVE: An exemplar-based              Shepard, R. N., Howland, C. L., and Jenkins, H. M.
     connectionist model of category learning. Psycho-              (1961). Learning and memorization of classifica-
     logical Review, 99:22‚Äì44.                                      tions. Psychological Monographs, 75(13 Whole
                                                                    No. 517).
Montague, P. R., Dayan, P., and Sejnowski, T. J. (1996).
     A framework for mesencephalic dopamine systems            Shultz, W., Dayan, P., and Montague, P. R. (1997). A
     based on predictive hebbian learning. Journal of               neural substrate of prediction and reward. Science,
     Neuroscience, 16:1936‚Äì1947.                                    275:1593‚Äì1599.
Nosofsky, R. M. (1984). Choice, similarity, and the con-       Sutton, R. S. (1988). Learning to predict by the methods
     text theory of classification. Journal of Experimen-           of temporal differences. Machine Learning, 3:9‚Äì44.
     tal Psychology: Learning, Memory, and Cognition,
     10(1):104‚Äì114.                                            Sutton, R. S. (1996). Generalization in reinforcement
                                                                    learning: Successful examples using sparse coarse
Nosofsky, R. M. (1986). Attention, similarity, and                  coding. Advances in Neural Information Processing
     the identification-categorization relationship. JEP:           Systems, 8:1038‚Äì1044.
     General, 115(1):39‚Äì57.
                                                           1106

