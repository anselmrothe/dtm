UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Similarity and Taxonomy in Categorization
Permalink
https://escholarship.org/uc/item/7dm0t6g0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Verbeemen, Timothy
Stroms, Gert
Verguts, Tom
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                   Similarity and Taxonomy in Categorization
                              Timothy Verbeemen (timothy.verbeemen@psy.kuleuven.ac.be)
                                        Departement Psychologie, K.U.Leuven, Tiensestraat 102
                                                         B-3000 Leuven, Belgium
                                         Gert Storms (gert.storms@psy.kuleuven.ac.be)
                                        Departement Psychologie, K.U.Leuven, Tiensestraat 102
                                                         B-3000 Leuven, Belgium
                                              Tom Verguts (tom.verguts@ugent.be)
                              Vakgroep Experimentele Psychologie, Ghent University, H. Dunantlaan 2
                                                          B-9000 Ghent, Belgium
                              Abstract                                  employed in making category decisions may be stored at the
In this paper, a two by three approach to modeling categorization is    category level, or it may be stored at the level of individual
presented. Similarity representations based upon a geometric, an        instances of a category. The former approach is known as
additive tree and an additive cluster model are combined with an        the prototype view (e.g., Hampton, 1979; Smith & Minda,
exemplar model and a prototype model in a single approach. The          1998), and the latter as the exemplar view (e.g., Medin &
six models are applied to the categorization of pictorial known and
unknown fruits and vegetables (Smits et al., 2002). For novel
                                                                        Schaffer, 1978; Nosofsky, 1986). In this paper, we argue for
stimuli, the geometric exemplar model and the cluster models gave       a systematic evaluation of these formal models in a two by
the best account, indicating a strategy where people compare            three approach that compares prototype and exemplar
stimuli with stored members on more general continua or a limited       models on the one hand, and geometric and feature
set of features. For well-known stimuli, the tree-based models          representations on the other hand.
gave the best account of the data, suggesting more elaborate
taxonomic knowledge. More generally, the results show that                     The Generalized Context Model and a
different categorization models may perform better for different
sets of stimuli, and that a systematic empirical comparison of such                   Geometric Prototype Model
models is needed.                                                       In the generalized context model (GCM; Nosofsky, 1984,
                                                                        1986, 1992), an exemplar model, categorization is assumed
                          Introduction                                  to be a function of similarity towards all relevant stored
A major contribution of categorization research over the last           exemplars. In case (physical) dimensions are unavailable,
decades has been to establish the relation between similarity           the GCM fitting procedure starts with a multidimensional
and categorization. Rosch and Mervis’ (1975) seminal paper              scaling procedure (MDS; Borg & Groenen, 1997) on
on the graded structure of categories showed that categories            proximity measures of all stimuli involved. The coordinates
are ill-defined, and that the extent to which an instance of a          of these stimuli are then used as input for the model. In the
category is seen as a typical member is positively related to           case of two categories, A and B, the probability that stimulus
similarity towards the category in question and inversely               x is classified in category A is given by:
related to similarity towards relevant contrast categories
(e.g., Verbeemen et al., 2001). Given the importance of                                     βAηXA
                                                                         P(A | X) =                                                (1)
similarity in categorization, a formal model should take a                          βAηXA + (1 − βA)ηXB
clear stance on two issues: The nature of similarity
computation and the relevant objects of comparison in this              where βΑ lies between 0 and 1 and serves as a response
calculation. First, the model must make assumptions about               bias parameter towards category A. The parameters ηXA and
the nature of similarity, especially when the structure of the          ηXB denote the similarity measures of stimulus x toward all
stimuli under investigation is not experimentally                       stored exemplars of category A and B, respectively:
controllable. There are two main approaches to similarity,
geometric and feature-based. The geometric approach (e.g.,                                  D               1/ r 
Carroll & Arabie, 1980; Shepard, 1964) represents stimuli                                                   r 
                                                                        η XA =  ∑   exp− c wk y xk − y jk
                                                                                               ∑                                  (2)
in abstract space where similarity is inversely related to the                                                  
distance between stimuli. In the feature-based approach (e.g.
                                                                                j∈A
                                                                                            k =1               
Shepard & Arabie, 1979; Tversky, 1977), similarity is
considered a function of feature overlap, where                         with yxk and yjk as the coordinates of stimulus x and the j-th
commonalities increase and differences decrease overall                 stored exemplar of category A (or B for ηXB, respectively)
similarity. Second, a model should specify the objects used             on dimension k. The weight of the k-th dimension is denoted
in similarity calculation. In particular, information                   by wk, with all weights restricted to sum to 1. The power
                                                                    1393

metric, determined by the value of r, is usually given a         from similarity data and provided excellent accounts of an
value of either 1 or 2, corresponding to city-block and          artificial learning experiment with ALCOVE (Kruschke,
Euclidean distance, respectively.                                1992). Takane and Shibayama (1992) analyzed
    A prototype model can be constructed with the GCM as a       identification data of digits taken from Keren and Baggen
start (Nosofsky, 1986, 1987, 1992; Smits et al., 2002). With     (1981) and they too obtained excellent results for a featural
the prototype defined as the central tendency of a category      version of the similarity-choice model (Luce, 1962) based
(Malt & Johnson, 1992; Malt & Smith, 1984; Rosch &               on ADDTREE (Corter, 1982; Sattath & Tversky, 1977).
Mervis, 1975), the object created by taking, on each             Whereas clustering provides a very flexible way of
dimension, the average coordinate over all members of the        representing similarity, allowing for overlapping clusters,
category, is a good way to define a prototype. The similarity    additive trees are more restrictive in that they impose a
function changes to:                                             hierarchy. There are, however, reasons to apply tree models,
                                                                 especially in the case of conceptual knowledge. A tree
                 D                   1/ r                     model produces, in general, a higher amount of features for
                                   r 
η XA    = exp− c wk y xk − y.k
                    ∑                                  (3)     the total set than additive clustering. But the amount of
                                                             shared features is lower in general, and most weight is given
                 k =1                   
                                                                 to idiosyncratic features. This may be appropriate for well-
                                                                 known stimuli (McCrae & Cree, 2002), as people can be
where y.k denotes the mean value of all stored members of        expected to have a fair amount of background knowledge
category A on dimension k. We will refer to (3) in               about these stimuli, but it is unclear whether this is plausible
combination with (1) as the Geometric Prototype Model            in the case of novel stimuli.
(GPT).                                                              The implementation of the feature structure in the GCM
    A number of studies have already been conducted that         yields the featural exemplar model with similarities as:
compared prototype and exemplar models (e.g., Nosofsky,
1992; Nosofsky & Zaki, 2002; Smith & Minda, 1998, 2000;
Smits et al., 2002). In many, the GCM performed better than                            F                                        
prototype models. In the next section we elaborate on the
major alternative to geometric similarity models, the
                                                                 η XA =  ∑   exp− c wk y xk (1 − y jk ) + y jk (1 − y xk )
                                                                                        ∑ (                                 )      (5)
                                                                         j∈A
                                                                                       k =1                                    
contrast model (Tversky, 1977).
                                                                 where yjk = 1 if stimulus j has feature k and yjk = 0
       The Contrast Model and Categorization                     otherwise. Therefore, the term yxk (1-yjk) is 1 if and only if
In the contrast model, similarity between two stimuli is         the target stimulus x possesses the feature and the
defined as a function of the features that these stimuli         “reference” stimulus j does not, and vice versa for yjk (1-yxk).
possess:                                                         Each feature has a weight wk that corresponds to the length
                                                                 of the segments in the tree. We will refer to this model as
 Sim(a, b) = θg ( A I B ) − αf ( A − B) − βf ( B − A)    (4)     GCM-F (generalized context model – featural).
                                                                    The featural prototype model will be illustrated using
where g ( A I B) is a function of the features shared by         Figure 1, for an additive tree solution for birds and
objects a and b (the common features), and f ( A − B) and        mammals. Distances between objects are defined by the
                                                                 sum of the horizontal segments on the shortest path between
 f ( B − A) are functions of the features that belong to one
                                                                 two stimuli (vertical segments are added for visual ease
stimulus but not the other (the distinctive features).           only). Each segment represents a feature that applies to all
Different models have been proposed, mostly focused on           of its children with more general features closer to the left
either the common feature component or the distinctive           (“root”) and more specific features located towards the right
feature components.                                              (endpoints) of the tree. The model is again formally similar
    Pruzansky, Tversky and Carroll (1982) reanalyzed 20 data     to the featural GCM with the prototype treated as a pseudo-
sets taken from various published studies, divided into two      exemplar. The distance function equals:
groups depending on the hypothesized structure of the
stimuli used: conceptual (e.g., vegetables) and perceptual
                                                                                  F                                                 
(e.g., polygons) stimuli. For 10 out of 11 studies of
conceptual stimuli, analyses of proximity data proved better                                (                                     )
                                                                 η XA = exp− c ∑ wk y xk (1 − y pk ) + freq pk y pk (1 − y xk )   (6)
                                                                               
when performed by ADDTREE, a distinctive features                               k =1                                              
approach to similarity. Seven out of nine studies of             where ypk is 1 if the prototype of A has the feature, and 0
perceptual stimuli showed a clear advantage for low-             otherwise. The frequency weighting term corresponds to the
dimensional MDS solutions.                                       relative frequency or proportion with which the feature
    A number of studies have been conducted that compared
geometric and featural exemplar models. Lee and Navarro
(2001) used additive clustering to extract common features
                                                             1394

                                                                          Schwanenflugel, 1981) for stored (well known) categories2.
                                                                          (This implies that one has to decide, a priori, which objects
                                                                          are considered to be stored members of a given category, as
                                                                          is the case for all other models. The choice of a root that
                                                                          best approximates separability serves to define the feature
                                                                          structure and not category membership for stored items,
                                                                          which is already determined.)
                                                                                       Analysis of Smits et al.’s data
                                                                          Smits et al. (2002) analyzed a stimulus set consisting of
                                                                          pictures of 79 well-known items, retained after an exemplar
                                                                          generation task for the categories fruit and vegetables, and
                                                                          30 fruits or vegetables, mostly exotic, that were completely
                                                                          unknown to participants. Ten participants completed a
                                                                          feature applicability task for all stimuli, for the 17 most
           Figure 1: Example of a rooted additive tree.                   frequently generated features for fruit and vegetables,
                                                                          generated by a different group of thirty participants. (Taking
occurs within A, i.e. the proportion of stored members of                 the most frequently generated features ensures that the
category A that possess that particular feature. This                     analysis is not clouded by potentially unreliable features that
corresponds to the idea that the impact of the features in the            are important to only a few subjects.) A similarity matrix
prototype, which is seen as a pseudo-exemplar, depends on                 was then obtained by correlating the feature applicability
the prevalence of those features in the category1. We will                vectors for all 109 stimuli, after summing over participants.
refer to this model as FPT (featural prototype model).                    A different group of thirty participants classified the well-
   The application of the featural models to an additive                  known stimuli as belonging to either fruit or vegetables. A
clustering solution, i.e. a common features model, is                     group of twenty different participants did the same for the
straightforward, as the feature structure is defined. This is             novel stimuli. Smits et al. then predicted category decisions
not the case for additive trees as they produce distinctive               based on the geometric versions of the GCM and the GPT
features: a feature that adds to the difference between two               and found a clear advantage of the GCM over the prototype
stimuli may belong to one or the other. This is not a                     model. Since their data from the categorization task had a
problem for exemplar models as distances between objects                  fair amount of variance in the categorization proportions
remain unchanged, but it will be required for a prototype                 even for the well-known stimuli, it is possible to fit the
model. To define a particular structure, one needs to define              respective models to old and novel stimuli separately.
a root. If the root in Figure 1 is placed anywhere else it                Therefore, we will analyze the data in a 2 × 3 × 2
would imply that some members of one category possess                     framework, where the last factor is added to assess the fit of
some of the most general features of the other category but               the models for old and novel stimuli separately.
share none of the features belonging to members of their
proper category. This is implausible as it would imply that               Generating Similarity Representations
some stimuli are seen as members of a category on a purely
                                                                          In order to obtain dimensions, similarities between 109 old
idiosyncratic basis and not because they share any features
                                                                          and novel fruits and vegetables were reanalyzed with
with that category, even though these stimuli would possess
                                                                          ALSCAL (Takane, Young & De Leeuw, 1977), using the
the most general features of a related contrast category.
                                                                          BIC criterion (Schwarz, 1978)3 to determine the optimal
Therefore, in the remainder of this article, we will assume
that the root is placed on the segment or path that best                  2
                                                                            ADDTREE starts by grouping together the closest pair of objects,
approximates this linearly separable structure (Medin &                   and then creates a dummy object with the average of the distance
                                                                          of the original objects to all other objects. This procedure is
1
   As the similarity structure was derived from the presented             repeated until there are three objects left, and the root is placed so
stimuli, we assume that a presented exemplar has all of its features      as to minimize the variance to the last three objects. Here we
to the full extent. Because a prototype is a construction after           minimize the variance on the path that provides the best linear
encountering exemplars, we assume that the activation and impact          separation for well-known stimuli in a similar way.
                                                                          3
of its features is dependent on the frequency of those features in its      BIC = -2 ln(L) + k ln(n), where L is the likelihood value (the
own category (Kellogg, 1980). Because the prototype is treated as         probability of the data given a certain model), k is the number of
a pseudo-exemplar, we assume that its features can be no more             free parameters, and n is the number of data points. Lower means
than fully active (in the case of a feature that applies to all of its    better, and only the difference in free parameters needs to be taken
members), resulting in a factor of 1. We assumed that the features        into account. The first term decreases with increasing model fit, the
of stored exemplars in the exemplar model were not weighted               second is a penalty term that increases with the number of free
dependant on the frequency of occurrence in other exemplars. This         parameters and data size. As such, the measure is a trade-off
was confirmed post hoc: fit values were much worse when                   between model fit and model complexity. The statistic is most
weighted for frequency.                                                   appropriate when the information provided by the data is relatively
                                                                          large as compared to any prior information, as is the case in all the
                                                                      1395

dimensionality (Lee, 2001b). A three-dimensional solution               cluster-based exemplar and prototype models is small. All
was chosen that explains 96 percent of the variance.                    ADDTREE-based models performed clearly worse. The
   Following the same procedure for additive clustering, an             current results do not clearly favor exemplar or prototype
analysis with ADCLUSGROW (Lee, 2001a) resulted in 32                    models for novel stimuli, but it appears that the geometric
clusters that explained 96 percent of the variance.                     approach to prototypes provides less explanatory power as
   Finally, the same similarity matrix was reanalyzed using             compared to the clustering approach.
ADDTREE/P (Corter, 1982). The explained variance was
84 percent. The algorithm does not readily lend itself to the             Table 1: -ln(L)5 and BIC (only the difference in parameters
BIC-guided approach and usually fits to maximum                            taken into account) for the category fruit for all models.
complexity, in this case with 209 arcs (features). To the
extent that this may cause the fitting of error, it may cause a                        MDS              ADCLUSGROW           ADDTREE/P
drawback for the categorization models as the error would                           GCM        GPT       GCM-F       FPT     GCM-F        FPT
be “plugged” into the model, clouding the explanatory                   1.New
power of the underlying feature structure. At first sight this,         -ln(L)       73.95     83.27       81.38     82.12     90.83      93.01
and the lower fit value, would indicate that these models are           BIC        160.69     179.33     162.76    164.24     181.66     186.02
less appropriate.
                                                                        2.Well-
   (It is important to note that these analyses were based on
                                                                        known
correlation patterns and not on the rough feature vectors, so
                                                                        -ln(L)     351.75     405.29     364.87    388.51     334.75     330.18
the similarity algorithms, especially in the featural
                                                                        BIC        719.04     826.12     729.74    777.02     669.50     660.36
approach, are in no way restricted to have as much or less
features than the original feature vectors.)
                                                                           Analyses of the well-known stimuli resulted in a very
Fitting the Similarity Representations to the                           different pattern. BIC values for the analyses of the 79 well-
                                                                        known stimuli separately are presented in the second panel
Categorization Models
                                                                        of Table 1. The BIC values for the geometric and the
The geometric models were fitted with the Euclidean                     cluster-based models were clearly higher (worse) than for
distance metric (r = 2), as this resulted in clearly better fit         the tree models. Clearly, the data from the well-known
values. The GCM and GPT were fitted as discussed                        subset is best accounted for by the tree-based models that
previously with four free parameters: the bias parameter β,             assume more elaborate taxonomic knowledge. The
the sensitivity parameter c, and two dimension weights, as              difference between the exemplar and prototype model is
weights are restricted to add to 1. Feature weights for the             rather small and should be interpreted with caution. This
tree- and cluster-based models were taken from the original             result appears to contradict the earlier fit values where the
solutions, however, to keep the number of parameters                    MDS and additive clustering solutions provided a
feasible for estimation, hence there are two free parameters,           substantially better fit to the similarity data. In fact, a better
β and c. Stored members are the same in all models and are              fit to similarity data need not imply a better fit of the
based on the earlier exemplar generation task. (Note that the           categorization model: those aspects of stimuli that are
tree-based models were based on the original ADDTREE                    activated in a similarity task may very well be different
solution after placing the root so as to provide the best linear        from what is activated in a categorization task, especially
separation, for known stimuli, between fruit and vegetables.            after a concept has become well-elaborated. In other words,
Compared to the actual generation task for well-known                   the less flexible and hierarchic structure of trees may not
stimuli, only one item, rhubarb, was generated as a fruit but           have captured all aspects of similarity, but the aspects it did
closer to vegetables according to the ADDTREE solution.4)               capture may be more relevant for categorization of well-
                                                                        known concepts. Indeed, every aspect of similarity that is
Results and Discussion All models were fitted by                        not used in categorization can be considered error in the
maximizing the binomial likelihood. Correlations between                model.
predicted and observed category decisions ranged from .85                  In fact, the most interesting pattern that emerges from
to .93 with the best performing models ≥ .92, indicating a              these data is the fact that categorization of novel stimuli is
fair but not perfect amount of explained variance. The                  best explained by those models that are based on the flexible
models were evaluated using BIC. Results are summarized                 representations that best explain similarity. These models
in Table 1.                                                             have either a limited number of dimensions or a limited
   Analyses of the 30 novel stimuli separately are presented            number of features, with little idiosyncratic features in the
in the first panel of Table 1. The lowest (best) BIC value
was obtained for the GCM, but the difference with the                   5
                                                                           This value is the most “democratic” measure as it only
                                                                        incorporates model fit, (incorrectly) disregarding the penalty term
                                                                        for free parameters. The measure is equal to the sum of minus the
analyses presented here. It is also fit to compare nonnested models.    log likelihoods of the individual data points and is therefore
(For an extensive discussion, see Kass & Raftery, 1995.)                sensitive to the size of the data set; hence differences in fit between
4
  In the actual classification task (not the generation task), the      the two datasets are not directly interpretable (the same is true for
proportion of classifications for rhubarb as a fruit was only .33.      the BIC measure).
                                                                    1396

latter case. Categorization of well-known stimuli, on the             determining the relevant underlying dimensions or features
other hand, is best explained by the models that use a                for categorization with natural language concepts is perhaps
representation that is less close to the similarity data but that     the most crucial problem in modeling natural language
impose a more elaborate taxonomy and more idiosyncratic               categories (see, e.g., Murphy & Medin, 1985). The two by
features.                                                             three framework that was presented here may serve as a
   An interesting interpretative property of additive trees in        valuable tool in this endeavor.
this respect is the fact that, at each node of the tree, a feature
that applies to all of its children is linked to a limited                                Acknowledgments
number of alternatives. Second, branches in the tree tend to          The first author is a research assistant of the Fund for
have a higher weight as one goes down in the tree. This               Scientific Research – Flanders. This project was in part
implies that the number and weight of commonalities                   sponsored by grant OT/01/15 of the University of Leuven
decreases with the number of nodes between stimuli. It also           research council to Gert Storms.
means that those features that add most weight to the
difference are less likely to be related as the number of                                      References
nodes increases and vice versa. A similar argument was
made by Markman & Gentner (1993) who presented stimuli                Bailey, T. M., & Hahn, U. (2001). Determinants of
with different ontological distances and found a similar                wordlikeness: Phonotactics or lexical neighborhoods?
pattern when subjects listed commonalities and alignable                Journal of Memory and Language, 44, 568-591.
and nonalignable differences. A possible explanation for the          Borg, I., & Groenen, P. (1997). Modern Multidimensional
good results of tree-based models could be that, as a concept           Scaling: Theory and Applications. Springer-Verlag, New
becomes more elaborated, people tend to gravitate to an                 York.
alignable structure that might dominate other, presumably             Carroll, J. D., & Arabie, P. (1980). Multidimensional
less alignable, aspects of similarity.                                  scaling. Annual Review of Psychology, 31, 607-649.
                                                                      Corter, J. E. (1982). ADDTREE/P: A PASCAL program for
Conclusion                                                              fitting additive trees based on Sattath & Tversky’s
                                                                        ADDTREE algorithm. Behavior Research Methods &
The goal of the present paper was two-fold. First we                    Instrumentation, 14, 353-354.
presented a general framework, in which different models              Hampton, J. A. (1979). Polymorphous concepts in semantic
(i.e., exemplar and prototype models, embedded in either                memory. Journal of Verbal Learning and Verbal
dimensional or featural similarity representations) could be            Behavior, 18, 441-461.
systematically formulated, compared and tested. Given the             Kass, R. E. , & Raftery, A. E. (1995). Bayes factors. Journal
framework, one can investigate precisely in what situations             of the American Statistical Association, 90, 773-795.
which model aspects perform best. Second, the framework               Kellogg, R. T. (1980). Simple feature frequency versus
was applied to categorization data of well-known and novel              feature validity models of formation of prototypes.
stimuli in the context of familiar natural language concepts.           Perceptual and Motor Skills, 51, 295-306.
The results indicate that, depending on the amount of                 Keren, G., & Baggen, S. (1981). Recognition of
knowledge and mastery of the stimuli, different                         alphanumeric characters. Perception and Psychophysics,
representational structures and different decision processes            23, 234-246.
may operate.                                                          Kruschke, J. K. (1992). ALCOVE: An exemplar-based
   One may wonder how these results relate to the findings              connectionist model of category learning. Psychological
from the category learning literature (e.g., Nosofsky, 1992;            Review, 99, 22-44.
Smith & Minda, 2000; Stanton, Nosofsky & Zaki, 2002). In              Lee, M.D. (2001a). On the complexity of additive clustering
most of these studies, exemplar models embedded in                      models. Journal of Mathematical Psychology, 45, 131-
multidimensional representations have been shown to                     148.
account very well for the categorization data. However, in            Lee, M. D. (2001b). Determining the dimensionality of
these studies, artificial categories are used almost                    multidimensional scaling representations for cognitive
invariably, with stimuli that vary along a limited number of            modeling. Journal of Mathematical Psychology, 45, 149-
salient dimensions. Formal models, such as the ones                     166.
described in our paper, have seldom been applied to natural           Lee, M. D., & Navarro, D. J. (2002). Extending the
language concepts, which are far more complex than the                  ALCOVE model of category learning to featural stimulus
stimuli used in the artificial category literature, and of which        domains. Psychonomic Bulletin and Review, 9, 43-58.
our participants arguably have a much richer and more                 Luce, R. D. (1962). An observable property equivalent to a
elaborate knowledge than even the best trained participants             choice      model     for   discrimination    experiments.
have of artificial stimuli. (For other attempts to apply formal         Psychometrika, 27, 163-167.
models to natural language concepts, see Bailey & Hahn,               Malt, B. C., & Johnson, E. C. (1992). Do artifact concepts
2001; Smits et al., 2002; Storms, De Boeck, & Ruts, 2000,               have cores? Journal of Memory and Language, 31, 195-
2001; Verbeemen et al., 2001.) However, in spite of                     217.
participants’ extensive knowledge of such concepts,
                                                                  1397

Malt, B. C., & Smith, E. E. (1984). Correlated properties in       overlapping properties. Psychological Review, 86, 87-
  natural categories. Journal of Verbal Learning and Verbal        123.
  Behavior, 23, 250-269.                                         Smith, J. D., & Minda, J. P. (1998). Prototypes in the mist:
Markman, A. B., & Gentner, D. (1993). Splitting the                The early epochs of category learning. Journal of
  differences: A structural alignment view of similarity.          Experimental Psychology: Learning, Memory and
  Journal of Memory and Language, 32, 517-535.                     Cognition, 24, 1411-1436.
McRae, K., & Cree, G. S. (2002). Factors underlying              Smith, J. D., & Minda, J. P. (2000). Thirty categorization
  category-specific semantic deficits. In E. M. E. Forde &         results in search of a model. Journal of Experimental
  G. W. Humphreys (Eds.), Category-specificity in brain            Psychology: Learning, Memory and Cognition, 26, 3-27.
  and mind (pp. 211-249). East Sussex, England:                  Smits, T., Storms, G., Rosseel, Y., & De Boeck, P. (2002).
  Psychology Press.                                                Fruits and vegetables categorized: An application of the
Medin, D. L., & Schaffer, M. M. (1978). Context theory of          generalized context model. Psychonomic Bulletin and
  classification learning. Psychological Review, 85, 207-          Review, 9, 836-844.
  238.                                                           Stanton, R. D., Nosofsky, R. M., & Zaki, S. R. (2002).
Medin, D. L., & Schwanenflugel, P. J. (1981). Linear               Comparisons between exemplar similarity and mixed
  separability in classification learning. Journal of              prototype models using a linearly separable category
  Experimental Psychology: Human Learning and Memory,              structure. Memory and Cognition, 30, 934-944.
  7, 355-368.                                                    Storms, G., De Boeck, P., & Ruts, W. (2000). Prototype and
Murphy, G. L., & Medin, D.L. (1985). The role of theories          exemplar-based information in natural language
  in conceptual coherence. Psychological Review, 92, 289-          categories. Journal of Memory and Language, 42, 51-73.
  316                                                            Storms, G., De Boeck, P., & Ruts, W. (2001).
Nosofsky, R. M. (1984). Choice, similarity, and the context        Categorization of unknown stimuli in well-known natural
  theory of classification. Journal of Experimental                language concepts: a case study. Psychonomic Bulletin
  Psychology: Learning, Memory and Cognition, 10, 104-             and Review, 8, 377-384.
  114.                                                           Takane, Y., & Shibayama, T. (1992). Structures in stimulus
Nosofsky, R. M. (1986). Attention, similarity, and the             identification data. In F. G. Ashby (Ed), Multidimensional
  identification-categorization relationship. Journal of           models of perception and cognition. Hillsdale,          NJ,
  Experimental Psychology: General, 115, 39-57.                    England: Lawrence Erlbaum Associates, Inc.
Nosofsky, R. M. (1987). Attention and learning processes in      Takane, Y., Young, F. W., & De Leeuw, J. (1977).
  the identification and categorization of integral stimuli.       Nonmetric individual differences multidimensional
  Journal of Experimental Psychology: Learning, Memory             scaling: An alternating least squares method with optimal
  and Cognition, 13, 87-108.                                       scaling features. Psychometrika, 42, 7-67.
Nosofsky, R. M. (1992). Exemplars, prototypes and                Tversky, A. (1977). Features of similarity. Psychological
  similarity rules. In A F. Healy, S. M. Kosslyn, & R. M.          Review, 84, 327-352.
  Shiffrin (Eds.), From learning theory to connectionist         Verbeemen, T., Vanoverberghe, V., Storms, G., & Ruts, W.
  theory: Essays in honour of William K. Estes, Vol. 1.            (2001). The role of contrast categories in natural language
  Lawrence Erlbaum, Hillsdale, NJ.                                 concepts. Journal of Memory and Language, 44, 618-643.
Nosofsky, R. M., & Zaki, S. R. (2002). Exemplar and
  prototype models revisited: Response strategies, selective
  attention, and stimulus generalization. Journal of
  Experimental Psychology: Learning, Memory and
  Cognition,                    28,                924-940.
Pruzansky, S., Tversky, A., & Carroll, J. D. (1982). Spatial
  versus tree representations of proximity data.
  Psychometrika, 47, 3-24.
Rosch, E., & Mervis, C. B. (1975). Family resemblances:
  Studies in the internal structure of categories. Cognitive
  Psychology, 7, 573-605.
Sattath, S., & Tversky, A. (1977). Additive similarity trees.
  Psychometrika, 42, 319-345.
Schwarz, G. (1978). Estimating the dimension of a model.
  The Annals of Statistics, 6, 461-464.
Shepard, R. N. (1964). Attention and the metric structure of
  the stimulus space. Journal of Mathematical Psychology,
  1, 54-87.
Shepard, R. N., & Arabie, P. (1979). Additive clustering:
  Representation of similarities as combinations of discrete
                                                             1398

