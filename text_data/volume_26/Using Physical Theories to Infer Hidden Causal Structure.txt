UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using Physical Theories to Infer Hidden Causal Structure
Permalink
https://escholarship.org/uc/item/4xz2k5jr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Griffiths, Thomas L.
Baraff, Elizabeth R.
Tenenbaum, Joshua B.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

        Using Physical Theories to Infer Hidden Causal Structure
                    Thomas L. Griﬃths                  Elizabeth R. Baraﬀ & Joshua B. Tenenbaum
                gruffydd@psych.stanford.edu                              {liz b,jbt}@mit.edu
                    Department of Psychology                Department of Brain and Cognitive Sciences
                       Stanford University                       Massachusetts Institute of Technology
                        Abstract                              some, 2003) turns on this issue. Causal graphical
   We argue that human judgments about hidden                 models provide a language in which the problem of
   causal structure can be explained as the operation         causal induction can be formally expressed. How-
   of domain-general statistical inference over causal        ever, conventional algorithms for inducing causal
   models constructed using domain knowledge. We              structure (e.g., Pearl, 2000; Spirtes et al., 1993) do
   present Bayesian models of causal induction in two
   previous experiments and a new study. Hypothet-            not provide a satisfying account of either the roles
   ical causal models are generated by theories ex-           of causal knowledge or statistical inference, or their
   pressing two essential aspects of abstract knowledge       interaction. These algorithms use tests of statisti-
   about causal mechanisms: which causal relations            cal independence to establish constraints that must
   are plausible, and what functional form they take.         be satisﬁed by causal structures consistent with the
   Everyday reasoning draws on notions that go far            observed data. No knowledge of how causal mech-
beyond the observable world, just as modern sci-              anisms operate, or the functional form of relation-
ence draws upon theoretical constructs beyond the             ships between cause and eﬀect, enters into the in-
limits of measurement. The richness of our naive              ference process. As we argue below, such knowledge
theories is a direct result of our ability to postu-          is necessary to explain how people are able to in-
late hidden causal structure. This capacity to rea-           fer causal structure from very small samples, and to
son about unobserved causes forms an essential part           infer hidden causes from purely observational data.
of cognition from early in life, whether we are rea-          Constraint-based methods are also unable to explain
soning about the forces involved in physical systems          people’s graded sensitivity to the strength of evi-
(e.g., Shultz, 1982), the mental states of others (e.g.,      dence for a causal structure, because they reason de-
Perner, 1991), or the essential properties of natural         ductively from constraints to consistent structures.
kinds (e.g., Gelman & Wellman, 1991).                            We will present a rational account of human in-
   The central role of hidden causes in naive theories        ference, Theory-Based Causal Induction, which em-
makes the question of how people infer hidden causal          phasizes the interaction between causal knowledge
structure fundamental to understanding human rea-             and statistical learning. Causal knowledge appears
soning. Psychological research has shown that peo-            in the form of causal theories, specifying the princi-
ple can infer the existence of hidden causes from oth-        ples by which causal relationships operate in a given
erwise unexplained events (Ahn & Luhmann, 2003),              domain. These theories are used to generate hy-
and determine hidden causal structure from very lit-          pothesis spaces of causal models – some with hid-
tle data (Kushnir, Gopnik, Schulz, & Danks, 2003).            den causes, some without – that can be evaluated
This work has parallels in computer science, where            by domain-general statistical inference. We will use
the development of a formalism for reasoning about            this framework to develop models of people’s infer-
causality – causal graphical models – has led to algo-        ences about hidden causes in two physical systems:
rithms that use patterns of dependency to identify            a mechanical system called the stick-ball machine
causal relationships (Pearl, 2000; Spirtes, Glymour,          (Kushnir et al., 2003), and a dynamical system in-
& Scheines, 1993). It has recently been proposed,             volving an explosive compound called Nitro X.
chieﬂy by Gopnik, Glymour, and their colleagues
(Glymour, 2001; Gopnik, Glymour, Sobel, Schulz,                     Theory-based causal induction
Kushnir, & Danks, 2004), that these algorithms may            Our account of causal induction builds on causal
also explain how people infer causal structure.               graphical models, extending the formalism to incor-
   A fundamental issue in explaining how people in-           porate the abstract knowledge about causal mecha-
fer causal relationships is accounting for the interac-       nism that plays an essential role in human inferences.
tion between abstract causal knowledge and statisti-          We will brieﬂy introduce causal graphical models,
cal inference. The classic debate between approaches          consider how prior knowledge inﬂuences causal in-
that emphasize cause-eﬀect covariation and those              duction, and describe how we formalize the contri-
that emphasize mechanism knowledge (e.g., New-                bution of causal theories.
                                                          500

Causal graphical models                                        These restrictions have important implications for
Graphical models represent the dependency struc-            causal induction algorithms. If all structures are
ture of a joint probability distribution using a graph      possible, both observations and interventions are
in which nodes are variables and edges indicate de-         typically required to identify hidden causes, and
pendence. The graphical structure supports eﬃcient          without strong assumptions about the functional
computation of the probabilities of events involv-          form of causal relationships, samples must be rel-
ing these variables. In a causal graphical model the        atively large. With limitations on the set of possible
edges indicate causal dependencies, with the direc-         causal structures and expectations about functional
tion of the arrow indicating the direction of causa-        form, however, it is possible to make causal infer-
tion, and they support inferences about the eﬀects          ences from just observations and from small samples
of interventions (Pearl, 2000). An intervention is an       – important properties of human causal induction.
event in which a variable is forced to hold a value,
independent of any other variables on which it might
                                                            Using causal theories in causal induction
depend. Intervention on a variable A is denoted             The causal mechanism knowledge that is relevant for
do(A). Probabilistic inference on a modiﬁﬁed graph,         statistical causal inference may be quite abstract,
in which incoming edges to A are removed, can be            and may also vary across domains. Much of this
used to assess the consequences of intervening on A.        knowledge may be represented in intuitive domain
   The structure of a causal graphical model implies        theories. In contrast to Gopnik et al. (2004), who
a pattern of dependency among variables under ob-           suggest that causal graphical models are the pri-
servation and intervention. Conventional algorithms         mary substrate for intuitive theories, we emphasize
for inferring causal structure use standard statistical     the role of intuitive theories at a more abstract level,
tests, such as Pearson’s χ2 test, to ﬁnd the pattern of     providing restrictions on the set of causal models
dependencies among variables, and then deductively          under consideration. Such restrictions cannot be
identify the structure(s) consistent with that pattern      represented as part of a causal graphical model:
(e.g., Spirtes et al., 1993). These “constraint-based”      causal graphical models express the relations that
algorithms can also exploit the results of interven-        hold among a ﬁnite set of propositions, while causal
tions, and often require both observations and in-          theories involve statements about all relations that
terventions in order to identify the hidden causal          could hold among entities in a given domain.
structure. Gopnik, Glymour, and colleagues have                Formally, we view causal theories as hypothesis
suggested that this kind of constraint-based reason-        space generators: a theory is a set of principles that
ing may underlie human causal induction (Glymour,           can be used to generate a hypothesis space of causal
2001; Gopnik et al., 2004; Kushnir et al., 2003).           models, which are compared via Bayesian inference.
                                                            The principles that comprise a theory specify which
The role of causal theories                                 relations are plausible and the functional form of
Constraint-based algorithms for causal induction            those relations. These principles articulate how
make relatively little use of prior knowledge. While        causal relationships operate in a given domain, but
particular causal relationships can be ruled out a pri-     need not identify the mechanisms underlying such
ori, there is no way to represent the belief that one       relationships: all that is necessary for causal induc-
structure may be more likely than another. Further-         tion is the possibility that some mechanism exists,
more, the use of statistical tests like χ2 makes only       and expectations about the functional form associ-
weak assumptions about the form of causal relation-         ated with that mechanism. This vague and abstract
ships: these tests simply assess dependency, regard-        mechanism knowledge is consistent with the ﬁnding
less of whether a relationship is positive or negative,     that people’s understanding of causal mechanism is
deterministic or probabilistic, strong or weak.             surprisingly shallow (Rozenblit & Keil, 2002).
   Several researchers (e.g., Shultz, 1982) have ar-           In the remainder of the paper, we will demonstrate
gued that knowledge of causal mechanism plays a             how Theory-Based Causal Induction can be used to
central role in human causal induction. Mechanism           explain human inferences about hidden causes in
knowledge is usually cited in arguments against sta-        physical systems. Diﬀerent systems require diﬀer-
tistical causal induction, but we view it as critical       ent causal theories. We will examine inferences in
to explaining how statistical inferences about causal       a mechanical system, the stick-ball machine (Kush-
structure are possible from sparse data. Knowledge          nir et al., 2003), and in a dynamical system, Nitro
about causal mechanisms provides two kinds of re-           X, which we explore in a new experiment. When
strictions on possible causal models: restrictions on       reasoning about these systems, people infer hidden
which relationships are plausible, and restrictions on      causal structure from very few observations, and are
the functional form of those relationships. Restric-        sensitive to graded degrees of evidence.
tions on plausibility might indicate that one causal
structure is more likely than another, while restric-                   The stick-ball machine
tions on functional form might indicate that a par-         Kushnir et al. (2003) conducted two experiments in
ticular relationship should be positive and strong.         which participants had to infer the causal structure
                                                        501

                         α                     α               α
 (a)               (b)           C                     C           C                                                        Common unobserved cause
                                                                                                             1
                                                                                              Probability
                             β                     β       β                 β                                                                       People
       A   B
                                 A                     B   A                 B                                                                       Bayes
                                                                                                            0.5
                                     Graph 0                   Graph 1                                                                               Bayes (γ,δ)
                         α                                               α                                   0
                                                                                                                        Independent unobserved causes
                             C                                               C
                                                                                                             1
                                                                                              Probability
                         β                                                       β
                                               β               β
                             A                     B       A                 B                              0.5
                                     Graph 2                   Graph 3
                                                                                                             0
                                                                                                                              One observed cause
Figure 1: (a) A stick-ball machine. (b) Graphs indi-                                                         1
                                                                                              Probability
cating potential causal structures for the stick-ball
machine. Nodes A and B correspond to the two                                                                0.5
balls, nodes marked C are hidden causes.
                                                                                                             0
                                                                                                                                 Pointing control
of a physical system, the “stick-ball machine”, con-                                                         1
                                                                                              Probability
sisting of two colored balls (A and B) mounted on
sticks which could move up and down on a box (see                                                           0.5
Figure 1(a)). The mechanical apparatus moving the
                                                                                                             0
balls was concealed, keeping the actual causal re-                                                                Graph 0     Graph 1      Graph 2     Graph 3
lationship unknown. In both experiments, all par-
ticipants were familiarized with the machine, and                                          Figure 2: Results of Kushnir et al. (2003), shown
told that if one ball caused the other to move it                                          with predictions of Bayesian models.
did so “almost always”. This probabilistic causal
relation was demonstrated by showing the two balls                                         judgments reﬂect a sensitivity to graded degrees of
move together four times, an event we denote 4AB,                                          evidence: in the independent unobserved causes con-
and A moving alone twice, 2AB̄. There were three                                           dition, over 95% of participants chose Graph 1, while
test conditions in Experiment 1, seen by all partic-                                       only 60-80% of people chose the most popular struc-
ipants. In the common unobserved cause condition,                                          ture in the other conditions. This was not simply a
participants saw 4AB, and four trials in which the                                         consequence of a preference for Graph 0 – the same
experimenter intervened, twice moving A with no ef-                                        structure was less popular in the pointing control
fect on B, 2B̄|do(A), and twice moving B with no                                           condition, suggesting that there is a diﬀerence in
eﬀect on A, 2Ā|do(B). In the independent unob-                                            the evidence that the data provide for Graph 0 in
served cause condition, participants saw 2AB̄, 2ĀB,                                       these two conditions. Constraint-based algorithms
1AB, 2Ā|do(B), and 2B̄|do(A). In the one ob-                                              are not sensitive to graded degrees of evidence: a
served cause condition, participants saw 4B|do(A)                                          causal structure is either consistent or inconsistent
and 2B̄|do(A). Experiment 2 replicated the com-                                            with the pattern of dependencies in a dataset.
mon unobserved cause condition, and compared this
with a pointing control condition in which interven-                                       A theory-based account
tions were replaced with observations (4AB, 2ĀB,                                          Our model of the stick ball machine uses a physical
2AB̄). The order of conditions and trials within                                           theory that contains three principles:
conditions was randomized across participants. In
                                                                                       1. Balls never move without a cause.
each condition, participants identiﬁed the underly-
                                                                                       2. A hidden cause moves with probability α.
ing causal structure by indicating graphs similar to
                                                                                       3. A moving cause moves its eﬀect with probability β.
those shown in Figure 1(b). The results of both
experiments are combined in Figure 2. One causal                                           If we add the restrictions that every ball has a sin-
structure was chosen by the majority of people in                                          gle cause and hidden causes never have causes (but
each condition – Graph 1 in the common unobserved                                          can move themselves, per Principle 2), we obtain the
cause condition, Graph 0 in the independent unob-                                          four structures shown in Figure 1(b). The principles
served causes condition, Graph 2 in the one observed                                       of the physical theory place strong constraints on the
cause condition, and Graph 0 in the pointing control.                                      functional form of the causal relationships identiﬁed
   The results of these experiments provide two chal-                                      in this structure, allowing us to compute the proba-
lenges to constraint-based accounts. First, people                                         bility of events involving A and B for each graphical
are able to make inferences from small samples – in                                        structure, as shown in Table 1.
many cases, far less data than might be required for                                          Given a dataset D, we compute a poste-
all relevant χ2 tests to yield results consistent with                                     rior probability distribution over these struc-
the appropriate causal structure. Second, people’s                                         tures, P (Graph i|D), combining prior probabilities,
                                                                                     502

  Table 1: Event probabilities for causal structures
   Event       Graph 0        Graph 1       Graph 2              (a)
     AB         (αβ)2           αβ 2           αβ 2
     ĀB     αβ(1 − αβ)      αβ(1 − β)          0
     AB̄     αβ(1 − αβ)      αβ(1 − β)     αβ(1 − β)
     ĀB̄     (1 − αβ)2   1 − 2αβ + αβ 2     1 − αβ
  A|do(B)        αβ             αβ             αβ                (b)
  B|do(A)        αβ             αβ              β
Note: Probabilities for Graph 3 are the same as
those for Graph 2, exchanging the roles of A and B.
                                                          Figure 3: (a) Four cans of the extremely unstable
P (Graph i), with the probability of the observed         compound Nitro X. (b) A simultaneous explosion.
data under each structure, P (D|Graph i), using
Bayes’ rule:                                                                     Nitro X
                                                          To provide a further demonstration of the impor-
      P (Graph i|D) ∝ P (D|Graph i)P (Graph i)            tance of graded degrees of evidence and the ability
                                                          to infer hidden causes from very little data, we con-
                                                          ducted an experiment that tested people’s ability to
P (D|Graph i) is the product of the probabilities of      infer the causal structure of a dynamical physical
the individual events making up D, which can be           system. Our experiment presents a more severe in-
obtained from Table 1.                                    ductive challenge than the tasks considered by Kush-
   If we assume a uniform prior for P (Graph i),          nir et al. (2003), as it requires inferring a hidden
the causal theory leaves two parameters unspeciﬁed:       common cause from just a single observation, with
α, the probability of a hidden cause moving on a          no verbal cues that such a structure might exist. In
given trial, and β, the probability that a moving         the experiment, we introduced people to a novel sub-
cause moves its eﬀect. We set β empirically, via          stance, Nitro X, and illustrated its dynamics: cans
a small experiment. We showed 10 participants a           of Nitro X could spontaneously explode, and could
computer simulation of the stick-ball machine, and        detonate one another after a time delay that was a
reproduced the familiarization trials used by Kush-       linear function of spatial separation, as would be ex-
nir et al. (2003): participants were told that when       pected from the slow propagation of pressure waves.
A causes B, it makes it move “almost always”, and         We then presented them with the simultaneous ex-
were shown that A moved B on four of six trials. We       plosion of several cans, without the delays charac-
then asked them how often they expected A would           teristic of pressure waves propagating from one can
move B. The mean and median response was that A           to the next. We expected that people would see
would move B on 75% of trials, so we used β = 0.75.       this suspicious coincidence as evidence for some kind
   Figure 2 shows the predictions of the Bayesian         of hidden common cause, such as an external force
model with α = 0.47. The model gave a correlation         shaking the table. We varied the number of cans, m,
of r = 0.94 with the data, and correctly predicted        to see whether the magnitude of the coincidence had
the most common response in each condition. The           an eﬀect on people’s inference to a hidden cause.
model also admits graded degrees of evidence, with
the observations and interventions in the indepen-        Method
dent unobserved causes condition providing stronger       Participants Participants were 64 members of the
evidence for Graph 0 than the observations in the         MIT Brain and Cognitive Sciences subject pool, split
pointing control. The model departs from people’s         evenly over four conditions (m = 2, 3, 4, 6).
judgments in one case, failing to predict the minor-
                                                          Stimuli The stimuli were pictures of cans sitting
ity preference for Graph 0 in the common unobserved
                                                          on a table, presented on a computer screen. A
cause condition. This disparity could have many ex-
                                                          new set of cans was shown on each trial, and by
planations, such as a default preference for indepen-
                                                          the end of the trial all cans on the screen had ex-
dence between objects, or diﬀerences in the salience
                                                          ploded, demonstrated by cartoon explosion graphics
of diﬀerent data types and causal structure. For in-
                                                          like those shown in Figure 3.
stance, interventions may be weighted higher than
observations by a factor of γ, and hidden common          Procedure The experiment consisted of three fa-
causes may receive only a fraction 1/δ of the prior       miliarization trials and ﬁve test trials. The famil-
probability accorded to other structures. Figure 2        iarization trials introduced the participants to Nitro
shows an almost-perfect ﬁt (r = 0.99) for such a          X. In the ﬁrst trial, participants were told that Ni-
model, Bayes (γ,δ), with γ = 4, δ = 2, α = 0.4.           tro X is very unstable, and this was demonstrated
Further experiments will be necessary to determine        by the experimenter tapping a can and the can ex-
whether these sorts of psychological variables play a     ploding. In the second trial, participants saw two
role in the process of causal induction.                  cans of Nitro X, the experimenter tapped one can,
                                                      503

                            1                                             not explode in each interval, then we can construct
                                                           People         a contingency table for each pair of cans. Statis-
  Probability of Graph 1
                           0.8                             Bayes          tical signiﬁcance tests will identify pairwise depen-
                                                                          dencies among all cans that explode simultaneously,
                           0.6                                            provided appropriate numbers of non-explosion tri-
                                                                          als are included. The existence of a hidden common
                           0.4                                            cause is consistent with such a pattern of depen-
                                                                          dency. However, as a result of reasoning deductively
                           0.2                                            from this pattern, the evidence for such a structure
                                                                          does not increase with m: a hidden common cause
                            0                                             is merely consistent with the pattern for all m > 2.
                                 2     3            4         6              This experiment also illustrates that people are
                                     Number of canisters                  willing to infer hidden causal structure from very
                                                                          small samples – just one datapoint – and from obser-
                     Figure 4: Results of the Nitro X experiment.         vations alone. Constraint-based algorithms cannot
                                                                          solve this problem: while a hidden common cause is
which exploded, and the can next to it exploded                           consistent with the observed pattern of dependency,
shortly afterwards. On the third trial, participants                      causal structures in which the cans inﬂuence one an-
were again reminded about the instability of Nitro                        other cannot be ruled out without intervention in-
X, and saw a single can explode without any action                        formation. People do not consider this possibility
by the experimenter, after waiting for a few seconds.                     because they have learned that the mechanism by
   The ﬁrst two test trials were identical for all four                   which cans inﬂuence one another has a time delay.
conditions, and both involved four cans exploding                         Further situations in which the temporal properties
in a causal chain, with a delay between successive                        of causal relationships inﬂuence causal induction are
explosions. In the third test trial, the number of cans                   described by Hagmayer and Waldmann (2002).
in the display was varied, m = 2, 3, 4 or 6, depending
on condition. After a brief delay, all of the cans                        A theory-based account
exploded simultaneously. The last two test trials                         The results of the Nitro X experiment are easy to
allowed the participants to interact with Nitro X by                      model: any increasing function of the number of cans
tapping, and will not be discussed further here.                          would be suﬃcient. Our goal in modeling these data
   After each test trial, participants were given a                       is to illustrate how Theory-Based Causal Induction
sheet of questions for each test trial. These sheets                      extends to a system with non-trivial dynamics and
gave three options:                                                       diﬀerent causal mechanisms, and to show that in-
1. The ﬁrst can exploded spontaneously. That explosion                    ferences to hidden causes from the smallest possible
   caused the other cans to explode, in a chain reaction.                 sample – a single observation – can have a physically
2. Each can exploded spontaneously, all on its own.                       plausible and statistically rational explanation.
   There was no causal connection between them.                              We model the explosion times of cans by assum-
3. Neither of the above is a likely explanation. Please                   ing that at each inﬁnitesimal moment, there is a cer-
   write a plausible alternative here.                                    tain probability that the can will explode. This as-
The order of the ﬁrst two options was counterbal-                         sumption means that the explosion time of each can
anced, but the third option was always last.                              follows a Poisson process, with a “rate parameter”
                                                                          determining the probability of explosion at each mo-
Results and Discussion                                                    ment. We set the rates using the following principles:
For all trials, two rates examined the written re-                    1. A can explodes spontaneously at rate α.
sponses of participants choosing the third option                     2. A hidden cause becomes active at rate γ.
above, and were in 100% agreement in classifying all                  3. At the moment a hidden cause is active, a can inﬂu-
such responses as indicating a hidden cause. Over                        enced by that cause explodes at rate α + β.
95% of participants correctly identiﬁed the causal
chain in the ﬁrst two trials. The proportion of partic-                   A complete theory of Nitro X would need to include
ipants identifying a hidden cause on the third trial,                     further principles stating the functional form of the
with the simultaneous explosion, is shown in Figure                       causal relationship between cans, encoding the fact
4. There was a statistically signiﬁcant eﬀect of m,                       that this relationship involves a time delay. We have
χ2 (3) = 11.36, p < 0.01. The number of cans inﬂu-                        omitted these principles because they do not directly
enced whether people inferred hidden causal stuc-                         aﬀect the inference to a hidden cause when all ex-
ture, with most people seeing two cans as indepen-                        plosions are simultaneous.
dent but six as causally related.                                            This theory generates a large number of possi-
  Constraint-based algorithms cannot explain our                          ble causal structures, with hidden causes inﬂuencing
results. If we imagine that time is broken into dis-                      various subsets of the cans. We will focus on the two
crete intervals, and a can either explodes or does                        structures shown in Figure 5: Graph 0, in which all
                                                                    504

                                                α
                                                   tC               should inﬂuence the evidence for a hidden cause,
                                                                    but is clearly not the only model compatible with
                                        β     β         β   β       these data. However, our analysis exposes the ratio-
  α       α       α       α        α       α       α      α         nal basis for human judgments, and makes further
      t1     t2      t3     t4         t1      t2      t3    t4
                                                                    intuitive predictions that we are in the process of
              Graph 0                           Graph 1             testing. For example, the −γt term in the expres-
                                                                    sion for b indicates that, all other things being equal,
Figure 5: Graphs indicating potential causal struc-                 decreasing the time before a simultaneous explosion
tures for the Nitro X experiment.                                   increases the evidence for a hidden cause.
                                                                                          Conclusion
cans explode spontaneously, is the “null hypothesis”
for any inference concerning hidden causes, while                   Explaining human causal induction requires supple-
Graph 1, in which all cans are also inﬂuenced by                    menting the formal methods developed in computer
a hidden cause, gives the highest probability to a si-              science with the causal domain knowledge that peo-
multaneous explosion. These structures are deﬁned                   ple possess. We have shown that using physical the-
on variables representing the time at which cans ex-                ories to inform rational statistical inference makes
plode, t1 , . . . , tm , and the time the hidden cause be-          it possible to explain how people infer hidden causal
comes active, tC . The inference to a hidden common                 structure from such limited data. We anticipate that
cause is modeled by computing the posterior prob-                   the same framework, using appropriately modiﬁed
ability P (Graph 1|T ), where T = {t1 , . . . , tm }. In a          causal theories, can shed light on inferences about
simultaneous explosion, all ti take the same value, t.              hidden causes in other domains.
    It follows from the theory outlined above that                  Acknowledgments We thank T. Kushnir and L. Schulz
for Graph 0, each ti is an independent Poisson                      for helpful discussions. TLG was supported by a Stan-
process with rate α, which gives P (T |Graph 0) =                   ford Graduate Fellowship, JBT by the P.E.Newton chair.
αm exp{−mαt}. For Graph 1, tC follows a Poisson
process with rate γ. Conditioned on tC , each ti is a                                     References
Poisson process with rate α, except at the moment                   Gelman, S. A. and Wellman, J. M. (1991). Insides and
                                                                      essence: Early understandings of the non-obvious. Cog-
when the hidden cause becomes active, at which                        nition, 38:213–244.
point the rate is α + β. Computing P (T |Graph 1)                   Glymour, C. (2001). The mind’s arrows: Bayes nets
requires integrating over all values of tC , which we                 and graphical causal models in psychology. MIT Press,
approximate by choosing tC to maximize P (T |tC ):                    Cambridge, MA.
                              ∞                                    Gopnik, A., Glymour, C., Sobel, D., Schulz, L., Kushnir,
                                                                      T., and Danks, D. (2004). A theory of causal learning
   P (T |Graph 1) =                P (T |tC )P (tC ) dtC              in children: Causal maps and Bayes nets. Psychological
                               0                                      Review, 111:1–31.
                         ≈ γ(α + β)m exp{−mαt − γt}                 Hagmayer, Y. and Waldmann, M. R. (2002). How tem-
                                                                      poral assumptions inﬂuence causal judgments. Memory
Applying Bayes’ rule, it follows1 that P (Graph 1|T )                 and Cognition, 30:1128–1137.
is a sigmoid function of m,                                         Kushnir, T., Gopnik, A., Schulz, L., and Danks, D.
                                                                      (2003). Inferring hidden causes. In Proceedings of the
                                           1                          25th Conference of the Cognitive Science Society.
           P (Graph 1|T ) =
                                 1 + exp{−gm − b}                   Luhmann, C. C. and Ahn, W.-K. (2003). Evaluating the
                                                                      causal role of unobserved variables. In Proceedings of
                                    P (Graph 1)                       the 25th Conference of the Cognitive Science Society.
for g = log α+β   α and b = log P (Graph 0) + log γ − γt.           Newsome, G. L. (2003). The debate between current
    The model predicts that increasing m should in-                   versions of the covariation and mechanism approaches
crease P (Graph 1|T ) for any positive values of α and                to causal inference. Philosophical Psychology, 16:87–
β, as this results in a positive gain, g. The theory                  107.
involves four parameters: α, β, γ, and P (Graph 0).                 Pearl, J. (2000). Causality: Models, reasoning and in-
Since these four parameters are not identiﬁable –                     ference. Cambridge University Press, Cambridge, UK.
multiple sets of parameter values are consistent with               Perner, J. (1991). Understanding the representational
                                                                      mind. MIT Press, Cambridge, MA.
the same sigmoid function – we set the parameters of
                                                                    Rozenblit, L. R. and Keil, F. C. (2002). The missunder-
the sigmoid g and b. Using g = 0.58 and b = −2.90                     stood limits of folk science: an illusion of explanatory
gives r = 0.958, and the predictions shown in Fig-                    depth. Cognitive Science, 26:521–562.
ure 4. These parameters indicate β = 0.79α and an                   Shultz, T. R. (1982). Rules of causal attribution. Mono-
initial preference for Graph 0.                                       graphs of the Society for Research in Child Develop-
    Our theory-based approach explains why the num-                   ment, 47(Serial no. 194).
ber of cans involved in a simultaneous explosion                    Spirtes, P., Glymour, C., and Schienes, R. (1993). Cau-
                                                                      sation prediction and search. Springer-Verlag, NY.
     1
       A full derivation of this result is available at
http://www-psych.stanford.edu/∼gruﬀydd/reports/nitrox.pdf
                                                                505

