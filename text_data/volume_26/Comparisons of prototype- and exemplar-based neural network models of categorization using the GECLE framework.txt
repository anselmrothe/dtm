UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Comparisons of prototype- and exemplar-based neural network models of categorization
using the GECLE framework
Permalink
https://escholarship.org/uc/item/5s86d3cp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Author
Matsuka, Toshihiko
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

         Comparisons of prototype- and exemplar-based neural network models of
                                  categorization using the GECLE framework
                                  Toshihiko Matsuka (matsuka@psychology.rutgsers.edu)
                                                  RUMBA, Rutgers University – Newark
                                        101 Warren St., Smith Hall 327, Newark, NJ 07102 USA
                             Abstract
                                                                                                   GECLE
   In the present study, GECLE (Matsuka, 2003) was used as a
   general modeling framework to systematically compare the             GECLE (for Generalized Exploratory models of Category
   plausibility of two prominent assumptions about internal             LEarning) is a general and flexible exploratory approach for
   representations of neural network (NN) models of human               modeling human category learning, that is capable of
   category learning. In particular, exemplar-model friendly            modeling human category learning with many variants using
   Medin and Schaffer’s 5/4 stimulus set (1978) was used for            different model assumptions (Matsuka, 2003). This general
   comparing prototype- and exemplar-based NN models. The               modeling framework allows model assumptions to be
   results indicate that some prototype-based models performed          manipulated separately and independently. For example,
   as good as or better than an exemplar-based model in                 one can manipulate assumptions about how stimuli are
   replicating the empirical classification profile. In addition, a     internally represented (e.g. exemplars vs. prototypes), or
   phenomenon called A2 advantage (i.e., people tend to                 about how people selectively pay attention to input feature
   categorize the less “prototypical” stimulus A2 more                  dimensions (e.g., paying attention to dimensions
   accurately than more “prototypical” stimulus A1) reported in         independently or not).
   empirical studies (e.g., Medin & Schaffer 1978) was also                The GECLE model uses the Mahalanobis distances (in the
   successfully reproduced by these prototype-based NN models.          quadratic form) between the internally represented reference
                                                                        points (RP: corresponding to either exemplars or
                          Introduction                                  prototypes) and the input stimuli as the measure of
There have been an increasing number of studies debating                similarity between them. Thus, unlike other neural network
how stimuli are internally represented in human cognition               models of category learning, GECLE does not necessarily
during the last few decades (e.g., Minda & Smith 2002;                  assume that attention is allocated independently dimension-
Nosofsky & Zaki 2002). Most of these debates have been                  by-dimension. Rather, it assumes that humans in some
based on quantitative models of categorization, and only a              cases might pay attention to correlations among feature
few have considered representational aspects of adaptive,               dimensions. This allows GECLE to model processes
network, or learning models of categorization. Several                  interpretable as dimensionality reduction or mental rotation
studies (Matsuka, 2002; Matsuka, Corter, & Markman,                     in the perception and learning of stimuli. Such processes
2003) have compared exemplar-based (EB) and prototype-                  may increase the interpretability of stimuli in categorization
based (PB) adaptive network models of categorization, but               tasks. Another motivation for using the Mahalanobis
there has been no systematic comparison of specific                     distance is that the capability for paying attention to
assumptions in EB and PB modeling. Although these                       correlations among feature dimensions may be necessary for
comparative studies provided information on the models’                 classification tasks defined on integral stimuli.
capabilities for reproducing human-like categorization                     In the GECLE framework, the attention parameters
learning, they did not necessarily provide information that             (which are the diagonal and off-diagonal elements of the
can lead to specific understanding of the nature of human               covariance matrices) can be considered as shape and
category learning.          That is because model-to-model              orientation parameters for receptive fields or attention
comparisons are not informative for testing the plausibility            coverage areas of the reference points. It should be noted,
of each specific assumption, rather such model comparisons              however, that one can constrain GECLE to incorporate the
are essentially omnibus tests collectively comparing all                “dimensional attention processes” assumption (i.e., attention
variations in assumptions at once. In other words, it has               is allocated independently on a dimension-by-dimension
been difficult to use the results of these previous                     basis) by forcing the off-diagonal entries in the covariance
comparative studies to understand which specific                        matrices to be equal to zero.
assumptions are supported by the empirical data. Therefore,                Another unique feature of GECLE’s attention mechanism
it seems desirable to make systematic comparisons between               is that it allows each reference point to have uniquely
competing model assumptions using a general modeling                    shaped and oriented attention coverage area, which is
framework that allows us to manipulate and test one or a                referred to as “local attention coverage structure” (Matsuka
limited number of model assumptions at a time.                          2003). Again, one can impose a restriction on the model’s
   In the present study, a generalized exploratory modeling             attention mechanism by fixing all covariance matrices to be
approach for human category learning is introduced. Then,               the same, which may be called “global attention coverage
using this general framework two assumptions about how                  structure”. Many NN models of category learning,
categories are internally represented, namely prototypes and            ALCOVE (Kruschke, 1992) for example, incorporate the
exemplars, are compared in a systematic fashion.                        global attention coverage structure.
                                                                    921

   The local attention coverage structure model is complex,
but may plausibly model attention processes in human
                                                                           (         ) (
                                                                       D nj x n , r j = x n − r j    ) Σ (x
                                                                                                      T   −1
                                                                                                          j
                                                                                                              n
                                                                                                                − rj )                   (1)
                                                                               n
category learning. For example, it allows models to be              where x is an I-ruple vector representing an input stimulus
sensitive to one particular feature dimension when the input        consisted of I feature dimensions presented at time n, rj, also
stimulus is compared with a particular reference point that is      an I-ruple vector, that corresponds to the centroids of
highly associated with category X, while the same feature           reference point j, expressing its characteristics, and Σj-1 is
dimension receives little or no attention when compared             the inverse of the covariance matrix, which defines the
with another reference point associated with category Y.            shape and orientation of the attention coverage area of
Thus the local attention coverage structure causes models to        reference point j. For a model with global attention
learn and be sensitive to within-cluster or within-category         coverage structure, there is only one global Σ -1 for all
feature configurations, while the global attention coverage         reference points.
structure essentially stretches or shrinks input feature               The psychological similarity measures Dj(x,r) cause some
dimensions in a consistent manner for all RP receptive fields       activations in internal “hidden” units or reference points
and all categories.                                                 (i.e., exemplars or prototypes). The activation of “hidden”
   Another way of interpreting GECLE’s capabilities for             basis unit j, or hj, is obtained by any differentiable nonlinear
paying attention to correlations among feature dimensions           activation transfer function (ATF), or
and having local attention coverage structures is that the             h j = G (D j ( x, r ) )                                           (2)
model learns to define what the feature dimensions are for          given that its first derivative G’(⋅) exists. An exponential
each RP and to allocate attention to those dimensions               function, exp(-cDj(x,r)), is an example of an ATF. The ATF
independently. In contrast, for almost all previous adaptive        must be a differentiable function, because GECLE uses a
models of category learning, the definition of the feature          gradient method for learning, where the partial derivatives
dimensions is static and supplied by individuals who use the        are used for updating the learnable parameters. However, it
models.                                                             is possible to eliminate this restriction by incorporating a
   Some studies showed that humans learn much better in             form of derivative-free learning algorithm such as stochastic
“filtration” tasks, in which information from only one              learning (Matsuka & Corter 2004).
dimension is required for (perfect) categorization, than in            The activations of hidden units are then fed forward to
“condensation” tasks, in which information from two                 output nodes. The activation of the kth output node, Ok, is
dimensions is required (e.g., Gottwald & Garner, 1975).             calculated by summing the weighted activations of all
This finding has been used as evidence that people pay              hidden units connected to the output node, or
attention to each dimension independently, rather than                          J
dependently (i.e., paying attention to correlations). Thus, a          Ok =         wkj h j                                              (3)
                                                                               j =1
model paying attention to correlations or having diagonal
attention coverage, as GECLE does, may not replicate                where wkj is the association weight between output node k
filtration advantage. However, Matsuka (2003, 2004)                 and reference point j. The probability that a particular
successfully replicated the filtration advantage using a            stimulus is classified as category Ck, denoted as P(C), is
prototype based correlation-attentive GECLE with local              assumed equal to the activity of category k relative to the
attention coverage structure. He suggested that for a               summed activations of all categories, where the activations
prototype based GECLE, the condensation stimuli require a           are first transformed by the exponential function (Kruschke,
stricter correspondence or synchronization between                  1992)
prototype search (i.e., shifting centroids of prototypes) and                         exp(φOc ) .                                        (4)
                                                                        P (C ) =
psychological scaling of the two feature dimensions (i.e.,                              exp(φOk )
attention processes) as compared with the filtration stimuli.                        k
This is because the “correct” prototypes and “correct”              φ is a real-value mapping constant that controls the
scaling are defined by two dimensions in the condensation           “decisiveness” of classification responses.
stimuli as compared to one dimension in the filtration                 GECLE uses the gradient method to update parameters.
stimuli.                                                            The error function is defined as the sum of squared
   In its natural form, the GECLE may be considered as a            differences between targeted and predicted output values
model using prototype internal representation, because it           (i.e., L2 norm), or
                                                                                            1 K 2 1 K
tries to learn to locate its reference points at the centers of        E ( w, r , Σ −1 ) =         ek =        (d k − Ok )2               (5)
each category cluster. However, with proper user-defined                                    2 k =1      2 k =1
parameter settings, it can behave like a model with an              Then the following functions are used to update parameters.
exemplar-based internal representation.                                              ∂E
                                                                       ∆w jk =              = −η w ek h j                                 (6)
                                                                                   ∂w jk
Quantitative Descriptions (Algorithm)
                                                                    where ηw is the learning rate for the association weights.
The feedforward and learning algorithms of the GECLE are
                                                                                 ∂E
                                                                                       = −η r ek w jk G '(D j ( x, r ) )Σ −j1 (x − r j )
                                                                                                 K
typical for implementation of the Generalized Radial Basis             ∆r j =                                                             (7)
Function (Haykin, 1999; Poggio & Girosi, 1989, 1990).                           ∂r j            k =1
GECLE uses the following function to calculate the                  where G’(⋅) is a derivative of G(⋅). Equation 7 can be
distances or similarity between internally represented              considered as a function that locates or defines prototypes of
reference points and input stimuli:
                                                                922

stimuli. For the exemplar-based modeling η r must be set to                    investigated with the same GECLE models used in
zero to maintain the static nature of reference points.                        Simulation 1. The plausibility of prototype models was
             ∂E                                                                further investigated using two variants of prototype-based
                  = η Σ ek w jk G '(D j ( x, r ) )(x − r j )(x − r j ) (8)
                         K
                                                                      T
   ∆Σ −j1 =    −1                                                              GECLE in Simulation 3.
            ∂Σ j        k =1
For models with global attention coverage structure,                           Simulation 1
Equation 8 should be summed over both k and j.
                                                                               In Simulation 1, I simulated category learning using the
Hierarchy of Constraints on Attention Parameters                               well-known Medin and Schaffer’s 5/4 stimulus set (1978).
                                                                               Table 1 shows the schematic representation of the stimulus
There is a hierarchy of constraints that one can impose on                     set. Eight different GECLE-based models were involved in
the attention parameters Σ-1 to manipulate GECLE’s                             the present simulation study. Among them there were seven
attention mechanisms. There are two levels of uniqueness                       prototype-based models (PB) with 2,3,4,5,6,7, or 8
of Σ-1 (global and local attention coverage structure), in each                prototypes and one exemplar-based model (EB) with all 9
of which there are three levels of constraints on entries in Σ.                unique exemplars. The global attention structure with
The following is a list of six possible levels of restriction.                 dimensional attentional processes (i.e., GUN) was used for
Note that regardless of the types of restriction, the entries                  all eight models. They were run in a simulated training
(sim) in Σj are assumed and constrained to satisfy the                         procedure to learn the correct classification responses for
following conditions: sii ≥ 0 & |sim| ≤ MIN(sii, smm).                         the training set. The models were run for 100 blocks of
                                                                               training, where each block consisted of a complete set of the
Global Attention Coverage Structures                                           training instances. The final parameter values used for each
A. Global Pure Radial (GPR): Constraints on Σj: sii = s, for                   model were chosen by a simulated annealing method to
all i: sim = 0, for all i ≠ m; Σj = Σ, for all reference points j.             minimize the objective function (i.e., sum of squared error:
B. Global Uncorrelated Non-radial (GUN): Constraints on                        SSE) in reproducing the classification profile reported in the
Σj: sim = 0, for all i ≠ m; Σj = Σ, for all reference points j.                original Medin & Schaffer’s work (1978). There are a total
C. Global Correlated Non-radial (GCN): Constraints on Σj:                      of 50 simulated subjects in each condition.
Σj = Σ, for all reference points j.                                                The following one-parameter exponential activation
                                                                               transfer function was used for the models:
Local Attention Coverage Structures                                             h j = exp (− c ⋅ D j ( x, r ) )
D. Local Pure Radial (LPR): Constraints on Σj: sii = s, for
all i; sim = 0, for all i ≠ m.                                                     One of the main interests of the present simulation study
E. Local Uncorrelated Non-radial (LUN): Constraints on                         was how well the eight models could reproduce observed
Σj: sim = 0, for all i ≠ m.                                                    classification profile reported in Medin & Schaffer (1978).
F. Local Correlated Non-radial (LCN): Constraints on Σj:                       The other related interest was how well each model
none.                                                                          performs on stimuli A1 and A2 (see Table 1). These two
                                                                               stimuli have been considered to be very important and
                                                                               diagnostic, because PB and EB tend to give different
                                                                               predictions for these particular stimuli (e.g., Nosofsky &
                                                                               Zaki, 2002). Specifically, EB models are used to explain
                                                                               empirical results that show that humans are better able to
                                                                               categorize less “prototypical” A2 than more “prototypical”
                                                                               A1 (e.g., Medin & Schaffer 1978). Moreover, simulation
                                                                               studies (e.g., Nosofsky & Zaki 2002) indicate that EB gives
                                                                               a better fit for differential performance on these particular
                                                                               stimuli.
                                                                               Table 1. Stimulus set used in Simulation 1
Figure 1. Six types of attention structures in the GECLE
framework. Clockwise from top left. GRP, GUN, GCN,                                           Training Set                   Transfer Set
LCN, LUN, and LRP.                                                                     Cat     D1 D2 D3           D4   D1    D2 D3 D4
                                                                               A1       A        1        1     1  0    1     0      0    1
                            Simulations                                        A2       A        1        0     1  0    1     1      1    1
                                                                               A3       A        1        0     1  1    0     1      0    1
In this section, three simulation studies were conducted to                    A4       A        1        1     0  1    0     0      1    1
compare adaptive network models of category learning                           A5       A        0        1     1  1    1     0      0    0
utilizing prototypes or exemplar internal representations                      B1       B        1        1     0  0    0     0      1    0
using the GECLE framework. Here, a classical category                          B2       B        0        1     1  0    0     1      0    0
learning study (Medin & Schaffer 1978) was replicated with                     B3       B        0        0     0  1
several variants of GECLE. Simulation 1 reports the                            B4       B        0        0     0  0
predictions by several GECLE models based on “optimal”
parameter values. In Simulation 2, the general tendencies in                   Results: Table 2 shows two fit indices for the eight models,
some key aspects associated with the stimulus set were                         namely SSE as an absolute fit index, and SSE multiplied by
                                                                           923

the number of learnable parameters (NLP) as a (crude)                the modules and input stimuli for categorizing. This
relative fit index that may account for the model                    combinatorial coding seems to be a very efficient use of
complexity. A pure prototype model (here a pure prototype            limited mental resources for categorizing virtually unlimited
is defined as a model that has as many RPs as the number of          number of unique instances.
categories) performed worst before and after controlling for            Alternatively, those models that were interpreted as
the model complexities. In addition, it failed to show the           prototype-based GECLE with many prototypes might have
A2 advantage. Rather as in many previous studies, it                 been utilizing RPs that were more sensible to be interpreted
predicted that A1 was easier than A2. However, other PB              as probabilistic, partial, or erroneous exemplars, instead.
models performed well; PB8 resulted in the best absolute fit,        That is, although the models might have tried to store
and PB5 resulted in the best relative fit.                           correct exemplars in their memory, the process was not fully
   When the PB models are compared with the EB model,                completed because of the limited mental resources, resulting
some PBs fit the observed profile better than EB,                    in imprecise exemplars memorization, in which a particular
particularly after controlling for the model complexities.           feature of a particular exemplar was more correctly
More interestingly, as the EB model, almost all PBs were             memorized than other features. Then, these imprecise
able to predict the A2 advantage (Table 2, last column).             exemplars were utilized for categorizing the stimuli.
   Although, this Medin and Schaffer 5/4 stimulus set has
been used as evidence supporting exemplar-based models
and undermining prototype-based models, the results of the
present simulation study appear to show no competitive
advantage of the exemplar-based model. Instead, some PB
models were able to reproduce the observed classification
profile and the A2 advantage equally successfully with
smaller numbers of learnable parameters.
Table 2. Results of simulation 1
   Model      NLP    NRP        SSE       SSE x NLP       A2-A1
    PB2        16      2       0.1438        2.301        -5.633
    PB3        22      3       0.0694        1.527        3.643
                                                                     Figure 2. Predicted classification profiles by two best
    PB4        28      4       0.0361        1.011        5.444      prototype based GECLE models (i.e., PB8-GUN: lowest
    PB5        34      5       0.0250        0.850        9.046      absolute fit; PB5-GUN: lowest relative fit).
    PB6        40      6       0.0215        0.860        2.663
    PB7        46      7       0.0193        0.888        4.314
    PB8        52      8       0.0182        0.946        3.273
    EB9       58*      9       0.0201        1.166        8.011
NLP: Number of Learnable Parameters
NRP: Number of Reference Points (e.g. prototype or exemplar)
* Location parameters for exemplar were static & not subject for
learning, but assumed that optimized locations were learned when
the exemplars were created.
Discussion of Simulation 1: All GECLE models that were
capable of learning to locate the reference points were
interpreted as prototype-based models in the present                 Figure 3. Predicted classification profiles by exemplar
simulation study. However, it might not have been a
                                                                     based GECLE model (i.e., EB9-GUN).
sensible interpretation for some of those models,
particularly for models with larger numbers of prototypes
(e.g., PB5 ~ PB8). That is, it does not seem logical to create
                                                                     Simulation 2
eight prototypes from only nine unique stimuli. Rather,              Simulation 2 is a replication of Simulation 1 with 10,000
there may be better interpretations for these models. Two            randomly chosen parameter configurations to investigate
possible alternative interpretations are discussed below.            general tendencies in the A2 advantage by the same eight
   First, it might be more sensible to interpret PB GECLE            models used in Simulation 1. Here, the 10,000 simulated
with larger numbers of prototype as models utilizing                 subjects with randomly assigned parameter values were
“fuzzy” or modular prototypes (or simply modules) as the             trained to classify the 5/4 stimulus set. The ranges of
reference points (RP) in a combinatorial fashion: it tries to        parameters were [0.1 10] for c and φ, [0.001 1] for the three
create and memorize modules (defined by or being                     learning rates.
prototypes of subsets of stimuli belonging to a particular
category) that summarize characteristics of particular               Results & discussion: Table 4 summarizes the results of
feature dimensions more correctly than the other feature             Stimulation 2. In short, the A2 advantage was observed in
dimensions for a particular category, and uses combinations          almost all PB and EB models, indicating that the results of
of the module activations triggered by similarities between          Simulation 1 are reasonably generalizable in this regard.
                                                                 924

   More interestingly, the EB model showed lesser                         Medin and Schaffer’s stimuli. However, these results might
magnitude of the A2 advantage than several PBs. This was                  have resulted from incorrect assumptions about the
mainly because EB9 learned to produce network output                      prototype modeling. For example, I assumed that the
activations correctly with many parameter configurations                  locations of prototypes were continuously updated
(i.e., minimizing the error defined as Equation 5 perfectly)              throughout the training, but in reality, people may quickly
since the model was supplied the correct locations of all                 identify prototypes which may be less likely to be updated
unique stimulus exemplars from the beginning of the                       unless absolutely necessary. Another possible explanation
training. This in turn, resulted in very small differences in             is that people may have a uniquely shaped activation area
classification responses for Stimuli A1 and A2, because the               for each prototype and/or pay attention to correlation among
activations triggered by Stimulus A1 and A2 for the output                feature dimensions. For example, Matsuka (2003 & 2004)
nodes were almost identical (i.e., L2 was minimized). This                showed that there may be an interaction between types of
implies that any EB-based GECLE or any EB-based model                     internal mental representation and types of attention
such as ALCOVE would find this learning task (here,                       mechanism: the prototype-based model performed better
learning task does not correspond to categorization, but L2               when it incorporated unique attention structure with the
minimization, i.e., Eq. 5) easy because it can satisfactorily             capability of paying attention to dimensional correlations;
complete the task with virtually any parameter settings                   whereas the exemplar-based model performed better with
inasmuch as the locations of exemplars were well defined.                 global attention structure with independent dimensional
Although this may be true if the condition of correctly                   attention processes (i.e., no attention to correlations).
memorizing exemplars is met, there is no guarantee for                       In the present simulation study, pure prototype modeling
satisfying the condition in real human cognition. But, more               was reinvestigated using two variants of the original PB2
likely, the condition would not be tenable for some people                GECLE. The first one, SPB-2 is a static version of PB-2.
(i.e., some memorize exemplars more correctly and/or faster               That is SPB-2 is identical to PB2 appeared in Simulations 1
than other individuals). This difference in memorization                  and 2, but the locations of prototypes were supplied from
ability may be one of the factors creating individual                     the beginning of the training and the learning rate for RPs
differences in category learning. This aspect of exemplar                 was set to zero. Thus, this model resembles EB-based
type modeling alone does not invalidate the assumption of                 GECLE (except that RPs were prototypes) in that the
exemplar-type internal representation, but it does suggest                locations of RPs were static. The second one, CPB2, is
that     exemplar-based          (computational)      models       of     PB2-GECLE with the most complex attention mechanism,
categorization could be benefited from integrating an                     namely LCN (see Figure 1, lower right panel), having a
algorithm or quantitative explanation of how people learn                 unique receptive field for each prototype and the capability
and memorize exemplars.                                                   of paying attention to correlation.
   On the contrary, exemplar theorists may argue that the                    For SPB2, the prototype for each category was created by
upper limits of the randomly selected learning rate                       averaging the feature values of each dimension of every
parameters (or the number of training epochs) were set                    object in a particular category, thus [0.8 0.6 0.8 0.6] for
unrealistically high. Although this argument is likely valid              Category A and [0.25 0.5 0.25 0.25] for Category B. The
and thus the interpretation of the results may require some               rest of the procedures of the present simulation study follow
caution, it is still true that exemplar model may need to have            those of Simulations 1 and 2.
learning algorithm for exemplar initialization, maintenance,
and memorization.                                                         Table 5a. Simulation 3: Results based on optimal
                                                                          parameters
Table 4. Results of Simulation study 2: Differences in
classification accuracies for A2 and A1. (numbers of                            Model    NLP   NRP       SSE     SSE x NLP      A2-A1
observed cases shown in parentheses).                                           SPB2      16     2      0.1972      3.155       -9.208
                                                                                CPB2      32     2      0.0377      1.206       11.130
Model      Overall       Classification Accuracy (CA) in training
                          100 CA >90%             90 CA >80%              Table 5b. Simulation 3: A2 advantage based on randomly
  PB2       1.011           -8.725(117)            -8.178(162)            drawn parameters.
  PB3       2.184            0.056(250)            0.539(295)
  PB4       2.521            0.331(556)            1.261(369)                 Model    Overall  Classification Accuracy (CA) in training
  PB5       3.071            0.885(905)            3.007(342)                                     100 CA >90%           90 CA >80%
  PB6       2.711           0.661(1212)            3.816(365)                 SPB2     -2.346      -3.740 (2263)         -4.535(1920)
  PB7       2.962           0.342(1690)            4.029(367)                 CPB2      2.931       -0.814(2215)          5.964(1505)
  PB8       2.446           0.330(2037)            2.885(393)
  EB9       0.050           0.014(7660)            0.087(837)             Results & discussion: A great decrease in SSE was
Note: Observed classification accuracy for the training set is 0.85       obtained for CPB2 as compared with the original PB2, and
                                                                          after controlling for the model complexity by the simple
Simulation 3                                                              linear adjustment (i.e., SSE x NLP) it performed nearly as
Simulations 1 and 2 showed that the pure prototype model,                 good as EB9 (1.206 vs.1.166). In addition, unlike PB2,
PB-2, accounted poorly for phenomena associated with the                  CPB2 was able to replicate the A2 advantage, and it was
                                                                      925

shown to be generalizable to some extent in the second part        prototype-based adaptive models of category learning with
of the present simulation study using the randomly drawn           different structures and model assumptions.
parameters (Table 5b). In contrast SPB2 performed worse               Although, several models were examined in some depth
than PB2 for replicating the observed classification profile.      in the present research, the results were based only on a
Moreover, SPB2 consistently failed to replicate the A2             simulation of one empirical study. More simulation studies
advantage in the randomized simulation study.                      with several other stimulus sets should help identify models
                                                                   or assumptions with descriptive validities more accurately.
Discussion on Simulations                                          In addition, measurements of several different cognitive
                                                                   processes associated with category learning, such as,
Medin and Schaffer’s 5/4 stimulus (1978) has been used as          attention allocation should be collected in empirical studies,
a benchmarking stimulus set for computational models of            in order to restrict model parameters and to better
categorization and category learning, usually favoring             differentiate among models.
exemplar models (e.g. Matsuka et al. 2003; Minda & Smith
2002; Nosofsky & Zaki, 2003). However, the results of the
present simulation studies showed that several GECLE                                        References
models with prototype internal representation performed as         Haykin, S. (1999). Neural Networks: A Comprehensive
good as or better than the exemplar-based GELCLE. One                 Foundation (2nd ed.). Upper Saddle River, NJ: Prentice
type of those successful prototype-based GECLE was the                Hall.
model that created and utilized multiple modular prototypes        Kruschke, J. E. (1992). ALCOVE: An exemplar-based
for categorization. The modular prototype is a prototype              connectionist model of category learning, Psychological
defined by subsets of stimuli belonging to a particular               Review, 99. 22-44.
category that summarize characteristics of particular feature      Matsuka, T (2002). Attention processes in computational
dimensions more correctly than the other feature dimensions           models of categorization.         Unpublished Doctoral
for the particular category (however, the modular prototypes          Dissertation. Columbia University, NY.
may be interpreted as imprecise exemplars). The other type         Matsuka, T. (2003). General exploratory model of human
of the successful prototype-based GECLE was the one with              category learning. Accepted for publication.
uniquely shaped and oriented attention coverage areas and          Matsuka, T. (2004). Interactions between representation
with the capability of paying attention to correlations among         and attention processes in category learning. Poster
feature dimensions.                                                   presented at the 11th Annual Meeting of Cognitive
  There are at least few concerns associated with the                 Neuroscience Society. San Francisco.
present simulation studies. First one, as discussed in             Matsuka, T. & Corter, J.E (2004). Stochastic learning
Simulation 1, is that as the number of GECLE’s reference              algorithm for modeling human category learning.
points (RP) increases, it become philosophically difficult            Accepted for publication.
within the cognitive science paradigm to interpret what            Matsuka, T., Corter, J. E. & Markman, A. B. (2003).
these RP are representing (e.g., modular prototypes vs.               Allocation of attention in neural network models of
imprecise exemplars). The other concern is the way the                categorization. Under revision.
numbers of learnable parameters were counted for the               Medin, D.L. & Schaffer, M.M. (1978). Context theory of
exemplar-based GECLE (see notes on Table 2). That is, in              classification learning, Psychological Review, 85, 207-
the present simulation studies, the location parameters of the        238.
exemplars were counted as learnable parameters. On one             Minda, J.P. & Smith, J.D. (2002). Comparing prototype-
hand, the locations of exemplars may be learnable, because            based and exemplar-based accounts of category learning
they are initialized at the “optimal” location without error.         and attentional allocation. Journal of Experimental
On the other hand, they may not be learnable, because they            Psychology: Learning, Memory, and Cognition, 28, 275-
reside in static locations.                                           292.
                                                                   Nosofsky, R.M. & Zaki, S. R. (2002). Exemplar and
                         Conclusions                                  prototype models revisited: Response strategies, selective
                                                                      attention, and stimulus generalization.        Journal of
Generalized Exploratory model of human Category
                                                                      Experimental Psychology: Learning, Memory, and
LEarning (GECLE) is a flexible and general framework for
                                                                      Cognition, 28, 924-940.
modeling human category learning that is capable of
                                                                   Nosofsky, R.M., Gluck, M.A., Palmeri, T.J., McKinley,
manipulating a limited number of assumptions
                                                                      S.C., & Glauthier, P. (1994). Comparing models of rule-
independently and systematically. In the present study, the
                                                                      based classification learning: A replication and extension
plausibility of two different assumptions about internal
                                                                      of Shepard, Hovland, and Jenkins
representation was investigated with GECLE using
                                                                   Poggio, T. & Girosi, F. (1989) A Theory of Networks for
exemplar-model-friendly Medin & Schaffer 5/4 stimulus set
                                                                      Approximation and Learning). AI Memo 1140/CBIP
(1978). The results of simulations showed no competitive
                                                                      Paper 31, Massachusetts Institute of Technology,
advantage of previously favored exemplar-based modeling.
                                                                      Cambridge, MA.
Rather, they appeared to suggest some prototype models
                                                                   Poggio, T. & Girosi, F. (1990). Regularization algorithms
performed better than an exemplar model. In addition, the
                                                                      for learning that are equivalent to multilayer networks.
exploratory nature of GECLE yielded new plausible
                                                                      Science, 247, 978-982.
                                                               926

