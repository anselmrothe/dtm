UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Dissociations Between Regularities and Irregularities in Language Processing:
Computational Demonstrations Without Separable Processing Components
Permalink
https://escholarship.org/uc/item/0794z85p
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Sibley, Daragh E.
Kello, Christopher T.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

      Dissociations Between Regularities and Irregularities in Language Processing:
         Computational Demonstrations Without Separable Processing Components
                                              Daragh E. Sibley (dsibley@gmu.edu)
                                        Department of Psychology, George Mason University
                                                     Fairfax, VA 22030-4444 USA
                                            Christopher T. Kello (ckello@gmu.edu)
                                        Department of Psychology, George Mason University
                                                     Fairfax, VA 22030-4444 USA
                            Abstract                                 regularities between the spellings and sounds of words, and
                                                                     a system of lexical knowledge serves to override the rules
   Two models are presented that compute a quasi-regular             when necessary (e.g., PINT does not rhyme with MINT).
   mapping. One was based on localist representations of items          Alternatively, single-route theories have been proposed in
   in the quasi-regular domain, the other was based on
                                                                     which the mechanisms and representations for handling
   distributed representations. In each model, a control
   parameter termed input gain was modulated over the one and
                                                                     regularities and irregularities are inseparable. For instance,
   only level of representation that mapped inputs to outputs.       Rumelhart and McClelland (1986) proposed a theory in
   Input gain caused both models to shift between regularity-        which a single route of processing was used to generate the
   based and item-based modes of processing. Performance on          past tense of both regular and irregular verbs (also see, e.g.,
   irregular items was selectively impaired in the regularity-       Joanisse & Seidenberg, 1999). Kello and Plaut (2003)
   based modes, whereas performance on novel items was               proposed a theory of word reading in which the mapping
   selectively impaired in the item-based modes. Thus, each          from spelling to sound is mediated by a single level of
   model exhibited a double dissociation without separable           learned representations (also see Plaut & Gonnerman,
   processing components. These results are discussed in the
                                                                     2000).
   context of analogous dissociations found in language domains
   such as word reading and inflectional morphology.
                                                                        A wide variety of evidence has been brought to bear on
                                                                     dual-route and single-route theories of language processing
                        Introduction                                 (for reviews, see Coltheart et al., 2001; McClelland &
                                                                     Patterson, 2002; Pinker, 1999; Pinker & Ullman, 2002;
The quasi-regular nature of language has played a central            Plaut, McClelland, Seidenberg, & Patterson, 1996). Much of
role in theories of language processing in the mind and              this evidence speaks to one or another particularity of a
brain. On the one hand, language processes must be able to           given theory. Every piece of evidence contributes to the
handle novel inputs, e.g., skilled readers can give reasonable       overall debate, but here we focus on one kind of evidence
pronunciations and conjugations to verbs that they have              that is relevant to all theories in question: dissociations
never encountered before. These abilities demonstrate how            between regularity-based and item-based processing.
language usage can be generative on the basis of                        Double dissociations have been observed in language
regularities. On the other hand, irregular items often exist         processing, and some have been interpreted as evidence for
for which the regularities do not apply. Thus, language              separable regularity-based and item-based components of
processes must be able to override the regularities, when            the language system. In the area of inflectional morphology,
appropriate, with knowledge that is applicable to only a few         Ullman and his colleagues (Ullman et al., 1997) reported
items, or even to just one. How are language processes               evidence for a dissociation between the past tense formation
structured to handle both regularities, and the exceptions to        of regular and irregular verbs in English. They found that
those regularities?                                                  Alzheimer’s patients, as well as aphasics with posterior
   One answer to this question is that any given quasi-              lesions, were poor at generating the past tense of verbs with
regular domain is processed by two complementary routes.             irregular inflections, but relatively normal with regular
A regularity-based route is specialized to capture the               inflections. They found the opposite pattern for Parkinson’s
regularities that span across linguistic items in the domain,        patients and aphasics with anterior lesions. Marslen-Wilson
and an item-based route is specialized to capture knowledge          and Tyler (1997; 1998) found a similar dissociation in a
that is specific to items in the domain. For instance, in the        priming paradigm with language-impaired patients.
words-and-rules theory (Pinker, 1999), rules are used to                In the area of word reading, deficits found in surface and
process regular inflectional morphologys (e.g., WALK-                phonological dyslexia have been interpreted analogously to
WALKED), and a lexicon is used to process irregular                  those found in posterior versus anterior aphasics. For
inflections (e.g., GO-WENT). In the dual-route cascaded              instance, Berhmann and Bub (1992) reported on a surface
(DRC) theory of word reading (Coltheart, Curtis, Atkins, &           dyslexic patient MP for whom the ability to read exception
Haller, 93; Coltheart et al., 2001), a set of grapheme-to-           words (particularly of low frequency) was greatly impaired,
phoneme correspondence rules is used to capture
                                                                 1249

whereas the ability to read both regular words and nonwords       tense formation in English. Therefore, the models are
was mostly intact. By contrast, Funnell (1983) reported on a      intended and reported only as proofs-of-concept.
phonological dyslexic patient WB for whom the ability to             The first model used a single level of localist nodes to
read nonwords (even simple CVC nonwords) was greatly              map input patterns onto output patterns. Each node
impaired, whereas the ability to read both easy and difficult     represented one item in the training corpus, and the
words was mostly intact.                                          activation of each node was a function of the similarity
   The impairments of these and other patients have a             between the item it represented, and the current input to the
straightforward explanation in terms of separable item-           model. Thus, this model could be considered as analogy-
based and regularity-based processing components. The             based because both known and novel inputs were explicitly
deficits in Alzheimer’s patients, posterior aphasics, and         processed in terms of the similarity of their input patterns to
surface dyslexics all reflect damage to an item-based             that of all items in the corpus (see Albright & Hayes, 2003;
component of processing (e.g., a lexicon) that is responsible     Nakisa, Plunkett, & Hahn, 2000).
for irregular items (not necessarily the same component              The second model used a distributed level of
across types of deficits). The deficits in Parkinson’s            representation to map input patterns onto output patterns.
patients, anterior aphasics, and phonological dyslexics all       Hidden representations were learned via backpropagation
reflect damage to a regularity-based component of                 (Rumelhart, Hinton, & Williams, 1985), and each hidden
processing (e.g., rules) that is responsible for novel items.     unit contributed to the processing of many, if not all, items
   These double dissociations appear to challenge single-         in the training corpus. Representations learned through
route theories because item-based and regularity-based            backpropagation tend to map similar inputs onto similar
processes are not separable in single-route theories.             outputs (Rumelhart et al., 1995). Thus, as in the analogy
Proponents of single-route theories have responded to this        model, the distributed model processed both known and
evidence in a number of ways. In some cases,                      novel inputs in terms of their similarity to items in the
methodologies or interpretations of data have been called         corpus. But unlike the analogy model, hidden
into question (e.g., McClelland & Patterson, 2002). In other      representations were shaped by similarities among both
cases, the data have been explained in terms of dissociations     input and output patterns in the corpus, as well as the
between semantic and phonological components of                   relationships between inputs and outputs.
processing, rather than item-based and regularity-based              In both models, input gain is a multiplicative scaling
components (e.g., Joanisse & Seidenberg, 1999). The               parameter on the net inputs to units, be they localist nodes
research to date has left open the question of whether            or hidden units. The current simulation results show that the
dissociations between the processing of novel and irregular       modulation of input gain at testing caused similar effects in
items can be explained without reference to an architectural      both models. At low levels of input gain, both models failed
dichotomy in the language system.                                 to map irregular items to their appropriate outputs, but
                                                                  succeeded in mapping regular items and novel inputs. At
                      Current Work                                high levels of input gain, both models succeeded at mapping
The primary aim of the current study was to demonstrate           both regular and irregular items, but performed poorly with
how a dissociation between item-based and regularity-based        novel inputs.
processing can occur in a single-route architecture without          The reason why input gain caused this double dissociation
any manipulation of separable processing components, i.e.,        was different for each model. In the analogy model, input
without reference to separable semantic and phonological          gain modulated the intensity of competition for activation
contributions to processing. The basic idea is that a single      among localist nodes. Low levels of competition caused
component of processing can shift between two                     outputs to be based on the summed contributions from many
qualitatively different “modes” of processing as a function       partially activated nodes. Regularities across nodes were
of one control parameter. Specifically, we present two            extracted in these summations to the point of overriding any
different kinds of connectionist models that possess a            exceptions to the regularities. By contrast, high levels of
control parameter termed input gain. We show that, in both        competition caused a winner-take-all mode of processing in
types of models, input gain can cause a shift in processing       which a known input correctly activated its corresponding
between an item-based mode and a regularity-based mode.           node, whereas a novel input incorrectly activated a node
Furthermore, we show how this shift can give rise to a            corresponding to a similar, known item.
double dissociation in performance on irregular versus               In the distributed model, input gain modulated the
novel inputs.                                                     sharpness of a sigmoidal activation function. Low levels of
   The models were built to process an abstract, quasi-           input gain caused hidden units to operate mostly in their
regular mapping. Properties of the mapping were analogous         linear range, thereby emphasizing the componential (i.e.,
to basic properties of quasi-regularity in language domains.      regular) relationships that were learned between inputs and
However, items did not correspond to any particular words         outputs. High levels of input gain caused hidden units to
in a particular language domain. The mapping was created          operate mostly in their asymptotic range, thereby
primarily to facilitate analysis of the models, rather than to    emphasizing the conjunctive relationships that were learned
simulate a particular language phenomenon such as the past        between inputs and outputs (for a discussion of
                                                              1250

                                                                                              γεI j
componential and conjunctive coding, see O’Reilly, 2001).
Componential relationships supported only the processing
                                                                                      aj = e        ∑ eγε
                                                                                                      i
                                                                                                           Ii
                                                                                                              ,
of regular and novel items, whereas conjunctive
relationships supported only the processing of known items.       where I was the net input to a unit, calculated as the dot
                                                                  product between the input vector and the incoming weight
                                                                  vector, was input gain, was noise sampled evenly in the
                                                                          γ
                   Simulation Methods                                                       ε
                                                                  range ±0.1, and i spanned all logogens. Each output unit was
Input and Output Representations were constructed from            then calculated as the sigmoid of the dot product between
a 12 dimensional binary space. Out of 212 = 4096 possible         the logogen vector and its incoming weight vector. Noise
input patterns, one fourth (1024) were chosen at random to        was included to break perfect ties between very small (e.g.,
constitute the corpus of items. Each chosen input pattern         two or three) numbers of activated logogens. Such ties
was associated with one output pattern. Output patterns           occurred more often at high levels of input gain.
were created in two steps. First, each input pattern was
copied to its corresponding output pattern (i.e., the identity    Distributed Model Architecture. In the distributed model,
mapping. Note, however, that the results apply to all linearly    the input units were fully connected to 200 hidden units, and
separable mappings). Second, the bit value of each                the hidden units were fully connected to the output units
dimension, for each output pattern, was flipped with a 5%         The number of hidden units was determined through pilot
probability. Thus, the identity mapping was a regularity, and     testing to be about 50 units more than the minimum needed
flipped values were exceptions to that regularity. This           to learn the mapping. However, results were very similar
procedure resulted in 563 fully regular items (no flipped         over a range of hidden unit numbers. Hidden units were
bits), and 461 irregular items with one to four flipped bits      calculated with the hyperbolic tangent function,
per item. The 3072 remaining patterns served as novel items
during testing.
                                                                                                    (
                                                                                        a j = tanh γεI j ,)
   For the analogy model, there were 12 input units               which is analogous to the logistic, except it has asymptotes
corresponding to the 12 input dimensions, and dimension           at ±1 instead of 0 and 1. Input gain ( ) was fixed at 1 during
                                                                                                        γ
values were coded as activations of ±1 on the inputs. For the     training, and varied during testing (see next section). Noise
distributed model, there were 24 input units, half of which       ( ) was fixed at 0.1 (as in the analogy model) during both
                                                                   ε
coded the 12 dimension values as activations of 0 or 1. The       training and testing. Output units were calculated as in the
other half were activated as flipped values of the first half,    analogy model.
i.e., 1–x, where x was each of the first 12 activations. The         Connection weights were initialized to random values in
x|1–x coding scheme was used because the distributed              the range ±0.1, and weights were learned by gradient
model was trained via backpropagation (this scheme was            descent,
not necessary in the analogy model because it was not                                             (
                                                                                       ∆wij = η ∂E ∂wij ,   )
trained; see next two sections). In backpropagation, no
                                                                  where wij was the connection weight from unit j to i, was
learning will occur on a unit’s sending weights when the
                                                                                                                           η
                                                                  the learning rate (fixed at 0.001), and E was cross-entropy
activation value of that unit is zero. Therefore, the x|1–x
                                                                  error (Rumelhart et al., 1995). Weight changes were made
coding scheme ensured that weight derivatives were
                                                                  each time after weight derivatives had been accumulated
generated for every input dimension, on every training
                                                                  over all 1024 items in the corpus. Weight derivatives were
episode.
                                                                  calculated for each item as follows: input units were set to
   For both models, there were 12 output units
                                                                  the item’s input pattern, activation was propagated forward
corresponding to the 12 output dimensions, and dimension
                                                                  through the network, an error signal was calculated from the
values were coded as targets of 0 or 1 on the outputs.
                                                                  difference between actual and target outputs, and the error
                                                                  signal was backpropagated to generate the weight
Analogy Model Architecture. In the analogy model, input
                                                                  derivatives. Weight updates were repeated until every
units were fully connected to 1024 “logogen” units. Each
                                                                  output unit was with 0.1 of its target for every item in the
logogen represented one item in the corpus, and the weights
                                                                  training corpus. This criterion was reached after 3000 passes
on incoming connections from input units were set
                                                                  through the corpus.
according to each logogen’s input pattern, i.e., +1 weights
for positive input dimensions, and -1 weights for negative
                                                                  Testing Procedure. For both models, performance was
dimensions. Each logogen projected outgoing connections
                                                                  assessed on each test item by setting the input units to the
to all 12 output units, and the weights on outgoing
                                                                  item’s input pattern, and then determining whether the
connections were set according to each logogen’s output
                                                                  activation of each output unit was within 0.5 of its target
pattern (as for incoming connections).
                                                                  (which was either 0 or 1). Model outputs were correct only
   To process a given item, input units were first set to the
                                                                  when the activations of all 12 output units were within
item’s input pattern. Logogen activations were then
                                                                  range. Targets for items in the corpus were set according to
calculated with the normalized exponential function (see
                                                                  each item’s output pattern. Targets for the 3072 novel items
Nosofsky, 1990),
                                                                  were set according to each item’s input pattern, i.e., the
                                                                  identity mapping.
                                                              1251

  To dissociate item-based and regularity-based processing,                                           considered as a regularization error because, for the quasi-
input gain was varied as a single control parameter over the                                          regular domain constructed here, the identity mapping is the
logogen units in the analogy model and over the hidden                                                regular mapping.
units in the distributed model. The reported levels of input                                             At high levels of input gain, performance on all items in
gain were between 0.5 and 3 for the analogy model, and                                                the corpus was near perfect in both models. By contrast,
0.333 and 3 for the distributed model. These ranges were                                              mean accuracies for the novel items dropped to as low as
chosen to show asymptotic performance at the lower and                                                16% for the analogy model, and 46% for the distributed
upper ends, i.e., the patterns of behavior did not change                                             model. Of all the analogy model’s erroneous responses to
substantially beyond these ranges.                                                                    novel items at the highest level of input gain, 97% were
                                                                                                      output patterns that corresponded to output patterns in the
                                       Simulation Results                                             training corpus. These responses can be considered as
Mean accuracies for the analogy model are graphed in                                                  lexicalization errors because they are responses for other
Figure 1 as a function of input gain and item type (regular,                                          items in the model’s “lexicon”. The same analysis of errors
irregular, or novel). The same are graphed for the distributed                                        made by the distributed model showed only 27%
model in Figure 2.                                                                                    lexicalization errors (where the chance rate was 25%).
                                                                                                         These results show that the manipulation of input gain as
                      100%                                                                            a single control parameter, over a single level of
                       90%                                                                            representation, caused a clear double dissociation in both
                       80%                                                                            models. To better understand the similarities and differences
                       70%
                                                                                                      in processing between these models, three visualizations of
                                                                            Regular Items
    Percent Correct
                                                                                                      the input-output mappings for each model are shown in
                       60%                                                  Irregular Items
                                                                            Novel Items
                                                                                                      Figure 3.
                       50%                                                                               In each visualization, all 4096 points in the 12
                       40%                                                                            dimensional input space are arranged on a grid such that all
                       30%                                                                            adjacent vertices differ by only one bit. To illustrate, near
                       20%                                                                            the lower left-hand corner of each plot is the vertex where
                       10%                                                                            all 12 input dimensions are negative. The next vertex up and
                        0%
                                                                                                      the next vertex to the right each have one positive input
                             0.50     0.67        1.00       1.50      2.00       2.50        3.00
                                                                                                      dimension, and so on. Each grid “wraps around” such that
                                                          Input Gain                                  vertices on the left edge are adjacent to the corresponding
                                                                                                      vertices on the right edge, and likewise for the top and
                       Figure 1: Mean accuracies for the analogy model                                bottom edges. Thus, the 2D space of each grid represents a
                                                                                                      portion of the similarity structure in the 12D input space. In
                      100%                                                                            addition, 10 evenly spaced points are interpolated in each
                       90%                                                                            space between each pair of vertices. Given that each side
                       80%                                                                            has 64 vertices (642 = 4096), there are 6402 = 409,600
                       70%
                                                                                                      points of the input space represented in each plot.
 Percent Correct
                                                                                                         At each point, a gray scale value is plotted that represents
                       60%
                                                                                                      the summed activation of four output units for the
                       50%
                                                                                                      corresponding input pattern. The same four output units
                       40%                                                                            (chosen arbitrarily) are shown at all points in all plots. The
                       30%                                                                            gray scale values are calculated such that, the darker the
                                                                          Regular Items
                       20%                                                Irregular Items             point, the closer the outputs were to 0.5. Conversely, whiter
                       10%                                                Novel Items                 points indicate where the outputs were at their asymptotes
                        0%                                                                            (0 or 1). Thus, the dark borders in each plot represent the
                             0.33   0.40   0.50    0.67     1.00   1.50    2.00    2.50     3.00      decision boundaries in each model, that is, where one or
                                                          Input Gain                                  more of the four outputs crossed the middle point between
                                                                                                      asymptotes as a function of change in the input space.
                      Figure 2: Mean accuracies for the distributed model
                                                                                                         Plots are shown for each model, at three different levels
                                                                                                      of input gain: the low end (0.5 in the analogy model and
   Figures 1 and 2 show that both models exhibited a clear
                                                                                                      0.333 in the distributed model; top row), the high end (3 in
dissociation in performance on irregular items compared
                                                                                                      both models; bottom row), and the point at which accuracies
with novel items. At low levels of input gain, generalization
                                                                                                      for irregular items and novel items are equal (1.1 in the
of the identity mapping to novel inputs was essentially
                                                                                                      analogy model and 0.8 in the distributed model; middle
perfect, as was performance on regular items. By contrast,
                                                                                                      row). Overall differences in plot densities for the analogy
performance on irregular items dropped to 0%, at which
                                                                                                      model, compared with plot densities for the distributed
point all inputs resulted in the identity mapping. For
                                                                                                      model, were due to differences in the polarity of the output
irregular items, application of the identity mapping can be
                                                                                                   1252

units: outputs in the distributed model tended to be closer to    Moreover, given that mean accuracies were about 80% for
0 or 1, i.e., values that corresponded to white points on the     novel items as well, one can infer that these distortions and
plots.                                                            pockets were mostly isolated to the irregular items. These
                                                                  plots show that a balance was struck at moderate levels of
          Analogy Model           Distributed Model               input gain between item-based and regularity-based
                                                                  processing.
                                                                     The bottom two plots show that, for each model, the grid
                                                                  pattern was mostly replaced by pockets of decision
                                                                  boundaries at the high end of input gain. These pockets have
                                                                  a fairly simple interpretation for the analogy model. Recall
                                                                  that, at the high end of input gain, 97% of the errors for
                                                                  novel items were lexicalizations. What this means is that the
                                                                  pockets show where known inputs were mapped correctly,
                                                                  and where novel items were mapped incorrectly to similar
                                                                  known items. These “item pockets” are a depiction of item-
                                                                  based processing in the analogy model.
                                                                     In the distributed model, the pockets cannot be readily
                                                                  interpreted as item pockets because a substantial number of
                                                                  novel items were mapped correctly at the high end of input
                                                                  gain (46%), and the proportion of lexicalization errors for
                                                                  novel items was not much above chance (27%). It appears
                                                                  that the distortions needed for accurate mappings of
                                                                  irregular items had “spread out” at high levels of input gain.
                                                                  Because the mapping of regular items is mostly correct at
                                                                  the high end of input gain, one can infer that the decision
                                                                  boundaries spread out over untrained (novel) regions of the
                                                                  space more than they did over trained (known) regions. It is
                                                                  this selective spread of decision boundaries that indicates
                                                                  item-based processing at the high end of input gain.
                                                                                          Conclusions
                                                                  The current simulations provide a new demonstration of
                                                                  how double dissociations can occur without separable
                                                                  processing components (see also Devlin & Gonnerman,
                                                                  1998; Juola, 2000). Performance on novel versus irregular
                                                                  stimuli was dissociated by shifting between regularity-based
    Figure 3. Visualizations for each model at low (top),         and item-based modes of processing. Unlike previous
   medium (middle), and high (bottom) levels of input gain        demonstrations, these modes existed at the ends of a
                                                                  continuum created by one control parameter.
   The grid patterns seen in the top two plots of Figure 3           It is important to acknowledge that the current work only
show that both models processed the identity mapping at the       opens the door to an alternative to the rules/lexicon and
low end of input gain. In fact, if all 12 outputs had been        phonology/semantics explanations of double dissociations.
represented, each plot would show a 64 by 64 grid pattern,        It is unclear whether input gain would provide a satisfying
where the grid lines fall exactly between the vertices. Thus,     account of specific empirical results. For instance, input
the grid reflects the finding that, at low input gain, the        gain would not appear to handle dissociations in which all
identity mapping was generalized to all inputs, including         regular items, both novel and known, are impaired
those for novel and irregular items. The grid is a depiction      (Marslen-Wilson & Tyler, 1997, 1998; Ullman et al., 1997).
of regularity-based processing in each model because the          Also, the current simulations did not include subregularities
identity mapping was the regularity in our quasi-regular          or variations in the frequency of items. These factors have
domain.                                                           been simulated successfully (Kello, Sibley, & Plaut,
   The middle two plots show that the grid pattern became         submitted), but only as demonstrations. Subregularities
distorted for both models at moderate levels of input gain,       allowed for model errors that were more like patient errors,
and “pockets” of decision boundaries began to appear.             but further work is necessary to test the simulated errors.
Given that mean accuracies were about 80% for irregular              The current simulations also raise a number of larger
items at these levels of input gain, one can infer that the       questions, such as: Are there any testable differences
distortions and pockets reflect the “warping” of the identity     between the analogy and distributed models presented here?
mapping that was necessary to process the irregular items.        Do these simulation results have implications for current
                                                              1253

theories of word reading and inflectional morphology? Are        Marslen-Wilson, W.D. & Tyler, L.K. (1997). Dissociating
the reported models consistent with the localization of            types of mental computation. Nature, 387, 592-594.
regularity-based and item-based processing in the brain, to      Marslen-Wilson, W.D. & Tyler, L.K. (1998). Rules,
the extent that evidence exists for such localization? What        representations and the English past tense. Trends in
might be the neural bases of input gain? These and other           Cognitive Science, 2, 428-436.
questions await further research.                                McClelland, J. L., & Patterson, K. (2002). Rules or
                                                                   connections in past-tense inflections: What does the
                    Acknowledgments                                evidence rule out? Trends in Cognitive Sciences, 6, 465-
This work was funded in part by NIH Grant MH55628, and             472.
NSF Grant 0239595. The computational simulations were            Nakisa, R., Plunkett, K. & Hahn, U. (2000). Single- and
run using the Lens network simulator (version 2.6), written        dual-route models of inflectional morphology. In P.
by Doug Rohde (http://tedlab.mit.edu/~dr/Lens). We thank           Broeder & J. Murre (Eds.), Models of language
David Plaut for his input on precursors to this work.              acquisition: Inductive and deductive approaches. New
                                                                   York: Oxford University Press.
                         References                              O'Reilly, R. C. (2001). Generalization in interactive
                                                                   networks: The benefits of inhibitory competition and
Albright, A., & Hayes, B. (2003). Rules vs. analogy in             Hebbian learning. Neural Computation, 13, 1199-1241.
  English past tenses: a computational/experimental study.       Plaut, D. C. (1995). Double dissociation without
  Cognition, 90, 119-161.                                          modularity:        Evidence       from        connectionist
Behrmann, M., & Bub, D. (1992). Surface dyslexia and               neuropsychology. Journal of Clinical and Experimental
  dysgraphia: Dual routes, single lexicon. Cognitive               Neuropsychology, 17, 291-321.
  Neuropsychology, 9, 209-251.                                   Plaut, D. C., McClelland, J. L., Seidenberg, M. S., &
Coltheart, M., Curtis, B., Atkins, P., & Haller, M. (1993).        Patterson, K. (1996). Understanding normal and impaired
  Models of reading aloud: Dual-route and parallel-                word reading: Computational principles in quasi-regular
  distributed-processing approaches. Psychological Review,         domains. Psychological Review, 103, 56-115.
  100, 589-608.                                                  Plaut, D. C. & Gonnerman, L. M. (2000). Are non-semantic
Coltheart, M., Rastle, K., Perry, C., Langdon, R. & Ziegler,       morphological effects incompatible with a distributed
  J. (2001). DRC: A dual route cascaded model of visual            connectionist approach to lexical processing? Language
  word recognition and reading aloud. Psychological                and Cognitive Processes, 15, 445-485.
  Review, 108, 204-256.                                          Pinker, S. (1999). Words and Rules: The Ingredients of
Devlin, J. T., Gonnerman, L. M., Andersen, E. S., &                Language. New York: Basic Books.
  Seidenberg, M. S. (1998). Category-specific semantic           Pinker, S., & Ullman, M. T. (2002). The past and future of
  deficits in focal and widespread brain damage: A                 the past tense. Trends in Cognitive Sciences, 6, 456-463.
  computational       account.    Journal     of    Cognitive    Rumelhart, D. E., Durbin, R., Golden, R., & Chauvin, Y.
  Neuroscience, 10, 77-94.                                         (1995). Backpropagation: The basic theory. In C. Yves &
Funnell, E. (1983). Phonological processes in reading: New         D. E. Rumelhart (Eds.), Backpropagation: Theory,
  evidence from acquired dyslexia. British Journal of              architectures, and applications (pp 1-34). Hillsdale, NJ:
  Psychology, 74, 159-180.                                         Lawrence Erlbaum.
Harm, M. W., & Seidenberg, M. S. (1999). Phonology,              Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).
  reading, and dyslexia: Insights from connectionist models.       Learning representations by back-propagating errors.
  Psychological Review, 163, 491-528.                              Nature, 323, 533-536.
Joanisse, M. F., & Seidenberg, M. S. (1999). Impairments in      Rumelhart, D. E., McClelland, J. L. (1986). On learning the
  verb morphology following brain injury: a connectionist          past tenses of English verbs. In D. E. Rumelhart,
  model. Proceedings of the National Academy of Sciences           McClelland, J. L., and The PDP Research Group (Eds.),
  of the United States of America, 96, 7592-7597.                  Parallel distributed processing: Explorations in the
Juola, P. (2000). Double dissociations and neurophysio-            microstructure of cognition (pp. 216-271). Cambridge,
  logical expectations. Brain & Cognition, 43, 257-262.            MA: MIT Press.
Kello, C. T. (2003). The emergence of a double dissociation      Ullman, M. T., Corkin, S., Coppola, M., Hickok, G., & et al.
  in the modulation of a single control parameter in a             (1997). A neural dissociation within language: Evidence
  nonlinear dynamical system. Cortex, 39, 132-134.                 that the mental dictionary is part of declarative memory,
Kello, C. T. & Plaut, D. C. (2003). Strategic control over         and that grammatical rules are processed by the
  rate of processing in word reading: A computational              procedural system. Journal of Cognitive Neuroscience, 9,
  investigation. Journal of Memory and Language, 48, 207-          266-276.
  232.                                                           Van Orden, G.C., Pennington, B.F., & Stone, G.O. (2001).
Kello, C.T., Sibley, D.E., & Plaut, D.C. (submitted).              What do double dissociations prove? Cognitive Science,
  Dissociations in performance on novel versus irregular           25, 111-172.
  items: Single-route demonstrations with input gain in
  localist and distributed models. Manuscript under review.
                                                             1254

