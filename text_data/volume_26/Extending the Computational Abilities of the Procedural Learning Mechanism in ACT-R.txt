UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Extending the Computational Abilities of the Procedural Learning Mechanism in ACT-R
Permalink
https://escholarship.org/uc/item/7zw5g0xj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Fu, Wai-Tat
Anderson, John R.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

 Extending the Computational Abilities of the Procedural Learning Mechanism in ACT-R
                                                  Wai-Tat Fu (wfu@cmu.edu)
                                                John R. Anderson (ja+@cmu.edu)
                                       Department of Psychology, Carnegie Mellon University
                                                    Pittsburgh, PA 15213, USA
                             Abstract                                       by-case feedback. For example, consider the case where the
                                                                            probabilities of effectiveness of three treatments 1, 2, and 3
  The existing procedural learning mechanism in ACT-R                       are as shown in Figure 1. Since the effectiveness of each
  (Anderson & Lebiere, 1998) has been successful in explaining              treatment follows a continuous distribution, a simple binary
  a wide range of adaptive choice behavior. However, the
                                                                            feedback function is obviously insufficient to represent the
  existing mechanism is inherently limited to learning from
  binary feedback (i.e. whether a reward is received or not). It is
                                                                            information received from the feedback.
  thus difficult to capture choice behavior that is sensitive to
                                                                                  P(Effectiveness)
  both the probabilities of receiving a reward and the reward
  magnitudes. By modifying the temporal difference learning
  algorithm (Sutton & Barto, 1998), a new procedural learning
  mechanism is implemented that generalizes and extends the
  computational abilities of the current mechanism. Models
  using the new mechanism were fit to three sets of human data
                                                                                                                                            Effectiveness
  collected from experiments of probability learning and                                              Treatment 1 Treatment 2 Treatment 3
  decision making tasks. The new procedural learning                        Figure 1. Probability of effectiveness of three treatments.
  mechanism fit the data at least as well as the existing
  mechanism, and is able to fit data that are problematic for the
                                                                              Another motivation for extending the current mechanism
  existing mechanism. This paper also shows how the principle
  of reinforcement learning can be implemented in a production              comes from recent findings of the functional role of
  system like ACT-R.                                                        dopaminergic signals in basal ganglia during procedural
                                                                            learning. Research shows that learning is driven by the
                         Introduction                                       deviation between the expected and actual reward (Schultz
                                                                            et al., 1995; Schultz, Dayan, & Montague, 1997). In other
Human choice behavior is often studied under various
                                                                            words, the reward magnitude is often processed as a scalar
probability learning situations. In a typical probability
                                                                            quantity – depending on whether the magnitude of the
learning situation, participants are asked to select one of the
                                                                            actual reward is higher or lower than expected, a positive or
many options available, and feedback on whether the choice
                                                                            negative reinforcement signal is generated respectively. The
is correct or not is given after the selection. There are
                                                                            pre-specification of correct and incorrect responses is
usually two main manipulations in a probability learning
                                                                            therefore inconsistent with the current understanding of the
task: (1) the probabilities for each of the options being
                                                                            procedural learning mechanism in basal ganglia.
correct, and (2) the magnitudes of reward (usually
monetary) received when the correct option is selected. One
robust result is that people tend to choose the options a
                                                                                                     The ACT-R 5.0 architecture
proportion of time equal to their probabilities of being                    Figure 2 shows the basic architecture of the ACT-R 5.0
correct – a phenomenon often called “probability matching”                  system. The core of the system is a set of production rules
(e.g. Friedman et al., 1964). However, when the reward                      that represents procedural memory. Production rules
magnitudes are varied, the observed choice probabilities are                coordinate actions in each of the separate modules. The
sometimes larger or smaller than the outcome probabilities                  modules communicate to each other through its buffer,
                                                                            which holds information necessary for the interaction
(e.g. Myers, Fort, Katz, & Suydam, 1963). These studies
                                                                            between the system and the external world. Anderson, Qin,
show consistently that people are sensitive to both outcome
                                                                            Sohn, Stenger, and Carter (2003) showed that the activity in
probabilities and reward magnitudes in making choices.                      these buffers match well to the activities in certain cortical
   One limitation of the current ACT-R procedural learning                  areas (see Figure 2). The basal ganglia are hypothesized to
mechanism (Lovett, 1998) is that it requires a pre-                         implement production rules in ACT-R, which match and act
specification of correct and incorrect responses. Besides,                  on patterns of activity in the buffers. This is consistent with
feedback received is limited to a binary function (i.e.                     a typical ACT-R cycle in which production rules are
whether a reward is received or not). Apparently, a simple                  matched to the pattern of activity in the buffers, a
binary function may not be sufficient to represent the                      production is selected and fired, and the contents in the
feedback from the environment. For example, imagine a                       buffers updated. In ACT-R, when there is more than one
situation in which there are several possible treatments for a              production matching the pattern of buffer activity, the
particular disease and a physician has to choose a treatment                system selects a production based on a conflict resolution
that has the highest expected effectiveness. One may have to                mechanism. The basis of the conflict resolution mechanism
evaluate the effectiveness of each treatment through case-                  is the computation of expected utility, which captures the
                                                                      416

effectiveness and efficiency of the                                           production       in         Procedural learning updates the value of P and C according
accomplishing the goals of the system.                                                                    to the following equations:
                                                                                                           P=
                                                                                                                   successes
                                                                                                                                      C=
                                                                                                                                                efforts    , where
      Intentional module
        (not identified)
                                                                        Declarative module
                                                                     (Temporal/Hippocampus)
                                                                                                              successes + failures          successes + failures
                                                                                                          successes and failures are the number of times the
         Goal Buffer
          (DLPFC)
                                                                           Retrieval Buffer
                                                                              (VLPFC)                     production has succeeded or failed to accomplish the current
                                                                                                          goal respectively (i.e. a reward or penalty), and efforts is the
                      Productions                                                                         total amount of time taken over all past uses of the
                                      Matching (Striatum)
                                                             Procedural
                                                              Learning
                                      Selection (Pallidum)
                                                             (Dopamine                                    production rule, successful or failed. These quantities start
                    (Basal Ganglia)                                                                       out with initial values that are updated with experience. For
                                                              Neurons)
                                      Execution (Thalamus)
        Visual Buffer                                                       Manual Buffer
                                                                                                          example, if for production n the initial successes equals 1,
          (Parietal)                                                          (Motor)                     failures equals 1, and efforts equals 0.5, when a pre-
        Visual module                                                       Manual module
                                                                                                          specified success is encountered 0.1 second after k has fired,
      (Occipital/Parietal)                                                (Motor/Cerebellum)              P will change from 0.5 to 0.67 (=2/(2+1)), C will change
                                               External World
                                                                                                          from 0.25 to 0.2 (=(0.5+0.1)/(2+1)). If G equals 20, then the
                                                                                                          expected utility (E=PG-C) will increase from 9.75
                                                                                                          (=0.5*20-0.25) to 13.13 (=0.67*20-0.2). The successful
Figure 2. The ACT-R 5.0 architecture.
                                                                                                          experience has thus acted as a reward and reinforced
                                                                                                          production n by increasing its expected utility, and as a
   To adapt to the environment, the system must learn from
                                                                                                          consequence, n will be more likely to be selected in the
the consequences of its actions so that when the same
conditions are met in the future, a better choice of                                                      future..
productions can be made. Procedural learning updates the                                                  Table 1. A list of free parameters and their definitions.
expected utility of a production from the consequences of
firing the production, and the dopamine systems in basal                                                   Parameters               Definition (Old mechanism)
ganglia are believed to be involved in the learning process.                                              G               Value of the goal (measured in seconds)
Specifically, procedural learning appears to be coded by                                                  successes/      Initial number of times the production has led
dopaminergic signals from the ventral tegmental area (VTA)                                                failures        to a success/failure state before the model
and substatntia nigra to the striatum in basal ganglia                                                                    starts
(Schultz, et al., 1995; Schultz, et al., 1997), and different                                             efforts         Total amount of time taken over all past uses
patterns are either reinforced or penalized according to the                                                              of the production, successful or failed.
dopaminergic signals. Previous studies (Ljungberg,                                                         Parameters               Definition (New mechanism)
Apicella, & Schultz, 1992; Mirenowicz, Schultz, 1994)
                                                                                                          rn              The actual reward received
show that the activation of dopamine neurons depends
entirely on the difference between the predicted and actual                                               K               The discount factor (0 < K ≤ 1). Future
rewards. Once an unpredicted reward is perceived, response                                                                rewards are discounted by 1/(1+KD), where D
in dopamine neurons is transferred to the earlier reward-                                                                 is the time between the firing of the current
predicting stimulus. Inversely, when a predicted reward fails                                                             and the next production.
to occur, dopamine neurons are depressed in their activity at                                             a               The learning rate.
exactly the time when the reward would have occurred                                                      Dn+1            The time between the consecutive firing of
(Schultz, Apicella, Ljungberg, 1993). It therefore appears                                                                production n and n+1
that dopamine signals do not simply report the occurrence of
rewards. Rather, outputs from dopamine neurons appear to                                                     Although the existing mechanism was able to match to
code for a deviation or error between the actual reward                                                   human choice behavior, there are aspects in which the
received and predictions or expectations of the reward. In                                                mechanism can be improved. First, in the existing
other words, dopamine neurons seem to be feature detectors                                                mechanism, learning of P requires pre-specification of
of the “goodness” of environmental events relative to the                                                 successful or failure states and the expected utility will
learned expectations about those events.                                                                  increase or decrease respectively when the state is reached.
                                                                                                          The use of success and failure states may not be sufficient in
The current procedural learning mechanism                                                                 situations where a continuous feedback function is required.
During each cycle of ACT-R, productions that match the                                                    From a practical perspective, pre-specification of success
contents of the buffer will be put into a conflict set. The                                               and failure states could be difficult especially in complex
productions in the conflict set are ordered in terms of their                                             tasks, in which some states are often considered “more
expected utility and ACT-R considers them according to                                                    successful” than others. One way to improve the current
that ordering. The expected utility of a production is defined                                            mechanism is to learn from a scalar reward value. Being
as E = PG-C, where P is the estimated probability that the                                                able to assign a scalar reward value to a production
goal will be achieved if that production is chosen, G is the                                              therefore allows more flexible pre-specification of the
value of the goal, and C is the estimated cost of achieving                                               reward structure of the environment and allows the model to
the goal if that production is chosen (see Table 1).                                                      adapt to the environment accordingly. Second, the existing
                                                                                                    417

procedural learning mechanism will change the expected                      U’(n) = U(n) + a[rn – Cn + U(n+1)/(1+KDn+1) - U(n)]
utilities of productions only when the actual outcome is
experienced, which requires keeping track of the whole              One can see that when the estimate is perfectly accurate,
sequence of previous productions that leads to the outcome.         TD(n) = 0, or U(n) = rn – Cn + U(n+1) /(1+KDn+1) and
This could be computationally expensive especially when             learning will stop. The value of TD(n) can therefore be
the number of productions is large. It is therefore desirable       considered the prediction error (as encoded by dopaminergic
to have a mechanism that learns from local information              signals), and the mechanism learns by reducing this
before the outcomes are known.                                      prediction error. It can easily be seen that once a primary
                                                                    reward is received, the expected utility of the productions
     The new procedural learning mechanism                          that lead to the reward will be credited with a discounted
In the artificial intelligence community, algorithms have           reward, and discounting is heavier the farther away the
been developed to allow agents to learn in different                production is from the reward.
environments (Sutton & Barto, 1998). One established                   The new mechanism updates the expected utility based on
algorithm is the Temporal Difference (TD) algorithm, which          the difference between the predicted and actual net reward.
was originally inspired by behavioral data on how animals           There are two main differences between the new and
learn prediction (Sutton & Barto, 1981). Research showed            existing mechanisms. In the new mechanism, the reward is a
that the TD algorithm is well suited to explain the functional      scalar quantity, and the amount of change is determined by
role of dopaminergic signals (e.g. Houk, et al., 1995;              the difference between the predicted and actual reward,
Holroyd & Coles, 2002, O’Reilly, 2003). The TD algorithm            which is consistent with the functional role of dopaminergic
is designed to learn to estimate future rewards based on            signals. This characteristic allows the new mechanism to
experience, and has a built-in credit assignment mechanism          extend its learning capabilities beyond a binary function as
that reinforces the predicting stimuli.                             in the existing mechanism. Second, in the existing
   In its simplest form, the new mechanism can be                   mechanism, learning requires keeping track of a long
represented as U’(n) = U(n) + aTD(n), where U’(n) is the            sequence of productions that lead to the reward. However,
updated value of the expected utility U(n) of production n          in the new mechanism, only the expected utility of the next
after an ACT-R cycle, a is the learning rate, and TD(n) is          production is required. The reinforcement signal will
the temporal difference error. TD(n) calculates the                 eventually propagate back to the productions that lead to the
difference between the actual and expected rewards, i.e.            reward.
TD(n) = R(n) – U(n). The basic learning mechanism is
therefore similar to the learning rule of Rescola and Wagner                      Testing the new mechanism
(1972) (e.g. see Sutton & Barto, 1981). The measure of              The goal of this paper is to show the limitations of the
future rewards has to take into account long-term as well as        existing mechanism and how the new mechanism is able to
short-term consequences. It is plausible to weigh immediate         extend the learning capabilities of ACT-R. However, owing
primary reinforcement more strongly than delayed primary            to space limitation, we are unable to show all properties of
reinforcement. We chose to use the hyperbolic function to           mechanism. For example, none of the data sets in this paper
discount delayed reinforcement (the justification of using          was sensitive to the discount parameter K, so we fixed it at
the hyperbolic function is beyond the scope of this paper,          1.0 and just varied the value of rn to fit the data2. The
but see Lowenstein & Prelec, 1991; Mazur, 2001). A good             learning rate a was also fixed at 0.1. We first used the new
estimate of the total future rewards is therefore R(n) ≈ rn +       mechanism to fit two data sets from the probability learning
U(n+1)/(1+KDn+1), where rn is the immediate reward                  tasks by Friedman et al. (1964) and Myers et al. (1963).
received for production n, U(n+1) is the expected utility of        Since these two sets of data were also modeled well by the
the production that fires after production n, K is the discount     existing mechanism (Lovett, 1998), we were able to
parameter, and Dn+1 is the time lag between the times when          compare the results of the two mechanisms and show that
production n and production n+1 fire. To implement the              the use of TD error to drive the learning process is at least as
mechanism in ACT-R, the basic algorithm has to be                   effective as the existing mechanism. Finally, we used the
modified to take both the reward and cost into account and          new mechanism to fit the data from a decision making task
translate them into a single dimension1 – i.e. the                  studied by Busemeyer and Myung (1992), which we believe
reinforcement will be the difference between the reward and         were problematic for the existing mechanism.
cost (i.e. the net reward). In other words, the estimate
becomes R(n) ≈ rn – Cn + U(n+1)/(1+KDn+1), where Cn is the          Probability matching behavior
cost of firing production n. Putting the estimate of R(n) back      In Friedman et al., participants completed more than 1,000
to the equation for U’(n), we have:                                 choice trials over the course of three days. For each trial, a
                                                                    signal light was illuminated, participants pressed one of the
                                                                    two buttons, and then one of the two outcome lights was
1
   ACT-R takes the agnostic economist’s position of simply
                                                                    2
assuming these map onto some internal values without deeply           Since the delay D is a constant for all data sets, it can be shown
inquiring why.                                                      that the parameter K is absorbed into the value or rn.
                                                                418

illuminated. Task instructions encouraged participants to try      when there was a monetary reward, participants seemed to
to guess the correct outcome for each trial. The study             be “overmatching”. From the data, it also appears that the
extended the standard probability learning paradigm by             higher the reward, the more the choice proportion exceeds
changing the two buttons’ success probabilities across 48-         the matching probability.
trial blocks during the experiment. Specifically, for the odd-
                                                                   Table 3. Observed and predicted choice proportions from
numbered blocks 1-17, the probabilities of success of the
                                                                   the experiment by Myers et al. (1963). Predicted scores are
buttons (p and 1-p) were 0.5. For the even-numbered blocks
                                                                   in parentheses.
2-16, p took on the values from 0.1, to 0.9 in a random
order. We focus on the analysis of the even-numbered                 Reward                          Probabilities
blocks, as they show how people adapted to the outcomes               (cents)        p = 0.6            p = 0.7             p = 0.8
with experience.                                                          0      0.624 (0.612)       0.753 (0.750)       0.869 (0.829)
Table 2. Observed and predicted choice proportions from                   1      0.653 (0.676)       0.871 (0.834)       0.925 (0.938)
the experiment by Friedman et al. (1964). Predicted scores               10      0.714 (0.711)       0.866 (0.836)       0.951 (0.944)
are in parentheses. Each block has 12 trials.
                                                                      Since the task is basically the same as in Friedman et al.,
                              Probabilities                        we used the same model to fit the data. We used the same
    P     Block 1       Block 2        Block 3      Block 4        set of parameters to fit the data in the no reward conditions
   0.1   0.34 (0.37)   0.23 (0.24) 0.18 (0.17)     0.15 (0.13)     (i.e. reward = 3). We chose the reward parameters
   0.2   0.37 (0.41)   0.26 (0.26) 0.29 (0.23)     0.31 (0.23)     (reward=±4.97 and ±5.7 for the ±1¢ and ±10¢ conditions
   0.3   0.49 (0.49)   0.41 (0.41) 0.44 (0.34)     0.35 (0.33)     respectively3) in the reward conditions to maximize the fit,
   0.4   0.46 (0.53)   0.44 (0.50) 0.38 (0.43)     0.38 (0.38)     and obtained R2 of 0.98 and MSE of 0.0008, which is
   0.6   0.56 (0.59)   0.51 (0.59) 0.52 (0.55)     0.52 (0.57)     similar to the fit obtained by the existing procedural
   0.7   0.50 (0.56)   0.53 (0.64) 0.58 (0.72)     0.62 (0.75)
                                                                   learning mechanism. However, we had only two free
   0.8   0.50 (0.51)   0.76 (0.71) 0.74 (0.77)     0.73 (0.78)
                                                                   parameters in this model, compared to three free parameters
   0.9   0.66 (0.62)   0.78 (0.79) 0.78 (0.81)     0.79 (0.81)
                                                                   in the model reported in Lovett (1998). In addition, the new
                                                                   mechanism provides a more natural interpretation of the
   Table 2 shows the observed and predicted proportion of          overmatching behavior – when the reward was large,
choices in the experiment by Friedman et al. Participants in       learning increases the expected utilities of the successful
general exhibited probability matching behavior. Across the        productions to higher values (since the deviation was
four 12-trial subblock, participants chose the correct buttons     larger). As a consequence, the model exhibited
in roughly 50% of the trials in the first block and                overmatching behavior. On the other hand, Lovett (1998)
approached the corresponding p values in each block. The           manipulated the architectural parameter G to fit the data,
predicted proportions were generated by the model, which           which seems awkward, as G is not supposed to be directly
had two critical productions, Choose-Right-Button and              under strategic control.
Choose-Left-Button, and the expected utilities of these
productions were learned according to the new mechanism.           Learning from normally distributed rewards
The exact sequence of outcomes as reported in Friedman et
al. was presented to the model. A reward of 3 is obtained          Busemeyer and Myung (1992) conducted an experiment in
when the correct button was chosen (i.e. rn=3). The initial        which participants were told to select one of the three
expected utilities of the two productions were set to 0. The       treatment strategies for patients suffering from a common
fit was good, R2 = 0.97, MSE = 0.003, which was similar to         set of symptom patterns. Feedback on the effectiveness
the model based on existing procedural learning mechanism.         produced by the treatment was given after each selection.
We conclude that the new mechanism can represent the               For the sake of convenience, the treatment with the highest
learning mechanism at least as well as the existing                expected effectiveness is called Treatment 3, and the next
mechanism with the same number of free parameters.                 less effective treatment is called Treatment 2, and so on (see
                                                                   Figure 1). The effectiveness produced by each treatment
Overmatching behavior                                              was normally distributed with equal standard deviation, but
                                                                   the mean payoffs are different (as explained below).
Myers et al. performed another probability learning                Participants had to evaluate each treatment based on trial-
experiment, but they also varied the amount of monetary            by-trial feedback. Participants were told to maximize the
reward that participants would receive for each correct            sum of the treatment effects over training and they were
response. Participants would either receive no reward or           paid 4¢ per point. The means of the normal distributions are
penalty, ±1¢, or ±10¢ for each correct and incorrect               m-d, m and m+d for Treatment 1, 2, and 3 respectively. The
responses. The probabilities that one of the alternatives was      two independent variables were mean difference (d) (i.e. the
correct were p=0.6, p=0.7, and p=0.8. Table 3 shows the
choice proportions for the participants in each of the
                                                                   3
conditions. When there was no reward, participants seemed             Since the reward values used in the model reflect subjective
to be exhibiting probability matching behavior. However,           values, they do not necessarily follow a linear relationship with the
                                                                   external reward values.
                                                               419

separation of the distributions in Figure 1) and standard          last case, we showed that the new mechanism fits data that
deviation (s) (which affects the amount of overlap in Figure       are problematic for the existing mechanism. The new
1). The exact values of d and s are shown in Table 4. Each         mechanism learned to probability match the true
participant was given 9 blocks (50 trials per block) of            probabilities of outcomes by reducing the difference
training in each condition. The model received the same            between the expected and actual reward. As the difference
amount of training as the participants.                            diminished, the change in the prediction decreased. When
   From Table 4, we can see that as the mean difference            the reward was large, learning increases the expected
increased, the observed choice proportions of the optimal          utilities of the successful productions to higher values (since
treatment increased. As the standard deviation increased, the      the deviation was larger). As a consequence, the chance of
observed choice proportions of the best treatment decreased        selecting the option that had the higher probability of being
except when the mean difference was 3.0. The results               correct increased – i.e. the model exhibited overmatching
showed that participants adapted their choice by learning the      behavior.
expected effectiveness of treatments. The results also                Although the first two sets of data can be modeled by the
showed that the more distinguishable the distributions were        existing learning mechanism, the new mechanism provided
(larger mean difference or smaller standard deviation), the        a more natural explanation to the results. In the final set of
more likely the participants would choose the best                 data, we showed how the new mechanism generalizes and
treatment.                                                         extends the computational abilities of the existing
                                                                   mechanism. The mechanism was able to learn the expected
Table 4. Observed and predicted choice proportions of the
                                                                   effectiveness of each treatment based on trial-by-trial
optimal treatment from the experiment by Busemeyer &
                                                                   feedback, without the need to pre-specify whether the
Myung (1992). Predicted scores are in parentheses.
                                                                   productions had led to successful or failure states.
     Standard                   Mean difference (d)
   deviation (s)        2.0            2.5             3.0                                   Discussion
        3.0         0.69 (0.74)    0.84 (0.79)    0.85 (0.84)      We have presented a new procedural learning mechanism in
        4.5         0.69 (0.72)    0.72 (0.76)    0.84 (0.80)      ACT-R. The use of the deviation between the expected and
        6.0         0.65 (0.68)    0.63 (0.69)    0.86 (0.83)      actual reward values in the new learning mechanism is
                                                                   consistent with the current understanding of the functional
   To model the data, we built three productions that chose        role of VTA dopamine neurons in basal ganglia. We showed
each of the treatments. The initial expected utility of each       that the new mechanism generalizes and extends the
production was set to 0. For each trial, the rewards obtained      computational abilities of the existing procedural learning
by the model were simulated by drawing a sample from the           mechanism. Specifically, the new mechanism is not limited
normal distribution that represents the effectiveness of the       to learning from binary feedback functions. Rather, the new
treatment chosen by the model. The value of r was chosen to        mechanism is able learn from continuous reward functions
be 1.76 to best fit the data. We obtained a fit of R2=0.94,        with similar sensitivity to the variations in the reward
RMSE=0.007. The good fit to the data show that the new             distributions. The current paper also showed how the
learning mechanism was able to build up the expected               reinforcement learning mechanism observed in basal
effectiveness of the treatments from trial-by-trial feedback,      ganglia can be implemented in production systems such as
and was able to exhibit similar sensitivity to the differences     ACT-R.
of the distributions as participants. Since the effectiveness         In practice, the current mechanism allows the use of a
was sampled from a normal distribution, it is difficult to         scalar reward parameter without the need to pre-specify
pre-specify which treatment was successful. It is therefore        success or failure states in a task. This pre-specification
difficult to use the existing learning mechanism to model          could be difficult especially in complex tasks in which a
these data. In the new mechanism, however, whenever the            state could sometimes be good or bad depending on one’s
actual reward was higher than the expected utility of the          experience with the task, as experience may change one’s
production, the production will be reinforced; otherwise the       expectation of different states. In addition, although the
production will be penalized. With the same amount of              existing mechanism can adapt to different magnitudes of
experience (50 trials), the expected utilities of the              reward, the change of the architectural parameter G (in
production were able to reflect the actual expected                E=PG-C) to fit the data may not be easy in complex tasks
effectiveness of the treatments.                                   that has many subgoals, especially when some subgoals
                                                                   may be considered “more successful” than the others.
Summary                                                               Owing to space limitations, we are not able to show all
We have fit a new procedural learning mechanism of ACT-            properties of the mechanism. In fact, we have only tested
R to three separate sets of data with all parameters held          the mechanism in single-choice tasks, which did not depend
constant except the reward magnitudes the models received          critically on the credit assignment mechanism. The
after each trial. In the first two cases, the new mechanism        discounting of future rewards therefore did not affect
did at least as well as the existing mechanism in capturing        performance of the models in all three tasks that we have
the observed choice proportions in different settings. In the      presented. However, we believe the discounting mechanism
                                                               420

is more plausible than the existing mechanism, in which              Houk, J. C., Adams, J. L., & Barto, A. G. (1995). A model
immediate and future rewards are weighted equally.                    of how the basal ganglia generate and use neural signals
   In all three data sets, the model had the same amount of           that predict reinforcement. In J. C. Houk, J. L. Davis, & D.
experiences as the participants and reached the same level of         G. Beiser (Eds.), Models of information processing in the
asymptotic performance. In the first data set, we also                basal ganglia (pp. 233-248). Cambridge, MA: MIT Press.
showed that the performance of the model in each of the              Ljungberg, T., Apicella, P., Schultz, W. (1992). Responses
four subblocks matched the participants well, suggesting              of monkey dopamine neurons during learning of
that the learning rate of the mechanism is comparable to that         behavioral reactions. Journal of neurophysiology, 67, 145-
of the participants. However, it is possible that the                 163.
reinforcement learning mechanism could be slow for more              Loewenstein, G. & Prelec, D. (1991). Negative time
complex tasks. It could be problematic, for example, when a           preference. The American Economic Review, 81 (2), 347-
primary reward is received after a long sequence production           352.
firings. Since only one production is updated during each            Lovett, M. C. (1998). Choice. In C. Lebiere and J. R.
ACT-R cycle, the primary reward may take several cycles to            Anderson, Atomic components of thought (chapter 8, pp.
propagate back to the production where the critical decision          255-296).
is made. It is not clear how people learn in such situations.        Mazur, J. E. (2001). Hyperbolic value addition and general
It is possible that they rely on direct instruction to point out      models of animal choice. Psychological review, 108 (1),
such contingencies rather than counting on a automatic                96-112.
learning mechanism. It does not seem that the mechanisms             Mirenowicz, J., Schultz, W. (1994). Importance of
behind the dopamine reward system are capable of spanning             unpredictedness for reward responses in primate dopamine
unbounded lengths of time in a way that would lead to rapid           neurons. Journal of neurophysiology, 72, 1024-1027.
convergence.                                                         Myers, J. L., Fort, J. G., Katz, L., & Suydam, M. M. (1963).
                                                                      Differential monetary gains and losses and event
                     Acknowledgments                                  probability in a two-choice situation. Journal of
The current work is supported by a grant from the office of           Experimental Psychology, 66, 521-522.
naval research (N00014-99-1-0097). We thank Niels                    O’Reilly, R. C. (2003). Making working memory work: a
Taatgen, Pat Langley, and two anonymous reviewers for the             computational model of learning in the prefrontal cortex
useful comments on an earlier version of this paper.                  and basal ganglia. Institute of Cognitive Science,
                                                                      University of Colorado, Boulder, Technical Report 03-03.
                         References                                  Rescorla, R. A. and Wagner, A. R. (1972). A theory of
                                                                      Pavlovian conditioning: variation in the effectiveness of
Anderson, J. R. (1990). Rules of the mind. Mahwah,                    reinforcement and non reinforcement. In Black, A. H. and
 NJ:Erlbaum.                                                          Prokasy, W. F. (Eds.). Classical Conditioning II: Current
Anderson, J. R. & Lebiere, C. (1998). Atomic components of            Research and Theory. New York, Appleton Century
 thought. Mahwah, NJ:Erlbaum.                                         Crofts.
Anderson, J. R., Qin, Y., Sohn, M.-H., Stenger, V. A.,               Schultz, W., Apicella, P., Ljungberg, T. (1993). Responses
 Carter, C. S. (2003). An information-processing model of             of monkey dopamine neurons to reward and conditioned
 the BOLD response in symbol manipulation tasks.                      stimuli during successive steps of learning a delayed
 Psychonomic Bulletin & Review, 10 (2), 241-261.                      response task. Journal of neuroscience, 13, 900-913.
Busemeyer, J. R. & Myung, I. J. (1992). An adaptive                  Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural
 approach to human decision making: Learning theory,                  substrate of prediction and reward., Science, 275, 1593.
 decision theory, and human performance. Journal of                  Schultz, W., Romo, R., Ljungberg, T., Mirenowica, J.,
 Experimental Psychology: General, 121 (2), 177-194.                  Hollerman, J. R., & Dickinson, A. (1995). Reward-related
Carter, C. S., Braver, T. S., Barch, D. M., Botvinick, M. M.,         signals carried by dopamine neurons. In J. C. Houk, J. L.
 Noll, D., & Cohen, J. D. (1998). Anterior cingulate cortex,          Davis, & D. G. Beiser (Eds.), Models of information
 error detection, and the online monitoring of performance.           processing in the basal ganglia (pp. 233-248). Cambridge,
 Science, 280, 747-749.                                               MA:MIT Press.
Friedman, M. P., Burke, C. J., Cole, M., Keller, L.,                 Sutton, R. S. & Barto, A. G. (1981). Toward a modern
 Millward, R. B., & Estes, W. K. (1964). Two-choice                   theory of adaptive networks: expectation and prediction.
 behavior under extended training with shifting probabilities         Psychological Review, 88 (2), 135-170.
 of reinforcement. In R. C. Atkinson (Ed.), Studies in               Sutton, R. S., & Barto, A. G. (1998). Reinforcement
 mathematical psychology (pp. 250-316). Stanford, CA:                 learning: An introduction. Cambridge, MA: MIT Press.
 Stanford University Press.
Holroyd, C. B. & Coles, M. G. H. (2002). The neural basis
 of human error processing: reinforcement learning,
 dopamine, and the error-related negativity. Psychological
 Review, 109 (4), 679-709.
                                                                 421

