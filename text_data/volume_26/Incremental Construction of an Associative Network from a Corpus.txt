UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Incremental Construction of an Associative Network from a Corpus
Permalink
https://escholarship.org/uc/item/3k98b25s
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Lemaire, Benoit
Denhiere, Guy
Publication Date
2004-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                      Incremental Construction of an Associative Network from a Corpus
                                     Benoît Lemaire (Benoit.Lemaire@upmf­grenoble.fr)
                                                L.S.E., University of Grenoble 2, BP 47
                                                    38040 Grenoble Cedex 9, France
                                           Guy Denhière (denhiere@up.univ­mrs.fr)
                                                L.P.C & C.N.R.S. Université de Provence
                                                      Case 66, 3 place Victor Hugo
                                                     13331 Marseille Cedex, France
                              Abstract                                6. compositionality: the way the meaning of a text can be
                                                                          inferred from the meaning of its words.
  This paper presents a computational model of the incremental
  construction of an associative network from a corpus. It is         After a description of existing models, we will discuss these
  aimed at modeling the development of the human semantic             features, present our model and describe an experiment that
  memory. It is not based on a vector representation, which           aims at comparing our model to human data.
  does not well reproduce the asymmetrical property of word
  similarity, but rather on a network representation. Compared                Existing computational models of the
  to Latent Semantic Analysis, it is incremental which is                  construction of semantic representations
  cognitively more plausible. It is also an attempt to take into
                                                                      Latent Semantic Analysis
  account higher­order co­occurrences in the construction of
  word similarities. This model was compared to children              LSA (Landauer, 2002) takes as input a corpus of free texts.
  association norms. A good correlation as well as a similar          The unit of context is the paragraph. The analysis of the
  gradient of similarity were found.                                  occurrences of each word within all paragraphs leads to a
                                                                      representation of the meaning of words as vectors, which is
                         Introduction                                 well suited for drawing semantic comparisons between
A computational model of the human semantic memory                    words. The underlying mechanism (singular value
may be valuable for its ability to mimic the human semantic           decomposition of the word­paragraph occurrence matrix)
representations, but also for its ability to mimic the                implicitly takes into account higher­order co­occurrences
construction of these representations over a long period of           (Kontostathis & Pottenger, 2002). Compositionality in this
time. Not all models possess both features. For instance,             model is straightforward: the meaning of a text is a linear
symbolic formalisms like semantic networks had proven to              combination of the meaning of its words. There is however
be interesting for representing human knowledge but they              no way of updating the semantic space with a new unit of
do not tell us how human beings build such representations            context without redoing the whole process. LSA'ssemantic
over their life. Several computational models of both the             representations have been largely tested in the literature
representation and construction of the human semantic                 (Foltz, 1996 ; Wolfe et al., 1998). This model can account
memory have been proposed in the recent years. Some of                for some mechanisms of the construction of knowledge
them are based on a general common mechanism that rely                (Landauer & Dumais, 1997).
on a huge input, composed of examples of associations
between words. The statistical analysis of the occurrences of
                                                                      Hyperspace Analogue to Language
each word within well­defined units of context leads to a             HAL (Burgess, 1998) is also a model of the semantic
computational representation of association links between             memory. It is similar to LSA except that (1) it does not take
words. The representation of word meanings per se is not of           into account higher­order co­occurrences since vectors are
significant interest, it is rather their association links which      just direct co­occurrence vectors; (2) the unit of context is a
combined will form a model of the long­term semantic                  sliding window of a few words which takes into account the
memory.                                                               lexical distance between words and (3) updating the
  These models can be distinguished along six features:               semantic space with a new paragraph can be done easily.
1. the kind of input they are based on (either a corpus or            Sparse Random Context Representation
    word association norms);
2. the knowledge representation formalism (either vector­             SRCR (Sahlgren, 2001, 2002) is also based on the use of a
    based or network­based);                                          sliding window applied to a large corpus. Words have an
3. the way a new context is added to the long­term                    initial random vector representation (1,800 dimensions),
    semantic memory (incrementally or not);                           which is updated with the vectors of the co­occurring
4. the unit of context in which co­occurrence information             words: they are all added to the current word, but with a
    is considered (either a paragraph or a sliding window);           multiplying factor which depends on their distance to the
5. the use or not of higher­order co­occurrences;                     current word within the window. The way the initial
                                                                  825

representation is computed is important: all 1,800 values are       and its associates should exist in a plausible model of the
set to 0 except eight which are randomly selected and set to        semantic memory.
1. This method is intrinsically incremental. It has better             Another problem with the vector representation is that
results than LSA on the famous TOEFL test. However, it              similarity is symmetrical: similarity(A,B)=similarity(B,A).
does not take into account higher­order co­occurrences.             This is not coherent with psycholinguistic findings showing
                                                                    that semantic similarity is not a symmetrical relation
Word Association Space                                              (Tversky, 1977). For instance, bird is a very close neighbor
WAS (Steyvers, Shiffrin & Nelson, in press) is not based on         of swallow, but the opposite is not so obvious.
a corpus but on association norms providing associates for             A network of words with simple numerical oriented links
5,000 words. The authors applied scaling methods to these           between nodes (what is called an oriented graph in graph
data in order to assign a high­dimensional representation to        theory) would be better for that purpose. Numerical links
each word. In particular, they relied on singular value             would represent semantic similarities. Such a basic network
decomposition, the mathematical procedure also used by              would offer a direct connection between a word and its
LSA. The idea is similar to LSA: words that appear within           neighbors and represent differently similarity(A,B) and
similar contexts (i.e. words with similar associative               similarity(B,A).
relationships) are placed in similar regions in the space.
                                                                    Memory updating
WAS appeared to be a better predictor of memory
performance than LSA.                                               A model of the construction of the semantic memory should
                                                                    describe the way processing a new piece of written data
                          Features                                  affects the representation of the long­term memory. Some
                                                                    models like Latent Semantic Analysis are not incremental,
We will now discuss the six previous features in order to           which means that the whole process needs to be restarted in
sketch out a model of construction and representation of the        order to take into account a new context. Actually, a new
long­term semantic memory that would attempt to                     paragraph can easily be represented by a vector in this
overcome existing limits.                                           model, by a simple linear combination of its words, but this
Input                                                               operation does not affect at all the semantic space.
                                                                    Incremental models are much more cognitively plausible:
A corpus of free texts as input is cognitively more plausible       processing new texts should modify, even slightly, the
than association norms or even a sublanguage of a few               semantic memory.
propositions (Frank et al. 2003). As humans, we do not
obviously construct our semantic representations solely             Unit of context
from written data (Glenberg & Robertson, 2000), but there           The semantic relations between words are constructed from
is currently no formalism able to model all perceptual data         the occurrences of words within contexts. The size of such
such that they can be processed by a computational model.           contexts plays an important role. Psychological experiments
In addition, written data, although it is not perfect, seems to     as well as computer simulations (Burgess, 1998) tend to
cover a large part of our semantic representations                  consider that a context composed of a few words before and
(Landauer, 2002).                                                   after the current word is reasonable. However,
Representation                                                      computational constraints have led some models to consider
                                                                    a whole paragraph as a unit of context, which is probably a
Most models are based on a vector representation of word            too large unit. Latent Semantic Analysis is such a model.
meaning. Dimensions of the semantic space can be the                The use of a sliding window allows models like HAL or
result of a statistical analysis which keeps hundreds of            SRCR to take into account the distance between words
dimensions like in LSA or SCRC (they are therefore                  within the window, whereas approaches based on
unlabelled), the most variant words as in HAL, the most             paragraphs deal with bags of words.
frequent ones (Levy & Bullinaria, 2001) or even a
predefined subset of words, either taken from a thesaurus           Higher­order co­occurrences
(Prince & Lafourcade, 2002) or selected as being the most           It has been shown that higher­order co­occurrences play an
reliable across various sub­corpora (Lowe & McDonald,               important role (Kontostathis & Pottenger, 2002) in the
2000).                                                              latent structure of word usage. Two words should be
   One major interest of the vector representation is that it       considered associated although they never co­occur in
offers a simple way to measure the similarity between               context units, provided that they occur within similar
words. The angle between the corresponding vectors or its           contexts. A is said to be a second­order co­occurrence of B
cosine are generally used.                                          if it co­occurs with C which also co­occurs with B. If C
   One drawback of the vector representation however is the         were a second­order co­occurrence of B, A would be
difficulty to determine the words that are similar to a given       considered as a third­order co­occurrence of B, etc.
word or, say differently, the words that are activated in              By means of the singular value decomposition procedure,
memory. It requires the scanning of all vectors in order to         LSA semantic similarity indeed involves higher­order co­
find the closest ones, which is both computationally and            occurrences (Lemaire & Denhière, submitted). Other
cognitively not satisfactory. A direct link between a word          approaches such as SCRC or HAL do not.
                                                                826

                                              Table 1: Features of different models
                   Input         Representation   Memory updating        Unit of context     higher­order co­ Compositionality
                                                                                                 occurrences
LSA               corpus            vectors        not incremental          paragraph                  yes             easy
HAL               corpus            vectors          incremental         sliding window                 no             easy
SCRC              corpus            vectors          incremental         sliding window                 no             easy
WAIS        association norms       vectors        not incremental             N/A                      no             easy
ICAN              corpus            network          incremental         sliding window                yes             hard
Compositionality                                                  respect to its preceding and following contexts. The size of
                                                                  the window can be modified. For the sake of simplicity, we
Compositionality is the ability of a representation to go
                                                                  will not use the third­order co­occurrence effect. The
from words to texts. The vector representation is very
                                                                  algorithm is the following:
convenient for that purpose because the linear combination
of vectors still produces a vector, which means that the
                                                                  For each word W, its preceding context C1..Ck and its
same representation is used for both words and texts. This
                                                                  following context Ck+1..C2k (the sliding window therefore
might be a reason why vector representations are so
                                                                  being [C1 C2 ... Ck W Ck+1 Ck+2... C2k]):
popular. On the contrary, symbolic representations of word
meaning like semantic networks do not offer such a feature:          Direct co­occurrence effect: reinforce the link W­Ci (if
it is not straightforward to build the representation of a           this link does not exist, create it with a weight of 0.5,
group of words from the individual representations of                otherwise increase the weight p by setting it to p+(1­p)/2;
words, especially if the representation is rich, for instance
                                                                     Second­order co­occurrence effect: let p be the weight
with labelled links.
                                                                     of the W­Ci link. For each M linked to Ci with weight m,
Summary                                                              reinforce the link W­M (if such a link does not exist,
                                                                     create it with a weigth of p.m, otherwise, increases the
Table 1 describes some of the existing models along the              weight q by setting it to q+A(1­q)(p.m), A being a
previous six features. We present ICAN, our proposal, at             parameter;
then end of the next section.
                                                                     Occurrence without co­occurrence effect: reduce the
                            ICAN                                     links between W and its other neighbors (if the weights
                                                                     were p, set them to a fraction of p, e.g., 0.9p). If some of
Basic mechanisms                                                     them fall under a threshold (e.g., .1), then remove these
                                                                     links.
Like others, this model takes as input a corpus of free texts
and produces a computational representation of word               Example
meanings. This model is based on a network representation,
which we believe is more accurate in modeling the process         As an example, consider the following association network,
of semantic activation in memory. The idea is to associate to     which is the result of processing several texts:
each word a set of neighbors as well as their association
weights in [0..1], exactly as in rough semantic network. The                             electric                 plug
model is incremental which means that the set of connected                                .7 .5                  .4
words for each word evolves while processing new texts. In                              .7 cable      .6   connector
                                                                                  wire             .4
particular, new words can be added according to the co­                         .2 .4  .3       .5
occurrence information and other words can be ruled out if                                            network
their association strengths with the current word become too                  rope
low.
   Links between words are updated by taking into account         The new text being analyzed is:
the results of a previous simulation on 13,637 paragraphs of         ... if you have such a device, connect the cable to the
a corpus (Lemaire & Denhière, submitted), which showed               network connector then switch...
that:
                                                                  We now describe how this text will modify the association
–   co­occurrence of W1 and W2 tends to strongly increase         network, according to the previous rules. Suppose a window
    the W1­W2 similarity;                                         of size 5 (2 preceding words, 1 current word, 2 following
– occurrence of W1 without W2 or W2 without W1 tends to           words). Since functional words are not taken into account,
    decrease the W1­W2 similarity;                                the current window is then [device, connect, cable, network,
– second and third­order co­occurrence of W1 and W2               connector], cable being the current word. The direct co­
    tends to slightly increase the W1­W2 similarity.              occurrence effect leads to reinforce the links between cable
In our model, a sliding window is used as a unit of context.      and the four co­occurring words. Two of them are new
Therefore, each word of the corpus is considered with             links, while others are existing links whose weights are
                                                                  simply increased. The network becomes:
                                                              827

                             electric                 plug               in these norms is the percentage of subjects who provided
                                          connect
              device                                                     the associate. For instance, the six associates to abeille(bee)
                          .5 .7 .5 .5                .4
                                                                         are:
                            .7 cable .8 connector
                    wire               .7
                 .2 .4
                                                                         –   miel(honey): 19%
                           .3       .5
                                          network                        –   insecte(insect): 14%
               rope                                                      –   ruche(hive): 9%
The second­order co­occurrence effect reinforces the links               –   animal(animal): 1%
between cable and all words connected to one of its four co­             –   oiseau(bird): 1%
occurring word. In this small example, this is only the case             –   vole(fly): 1%
for the word plug. Finally, the occurrence without co­
                                                                         Actually, 16 words were not part of the corpus. Only 1184
occurrence effect leads to a decrease of the links between
                                                                         pairs of words were therefore used.
cable and its other neighbors. The network is then:
                                                                         We then compared these values to the similarity values
                                                                         provided by the model. We had two hypotheses. First, the
                            electric connect         plug                model should distinguish betwen the three first associates
             device
                         .5 .7 .4 .5          .3    .4                   and the last ones and there should be a gradient of similarity
                           .6 cable .8 connector                         from the first one to the last ones. Second, there should be a
                   wire               .7
                .2 .4     .2
                                                                         good correlation between human data and model data.
                                   .5 network                              Several parameters have to be set in the model. The best
              rope
                                                                         correlation with the human data was obtained with the
                                                                         following parameters (see the algorithm presented earlier):
The next current word is network, the window is [connect,
cable, network, connector, switch] and the process repeats               –   window size = 11 (5 preceding and 5 following words);
again.                                                                   –   co­occurrence effect: p ­­> p+(1­p)/2;
                                                                         –   2nd­order co­occurrence effect : p ­­> q+.02(1­q)(p.m);
Measure of similarity                                                    –   occurrence without co­occurrence effect : p ­­> .9p.
Similarity between words W1 and W2 is the combination                    Using these parameters, the average similarity values
(i.e. the product) of the links of the shortest path between             between stem words and associates, as well as the children
W1 and W2. If W2 is connected to W1, it is just the weight of            data, are the following:
the link; if W1 is connected to Z which is connected to W2, it
is the combination of the two weights. If W2 does not
                                                                                               1st        2nd        3rd        Last
belong to the neighbors of W1'            s neighbors it is probably
                                                                                          associates associates associates associates
sufficient to set the semantic similarity to 0. Since the graph
is oriented (the link weight between A and B might be                     ICAN               .415        .269      .236         .098
different from the link weight between B and A), this way
                                                                          Norms              30.5        13.5       8.2           1
of measuring the similarity mimics the asymmetrical
property of the human judgment of similarity better than the
cosine between vectors.                                                  All model values are highly significantly different, except
                                                                         for the 2nd and 3rd associates which differ only at the 10%
                                 Tests                                   level. Our model reproduces quite well the human gradient
                                                                         of association.
Comparison to association norms                                            We also calculated the coefficient of correlation between
In order to test this model, we compared the association                 human data and model data. We found an interesting
links it provides to human association norms. The corpus                 significant correlation: r(1184)=.50.
we relied on is a 3.2 million word French child corpus                     The exact same test from the same corpus was also
composed of texts that are supposed to reproduce the kind                applied to Latent Semantic Analysis. Results are the
of texts children are exposed to: stories and tales for                  following:
children (~1,6 million words), children productions
(~800,000 words), reading textbooks (~400,000 words) and                                    1st         2nd         3rd        Last
children encyclopedia (~400,000 words). All functional                                 associates associates associates associates
words were ruled out. Words whose frequency was less than                 LSA              .26          .23        .19           .11
3 were not taken into account. The program is written in C,
it is available on demand. Processing the whole corpus takes
a few hours on a standard computer, depending on the                     Similarities between the stem word and the first associates
window size.                                                             appear stronger in the ICAN model. LSA'scorrelation with
   Once the association network was built, we measured the               human data is r(1184)=.39, which is worse than our
similarity between 200 words and 6 of their associates (the              correlation.
first three and the last three), as provided by the de la Haye
(2003) norms for 9 year­old children. The association value
                                                                     828

Similarity as direct co­occurrence                                be expected. A final reason could be that higher­order co­
One can wonder whether the similarity could be mainly due         occurrence does not play any role. But, how then could we
to the direct co­occurrence effect. Similarity between words      explain the high similarity between words that almost never
is indeed often operationalized in psycholinguistic               co­occur? More experiments and simulations need to be
researches by their frequency of co­occurrence in huge            carried out to investigate this issue.
corpus. Experiments have indeed revealed the correlation
between both factors (Spence & Owens, 1990). However,             Window size
this shortcut is questionable. In particular, there are words     We also modified the model in order to shed light on the
that are strongly associated although they never co­occur.        role of the window size. Results are as follows:
Burgess & Lund (1998) mentioned the two words road and
street that almost never cooccur in their huge corpus                     Window size             Correlation with human data
although they are almost synonyms. In a 24­million words
French corpus from the daily newspaper Le Monde in 1999,                   3 (1+1+1)                          .34
we found 131 occurrences of internet, 94 occurrences of                    5 (2+1+2)                          .38
web, but no co­occurrences at all. However, both words are
strongly associated. Edmonds (1997) showed that selecting                  7 (3+1+3)                          .44
the best typical synonym requires that at least second­order               9 (4+1+4)                          .48
co­occurrence is taken into account. There is clearly a
debate: is the frequency of co­occurrence a good model of                 11 (5+1+5)                          .50
word similarity?                                                          13 (6+1+6)                          .49
   In order to test that hypothesis, we modified our model so
that only direct co­occurrences are taken into account: the               15 (7+1+7)                          .47
2nd order co­occurrence effect as well as the occurrence
without co­occurrence effect were inhibited. Results are the      We found that the best window size is 11 (5 preceding
following:                                                        words and 5 following words). This is in agreement with the
                                                                  literature: Burgess (1998) as well as Lowe and McDonald
                                   1st    2nd    3rd    Last      (2000) use a window of size 10, Levy and Bullinaria (1998)
                                assoc. assoc. assoc. assoc.       found best performance for a window size from 8 to 14,
                                                                  according to the similarity measure they relied on.
          ICAN                   .903    .781   .731    .439
 (only direct co­occurrences)                                                              Conclusion
                                                                  This model could be improved in many ways. However,
The gradient of similarity is still there but the correlation     preliminary results are encouraging: the model produces
with human data is worse (r(1184)=.39). This is in                better results than the outstanding Latent Semantic Analysis
accordance with our previous findings (Lemaire &                  model on a word association test. In addition, it adresses
Denhière, submitted) which show that the frequency of co­         two major LSA drawbacks. The first one has to do with the
occurrence tends to overestimate semantic similarity.             representation itself: the fact that LSA'   s associations are
                                                                  symmetrical is not satisfactory. A network representation
Effect of second­order co­occurrence                              seems better for that purpose than a vector representation.
Another test consisted in measuring the effect of second­         The second limitation of LSA concerns the way the
order co­occurrences. This time, we only inhibited this           semantic space is built. LSA is not incremental: adding a
effect in order to see whether the loss would be significant.     new piece of text requires that the whole process is run
Results are presented in the next table:                          again. Like HAL or SCRC, ICAN has the advantage of
                                                                  being incremental.
                                   1st    2nd    3rd    Last         ICAN'  s main limitation is related to compositionality. The
                                assoc. assoc. assoc. assoc.       construction of a text's representation is not straightforward,
                                                                  given the representation of its words. Representing every
      ICAN (no 2nd­order         .371    .225   .191    .056      text as a simple function of its words, and in the same
        co­occurrences)                                           formalism, as in the vector representation, is very
                                                                  convenient since text comparisons are then easy to perform.
Correlation with human data was not significantly different       Compared to other approaches, LSA is for instance very
from the full model. It only decreased from .50 to .48. This      good at simulating the human judgment of text comparisons
means that second­order co­occurrences do not seem to             (Foltz, 1996). However, the cognitive plausibility of such a
have much effect in this simulation. One reason might be          representation can be questioned. Do we really need the
due to the mathematical formula we used to model higher­          exact same representation for words and texts? Is it
order co­occurrences. It might not be the right one. Another      cognitively reasonable to go directly without any effort
reason could be that we only implemented second­order co­         from words to texts? Why having two ways of processing
occurrence effects. Third and higher­order co­occurrence          texts: one which would be computationally costly (singular
effects might play a much more significant role than could
                                                              829

value decomposition in LSA) and another one very quick              (Ed.), The psychology of Learning and Motivation, 41,
(adding its words)?                                                 43­84.
A solution could be to process each new text by the               Landauer, T. K. & Dumais, S. T. (1997). A solution to
mechanism described in this paper: a text would then be             Plato's problem : the Latent Semantic Analysis theory of
represented by a subgraph, that is by a small subset of the         acquisition, induction and representation of knowledge.
huge semantic network, composed of the text words, their            Psychological Review, 104, 211­240.
neighbors and their links. An information reduction               Lemaire, B. & Denhière, G. (submitted). Effects of higher­
mechanism like the integration step of the construction­            order co­occurrences on semantic similarity of words.
integration model (Kintsch, 1998) could then be used to           Levy, J.P., Bullinaria, J.A. & Patel, M. (1998). Explorations
condense this subgraph in order to retain the main                  in the derivation of semantic representations from word
information. This smaller subgraph would constitute the text        co­occurrence statistics. South Pacific Journal of
representation. This way, there would be a single                   Psychology, 10, 99­111.
mechanism used to process a text, construct its                   Levy, J.P., Bullinaria, J.A. (2001). Learning lexical
representation and update the long­term semantic memory.            properties from word usage patterns: which context words
However, much work remains to be done in that direction.            should be used? In R. French & J.P. Sougne (Eds)
   Once a corpus was processed, it would be interesting to          Connectionist Models of Learning, Development and
study the resulting network structure. In particular, this          Evolution: Proceedings of the Sixth Neural Computation
structure could be compared to existing semantic networks,          and Psychology Workshop, 273­282. London:Springer.
in terms of connectivity or average path­lengths between          Lowe, W. & McDonald, S. (2000). The direct route:
words, much like Steyvers & Tenenbaum (submitted) did               mediated priming in semantic space. In Gernsbacher, M.
recently.                                                           A. and Derry, S. D., editors, Proceedings of the 22nd
                                                                    Annual Meeting of the Cognitive Science Society, 675­
Acknowledgements                                                    680, New Jersey. Lawrence Erlbaum Associates.
We would like to thank Maryse Bianco, Philippe Dessus             Prince, V. & Lafourcade, M. (2003). Mixing semantic
and Sonia Mandin for their valuable comments on this                networks and conceptual vectors: the case of
model, as well as Emmanuelle Billier, Valérie Dupont and            hyperonymy. In Proc. of ICCI­2003 (2nd IEEE
Graham Rickson for the proofreading .                               International Conference on Cognitive Informatics),
                                                                    South Bank University, London, UK, August 18 ­ 20,
References                                                          121­128.
                                                                  Sahlgren, M. (2001). Vector­based semantic analysis:
Burgess, C. (1998). From simple associations to the
                                                                    representing word meaning based on random labels.
   building blocks of language: modeling meaning in
                                                                    Semantic Knowledge Acquisition and Categorisation
   memory with the HAL model. Behavior Research
                                                                    Workshop at ESSLLI '    01, Helsinki, Finland.
   Methods, Instruments, & Computers, 30, 188­198.
                                                                  Sahlgren, M. (2002). Towards a flexible model of word
Burgess, C., Livesay, K & Lund, K. (1998). Explorations in
                                                                    meaning. AAAI Spring Symposium 2002. March 25­27,
   context space: words, sentences, discourse. Discourse
                                                                    Stanford University, Palo Alto.
   Processes, 25, 211­257.
                                                                  Spence, D.P. & Owens K.C. (1990). Lexical co­occurrence
de la Haye, F. (2003). Normes d'as   sociations verbales chez
                                                                    and association strength. Journal of Psycholinguistic
   des enfants de 9, 10 et 11 ans et des adultes. L'Ann    ée
                                                                    Research 19, 317­330.
   Psychologique, 103, 109­130.
                                                                  Steyvers, M., Shiffrin R.M., & Nelson, D.L. (in press).
Edmonds, P. (1997). Choosing the word most typical in
                                                                    Word Association Spaces for predicting semantic
   context using a lexical co­occurrence network. Meeting of
                                                                    similarity effects in episodic memory. In A. Healy (Ed.),
   the Association for Computational Linguistics, 507­509.
                                                                    Cognitive Psychology and its Applications: Festschrift in
Foltz, P. W. (1996). Latent Semantic Analysis for text­
                                                                    Honor of Lyle Bourne, Walter Kintsch, and Thomas
   based research. Behavior Research Methods, Instruments
                                                                    Landauer. Washington DC: American Psychological
   and Computers, 28­2, 197­202.
                                                                    Association.
Frank, S.L., Koppen, M., Noordman, L.G.M. & Vonk, W.
                                                                  Steyvers, M., & Tenenbaum, J. (submitted). Graph theoretic
   (2003). Modeling knowledge­based inferences in story
                                                                    analyses of semantic networks: small worlds in semantic
   comprehension. Cognitive Science 27(6), 875­910.
                                                                    networks.
Glenberg, A. M. & Robertson, D. A., (2000). Grounding
                                                                  Tversky, A. (1977). Features of similarity. Psychological
   symbols and computing meaning: a supplement to
                                                                    Review, 84(4), 327­352.
   Glenberg & Robertson. Journal of Memory and
                                                                  Wolfe, M. B. W., Schreiner, M. E., Rehder, B., Laham, D.,
   Language, 43, 379­401.
                                                                    Foltz, P. W., Kintsch & W., Landauer, T. K. (1998).
Kintsch, W. (1998). Comprehension: A Paradigm for
                                                                    Learning from text: Matching readers and texts by Latent
   Cognition. Cambridge University Press.
                                                                    Semantic Analysis. Discourse Processes, 25, 309­336.
Kontostathis, A. & Pottenger, W.M. (2002). Detecting
   patterns in the LSI term­term matrix. Workshop on the
   Foundation of Data Mining and Discovery, IEEE
   International Conference on Data Mining.
Landauer T.K. (2002). On the computational basis of
   learning and cognition: Arguments from LSA. In N. Ross
                                                              830

