UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
You Can’t Play Straight TRACS and Win: Memory Updates in a Dynamic Task Environment
Permalink
https://escholarship.org/uc/item/2b06f1g7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Neth, Hansj org
Sims, Chris R.
Veksler, Vladislav D.
et al.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                   You Can’t Play Straight TRACS and Win:
                            Memory Updates in a Dynamic Task Environment
                 Hansjörg Neth              Chris R. Sims            Vladislav D. Veksler               Wayne D. Gray
                  (nethh@rpi.edu)            (simsc@rpi.edu)            (vekslv@rpi.edu)                 (grayw@rpi.edu)
                                                       Cognitive Science Department
                                                      Rensselaer Polytechnic Institute
                              Abstract
                                                                        Table 1: Baseline distribution of cards in the deck. The back
   To investigate people’s ability to update memory in a dy-            of every card shows only its shape, whereas the front shows
   namic task environment we use the experimental card game             both its shape and color.
   TRACS™ (Burns, 2001). In many card games card count-
   ing is a component of optimal performance. However, for                Shape:               N                     N            
   TRACS, Burns (2002a) reported that players exhibited a base-
   line bias: rather than basing their choices on the actual num-         Color:              red     red    red      blue   blue   blue
   ber of cards remaining in the deck, they chose cards based on          Initial deck:         6      4       2       2      4      6
   the initial composition of the deck. Both a task analysis and
   computer simulation show that a perfectly executed memory
   update strategy has minimal value in the original game, sug-
   gesting that a baseline strategy is a rational adaptation to the
   demands of the original game. We then redesign the game              bilities of the actor (Simon, 1990). To capture functional
   to maximize the difference in performance between baseline           relationships of complex tasks while abstracting away from
   and update strategies. An empirical study with the new game          domain specific details we advocate the use of synthetic task
   shows that players perform much better than could be achieved
   by a baseline strategy. Hence, we conclude that people will          environments, or microworlds (Gray, 2002). If the properties
   adopt a memory update strategy when the benefits outweigh            of the synthetic task environments are known and manipula-
   the costs.                                                           ble, the scope and limits of human rationality can be assessed.
                                                                        Moreover, the effects of environmental changes are tractable.
                          Introduction                                  Straight TRACS
Optimal performance in dynamic environments requires that               TRACS™ is a ‘Tool for Research on Adaptive Cognitive
we base our decisions on the current state of the world, not on         Strategies,’ designed and developed by Kevin Burns (2001,
past states. Radar operators must act on the basis of continu-          2004). Being both entertaining card game and experimental
ously changing variables such as plane altitude and heading.            research tool, TRACS provides a microworld which promises
Drivers constantly need to monitor the current speed limit,             to bridge the gap between mathematical rigor and real-world
posted road signs and the traffic behind and in front of them.          relevance. We will limit our discussion to Straight TRACS,
Failure to mentally update these types of information can lead          which is the simplest version of an entire family of games.1
to dangerous decisions and catastrophic behavior. Even our                  TRACS is played with a deck of 24 cards. The back of each
chances to win at card games like Blackjack or Bridge are               card shows one of three shapes—circle, triangle, or square—
closely tied to our ability to count cards and update memory.           filled in with black. The front of each card shows both its
   Previous research suggests that human ability to monitor             shape, and one of two colors (red or blue). Table 1 shows
and adjust to change is limited and dependent on various fac-           the initial deck distribution for each of the six possible card
tors. Yntema (1963) found that people are better at tracking a          types. This baseline information is always available to the
small number of variables with a large range of values each,            player. As hands are played the number of cards remaining
than a large number of variables with a small set of possi-             in the deck decreases, and the odds for each shape change
ble values each. In addition, reducing the frequency of up-             accordingly.
date can improve performance. Other manipulations, such                     At the start of a game, three cards are dealt in a row. The
as increased predictability of a sequence, provide little or no         middle card is dealt face up (showing both its shape and
advantage in remembering the current state of the environ-              color), while the left and right cards are dealt face down,
ment. Venturino (1997) distinguished the memory capacity                showing their shape not their color. The task for the player
for static information from that for dynamically changing in-           is to choose the card, either left or right, most likely to match
formation and showed that the latter is highly limited, par-            the color of the middle card. The chosen card is then turned
ticularly when the to-be-remembered attributes are similar.             over, revealing its color. If the chosen card matches the color
Hess, Detweiler and Ellis (1999) added that update perfor-              of the middle card, a hit is credited to the player’s score. A
mance is improved when spatial invariants constrain where               mismatch is scored as a miss. The two face up cards (the mid-
different data values are presented on a visual display.                dle and the chosen card) are then removed from the game. On
   In general, human rational behavior is constrained by the
                                                                            1
structure of task environments and the computational capa-                    Online versions are available at www.tracsgame.com.
                                                                    1017

the next turn, the unchosen card is flipped over and becomes                                                   New turn:
the new middle card, and two new cards are dealt face down                     Initial deck
                                                                                 of cards
to the left and right. A game lasts 11 turns, at which point
there are not enough cards in the deck to deal another hand.
A player’s objective in TRACS is to maximize the number of                                      Update 1
hits.                                                                                                  1. Memory retrieval task:
   As a probe of the player’s assessment of odds at each turn,                                            Recall frequencies
Burns (2002a, 2002b) added a confidence meter to the task.
                                                                              Memory:
On each turn, players were presented with a red to blue color                Initial deck
gradient for each of the two face-down cards. Prior to choos-              − cards played                 2. Conversion task:
                                                                                                         Compute probabilities
ing a card, the participants used the gradient to indicate the
likelihood of each candidate card to be red or blue. In an-
other condition, Burns used a scale of nine buttons rather than                                            3. Decision task:
a continuous spectrum. For consistency reasons all gradient                                                  Choice of card
estimates were rounded to the nearest button, corresponding
to the nearest 12.5%.                                                   Cognition           Update 2
   Burns (2002a) characterized players’ likelihood estimates
as exhibiting a baseline bias; i.e., their judgments of odds                                                   Outcome:
deviate systematically from the actual odds in the direction
of the initial card distribution. There are six types of color–
shape combinations. Burns (2002b) reports that players could
                                                                   Figure 1: Subtasks and memory updates required on each turn
only monitor 2–4 types of cards with reasonable reliability.
He concludes that the dual tasks of concurrently counting and      of Straight TRACS.
normalizing numbers ‘are naturally hard’ and that continu-
ously updating odds exceeded the cognitive capacity of the
‘unaided mind’ (Burns, 2002a, p. 159).                             termine the likelihood of a red triangle, a player has to divide
                                                                   the number of red triangles currently left in the deck by the
   In the following sections, we will challenge this claim both
                                                                   sum of red and blue triangles left in the deck. As people
theoretically and empirically. To preview our conclusions,
                                                                   are notoriously bad at dealing with probability information
we find that subtle constraints in the task environment can
                                                                   (see Gigerenzer, 2000, and Koehler, 1996, for reviews) it is
have profound effects on the strategy adopted by participants.
                                                                   conceivable that this translation process incurs a loss of accu-
The reported baseline bias is revealed as both rational and
                                                                   racy. If so, merely asking for likelihood estimates confounds
adaptive when considered in light of a cost-benefit analysis of
                                                                   memory updates with probability judgments and may under-
the environment. We then demonstrate that players will adopt
                                                                   estimate players’ true memory capacity. As a third subtask, a
a more effortful memory strategy if the cost-benefit structure
                                                                   player needs to integrate all estimates and decide which can-
of the environment rewards this.
                                                                   didate card is more likely to score a hit on the current trial.
                                                                      In addition to these three subtasks, each turn requires two
                     Tracking TRACS                                distinct updates of memory. The first update is necessary as
Given the original finding that players find it challenging to     soon as the middle card is revealed. If the middle card hap-
succeed at TRACS, a natural starting point for our investiga-      pens to be a red triangle, the player needs to realize that there
tion is a task analysis. What specifically makes this game so      now is one less red triangle left in the deck. The second up-
difficult to play?                                                 date ought to occur at the end of a turn when the chosen card
                                                                   is revealed. This second update is critical, as at this point in
Task Analysis                                                      the game, players may be distracted by focusing exclusively
In describing TRACS as a game of ‘confidence and conse-            on the correctness of their choice and ignoring the additional
quence’ Burns (2001, 2002b) distinguishes two subtasks of          information revealed.
diagnosis and decision. On each turn, a player first provides         This task analysis reveals both the complexity and sim-
an odds judgment for each face-down card and then chooses          plicity of TRACS. On one hand, multiple subtasks and mem-
one on the basis of these estimates.                               ory update requirements make the game quite challenging.
   Extending Burns’ analysis, we suggest that each turn in-        Even if frequency information on card types was readily
volves a minimum of three distinct cognitive tasks: a mem-         available, the conversions into probabilities, comparisons be-
ory retrieval task, an odds conversion task, and a decision        tween odds, and selection of cards introduce potential sources
task (see Figure 1). The first subtask on each turn consists       of error. On the other hand, remembering and updating a list
in remembering how many cards of each candidate shape and          of six numbers (representing the current frequency of each
color remain in the deck. As the initial card distribution is      card type) does not in itself seem beyond the capacity of hu-
provided in terms of frequencies and players encounter card        man memory.
instances through a process of natural sampling, we assume
that this retrieval is framed in terms of natural frequencies.     The Impact of Memory
Secondly, the retrieved frequencies need to be converted into      At first glance, it seems that TRACS is a ‘memory game’
odds, which is a non-trivial process involving Bayes’ rule for     (Burns, 2001, 2002a) in which players can succeed only by
natural frequencies (Gigerenzer, 2000). For example, to de-        remembering which cards have left the deck. However, our
                                                               1018

experience playing TRACS casts some doubts on the impor-                100%
tance of memory. Due to the random card selection process a                                                             Random
typical game contains many knowledge-indeterminate turns.                                                               Baseline
                                                                             90%                                        Update
For example, whenever both face-down cards show the same                                                                Omniscient
shape, a player has no choice but to guess. Likewise, both
face-down cards frequently have the same color, so that the                  80%
player scores a hit or miss regardless of knowledge or choice.
Even when the cards differ in shape, color, and odds, it is pos-      Hits   70%
sible that selecting the card with higher actual odds results in
a miss, whereas choosing the ‘wrong’ card scores a hit.                      60%
   These concerns raise questions about whether memory
really matters. To what extent can poor performance be
                                                                             50%
blamed on failures of memory? Would better memory im-
prove performance? The non-deterministic nature of the
game makes it hard to answer these questions analytically;                   40%
                                                                                   1   2   3   4     5     6   7    8     9    10    11
thus, we implemented the game as a computer simulation.                                        Turn (within game)
Simulation As Allen Newell and Herbert Simon famously              Figure 2: Simulation results for four artificial agents playing
stated, “Just as a scissors cannot cut paper without two blades,   10,000 games of original TRACS.
a theory of thinking and problem solving cannot predict be-
havior unless it encompasses both an analysis of the structure
of task environments and an analysis of the limits of rational     constraints and the minimal benefits of an update strategy,
adaptation to task requirements.” (1972, p. 55). In this spirit,   participants might well have adopted a baseline strategy for
we created a simulation in MATLAB™ in which ‘pure’ cog-            good reasons. Thus, our analysis suggests a re-interpretation
nitive strategies could be formalized and implemented. By          of Burns’ original findings: In Straight TRACS, memory up-
running these artificial agents for thousands of trials, we were   date yields no performance benefit over adopting a much eas-
able to determine precise performance levels, despite the dy-      ier baseline strategy. Hence, adopting the baseline strategy is
namic and nondeterministic aspects of the game.                    both adaptive and rational.
   We compared four cognitive agents that differed in their
memory resources and strategies, but did not make any errors       TRACS*
in odds translation or judgment. A baseline agent has perfect      The simulation results suggest that—by not offering an incen-
knowledge of the initial deck distribution, but is amnesic with    tive to a memory update strategy—Straight TRACS is inad-
regards to the cards played during a game. In contrast, the        equate for investigating people’s willingness and capacity to
update agent enjoys perfect memory of every hand played,           monitor and update changing environmental circumstances.
and bases all choices on the actual odds at any given moment.      In this section we introduce TRACS*, which provides a clear
   Two additional agents bracket the performance of baseline       benefit for adopting an update strategy, as well as introduces
and update agents: random agent has neither memory nor             additional probes of memory performance.
knowledge of the initial distribution, and hence is forced to         In designing TRACS*, we sought to create a variant of the
blindly guess at every turn. On the other end of the scale, om-    game for which a memory update strategy clearly benefits
niscient agent effectively enjoys X-ray vision and can observe     performance. We achieved this by carefully controlling the
the colors of both candidate cards, allowing for optimal card      cards dealt to the players. While cards were selected ran-
selections without the need for memory or odds estimates.          domly, they were selected from a card space constrained by
   The mean score for the random agent across 10,000 simu-         two rules. First, only pairs of face-down cards that would not
lated games was 5.24 (out of 11 possible) hits per game. To        have equal odds of matching the target color would be dealt.
our surprise, baseline and update agents performed about the       By eliminating ties, this rule eliminates the need to guess.
same, scoring 6.57 and 6.79, respectively. Thus, the aver-         Second, pairs were not selected if the card with the lower
age performance difference between the baseline and update         odds resulted in a hit, or if the card with the higher odds did
agents was roughly two tenths of one point per game. Fur-          not. This rule aimed to reduce the influence of luck by elimi-
ther, both strategies achieved only marginally better scores       nating win-win and lose-lose situations, thus driving a wedge
than the random strategy.                                          between the baseline and update strategies.
   Figure 2 shows the mean percentage of hits per turn for            Figure 3 illustrates the effects of these changes. The mean
each agent. It is obvious that the performance of baseline         score for the random agent in TRACS* remained stable, at
and update agents are very similar, except for an increasing       5.49 (out of 11) hits per game across 10,000 games. How-
benefit of update strategy late in a game. The entire range        ever, baseline and update scores rose to 8.22 and 10.83, re-
between random and omniscient performance scores is only           spectively. Hence, our game modifications were successful
25%, which is essentially due to 25% of all turns not allowing     in introducing a substantial benefit of the update strategy over
for a hit.                                                         the random and baseline strategies. Given that baseline and
   While an optimal update agent acts to maximize perfor-          update strategies now yield unique performance signatures, it
mance regardless of the effort involved, humans have limited       should be possible to determine which strategy our partici-
cognitive resources and are required to negotiate cost–benefit     pants actually adopt in the game.
tradeoffs (Anderson, 1990; Simon, 1990, 1992). Given these            Our second alteration in TRACS* was procedural. In addi-
                                                               1019

   100%
     90%
                                                  Random
                                                  Baseline
     80%                                          Update
                                                  Omniscient
  Hits
     70%
     60%
     50%
     40%
           1   2   3   4      5       6      7    8      9     10   11
                             Turn (within game)
Figure 3: Simulation results for four artificial agents playing             Figure 4: Screenshot of the TRACS* interface requesting
10,000 games of modified TRACS*.                                            odds estimates (after the completion of the recall task).
tion to using continuous color gradients to assess our partici-             Performance TRACS* allows for a straightforward corre-
pants’ odds calculations, we introduced memory recall boxes                 spondence between a player’s awareness of the current game
to judge the accuracy of their memory. In this way we hoped                 state and his or her outcome score. Thus, scores reliably
to elucidate whether Burns’ findings indicated an actual base-              exceeding the expected values of a simulated baseline agent
line bias, or merely just difficulty in converting accurately re-           would signal a memory update strategy.
called frequencies into points along a likelihood gradient.                    On average, participants scored 9.3 hits per game with 22
                                                                            out of 25 players (88%) exceeding the theoretic baseline score
                           Experiment                                       of 8.2 hits. This strongly suggests that memory updates con-
                                                                            tributed to task performance.
Method                                                                         To allow for a statistical assessment of these differences,
Twenty-five undergraduates from Rensselaer Polytechnic In-                  we let our simulated baseline and update agents both play the
stitute participated in partial fulfillment of a course require-            same number of games as human participants. A compari-
ment. They ranged in age from 18 to 22 years, with an aver-                 son of mean scores over the sequence of ten games per player
age of 19.6 years. Participants were tested individually.                   showed that human players scored significantly more points
   The experimenter spent about ten minutes instructing each                than baseline agents [9.3>8.2, t(26)=2.1, p<.001], and sig-
participant on the rules of original TRACS. Each participant                nificantly fewer hits than update agents [9.3<10.8, t(25)=2.1,
played a total of 10 games of 11 turns each. On every turn,                 p<.001]. Figure 5 contrasts the performance of human par-
players had to complete the recall task, provide odds esti-                 ticipants with that of simulated agents on a within-game res-
mates, and choose a card.                                                   olution. It is obvious that human players did not perform on
   On the newly added recall task participants were asked,
for each face-down card, to report the number of red and
blue cards of that shape which remained in the deck. An-
swers were typed into text boxes immediately below each                          100%
face-down card. Players then estimated the odds of each face-
down card being red or blue by placing a marker on a continu-                          90%
ous color gradient. Gradients were red on the left and blue on
the right, and 300 pixels wide (≈10 cm), allowing for a pre-                           80%
cision below one percent (see Figure 4 for a screenshot). Fi-                                        Baseline
                                                                                Hits                 Update
nally, participants chose a card by clicking on it. Feedback on                        70%           Subjects
correctness was then provided by a thumbs-up/thumbs-down
image and the next turn was initiated by clicking on the feed-                         60%
back image.
   The game was implemented in Macintosh Common Lisp
                                                                                       50%
5.0 running on OS 10.2 with a 17” flat panel display set to a
1024×768 screen resolution. The initial card distribution and
                                                                                       40%
a hit/miss counter were shown to the left of the game window.                                1   2        3     4    5       6      7    8   9   10   11
                                                                                                                    Turn (within game)
Results                                                                     Figure 5: Participants’ mean percentage of hits by turn com-
We will assess participants’ performance before turning to                  pared to those of simulated baseline and update agents. (Error
more detailed analyses of various error types.                              bars indicate 95% confidence intervals.)
                                                                         1020

                                 40%
                                                                            Recall-Odds error
                                                                                                                                                            80%   77.1%
                                                                            Gradient-odds error
                                                                                                                                                                                                        Recall Odds
                                 35%                                        Baseline-odds error                                                                                                         Gradient Odds
                                                                                                                                                            70%
                                 30%
  Error = |Odds - Actual Odds|
                                                                                                                                                            60%
                                                                                                                    Percentage of Odds Estimate Positions
                                                                                                                                                                               57.6%
                                 25%
                                                                                                                                                            50%
                                                                                                                                                                                                        42.4%
                                 20%
                                                                                                                                                            40%
                                 15%
                                                                                                                                                            30%
                                                                                                                                                                                           22.9%
                                 10%
                                                                                                                                                            20%
                                 5%
                                                                                                                                                            10%
                                 0%
                                                                                                                                                            0%
                                       1   2   3   4     5      6      7       8      9     10    11                                                              Closer to Actual Odds   Closer to Baseline Odds
                                                       Turn (within game)
Figure 6: Average errors of odds by turn. (Error bars indicate                                            Figure 7: Percentage of odds selections closer to the baseline
95% confidence intervals.)                                                                                vs. closer to the actual value (based on n=4404 estimates).
the level of an ideal update agent, but did reliably better than                                          systematic preference. If participants—due to update failure
a baseline agent.                                                                                         or memory decay—were more likely to choose odds closer to
   To assess possible effects of learning we conducted an                                                 the baseline than to the update value this would constitute a
ANOVA with game number as a within-subjects factor. A                                                     baseline bias. Likewise, an “update bias” could be diagnosed
significant main effect [F(9,216)=3.0, p<.01] indicated that                                              if participants were more likely to select odds in the vicinity
players improved their scores reliably from an average of 8.8                                             of the actual value. Figure 7 shows that, in TRACS*, the evi-
hits in earlier to about 9.7 hits in later games. Subsequent                                              dence for an update bias clearly outweighs the evidence for a
comparisons showed that human participants outperformed a                                                 baseline bias. Participants’ preference for actual values seems
pure baseline agent in all but the initial two games.                                                     particularly pronounced when odds are based on recall fre-
                                                                                                          quencies (77.1% vs. 22.9%). In contrast, the same preference
Errors Even though human participants performed better                                                    is weaker when odds estimates are measured by probability
than a baseline agent, their performance was worse than that                                              gradients (57.6% vs. 42.4%). As the baseline attractor seems
of an ideal update agent. In this section, we examine this dis-                                           to exert less gravitational pull when providing frequency es-
crepancy by first considering erroneous frequency and likeli-                                             timates than when responding on a gradient scale, examining
hood estimates before assessing errors of internal consistency.                                           only the latter (e.g., Burns, 2002a, 2002b) might overestimate
   As participants estimated card frequencies as well as likeli-                                          the size of a baseline bias.
hoods we were provided with two distinct indices of memory.                                                  All errors reported so far were deviations of empirical es-
To allow for direct comparisons of both indices on a single                                               timates from either true or baseline values. Our finding that
scale, we converted reported frequencies into ‘recall odds’.                                              participants’ frequency estimates are closer to the actual val-
For both recall odds and likelihood estimates (as indicated on                                            ues than to the initial baselines makes it implausible that
the gradient scales) we then calculated and summed up the                                                 participants’ frequency estimates are governed by a baseline
absolute difference from the actual odds.                                                                 bias. At the same time, it raises questions about alternative
   Figure 6 illustrates that both recall-odds and gradient-odds                                           breakdowns in performance. On the basis of our initial task
errors increase over the course of a game, but errors in fre-                                             analysis, the complexity of TRACS allows for a variety of
quency recall (with a mean of 8.0%) are significantly lower                                               non-memory related errors. In the following and final sec-
than the errors in likelihood estimates provided on gradient                                              tions we consider conversion errors and errors of choice as
scales (12.6%). The third line in Figure 6 shows the mean                                                 examples of errors of internal consistency.
size of the ‘baseline-odds’ error (16.5%) which would result                                                 Due to our sequential procedure of first requiring frequency
if participants had adopted a baseline strategy on the given                                              information and then asking for probability estimates, partic-
trial. Even though the mean gradient-odds error exceeded the                                              ipants’ responses on the likelihood gradients ought to be a
baseline-odds error on the first three trials, the general trend                                          direct function of recall performance. Nonetheless, people’s
indicates that participants’ actual errors on both scales were                                            notorious problems with probabilities can cause conversion
lower than suggested by a baseline bias.                                                                  errors when transforming recalled frequencies into odds on
   Taking into account the direction of deviations rather than                                            continuous scales. To assess the occurrence of such errors, we
just error magnitudes, we can also ask whether empirical re-                                              compared subjective recall odds (based on the card frequency
call and gradient odds are closer to the baseline or to the ac-                                           entries of each participant and turn) with the likelihood es-
tual odds. Whenever the actual odds value deviates from the                                               timates provided on the same turn. An average deviation of
baseline value there are two possible attractors: Participants                                            6.6% indicates that this translation process was indeed non-
might specify odds closer to the baseline odds, or they might                                             trivial and error-prone. The magnitude of this error is striking
select odds closer to the actual odds. A bias is defined by a                                             not only as it is almost as large as the average error in fre-
                                                                                                       1021

quency recall (8.0%, see Figure 6), but also when considering       may have inadvertently prompted different memory strategies
that players reported their subjective frequencies immediately      is an empirical question to be addressed in future studies.
before indicating their judgment of odds and had all relevant          Finally, the performance results of our modified version
frequencies displayed directly above the gradient scales (see       TRACS* provide a more optimistic view of the human ca-
Figure 4). Thus, we conclude that a large proportion of par-        pacity for concurrent memory updates than do previous stud-
ticipants’ error-prone responses on likelihood scales were due      ies. As our players were able to reliably exceed baseline per-
to errors in odds conversion.                                       formance, we conclude that the previously reported ‘baseline
   Two curious errors of internal consistency address the re-       bias’ may be an artifact of the original game.
lation between odds estimates and card selections. Recall-             Despite our criticisms, our results agree with those of
choice errors can be defined as instances in which the card         Burns (2002a, 2002b) that people are able to take baserate
with lower recall odds (based on the subjective card fre-           information into account. However, we additionally demon-
quency estimates) is selected by the participant. Similarly,        strate that—when memory matters—people are also able to
gradient-choice errors occur whenever the card with lower           dynamically update their memory while being engaged in a
likelihood odds (based on probability estimates) is chosen.         highly demanding task.
   There were 4.3% (119 out of 2750 choices) recall-choice
errors, but 8.3% (229) gradient-choice errors. Given that any                              Acknowledgments
conflict between judgment and choice is relatively bizarre,         We are grateful to Kevin Burns for allowing us to use TRACS
both errors are more frequent than we would have expected.          and providing many helpful comments. In addition, we thank
As the gradients are evaluated immediately before a choice          Christopher Myers, Bram van Heuveln and Jamie Sowder for
is made, we interpret the relative size of both errors as evi-      many valuable contributions. The work reported was sup-
dence that players were more likely to base their choices on        ported by grants from the Air Force Office of Scientific Re-
perceived frequencies than on perceived odds.                       search (AFOSR #F49620-03-1-0143), as well as the Office of
                                                                    Naval Research (ONR #N000140310046).
                          Discussion
Our first result is of a methodological nature: When creating                                    References
artificial task environments to assess the scope of human ra-       Anderson, J. R. (1990). The adaptive character of thought. Hills-
tionality, the cost–benefit structure of the task must provide         dale, NJ: Lawrence Erlbaum.
an incentive to display the behavior in question. Our simula-       Burns, K. (2001). TRACS: A tool for research on adaptive cogni-
tion of Straight TRACS revealed that the original game pro-            tive strategies: The Game of Confidence and Consequence. At
vides only minimal benefits for adopting an effortful memory           www.tracsgame.com (May 2004).
update strategy. This led us to re-interpret Burns’ (2002a,         Burns, K. (2002a). On Straight TRACS: A baseline bias from men-
2002b) original finding of a ‘baseline bias’ as an adaptive and        tal models. Proceedings of the 24th Annual Conference of the
rational response to the properties of the task environment.           Cognitive Science Society, 154-159. Hillsdale, NJ: Lawrence Erl-
                                                                       baum.
   Our critique, however, does not imply that TRACS is not
                                                                    Burns, K. (2002b). Dealing with TRACS: The game of confidence
an interesting game and valuable research paradigm—quite to            and consequence. Proceedings of the American Association for
the contrary! We now believe that TRACS is both more com-              Artificial Intelligence, Symposium on Chance Discovery.
plex and more interesting than it at first appeared. Our task       Burns, K. (2004). Making TRACS: The diagrammatic design of a
analysis has suggested the need to distinguish three cognitive         double-sided deck. Proceedings of the 3rd International Confer-
components: retrieving numbers of cards from memory, con-              ence on the Theory and Application of Diagrams.
verting frequencies into probabilities, and mapping frequency       Gigerenzer, G. (2000). Adaptive thinking. Rationality in the real
or probability estimates to choices.                                   world. Oxford, UK: Oxford University Press.
   We are particularly intrigued by the errors our players          Gray, W.D. (2002). Simulated task environments: The role of high-
made when converting natural frequency information to like-            fidelity simulations, scaled worlds, synthetic environments, and
lihood estimates. Players who had to provide the same in-              microworlds in basic and applied cognitive research. Cognitive
formation in two different formats within seconds and saw              Science Quarterly 2(2), 205–227.
the frequencies displayed in front of them while computing          Hess, S.M., Detweiler, M.C. and Ellis, R.D. (1999). The utility of
probabilities still made substantial errors when coming up             display space in keeping track of rapidly changling information.
                                                                       Human Factors 41(2), 257–281.
with simple likelihood estimates. Interestingly, our analysis
of choice errors revealed that players seemed less likely to        Koehler, J.J. (1996). The base rate fallacy reconsidered: Descriptive,
act on their inaccurate probability estimates than on their per-       normative and methodological challenges. Behavioral and Brain
                                                                       Sciences, 19, 1–53.
ceived frequencies even though the former just preceded their
choice.                                                             Newell, A., and Simon, H.A. (1972). Human problem solving. En-
                                                                       glewood Cliffs, NJ: Prentice-Hall, Inc.
   A potential caveat of our study is that by altering the cost-
benefit structure of the task and assessing players’ memory         Simon, H.A. (1990). Invariants of human behavior. Annual Review
                                                                       of Psychology, 41, 1–19.
for card frequencies we introduced two changes to the origi-
nal game. It is conceivable that the mere query for frequen-        Venturino, M. (1997). Interference and information organization in
                                                                       keeping track of continually changing information. Human Fac-
cies made the necessity to count cards more explicit, whereas          tors, 39(4), 532–539.
it remains rather implicit in the original game. The extent
                                                                    Yntema, D.B. (1963). Keeping track of several things at once. Hu-
to which each of our modifications contributed to the im-              man Factors 5, 7–17.
proved performance and to which a procedural task demand
                                                                1022

