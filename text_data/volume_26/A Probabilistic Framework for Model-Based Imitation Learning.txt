UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Probabilistic Framework for Model-Based Imitation Learning
Permalink
https://escholarship.org/uc/item/45q6r33w
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 26(26)
Authors
Shon, Aaron P.
Grimes, David B.
Baker, Chris L.
et al.
Publication Date
2004-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        A Probabilistic Framework for Model-Based Imitation Learning
                Aaron P. Shon, David B. Grimes, Chris L. Baker, and Rajesh P.N. Rao
                                  {aaron, grimes, clbaker, rao}@cs.washington.edu
                 CSE Department, Box 352350           University of Washington          Seattle WA 98195 USA
                           Abstract                                frameworks have been proposed for imitation learn-
                                                                   ing in machines [Breazeal, 1999, Scassellati, 1999,
   Humans and animals use imitation as a mechanism for             Billard and Mataric, 2000], but most of these are not de-
   acquiring knowledge. Recently, several algorithms and
   models have been proposed for imitation learning in             signed around a coherent probabilistic formalism such as
   robots and humans. However, few proposals o er a                Bayesian inference. Probabilistic methods, and Bayesian
   framework for imitation learning in a stochastic environ-       inference in particular, are attractive because they han-
   ment where the imitator must learn and act under real-          dle noisy, incomplete data, can be tuned to handle realis-
   time performance constraints. We present a probabilis-
   tic framework for imitation learning in stochastic envi-        tically large problem sizes, and provide a unifying math-
   ronments with unreliable sensors. We develop Bayesian           ematical framework for reasoning and learning. Our ap-
   algorithms, based on Meltzo and Moore‚Äôs AIM hypoth-             proach is unique in combining a biologically inspired ap-
   esis for infant imitation, that implement the core of an        proach to imitation with a Bayesian framework for goal-
   imitation learning framework, and sketch basic propos-          directed learning. Unlike many imitation systems, which
   als for the other components. Our algorithms are com-
   putationally efficient, allowing real-time learning and         implement only software simulations, this paper demon-
   imitation in an active stereo vision robotic head. We           strates the value of our framework through both simula-
   present results of both software simulations and our al-        tion results and a real-time robotic implementation.
   gorithms running on the head, demonstrating the valid-
   ity of our approach.                                                Components of an imitation learning
                                                                                            system
       Imitation learning in animals and                           The observer must surmount a number of problems
                          machines                                 in attempting to replicate the behavior of the instruc-
Imitation is a common mechanism for transfer-                      tor. Although described elsewhere [Schaal et al., 2003,
ring knowledge from a skilled agent (the instruc-                  Rao and Meltzoff, 2003], we briefly reformulate them as
tor ) to an unskilled agent (or observer ) using                   follows:
direct demonstration rather than manipulating
                                                                  1. State identification: Ability to classify high-
symbols.        Various forms of imitation have been
                                                                      dimensional sensor data into a lower-dimensional, rel-
studied in apes [Visalberghy and Fragaszy, 1990,
                                                                      evant state robust to sensor noise. State identifica-
Byrne and Russon, 2003],             in     children       (in-
                                                                      tion should differentiate between the internal state of
cluding        infants     only       42    minutes       old)
                                                                      the observer (proprioceptive feedback, etc.) and the
[Meltzoff and Moore, 1977, Meltzoff and Moore, 1997],
                                                                      state of the environment, including the states of other
and in an increasingly diverse selection of machines
                                                                      agents, particularly the instructor.
[Fong et al., 2002, Lungarella and Metta, 2003]. The
attraction for machine learning is obvious: a machine             2. Action identification: Ability to classify sequences
with the ability to imitate has a drastically lower cost of           of states in time.
reprogramming than one which requires programming
by an expert. Imitative robots also offer testbeds for            3. State mapping: Transformation from the egocentric
cognitive researchers to test computational theories,                 coordinate system of the instructor to the egocentric
and provide modifiable agents for contingent interaction              coordinate system of the observer.
with humans in psychological experiments.
   Few previous efforts have presented biologically plau-         4. Model learning: Learning forward and inverse mod-
sible frameworks for imitation learning.             Bayesian         els [Blakemore et al., 1998] to facilitate interaction
imitation learning has been proposed to accelerate                    with the environment.
Markov decision process (MDP) learning for reinforce-             5. Policy learning: Learning action choices that maxi-
ment learning agents [Price, 2003]; however, this frame-              mize a reward function, as observed from the actions
work chiefly addresses the problem of learning a forward              selected by the instructor in each given state.
model of the environment [Jordan and Rumelhart, 1992]
via imitation (see below), and the correspondence                 6. Sequence learning and segmentation: Ability to
with cognitive findings in humans is unclear. Other                   memorize sequences of key states needed to complete
                                                               1237

   an imitation task; ability to segment imitation tasks,
   and to divide tasks into subtasks with particular sub-
   goal states.
 A Bayesian framework for goal-directed
                      imitation learning
Imitation learning systems that learn only state and ac-
tion mappings (without modeling the environment or
the instructor‚Äôs goals) ignore the separability of the
instructor‚Äôs intent from the actions needed to accom-
plish that intent. Systems that use deterministic mod-
els rather than probabilistic ones ignore the stochastic
nature of realistic environments. We propose a goal-
directed Bayesian formalism that overcomes both of
these problems. The notation st denotes the state (both
internal and external to an agent) at time t, and at de-                      Figure 1:       AIM hypothesis model for infant
                                                                              imitation:      The AIM hypothesis of Meltzo              and
notes the action taken by an agent at time t. sG denotes                      Moore [Meltzo and Moore, 1997] argues that infants match
a special ‚Äúgoal state‚Äù that is the desired end result of the                  observations of adults with their own proprioceptions using
imitative behavior. The key to viewing imitation learn-                       a modality-independent representation of state. Our compu-
ing as a model-based, goal-directed Bayesian task is to                       tational framework suggests an efficient, probabilistic imple-
                                                                              mentation for this hypothesis.
identify:
 Forward model: Predicts a distribution over future
   states given current state(s), action(s), and goal(s)‚Äî                        Fig.       1 graphically represents Meltzoff and
   P (st+1 |at , st , sG ). Models how different actions affect               Moore‚Äôs Active Intermodal Mapping (AIM) hypothe-
   environmental state.                                                       sis [Meltzoff and Moore, 1997]. According to this cog-
                                                                              nitive model, imitation begins with an infant (or other
 Inverse model: Infers a distribution over actions                           agent) forming a representation of features in the out-
   given current state(s), future state(s), and goal(s)‚Äî                      side world. Next, this representation is transformed into
   P (at |st , st+1 , sG ). Models which action(s) should be                  a ‚Äúsupra-modal,‚Äù or modality-independent, representa-
   selected to transition from one environmental state to                     tion of those features. An equivalence detector matches
   another.                                                                   the current modality-independent representation of the
                                                                              instructor‚Äôs state with a modality-independent represen-
 Prior model: Infers a distribution over actions given                       tation of the infant observer‚Äôs state. Proprioceptive feed-
   current state(s) and goal(s)‚ÄîP (at |st , sG ). Models the                  back guides the infant‚Äôs motor output toward matching
   policy (or preferences) followed by a particular in-                       the instructor‚Äôs state. Our framework for Bayesian ac-
   structor in transitioning through the environment to                       tion selection using learned models captures this idea of
   achieve a particular goal.                                                 imitation as a ‚Äúmatching-to-target‚Äù process.
                                                                                 Fig. 2 depicts a block diagram of our architecture.
Thus the prior model involves learning an MDP (or a                           Like AIM, our system begins by running several feature
partially observable MDP), while the forward model in-                        detectors (skin detectors, face trackers, etc.) on sen-
volves learning a ‚Äúsimulator‚Äù of how the environment                          sor inputs from the environment. Detected features are
(possibly including other agents) reacts to actions per-                      monitored over time to produce state sequences. In turn,
formed within it. Learning inverse models is a noto-                          these sequences define actions. The next step is to trans-
riously difficult task [Jordan and Rumelhart, 1992], not                      form state and action observations into instructor-centric
least because multiple actions could have mapped from                         values, then map from instructor-centric to observer-
st to st+1 . However, using Bayes‚Äô rule, we can infer the                     centric coordinates. Observer-centric values are em-
distribution returned by the inverse model using the for-                     ployed to update probabilistic forward and prior models
ward and prior models:
                                                                              in our Bayesian inference framework. Finally, combin-
    P (at |st , st+1 , sG ) ‚àù P (st+1 |at , st , sG ) Pr(at |st , sG ) (1)    ing distributions from the forward and prior models as in
                                                                              Eqn. 1 yields a distribution over actions. The resulting
  Equation 1 can be used to either select the maxi-                           distribution over actions is converted into a single motor
mum a posteriori action to complete a state transi-                           action the observer should take next, with an efference
tion, or to sample over a distribution of alternatives,                       copy conveyed to the feature detectors to cancel out the
refining the model (and representing an exploration- ex-                      effects of self-motion.
ploitation tradeoff reminiscent of reinforcement learn-
ing). Sampling from the distribution over actions is                          State and action identification
also called probability matching. Evidence exists that                        Deriving state and action identity from sensor data
the brain employs probability matching in at least some                       involves task- and sensor-specific functions.             Al-
cases [Herrnstein, 1961, Krebs and Kacelnik, 1991].                           though it is impossible to summarize the extensive
                                                                          1238

                                                                              to learn models of the environment, and to discover
                                 Feature                                      policies to maximize rewards obtained from the environ-
     E                          Detectors
     n                                                                        ment. Evidence demonstrates that infants learn forward
     v                                                                        models of how their limbs, facial muscles, and other
     i           State             State     ...     State
                                                                              body parts react to motor commands, a process referred
     r       estimator 1       estimator 2       estimator N
     o                                                                        to by Meltzoff and Moore [Meltzoff and Moore, 1997]
     n                       t          t‚àí1    t‚àí2                            as ‚Äúbody babbling.‚Äù Such forward model learning could
     m                    Action estimator 2                                  occur both prenatally and during infancy. We anticipate
     e                                                                        using well-established supervised algorithms to acquire
     n                                                                        forward models of environmental dynamics. Unsuper-
     t                         Transform               Consolidate            vised learning of forward and inverse models to generate
                       observation coordinates
                         to instructor‚àícentric         states into            motor policies is a well-known problem in the reinforce-
         Efference             coordinates          hierarchical graph        ment learning community (see [Kaelbling et al., 1996]
          copy                                                                for a survey). In reinforcement learning, an agent‚Äôs
                          Map instructor‚Äôs
                                                                              internal reward signal alone is used to learn models
                               DOFs to
                          observer‚Äôs DOFs                                     of the environment, rather than relying on examples
        Motor                                     Forward        Prior        provided by a teacher as in imitation learning.
       System                   Bayesian            Model        Model
                                Imitation
                                 Module                  Inverse
                                                                              Sequence learning and segmentation
                                                         Model
                                                                              Realistic imitation learning systems must be able to
                                                                              learn sequences of states that define actions, and to seg-
Figure 2: Overview of model-based Bayesian imita-                             ment these sequences into meaningful chunks for later
tion learning architecture: As in AIM, the initial stages
of our model correspond to the formation of a modality-                       recall or replay. Part of our ongoing work is to de-
independent representation of world state. Mappings from                      fine how semantically meaningful chunks can be defined
instructor-centric to observer-centric coordinates and from                   and recalled in real time. Recent developments in con-
the instructor‚Äôs motor degrees of freedom (DOFs) to the ob-                   cept learning (e.g., [Tenenbaum, 1999]) suggest how sim-
server‚Äôs motor DOFs play the role of equivalence detector
in our framework, matching the instructor‚Äôs motor output to                   ilar environmental states might be grouped together, en-
the motor commands of the observer. E erence copy provides                    abling development of hierarchical state and action rep-
proprioceptive feedback to close the motor control loop.                      resentations in machine systems.
                                                                              A Bayesian algorithm for inferring intent
body of work in action and state identification here,
we note recent progress in extracting actions from                            Being able to determine the intention of others is a
laser rangefinder and radio [Fox et al., 2003] and vi-                        crucial requirement for any social agent, particularly
sual [Efros et al., 2003] data. In most cases, computa-                       an agent that learns by watching the actions of oth-
tional expediency necessitates employment of dimension-                       ers. Recent studies have revealed the presence of ‚Äúmir-
ality reduction techniques such as principal components                       ror neurons‚Äù in monkey cortex that fire both when an
analysis, Isomap [Tenenbaum et al., 2000], or locally lin-                    animal executes an action and when it observes oth-
ear embedding [Roweis and Saul, 2000]. Saliency detec-                        ers performing similar actions. These findings sug-
tion algorithms [Itti et al., 1998] may also help reduce                      gest a neurological substrate for intent inference in pri-
high-dimensional visual state data to tractable size.                         mates [Rizzolatti et al., 2000]. One appealing aspect of
                                                                              our framework is that it suggests a probabilistic algo-
Learning state mappings                                                       rithm for determining the intent of the instructor. That
A prerequisite for any robotic imitation task is to deter-                    is, an observer can determine a distribution over goal
mine a mapping from the instructor‚Äôs state to the ob-                         states based on watching what actions the instructor ex-
server‚Äôs [Nehaniv and Dautenhahn, 2002]. We view this                         ecutes over some period of time. This could have appli-
state mapping problem as an instance of subgraph iso-                         cations in machine learning systems that predict what
morphism, where the goal is to match subgraphs from                           goal state the user is attempting to achieve, then offer
the instructor (corresponding to effectors, e.g. limbs) to                    suggestions or assist in performing actions that help the
their corresponding graphs in the observer. In the sim-                       user reach that state. The theory could lead to quantita-
ulation and robotic head results shown below, the map-                        tive predictions for future cognitive studies to determine
pings are trivial; developing detailed graph-theoretic ap-                    how humans infer intent in other intelligent agents.
proaches to mapping from instructor states to observer                           Our algorithm for inferring intent uses applications of
states remains an ongoing topic of investigation.                             Bayes‚Äô rule to compute the probability over goal states
                                                                              given a current state, action, and next state obtained
Learning forward models                                                       by the instructor, P (sG |st+1 , at , st ). This probability
                                                                              distribution over goal states represents the instructor‚Äôs
Numerous           supervised            and     unsupervised          ap-    intent. One point of note is that P (st+1 |at , st , sG ) ‚â°
proaches (see e.g. [Jordan and Rumelhart, 1992,                               P (st+1 |at , st ); i.e., the forward model does not depend
Todorov and Ghahramani, 2003]) have been proposed                             on the goal state sG , since the environment is indifferent
                                                                          1239

to the desired goal. Our derivation proceeds as follows:                              instructor‚Äôs reward model). The observer must rely on
                                                                                      features of its own reward model, as well as telltale signs
                                   P (st+1 , st , at , sG )
   P (st+1 |at , st , sG )    =                                                (2)    of desired states (e.g., states that the instructor tends to
                                      P (at , st , sG )
                                                                                      act to remain in, or that cause the instructor to change
                                   P (sG |st+1 , at , st ) P (st+1 , at , st )        the context of its actions, could be potential goal states)
   P (st+1 |at , st , sG )    =                                                (3)
                                      P (sG |at , st )        P (at , st )            to infer this prior distribution. The difficulty of learning
  Because P (st+1 |at , st , sG ) ‚â° P (st+1 |at , st ), and since                     this distribution could explain why it takes so long for
   P (at ,st )              1                                                         infants to acquire the final piece of the imitation puzzle,
P (st+1 ,at ,st ) = P (st+1 |at ,st ) :                                               determining the intent of others. We did not explic-
                  P (sG |at , st )   = P (sG |st+1 , at , st )                 (4)
                                                                                      itly design the terms in our intent inference algorithm
                                                                                      to match childhood developmental stages; rather, the
                 P (sG , at , st )                                                    derivation follows from the inverse model formulation in
                                     = P (sG |st+1 , at , st )                 (5)
                   P (at , st )                                                       Eqn. 1 and straightforward applications of Bayes‚Äô rule.
    P (at |sG , st ) P (sG , st )
                                     = P (sG |st+1 , at , st )                 (6)                      Simulation results
              P (at , st )
    P (at |sG , st ) P (sG , st )    ‚àù P (sG |st+1 , at , st )                 (7)    Fig. 3 demonstrates imitation results in a purely sim-
           P (sG |st+1 , at , st )   ‚àùP (at |sG , st ) P (st |sG ) P (sG )(8)
                                                                                      ulated environment. The task is to reproduce observed
                                                                                      trajectories through a maze containing three different
  The first of the terms in Eqn. 8 represents the prior                               goal states (maze locations marked with ovals). This
model. The second term represents a distribution over                                 simulated environment simplifies a number of the issues
states at time t, given a goal state sG . This could be                               mentioned above: the location and value of each goal
learned by, e.g., observing the instructor manipulate an                              state is known by the observer a priori; the movements
object, with a known intent, and recording how often                                  of the instructor are observed free from noise; the for-
the object is in each state. Alternatively, the observer                              ward model is restricted so that only moves to adjacent
could itself ‚Äúplay with‚Äù or ‚Äúexperiment with‚Äù the object,                             maze locations are possible; and the observer can detect
bearing in mind a particular goal state, and record how                               when it is next to a wall (although it does not know a
often each object state is observed. The third term is a                              priori that it cannot move through walls).
prior over goal states; it can be derived by modeling the                                The observer first learns a forward model by interact-
reward model of the instructor. If the observer can either                            ing with the simulated environment for 500 simulation
assume that the instructor has a similar reward model                                 steps. The instructor then demonstrates 4 different tra-
to itself (the ‚Äúlike-me‚Äù hypothesis [Meltzoff, 2002]), or                             jectories to the observer (1 to the white goal, 2 to the
model the instructor‚Äôs desired states in some other way,                              light gray goal, 1 to the dark gray goal), allowing the ob-
it can infer P (sG ).                                                                 server to learn a prior model. Fig. 3(a) shows the maze
   Interestingly, these three terms roughly match                                     environment used in our simulations. Fig. 3(b) shows a
the three developmental stages laid out by Melt-                                      sample training trajectory (black arrows) where the in-
zoff [Meltzoff, 2002]. According to our hypothesis, the                               structor moves from location (1,1) to the goal state at
first term in Eqn. 8 corresponds to a distribution over                               (3,3). The solid white line (over arrows) demonstrates
actions as learned during imitation and goal-directed                                 the observer reproducing the same trajectory after learn-
actions. This distribution can be used if all the ob-                                 ing. The observer‚Äôs trajectory varies somewhat from the
server wants to do is imitate body movements (the first                               instructor‚Äôs due to the stochastic nature of the environ-
step in imitation that infants learn to perform accord-                               ment. Fig. 3(c) shows another training trajectory, com-
ing to Meltzoff‚Äôs theory of development). The second                                  prising 47 steps, where the instructor moves toward the
term in Eqn. 8 refers to distributions over states of ob-                             white goal (goal 1). The observer‚Äôs task for this tra-
jects given a goal state. Because the space of actions                                jectory is to estimate, at each time step of the trajec-
an agent‚Äôs body can execute is presumably much less                                   tory, a distribution over which goal state the instruc-
than the number of state configurations objects in the                                tor is headed toward. During the inference process, the
environment can assume, this distribution requires col-                               observer does not have direct knowledge of the actions
lecting much more data than the first. Once this second                               selected by the instructor; it must infer these by moni-
term is learned, however, it becomes easier to manip-                                 toring state changes in the environment. The graph in
ulate objects to a particular end‚Äîan observer that has                                Fig. 3(d) shows this distribution over goals, where data
learned P (st |sG ) has learned which states of an object or                          points represent inferred intent averaged over epochs of
situation ‚Äúlook right‚Äù given a particular goal. The com-                              8 simulation steps each (i.e., the first data point on the
plexity of this second term could explain why it takes                                graph represents inferred intent averaged over simula-
babies much longer to learn to imitate goal-directed ac-                              tion steps 1-8, the second data point spans simulation
tions on objects than it does to perform simple imitation                             steps 9-17, etc., with the last epoch spanning 7 simula-
of body movements (as claimed in Meltzoff‚Äôs theory). Fi-                              tion steps). Note that the estimate of the goal is correct
nally, the third term, P (sG ), is the most complex term                              over all epochs. The algorithm is particularly confident
to learn. This is both because the number of possible                                 once the ambiguous section of the trajectory, where the
goal states sG is huge, and the fact that the observer                                instructor could be moving toward the dark gray or the
must model the instructor‚Äôs distribution over goals indi-                             light gray goal, is passed. Performance of the algorithm
rectly (the observer obviously cannot directly access the                             would be enhanced by more training; only 4 sample tra-
                                                                                  1240

    20                              20
                                                                                                           a)                                                         b)
  a)                               b)
    10                              10
    1                               1
         1       10           20                    1               10                       20
    20
                                                         1
                                                                                                           c)                 45                                      d)
  c)                               d)                   0.8
                                                                                                         pan angle (deg.)
                                                                                                                             22.5
                                     )                                              Goal 1
                                             t t t+1
                                                        0.6                         Goal 2
                                     P(s |a ,s ,s
                                                                                    Goal 3
                                                                                                                               0
    10                                       G
                                                        0.4
                                                                                                                            ‚àí22.5
                                                        0.2
                                                                                                                             ‚àí45
                                                         0                                                                    ‚àí60     ‚àí30         0       30    60
    1                                                         1         24                   47
         1       10           20                                  Simulation step
                                                                                                                                            tilt angle (deg.)
Figure 3: Simulated environment for imitation learn-                                                 Figure 4: Gaze tracking in a robotic head: (a) Bi-
ing: (a) Maze environment used to train observer. Thick                                              clops active stereo vision head from Metrica, Inc. (b) In-
black lines denote walls; ovals represent goal states. Lightness                                     fants as young as 9 months can detect gaze based on head
of ovals is proportional to the probability of the instructor se-                                    direction; older infants (‚â• 12 months) use opened eyes as
lecting each goal state (re ecting, e.g., relative reward value                                      a cue to detect whether they should perform gaze tracking
experienced at each state). (b) Example trajectory (black                                            (from [Brooks and Meltzo , 2002]). (c) Likelihood surface
arrows) from the instructor, ending at the second goal. Re-                                          for the face shown in (d), depicting the likelihood over pan
production of the trajectory by the observer is shown as a                                           and tilt angles of the subject‚Äôs head. The region of highest
solid white line overlying the arrows; inference is performed                                        likelihood (the brightest region) matches the actual pan and
as in Eqn. 1. The instructor required 23 steps to reach                                              tilt angles (black X) of the subject‚Äôs face shown in (d).
the goal; the observer required a slightly larger number of
steps due to both the stochastic nature of the environment
and imperfect learning of the forward and prior models. (c)                                          looking straight ahead).
Instructor‚Äôs trajectory in the intention inference task. (d)
Graph showing a distribution over instructor‚Äôs goal states, as                                          Fig. 4(c) depicts a likelihood surface over pan and tilt
inferred by the observer at di erent time points in the sim-                                         angles of the instructor‚Äôs head in the pose shown in Fig.
ulation. Note how the actual goal state, goal 1, maintains a                                         4(d). Our system generates pan and tilt motor com-
high probability relative to the other goal states throughout                                        mands by selecting the maximum a posteriori estimate
the simulation. Goal 2 brie y takes on a higher probability
due to limited number of training trajectories.                                                      of the instructor‚Äôs pan and tilt, and performing a sim-
                                                                                                     ple linear transform from instructor-centric to egocentric
                                                                                                     coordinates. Out of 27 out-of-sample testing images us-
jectories were presented to the algorithm, meaning that                                              ing leave-one-out cross-validation, our system is able to
its estimates of the distributions on the right hand side                                            track the angle of the instructor‚Äôs head to a mean error of
of Eqn. 8 were extremely biased.                                                                     ¬±4.6 degrees. 1 Our previous efforts [Shon et al., 2003]
 Real-time application in a robotic head                                                             demonstrated the ability of our system to track the gaze
                                                                                                     of an instructor; ongoing robotics work involves learning
We have also implemented our probabilistic approach in                                               policy models specific to each instructor, and inferring
a Biclops active stereo vision head (Fig. 4(a)). The head                                            instructor intent based on object saliency.
follows the gaze of a human instructor, and tracks the
orientation of the instructor‚Äôs head to determine where                                                                                                     Conclusion
to look next. Gaze following [Brooks and Meltzoff, 2002,                                             This paper describes a Bayesian framework for imitation
Scassellati, 1999] (Fig. 4(b)) represents a key step in the                                          learning, based on the AIM model of imitation learning
development of shared attention, in turn bootstrapping                                               by Meltzoff and Moore. The framework emphasizes im-
more complicated imitation tasks. Our system begins                                                  itation as a ‚Äúmatch-to-target‚Äù task, and promotes sepa-
by identifying an image region likely to contain a face                                              ration between the dynamics of the environment and the
(based on detecting skin tones and bounding box as-                                                  policy a particular teacher chooses to employ in reach-
pect ratio). We employ a Bayesian pose detection algo-                                               ing a goal. We have sketched the basic components for
rithm [Wu et al., 2000] that matches an elliptical model                                             any imitation learning system operating in realistically
of the head to the human instructor‚Äôs face. Our algo-                                                large-scale environments with stochastic dynamics and
rithm then transforms the estimated gaze into the Bi-                                                noisy sensor observations. Our model naturally leads
clops‚Äô egocentric coordinate frame, causing the Biclops                                              to a Bayesian algorithm for inferring the intent of other
to look toward the same point in space as the human                                                                  1
                                                                                                                            We
                                                                                                                            q de ne error as:
instructor. We trained the pose detector on a total of 13                                                                                              2                       2
faces, with each training subject looking at 36 different                                            E=                             Œ∏pan       Œ∏ÃÇpan        + Œ∏tilt    Œ∏ÃÇtilt
targets; each target was associated with a different pan                                             where Œ∏ is the true angle, and Œ∏ÃÇ is our system‚Äôs estimate of
and tilt angle relative to pan 0, tilt 0 (with the subject                                           the angle.
                                                                                                  1241

agents. We presented preliminary results of applying                [Lungarella and Metta, 2003] Lungarella, M. and Metta, G.
our framework to a simulated maze task and to gaze fol-                (2003). Beyond gazing, pointing, and reaching: a survey
lowing in an active stereo vision robotic head. We are                 of developmental robotics. In EPIROB ‚Äô03, pages 81‚Äì89.
currently investigating the ability of the framework to             [Meltzo , 2002] Meltzo , A. N. (2002). Elements of a devel-
scale up to more complex robotic imitation tasks in real-              opmental theory of imitation. In Meltzo , A. N. and Prinz,
world environments. We are also exploring the connec-                  W., editors, The imitative mind: Development, evolution,
tions between our probabilistic framework and findings                 and brain bases, pages 19‚Äì41. Cambridge: Cambridge Uni-
                                                                       versity Press.
from developmental psychology.
                                                                    [Meltzo and Moore, 1977] Meltzo , A. N. and Moore,
                   Acknowledgements                                    M. K. (1977). Imitation of facial and manual gestures by
                                                                       human neonates. Science, 198:75‚Äì78.
We thank Andy Meltzo for extensive discussions and feed-
back, and for supplying Figs. 1 and 4(b). APS was funded            [Meltzo and Moore, 1997] Meltzo , A. N. and Moore,
by a NSF Career grant to RPNR, and DBG was funded by                   M. K. (1997). Explaining facial imitation: A theoretical
NSF grant 133592 to RPNR. We also thank the anonymous                  model. Early Development and Parenting, 6:179‚Äì192.
reviewers for their helpful comments.                               [Nehaniv and Dautenhahn, 2002] Nehaniv, C. and Dauten-
                                                                       hahn, K. (2002). The correspondence problem. In Imita-
                          References                                   tion in Animals and Artifacts. MIT Press.
[Billard and Mataric, 2000] Billard, A. and Mataric, M. J.
   (2000). A biologically inspired robotic model for learn-         [Price, 2003] Price, B. (2003). Accelerating Reinforcement
   ing by imitation. In Sierra, C., Gini, M., and Rosenschein,         Learning with Imitation. PhD thesis, University of British
   J. S., editors, Proceedings of the Fourth International Con-        Columbia.
   ference on Autonomous Agents, pages 373‚Äì380, Barcelona,          [Rao and Meltzo , 2003] Rao, R. P. N. and Meltzo , A. N.
   Catalonia, Spain. ACM Press.                                        (2003). Imitation learning in infants and robots: Towards
[Blakemore et al., 1998] Blakemore, S. J., Goodbody, S. J.,            probabilistic computational models. In Proc. AISB.
   and Wolpert, D. M. (1998). Predicting the consequences
   of our own actions: the role of sensorimotor context esti-       [Rizzolatti et al., 2000] Rizzolatti, G., Fogassi, L., and
   mation. J. Neurosci., 18(18):7511‚Äì7518.                             Gallese, V. (2000). Mirror neurons: intentionality detec-
                                                                       tors? Int. J. Psychol., 35:205.
[Breazeal, 1999] Breazeal, C. (1999). Imitation as social ex-
   change between humans and robots. In Proc. AISB99,               [Roweis and Saul, 2000] Roweis, S. and Saul, L. (2000). Non-
   pages 96‚Äì104.                                                       linear dimensionality reduction by locally linear embed-
                                                                       ding. Science, 290(5500):2323‚Äì2326.
[Brooks and Meltzo , 2002] Brooks, R. and Meltzo , A.
   (2002). The importance of eyes: How infants interpet adult       [Scassellati, 1999] Scassellati, B. (1999).      Imitation and
   looking behavior. Developmental Psychology, 38:958‚Äì966.             mechanisms of joint attention: A developmental structure
                                                                       for building social skills on a humanoid robot. Lecture
[Byrne and Russon, 2003] Byrne, R. W. and Russon, A. E.                Notes in Computer Science, 1562:176‚Äì195.
   (2003). Learning by imitation: a hierarchical approach.
   Behavioral and Brain Sciences.                                   [Schaal et al., 2003] Schaal, S., Ijspeert, A., and Billard, A.
                                                                       (2003). Computational approaches to motor learning by
[Efros et al., 2003] Efros, A. A., Berg, A. C., Mori, G., and          imitation. Phil. Trans. Royal Soc. London: Series B,
   Malik, J. (2003). Recognizing action at a distance. In              358:537‚Äì547.
   ICCV ‚Äô03, pages 726‚Äì733.
                                                                    [Shon et al., 2003] Shon, A. P., Grimes, D. B., Baker, C. L.,
[Fong et al., 2002] Fong, T., Nourbakhsh, I., and Dauten-              and Rao, R. P. N. (2003). Bayesian imitation learning in
   hahn, K. (2002). A survey of socially interactive robots.           a robotic head. In NIPS (demonstration track).
   Robotics and Autonomous Systems, 42(3‚Äì4):142‚Äì166.
                                                                    [Tenenbaum, 1999] Tenenbaum, J. (1999). Bayesian model-
[Fox et al., 2003] Fox, D., Hightower, J., Liao, L., Schulz, D.,       ing of human concept learning. In Kearns, M. S., Solla,
   and Borriello, G. (2003). Bayesian ltering for location             S. A., and Cohn, D. A., editors, Advances in Neural Infor-
   estimation. IEEE Pervasive Computing.                               mation Processing Systems, volume 11. MIT Press, Cam-
                                                                       bridge, MA.
[Herrnstein, 1961] Herrnstein, R. J. (1961). Relative and ab-
   solute strength of responses as a function of frequency of       [Tenenbaum et al., 2000] Tenenbaum, J. B., de Silva, V.,
   reinforcement. J. Exp. Anal. Behaviour, 4:267‚Äì272.                  and Langford, J. C. (2000). A global geometric frame-
[Itti et al., 1998] Itti, L., Koch, C., and Niebur, E. (1998).         work for nonlinear dimensionality reduction. Science,
   A model of saliency-based visual attention for rapid scene          290(5500):2319‚Äì2323.
   analysis. IEEE PAMI, 20(11):1254‚Äì1259.                           [Todorov and Ghahramani, 2003] Todorov, E. and Ghahra-
[Jordan and Rumelhart, 1992] Jordan, M. I. and Rumelhart,              mani, Z. (2003). Unsupervised learning of sensory-motor
   D. E. (1992). Forward models: supervised learning with a            primitives. In Proc. 25th IEEE EMB.
   distal teacher. Cognitive Science, 16:307‚Äì354.
                                                                    [Visalberghy and Fragaszy, 1990] Visalberghy, E. and Fra-
[Kaelbling et al., 1996] Kaelbling, L. P., Littman, L. M., and         gaszy, D. (1990). Do monkeys ape? In Language and in-
   Moore, A. W. (1996). Reinforcement learning: A survey.              telligence in monkeys and apes: comparative developmental
   J. Artificial Intelligence Res., 4:237‚Äì285.                         perspectives, pages 247‚Äì273.
[Krebs and Kacelnik, 1991] Krebs, J. R. and Kacelnik, A.            [Wu et al., 2000] Wu, Y., Toyama, K., and Huang, T.
   (1991). Decision making. In Krebs, J. R. and Davies,                (2000). Wide-range, person- and illumination-insensitive
   N. B., editors, Behavioural Ecology (3rd edition), pages            head orientation estimation. In AFGR00, pages 183‚Äì188.
   105‚Äì137. Blackwell Scienti c Publishers.
                                                                1242

