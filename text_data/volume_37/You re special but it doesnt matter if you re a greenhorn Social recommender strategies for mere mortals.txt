                       You’re special, but it doesn’t matter if you’re a greenhorn:
                                Social recommender strategies for mere mortals
  Pantelis P. Analytis (analytis@mpib-berlin.mpg.de)                         Daniel Barkoczi (barkoczi@mpib-berlin.mpg.de)
                 Center for Adaptive Behavior and Cognition (ABC), Max Planck Institute for Human Development
                                                   Lentzeallee 94, 14195, Berlin, Germany
                                          Stefan M. Herzog (herzog@mpib-berlin.mpg.de)
                         Center for Adaptive Rationality (ARC), Max Planck Institute for Human Development
                                                   Lentzeallee 94, 14195, Berlin, Germany
                               Abstract                                   is neither an algorithm nor “big data” at hand. How can in-
                                                                          dividuals leverage the experience of other people when they
   From choosing a book to picking a restaurant, most choices
   people encounter are about “matters of taste” and thus no uni-         have no access to big data but access only to a relatively small
   versal, objective criterion about the options’ quality exists.         community of other people with whom they share some prior
   Tapping into the knowledge of individuals with similar tastes          experience about the available options?
   who have already experienced and evaluated options—as har-
   nessed by recommender system algorithms—helps people se-                  In this paper we make three contributions. First, we have
   lect options that they will enjoy. Although recommender sys-           undertaken an exercise in theory integration by mapping
   tems are available in some domains, for most everyday deci-            the striking conceptual similarities between seminal recom-
   sions there is neither an algorithm nor “big data” at hand. We
   mapped recommender system algorithms to models of human                mender system algorithms and both (i) models of judgment
   judgment and decision making about “matters of fact” and then          and categorization and (ii) models of social learning and so-
   recast the latter as social recommender strategies for “matters        cial decision making (from psychology, cognitive science,
   of taste”. This allowed us to investigate how people can lever-
   age the experiences of other individuals to make better deci-          judgment and decision making, anthropology, and biology).
   sions when no machine recommender systems are available.               Second, we have recast the latter two classes of models as
   Using computer simulations on a widely used data set from              social recommender strategies. Finally, based on this map-
   the recommender systems literature, we show that experienced
   individuals can benefit from relying on only the opinions of           ping, we have investigated how ordinary people can lever-
   seemingly similar people. Inexperienced individuals, in con-           age the experience of other people to make better decisions
   trast, are often well-advised to pick the mainstream option (i.e.,     about matters of taste. To this end we studied the inevitable
   the one with the highest average evaluation) even if there are
   interindividual differences in taste; this is because reliable es-     trade-off between (i) harnessing the apparent (dis)similarity
   timation of similarity requires considerable experience.               between people’s tastes—to discriminate between more and
   Keywords: Social learning; wisdom of crowds; expert crowd,             less relevant advisers—and (ii) estimating those similarities
   recommender systems; learning.                                         accurately enough. We have investigated how this trade-off
                                                                          evolves with the amount of experience a decision maker has
                           Introduction                                   (i.e., the number of options previously evaluated).
Where should I go for my next vacation? Which car should                     Outside of the recommender systems literature, social rec-
I buy? Most choices people encounter are about “matters of                ommender strategies remain an under-explored topic. Re-
taste” and thus no universal, objective criterion about the op-           search on advice taking, social learning, and judgment ag-
tions’ exists. How can people increase their chances of se-               gregation in psychology, cognitive science, judgment and de-
lecting options that they will enjoy?                                     cision making, anthropology, and biology has focused al-
   One promising approach is to tap into the knowledge of                 most exclusively on “matters of fact” where there is an ob-
other individuals who have already experienced and evaluated              jective criterion to be inferred (“wisdom of crowds”; e.g.,
options. The recommender systems community has lever-                     Larrick, Mannes, & Soll, 2012). To the best of our knowl-
aged this source of knowledge to develop collaborative fil-               edge, there are only a handful of studies on social recom-
tering methods, which estimate the subjective quality of op-              mender strategies (Van Swol, 2011; Yaniv, Choshen-Hillel, &
tions that people have not yet experienced (Resnick & Varian,             Milyavsky, 2011; Müller-Trede, Choshen-Hillel, Barneron,
1997; Adomavicius & Tuzhilin, 2005). One key insight is                   & Yaniv, 2015). They show that people rely on the similar-
that building recommendations based only on the evaluations               ity between themselves and their advisers when making deci-
of individuals similar to the target individual often improves            sions about matters of taste and that this is a good strategy.
the quality of the recommendations (e.g., Herlocker, Kon-
stan, Borchers, & Riedl, 1999)—where similarity between                    Mapping recommendation systems algorithms
two people is typically defined as the correlation in their eval-         to informational and social cue-based strategies
uations across options they have both evaluated.                          Table 1 displays several social recommender strategies that
   Although the consumer industry enables people to benefit               predict one’s own future evaluations based on the past evalu-
from recommender systems in some domains (e.g., choos-                    ations provided by other people.
ing a movie on Netflix), for many everyday decisions there
                                                                      1799

                                          Social recommender strategy                                                                 Parallels in the literature
   Strategy               Verbal description                      Formal definition                 Informational cues               Social cues                       Recommender systems
   Follow your            Find individual s with the                                                Take the best (Gigerenzer
                                                                                                                                     Imitate the best
  Doppelgänger           most similar taste and adopt                                              & Goldstein, 1996),                                                 Nearest neighbors
                                                                  ûi = us                                                          (Richerson & Boyd,
  (cf. Yaniv et al.,     that individual’s evaluations                                             single attribute, (Hogarth                                         (k = s = 1)
                                                                                                                                    2008)
  2011)                  as your own estimates.                                                    & Karelaia, 2005)
                                                                                                                                                                       Nearest neighbors
                          Average evaluations of all N                                                                               Averaging (Einhorn,              (k = N), often used as a
   Follow the                                                                                       Equal/unit weights
                         other individuals (i.e., go with         ûi = 1/N × ∑Nj=1 u j                                             Hogarth, & Klempner,              benchmark (e.g.,
  whole crowd                                                                                      (Dawes, 1979)
                         the mainstream).                                                                                           1977)                             Shardanand & Maes,
                                                                                                                                                                      1995)
                                                                                                                                     Select crowd (Mannes et           Nearest neighbors
   Follow your            Average evaluations of the k
                                                                  ûi = 1/k × ∑kj=1 u j                         –                   al., 2014), expert crowd          (1 < k < N; Shardanand
  clique                 most similar individuals.
                                                                                                                                    (Goldstein et al., 2014)          & Maes, 1995)
                          Average evaluations of all k                                                                                                                 Common implementation
   Follow your           individuals whose taste is                                                                                                                   of nearest neighbors
                                                                  ûi = 1/k × ∑kj=1 u j                         –                                  –
  similar crowd          correlated with yours above a                                                                                                                (Desrosiers & Karypis,
                         similarity threshold t.                                                                                                                      2011).
   Follow the
  similarity-                                                                                       Weighted average                                                   Weighted neighbors
                          Weight evaluations of all N                                                                                Weighted crowd
  weighted crowd                                                  ûi =      1
                                                                                   ∑Nj=1 w j × u j (Hammond, Hursch, &                                                (Resnick, Iacovou,
                         individuals according to their                  ∑Nj=1 w j                                                  (Davis-Stober et al.,
  (cf.                                                                                             Todd, 1964; Dana &                                                 Suchak, Bergstrom, &
                         similarity to your taste.                                                                                  2014)
  Müller-Trede et                                                                                  Dawes, 2004)                                                       Riedl, 1994)
  al., 2015)
                          Find k most similar options
                         (i.e., with similar evaluation
                                                                                                    Exemplar models                                                    Item-based algorithms
   Consider similar      profiles across people) and                         1
                                                                  ûi =
                                                                         ∑kl=1 wl
                                                                                   ∑kl=1 wl × ul   (Kruschke, 1992; Juslin &                       –                  (Sarwar, Karypis,
  options                weight your own evaluations
                                                                                                   Persson, 2002)                                                     Konstan, & Riedl, 2001)
                         for them according to their
                         similarity.
                          Select an individual r at
   Follow random
                         random and adopt that                                                      Minimalist (Gigerenzer           Random copying                    Occasionally used as
  other (cf. Gilbert                                              ûi = ur
                         individual’s evaluations as                                               & Goldstein, 1996)               (Cavalli-Sforza, 1981)            benchmark strategy
  et al., 2009)
                         your own estimates.
Table 1: Social recommender strategies conceptually similar to strategies using informational cues or social cues (i.e., people’s opinions as cues). Due to limited space we report only
representative references. All strategies first estimate the expected utility ûi (i.e., enjoyment) of each option i and then select the option with the highest estimated utility; when several
options have the same estimated utility, one of the tied options is chosen at random. Strategies incorporating similarity information are typeset in italics and those averaging across several
individuals’ evaluations are typeset in bold. All strategies are person based, except consider similar options, marked in blue.
                                                                        1800

   We identified conceptual similarities between the proposed             als with possibly very different—or even antithetical—tastes.
social recommender strategies (inspired by seminal algo-                  Alternatively, he could search for a movie that everybody
rithms from recommender systems research) and heuristics                  rated similarly to the target movie and then use his own eval-
and strategies for predicting matters of fact, where people               uation for that similar movie as a proxy (Consider similar
have access to either informational cues potentially related to           options; e.g., Spiderman is similar to Batman).
an objective criterion (e.g., number of movie theaters in a city
to predict its population size) or social cues (i.e., the opinions                                 Simulation study
of other people concerning the same objective criterion).                 We investigated the performance of the proposed social rec-
   This mapping emphasizes the close correspondence be-                   ommender strategies (see Table 1) by simulating their pre-
tween recommendation algorithms on the one hand and in-                   dictions for a large-scale, real-world data set. We varied the
formational and social cue-based strategies on the other. The             experience of the simulated decision makers (i.e., the num-
social recommender strategies can be placed in a continuum,               ber of options previously experienced in that domain; that is,
on the boundaries of which strategies rely either only on simi-           the number of rows in Table 2). As experience increased, the
larity information or only on aggregation of opinions (see Ta-            strategies relying on similarity could thus base their similarity
ble 1). As we move away from the boundaries the strategies                estimates on more data.1
rely increasingly on both these two fundamental principles.                  The social network from which a person could leverage
Below we illustrate some of the strategies using a fictional              vicarious experience would likely be much smaller than the
data set that has the same structure as the large-scale data sets         thousands of people available in typical recommender system
used in recommender systems research and in our own simu-                 data sets. The cognitive limit of the number of stable relation-
lation study below.                                                       ships that people can maintain is estimated to be around 250
                                                                          (Dunbar, 2010). We therefore opted to simulate small “com-
     Example: Deciding which movie to watch                               munities” of 250 members each to mirror this real-world fea-
      based on other people’s past experiences                            ture (as opposed to letting decision makers have access to all
Amit likes superhero movies and wants to watch Batman or                  other individuals in the population).
Fantastic Four. His friends have already seen both movies.
Furthermore, he and his friends have all watched and eval-                Method
uated several other movies (see Table 2). In addition to                  Dataset We used the funniness ratings of 100 jokes col-
any other contextual information (e.g., director, cast, movie             lected in the Jester data set. Jester2 was created by an online
length), Amit can use his friends’ evaluations to inform his              recommender system that allows Internet users to read and
movie choice.                                                             rate jokes. Users evaluated jokes on a scale ranging from not
                                                                          funny (–10) to funny (+10). At the beginning of the recom-
  Movie           John    Bob   Linda   Mary     Lou  Avg.   Amit
  Superman          3      4      2.5     4.5     3    3.8     2.5
                                                                          mendation process, a set of 10 jokes was presented to the user.
  Spiderman         4     4.5      3       2     3.5   3.4      3         Thereafter, Jester recommended jokes and continued to col-
  Batman            5      5       2       1      3    3.2      ?         lect ratings for each of them. The data set contains 4.1 million
  Fantastic Four    2      3      2.5      3     2.5   2.6      ?         evaluations of 100 jokes by 73,421 participants. In contrast to
  X-Men             1     1.5      2      1.5     3    1.8      2         other data sets studied by the recommender system commu-
Table 2: A typical recommender system problem. The movies are             nity, here a large number of participants evaluated all options.
rated on a scale of 1 to 5 (higher values indicate more positive rat-     Since its publication, the Jester data set has been used exten-
ings). Avg. = average.
                                                                          sively to study collaborative filtering algorithms.
   From Amit’s perspective, his own future evaluations are                Simulation procedure For simplicity we worked only with
the criterion values he seeks to maximize and the evalua-                 participants who evaluated all jokes (reducing the number of
tions of his friends are informational cues he can use to pre-            participants from 73,421 to 14,116). We randomly selected
dict his own future evaluations. Based on his past evalua-                14,000 participants in order to partition them into evenly
tions of the other movies, Amit thinks that he and Linda have             sized communities of 250 members each. In line with pre-
similar taste. If Linda truly were his “taste Doppelgänger”               vious work in the recommender system literature, we used
he could simply copy her evaluations and arrive at very ac-               the Pearson correlation coefficient as a measure of similarity
curate estimates of his own future enjoyment (Follow your                 (Herlocker, Konstan, Terveen, & Riedl, 2004).3
Doppelgänger). However, it is unclear to what extent this                    In each simulation run, we followed the following steps:
seeming similarity—based on only a small set of joint past                    1 A similar challenge is faced by recommender system algorithms
experiences—would generalize well to future cases. Amit                   when recommending options to new users about whom they know
may thus prefer to take the evaluations of others into account,           nothing or only very little. This challenge is commonly referred to
as well. For example, he could assign equal weights to all                as the user cold start problem (Ekstrand, Riedl, & Konstan, 2011).
                                                                              2 http://eigentaste.berkeley.edu
individuals and simply use the average evaluation (i.e., the                  3 The Pearson similarity coefficient between two individuals or
“mainstream” option; Follow the whole crowd). Yet by do-                                                               k
ing so he would also incorporate evaluations from individu-               two items i and j is defined as w(i, j) = k∑n=1√(uin −ūi )(u
                                                                                                                                     2
                                                                                                                                        jn −u¯j )
                                                                                                                                                  2
                                                                                                                    ∑n=1  (uin −ūi ) (u jn −u¯j )
                                                                      1801

Figure 1: Panel A: The performance of strategies as a decision maker’s experience with the domain of jokes (i.e., number of jokes previously
experienced and evaluated) increases; the strategies are grouped by color to those that rely primarily on aggregation (blue), those that rely
heavily on similarity information (red) and benchmark strategies (black) (see also Table 1). Panel B: Performance of the Follow your clique
strategy as a function of the experience with the domain (i.e., number of options experienced; x axis) and the size of the clique (i.e., number
of most similar people consulted; y axis). Note that Follow your Doppelgänger and Follow the whole crowd are special cases of this strategy
when the number of similar people consulted equals 1 and N, respectively. FD: Follow your Doppelgänger. FC: Follow your clique. FWC:
Follow the whole crowd.
First, we randomly generated 56 communities with 250 mem-                lar crowd—relying on similarity information more crudely—
bers each (14,000/250). Second, we randomly divided the                  performed slightly worse at 63%. Strategies relying solely on
jokes into a training (x jokes) and a test (10 jokes) set; this          either user similarity (Follow your Doppelgänger) or aggre-
assignment was the same for all individuals within all com-              gation (Follow the whole crowd) performed worse than the
munities. The strategies were then fitted on the training set.           other strategies, reaching 59% and 62%, respectively.
Individuals could access only advisers within their own com-
                                                                         The usefulness of less similar advisers These results pro-
munity. Third, for each individual (within all communities)
                                                                         vide a rationale for why people rely on similar advisers (Yaniv
we generated all 45 possible pair comparisons within the test
                                                                         et al., 2011; Müller-Trede et al., 2015). However, relying
set [10 × (10 − 1) ÷ 2] and examined the performance of the
                                                                         purely on similarity (Follow your Doppelgänger) does not
strategies in predicting which of the two jokes in a pair had a
                                                                         perform that well because of the difficulty of reliably esti-
higher evaluation for that individual, resulting in 45 pair com-
                                                                         mating similarity in light of sampling error. Mirroring re-
parisons per individual, 11,250 per community (45 × 250),
                                                                         sults from research on the wisdom of crowds (Goldstein et al.,
and 630,000 in total (11, 250 × 56). For each strategy we
                                                                         2014; Mannes et al., 2014), taking into account additional—
recorded the proportion of correct predictions. This proce-
                                                                         although less similar—advisers and averaging their recom-
dure was repeated 100 times and results were averaged. We
                                                                         mendations markedly improves performance.
investigated how the performance of the strategies changed
as a function of experience by repeating this procedure for              Experience within a domain Strategies that rely heavily
different numbers (x) of jokes experienced in the training set           on similarity information have steep learning curves. For
(varying from 5 to 90 in increments of 5).                               small amounts of experience, Follow your similar crowd is
                                                                         the best performing strategy. Consider similar options, Fol-
Results                                                                  low your clique, and Follow the similarity-weighted crowd
How did the strategies perform? Figure 1 shows the per-                  start to outperform Follow the whole crowd once approxi-
formance of each strategy as a function of the number of                 mately 15 options have been added to the training set and
options evaluated. For the highest level of experience the               Follow your similar crowd after approximately 25 options.
strategy based on item similarity (Consider similar options)                Thus, decision makers who have not yet experienced many
performed best (predicting 65% of the pair comparisons cor-              options are well-advised simply to aggregate the evaluations
rectly). This was followed by the strategies that relied on both         of individuals who seem to have at least minimally similar
similarity information and aggregation: Follow your clique               (i.e., positively correlated) tastes (Follow your similar crowd)
and Follow the similarity-weighted crowd predicted approx-               or even to unconditionally aggregate the evaluations of all in-
imately 64% of the cases correctly and Follow your simi-                 dividuals (Follow the whole crowd). Although the opinions
                                                                    1802

of truly similar individuals are more informative than those of             Experience and the bias–variance trade-off
truly dissimilar individuals, this discrimination is only benefi-           With increasing experience with the domain, the performance
cial to the extent that it is accurate enough. For small training           of all top-notch strategies increased—except for the wis-
samples, estimates of similarity are apparently often not ac-               dom of crowds strategy (Follow the whole crowd), which
curate enough to be of any use.                                             unconditionally averages across all people and is thus—by
How large should your clique be? When following your                        design—unaffected by the increasing accuracy of the sim-
clique, the size of k (i.e., the number of neighbors whose                  ilarity estimates. Such an averaging strategy assumes that
evaluations are averaged) is a hyperparameter that needs to                 everybody has the same taste and performs well to the ex-
be chosen beforehand (in Figure 1A we fixed k = 10). Figure                 tent that the tastes in the population are indeed homogeneous.
1B shows how performance changes as a function of k and                     From a bias–variance trade-off perspective (e.g., Gigerenzer
experience (i.e., the number of options experienced). With                  & Brighton, 2009; Geurts, 2010), this strategy suffers from
little experience it is better to rely on large cliques (ca. 100),          potentially high bias to the extent that its homogeneity as-
whereas for more extensive experience, performance peaks at                 sumption is wrong, but exhibits zero variance in its prediction
moderately sized cliques (ca. 30).                                          error because it does not estimate any free parameters.5
                                                                               In contrast, the strategies relying on similarity have a com-
The potential of one-reason decision making Following a                     paratively low bias because they can adapt to the homogene-
random other person correctly predicted 54% of the compar-                  ity or heterogeneity of tastes in the population. However, they
isons, which indicates a very modest, minimally shared sense                potentially suffer from variance because their predictions de-
of humor in the population (i.e., slightly better than chance).             pend on the similarity estimates—to differing degrees—and
We also tested another benchmark strategy that simply used                  thus they lie on a bias–variance continuum. At one extreme,
the length of a joke as a cue to predict its evaluation (i.e.,              a strategy of adopting the evaluations of only the seemingly
some people may prefer long, story-like jokes while others                  most similar person has the potential to profit from the vi-
may prefer short and witty linguistic puns). This one-cue                   carious experiences of one’s taste Doppelgänger but is most
strategy predicted 57% of cases correctly—it was almost as                  reliant on an accurate estimation of similarity. At the other
accurate as the Follow your Doppelgänger strategy (59%),                    extreme, a strategy of relying on a large crowd of at least
which also relied on one cue, yet a social one.4                            minimally similar people (i.e., with at least positively corre-
                                                                            lated tastes) is more biased but also more robust because it
                       General discussion
                                                                            depends on only roughly discriminating between similar and
Mapping out the striking conceptual similarities between                    dissimilar advisers (see also Goldstein et al., 2014; Mannes
seminal recommender system algorithms, on the one hand,                     et al., 2014).
and extant models of judgment and decision making (based
on informational or social cues), on the other, allowed us to               Theory integration: Reconnecting the cognitive
recast the latter models as social recommender strategies (see              sciences with recommender systems research
Table 1). This theory integration allowed us to analyze the                 New statistical tools haven often served as an inspiration for
performance of social recommender strategies for mere mor-                  the development of new psychological theories (Gigerenzer,
tals who have access to only a small pool of potential ad-                  1991). In the case of recommender systems, however, the in-
visers, rather than the "big data" available to recommender                 sights developed within the last two decades have not been
systems.                                                                    much incorporated into cognitive science6 —despite recom-
    Two results stand out. First, the successful strategies                 mender systems being widely available and relevant for ev-
all have one thing in common: They aggregate evaluations                    eryday decision making and seminal recommender systems
across several people (or items). Second, the amount of ex-                 being inspired by the work of cognitive scientists (Rich,
perience within a domain turns out to be a crucial determi-                 1979). We hope that the current paper initiates a cross-
nant of the success of strategies using similarity information.             fertilization between the two until now largely unconnected
Whereas experienced people can benefit from relying on only                 research streams.
the opinions of seemingly similar people, inexperienced peo-
ple are often well-advised to aggregate the evaluations of a                Context-based and hybrid recommender strategies
large set of people (picking the option with the highest aver-              In everyday life, people and machines have access to informa-
age evaluation either across all people or across at least min-             tion beyond their own and other people’s past experiences:
imally similar people) even if there are interindividual dif-               informational cues describing options and advisers (e.g., a
ferences in taste, because reliable estimation of similarity re-            movie’s genre and a person’s clothing style, respectively).
quires considerable experience.
                                                                                5 Also from a Bayesian perspective, it is prudent to go with the
    4 This result conflicts with a relevant finding from a speed dating     crowd: An inexperienced decision maker—by statistical necessity—
experiment (Gilbert et al., 2009), where the experience of a random         is a priori more likely than not to have “mainstream taste” unless
other person (from the same population) predicted the actual dating         there is diagnostic private information to the contrary (Herzog &
enjoyment better than the same participants’ predictions (based on          Hertwig, 2013, p. 210).
an extensive set of informational cues available before the speed date          6 In a similar vein, Analytis et al. (2014) pointed out the over-
started, namely, among other things, a picture and information about        looked analogy between ranking models from machine learning and
age, height, favorite movie, sport, book, song, and food).                  human search behavior.
                                                                        1803

The use of such contextual information has been examined                   Hammond, K. R., Hursch, C. J., & Todd, F. J. (1964). Analyzing the
in multiple-cue judgment and categorization learning in cog-                 components of clinical inference. Psychological Review, 71(6),
                                                                             438–456.
nitive science, in context-based recommender systems, and,                 Herlocker, J. L., Konstan, J. A., Borchers, A., & Riedl, J. (1999).
more generally, in supervised learning in machine learning.                  An algorithmic framework for performing collaborative filtering.
   People might thus improve their predictions about matters                 In Proceedings of the 22nd annual international ACM SIGIR con-
                                                                             ference on research and development in information retrieval (pp.
of taste by using informational cues (i) to take advantage of                230–237).
the predictive information in the options’ features themselves             Herlocker, J. L., Konstan, J. A., Terveen, L. G., & Riedl, J. T. (2004).
(e.g., it’s a superhero movie) or (ii) to improve their assess-              Evaluating collaborative filtering recommender systems. ACM
                                                                             Transactions on Information Systems (TOIS), 22(1), 5–53.
ment of advisers’ similarities (e.g., by looking at their cloth-           Herzog, S. M., & Hertwig, R. (2009). The wisdom of many in one
ing style). Such context-based—or even hybrid—approaches                     mind: Improving individual judgments with dialectical bootstrap-
might further improve people’s ability to make good deci-                    ping. Psychological Science, 20(2), 231–237.
                                                                           Herzog, S. M., & Hertwig, R. (2013). The ecological validity of flu-
sions for themselves by taking advantage of different sources                ency. In C. Unkelbach & R. Greifeneder (Eds.), The experience
of information and different approaches (see also Herzog &                   of thinking: How the fluency of mental processes influences cog-
Hertwig, 2009; Herzog & von Helversen, 2013; Herzog &                        nition and behavior (pp. 190–219). London: Psychology Press.
                                                                           Herzog, S. M., & Hertwig, R. (2014). Harnessing the wisdom of the
Hertwig, 2014).                                                              inner crowd. Trends in Cognitive Sciences, 18(10), 504–506.
                                                                           Herzog, S. M., & von Helversen, B. (2013). Blending and choosing
                                                                             within one mind: Should judgments be based on exemplars, rules,
                       Acknowledgments                                       or both? In M. Knauff, M. Pauen, N. Sebanz, & I. Wachsmuth
                                                                             (Eds.), Cooperative minds: Social interaction and group dynam-
We would like to thank the members of the ABC group for                      ics. Proceedings of the 35th Annual Conference of the Cognitive
their constructive comments during internal presentations and                Science Society (pp. 2536–2541). Austin, TX: Cognitive Science
Anita Todd for editing the manuscript.                                       Society.
                                                                           Hogarth, R. M., & Karelaia, N. (2005). Ignoring information in
                                                                             binary choice with continuous variables: When is less “more”?
                            References                                       Journal of Mathematical Psychology, 49(2), 115–124.
                                                                           Juslin, P., & Persson, M. (2002). PROBabilities from EXemplars
Adomavicius, G., & Tuzhilin, A. (2005). Toward the next gen-                 (PROBEX): A “lazy” algorithm for probabilistic inference from
   eration of recommender systems: A survey of the state-of-the-art          generic knowledge. Cognitive Science, 26(5), 563–607.
   and possible extensions. Knowledge and Data Engineering, IEEE           Kruschke, J. K. (1992). ALCOVE: An exemplar-based connec-
   Transactions on, 17(6), 734–749.                                          tionist model of category learning. Psychological Review, 99(1),
Analytis, P. P., Kothiyal, A., & Katsikopoulos, K. (2014). Multi-            22–44.
   attribute utility models as cognitive search engines. Judgment and      Larrick, R. P., Mannes, A. E., & Soll, J. B. (2012). The social psy-
   Decision Making, 9(5), 403–419.                                           chology of the wisdom of crowds. In J. I. Krueger (Ed.), Frontiers
Cavalli-Sforza, L. L. (1981). Cultural transmission and evolution:           in social psychology: Social judgment and decision making (pp.
   A quantitative approach (No. 16). Princeton University Press.             227–242). New York, NY: Psychology Press.
Dana, J., & Dawes, R. M. (2004). The superiority of simple al-             Mannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wisdom
   ternatives to regression for social science predictions. Journal of       of select crowds. Journal of Personality and Social Psychology,
   Educational and Behavioral Statistics, 29(3), 317–331.                    107(2), 276–299.
Davis-Stober, C. P., Budescu, D. V., Dana, J., & Broomell, S. B.           Müller-Trede, J., Choshen-Hillel, S., Barneron, M., & Yaniv, I.
   (2014). When is a crowd wise? Decision, 1(2), 79–101.                     (2015). The wisdom of crowds for matters of taste. Manuscript
Dawes, R. M. (1979). The robust beauty of improper linear models             submitted for publication.
   in decision making. American Psychologist, 34(7), 571–582.              Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., & Riedl, J.
Desrosiers, C., & Karypis, G. (2011). A comprehensive survey of              (1994). Grouplens: An open architecture for collaborative filter-
   neighborhood-based recommendation methods. In Recommender                 ing of netnews. In Proceedings of the 1994 acm conference on
   systems handbook (pp. 107–144). Springer.                                 computer supported cooperative work (pp. 175–186).
Dunbar, R. (2010). How many friends does one person need?:                 Resnick, P., & Varian, H. R. (1997). Recommender systems. Com-
   Dunbar’s number and other evolutionary quirks. Faber & Faber.             munications of the ACM, 40(3), 56–58.
Einhorn, H. J., Hogarth, R. M., & Klempner, E. (1977). Quality of          Rich, E. (1979). User modeling via stereotypes. Cognitive Science,
   group judgment. Psychological Bulletin, 84(1), 158–172.                   3(4), 329–354.
Ekstrand, M. D., Riedl, J. T., & Konstan, J. A. (2011). Collabora-         Richerson, P. J., & Boyd, R. (2008). Not by genes alone: How cul-
   tive filtering recommender systems. Foundations and Trends in             ture transformed human evolution. University of Chicago Press.
   Human-Computer Interaction, 4(2), 81–173.                               Sarwar, B., Karypis, G., Konstan, J., & Riedl, J. (2001). Item-based
Geurts, P. (2010). Bias vs variance decomposition for regression and         collaborative filtering recommendation algorithms. In Proceed-
   classification. Data Mining and Knowledge Discovery Handbook,             ings of the 10th international conference on world wide web (pp.
   733–746.                                                                  285–295).
Gigerenzer, G. (1991). From tools to theories: A heuristic of discov-      Shardanand, U., & Maes, P. (1995). Social information filtering:
   ery in cognitive psychology. Psychological Review, 98(2), 254–            Algorithms for automating “word of mouth”. In Proceedings of
   267.                                                                      the SIGCHI conference on human factors in computing systems
Gigerenzer, G., & Brighton, H. (2009). Homo heuristicus: Why                 (pp. 210–217).
   biased minds make better inferences. Topics in Cognitive Science,       Van Swol, L. M. (2011). Forecasting another’s enjoyment versus
   1(1), 107–143.                                                            giving the right answer: Trust, shared values, task effects, and
Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning the fast and            confidence in improving the acceptance of advice. International
   frugal way: Models of bounded rationality. Psychological Re-              Journal of Forecasting, 27(1), 103–120.
   view, 103(4), 650–669.                                                  Yaniv, I., Choshen-Hillel, S., & Milyavsky, M. (2011). Receiving
Gilbert, D. T., Killingsworth, M. A., Eyre, R. N., & Wilson, T. D.           advice on matters of taste: Similarity, majority influence, and taste
   (2009). The surprising power of neighborly advice. Science,               discrimination. Organizational Behavior and Human Decision
   323(5921), 1617–1619.                                                     Processes, 115(1), 111–120.
Goldstein, D. G., McAfee, R. P., & Suri, S. (2014). The wisdom
   of smaller, smarter crowds. In Proceedings of the fifteenth ACM
   conference on economics and computation (pp. 471–488).
                                                                       1804

