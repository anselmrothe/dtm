Helping Students Understand Posterior Probabilities:
Research with a Digital Learning Environment on the Monty Hall Dilemma
Lore Saenena (Lore.Saenen@ppw.kuleuven.be),
Mieke Heyvaerta (Mieke.Heyvaert@ppw.kuleuven.be),
Wim Van Doorenb (Wim.VanDooren@ppw.kuleuven.be),
& Patrick Onghenaa (Patrick.Onghena@ppw.kuleuven.be)
a

Methodology of Educational Sciences, KU Leuven, Tiensestraat 102, 3000 Leuven
Instructional Psychology and Technology, KU Leuven, Dekenstraat 2, 3000 Leuven, Belgium

b

Abstract
When initially confronted with the Monty Hall dilemma
(MHD), people show a very strong tendency to stick with
their initial choice, although switching maximizes winning
chances. Previous research demonstrated that certain
interventions helped participants to discover and apply the
optimal strategy, but generally failed to increase participants’
understanding of the MHD solution. An exception on the
latter finding is DiBattista’s (2011) digital learning
environment study, reporting that the majority of participants
who used the learning environment learned to understand the
MHD solution. However, a major shortcoming was
DiBattista’s (2011) methodology, which did not allow to infer
causal relations and to conclude which (combination of)
manipulation(s) was most important for participants’
understanding of the MHD solution. The aim of the present
study was to fill this research gap by conducting a controlled
randomized experiment with an analogous digital learning
environment. Participants were high-school students between
16 and 19 years old. The results showed that receiving
explanation about the MHD solution was the most important
manipulation to improve understanding. Implications for
education in (posterior) probability are discussed.
Keywords: Monty Hall dilemma; probability; posterior
probability; digital learning environment; experience-based
learning; traditional learning

Introduction
The Monty Hall dilemma (MHD) was adapted from the
popular TV game show Let’s Make a Deal and is known as
one of the most counterintuitive posterior probability
problems (Friedman, 1998). In the classic version of the
MHD, a guest is confronted with three identical doors. One
door conceals a valuable prize, usually a car. The two
remaining doors conceal worthless prizes such as goats.
After the guest initially chooses one door, the host, who is
aware of the location of the prize, opens a non-chosen door
to show that there is a worthless prize behind it. Next, the
guest faces a dilemma when the host asks him to either stay
with his initial choice, or to switch to the other unopened
door. By applying Bayes’ Theorem with the correct prior
and marginal likelihoods, it can be derived that switching is
the strategy that maximizes the probability to win the
valuable prize. More specifically, switching yields a
posterior winning probability of 2/3, whereas staying only
yields a 1/3 posterior winning probability.

Previous research on the MHD has provided strong
evidence for the following four findings. First, there exists a
strong sticking tendency: When first confronted with the
dilemma, the vast majority of participants choose to stay
with the initial choice (Burns & Wieth, 2004; Friedman,
1998; Granberg & Brown, 1995; Granberg & Dorr, 1998).
Cross-cultural research revealed that staying percentages
range between 79% and 87% (Granberg, 1999). These high
percentages indicate how extremely counterintuitive the
MHD solution is.
Second, participants have a strong belief that their choice
– either staying or switching – does not matter, because they
consider both posterior probabilities as being equal (FrancoWatkins, Derks, & Dougherty, 2003; Granberg & Brown,
1995; Stibel, Dror, & Ben-Zeev, 2009). Participants’
preference to stay with their initial choice, despite the fact
that they judge winning probabilities for staying and
switching as equally large, can be explained by the larger
amount of regret participants anticipate to experience after a
loss due to switching compared with a loss due to staying
(Stibel et al., 2009).
Third, previous research has shown that many factors can
alter participants’ sticking tendency. For example, switching
behavior is more likely to occur when more alternatives are
included in the problem compared with only three
alternatives in the classic MHD (Burns & Wieth, 2004;
Franco-Watkins et al., 2003; Saenen, Heyvaert, Grosemans,
Van Dooren, & Onghena, 2014; Stibel et al., 2009). Also
repeated experience with the problem has a strong impact:
When participants are given a series of MHD trials,
switching rates increase across trials, showing that people
adjust their behavior to increase the gain (e.g., FrancoWatkins et al., 2003; Petrocelli, 2013; Petrocelli & Harris,
2011; Saenen, Van Dooren, & Onghena, 2015a).
Importantly, none of these studies, containing the repeated
experience with the dilemma, led participants to consistently
switch on all trials. Thus, optimal behavioral performance
was not observed.
Fourth, there exists a dissociation between participants’
behavioral MHD performance and their understanding of
the problem’s solution. Some studies did not only examine
participants’ behavioral MHD performance, but also asked
participants to estimate the posterior probabilities in order to
investigate their MHD understanding (Burns & Wieth,
2003, 2004; DiBattista, 2011; Franco-Watkins et al., 2003;

2057

Saenen et al., 2014, 2015a; Saenen, Heyvaert, Van Dooren,
& Onghena, 2015b; Stibel et al., 2009). The results showed
that although behavioral performance was easily improved
by adding particular interventions, correct posterior
probability estimates ranged from 0% to 50%. Thus, overall,
participants still failed to understand the MHD and its
underlying probabilities (Burns & Wieth, 2003, 2004;
Franco-Watkins et al., 2003; Saenen et al., 2014, 2015a,
2015b; Stibel et al., 2009).
DiBattista (2011) developed a digital learning
environment aimed at tackling people’s general inability to
understand to MHD solution. Its characteristics were
specially designed to increase people’s understanding of the
MHD solution and can be described as follows. First, in the
‘playing’ part, one could complete as many trials as one
wanted of both a 3-door and 20-door MHD. After each trial,
feedback about the number of trials one had won and lost,
conditional on the behavior (i.e., staying or switching), was
updated. Second, in the ‘simulation’ part, one could ask the
computer to generate N trials of both a 3-door and 20-door
MHD and choose the desired type of behavior (i.e., always
staying, always switching, or alternating between staying
and switching). In this part, constantly updated feedback
was also provided. Third, in the ‘explanation’ part, one
could access explanations for both the 3-door and 20-door
MHD solution.
In DiBattista’s (2011) study, participants solved the
classic MHD in paper-and-pencil format as a pretest
measure. Participants were asked to indicate the optimal
behavioral response in order to win the prize (staying,
switches, or it makes no difference) and to explain in detail
their reasoning behind their chosen response. Next, the
participants were motivated to use the digital learning
environment with unlimited access for a period of five
weeks. Hereafter, the participants completed a 6-door
variant of the MHD as a posttest measure. Another four
weeks later (i.e., nine weeks after the pretest), participants
again completed the classic MHD – identical to the pretest –
as a follow-up measure. The results of DiBattista’s (2011)
pretest-posttest study revealed that at the pretest, only 4.5%
of the participants correctly indicated switching as the
optimal behavioral response and none of them could give a
satisfactory explanation for why switching was beneficial.
For the posttest and follow-up measure, the answers of
participants who accessed the digital learning environment
at least once were compared with the answers of those who
never accessed it. The results were impressive: At the
posttest, participants who accessed the digital learning
environment gave the optimal behavioral response and a
satisfactory explanation statistically significantly more often
compared with participants who never accessed it (77.5%
and 61.2% vs. 41.4% and 13.8% respectively). At the
follow-up, no statistically significant difference was found
on how often the optimal behavioral response was given
between participants who accessed the digital learning
environment and those who never accessed it (89.4% and
87.5% respectively). However, participants who accessed

the digital learning environment statistically significantly
more often gave a satisfactory explanation for why
switching was beneficial, compared to those who never
accessed it (62.7% and 6.2% respectively). No other
empirical study so far ever reported percentages of
participants understanding the MHD solution as high as
61.2% and 62.7%.
A major shortcoming to DiBattista’s (2011) study is that
the characteristics that were designed to promote the
understanding of the MHD solution were not systematically
manipulated between (or within) participants. Next, the
study was not conducted in a controlled environment, and
its use was not experimentally manipulated. Moreover,
participants’ use of the digital learning environment was
self-selected and thus not randomly assigned. Thus, it is
impossible to infer causal relations and to conclude which
(combination of) manipulation(s) was most important to
improve participants’ understanding of the MHD solution.
To investigate that question, we developed our own MHD
digital learning environment, analogous to the one
developed by DiBattista (2011), and conducted various
controlled randomized experiments. This paper presents the
results of our first experiment, in which we examined the
effects of both repeated experience with the MHD and
explanation. The choice for the inclusion of repeated
experience was made because there is already a lot of
research literature available on this manipulation (e.g.,
Franco-Watkins et al., 2003; Petrocelli, 2013; Petrocelli &
Harris, 2011; Saenen et al., 2015a), which makes it easy to
compare our results. The choice for the inclusion of
explanation about the MHD solution as a manipulation was
made because of the practical relevance for education (see
discussion section).

Methods
Participants and Design
Two-hundred and thirteen Flemish high-school students
participated in the experiment. Seventy-eight of them were
excluded from the data analyses because of prior familiarity
with the MHD. As a result, our final sample consisted of
135 participants (80 females, 55 males; age range: 16-19
years, Mage = 16.92, SDage = 0.54).
Participants were randomly assigned to one of four
conditions, created by a 2 × 2 between-subjects design. The
first independent variable was ‘Explanation’ and indicated
whether or not participants could access the ‘explanation
part’ of the MHD game. The second independent variable
was ‘Playing’ and indicated whether or not participants
could access the ‘playing part’ of the MHD game. This led
to the following four conditions: control condition (neither
explanation nor playing), ‘playing only’ condition (playing,
but no explanation), ‘explanation only’ condition
(explanation, but no playing), ‘playing and explanation’
condition (both playing and explanation). Data of
respectively 28, 38, 34, and 35 participants were included in
the analyses.

2058

The study protocol was approved by the Ethical
Committee of the KU Leuven − University of Leuven.

Materials
The following materials were used in the study: a paperand-pencil questionnaire operating as the pretest measure,
another paper-and-pencil questionnaire operating as the
posttest measure, and a digital learning environment.
Our pretest questionnaire included the classic MHD, as in
DiBattista’s (2011) study. Participants were asked to answer
three questions. First, participants were asked to indicate the
optimal behavioral response (i.e., question 1) by choosing
between one of three options: staying, switching, or it does
not matter. This behavioral response question was the same
as in DiBattista’s (2011) study. Unlike DiBattista (2011),
we operationalized understanding of the MHD solution by
asking participants to estimate the posterior winning
probability for both staying (i.e., question 2) and switching
(i.e., question 3), instead of letting them explain their
reasoning behind the behavioral response answer they gave
on question 1.
In our posttest questionnaire, we included the items that
DiBattista (2011) used for his posttest and follow-up
measure. Thus, our own posttest questionnaire included two
items. The first item was a 6-door MHD variant with one
prize (cf. DiBattista’s (2011) posttest), in which the
participant initially selected two doors, the host then opened
three other non-winning doors, and the participant then was
faced with the dilemma to either stay with his two initially
selected doors (winning the prize when located behind one
of those two doors), or to switch to the one remaining
unopened door. Note that the posterior probabilities of this
6-door MHD variant are equal to the posterior probabilities
of the classic MHD: Staying leads to winning the prize in
1/3 of the cases, while switching yields a 2/3 winning
probability. The second item of our posttest questionnaire
was the classic MHD (cf. DiBattista’s (2011) follow-up
measure), completely identical to the item of the pretest
questionnaire. For both items of our posttest questionnaire,
participants were asked to complete the same three
questions as in our pretest questionnaire. Summarized, for
all MHD items there were three dependent variables. The
first dependent variable was the behavioral response, the
second one was the posterior winning probability for
staying, and the third one was the posterior winning
probability for switching.
The MHD digital learning environment we created1 was
analogous to the one developed by DiBattista (2011) and
contained the same three major parts: a ‘playing’ part, a
‘simulation’ part, and an ‘explanation’ part. In the ‘playing’
and ‘simulation’ parts, feedback about the number of trials
one had won or lost, conditional on the behavior (i.e.,
staying or switching), was constantly updated and provided.
In the ‘explanation’ part, the MHD solution was explained
1

Researchers interested in using our digital learning environment
for research and/or educational purposes can contact the authors.

stepwise by providing little information at a time on each
screen. Both forward and backward navigation was possible
in the ‘explanation’ part.
When opening the digital learning environment, a
description of the classic MHD was always presented first
in which all necessary elements were mentioned: the host,
the three doors, the random location of the prize, the
participant’s initial choice, followed by the host opening
another door than the one chosen by the participant showing
it did not contain the prize, and ultimately the participant’s
final choice. Next, the same description was given for a 20door MHD variant in which the host, after the participant
made an initial choice, opened 18 other doors that did not
contain the prize. After navigating through the descriptions
of both the classic MHD and the 20-door MHD variant, the
participant got to see a menu bar that listed the specific parts
of the digital learning environment the participant could use.
Which parts were listed in the menu bar depended on the
condition a participant was assigned to. For example, a
participant assigned to the ‘playing only’ condition only saw
the ‘playing: 3 doors’ and the ‘playing: 20 doors’ parts
listed in the menu bar, whereas a participant assigned to the
‘explanation only’ condition only saw the ‘explanation: 3
doors’ and the ‘explanation: 20 doors’ parts. Thus, in
contrast to DiBattista’s (2011), in our digital learning
environment, it was possible to limit participants’ access to
particular parts of the learning environment. Also in contrast
with DiBattista (2011), it was possible to set time
limitations on the use of our digital learning environment.
These adaptions were made in order to conduct controlled
randomized experiments.

Procedure
Participants came to the laboratory in groups of eight for the
experiment. Before the start of each experimental session,
the experimenter placed an informed consent form and a
laptop on eight separate tables. Tables and seats were placed
so that no interaction was possible between participants.
Upon arriving, the experimenter asked the participants to
take place at one of the eight tables on which there was an
informed consent form. Participants were randomly
assigned to the experimental conditions, with the limitation
that in each experimental session two participants were
assigned to each of the four different conditions. After
completing the informed consent form, participants received
the pretest questionnaire. Next, the six participants that were
assigned to an experimental condition (i.e., ‘playing only’
condition, ‘explanation only’ condition, or ‘playing and
explanation’ condition) were asked to use the digital
learning environment for a duration of 20 minutes.
How the participants exactly spent and divided those 20
minutes between the different parts of the digital learning
environment that were made available, was up to the
participants themselves. After 20 minutes, the digital
learning environment automatically stopped working. Next,
the participants received the posttest questionnaire from the
experimenter. During the 20 minutes that the six

2059

participants assigned to an experimental condition used the
digital learning environment, the two participants assigned
to the control condition immediately completed the posttest
questionnaire. Afterwards, they were asked to use the digital
learning environment as well (with unlimited access) so that
they would keep quiet during the remaining time of the
experimental session. An entire experimental session lasted
approximately 50 minutes.

Table 2: Logistic regression analysis results for variables
predicting outcomes on the classic MHD pretest questions.
β

Statistical Analysis
To investigate participants’ behavioral responses and
understanding of the underlying posterior probabilities of
the MHD, we performed a logistic regression analysis with
‘Explanation’, ‘Playing’, and the interaction between
‘Explanation’ and ‘Playing’ as predictors. The significance
level was set at α = .05. To follow up on statistically
significant effects, post-hoc pairwise comparisons were
performed using a Tukey-Kramer (HSD) correction.

SE β

OR

95% Wald
CI

Wald
χ²(1)

Optimal behavior
Play
0.37
0.57
1.44 [-0.76; 1.49]
0.40
Explain
-0.75
0.68
0.47 [-2.08; 0.57]
1.24
Play*Explain
0.68
0.89
1.97 [-1.08; 2.43]
0.57
P (win | stay)
Play
-0.03
0.75
0.97 [-1.51; 1.44]
0.00
Explain
0.09
0.75
1.10 [-1.38; 1.56]
0.02
Play*Explain -0.58
1.04
0.56 [-2.63; 1.46]
0.31
P (win | switch)
Play
-0.73
0.90
0.48 [-2.50; 1.04]
0.65
Explain
-0.28
0.95
0.75 [-2.14; 1.57]
0.09
Play*Explain
0.39
1.25
1.48 [-2.05; 2.83]
0.10
Note. β = unstandardized β coefficients; SE = standard error; OR =
odds ratio; CI = confidence interval.

Posttest: Classic MHD

Results
Pretest: Classic MHD
For each dependent variable and each of the four conditions,
percentages correct answers are presented in Table 1. As
can be derived from Table 1, participants performed poorly
on both the behavioral response question and the posterior
probability estimate questions during the pretest.
The results of the logistic regression analyses (see
Table 2) showed no statistically significant differences
between the conditions before the intervention started,
which is consistent with our random assignment scheme.
Table 1: Percentages correct answers given on the items
of both the pretest and posttest questionnaire.

Explanation
and playing

25.0
17.9
10.7

26.5
11.8
11.8

10.5
10.5
7.9

20.0
11.4
5.7

25.0
17.9
10.7

88.2
84.8
81.8

68.4
39.5
21.1

94.3
79.4
61.8

28.6
28.6
46.4

76.5
70.6
73.5

39.5
43.2
18.9

62.9
66.7
54.5

Playing
only

Explanation
only

Dependent variable
Pretest: Classic MHD
Optimal behavior
P (win | stay)
P (win | switch)
Posttest: Classic MHD
Optimal behavior
P (win | stay)
P (win | switch)
Posttest: 6-door MHD
Optimal behavior
P (win | stay)
P (win | switch)

Control

Condition

For the 3-door MHD in the posttest Table 1 clearly shows
that in all experimental conditions participants performed
better compared with participants assigned to the control
condition. Participants assigned to the ‘playing and
explanation’ condition performed best on the optimal
behavioral response question, whereas participants assigned
to the ‘explanation only’ condition performed best on both
posterior probability questions.
First, the results of the logistic regression analyses (see
Table 3) showed a statistically significant main effect of
Explanation on behavioral response, Wald χ²(1) = 6.32, p =
.012. The odds ratio equaled 7.61, meaning that it is 7.6
times more probable that a participant indicates switching as
the optimal behavioral response when assigned to a
condition with explanation compared to a condition without
explanation. Second, there were also statistically significant
main effects of Explanation on the posterior winning
probability when staying and when switching questions,
Wald χ²(1) = 10.89, p = .001, OR = 5.91, and Wald χ²(1) =
11.47, p = .001, OR = 6.06, respectively. Those odds ratios
mean that it is approximately 6 times more probable that a
participant gives a correct posterior winning probability
estimation for both staying and switching when assigned to
a condition with explanation compared to a condition
without explanation. Finally, there was a statistically
significant interaction effect on the posterior winning
probability when switching question, Wald χ²(1) = 3.87, p =
.049, OR = 6.19. To follow up on this interaction effect,
post-hoc pairwise comparisons (HSD) revealed that the
following comparisons reached statistical significance.
Participants in the ‘explanation only’ condition more often
gave the correct answer on the posterior winning probability
when switching question compared with participants
assigned to the ‘playing only’ condition, p < .001, OR =
11.905, and compared with participants assigned to the
control condition, p < .001, OR = 3.205. In addition,
participants in the ‘playing and explanation’ condition more
often correctly answered the posterior winning probability

2060

when switching question compared with participants
assigned to the ‘playing only’ condition, p < .001, OR =
5.143, and compared with participants assigned to the
control condition, p < .001, OR = 1.385.

participant indicates switching as the optimal behavioral
response and gives the correct answer on the posterior
winning probability when switching question when assigned
to a condition with explanation compared to a condition
without explanation.

Posttest: 6-door MHD
For the 6-door MHD variant in the posttest Table 1
demonstrates that especially participants assigned to the
‘playing and explanation’ condition and the ‘explanation
only’ condition performed better compared with the control
condition. Participants assigned to the ‘explanation only’
condition performed best on all three questions.
The logistic regression analyses (see Table 4) indicated
statistically significant main effects of Explanation on
behavioral response, Wald χ²(1) = 3.91, p = .048, OR =
2.60, and on the posterior winning probability for switching
question, Wald χ²(1) = 8.99, p = .003, OR = 5.14. Thus, it
is respectively 2.6 and 5.1 times more probable that a
Table 3: Logistic regression analysis results for variables
predicting outcomes on the classic MHD posttest questions.
β

SE β

OR

95% Wald
CI

Wald
χ²(1)

Optimal behavior
Play
0.79 0.90 2.20 [-0.98; 2.56]
0.76
*
Explain
2.03 0.81 7.61
[0.45; 3.61]
6.32
Play*Explain
1.08 1.06 2.95 [-1.00; 3.16]
1.04
P (win | stay)
Play
-0.37 0.64 0.69 [-1.64; 0.89]
0.33
**
Explain
1.78 0.54 5.91
[0.72; 2.83]
10.89
Play*Explain
1.47 0.88 4.35 [-0.25; 3.19]
2.82
P (win | switch)
Play
-1.03 0.57 0.36 [-2.15; 0.10]
3.20
**
Explain
1.80 0.53 6.06
[0.76; 2.84]
11.47
*
Play*Explain
1.82 0.93 6.19
[0.01; 3.64]
3.87
Note. β = unstandardized β coefficients; SE = standard error; OR =
odds ratio; CI = confidence interval.
*
**
p < .05. p < .01.

Table 4: Logistic regression analysis results for variables
predicting outcomes on the 6-door MHD posttest questions.
β

SE β

OR

95% Wald
CI

Wald
χ²(1)

Optimal behavior
Play
-0.65 0.53 0.52 [-1.70; 0.40]
1.49
*
Explain
0.95 0.48 2.60
[0.01; 1.90]
3.91
Play*Explain
1.14 0.76 3.13 [-0.34; 2.62]
2.28
P (win | stay)
Play
-0.18 0.53 0.83 [-1.22; 0.85]
0.12
Explain
0.97 0.50 2.62 [-0.01; 1.94]
3.78
Play*Explain
0.83 0.75 2.29 [-0.64; 2.30]
1.21
P (win | switch)
Play
-0.84 0.52 0.43 [-1.86; 0.19]
2.58
**
Explain
1.64 0.55 5.14
[0.57; 2.71]
8.99
Play*Explain -0.47 0.77 0.62 [-1.98; 1.04]
0.38
Note. β = unstandardized β coefficients; SE = standard error; OR =
odds ratio; CI = confidence interval.
*
**
p < .05. p < .01.

Discussion
Previous research on the MHD demonstrated that although
participants’ behavioral performance could be enhanced by
particular interventions, participants’ understanding of the
MHD solution did not improve very much (Burns & Wieth,
2003, 2004; Franco-Watkins et al., 2003; Saenen et al.,
2014, 2015a, 2015b; Stibel et al., 2009). So far, only
DiBattista’s (2011) study showed a major increase in
participants’ understanding of the MHD solution. In his
study, participants used an MHD digital learning
environment, developed to improve participants’
understanding of the problem’s solution. The problem with
DiBattista’s (2011) study, however, is that it could not
answer the question which (combination of) manipulation(s)
of the digital learning environment exactly was most helpful
to increase participants’ understanding of the MHD
solution. This is because his study was not conducted in a
controlled environment, the several characteristics of the
digital learning environment were not experimentally
manipulated, and there was no random assignment of his
participants.
With the present study, we aimed to fill (part of) this
research gap and to extend DiBattista’s (2011) research. To
this end, we developed our own digital learning
environment – analogous to the one developed by DiBattista
(2011) – in which it was possible to limit participants’
access to particular parts of the learning environment so that
it would be possible to conduct controlled randomized
experiments and next, to infer causal relations. The current
paper reports on the first experiment we carried out, in
which we focused on two out of the three major parts of the
digital learning environment: repeated experience with the
MHD and explanation about the MHD solution.
First, the results of our experiment are consistent with
previous research on the MHD. At the pretest measure,
participants in all conditions massively failed to indicate the
optimal behavioral response (see Burns & Wieth, 2004;
Friedman, 1998; Granberg, 1999; Granberg & Brown, 1995;
Granberg & Dorr, 1998) and to give correct posterior
winning probability estimates (see Burns & Wieth, 2003,
2004; Franco-Watkins et al., 2003; Saenen et al., 2014,
2015a, 2015b; Stibel et al., 2009). Next, the results of our
posttest measure showed that when participants completed a
series of MHD trials without receiving further explanation
about the MHD solution (i.e., ‘playing only’ condition),
their behavioral response improved, but the majority of
participants still did not grasp the underlying posterior
probabilities of the problem (see Franco-Watkins et al.,
2003; Saenen et al., 2015b).
Second and most important, our study showed which
specific manipulation helped participants most in

2061

understanding the MHD solution. The results provide strong
evidence for the effect of receiving explanation about the
MHD solution. Interestingly, being able to experience
multiple MHD trials – besides having access to explanation
about the MHD solution – did not further increase
participants’ MHD understanding nor did it affect their
understanding in a negative way. This finding is of practical
importance for (posterior) probability education. Although
experience-based learning may occur in many areas, it
appears that repeated experience with the MHD is not
enough to help participants reflect about and understand the
solution. Explanation about the MHD solution, however,
which parallels much more the traditional learning
environment (cf. teacher controlled), seems to be more
appropriate for teaching students the difficult concept of
posterior probabilities.
However, this conclusion should be considered very
carefully for several reasons. First, general implications
about the use of digital learning environments for (posterior)
probability learning are hard to make given the narrow
nature of our study and research paradigm. Second, there is
a crucial limitation in both DiBattista’s (2011) and our own
performed study. More specifically, the operationalization
of understanding the MHD solution may have been
inadequate in both studies. In DiBattista’s (2011) study,
participants’ explanation for why they indicated a particular
behavioral response as the optimal one were interpreted as
(no) understanding of the MHD solution. Which criteria
were used to interpret participants’ explanation as either
correct or incorrect, is however unclear. Therefore, we
operationalized understanding the MHD solution as being
able to give correct posterior probability estimates.
However, these do not necessarily reflect understanding:
Participants might give correct probability judgments
accidentally by guessing. Furthermore, the 6-door MHD
variant – which we included in order to be able to compare
our results with the results of DiBattista (2011) – has the
same underlying posterior probabilities as the classic MHD.
Therefore, it is impossible to determine whether
participants’ – who were assigned to a condition in which
they received explanation about the solution – correct
posterior probability estimates were the result of
understanding the MHD solution, or only showed that they
had copied the probabilities they just had been reading but
still did not understand these posterior probabilities and the
problem’s solution. Future research could clarify this by
including MHD variants in the posttest questionnaire with
other optimal behavioral responses (e.g., staying) and other
underlying posterior probabilities.

Acknowledgments
This research was carried out with the financial support of
the Concerted Research Action ‘Number sense: Analysis
and Improvement’ of the KU Leuven (GOA/12/010).
The authors would like to thank Kevin Van Langendonck
for his help with the data collection.

References
Burns, B. D., & Wieth, M. (2003). Causality and reasoning:
The Monty Hall dilemma. Proceedings of the 25th annual
conference of the cognitive science society (pp. 198−203).
Boston, MA: Cognitive Science Society.
Burns, B. D., & Wieth, M. (2004). The collider principle in
causal reasoning: Why the Monty Hall dilemma is so
hard. Journal of Experimental Psychology: General, 133,
434−449.
DiBattista, D. (2011). Evaluation of a digital learning object
for the Monty Hall dilemma. Teaching of Psychology, 38,
53−59.
Franco-Watkins, A. M., Derks, P. L., & Dougherty, M. R. P.
(2003). Reasoning in the Monty Hall problem: Examining
choice behaviour and probability judgements. Thinking &
Reasoning, 9, 67−90.
Friedman, D. (1998). Monty Hall’s three doors:
Construction and deconstruction of a choice anomaly. The
American Economic Review, 88, 933–946.
Granberg, D. (1999). Cross-cultural comparison of
responses to the Monty Hall dilemma. Social Behavioral
and Personality, 27, 431−438.
Granberg, D., & Brown, T. A. (1995). The Monty Hall
dilemma. Personality and Social Psychology Bulletin, 21,
711−723.
Granberg, D., & Dorr, N. (1998). Further exploration of
two-stage decision making in the Monty Hall dilemma.
American Journal of Psychology, 111, 561−579.
Petrocelli, J. V. (2013). Pitfalls of counterfactual thinking in
medical practice: Preventing errors by using more
functional reference points. Journal of Public Health
Research, 24, 136−143.
Petrocelli, J. V., & Harris, A. K. (2011). Learning inhibition
in the Monty Hall problem: The role of dysfunctional
counterfactual prescriptions. Personality and Social
Psychology Bulletin, 37, 1297−1311.
Saenen, L., Heyvaert, M., Grosemans, I., Van Dooren, W.,
& Onghena, P. (2014). The equiprobability bias in the
Monty Hall dilemma: A comparison of primary school,
secondary school, and university students. Proceedings of
the 36th annual conference of the cognitive science
society (pp. 2859−2864). Austin, TX: Cognitive Science
Society.
Saenen, L., Van Dooren, W., & Onghena, P. (2015a). A
randomized Monty Hall experiment: The positive effect
of conditional frequency feedback. Thinking &
Reasoning, 21, 176−192.
Saenen, L., Heyvaert, M., Van Dooren, W., & Onghena, P.
(2015b). Inhibitory control in a notorious brain teaser:
The Monty Hall dilemma. ZDM Mathematics Education.
Advance online publication.
Stibel, J. M., Dror, I. E., & Ben-Zeev, T. (2009). The
collapsing choice theory: Dissociating choice and
judgment in decision making. Theory and Decision, 66,
149−179.

2062

