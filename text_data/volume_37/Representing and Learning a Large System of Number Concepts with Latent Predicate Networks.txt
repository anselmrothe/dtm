                Representing and Learning a Large System of Number Concepts
                                                with Latent Predicate Networks
                  Joshua Rule, Eyal Dechter, Joshua B. Tenenbaum: {rule, edechter, jbt} @ mit.edu
                                MIT, 46-4053, 77 Massachussetts Avenue, Cambridge, MA 02139 USA
                               Abstract                                   of how children link physical sets with the counting routine
   Conventional models of exemplar or rule-based concept learn-           and develop their first number concepts are crucial, we direct
   ing tend to focus on the acquisition of one concept at a time.         our attention elsewhere in this paper. We focus on this sec-
   They often underemphasize the fact that we learn many con-             ond problem, on how children might acquire knowledge of
   cepts as part of large systems rather than as isolated individu-
   als. In such cases, the challenge of learning is not so much in        an infinite number system, particularly for numbers they hear
   providing stand-alone definitions, but in describing the richly        discussed but are unlikely to ever see counted out explicitly.
   structured relations between concepts. The natural numbers
   are one of the first such abstract conceptual systems children            We ground our learning proposal in a new framework for
   learn, serving as a serious case study in concept representa-          representing number as a conceptual system, which on its
   tion and acquisition (Carey, 2009; Fuson, 1988; Gallistel              own has presented a non-trivial challenge met in different
   & Gelman, 2005). Even so, models of natural number learn-
   ing focused on single-concept acquisition have largely ignored         ways by linguists and developmentalists. For example, Hur-
   two challenges related to natural number’s status as a system          ford (1975) proposed a single system differentiating primitive
   of concepts: 1) there is an unbounded set of exact number              and compound number concepts, while Siegler and Robinson
   concepts, each with distinct semantic content; and 2) people
   can reason flexibly about any of these concepts (even fictitious       (1982) proposed a system with several stages of development,
   ones like eighteen-gazillion). To succeed, models must instead         each containing minimal internal structure.
   learn the structure of the entire infinite set of number concepts,        Our approach to representation and learning is in part in-
   focusing on how relationships between numbers support refer-
   ence and generalization. Here, we suggest that the latent pred-        spired by, and shares much in common with, the recent fam-
   icate network (LPN) – a probabilistic context-sensitive gram-          ily of Rational Rules models (Goodman, Tenenbaum, Feld-
   mar formalism – facilitates tractable learning and reasoning           man, & Griffiths, 2008; Piantadosi et al., 2012; Ullman,
   for natural number concepts (Dechter, Rule, & Tenenbaum,
   2015). We show how to express several key numerical rela-              Goodman, & Tenenbaum, 2012), exploring concept learning
   tionships in our framework, and how a Bayesian learning al-            through Bayesian induction of compositional representations
   gorithm for LPNs can model key phenomena observed in chil-             using sparse evidence. We agree that this framework is fun-
   dren learning to count. These results suggest that LPNs might
   serve as a computational mechanism by which children learn             damental to understanding concept learning.
   abstract numerical knowledge from utterances about number.                The major difference is in how our models represent con-
   Keywords: child development; concept learning; number;                 cepts. In Rational Rules models, each concept is a sin-
   generalization; computational model; grammar induction                 gle stand-alone rule supported by its own evidence. These
                           Introduction                                   rules are generated from a static grammar which defines the
                                                                          hypothesis space. Learning is determining which concepts
Humans seldom learn concepts in isolation. We learn about
                                                                          (which rules) are supported by the evidence. In the model
left by comparing and contrasting it with up, down, and right,
                                                                          we present here, concepts are not stand-alone rules, but net-
and about red by noting its similarities and differences with
                                                                          works of possible relations generated according to a grammar.
green and blue. The natural numbers (1, 2, 3, . . .) are no
                                                                          The hypothesis space is thus not over stand-alone rules gen-
exception: to understand a number such as one, we must not
                                                                          erated by a prespecified grammar, but over millions of possi-
only ground it in terms of concepts and percepts we already
                                                                          ble grammars, each defining a different network of relations.
know, but we must also relate it to other number concepts
                                                                          Learning is determining which grammar, which sets of rela-
we are still in the process of acquiring. The natural numbers
                                                                          tions, are supported by the evidence.
are particularly interesting in this respect. Because they are
                                                                             We begin by discussing how to represent the infinite con-
infinite, there is no way to learn all the individual concepts
                                                                          ceptual system of natural number, and show how a particular
without learning a compositional structure for the system.
                                                                          formalism – the Probabilistic Range Concatenation Grammar
   A great deal of empirical work has focused on the first part
                                                                          (PRCG) – can represent number concepts this way (Boullier,
of this problem, on how initial number concepts are grounded
                                                                          2005). We then show how a portion of this grammar can
in counting routines and the core systems of approximate
                                                                          be learned using Bayesian inference in an LPN, a learning
magnitude and parallel object individuation (Carey, 2009;
                                                                          framework for PRCGs (Dechter et al., 2015).
Dehaene, 2011; Feigenson, Dehaene, & Spelke, 2004). Re-
cent studies have also proposed computational mechanisms to
                                                                           A Grammar Representing Number Knowledge
explain several key behavioral changes during early number
learning (Piantadosi, Goodman, & Tenenbaum, 2012).                        Three challenges make learning systems of number concepts
   Far fewer studies have focused on the second half of the               particularly difficult and interesting. First, very few number
problem, on how numbers are learned as a system and par-                  concepts are perceptually grounded. When, for example, did
tially defined with respect to each other. While the problems             you last count exactly 254 objects? The problem only inten-
                                                                      2051

sifies as we begin applying numbers to events, time periods,        correct, human-readable, and fits a prefix-base-suffix under-
sets of objects, and eventually even other numbers. Second,         standing of number, as discussed below. We do not claim this
the fact that there are infinitely many number concepts means       particular grammar is used by children or adults but rather
that, much like in natural language sentences, the meanings of      that this framework, regardless of the specific grammar given,
the numbers are compositional. The meaning of four-hundred          captures important aspects of concept learning, such as the
fifty-two, for example, depends on but goes significantly be-       rich and systematic relations between concepts, that are un-
yond the meanings of four and hundred. Third, to under-             deremphasized in other models. Third, while the natural
stand numbers is also to understand the relations in which          numbers form an infinite set, many numbers do not have con-
numbers participate. We are often interested in a number not        venient names. Our choice to examine what can be learned
for its cardinality but for some more complex property, such        from conventional number names analyzed as words, rather
as whether it is more or less than another number or how it         than as morphemes or phonemes, means we examine only a
changes through addition or division. This diverse range of         finite subset of the natural numbers.
uses makes it impossible to fully describe three without ref-          Intuitively, a number word like six-hundred thirty-seven
erencing two, four, and eventually all other numbers.               is valid because we have six units of one hundred each and
    How can we hope to represent systems of concepts which          thirty-seven remaining units of one each. That is, we have
are: 1) learnable without direct perceptual grounding; 2)           some base unit (hundred) and we track both how many of
compositionally constructed; and 3) relationally defined?           them we have (six), and how many of the next smallest
Happily, these properties are similar to those linguists face       base unit (one) we have (thirty-seven). We denote the sum
in studying natural language syntax. Grammars can be in-            of these (six-hundred + thirty-seven) simply by concatenat-
duced directly from a stream of utterances, are highly com-         ing the two terms from largest to smallest base (six-hundred
positional, and define their constituents based on their rela-      thirty-seven). This structure is recursive. Nine-thousand
tionships to each other rather than as discrete objects.            seven-hundred sixteen is created by taking nine thousands
                                                                    units and tacking on a remainder, which is seven hundreds
    Motivated by this insight, we now present a grammar of
                                                                    plus its remainder of sixteen ones: nine × thousand + (seven
number knowledge we have constructed to capture five key
                                                                    × hundred + (sixteen × one)). Note that there is no explicit
number relations learned during childhood and carried into
                                                                    mention of the base one in a valid number word - it is implied
adulthood: Number, capturing the distinction between valid
                                                                    and marked by appending ∅, the empty string, instead of one.
and invalid number words; Succ and Pred, the successor and
                                                                       Our grammar similarly uses a prefix-base-suffix system,
predecessor relations, respectively; and More and Less, the
                                                                    and Figure 2 shows the concepts involved in deciding that
more-than and less-than relations, respectively. While seem-
                                                                    six-hundred thirty-seven is a valid number word. As in our
ingly basic tasks, children require years to master them (Fu-
                                                                    example above, we must show that six is a valid prefix for
son, Richards, & Briars, 1982). Whereas most work in natu-
                                                                    hundred and thirty-seven is a valid suffix or remainder:
ral language syntax uses context-free grammars, our focus on
capturing structural relationships between concepts demands            Number(six hundred thirty seven) ←                       (1)
that we use a context-sensitive grammar. We specifically                   Prefix(six, hundred), Suffix(hundred, thirty seven).
use PRCGs because they are expressive and context-sensitive
while remaining relatively tractable (Boullier, 2005).              Six is a valid prefix for hundred because it is a number word
                                                                    representing a ones number, a number between one and nine.
    Capturing these relations with an RCG is not only possi-        It would be incorrect for hundred to have no prefix, and it
ble but can be done quite compactly. Our grammar for the            would also be incorrect to use a prefix larger than nine:
concepts of Number, Succ, Pred, Less, and More covers all
numbers between zero and one-quadrillion, exclusive, and re-           Prefix(six, hundred) ← Ones(six).                        (2)
quires only 218 rules. Even considering just Number, Succ,          Thirty-seven is a valid suffix because it is a valid number for
and Pred, these 218 rules cover more than 1024 true relations.      a previous base, in this case ∅, the ones base:
Figure 1 shows a schematic of the rules concerned with de-
termining valid and invalid numbers, while the rest, due to            Suffix(hundred, thirty seven) ←                          (3)
space constraints, can be found online (https://git.io/                    LargerBase(hundred, ∅), Number(thirty seven).
ruleEtAl2015CogSci).
    Three clarifications: first, our grammar never produces nor        LargerBase(hundred, ∅) ← PrevBase(hundred, ∅).           (4)
parses full English sentences. We model the structure of con-       Thirty-seven is one of these numbers because it is merely the
cepts, not the structure of language. When attempting to            concatenation of a decade word and a ones word:
parse something like Succ(ninety nine, one hundred), we as-
sume another system more directly involved in language pre-            Number(thirty seven) ←                                   (5)
processes utterances into predicates which are then checked                Prefix(thirty seven, ∅), Suffix(∅, ∅).
against the knowledge encoded in our conceptual grammar.
Second, this grammar has not been optimized for compact-               Prefix(thirty seven, ∅) ←                                (6)
ness or efficiency. We focus on providing a grammar that is                Decades(thirty), Ones(seven).
                                                                2052

 Ones(one).           (1, 5) Number(P BS) ← Prefix(P, B), Suffix(B, S).                                 Prefix(P, B) ← LargerBase(B, hundred), NormalPrefix(P ).
 ···                         Number(P B) ← Prefix(P, B).                                            (2) Prefix(P, hundred) ← Ones(P ).
 Ones(nine).                                                                                            Prefix(X, ∅) ← Ones(X).
                             LargerBase(X, Z) ← LargerBase(X,Y), LargerBase(Y,Z).                       Prefix(X, ∅) ← Teens(X).
 Teens(ten).             (4) LargerBase(X, Y ) ← PrevBase(X, Y ).                                       Prefix(X, ∅) ← Decades(X).
 ···                                                                                                (6) Prefix(XY, ∅) ← Decades(X), Ones(Y ).
 Teens(nineteen).            PrevBase(million, thousand).
                             PrevBase(thousand, hundred).                                               NormalPrefix(S) ← Suffix(thousand, S).
 Decades(twenty).            PrevBase(hundred, ∅).
 ···                                                                                                (3) Suffix(B, P CS) ← LargerBase(B, C), Number(P CS).
 Decades(ninety).                                                                                       Suffix(∅, ∅).
                    Figure 1: An RCG whose strings are valid number words. Numbered rules correspond to Figure 2.
           (1) Number(six hundred thirty seven)                                                        Succ(one hundred ninety nine, two hundred)
 (2) Prefix(six, hundred)        (3) Suffix(hundred, thirty seven)
                                                                                    PrevBase(hundred, ∅). MaxForBase(ninety nine, ∅) Succ(one,two) Prefix(two, hundred)
        Ones(six).      (4) LargerBase(hundred, ∅). (5) Number(thirty seven)
                   PrevBase(hundred, ∅).      (6) Prefix(thirty seven, ∅)   Suffix(∅, ∅).    MaxDecades(ninety).    MaxOnes(nine). Succ1(one,two).      Ones(two).
                                          Decades(thirty).     Ones(seven).
                                   Figure 2: Example RCG parses for Number (Blue) and Succ (Red) relations.
The compositional use of simple predicates thus helps us ana-                               and the lowest layer of latent predicates is defined in terms
lyze the structure of a complex phrase like six-hundred thirty-                             of a collection of lexicon predicates, each of which is a unary
seven and show that while it is a valid number word, hun-                                   predicate that is true of the atomic units (the words) of the
dred six seven thirty is not. Succ can similarly be encoded                                 system. The rules of the LPN consist of all definitions possi-
(Figure 2) as can More (not shown), while Pred and Less                                     ble within the network architecture (for details see Dechter et
can be encoded quite simply as Less(X,Y ) ← More(Y, X) and                                  al. (2015)). The parameters of the network are the probabili-
Pred(X,Y ) ← Succ(Y, X).                                                                    ties of the rules.
                                                                                               Our model learns a distribution over the parameters of the
                 Learning Number Knowledge                                                  LPN given the available data using hierarchical Bayesian in-
How might children learn the number knowledge captured in                                   ference: the model assumes that there is a prior distribution
the representation above? In this section, we present a com-                                over the parameters of the LPN and, using Bayes’ rule, infers
putational model of learning PRCGs and take a first step to-                                a distribution over parameter values that balances the fit of
ward evaluating this model against the learning trajectories                                the observations against the prior. We use a sparsity-inducing
and patterns of error reported in the literature on counting.                               prior to formalize the intuition that latent predicates and rules
   To match the literature’s focus on counting, we restrict our                             should be shared in order to learn grammars that can general-
experiments here to the successor relation, and, in particular,                             ize beyond the observed data.
to learning to count from one to one-hundred. Several stud-                                    Since exact inference in probabilistic grammars is compu-
ies track children’s learning trajectories and patterns of er-                              tationally intractable, our model is simulated using the Vari-
rors when acquiring the count sequence (Fuson et al., 1982;                                 ational Bayes EM approximate inference algorithm as imple-
Miller & Stigler, 1987), making count sequence learning                                     mented in the PRISM programming language (Sato, Kameya,
an interesting domain for evaluating our model of learning                                  & Kurihara, 2008).
PRCGs against empirical data.                                                                  All the simulations below were run on an LPN with three
Latent Predicate Networks Latent Predicate Networks                                         layers of five predicates each. The learning algorithm was
(LPNs) are PRCGs with three types of predicates connected                                   run for a single iteration with a concentration parameter of
in a layered fashion. Observed predicates are relations di-                                 α = 0.1, and a convergence criterion of ε = 1e − 4. We will
rectly present in the data (e.g. Succ is observed if the data                               refer to each separate simulation below as a simulated child.
includes Succ relations). Observed predicates are defined in                                Acquiring the count sequence Fuson et al. (1982) de-
terms of layers of latent predicates. These relations are not                               scribes qualitative phenomena of count sequence acquisition
directly observable in the data and their meanings are deter-                               and elaboration based on several surveys in which the authors
mined through learning. For example, the Decade predicate,                                  asked American children between three and five years of age
which is true for “ten”, “twenty”, etc., might correspond to                                to count (either while counting a collection of objects or just
one of the latent predicates after the model is trained on pairs                            reciting the count word sequence). The learning trajectories
of successive number words. Each layer of latent predicates                                 and error patterns they describe have inspired computational
is defined in terms of the latent predicate layer beneath it,                               modeling efforts using connectionist networks; for example,
                                                                                       2053

                                    Fuson et al. 1982
                                                                             The stage 1 simulations are variable in performance, with
   Child age
                                                                         some of simulated children unable to count further than the
                                                                         first few words and a few having a relatively high chance
                                                                 a)      of reaching “twenty.” The sharp drops in performance at
                                       Our model                         “twenty,” “thirty” and “forty” in stage 2, and the horizon-
   Training Data
                   stage 4*                                              tal lines between them, indicate that here the simulation has
                   stage 3*
                   stage 2
                                                                         learned the within-decade structure of the count list but is un-
                   stage 1                                       b)
                                                                         certain about the transitions between decades. In stages 3
                                                                         and 4, nearly all simulated children master the numbers up to
                              Largest number correctly reached           “twenty nine” but are unable to transition from “twenty nine”
Figure 3: Our model compared with children’s counting data               to “thirty.” Only in stage 4 do we see any children making the
a) Data from Fuson et al. (1982). The x-axis shows the high-             transition from this state of knowledge to one in which they
est number correctly reached when children were asked to                 can reach “ninety nine.”
count starting at “one.” Boxes correspond to the standard de-                The simulations in these first four stages suggest that even
viation, central bands to the means, and whiskers to the range.          with large increases in the quantity of data, our model is un-
b) Model performance, averaged over ten runs at four stages              likely to progress beyond “twenty nine.” We hypothesized
of increasing data quantity.                                             that this is due to a lack of evidence for the decade transi-
                                                                         tions. Mastering the decade transitions requires both learning
                                                                         that there is a special rule for the successor of numbers ending
Ma and Hirai (1989) use an associative network to model the              in “nine,” and learning the order of the decade words. This
errors that young children typically make when learning the              adds considerable complexity to the grammar, and our simu-
count sequence up to twenty or thirty. But, to our knowledge,            lations favor a more parsimonious explanation of the heavily
such models have not been used to study how children acquire             weighted smaller numbers. Children, however, do not learn
the count sequence beyond thirty.                                        to count to a hundred by unsupervised exposure to naturally
   Figure 3a shows the highest number correctly reached by               occurring count words; they are actively taught to do so. Al-
children of various ages when tested by Fuson et al. The au-             though we know of no study of the pedagogical language used
thors hypothesize that the large jump in range between the               in teaching children to count, some kindergarten teaching
young three-year-olds and the four-year-olds and five-year-              blogs (e.g. http://www.heidisongs.com/blog/2012/05/
olds is due to the older children partially solving what they            teaching-kids-to-count-to-100.html) mention empha-
term “the decade problem” – i.e. recognizing both that there             sizing decade transitions as useful in helping struggling stu-
is a pattern that repeats across decades greater than twenty             dents to learn the count sequence.
and that there is a particular sequence to the decade words.                 To confirm that increased emphasis on decade transitions
   We asked whether our model goes through a similar tran-               can facilitate the transition to mastering counting up to a hun-
sition. To simulate learning, we generated data sets consist-            dred, we created two additional data sets, stages 3∗ and 4∗ ,
ing of successor pairs between one and one-hundred, with the             that contain the same data as stages 3 and 4, respectively, but
number of examples N of each pair Succ(i, i + 1) following               have an additional 10% of the data evenly distributed across
the power law N = Ki , where K determines the overall size of            the decade transitions (twenty nine, thirty; thirty nine, forty;
the data set. To explore the effect of evidence quantity, and            ...; eighty nine, ninety). The simulated data for these stages is
to simulate the effect that overall quantity of evidence has on          shown in Figure 4e-f. In both simulations, we observe a sharp
a child’s acquisition of the count list, we generated data sets          increase in the number of simulated children who transition
for K = 10, 100, 1000, 10000, which we denote stages 1-4,                to counting to a hundred (from 0 to 4 children in stage 3∗ , and
respectively. The resulting histograms of data are shown in              2 to 6 children in stage 4∗ ).
Figure 4a-d (the y-axes are logarithmically scaled).                         Figure 3b summarizes the simulation data for stages 1,2,3∗
   For each of these data sets we ran our learning algorithm             and 4∗ for comparison against the Fuson et al. data in Fig-
ten times, generating ten simulated children at each stage (the          ure 3a. For each stage and each simulated child, we computed
simulations differ due to different random parameter initial-            the probability that the highest number reached by counting,
izations). In Figure 4a-d, each line corresponds to one of the           starting from “one,” would be x for x = 1, . . . , 99. We aver-
simulated children and shows the probability that the child              aged these values across simulated children within a stage and
will correctly count to the corresponding number on the x-               used the resulting densities to calculate the means, standard
axis. To generate this data, we asked the model for the dis-             deviations, and 10th and 90th percentiles for each stage (these
tribution of successors for a given number and used a simple             percentiles were chosen to be comparable with the empirical
softmax decision procedure to determine the probability of               ranges described by Fuson et al.).
the simulated child reporting each word. Specifically, if the                In addition to examining the learning trajectories of our
simulated child believes x follows a with probability pa (x),            model, we also examined its mistakes. One interesting pat-
then it says x after a with probability proportional to pa (x)2 .        tern of mistakes that young English-speaking children make
                                                                      2054

log p(N)
       1
P(correct)
                                    a) stage 1                                      c) stage 3                                         e) stage 4
       0
log p(N)
       1
P(correct)                          b) stage 2                                      d) stage 3*                                        f) stage 4*
       0        10   20   30   40   50   60   70   80   90      10   20   30   40   50    60   70   80   90        10   20   30   40   50   60   70   80   90
                                                                               Number (N)
Figure 4: Our model’s performance correctly reciting the count sequence. Each colored curve corresponds to a single run of
the learning algorithm given the distribution of data in the histogram directly above it. For each number, N, along the x-axis,
the y-axis corresponds to the probability that the model correctly counts from one up to N. The y-axes on the data histograms
are shown on a logarithmic scale. The stages refer to the distributions of data available to the model (see text for details).
                   twenty ten
                   eleventeen
                                               twenty ten
                                           twenty eleven
                                                                                                                 Discussion
                 twenty sixty            twenty fourteen
                  twenty fifty          twenty eighteen                                  In this work, we have shown how exact number concepts and
              twenty thirteen                    thirty ten
                 twenty forty             twenty sixteen                                 the relations among them can be represented using probabilis-
                twenty thirty                thirty eleven
               twenty fifteen           twenty nineteen                                  tic context-sensitive grammars. We have also given a model
               twenty eleven               thirty fourteen
             twenty fourteen              thirty eighteen                                for how children might learn such representations based on
                              1       3                   0.8       1.8 1e 3             hierarchical Bayesian inference. Our simulations suggest this
                          Mean use per child               Probability
                                                                                         model captures several behavioral phenomena children ex-
Figure 5: Top ten invented number words in children’s count-                             hibit while learning the count sequence – a critical and dif-
ing. a) Data from Fuson et al. b) A simulated child at stage 2.                          ficult prerequisite to adult-like numerical knowledge.
                                                                                            An interesting aspect of this process is the seemingly
                                                                                         sudden transition from counting only through the first few
                                                                                         decades to counting all the way to a hundred. Our model ex-
when reciting the count sequence is that they invent number                              plains this transition as an inductive leap: for small amounts
words. Fuson et al. report that children invent such words                               of data, learning is slow and incremental – adding a decade
both by combining morphological components of number                                     at a time – because the increased complexity of the concep-
words (such as “fiveteen” and “eleventy”) and by combin-                                 tual knowledge is large compared to the gains in explanatory
ing decade words with incorrect digit-place words (such as                               power. Eventually, however, enough evidence accumulates to
“twenty-eleven” and “twenty-twenty”). In particular, they re-                            warrant a more complex and more general grammar, resulting
port that appending teen words to decade words is most com-                              in a kind of phase transition between states of knowledge.
mon, creating sequences like “twenty-ten”, “twenty-eleven”,                                 In many ways this phenomenon is analogous to the Car-
“twenty-twelve”, etc. In Figure 5a we show the most common                               dinal Principle (CP) transition, in which younger children
invented words that Fuson et al. report and the mean number                              learning the relationship between small numbers and set sizes
of times a child used the word.                                                          make slow and incremental progress when learning to count
   Since we do not model in this work how number word mor-                               out sets matching the first three or four number words but then
phemes are composed to construct number words, our model                                 suddenly expand their ability to every other memorized num-
cannot account for morphologically-based errors. We asked,                               ber word. The theory that this rapid transition is due to what
however, to what extent it can model the other invented word                             Carey (2009) refers to as Quinian bootstrapping has been for-
errors that Fuson et al. report; the most common invented                                malized by Piantadosi et al. (2012) as probabilistic inference
words are shown in Figure 5a. To compare these data to our                               over a space of recursive programs defined by a grammar. As
model, we asked a stage 2 simulated child for the top ten                                we do here, they explain the inductive leap of the CP tran-
non-number words that could appear as successor to a num-                                sition as a result of the tension between program complexity
ber word or non-number word (because this was a computa-                                 and fit to the available data. Whereas Piantadosi et al. place a
tionally expensive procedure, we restrict our analysis here to                           distribution over programs using a probabilistic context-free
a single randomly selected simulation). The marginal proba-                              grammar, however, our model is learning a complete gram-
bilities of those non-number words is shown in Figure 5b.                                mar, one that can accommodate many different concepts and
                                                                                2055

relations and that can be seen as a probabilistic and declara-         More broadly, we see this paper as growing out of the hy-
tive knowledge base.                                                pothesis that much of human learning, including the explo-
   Another difference is that our simulations require a peda-       sion of knowledge during development, can be understood as
gogical emphasis on critical evidence – the decade transitions      inducing, from sparse and noisy data, a library of bits of con-
– to master the count sequence robustly, suggesting that peda-      ceptual knowledge, written in something like a programming
gogy may play an important role in facilitating these kinds of      language of thought. This vision of the child-as-hacker draws
inductive leaps. Focusing on concept acquisition in slightly        on and extends the notion of the child-as-scientist (Gopnik,
older children allows us to explore the relationship between        1996); not only are children forming theories about the world,
computational level considerations driving inductive reason-        but they are simultaneously developing the very conceptual
ing and the pedagogical factors enabling it in practice.            language they use to formulate those theories.
   An important goal for future work is to apply our model                                Acknowledgments
to learning systems of number concepts in other languages
besides English. In preliminary work we have applied our            The authors benefited significantly from conversations with
model to learning the Chinese number system and shown that          Leon Bergen, Timothy O’Donnell and Steven Piantadosi.
it both learns the adult system with relative ease and explains     This material is based upon work supported by the Center for
why Chinese children generally make different patterns of           Minds, Brains and Machines (CBMM), funded by NSF STC
mistakes than English-speaking children – in particular, why        award CCF-1231216, an NSF Graduate Research Fellowship,
they are much less likely to invent number words like “twenty       and the Eugene Stark Graduate Fellowship.
eleven” even though Chinese uses the same words to refer to
                                                                                                 References
both decade and ones values (e.g., “twenty one” is “two ten
                                                                    Boullier, P. (2005). Range concatenation grammars. In New devel-
one”) (Miller & Stigler, 1987).                                            opments in parsing technology. Springer.
   Another important future step for this research will be to       Carey, S. (2009). The origin of concepts. Oxford University Press.
                                                                    Dechter, E., Rule, J., & Tenenbaum, J. B. (2015). Latent predi-
relate our model to those, like Piantadosi et al. (2012) for               cate networks: Concept learning with probabilistic context-
counting and Dehaene (2011) for the approximate magni-                     sensitive grammars. In Papers from the 2015 AAAI Spring
tude system, that attempt to explain how abstract number                   Symposium.
                                                                    Dehaene, S. (2011). The number sense: How the mind creates
knowledge becomes grounded in the perceptual and procedu-                  mathematics. Oxford University Press.
ral primitives through which children learn about the world.        Feigenson, L., Dehaene, S., & Spelke, E. (2004). Core systems of
The model we presented here does not attempt to explain how                number. Trends in Cognitive Sciences, 8(7), 307–314.
                                                                    Fuson, K. C. (1988). Children’s counting and concepts of number.
children come to understand that number words refer to car-                Springer-Verlag.
dinalities, though this is crucial to understanding number.         Fuson, K. C., Richards, J., & Briars, D. J. (1982). The acquisition
                                                                           and elaboration of the number word sequence. In Children’s
   That said, we see no fundamental incompatibility between                logical and mathematical cognition. Springer.
the model presented here and extensions to include approx-          Gallistel, C., & Gelman, R. (2005). Mathematical Cognition. In The
imate magnitude, object tracking, set manipulation, more                   Cambridge handbook of thinking and reasoning. Cambridge
                                                                           University Press.
complex morphology (e.g. the meaning of -illion or -teen), or       Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths, T. L.
different counting strategies (e.g. as used in Turkish, French,            (2008). A rational analysis of rule-based concept learning.
or Mandarin) as would be needed for a more comprehensive                   Cognitive Science, 32(1), 108–154.
                                                                    Gopnik, A. (1996). The scientist as child. Philosophy of Science,
model of number learning. In fact, a key next step for us is to            63(4), 485–514.
model the link between the relatively small set of named num-       Hurford, J. R. (1975). The linguistic theory of numerals. Cambridge
bers (as modeled here) and the infinite set of numbers through             University Press.
                                                                    Ma, Q., & Hirai, Y. (1989). Modeling the acquisition of counting
more complex morphology and word invention (i.e. the -illion               with an associative network. Biological Cybernetics, 61(4),
system, including gazillion or bajillion) or systems like Ara-             271–278.
bic or tally notation, where the infinite sequence is easier        Miller, K. F., & Stigler, J. W. (1987). Counting in Chinese: Cultural
                                                                           variation in a basic cognitive skill. Cognitive Development,
to express. We see our work here as a first demonstration                  2(3), 279–305.
of LPN’s suitability for capturing a broad range of concepts        Piantadosi, S. T., Goodman, N. D., & Tenenbaum, J. B. (2012).
in number and other semantic domains including space, kin-                 Bootstrapping in a language of thought: A formal model of
                                                                           numerical concept learning. Cognition, 123(2), 199-217.
ship, and natural kinds. Whether these more general mod-            Sato, T., Kameya, Y., & Kurihara, K. (2008). Variational Bayes via
els are best approached by working strictly within the LPN                 propositionalized probability computation in PRISM. Annals
formalism or by using it as one module within a more com-                  of Mathematics and Artificial Intelligence, 54(1-3), 135–158.
                                                                    Siegler, R., & Robinson, M. (1982). The development of numerical
plex framework is an open question. Certainly, the human                   understandings. Advances in Child Development and Behav-
mind is more powerful than an RCG and is at least Turing-                  ior, 16, 241–313.
complete. RCGs provide a tractable way, however, to explore         Ullman, T. D., Goodman, N. D., & Tenenbaum, J. B. (2012). The-
                                                                           ory learning as stochastic search in the language of thought.
a restricted subclass of problems. The strategies and solutions            Cognitive Development, 27(4), 455–480.
we discover here are also available in Turing-complete sys-
tems, and are in fact implemented in one (PRISM Prolog), so
our findings easily generalize to more expressive grammars.
                                                                2056

