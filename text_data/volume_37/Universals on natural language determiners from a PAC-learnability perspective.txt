Universals on Natural Language Determiners from a PAC-learnability Perspective
                                               Giorgio Magri (magrigrg@gmail.com)
                   SFL UMR 7023 (CNRS, University of Paris 8)               UiL-OTS (Utrecht University)
                               61 rue Pouchet, 75017 Paris, France          Trans 10, 3512 JK Utrecht, The Netherlands
                              Abstract                                     Here are some examples. The property dog is a subset of the
   A classical conjecture in generative linguistics is that universal      property animal. The determiner some is monotone+ because
   restrictions on determiners in Natural Language (e.g. mono-             Some dog is tall entails Some animal is tall. The determiner
   tonicity, invariance, and conservativity) serve the purpose of          no is monotone− because No animal is tall entails No dog
   simplifying the language acquisition task. This paper formal-
   izes this conjecture within the PAC-learnability framework.             is tall. The determiners some, every and no are conserva-
   Keywords: Natural Language determiners; PAC-learnability.
                                                                           tive because the sentence Some/every/no dog is tall is true iff
                                                                           Some/every/no dog is a tall dog is true. The latter equiva-
                          Introduction                                     lence does not hold for only: Only dogs are tall is stronger
                                                                           than Only dogs are tall dogs. And indeed only is not a de-
Determiners and quantifiers. According to Natural Lan-
                                                                           terminer (rather an adverb), showing that the conservativity
guage Semantics (Montague, 1973; Heim & Kratzer, 1978),
                                                                           universal for lexical determiners has empirical bite.
a noun like animal and a predicate like tall denote properties,
namely subsets of the domain of quantification D. Determin-                The challenge of a learnability approach. Gamut (1991)
ers like some, every, no denote a function ℘(D) → ℘(℘(D))                  points out that, although these universals are “a significant
that takes a property such as the one denoted by animal and                contribution to the characterization of the notion of possible
returns a collection of properties, exemplified in (1).                    human language” they do “not give any clue as to why this
                                                                           should be so. [. . . ]. A formulation of a universal [. . . ] is
              some(animal) = {B | animal ∩ B 6= 0}       /                 one thing; the explanation for it is something else.” This is
             every(animal) = {B | animal ⊆ B}                       (1)    the problem addressed in this paper: how can these univer-
                no(animal) = {B | animal ∩ B = 0}        /                 sal restrictions on the denotations of lexical determiners be
                                                                           explained? Generative linguistics has focused mainly on the
The meaning of the sentence Some/every/no animal is tall
                                                                           conservativity universal, and has suggested that conservativ-
with the parse [ [NP Some/every/no animal ] tall ] is derived
                                                                           ity (and perhaps also the other universals) should be explained
compositionally as follows. The denotation of the NP is ob-
                                                                           from a learnability perspective. For instance, Hunter and Lidz
tained by applying the property animal to the function de-
                                                                           (2013) show that four- and five-year-olds fail to learn a novel
noted by the determiner. This yields a collection of properties
                                                                           nonconservative determiner but succeed in learning a compa-
as in (1), called a generalized quantifier. The sentence is then
                                                                           rable conservative determiner, consistently with the learnabil-
true iff the property tall belongs to that generalized quantifier.
                                                                           ity hypothesis.1 The question, then, is what might make con-
Universal restrictions. Westerståhl (1989) formulates “the                servativity and the other universals well suited from a learn-
basic question facing anyone who studies Natural Language                  ability perspective.
quantification” as follows: “Logically, the category of deter-                Keenan and Stavi (1986) take on this issue. To start, they
                                                                                                     n
miners is extremely rich. For example, even on a [domain D]                note that there are 24 possible determiners over a domain
with two elements, there are 216 = 65.536 possible determin-                                                                         n
                                                                           of quantification Dn of cardinality n but that only 23 are con-
ers. But, in natural languages, just a small portion of these              servative. For instance, “in a model with only two individuals
are [lexicalized]. Which ones [. . . ]? What are the constraints           there are [. . . ] 164 = 65.536 determiners [. . . but] only 512 of
on determiner denotations in natural languages?”. It has been              these [. . . ] are conservative! The constraint then is extremely
suggested (Westerståhl, 1989; Gamut, 1991; Keenan & Stavi,                strong [. . . ]: the language learner does not have to seek the
1986; Barwise & Cooper, 1981; Benthem, 1983) that all lex-                 meaning of a novel determiner among all the logically pos-
ical (i.e., not syntactically complex) determiners in Natural              sible [ones]. He only has to choose from among those ways
Language satisfy the monotonicity, invariance and conserva-                which satisfy conservativity.” This argument is rather weak.
tivity constraints defined below.                                          If the effectiveness of conservativity were to be measured in
DEF   1 A determiner ∆ : ℘(D) → ℘(℘(D)) is:                                terms of its ability to prune the learner’s search space, then
 (a) monotone+ (monotone− ) iff B ∈ ∆(A) entails B0 ∈ ∆(A),                the constraint is not at all “extremely strong” but rather quite
      for any properties A, B, B0 ⊆ D such that B ⊆ B0 (B ⊇ B0 ).          weak, as it does not alter the asymptotic exponential growth
      It is monotone iff it is either monotone+ or monotone− ;             of the search space as a function of the cardinality n of the
 (b) conservative provided B ∈ ∆(A) holds iff A ∩ B ∈ ∆(A),                domain of quantification. Furthermore, this argument does
      for any properties A, B ⊆ D;                                         not explain why Natural Language enforces precisely conser-
                                                                      
 (c) invariant provided B ∈ ∆(A) holds iff π(B) ∈ ∆ π(A) ,                     1 But see Fox (1999) for an approach to conservativity not based
      for any permutation π over the domain of quantification              on learnability; and Piantadosi (2012) for modeling evidence that
      D and any properties A, B ⊆ D.                                       conservativity does not improve learnabaility.
                                                                       1494

vativity (together with the other universals) among the many               two concepts c, h ∈ Cn and any probability measure P over
alternative possible ways of restricting the search space.                 Xn , the aerror ec,P (h) of h w.r.t. c relative to P is the probabil-
   Keenan and Stavi also show that the family of conser-                   ity P(c h)of the symmetric difference between c and h. A
vative determiners coincides with the closure of the set                   concept class is learnable provided any concept in the class
{some, every} w.r.t. to the operations of conjunction, disjunc-            can be identified from a labeled sample with high accuracy
tion and adjectival restrictions. They then observe that this              (error smaller than ε) and with high confidence (larger than
result “do[es] provide a basis for saying that the set of conser-          1 − δ), as formalized in Definition 2 (Valiant, 1984; Kearns &
vative determiners is cognitively apprehendable. Namely, we                Vazirani, 1994; Kearns, 1999).
do in fact understand the denotations of simple determiners                DEF 2 A concept class C is PAC-learnable with sample car-
(every and some) [. . . ]. And we do have a cognitive grasp of             dinality m : (ε, δ, n) ∈ (0, 1) × (0, 1) × N 7→ m(ε, δ, n) ∈ N pro-
boolean operations and adjectival restriction, as [. . . ] we use          vided there exists a learning function A of the form
them in understanding meanings associated with essentially
all categories.” Again, the argument is rather weak. The de-                            A : (x, t) ∈ Xnm × {0, 1}m 7→ A(x, t) ∈ Cn              (2)
terminer denoted by only is analogous to that denoted by ev-
ery, just with the reversed set-inclusion (takes a property A              such that for any ε, δ ∈ (0, 1), any n ∈ N, any concept c ∈ Cn
and returns properties B such that B ⊆ A rather than A ⊆ B).               and any probability measure P over Xn , the following condi-
Thus, only is not conservative as noted above, and yet not in              tion holds with m = m(ε, δ, n)
any way more complex than every. Why has Natural Lan-                                       n              
guage restricted determiners to the conservative ones rather                                                                  o
                                                                                        Pm x ∈ Xnm ec,P A x, c(x) ≥ ε ≤ δ                       (3)
than, say, to the closure of the set {some, only}?
Meeting the challenge with PAC-learnability. These objec-                  and furthermore m(·, ·, ·) grows polynomially in 1ε ,        1
                                                                                                                                          and n.2
                                                                                                                                        δ
tions are not meant to challenge Keenan and Stavi’s learn-
                                                                           A subset S ⊆ Xn is shattered by Cn provided {S ∩ c | c ∈ Cn } =
ability approach to universal restrictions on lexical determin-
                                                                          ℘(S). The concept class Cn has Vapnik-Chervonenkis dimen-
ers. Rather, they are meant to argue that this learnability ap-
                                                                           sion VCD(Cn ) equal to d ∈ N provided there exists a shat-
proach needs to be cast within an explicit, formal learning
                                                                           tered subset S ⊆ Xn with cardinality d but no shattered sub-
framework, contrary to what done by Keenan and Stavi and
                                                                           set S ⊆ Xn with cardinality d + 1. VCD controls the sample
much of the subsequent literature. This paper thus develops
                                                                           cardinality needed for PAC-learnability (Ehrenfeucht et al.,
this learnability approach to universal restrictions on deter-
                                                                           1989): no learning function A satisfies condition (3) of PAC-
miners within the PAC learnability paradigm. As argued by
                                                                           learnability with sample cardinality m = m(ε, δ, n) smaller
Natarajan (1991), “the PAC paradigm appears to be a good                               Cn )−1
model of the natural learning process while lending itself to              than VCD(32ε       . With these preliminaries in place, let’s now
analysis” (but seeClark & Lappin, 2011 for discussion). It                 go back to generalized quantifiers, defined as follows.
shows that the entire class of determiners is not learnable,               DEF    3 Consider the sample space X = ∞
                                                                                                                           S
                                                                                                                             n=1 Xn where Xn =
not even according to a very weak PAC-learnability notion.                ℘(Dn ) is the collection of properties over a domain of quan-
This result provides support for the hypothesis that Univer-               tification Dn of cardinality n. Consider the concept class
sal Grammar enforces restrictions on lexical determiners to                Q = ∞n=1 Qn where Qn =℘(Xn ) is the collection of gener-
                                                                                   S
boost acquisition. It then shows that monotonicity has almost              alized quantifiers q∈℘(℘(Dn )) over Dn .
no effects on learnability. Thus, there are restrictions that do
                                                                           By definition, Qn shatters Xn . And Xn has cardinality 2n .
not affect learnability, reinforcing my point against Keenan
                                                                           Hence, VCD(Qn ) = 2n . Any learning function for Q thus
and Stavi’s naı̈ve approach. Finally, it looks at conservativity                                                                  n −1
                                                                           needs a sample cardinality m(·, ·, ·) larger than 232ε       and there-
and invariance, showing that those restrictions have a strong
                                                                           fore not polynomial in n. I thus conclude that:
learnability effect, allowing PAC-learnability from malicious
positive examples only.                                                    RESULT 1 The whole class Q of generalized quantifiers is
                                                                           not PAC-learnable.
     No PAC-learnability without restrictions                              Not even uniformly weakly PAC-learnable. Let’s weaken
Generalized quantifiers      are not PAC-learnable. A sample               condition (3) to (4): the error is not required to be arbitrary
space is a set X = ∞
                    S
                      n=1  Xn where X1 , X2 . . . are finite (to avoid     small (i.e., smaller than ε) but just smaller than chance, with
measurability problems) and disjoint sets, whose elements          are     the polynomial q(·) controlling the improvement over chance.
called examples. A concept class on X is a set C = ∞            n=1 Cn
                                                              S
                                                                           DEF 4 A concept class C is weakly PAC-learnable with sam-
where each Cn ⊆ ℘(Xn ) is a collection of subsets of Xn ,                  ple cardinality m : (ε, δ, n) ∈ (0, 1) × (0, 1) × N 7→ N provided
called concepts. A sample is an m-tuple x = (x1 , . . . , xm )             there exists a learning function (2) such that for any δ ∈ (0, 1),
in Xnm = Xn × . . . × Xn . A concept c ∈ Cn can be identified
with its characteristic function c : Xn → {0, 1}. The labels as-               2 The learning function A is often also required to be computable
signed by a concept c to a sample x can be collected into the              in time polynomial in 1ε , 1δ and n (Valiant, 1984). In this paper, I ig-
boolean vector c(x) = (c(x1 ), . . . , c(xm )) ∈ {0, 1}m . For any         nore computational efficiency, and focus on a statistical perspective.
                                                                       1495

any n ∈ N, any distribution P over Xn and any concept c ∈ Cn ,       Where (*) holds because Sn is shattered by monotone quanti-
condition (4) holds with m = m(ε, δ, n)                              fiers, as all properties in Sn have the same cardinality and thus
           n                         1         1 o                cannot be in a subset relation. And (**) holds because |Sn | is
        Pm x ∈ Xnm ec,P A x, c(x) ≥ −                 ≤ δ (4)        the number of subsets of cardinality n2 out of n elements. 
                                            2 q(n)
                                                                     A restriction that    has no learnability effects. Given          a con-
and m(·, ·, ·) grows polynomially in 1ε , 1δ and n.
                                                                     cept class C = ∞       C
                                                                                      S                                         S∞
                                                                                        n=1    n over  a  sample    space   X =    n=1 n and
                                                                                                                                       X
Weak and strong PAC-learnability are equivalent (Schapire,           another concept class     C 0 = S∞ C 0 over a possibly different
                                                                                                         n=1 n
1990), because of their distribution-independent nature (i.e.,       sample space X 0 = ∞           0                                0
                                                                                             n=1 Xn , C is PAC-riducible to C accord-
                                                                                           S
the learning function needs to succeed for any distribution P).      ing to Pitt and Warmuth (1990) iff there exists a polynomial
Thus, weak PAC-learning is only interesting for fixed prob-          p(·) and two PAC-reduction maps
ability distributions. Consider the uniform distribution U.
                                                                                                  0                                       0
VCD also controls the sample cardinality for uniform weak              R1 : x ∈ Xn 7→ R1 (x) ∈ X p(n)          R2 : c ∈ Cn 7→ R2 (c) ∈ C p(n)
PAC-learnability (Blumer et al., 1989): no learning function
A satisfies (4) with sample cardinality m = m(ε, δ, n) smaller       such that for any example x ∈ Xn and any concept c ∈ Cn
than VCD(    Cn )−1 
                      . Reasoning as above, I thus obtain Result     we have that x ∈ c iff R1 (x) ∈ R2 (c), i.e. the behavior of the
          1      1
       32 2 − q(n)                                                   transformed concept on the transformed examples is exactly
2, that substantially strengthens Result 1.                          the behavior of the original concept on the original exam-
RESULT 2 The class Q of generalized quantifiers is not               ples. If C is PAC-reducible to C 0 and C 0 is PAC-learnable,
weakly PAC-learnable relative to the uniform distribution.           then C is    PAC-learnable too (Pitt & Warmuth, 1990). Let
                                                                     Q M,+ = ∞n=1 QnM,+ be the concept class of monotone+ quan-
                                                                               S
Results 1 and 2 lend support to the initial conjecture that UG
                                                                     tifiers. Result 4 says that, despite the fact that Q M,+ is smaller
needs to enforce universal restrictions on the denotations of
                                                                     than Q M , it has no learnability advantages. This result shows
quantifiers in order to make the acquisition of quantifiers fea-
                                                                     the importance of exploring the learnability implications of
sible. The rest of the paper thus investigates the learnability
                                                                     universal restrictions within an explicit learnability frame-
implications of these universals restrictions.
                                                                     work. The proof is based on a technique from Kearns et al.
   Monotonicity does not help with learnability                      (1994) and Pitt and Warmuth (1990).
Monotonicity has a modest learnability effect. A universal           RESULT     4 The subclass Q M,+ is PAC-reducible to the entire
restriction on quantifiers in Natural Language is that they be       class Q of monotone quantifiers.
                                                                               M
monotone, according to Definition 1a. I thus focus on the            Proof. For any n, order the elements in the domain of quan-
PAC-learnability of the class of monotone quantifiers.               tification Dn into a sequence, so that Dn = (d1 , . . . , dn ). Let
                                             S∞
DEF 5 Consider the sample space X = n=1 Xn where Xn =                p(n) = 2n. Define the reduction map R1 : x ∈ Xn 7→ R1 (x) ∈
℘(Dn ) is the collection of properties over a domain of quan-        X2n as follows: for i = 1, . . . , n, let di ∈ R1 (x) iff di ∈ x;
tification Dn of cardinality n. A generalized quantifier q ∈ Qn      and for i = n + 1, . . . , 2n, let di ∈ R1 (x) iff di−n 6∈ x. Then,
is monotone+ provided that, if B1 ∈ q and B1 ⊆ B2 , then             R1 (y) ⊆ R1 (x) entails y = x. Define next the reduction map
B2 ∈ q, for any two properties B1 , B2 ∈ Xn ; it is monotone−        R2 : q ∈ Qn 7→ R2 (q) ∈ Q2n   M,+
                                                                                                       as follows: if q = {x1 , . . . , xm } ∈
provided that, if B1 ∈ q and B2 ⊆ B1 , then B2 ∈ q; it is            Qn , then R2 (q) ∈ Q2n is the monotone+ quantifier that con-
                                                                                            M,+
monotone     provided it is either monotone+ or monotone− .          sists of the properties R1 (x1 ), . . . , R1 (xm ) as well as of all their
Q = n=1 QnM is the concept class of monotone quantifiers.
   M
        S∞
                                                                     supersets. It is easy to check that x ∈ q iff R1 (x) ∈ R2 (q). 
Result 2 says that the entire class Q of generalized quantifiers
is not weakly PAC-learnable w.r.t. the uniform distribution.          Conservativity and invariance help learnability
A construction by Kearns et al. (1994) can be readapted to           A determiner ∆ is conservative (Definition 1b) provided that
show that the subclass Q M of monotone quantifiers is instead        B ∈ ∆(A) iff A ∩ B ∈ ∆(A), for any properties A, B ⊆ D. As-
weakly PAC-learnable w.r.t. the uniform distribution. Mono-          sume that ∆ is furthermore invariant (Definition 1c). Thus,
tonicity thus does lead to a learnability advantage. But the         whether it is the case that A ∩ B ∈ ∆(A) depends only on the
advantage is very modest, as it relies on the assumption of          cardinality of A ∩ B. Thus, to learn from examples the deno-
uniform distribution, which cannot be relaxed, by Result 3.          tations of quantified noun phrases projected by conservative
RESULT 3 The class Q M = n=1 QnM of monotone quantifiers
                               S∞
                                                                     and invariant determiners means to learn the concept class
is not (weakly) PAC-learnable (w.r.t. arbitrary distributions).      Q C,I defined below, that is the focus of this section.
                                                                           6 Consider the sample space X = ∞
                                                                                                                       S
Proof. As recalled, it is sufficient to show that the Vapnik-        DEF                                                 n=1 Xn where Xn =
Chervonenkis dimension VCD(QnM ) of monotone quantifiers            ℘(Dn ) is the collection of properties over a domain of quan-
grows super-polynomially in n. Assume n is even and let              tification Dn of cardinality n. Consider the concept class
Sn ⊆ Xn be the subset of properties of cardinality n2 . Thus:        Q C,I = ∞n=1 QnC,I where QnC,I is the collection of those gen-
                                                                              S
                             n/2  n 2 n/2  n                   eralized quantifiers q ∈ ℘(Xn ) that are conservative and in-
             (∗)       (∗∗) n                             √          variant, namely satisfy the following implication: if A ∈ q and
VCD(QnM ) ≥ |Sn | = n = ∑ 2 ≥ ∑ 2 = 2n
                             2    k=0 k         k=0 k
                                                                     |A| = |B|, then B ∈ q, for any properties A, B ⊆ Xn .
                                                                 1496

Plain PAC-learnability. A learning function A as in (2)                         The learning function (6) differs slightly from (2): the pa-
is consistent provided that for any labeled sample (x, t) ∈                     rameter δ has been suppressed because uninformative sam-
Xnm × {0, 1}m , it returns a concept ĉ = A(x, t) that classi-                  ples are already averaged out by the statistical information;
fies the examples x = (x1 , . . . , xm ) according to the labels                the parameter ε is provided to the learning function; the pa-
t = (t1 , . . . ,tm ), i.e. ĉ(xi ) = ti . A consistent learning func-          rameter n is provided as well (that was not necessary in (2),
tion satisfies the PAC-learnability condition (3) provided its                  because implicit in the sample x ∈ Xnm ). We have that:
sample cardinality m is large enough (Blumer et al., 1989):                     RESULT 6 The class Q C,I of conservative and invariant gen-
                                  
                                    4        2 8VCD(Cn )            13
                                                                               eralized quantifiers is PAC-learnable from approximated sta-
         m(ε, δ, n) ≥ max             log ,                     log      (5)    tistical information.
                                    ε        δ              ε        ε
                                                                                Proof. Define the functions Φ0 , Φ1 , . . . : X × {0, 1} → {0, 1}
The Vapnik-Chervonenkis dimension of the concept class                                                                                                     .
                                                                                by setting Φi (x,t) = 1 iff t = 1 and |x| = i. Let m(ε, n) = n + 1
Q C,I of conservative and invariant quantifiers is n + 1. The                                         . ε
                                                                                and τ = τ(ε, n) = 3n . Define the learning function A of the
bound in (5) is thus compatible with a polynomial sample                        form (6) as follows: for any parameters ε and n and for any
cardinality function m. I thus only need to construct a con-                    vector p ∈ [0, 1]m (whose m = n + 1 components are indexed
sistent learning function A for Q C,I . Assume A takes a la-                    from 0 to n), let A(ε, n, p) be the generalized quantifier in
beled sample (x, t) ∈ Xnm × {0, 1}m and returns the general-                    QnC,I that contains properties of cardinality i ∈ {0, . . . , n} iff
ized quantifier q = A(x, t) that contains the properties of a                   pi ≥ 3n2ε
                                                                                           . For any q ∈ QnC,I , I can thus bound as follows:
certain cardinality i ∈ {0, 1, . . . , n} iff one of the properties in
                                                                                                                                      n                          
the sample x with a positive label in t has cardinality i. This                                                   q                                    |x| = i
                                                                                  P x ∈ q x 6∈ A ε, n, ( p̂0 . . . p̂qm ) = ∑ P x ∈ q q 2ε
                                                                                                                             
learning function is obviously consistent. Hence:                                                                                    i=0
                                                                                                                                                       p̂i < 3n
                                                                                        n                                   n
RESULT 5 The class Q C,I of conservative and invariant quan-                                                                       
                                                                                  = ∑ P x ∈ q |x| = i = ∑ P x ∈ Xn Φi (x, q(x)) = 1
tifiers is PAC-learnable.                                                                q   2ε                              q   2ε
                                                                                  i=0, p̂i < 3n                       i=0, p̂i < 3n
Result 5 contrasts sharply with Results 1 and 3, lending sup-                           n
                                                                                                q
                                                                                                                       n
                                                                                                                               2ε       ε
port to a learnability explanation of the universals of invari-                   ≤   ∑       p̂i + τ(ε, n) ≤        ∑              +     ≤ε
                                                                                         q 2ε                           q 2ε   3n      3n
ance and conservativity. I now strengthen Result 5 by looking                     i=0, p̂i < 3n                 i=0, p̂i < 3n
at more demanding PAC-learnability notions.                                     If a property x of cardinality i does not belong to the quantifier
PAC-learnability from statistical information. According                        q, Φi x, q(x) =0 for any x and P{x∈Xn |Φi (x, q(x)) = 1}=0.
to PAC-learnability (Definition 2), a learning function has                                 q                 ε
                                                                                Hence p̂i ≤ τ(ε, n) ≤ 3n         , and the following quantity is zero.
access to a labeled sample of a concept. I now consider                                                       q                                   q
                                                                                P x 6∈ q x ∈ A ε, n, ( p̂0 . . . p̂qm ) = P x 6∈ q p̂i ≥ 3n            2ε
                                                                                                                                   
a stronger notion of PAC-learnability, whereby the learning                                                                                               , i = |x|
function has access only to statistical information about a                                                          q          q   
                                                                                As the error eq,P A ε, n, ( p̂1 , . . . , p̂m ) is the sum of the two
concept. For instance, statistical information concerning a
                                                                                terms just bounded, (7) holds.                                                   
concept c ⊆ Xn over the set Xn of patients of age n could be
the probability w.r.t. a certain distribution P that a patient in c             PAC-learnability from misclassified examples. Accord-
is overweight. This probability is P{x ∈ Xn | Φ(x, c(x)) = 1}                   ing to PAC-learnability (Definition 2), the learning function
where Φ(x,t) = 1 iff t = 1 and x is overweight.                                 is trained on a sample x = (x1 , . . . , xm ) ∈ Xnm together with
                                                                                the corresponding correct labels c(x) = (c(x1 ), . . . , c(xm )) as-
DEF 7 A concept class C is PAC-learnable from the exact sta-
                                                                                signed by a target concept c. I now consider a stronger notion
tistical information induced by Φ1 , Φ2 , . . . : X×{0, 1}→{0, 1}               of PAC-learnability, whereby some of the labels c(xi ) are al-
with sample cardinality m : (ε, n) ∈ (0, 1)×N → m(ε, n) ∈ N                     tered. Given ξ ∈ {0, 1} and x ∈ Xn , define c(x, ξ) = c(x) iff
polynomial in 1ε , n provided there exists a learning function                  ξ = 1. Assume that ξ is sampled according to a Bernoulli
    A : ε, n, p ∈ (0, 1) × N × [0, 1]m 7→ A ε, n, p ∈ Cn (6)
                                                                              Bη with probability of success ξ = 1 equal to η ∈ [0, 1]. The
                                                                                m-tuple (c(x1 , ξ1 ), . . . , c(xm , ξm )) is denoted by c(x, ξ).
such that for any ε ∈ (0, 1), any n ∈ N, any concept c ∈ Cn ,                   DEF 8 A concept class C is PAC-learnable from misclassified
any probability P over Xn , the following condition holds                       examples with sample cardinality function m : (ε, δ, n, η) ∈
                                                                                (0, 1) × (0, 1) × N × [0, 12 ) 7→ N if there is a learning function
                                                           
                        ec,P A ε, n, ( p̂c1 , . . . , p̂cm ) ≤ ε         (7)
                                                                                                                    1
where p̂ci is exact statistical information, namely:                            A : (ε, η, x, t) ∈ (0, 1)×[0, )×Xnm ×{0, 1}m 7→ A(ε, η, x, t) ∈ Cn
                                                                                                                    2
                        p̂ci = P{x ∈ Xn |Φi (x, c(x)) = 1}               (8)    such that for any ε, δ ∈ (0, 1), any n ∈ N, any η ∈ [0, 12 ), any
Also, C is PAC-learnable from approximated statistical infor-                   concept c ∈ Cn , and any probability P over Xn , we have
                                                                                                     n                                              o
mation provided (7) holds with (8) replaced by                                          Pm × Bm                                                    ≥ε δ
                                                                                                   η (x, ξ) ec,P A ε, η, x, c(x, ξ)
                  | p̂ci − P{x ∈ Xn |Φi (x, c(x)) = 1}| ≤ τ
                                                                                and furthermore the sample cardinality                 m(·, ·, ·, ·) grows poly-
                                                 1
                                                                                nomially in 1ε , 1δ , n and 1/ 12 − η .
                                                                                                                              
for some constant τ ∈ (0, 1] with                τ   ≤ m(ε, n).
                                                                            1497

The misclassification rate η cannot be larger than 21 , other-          and furthermore the sample cardinality function m(·, ·, ·)
wise learning would be impossible. As the complexity of the             grows polynomially in 1ε , 1δ , and n
learning task increases as η gets closer to the threshold 12 ,          PAC-learnability from misclassified examples (Definition 8)
the cardinality m of the    sample is allowed to grow (polyno-         allows the error rate η to vary arbitrarily between 0 and 12 . In
mially) with 1/ 21 − η . The learning function A is provided            the more demanding case of PAC-learnability with malicious
with the noise rate η and the accuracy parameter ε. PAC-                error, η is only required to vary between 0 and the malicious
learnability from statistical information is known to entail            error-rate µ(ε, δ, n) < 12 . I now show that:
PAC-learnability from a misclassified sample (Kearns, 1998).
                                                                        RESULT 8 The class Q C,I of conservative and invariant quan-
From Result 6 we thus have:
                                                                        tifiers is PAC-learnable from positive examples only with
RESULT 7 The class Q C,I of conservative and invariant gen-             sample cardinality m and malicious error rate µ as follows:
eralized quantifiers is PAC-learnable from misclassified ex-                                                       
amples.                                                                                     24n                   4                     ε
                                                                           m(ε, δ, n) ≥           (n + 1) + ln        , µ(ε, δ, n) ≤      (12)
                                                                                              ε                   δ                   8n
PAC-learnability from positive, malicious examples. Ac-
cording to PAC-learnability (Definition 2), when the learning           The proof rests on the following result (Kearns & Li, 1993).
function is trained on a target concept c, it is provided with          Suppose the sample cardinality m is large, as in (13).
a sample x = (x1 , . . . , xm ) ∈ Xnm that in general contains both                                           24      4|C | 
                                                                                                                           n
positive examples xi ∈ c and negative examples xi ∈ c (where                                   m(ε, δ, n) ≥      log                      (13)
                                                                                                               ε         δ
c is the complement of c w.r.t. Xn ). I now consider a stronger
notion of PAC-learnability, whereby x is sampled w.r.t. a dis-          Suppose furthermore that for some ε, δ ∈ (0, 1) and η ∈ [0, 4ε ),
tribution concentrated on c so that the learning function re-           the learned concept A(ε, µ, x) assigns a positive label to many
ceives only positive examples (Kearns et al., 1994).                    of the examples xi in the sample x = (x1 , . . . , xn ), as in (14).
                                                                               m
DEF 9 A concept class C is PAC-learnable from positive ex-                   O        n                                 ε o          δ
amples only with sample cardinality m : (0, 1) × (0, 1) × N →                     P̃k x |{xi |xi 6∈ A(ε, η, x)}| ≤ m ≥ 1 −                (14)
                                                                             k=1
                                                                                                                        2            2
    provided there exists a learning function A : ∞            m
                                                       n,m=1 Xn →
                                                      S
N
   n=1 Cn such that for any ε, δ ∈ (0, 1), any n ∈ N, any concept
                                                                        Then, A has small error relative to P in the sense that:
S∞
c ∈ Cn and any probability measures P, P concentrated over c                          Om       n                          o        δ
and c respectively, condition (9) holds with m = m(ε, δ, n)                                P̃k x ec,P (A(ε, η, x)) ≤ ε ≥ 1 −              (15)
                                                                                      k=1
                                                                                                                                   2
                  (                             )
               m         m ec,P (A(x)) ≤ ε                              Proof. Consider the following learning function: A(ε, µ, x) is
             P      x ∈ Xn                        ≥ 1−δ          (9)
                              ec,P (A(x)) ≤ ε                           the generalized quantifier in QnC,I that contains the properties
                                                                        of cardinality i ∈ {0, . . . , n} iff the sample x contains at least
and furthermore the sample cardinality function m(·, ·, ·)               ε
grows polynomially in 1ε , 1δ and n.                                    4n m properties with cardinality i. As A does not depend on
                                                                        µ, I will write just A(ε, x). Obviously |Q C,I | = 2n+1 , so that
Consider next a noisy variant of this framework, whereby the            (12) entails (13). For any sample x = (x1 , . . . , xm ), the quan-
distribution P used to sample points from c is corrupted: with          tifier A(ε, x) classifies as negative at most 4n      ε
                                                                                                                                m(n + 1) < 2ε m
probability µ, the example xi of the sample is chosen not ac-           of the m examples in x, so that (14) holds too. By the result
cording to the distribution P concentrated on c but according           mentioned above, we thus have for any quantifier q ∈ Q C,I :
to a distribution Qi over the entire Xn . The distribution Qi can
                                                                                    m
be chosen by a malicious adversary that knows the concept c,                                                                         δ
                                                                                  O         n                                o
                                                                                        P̃k x ∈ Xnm eq,P (A(ε, x)) ≤ ε ≥ 1 −
the distribution P, the learning strategy A.                                       k=1
                                                                                                                                     2
DEF 10 The concept class C is PAC-learnable from positive
                                                                        The following chain of inequalities finally proves (11).
examples only with malicious error rate µ : (ε, δ, n) ∈ (0, 1) ×
                                                                                 m
(0, 1) × N 7→ [0, 12 ) and sample cardinality m : (ε, δ, n) ∈                   O       n                            o
                                                                                     P̃k x ∈ Xnm eq,P A(ε, x) 0
(0, 1) × (0, 1) × N 7→ N if there is a learning function
                                                                                k=1
                                                                                    m
                                  1                                                O        n                              o
    A : (ε, µ, x) ∈ (0, 1) × [0, ) × Xnm 7→ A(ε, µ, x) ∈ Cn    (10)             ≤       P̃k x ∃x 6∈ q s.t. x ∈ A(ε, x)
                                  2                                                k=1
                                                                                    n O  m                               
such that for any ε, δ ∈ (0, 1), any n ∈ N, any η ∈                                                  ∃x 6∈ q s.t.
                                                                                ≤∑           P̃k x
[0, µ(ε, δ, n)), any concept c ∈ Cn , any distributions P and                      i=0 k=1
                                                                                                     |x| = i, x ∈ A(ε, x)
                                                                                                                                       
P concentrated over c and c, any additional m distribu-                             n O  m       there is x 6∈ q s.t. |x| = i and 
tions Q1 , . . . , Qm over Xn , conditionN(11) holds, where m =                 =∑           P̃k x the sample x contains at least
m(ε, δ, n), P̃k = (1 − µ)P + µQk and          is measure-product:                  i=0 k=1
                                                                                                 ε
                                                                                                 4n m properties of cardinality
                                                                                                                                     i
                                                                                                                                        
       m
             (                                   )                                  n O  m       x contains at least 
                       m ec,P (A(ε, µ, x)) ≤ ε                                  ≤∑           P̃k x 4n ε
                                                                                                         m properties in q
      O
          P̃k x ∈ Xn                               ≥ 1−δ       (11)
                           ec,P (A(ε, µ, x)) ≤ ε                                   i=0 k=1
                                                                                                
                                                                                                     of cardinality i
                                                                                                                             
      k=1
                                                                    1498

                                         
        n   m      x contains                 n
                                                              δ                             Acknowledgments
                                             ≤ ∑ e−mε/24n ≤
           O
                                    ε
     ≤∑        P̃k x at least 4n      m                                 I would like to thank G. Chierchia. This work was supported
                                                              2
       i=0 k=1          properties in q        i=0
                                         
               |             {z            }                            by a Marie Curie Fellowship (PIEF-GA-2011-301938).
                             (∗)
                                                                                                 References
In the penultimate step, I have noted that the probability (*) is       Barwise, J., & Cooper, R. (1981). Generalized quantifiers and
the probability that m Bernoulli trials each with a probability           natural language. Linguistics and Philosophy, 4, 159–219.
                  ε                           ε
of success µ ≤ 8n    overall yield at least 4n  m successes, which      Benthem, J. van. (1983). Determiners and logic. Linguistics
is bound by e  −mε/24n   through Chernoff inequality.                    and Philosophy, 6.8, 447–47.
   Could a more sophisticated learning function than the one            Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K.
considered in the preceding proof lead to a stronger result?              (1989). Learnability and the vapnik-chervonenkis dimen-
namely a higher noise tolerance or a smaller sample cardinal-             sion. Journal of the ACM, 36.4, 929–965.
ity? No learning function        from positive examples only for a      Clark, A., & Lappin, S. (2011). Linguistic nativism and the
concept class C = ∞
                     S
                             C                                            poverty of the stimulus. Wiley-Blackwell.
                        n=1 n can tolerate a rate of malicious er-
ror µ(ε, δ, n) ≥ VCD(εCn )−2 , as shown in Kearns and Li (1993).        Ehrenfeucht, A., Haussler, D., Kearns, M., & Valiant, L.
                                                                          (1989). A general lower bound on the number of exam-
The concept class Q C,I has Vapnik-Chervonenkis dimension
                                                                          ples needed for learning. Information and Computation,
VCD(QnC,I ) = n + 1. Hence, the largest tolerable rate of mali-
                                                                          82.3, 247–251.
cious error for Q C,I is of the order of nε . The learning function
                                                                        Fox, D. (1999). Reconstruction, binding theory, and the in-
in the proof above thus tolerates the largest possible rate of
                                                                          terpretation of chains. Linguistic Inquiry, 30.2, 157–196.
malicious error.
                                                                        Gamut, L. T. F. (1991). Logic, language and meaning.
   Furthermore, no learning function satisfies the plain PAC-
                                                                          Chicago and London: The University of Chicago Press.
learnability condition (3) with sample cardinality m(ε, δ, n)
                     Cn )−1                                             Heim, I., & Kratzer, A. (1978). Semantics in generative
smaller than VCD(32ε        , as recalled above. And Blumer et            grammar. Blackwell Textbooks in Linguistics.
al. (1989) show that m(ε, δ, n) cannot be smaller than 4ε log 2δ        Hunter, T., & Lidz, J. (2013). Conservativity and learnability
either. PAC-learnability thus requires:                                   of determiners. Journal of Semantics, 30.3, 315–334.
                                                                        Kearns, M. (1998). Efficient noise-tolerant learning from
                               n4       2 VCD(n) − 1 o                    statistical queries. Journal of the ACM, 45.6, 983–10061.
             m(ε, δ) ≥ max           log ,                     (16)
                                  ε     δ      32ε                      Kearns, M. (1999). PAC-learning. In R. A. Wilson &
Thus, the learning function in the preceding proof meets the              F. C. Keil (Eds.), The MIT encyclopedia of cognitive sci-
demanding condition of PAC-learnability from positive ma-                 ences. Cambridge, MA: The MIT Press.
licious examples while using a sample cardinality (12) that             Kearns, M., & Li, M. (1993). Learning in the presence of
asymptotically exceeds only by a factor n the lower bound                 malicious errors. Journal on Computing, 22.4, 807–837.
(16) needed for plain PAC-learnability.                                 Kearns, M., Li, M., & Valiant, L. (1994). Learning boolean
                                                                          formulas. Journal of the ACM, 41.6, 1298–1328.
                           Conclusion                                   Kearns, M., & Vazirani, U. (1994). An introduction to compu-
                                                                          tational learning theory. Cambridge, MA: The MIT Press.
I have looked at the conjecture informally made in the re-              Keenan, E. L., & Stavi, J. (1986). A semantic character-
cent linguistic literature that universal restrictions on Natural         ization of natural language determiners. Linguistics and
Language determiners serve the purpose of simplifying the                 Philosophy, 9, 253–326.
learning task. To start, I have looked at the monotonicity uni-         Montague, R. (1973). The proper treatment of quantification
versal, and I have shown that it contributes only little to sim-          in ordinary english. In P. Suppes, J. Moravcsik, & J. Hin-
plifying the learning task. This result shows the importance              tikka (Eds.), Approaches to natural language. Reidel.
of investigating the conjectured link between universals and            Natarajan, B. K. (1991). Machine learning. A theoretical
learnability within an explicit, formal learnability framework.           approach. Morgan Kaufmann Publishers.
I have then focused on the universals of conservativity and             Pitt, L., & Warmuth, M. (1990). Prediction-preserving re-
invariance. And I have provided support for the conjecture                ducibility. Journal of Computer and System Science, 41.3,
that they crucially simplify the learning task, by showing that           430–467.
the class Q C,I of conservative and invariant quantifiers has the       Schapire, R. E. (1990). The strength of weak learnability.
property that the simplest and most straightforward learning              Machine Learning, 5.2, 197–227.
strategy, namely the one considered in the proof of Result 8,           Valiant, L. (1984). A theory of the learnable. Communica-
is the optimal one, namely the one that tolerates the largest             tions of the ACM, 27.11, 1134–1142.
tolerable rate of malicious error. Furthermore, the class Q C,I         Westerståhl, D. (1989). Quantifiers in formal and natural lan-
has the property that the presence of noise (even malicious               guages. In D. Gabbay & F. Guenthner (Eds.), Handbook of
noise) does not require any substantial increase of the sample            philosophical logic (Vol. 4, pp. 1–131). Reidel Publishing.
cardinality compared to the noise-free case.
                                                                    1499

