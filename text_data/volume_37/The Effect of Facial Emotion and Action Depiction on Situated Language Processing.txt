The Effect of Facial Emotion and Action Depiction on Situated Language
Processing
Katja Münster (kmuenster@cit-ec.uni-bielefeld.de)1,2
Maria Nella Carminati (mnc@interfree.it)
Pia Knoeferle (knoeferl@cit-ec.uni-bielefeld.de)1,2
1 Cognitive Interaction Technology Excellence Center
2 Department of Linguistics
CITEC, Inspiration 1, Bielefeld University
33615 Bielefeld, Germany

Abstract
Two visual world eye-tracking studies investigated the
effect of emotions and actions on sentence processing.
Positively emotionally valenced German non-canonical
object-verb-subject (OVS) sentences were paired with a
scene depicting three characters (agent-patient-distractor)
as either performing the action described by the sentence,
or not performing any actions. These scene-sentence pairs
were preceded by a positive prime in the form of a happy
looking smiley (vs. no smiley) in experiment 1 and in the
form of a natural positive facial expression (vs. a negative
facial expression) in experiment 2. Previous research has
demonstrated the effect of action depiction on sentence
processing of German OVS sentences (Knoeferle, Crocker,
Scheepers, & Pickering, 2005). Moreover, emotional
priming facilitates sentence processing for older and
younger adults (Carminati & Knoeferle, 2013). However,
up to date there is no evidence as to whether schematic
faces such as smileys are as effective as natural faces in
facilitating sentence processing. These insights lead to the
hypotheses that participants would not only profit from
depicted events, but that processing of OVS sentences
might also be positively affected by emotional cues. Plus,
we assessed the degree of naturalness the emotional face
needs to possess to affect sentence processing. Results
replicate the predicted effect of action depiction (vs. no
action depiction). The expected facilitatory effect of
emotional prime is trending in both experiments. However,
the effect is more pronounced in the natural face version
(exp. 2) than in the smiley version (exp. 1).
Keywords: Sentence Processing; Visual World Paradigm;
visually situated language comprehension; eye movements;
emotional priming; iconic, natural facial expression.

Introduction
Monitoring people’s gaze behavior in a visual context
provides a unique opportunity for examining the
incremental integration of visual and linguistic
information (Tanenhaus et al., 1995). Non-linguistic
visual information can rapidly guide visual attention
during on-line language processing in adults (e.g.,
Chambers, Tanenhaus, & Magnuson, 2004; Knoeferle et

al., 2005; Sedivy et al., 1999; Spivey et al., 2002).
However, existing research has focused mostly on
assessing how object- and action-related visual
information influences spoken language comprehension.
By contrast, the integration and role of social contextual
cues into language processing, such as emotional facial
expressions is still under-examined, and we do not know
which degree of naturalness (and corresponding degree of
detail) is needed for comprehenders to exploit them. Yet,
in natural conversation such social cues are highly
relevant for interpretation and the extent to which they
rapidly impact language comprehension deserves
attention.
Additionally, we do not know to which extent the
portrayal of facial emotions and action events relative to
one another modulates visual attention and language
comprehension. The link between social cues (e.g., a
smile) and their associated scene aspects (other smiles or
a positively valenced action) is naturally more subtle than
the link between an action and its corresponding verb.
We therefore assessed whether adults can make use of
direct action-related visual information on the one hand,
and contextual social information of varying degree of
naturalness on the other hand for sentence comprehension
of non-canonical German object-verb-subject (OVS)
sentences. We used OVS sentences since these are
grammatical but non-canonical in German. They permit
us to assess to which extent their associated processing
difficulty is alleviated by the two manipulated cues. To
motivate our studies in more detail, we first review
relevant literature on the use of visual referential context
of depicted actions in sentence comprehension, on the
effect of extralinguistic social cues on sentence
comprehension and on the difference between schematic
versus natural facial expression depictions.

Sentence Comprehension and the Visual Context
Adults can rapidly use visual referential context for
disambiguating structurally ambiguous sentences. In a
real-world study, the context showed an apple, an apple
on a towel, an empty towel and a box and participants

1673

listened to related sentences (e.g., Put the apple on the
towel in the box). They immediately fixated the apple
located on the towel upon hearing the ambiguous phrase
“on the towel”. They thus used the visual context to
interpret “on the towel” as the modifier of “the apple”,
specifying its location (Tanenhaus et al., 1995). Adults
can further use depicted actions to facilitate role
assignments in non-canonical OVS sentences (Knoeferle
et al., 2005). In a visual world eye-tracking study, clipart
scenes depicted agent – action – patient events; as
participants inspected the scene, they listened to sentenceinitial ambiguous German SVO and OVS sentences
describing the events (e.g., a princess was depicted and
described as being painted by a fencer). Results
demonstrated that participants used the depicted action
events to rapidly anticipate role relations and resolve the
initial ambiguity.

Effects of Extralinguistic Social Cues (Faces)
Can other aspects of the visual context such as
extralinguistic social cues likewise modulate and facilitate
the processing of structurally difficult sentences, i.e.,
facilitate role assignment in non-canonical OVS
sentences? Emotions and emotional facial expressions are
essential for social interactions. They are extensively
exploited when interpreting an interlocutor’s utterances
and feelings (cf. Nummenmaa, Hyönä, & Calvo, 2006).
Moreover, using emotional priming in a visual world
paradigm, Münster, Carminati and Knoeferle (2014)
showed that videos of dynamically unfolding emotional
facial expressions of real faces facilitated the processing
of emotionally valenced canonical SVO sentences when
the emotional valence between prime face and target
sentence matched. Dynamic emotions are further
recognized faster and more accurately and elicit enhanced
and prolonged cortical responses vs. static counterparts
(see e.g., Harwood, Hall, & Shrinkfield, 1999 and Recio,
Sommer, & Schacht, 2011 for ERP evidence). Despite
this, the processing of emotional sentences was equally
facilitated by natural dynamic and static faces in Münster
et al., (2014; see also Carminati & Knoeferle, 2013).
In summary, the findings by Münster et al. (2014) and
by Carminati and Knoeferle (2013) showed for the first
time that facial expressions can incrementally modulate
adults’ processing of emotional sentences. Yet, the
referential integration of emotional valence was purely
semantic. However, using non-canonical OVS sentences,
the present study focuses on the facilitation of
compositional integration, i.e., the anticipation of a target
agent prior to it being mentioned in a sentence-initial role
ambiguous situation.

Degree of Naturalness: Real vs. Schematic Faces
For interpreting utterances in natural social situations, we
can rely on the full range of facial features and motions
used to denote an emotional expression. However,
whether the degree of naturalness of the facial expression
matters (in modulating emotional effects on sentence
processing) remains to be seen. For instance, research on

emotional face recognition has also used computergenerated schematic faces; generally the evidence
suggests that these are recognized as well as natural faces
(e.g., Ruffman, Ng, & Jenkin, 2009, Öhman, Lundqvist,
& Esteves, 2001; Chang, 2006).
We do not yet know, however, whether schematic or
real facial expressions are equally facilitating cues during
sentence processing. Is a schematic expression where
emotion is stripped down to its bare essential (e.g., in
smileys) sufficient, or do we need more detailed and
natural emotional information to elicit emotional priming
effects on online sentence comprehension? The present
research addressed this question while building on the
reviewed results.

The Present Research
In two visual world eye-tracking experiments we
investigated how the processing of non-canonical German
OVS sentences is influenced by two cues, i.e., (a) the
presence (vs. absence) of an emotional prime (a “smiley”
in exp.1; a natural facial expression in exp. 2, Fig. 1), and
(b) the presence (vs. absence) of depicted action events.
Participants saw either an emotionally positive prime
(smiley in exp. 1 or positive natural expression in exp. 2,
see Fig. 1) or an incongruent prime (a star in exp 1 and a
sad natural expression in exp. 2) followed by a clipart
scene depicting three characters (agent – patient –
distractor, Fig. 2). Shortly after the onset of the scene,
participants listened to a positively emotionally valenced
OVS sentence describing the scene in a “who-does-whatto-whom” fashion (Den Marienkäfer kitzelt vergnügt der
Kater, transl.: ‘The ladybug (patient) tickles happily the
cat (agent)’). In half of the trials the action mentioned in
the sentence (e.g., tickling) was performed by the
characters and depicted as an object on the screen (i.e., a
feather, see Fig. 2); in the other half of the trials, the
characters did not perform any actions i.e., no objects
were depicted (Table 1). Participants orally answered a
comprehension question in the active or passive voice
asking for the agent or the patient of the action (exp. 1) or
a passive comprehension question asking for the patient
of the action (exp. 2) after each sentence.
Regarding the processing advantage for depicted
actions (Knoeferle et al., 2005), we predicted facilitation
in processing the OVS sentence when an action (vs. no
action) was depicted. This should manifest itself in more
fixations towards the agent of the sentence (vs. the
distractor) in the depicted-action condition than in the noaction condition. Importantly, in the depicted-action
conditions we expect fixations to the agent to be
anticipatory (i.e., appearing before the mention of the
agent), if listeners are able to use the actions proactively
while they process the sentence (Knoeferle et al., 2005).
Additionally, we predict more looks (anticipatory or not)
towards the agent (vs. distractor) in the positive (vs.
incongruent) prime condition, if the positive emotional
prime facilitates processing of positive non-canonical
(OVS) sentences.

1674

with each other, the single-cue conditions should show
higher accuracy.

Experiment
Participants
Figure 1: Positive emotional primes for exp. 1 (smiley)
and exp. 2 (happy natural facial expression)

40 adults (ages 18-30) participated in experiment 1 and 40
different adults in the same age range participated in
experiment 2. All had German as their only mother
tongue and normal or corrected-to-normal vision.

Materials and Design

Figure 2: Example scene of an experimental trial with
depicted actions associated with the OVS sentence Den
Marienkäfer kitzelt vergnügt der Kater.

a)

b)
c)
d)

Table 1: Experimental Conditions
positive
action
Den Marienkäfer kitzelt
prime
depiction vergnügt der Kater .
Transl.: ‘The ladybug
(patient) tickles happily
the cat (agent)’
positive
no action Den Marienkäfer kitzelt
prime
vergnügt der Kater .
incongruent action
Den Marienkäfer kitzelt
prime
depiction vergnügt der Kater .
incongruent no action Den Marienkäfer kitzelt
prime
vergnügt der Kater .

Concerning the prime manipulation, we predict more
pronounced effects with the real face (exp. 2) than the
smiley (exp. 1) if the natural facial expression is a
stronger cue than the iconic smiley. A stronger effect of
the natural face should manifest itself in more fixations
towards the agent in the positive (vs. incongruent) prime
condition compared to the smiley version. Regarding the
interaction between the depicted action and the positive
prime, we predicted more looks towards the agent (vs.
distractor) in the condition where both cues are present
(vs. single cue or no cue conditions) if both cues can be
integrated and used to facilitate role assignment at the
same time. If, however, the different cues interfere with
each other, we should see more fixations towards the
agent (vs. distractor) in the single cue conditions (vs. the
two-cue condition).
Offline, a processing advantage for the depicted-action
condition (vs. no-action) should manifest itself also in
higher accuracy for the comprehension questions.
Likewise we predict higher accuracy for the prime (vs.
incongruent prime) condition, if the positive emotional
prime facilitates the comprehension of the non-canonical
OVS sentences. Regarding a possible interaction of cues,
we predict higher accuracy for the two-cue condition (vs.
single-cue) if both cues additively facilitate answering the
comprehension question. If, however, the cues interfere

Materials and design were identical for both experiments,
except that exp.1 used a happy smiley (vs. a star), while
exp. 2 used a happy (vs. unhappy) natural facial
expression as a prime (Fig. 1).
We created 16 experimental positively valenced
German OVS sentences in the form ‘The [agent accusative
case ] [Verb] [positive Adverb] the [patientnominative case].’ and
28 filler sentences (thereof 24 SVO sentences). Filler
sentences were balanced for neutral and positive valence.
For each sentence we created a clipart scene. The 16
experimental scenes each consisted of three characters
(agent – patient – distractor). The sentential patient was
always the middle character. Both agent and distractor
character were facing the patient. The patient always
faced the agent of the sentence (see Fig. 2). The
characters were balanced for left and right positioning,
across items and the different experimental lists. The
scenes for the 28 filler sentences depicted either 2 or 3
characters, balancing out the number of characters in a
scene across all 44 trials. In addition, in exp 2 we changed
the positioning of filler characters, so that agent and
patient did not always face each other and/or were
positioned next to each other. This was done to prevent
strategy development as to who is interacting with whom.
To match the positive sentence valence, the agent
portrayed a happy facial expression in all experimental
scenes. The patient always portrayed a neutral expression
and the distractor character a slightly negative one.
For each experimental scene we created two versions of
the same picture. One version where the action verb
mentioned in the sentence was depicted (i.e., the
characters are performing the action of the sentence), and
one version where the characters were not performing any
action, i.e., they simply stand next to each other. If actions
were depicted, both the agent and the distractor character
performed an action towards the patient (middle
character, see Fig. 2). In the filler pictures, action
depiction was balanced, so that in half of all 44 trials an
action was depicted.
We crossed the depicted-action vs. no-action scenes
with the positive prime vs. incongruent prime condition.
In exp. 1 the prime consisted of a happy looking smiley;
the incongruent prime condition consisted of a star. In
exp. 2 the prime condition consisted of a woman’s facial
expression showing a broad and open smile, the
incongruent prime condition had the same woman

1675

portraying a sad expression. As only the positive
emotional expression matches in valence with the
sentence and the facial expression of the target agent, we
call the positive facial expression the prime. We will refer
to the star (exp. 1) and the negative facial expression (exp.
2) as the incongruent prime condition.

Procedure
An Eyelink 1000 Desktop Mounted System with remote
setup monitored participants’ eye movements. Only the
right eye was tracked, but viewing was binocular. Each
trial started with a prime (happy smiley for exp. 1 or
positive natural facial expression for exp. 2) or
incongruent prime (star for exp. 1 or negative facial
expression for exp. 2). The prime in exp. 1 was presented
for 1750 ms, the prime in exp. 2 was presented for 5500
ms. Both prime versions were accompanied by the spoken
phrase “Look!” to catch participants’ attention and direct
it to the following scene. 2000 ms after the onset of the
target screen the sentence was presented. 500 ms after the
end of the sentence, all depicted actions (if present) were
removed from the scene and the comprehension question
was heard. After participants had responded to the
question, the experimenter proceeded to the next trial.
Each experiment took approximately 30 minutes.

Analysis
We divided the sentence into individual word regions and
a long region encompassing the whole sentence. We will
focus on our critical regions (the Verb and Adverb). The
Verb region is the first region where the agent of the
sentence can be anticipated if an action is depicted on the
screen. The Adverb region gives away the valence of the
sentence and thus matches in positive valence with the
prime in the prime condition. For each region (and a
combined Verb-Adverb region), we computed the mean
log gaze probability ratio according to the formula: Ln
(p(agent)/p(distractor)). Ln refers to the logarithm and p
refers to probability. This ratio expresses the bias of
inspecting the agent relative to the distractor. The ratio
does not violate the independence and homogeneity of
variance assumptions, which makes it suitable for
comparing looks to two scene regions with parametric
tests such as Analyses of Variance (ANOVAs, see, e.g.,
Arai, Van Gompel, & Scheepers, 2007). More looks to
the agent (vs. distractor) are indexed by a positive log
ratio. More looks to the distractor (vs. agent) are indexed
by a negative log ratio.
We computed mean log gaze probability ratios for each
region separately by participants and items. These means
were then subjected to ANOVAs with participants and
items as random effects. Accuracy scores were computed
for each experiment by condition.

Results
Main results for the accuracy analysis in exp. 1. Only
experimental trials were included in the analyses.
Participants answered 95.93% of all trials correctly. We

did not find main effects of prime or action depiction. The
interaction between prime and action depiction was also
not significant. However, we saw a main effect of voice
(p<.05). Subjects’ answers were significantly more
accurate when the comprehension question was posed in
the active than in the passive voice. Moreover, we found
an unexpected significant voice x prime interaction in the
item analysis (p<.05). This interaction shows that the
presence of the emotional prime proves helpful for
answering the comprehension question but only with the
questions in the passive voice (Fig. 3).

Figure 3: Voice x prime interaction in the accuracy results
(exp. 1)
Main results for the accuracy analysis in exp. 2.
Participants answered 95.6% of the comprehension
question correctly. Results yielded a significant effect of
action depiction (p<.05) and a marginal effect of prime.
When an action was (vs. wasn’t) depicted, participants
were significantly more accurate in answering the
question. Moreover, they were more accurate when the
valence of the prime was positive (i.e., congruent) than
when it was incongruent.
Main results for the eye-movement analysis in exp. 1.
Results revealed a significant effect of action depiction in
all analyzed regions (all ps<.05), indicating a strong
preference to look more at the agent (vs. distractor) when
the mentioned action was (vs. wasn’t) depicted. Although
we did not find a significant effect of the emotional
prime, nor any interactions, the means of the interactions
of all analyzed regions show that participants look more
at the agent, i.e., the subject of the sentence (vs. the
distractor) when the emotional prime is positive (vs.
incongruent), but only in the condition for which an
action is also present (Fig. 4).
Main results for the eye-movement analysis in exp. 2.
Results revealed a significant effect of action depiction in
all analyzed regions (all ps <.05), indicating a strong
preference to look more at the agent than the distractor
when the mentioned action was (vs. wasn’t) depicted.
Moreover, we found a marginal effect of prime for the
Adverb, Verb-Adverb (Fig. 5) and the long region. While
the interactions were not significant, the means of all
analyzed regions clearly show that participants look more
at the agent (vs. the distractor) when the emotional prime

1676

(happy face) is presented (vs. incongruent prime), but
only when actions are also depicted.

Figure 4: Prime x action interaction for the adverb region
(exp. 1)

Figure 5: Marginal effect of prime for verb-adverb region
(exp. 2)

Discussion
In two visual-world eye-tracking experiments, we
assessed to which extent adults can use depicted actions
and positive facial emotions for understanding noncanonical OVS sentences. Moreover, we investigated
whether the form of the prime face (schematic vs. real
face) modulates potential facilitative effects of the
emotional primes on sentence processing.
Our results show that participants make extensive use
of depicted action events (vs. no actions) in identifying
the agent of the sentence when processing the OVS
sentences. More interestingly, adults also seem to make
use of the non-linguistic social cue, i.e., the positive
emotional facial prime in processing the OVS sentences.
Crucially, the means of the action x prime interactions
indicate that the emotional cue is only used in
combination with a depicted action being also present,
irrespective of its schematic or natural appearance. One
possibility is that the facial expression is merely used for
reassurance purposes, i.e., because it corroborates the
already powerful cue provided by the depicted action. In
fact, the action object depicted on the screen referentially
links to the linguistic input (the verb). On the other hand,
when the positive face prime appears without the action,
the link between the prime, the agent’s positive facial
expression and the positive valence of the sentence is
much less direct, perhaps also because the valence of the

prime is not explicitly mentioned in the sentence. The
listener arguably must infer the relationship between these
different kinds of cues (facial prime, the target agent’s
facial expression and sentence valence) to integrate them
during sentence comprehension. It appears that this
integration does not happen on the fly, unlike establishing
a direct link between the depicted actions and the verb.
Thus, the emotional cue may only be used to re-confirm
the role relations that are assigned by “direct” reference to
the visual context when the action is depicted.
As regards the facial prime manipulation, we observed
essentially the same prime x action interaction pattern
(Fig. 4) with the schematic smiley (exp. 1) as with the
natural facial expression (exp. 2). These results are
broadly in line with findings that viewers recognize
schematic and natural faces equally well (Ruffman et al.,
2009; Öhman et al., 2001; Chang, 2006, but see Prazak &
Burgund, 2014, for evidence that processing of real faces
may be more holistic than that of schematic ones).
However, our analyses revealed a marginal effect of
prime on sentence processing with natural face primes
(but not with the smileys). The natural facial expression
appears to be used to a greater extent during sentence
interpretation and thematic role assignment than the
smiley. A further reason for this difference could be a
design feature of our experiments. In exp. 2 the
incongruent prime condition was a sad face while in exp.
1 this condition was a neutral star symbol (i.e., not a sad
smiley). This may have contributed to enhancing attention
to the positive face (i.e., because it appeared in contrast to
a sad one in exp. 2). In turn this may have contributed to a
greater effect of the natural happy faces on sentence
processing in exp 2 than for the happy smiley in exp 1. In
addition, the natural faces were presented longer,
arguably enabling more in-depth interpretation of the
emotions and thus stronger effects on visual attention.
Another factor that may have contributed to a better
integration of the natural face prime into the processing of
the sentence (in combination with the design factor
mentioned above) is that in exp. 2 the natural prime face
‘fitted’ better with the experimental context, in the sense
that this face was more readily perceived to be the face of
the speaker of the following sentence (which was spoken
by a female voice). This arguably contributed to rendering
the experimental scenario of exp. 2 more naturalistic, i.e.,
more similar to everyday situations in which we can use
non-linguistic information such as the facial expression of
our interlocutor to help us interpret the linguistic input
within its (non-)linguistic context. Moreover, as in exp. 2
we also ruled out possible strategic effects regarding the
facing of agent and patient characters, the effects cannot
be due to an inspection of the pictures alone. In order to
reliably assign thematic roles, visual cues have to be
integrated into the processing of the linguistic input.
From clues such as facial expressions we attribute
mental states to others (“Theory of mind”, Premack &
Woodruff, 1978) and we expect human interlocutors to
behave coherently. Thus we expect a person with a happy
face to say something positive. In exp. 2 the happy face

1677

prime set up this expectation, and this may have enhanced
looks to the target agent. By contrast, we do not usually
interact with smileys and thus a smiley as a “speaker”
may not elicit the same kind of expectation. The subtle
effects of the emotional prime vs. e.g., results by
Carminati & Knoeferle (2013) and Münster et al. (2014)
could also be due to the different processing types
investigated. Thematic role assignment is arguably a
cognitively more challenging task than semanticreferential processing, and thus may result in weaker
integration of the social cue since attentional resources
needed are higher.
To conclude, although a schematic smiley provides
sufficient information to facilitate sentence processing in
combination with depicted actions, our results provide
some evidence for the view that a natural facial emotional
expression can lead to a subtly stronger priming effect
and thus is better integrated into the interpretation of
emotionally valenced OVS sentences.

Acknowledgements
This research was funded by the Cognitive Interaction
Technology Excellence Center 227 and the SFB 673
“Alignment in Communication” (German Research
Foundation, DFG). We thank participants for their
support.

References
Arai, M., Van Gompel, R. P. G., & Scheepers, C. (2007).
Priming ditransitive structures in comprehension.
Cognitive Psychology, 54, 218-250.
Blow, M., Dautenhahn, K., Appleby, A., Nehaniv, C. L.,
& Lee, D. (2006). The art of designing robot faces:
Dimensions
for
human-robot
interaction.
In
Proceedings of the 1st ACM SIGCHI/SIGART
conference on Human-robot interaction (pp. 331-332).
Carminati, M. N., & Knoeferle, P. (2013). Effects of
speaker emotional facial expression and listener age on
incremental sentence processing. PloS one, 8(9),
e72559.
Carstensen, L. L., Fung, H. H., & Charles, S. T. (2003).
Socioemotional selectivity theory and the regulation of
emotion in the second half of life. Motivation and
emotion, 27(2), 103-123.
Chambers, C. G., Tanenhaus, M. K., & Magnuson, J. S.
(2004). Actions and affordances in syntactic ambiguity
resolution. JEP: LMC, 30, 687-696.
Chang, Y. H. (2006). Comparing affective perception in
cartoon, schematic and real faces. In Perception, 35,
206-206.
Frijda, N. H. (1953). The understanding of facial
expression of emotion. Acta Psychologica 9, 294-362.
Harwood, N. K., Hall, L. J., Shinkfield, A. J. (1999).
Recognition of facial Emotional expressions from
moving and static displays by individuals with mental
Retardation. American Journal on Mental Retardation,
104(3), 270-278.

Isaacowitz, D. M., Wadlinger, H. A., Goren, D., &
Wilson, H. R. (2006). Selective preference in visual
fixation away from negative images in old age? An eyetracking study. Psychology and Aging, 21, 40-48.
Isaacowitz, D. M., Allard, E. S., Murphy, N. A.,
&Schlangel, M. (2009). The time course of age-related
preferences toward positive and negative stimuli. The
Journals of Gerontology Series B: Psychological
Sciences and Social Sciences, gbn036.
Knoeferle, P., Crocker, M. W., Scheepers, C., &
Pickering, M. J. (2005). The influence of the immediate
visual con- text on incremental thematic roleassignment: Evidence from eye-movements in depicted
events. Cognition, 95, 95-127.
Kozel, N. J., Gitter, A. G. (1968). Perception of emotion:
Differences in mode of presentation, sex of perceiver,
and race of expressor. CRC Rep. 18, 36.
Münster, K., Carminati, M. N., & Knoeferle, P. (2014).
How Do Static and Dynamic Emotional Faces Prime
Incremental Semantic Interpretation?: Comparing Older
and Younger Adults. In Proceedings of the 36th Annual
Meeting of the Cognitive Science Society (pp. 2675–
2680). Québec City, Canada.
Nummenmaa, L., Hyönä, J., & Calvo, M. (2006). Eye
movement assessment of selective attentional capture
by emotional pictures. Emotion, 6, 257-268.
Öhman A., Lundqvist, D., & Esteves, F. (2001). The
facein the crowd revisited: A threat advantage with
schematic stimuli. Journal of Personality and Social
Psychology, 80, 381-396.
Premack, D., & Woodruff, G. (1978). Does the
chimpanzee have a theory of mind? Behavioral Science,
4, 515–526.
Recio, G., Sommer, W., Schacht, A. (2011).
Electrophysiological correlates of perceiving and
evaluating static and dynamic facial emotional
expressions. Brain Research, 1376, 66-75.
Ruffman, T., Ng, M., & Jenkin, T. (2009). Older adults
respond quickly to angry faces despite labeling
difficulty. The Journals of Gerontology Series B: Psych
Sciences and Social Sciences, 64, 171-179.
Sedivy, J. C., Tanenhaus, M. K., Chambers, C. G., &
Carlson, G. N. (1999). Achieving incremental semantic
interpretation through contextual representation.
Cognition, 71, 109 -147.
Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., &
Sedivy, J. C. (2002). Eye movements and spoken
language comprehension: Effects of visual context on
syntactic ambiguity resolution. Cognitive Psychology,
45, 447-481.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.
M., Sedivy, J. C. (1995). Integration of visual and
linguistic
information
in
spoken
language
comprehension. Science, 268(5217), 1632-1634.

1678

