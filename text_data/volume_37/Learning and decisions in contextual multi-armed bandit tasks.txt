         Learning and decisions in contextual multi-armed bandit tasks
Eric Schulz1 (eric.schulz.13@ucl.ac.uk), Emmanouil Konstantinidis2 (em.konstantinidis@gmail.com), &
                               Maarten Speekenbrink1 (m.speekenbrink@ucl.ac.uk)
              1
                Department of Experimental Psychology, University College London, London, WC1H 0AP
           2
             Department of Social and Decision Sciences, Carnegie Mellon University, Pittsburgh, PA 15213
                          Abstract                              textual bandit as well as a restless bandit (in which the
                                                                average rewards associated with the arms vary over time)
   Contextual Multi-Armed Bandit (CMAB) tasks are a
   novel framework to assess decision making in uncertain       by ignoring contextual information, but are designed
   environments. In a CMAB task, participants are presented     such that only taking the context into account will lead
   with multiple options (arms) which are characterized by      to above-chance performance. We will show that hu-
   a number of features (context) related to the reward as-
   sociated with the arms. By choosing arms repeatedly          mans are able to learn well within the CMAB and are best
   and observing the reward, participants can learn about       described by sensitive exploration-exploitation behavior
   the relation between context and reward and improve          based on probability matching of choices to the predic-
   their decision strategy. We present two studies on how
   people behave in CMAB tasks. Within a stationary en-         tions of non-parametric Bayesian models (Srinivas et al.,
   vironment, we find that participants are best described      2009). These models do not try and learn one particu-
   by Thompson Sampling-based Gaussian Process mod-             lar parametric structure, but rather a distribution over
   els. This decision rule incorporates probability match-
   ing to the expected outcomes derived from a rational         different generating mechanisms in a particular environ-
   model of the task and it is especially well-adapted to       ment (see Gershman & Blei, 2012). Thompson sampling,
   non-stationary environments. In a dynamic CMAB task          a form of probability matching, offers a simple yet pow-
   we again find that participants are best described by
   probability matching of Gaussian Process expectations.       erful way to balance exploration and exploitation, espe-
   Our findings imply that behavior previously referred to      cially in non-stationary environments (Agrawal & Goyal,
   as ‚Äúirrational‚Äù can actually be seen as a well-adapted       2012; Speekenbrink & Konstantinidis, 2015). Our sec-
   strategy based on powerful inference algorithms.
   Keywords: Decision Making, Learning, Exploration-            ond experiment shows that the evidence for our model is
   Exploitation, Contextual Multi-Armed Bandits                 even more pronounced in a dynamic environment where
                                                                participants‚Äô choices influence future outcomes.
                      Introduction
Multi-armed bandit tasks have proven a useful frame-                   Contextual multi-armed bandits
work to study learning and decision making (e.g.,               A CMAB task can be seen as a game in which in each round
Steyvers et al., 2009). In a multi-armed bandit task,           t = 1, . . . , T , an agent observes a context st ‚àà S from a
participants repeatedly choose between multiple options         set S of possible contexts and has to choose an action
(arms) which have an associated reward and only the re-         at ‚àà A from a set A of possible actions. Afterwards, she
ward of the chosen option can be observed. Performing           receives a reward yt = f (st , at ) + t and it is her task to
well in these tasks requires a fine balance between explo-      take those actions that produce the highest reward. The
ration (choosing arms in order to learn about their asso-       expected reward depends on the context, such that the
ciated rewards) and exploitation (choosing arms which           agent has to learn the underlying function f ; sometimes,
are thought to provide the maximum reward). In stan-            this may require the agent to choose an action which is
dard multi-armed bandit tasks, there is no additional in-       not expected to give the highest reward, but one that
formation about the rewards that can be expected from           might provide useful information about f , thus choosing
an arm. In real life, such information is often present.        to explore rather than exploit.
For instance, when choosing a restaurant to eat in, there          For an agent who ignores the context st , the task
are various cues to the quality of the food on offer, such      would appear as a restless bandit task, as the rewards
as the number of customers, the price of the dishes, the        associated with an arm will vary over time due to the
location of the restaurant, etc. These features provide         changing context. Learning the function f will make
contextual information that allows people to form expec-        these changes in reward predictable and choosing the
tations about the satisfaction the restaurant will provide.     optimal arm easier. As it is not given that participants
Contextual multi-armed bandits (Li et al., 2010) are a          will learn the function, we will compare models of their
natural extension of classic multi-armed bandits and it         behavior which are either context-blind and only learn
is surprising that not much is known about learning and         based on direct feedback of the chosen arms, or contex-
decision making in these tasks.                                 tual and learn the function relating context to reward.
   In what follows, we will introduce the Contextual            All models are based on inferring a predictive distribu-
Multi-Armed Bandit (CMAB) task and assess how par-              tion of the reward yk,t+1 on trial t+1 associated with arm
ticipants perform in two different versions thereof. The        k from the previous rewards y1:t = (y1 , . . . , yt ), chosen
experimental tasks can be approached as both a con-             arms a1:t , and contexts c1:(t+1) . For all models consid-
                                                            2122

ered here, this predictive distribution is a normal distri-           Bayesian linear regression Linear regression as-
bution                                                                sumes the expected reward of an arm is an addi-
                                                                      tive function of the m attributes of the context st =
         p(yt+1 |y1:t , a1:t , c1:(t+1) ) = N (Mt+1 , Vt+1 )  (1)     (s1,t , . . . , sm,t ):
                                                                                                                   m
but the models differ in how they compute the mean                                                                X
Mt+1 and variance Vt+1 .                                                          ykt = fk (st ) + k,t = Œ≤0 +        Œ≤i si,t + k,t
                                                                                                                  i=1
Learning
                                                                      Bayesian linear regression starts with a prior distribu-
Context-blind learning Context-blind models only                      tion on the parameters Œ≤i , i = 0, . . . , m and, from the
respond to the observed outcomes over time thereby ig-                contexts s1:t and rewards y1:t infers the posterior distri-
noring the context completely.                                        bution over these parameters. These can then be used
                                                                      to compute the predictive reward distribution (1), with
¬µ-tracking The first context-blind model is based on                  mean
                                                                                                         1
tracking the mean ¬µk reward associated to each arm k.                                           Mk,t = 2 st+1 > A‚àí1 Sy                (5)
                                                                                                        œÉ
The Bayesian ¬µ-tracking model computes, on each trial,
the posterior distribution of the mean and was imple-                 and variance
mented by a mean-stable version of the Kalman filter
                                                                                                 Vk,t = st+1 > A‚àí1 st+1               (6)
described next (by setting œÉŒ∂2 = 0).
                                                                      where A = œÉ ‚àí2 SS > + Œ£‚àí1 , with S being the context
Kalman filter Unlike the model above, the Kalman                      and y the outcomes observed so far.
filter is a suitable model for tracking a time-varying
mean. It is based on the following structural model                   Gaussian Process regression The second class of
                                                                      used models is non-parametric. Instead of postulating a
         ¬µk,t = ¬µk,t‚àí1 + Œ∂k,t               Œ∂k,t ‚àº N (0, œÉŒ∂ )         specific parametric form, Bayesian non-parametric mod-
         yk,t = ¬µk,t + k,t                 k,t ‚àº N (0, œÉ )         els implicitly assume that the function can be repre-
                                                                      sented by an infinite number of parameters and let the
The mean of the predictive reward distribution of an arm              data speak directly by the means of Bayesian inference.
k is computed as                                                      One example of a non-parametric model in the functional
                                                                      domain is a Gaussian Process (Rasmussen, 2006).
           Mk,t = Mk,t‚àí1 + Œ¥k,t Kk,t [yt ‚àí Mk,t‚àí1 ]           (2)        A Gaussian Process (henceforth GP) is a collection of
                                                                      random variables from which every finite marginal dis-
where Œ¥k,t = 1 if arm k was chosen on trial t, and 0                  tribution is multivariate Gaussian. A Gaussian Process
otherwise. The ‚ÄúKalman gain‚Äù term is computed as                      can be expressed as
                                   Sk,t‚àí1 + œÉŒ∂2                                               f (s) ‚àº GP m(s), k(s, s0 ) .
                                                                                                                         
                                                                                                                                      (7)
                   Kk,t =
                               Sk,t‚àí1 + œÉŒ∂2 + œÉ2
                                                                      where m(s) = E[f (s)] is the mean function and k(s, s0 ) =
where Sk,t is the variance of the posterior distribution of           E[(f (s) ‚àí m(s))(f (s0 ) ‚àí m(s0 ))] the covariance function.
the mean reward, computed as                                          We assumed a squared exponential kernel
                                                                                                                (s ‚àí s0 )2
                                                                                                                          
              Sk,t = [1 ‚àí Œ¥k,t Kk,t ][Sk,t‚àí1 + œÉŒ∂2 ]          (3)                           k(s, s0 ) = exp ‚àí                         (8)
                                                                                                                   2Œª2
The variance of the predictive distribution is
                                                                      as covariance function with the lengthscale parameter Œª.
                        Vt = St +     œÉŒ∂2 +  œÉ2              (4)     The predictive reward distribution (1) has mean
When fitting the model to participants‚Äô behavior, prior                               Mk,t = K(st+1 , S)[K(S, S) + œÉI]‚àí1 y            (9)
means and variances were initialized to Mk,0 = 0 and
                                                                      and variance
Sj (0) = 1000, while œÉŒ∂ and œÉ were estimated by maxi-
mum likelihood.                                                             Vt = K(st+1 , st+1 )
Contextual learning The contextual models learn                                  ‚àí K(st+1 , S)[K(S, S) + œÉI]‚àí1 K(S, st+1 )           (10)
the functions fk that map the context to the rewards.
We will consider two contextual models: linear and                    where K is the covariance matrix, S is the context seen
Gaussian Process regression.                                          so far, and œÉ is the noise level.
                                                                  2123

Decision strategies                                            H1: Participants will manage to learn within the intro-
                                                                  duced CMAB-setting and therefore be better described
We will consider two strategies to make decisions in a
                                                                  by contextual than by context-blind models.
CMAB based on the expected outcomes according to the
above learning models: the Upper Confidence Bound              H2: Participants will approach contextual learning in a
strategy and Thompson Sampling.                                   non-parametric fashion, allowing them to potentially
Upper Confidence Bounds (UCB) The upper con-                      learn different types of functions. Therefore, partici-
fidence bound (UCB) algorithm, which has been shown               pants will be better described by the Gaussian Process
to perform well in many real world tasks (Krause &                than by the linear regression model.
Ong, 2011), balances the current expected value and the
variance per arm and chooses the arm with the high-            H3: Instead of maximizing output by a deliberate mean-
est upper confidence bound. The UCB-algorithm can                 variance trade-off, participants approach dynamic de-
be described as a selection strategy with an exploration          cision making problems using a probability match-
bonus, where the bonus depends on the 95% confidence              ing heuristic. Thus, they will be best described by
interval of the estimated mean reward. As the UCB-                Thompson sampling.
algorithm is essentially deterministic while participants‚Äô     Whereas H1 is based on the assumption that partici-
decisions are expected to be more noisy, the following         pants can learn the true functions in the CMAB setting,
Softmax-transformation was used when fitting the strat-        H2 follows recent successful attempts to describe func-
egy to participants‚Äô behavior                                  tion learning as non-parametric by Gaussian Process re-
                                        p                      gression (Griffiths et al., 2009). That participants are
                     exp{Œ≥(Mk,t + 1.96 Vk,t )}                 better described by probability matching expected out-
    p(at = k) = Pn                        p           (11)
                     i=1 exp{Œ≥(Mi,t + 1.96 Vi,t )}             comes instead of a mean-variance trade-off (H3) has been
                                                               shown by Speekenbrink & Konstantinidis (2015) in a
The temperature parameter Œ≥ governs how consistent             large model comparison study within the restless ban-
participants choose according to the values generated          dit setting.
by the different models and was estimated by maximum
likelihood.                                                          Experiment 1 : Stationary CMAB
Thompson Sampling Thompson sampling chooses                    The first experiment was designed to test if participants
each arm according to the probability that it provides         can learn the functions in a stationary contextual bandit
the highest reward out of all arms in a particular con-        task.
text (May et al., 2012). This is a form of probability
matching. The algorithm can be implemented by sam-
                                                               Task
pling for each arm a reward from the predictive reward         In the task, there were four different arms that could
distribution (1) and choose the arm with the highest           be played. In addition, three binary variables, sj,t ,
sampled reward. Even though this model seems rela-             j = 1, 2, 3, were introduced as the general context. These
tively simplistic, it can describe human choices in (non-      variables could either be on (+) or off (-). The outcomes
contextual) restless bandit tasks well (Speekenbrink &         of the four arms were dependent on the context as fol-
Konstantinidis, 2015). Whereas psychology has gener-           lows:
ally viewed probability matching as an inferior decision
strategy, Thompson Sampling has been shown to per-                        y1,t = 50 + 15 √ó s1,t ‚àí 15 √ó s2,t + 1,t
form well in bandit tasks and can easily adapt to chang-                  y2,t = 50 + 15 √ó s2,t ‚àí 15 √ó s3,t + 2,t
ing environments as it keeps on exploring other options                   y3,t = 50 + 15 √ó s3,t ‚àí 15 √ó s1,t + 3,t
over time.
                                                                          y4,t = 50 + 4,t
   The probability of an arm to be chosen can be ex-
pressed as                                                     with k,t ‚àº N (0, 5). Thus, the reward was a different
                                                               linear function fk (st ) of the context st = (s1,t , s2,t , s3,t ),
            p(at = k) = p(‚àÄj 6= k : yk,t ‚â• yj,t )     (12)     producing an outcome fk (st ) + k,t .
                                                                  On each trial, the probability that a context feature
and computation from the predictive reward distribu-           was on was set to p(sj,t = +) = 0.5. The functions fk
tions is straightforward (see Speekenbrink & Konstan-          were deliberately designed such that the expected reward
tinidis, 2015).                                                over all possible contexts are identical with E[yk,t ] = 50
                                                               in order to avoid first order stochastic dominance of
                       Hypotheses                              context-blind choices. This means that the only way to
We conducted two experiments to test the following 3           gain higher rewards than the average of 50 is by learn-
hypotheses:                                                    ing how the context features influence the rewards. The
                                                           2124

           Figure 1: Screenshot of Experiment.
context-blind strategies therefore would not perform bet-
ter than chance. Moreover, introducing an arm that only
returns the overall mean with some added noise (Arm 4)
helps us to distinguish even further between contextual
and context-blind models. As context blind models only
take the outcome into account, they should prefer Arm          Figure 2: Distribution of obtained rewards (score) over
4 as it produces the same mean over time, but exhibits         participants by trial in Experiment 1.
less variance and therefore second-order dominates all
the other arms. Contextual models on the other hand
should tend to never select Arm 4 as taking the context        Table 1: Average AIC, standard deviations, and the
into account will generally lead to outcomes which are         number of participants best fit by the different models.
better than the overall mean.
                                                                 Model                  AICmean    AICSD     #best
Participants                                                     Random                 415.9      0         1
47 participants (26 males, age: M = 31.9, SD = 8.2)              ¬µ-track-UCB            392.1      39        2
were recruited via Amazon Mechanical Turk and re-                ¬µ-track-Thompson       388.1      56        3
ceived $0.3 plus a performance-dependent bonus of up             Kalman-UCB             390.9      33        2
to $0.5 as a reward.                                             Kalman-Thompson        375.5‚àó     50        11
                                                                 Linear-UCB             387.8      34        3
Procedure                                                        Linear-Thompson        383.0      46        10
Participants were told that they had to mine for ‚ÄúEmer-          GP-UCB                 389.4      34        4
alds‚Äù on different planets. Moreover, it was explained           GP-Thompson            381.6      42        12‚àó
that at each time of mining the galaxy was described
by 3 different environmental factors, ‚ÄúMercury‚Äù, ‚ÄúKryp-
ton‚Äù, and ‚ÄúNobelium‚Äù, that could either be on (+) or off       firmed by a simple t-test against ¬µ = 50, t(46) = 7.17,
(-) and had different effects on different planets. Partic-    p < 0.01. That participants actually do learn over time
ipants were told that they had to maximize the overall         while also sticking to some exploratory behavior can be
production of Emeralds over time by learning how the           see in Figure 2, where the density for higher scores in-
different elements influence the planets and then pick-        creases and the density for lower scores decreases over
ing the planet they thought would produce the highest          trials.
outcome, given the currently available elements. It was           The overall performance of all models is shown in Ta-
explicitly noted that different planets can react differ-      ble 1. In addition to the aforementioned models, we also
ently to different elements. There were a total of 150         included a Random baseline model, which assumes par-
trials and which planet corresponded to which reward           ticipants decided by random uniform guessing. It can
function fk was determined randomly at the start of the        be seen that the contextual models described partici-
experiment.                                                    pants behavior better than the two context-blind mod-
                                                               els. Altogether, 17 participants were best described by
Results                                                        the context-blind models, whereas 29 participants were
The average score per round was 66.78 (SD=13.02) and           best described by the contextual models.
most participants (38 out of 47) performed better than            Even though the Kalman-Thompson model resulted in
chance (an average score of higher than 50) as is con-         the lowest average AIC-value overall, the Gaussian Pro-
                                                           2125

cess models described more participants best (16), more          Participants
than the linear regression models (13) or the Kalman             47 participants (30 males, age: M = 29.1, SD = 8.6)
filter (13). The good performance of the Kalman filter           were recruited via Amazon Mechanical Turk and re-
might be due to the fact that some people did mostly             ceived $0.3 plus a performance-dependent bonus of up
try to learn on which planet they should mine, which is          to $0.5 as a reward.
also indicated by the relative large variance of the two
Kalman models. Even though there is only a small dif-            Procedure
ference between the Gaussian Processes and the linear            The procedure was as in Experiment 1, but participants
model, it is evermore surprising as the linear model here        were told that their choices could influence future out-
would be the best description of the underlying system           comes.
a priori ‚Äì the task is a linear system after all. What this
tells us is that instead of approaching the problem with
                                                                 Results
a fixed parametric representation in mind, participants          On average, participants obtained rewards of 59.84 (SD
might indeed apply a learning strategy that is more eas-         = 9.41). Even though this task was deliberately set up to
ily adaptable to other scenarios than a linear one.              be more difficult, participants‚Äô overall average score was
   Lastly, more people were described best by Thompson           again above chance, t(46) = 7.17, p < 0.01. In total, 41
sampling than by the UCB strategy (36 vs. 10). This              out 47 participants performed better than chance. Fig-
indicates that participants seem to apply this probability       ure 3 indicates that the evidence of learning was some-
matching heuristic.                                              what weaker than before.
             Intermediate Conclusion
Within a newly introduced task called the Contextual
Multi-Armed Bandit task, we have found that partic-
ipants can best be described by probability matching
outcomes of a (close to) rational non-parametric func-
tion learning engine. Probability matching used to be re-
ferred to as ‚Äúbiased‚Äù or ‚Äúirrational‚Äù (Stanovich & West,
2008), but can actually constitute a very sensible strat-
egy, especially in dynamically changing environments
(Agrawal & Goyal, 2012). Therefore, one would expect
that participants should still be able to perform well even
in a dynamically changing environment. The second ex-
periment was designed to test this.
     Experiment 2: Dynamic Contextual
                Multi-Armed Bandit
The second experiment used a similar task as before.
However, this time the reward of a given arm (planet)
was dependent on how often the particular arm had or
had not been chosen previously. The rewards were de-
termined according to the following functions                    Figure 3: Distribution of obtained rewards (score) over
                                                                 participants by trial in Experiment 1.
       y1,t = 50 + 15 √ó s1,t ‚àí 15 √ó s2,t + 1,t + Œ∂1 (t)
       y2,t = 50 + 15 √ó s2,t ‚àí 15 √ó s3,t + 2,t + Œ∂2 (t)         While scores tended to increase over trials, this was not
       y3,t = 50 + 15 √ó s3,t ‚àí 15 √ó s1,t + 3,t + Œ∂3 (t)         as pronounced as in Experiment 1. This might be due
       y4,t = 50 + 4,t + Œ∂4 (t),                                to the increase in difficulty of the task, as participants
                                                                 had to both learn a function and take the dynamics of
where                                                            their actions into account.
                          (
                            ‚àí1, if at‚àí1 = j                         The overall performance of all models is shown in Ta-
                 Œ∂j (t) =   1
                                                         (13)    ble 2. Again, the contextual models described more
                            3,    otherwise
                                                                 people best than the context-blind models (14 vs. 30).
This means that every time an arm is chosen its mean             Thus, even in this more complicated scenario, people
reward decreases by 1 point while the means of the un-           seem able to learn about the relation between the con-
chosen arms increase by 13 , thereby creating a dynamic          text and rewards. The non-parametric models again de-
environment in which past choices directly influence fu-         scribed more people best than the linear regression mod-
ture outcomes.                                                   els (19 vs. 12) or the Kalman filter (19 vs. 12). Finally,
                                                             2126

                                                             10). Another simple modification could be to check dif-
Table 2: Average AIC, standard deviations, and the
                                                             ferent parameterizations of the underlying functions to
number of participants best fit by the different models.
                                                             differentiate even further between the different candidate
  Model                    AICmean      AICSD    #best       models.
  Random                   415.9        0        2              Finally, we have only introduced a comparison be-
  ¬µ-tracking-UCB           414.5        10       0           tween a linear model and a Gaussian process in what
  ¬µ-tracking-Thompson      416.8        5        0           can be described as an active learning task. In future
  Kalman-UCB               385.1        44       5           experiments we aim to try and compare even more elab-
  Kalman-Thompson          387.8        39       9           orate models within this context. Using an exploration-
  Linear-UCB               331.0        88       3           exploitation domain as a platform to compare models
  Linear-Thompson          321.1        99       9           against each other might be a useful additional approach
  GP-UCB                   349.5        72       3           to decide among models from a list of many potential
  GP-Thompson              316.9‚àó       104      16‚àó         candidates (Schulz et al., 2014).
                                                                               Acknowledgements
the Thompson-sampling based strategies described more        ES is supported by the UK Centre for Training in Financial
                                                             Computing & Analytics. Parts of this work have been pre-
people best than the UCB strategy (34 vs 11). Overall,       sented as Schulz et al. (2015).
the Thompson sampling GP-model described most peo-           Materials can be found at:
ple best (16) reaching a mean AIC of 316.9.                  https://github.com/ericschulz/contextualbandits
           Discussion and Conclusion                                                References
                                                             Agrawal, S., & Goyal, N. (2012). Thompson sampling for
We have introduced the Contextual Multi-Armed Ban-              contextual bandits with linear payoffs. arXiv preprint
dit (CMAB) task as a paradigm to investigate decision           arXiv:1209.3352 .
making in situations where one has to learn contextual       Gershman, S. J., & Blei, D. M. (2012). A tutorial on bayesian
                                                                nonparametric models. Journal of Mathematical Psychol-
functions and simultaneously make decisions according           ogy, 56 , 1‚Äì12.
to the predictions of those functions. The CMAB-task here    Griffiths, T. L., Lucas, C., Williams, J., & Kalish, M. L.
can be seen as a natural extension of past experiments          (2009). Modeling human function learning with gaussian
on learning in traditional multi-armed bandit tasks.            processes. In Advances in Neural Information Processing
                                                                Systems, (pp. 553‚Äì560).
   In both a stationary and a dynamic task, we found         Krause, A., & Ong, C. S. (2011). Contextual gaussian process
that participants mostly performed above chance and             bandit optimization. In Advances in Neural Information
were best described as probability matching to expected         Processing Systems, (pp. 2447‚Äì2455).
outcomes according to a rational Gaussian process func-      Li, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A
                                                                contextual-bandit approach to personalized news article
tion learning model. The above-chance performance               recommendation. In Proceedings of the 19th international
shows that participants were able to learn the relation         conference on World wide web, (pp. 661‚Äì670). ACM.
between context and rewards. The good performance            May, B. C., Korda, N., Lee, A., & Leslie, D. S. (2012). Op-
                                                                timistic bayesian sampling in contextual-bandit problems.
of the GP model opens up the field of decision making           The Journal of Machine Learning Research, 13 (1), 2069‚Äì
to a powerful class of general purpose non-parametric           2106.
learning models. The good performance of the Thomp-          Rasmussen, C. E. (2006). Gaussian processes for machine
son sampler replicates the results of Speekenbrink &            learning.
                                                             Schulz, E., Konstantinidis, E., & Speekenbrink, M. (2015).
Konstantinidis (2015) in a non-contextual restless ban-         Exploration-exploitation in a contextual multi-armed ban-
dit task. It shows that probability matching, a behav-          dit task. In International Conference on Cognitive Model-
ior often frowned upon as irrational, provides a sensi-         ing, (pp. 118‚Äì123).
tive strategy that people might actually apply to solve      Schulz, E., Speekenbrink, M., & Shanks, D. R. (2014). Pre-
                                                                dict choice ‚Äì a comparison of 21 mathematical models. In
the exploration-exploitation dilemma in a range of sit-         36th Annual Conference of the Cognitive Science Society,
uations. This is also what we have confirmed in our             (pp. 2889‚Äì2894). Cognitive Science Society.
second experiment, where participants were even bet-         Speekenbrink, M., & Konstantinidis, E. (2015). Uncertainty
                                                                and exploration in a restless bandit problem. Topics in
ter described by a Thompson sampling algorithm in a             Cognitive Science, 7 , 351‚Äì367.
more dynamic scenario, where rewards depended on past        Srinivas, N., Krause, A., Kakade, S. M., & Seeger, M.
choices. In conclusion, all of our three main hypotheses        (2009). Gaussian process optimization in the bandit set-
were confirmed.                                                 ting: No regret and experimental design. arXiv preprint
                                                                arXiv:0912.3995 .
   This paper is only a first step into research on CMAB     Stanovich, K. E., & West, R. F. (2008). On the relative inde-
problems. Future studies could try to assess how peo-           pendence of thinking biases and cognitive ability. Journal
ple behave in scenarios where more context is provided          of personality and social psychology, 94 (4), 672.
either by creating a multi-context environment (for ex-      Steyvers, M., Lee, M. D., & Wagenmakers, E.-J. (2009).
                                                                A bayesian analysis of human decision-making on ban-
ample, one context per planet) or by providing continu-         dit problems. Journal of Mathematical Psychology, 53 (3),
ous context variables (for example, values between 0 and        168‚Äì179.
                                                         2127

