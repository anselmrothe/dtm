Shifting Covert Attention to Spatially Indexed Locations Increases Retrieval
Performance of Verbal Information
Anja Prittmann (anja.prittmann@psychologie.tu-chemnitz.de)
Technische Universität Chemnitz, Department of Psychology, Cognitive and Engineering Psychology
Wilhelm-Raabe-Strasse 43, D-09120 Chemnitz, Germany

Agnes Scholz (agnes.scholz@psychologie.tu-chemnitz.de)
Technische Universität Chemnitz, Department of Psychology, Cognitive and Engineering Psychology
Wilhelm-Raabe-Strasse 43, D-09120 Chemnitz, Germany

Josef F. Krems (josef.krems@psychologie.tu-chemnitz.de)
Technische Universität Chemnitz, Department of Psychology, Cognitive and Engineering Psychology
Wilhelm-Raabe-Strasse 43, D-09120 Chemnitz, Germany
Abstract
People look at emptied spatial locations where information has
been presented during encoding. There is evidence that this socalled ‘looking at nothing’ behaviour plays a functional role in
memory retrieval of visuospatial and verbal information. However, it is unclear whether this effect is caused by the oculomotor movement of the eyes per se or if covertly shifting attention is sufficient to cause the observed differences in retrieval
performance. In an experimental study (N = 26), participants
were manipulated in being able to shift either their eyes or their
focus of attention to a blank spatial location whilst retrieving
verbal information that was associated with the location during a preceding encoding phase. Results indicate that it is not
the oculomotor movement of the eyes that causes the facilitation while retrieving verbal materials, but rather covert shifts
of attention are sufficient to promote differences in retrieval
performance.
Keywords: memory retrieval; eye movements; visuospatial
attention; memory representation; encoding-retrieval relationship

Introduction
We encode the stimulus location regardless of the task (e.g.,
Andrade & Meudell, 1993). When we attempt to retrieve
the information, we automatically recall the location of the
stimulus, even if the stimulus is no longer present. Recent
research has shown that people even look at emptied spatial
locations where information was presented during encoding
when retrieving information that is associated with this location (e.g., Altmann, 2004; Jahn & Braatz, 2014; Richardson & Kirkham, 2004; Richardson & Spivey, 2000; Scholz,
Helversen, & Rieskamp, 2015). There is evidence that this
so-called ‘looking at nothing’ behaviour plays a functional
role in memory retrieval of visuospatial (e.g., Johansson &
Johansson, 2014) and verbal information (Scholz, Mehlhorn,
& Krems, 2014). In the study by Scholz et al. (2014), participants had to retrieve pieces of auditorily presented information that were associated with a spatial location on a computer screen during a preceding encoding phase. Retrieval
performance was higher when participants fixated on the area
associated with the to-be-retrieved information than when fixating on another area. This result shows a facilitatory effect
of memory retrieval that can be explained by an overlap be-

tween processes engaged in the encoding and retrieval of verbal information from memory. It remains unclear however
which exact mechanisms drive this facilitatory effect. Two
likely candidates are the oculomoter movement of the eyes or
covert shifts of attention (e.g., Richardson & Spivey, 2000;
Thomas & Lleras, 2009).
Guérard and Tremblay (2011) suggest that presented information is automatically encoded in an internal memory representation, (a common map) independently of whether it needs
to be remembered or can be ignored. We thus encode the location of information even if it is irrelevant to the task. ‘Spatial indexes’are assumed to function the role of linking elements in the internal memory representation with the external world (e.g., Pylyshyn, 2002; Laeng, Bloem, D’Ascenzo,
& Tommasi, 2014). The spatial index is stored as part of
the internal representation and drives the eyes back to associated spatial locations (Richardson & Kirkham, 2004). Consequently, in order to explain the facilitatory effect of the
looking at nothing behaviour, addressing the spatial index
by shifting attention to associated but emptied spatial locations could be sufficient to elicit enhanced retrieval performance. Several findings support this assumption. Godijn and
Theeuwes (2004) demonstrated that attention is focused at
the saccade target location just prior to the saccade being executed. Furthermore, attention influences what is remembered
between eye movements; it selects objects for visual processing, guides motor action, and facilitates storage in working memory (e.g., Deubel & Schneider, 1996; Theeuwes,
Kramer, & Irwin, 2010).
Although attention usually precedes eye movements
(Deubel & Schneider, 1996), attention and eye movements
are still distinguishable processes that can be separated
(Theeuwes, Belopolsky, & Olivers, 2009; Thomas & Lleras,
2009). Thomas and Lleras (2009) for instance showed that
insight problem solving can be enhanced by covert shifts of
attention independent of eye movements. They asked participants to solve Duncker’s radiation problem and instructed
them to simultaneously track random digits which appeared
in an order representing the solution of the problem. The two

1907

groups either tracked the digits with their eyes or with their
attention whilst keeping their eyes at the centre of the screen.
Both groups solved the problem faster than a group which
observed digits appearing only at the centre of the screen.
This experiment illustrates two things; firstly when participants separated eye movements and shifts of attention, they
performed equally, and secondly, attention shifts appear to
be sufficient to guide insight. Another study carried out by
Richardson and Spivey (2000) showed that looking at nothing behaviour exists even when participants do not execute
eye movements during the encoding phase. During encoding
in an initial step, participants observed a grey matrix with four
quadrants. A mask window then closed into the centre of the
screen. The matrix moved behind the mask window to bring
each quadrant to the centre of the window, and subsequently
an auditory sentence was presented. After the presentation of
four sentences, the window expanded outwards. During retrieval, participants saw the empty matrix and were probed to
retrieve one of the sentences. The experiment demonstrated
that people looked at a blank region on a screen during retrieval, even when it was unnecessary to move their eyes during encoding. These results may suggest that it is not the
oculomotor movement of the eyes but covert shifts of attention that lead to enhanced memory retrieval when looking at
nothing.
In order to test this assumption, an eye tracking experiment
was conducted. Similarly to Richardson and Spivey (2000),
participants listened to four sentences in each trial. Each sentence was associated with a spatial area henceforth called ‘relevant quadrant’of a grey two by two matrix on the computer
screen. In a retrieval phase, participants judged a statement
regarding one of the before heard sentences to be true or false.
Simultaneous to this task, participants were asked to solve a
tracking task (see Thomas & Lleras, 2009), undertaken to manipulate participants’ eye movement behaviour/shifts of attention. Given previous findings (Johansson & Johansson,
2014; Scholz et al., 2014), firstly, it was assumed that participants would reach higher response accuracy scores when being guided to the quadrant where the to-be-retrieved information was presented during encoding (match condition). When
participants’ eye movements or their attention was guided
away from the location associated with the to-be-retrieved
information, the study expected to find no facilitation effect
resulting in lower response accuracy (mismatch condition).
When participants looked at the centre of the screen (central
condition), response accuracy was expected to fall in between
the match and mismatch conditions. This is due to the participants’ gaze or attention being located at a relatively short distance away from the relevant quadrant (i.e., shorter than 15o of
visual angle as suggested by results on the so-called ‘useful
field of view’, for an overview see Irwin, 2004). Secondly,
this study assumes that eye movements do not have advantages over attention shifts for rehearsing verbal information
in working memory. It is expected therefore that response accuracy should not differ between the attention shift and eye

movement conditions.

Method
Participants
Twenty-six students (21 female, 24 years, ranging from 20-32
years) enrolled at the Technische Universität Chemnitz volunteered in the experiment in exchange for student course credit
or monetary compensation. All participants had normal or
corrected to normal vision and were native German speakers.

Apparatus and Stimuli
The study recorded gaze data using a binocular IViewX RED
eye-tracking-system from SensoMotric Instruments with a
sampling rate of 120 Hz. Data was analyzed with BeGaze
3.0, Microsoft Excel 2007 and IBM Statistics 19 (SPSS).
Stimuli was presented on a 22 inch computer screen using EPrime 2.0 software and with a resolution of 1680 ×
1050 pixels. All subjects were seated at a distance of 600 mm
in front of the screen. Visual stimuli consisted of grey two
by two matrixes with quadrants sized at a height of 14.25◦
of visual angle and a width of 15.97◦ visual angle (Figure 1).
During encoding, participants observed a white circle with
a speaker symbol located in the centre of the spatial areas
or quadrants. During retrieval, participants viewed a circle
with random digits appearing in one of five locations, alternating with an empty matrix in a randomized order and with
a frequency 1 Hz (tracking task, see Thomas & Lleras, 2009).
Within each trial the digit always appeared in the same circle.
Circles with speaker symbols or digits were of equal size with
a visual angle of 2.4◦ . Digits had a size of approximately 1.2◦
visual angle. The distance between the centre of the screen
and the circles in the quadrants was 9.5◦ visual angle.
The auditory material was presented using Sinnheiser
HD270 headphones. Four facts were presented during each
encoding phase. One of these facts was tested during the
subsequent retrieval phase. Material for the encoding phases
consisted of 32 sentences involving fictitious scenes with a
name of an artificial city and four attributes (e.g., ‘In Rinteln
you can find a main station, a video store, a silver mine and a
television tower.’). Auditory materials for the retrieval phases
consisted of eight statements. Each statement was a true or
a false version referred to in one of the attributes of one of
the sentences presented during the encoding phase (e.g., ‘In
Rinteln you can find a main station.’ true, attribute 1).

Design and Procedure
After the instructions and a calibration phase, the experiment
began with two practice trials which adhered to the same procedure carried out in the test trials. Subsequently eight experimental trials were undertaken. Each trial followed the
same procedure and was divided into an encoding and a retrieval phase (see Figure 2). During the encoding phase, participants heard four sentences, with each sentence being associated with one quadrant on the screen. Participants were
instructed to listen carefully and memorize the sentences to

1908

Figure 1: Example of visual stimulus material during encoding (left side) and retrieval (right side)

the best of their ability. After the presentation of a fixation
cross, the retrieval phase was initiated. Participants heard one
test statement and were instructed to judge whether this statement was true or false by selecting a blue (true) or red (false)
button on the keyboard. While providing the answer, participants were instructed to react to the digits of the tracking task
by pressing the space bar on the keyboard. The digits of the
tracking task either appeared in the relevant quadrant (match
condition, two trials), in one of the adjacent quadrants (adjacent condition, two trials), in the diagonal quadrant (diagonal
condition, two trials) or in the centre of the screen (central
condition, two trials). Trials with the digits in the adjacent
and diagonal quadrants were combined to a mismatch condition because this study did not expect differences to exist
between these trials. The tracking task was designed to enable or disable looking at nothing behaviour by guiding participants’ eyes either to the relevant quadrant or away from it
(eye movement condition).1 In a second experimental group,
participants were asked to fixate on the centre of the screen
and to react to the digits by covertly shifting their attention
towards them (attention shift condition). The study consisted
of a 3 × 2 design. Whereby the variable tracking task was
varied within participants (match, mismatch, central) and the
variable gaze instruction (attention shift vs. eye movement)
was manipulated between participants.

Analysis
To analyze whether covert shifts of attention or eye movements lead to differences in retrieval performance, gaze patterns and performance in the tracking task were initially
tested to ensure that the manipulation had been successful.
In order to analyze gaze patterns, five Areas of Interest
(AOIs) were defined by positioning circles around the speaker
symbols or digits on the quadrants as opposed to the centre of
the screen. Fixations were defined by a dispersion threshold of 100 pixel and a duration threshold of 100 ms. Mean
fixation times were aggregated over the trials and for the participants in each AOI.
To analyze the performance of the tracking task, two measures were compared (see Thomas & Lleras, 2009). Firstly,
1 Eye

movements have also been called ‘overt shifts of
attention’(Theeuwes et al., 2009). In this study, we label the condition ‘eye movement’to emphasize that this condition includes an
oculomotoric movement of the eyes.

the average reaction time between the onset of a digit featuring on the screen to the reaction of a participant in pressing the space bar was recorded. Reaction times were expected to be similar over all the conditions, because tasks
were equal for both groups excluding the gaze instruction.
The second measure used in the comparison was the digit
identification accuracy (DIA) between the count of digits a
participant reacted to and the count of digits a participant observed until they selected the response button. Again, no differences were expected between the two experimental groups.
The dependent measure was the retrieval performance, which
was assessed as the mean percentage of correct answers for
each condition. All trials were aggregated for each condition
(match, mismatch, central) for all participants.

Results
Manipulation check
It was necessary to exclude four participants (one participant in the eye movement condition and three participants
in the attention shift condition) because more than 50 % of
their fixation time fell at the centre of the screen and less
than 25 % was located in the quadrant with the speaker
symbol during all the trials of the encoding phase. As
shown in the upper part of Table 1, the remaining participants fixated for the majority of the time the quadrant in
which the speaker symbol was presented (attention shift condition: F(4, 40) = 17.43, p < .001; η2p = .64; Bonferroni
pairwise comparisons all ps < .001; eye movement condition: F(4, 60) = 29.07, p < .001; η2p = .66; Bonferroni pairwise comparisons all ps < .001). During the retrieval phase
(middle part of Table 1), participants in the attention shift
condition followed the instruction and fixated at the centre
of the screen for a longer duration than on any quadrant
(F(4, 37) = 12.38, p < .001; η2p = .57; all ps < .001). In
the eye movement condition, participants followed the digits with their eyes and fixated on the field showing the digit
significantly longer than on any other quadrant (F(4, 59) =
6.93, p < .001; η2p = .32; Bonferroni pairwise comparisons
all ps < .01). The difference however in fixation duration
between the quadrant featuring the digit and the centre of
the screen is not significant (p = 1.0). Nevertheless with a
mean fixation duration of 3326 ms, participants looked on
average approximately 47.26 % of the time at the quadrant
featuring the digit, which is well above the chance level of
20 %. A one sample t-test supports this statistic (t(12) =
6.84, p < .001, g = 1.98). Participants of both groups performed comparably well in the tracking task (lower part of
Table 1). Calculating a two sample t-test this study did not
find any significant differences in the reaction times to the
digits (t(20) = 1.04, p = .31) or the digit identification accuracy (t(20) = −0.70, p = .50) between the attention shift
condition and the eye movement condition.

1909

Figure 2: Example trial for the encoding phase (left side) and the retrieval phase with the tracking task (right side). Original
materials in German.

Response accuracy
This study assumed superior retrieval performance in the
match compared to the mismatch condition and that the retrieval performance in the central condition could fall in between the performance of the match and mismatch conditions. Furthermore, concerning the gaze instruction, no differences were assumed between the attention shift and the
eye movement conditions. Figure 3 shows the results of the
gaze instruction and tracking task manipulations on response
accuracy. To test our hypotheses, we calculated a contrast
analysis for comparing hypotheses (see Rosenthal, Rosnow,
& Rubin, 2000). We chose this analysis, because it allows
testing more precise hypothesis and with a higher statistical power than a standard analysis of variance (ANOVA).2
Generally, in contrast analyses, the fit between a model (conveyed by contrast weights) and the data for a given participant is expressed in a single value, most commonly an Lvalue (Rosenthal et al., 2000, p. 128-130). In our case, Lvalues for two different models are calculated as the product
of the weights of the models and the corresponding response
accuracies (Model A: Eye movements do not have an advantage over shifts of attention. Participants in both conditions
show the highest response accuracy in the match trials and
lowest response accuracy in the mismatch trials.; Model B:
Eye movements drive the facilitation effect in the looking at
nothing paradigm and only participants in the eye movement
group show higher response accuracies in the match trials.).
Higher L-values indicate better fits between a model as specified by its contrast weights and the data. An univariate F-test
tests which of the two models better fits with the data. If
Model A better fits with the data than model B, the difference of the L-values (Model A − Model B) should lead to
a positive F-value. The p and η2p values indicate the significance and effect size of the observed difference. To reflect our
Model A, we assigned the match trials in both gaze instruction conditions with a contrast weight of +1, the central trials
2 Furthermore, when the sample size is small like in this study,
non-significant results of a ANOVA can be hard to interpret.

Figure 3: Mean response accuracy for the three conditions of
the tracking task (match, mismatch, central), between the two
conditions of the gaze instruction (attention shift, eye movement). Error bars represent standard errors.

with 0 and the mismatch trials with −1. To reflect our Model
B, we assigned contrasts similar to Model A but only for the
eye movement condition and 0 for all trials in the attention
shift condition. The results reveal a positive F-value which
is highly significant (F(5, 60) = 50.28, p < .001; η2p = 0.81)
giving strong support to our Model A that assumed differences in the conditions of the tracking task, but no differences in the variable gaze instruction. To further show that
no meaningful differences between the gaze instruction conditions exist we calculated a two samples t-test comparing
response accuracy for the match condition between the eye
movement and the attention shift conditions. The test indicates no significant difference between gaze instructions in
the match condition (t(20) = 1.42, p = .17, d = 0.65). Therefore our results suggest that the mere shift of attention facilitates memory retrieval in the looking at nothing paradigm
and that eye movements to nothing have no advantages over
attention shifts (Figure 3).

1910

Table 1: Means (SDs) of fixation times and of performance in the tracking task.

Gaze instruction
Eye movement condition Attention shift condition
Encoding

Mean fixation times to speaker symbol
Mean fixation times to other quadrants
Mean fixation times to centre

2015 ms (685)
526 ms (400)
408 ms (232)

4555 ms (2031)
465 ms (499)
2864 ms (1619)

Retrieval

Mean fixation times to digit
Mean fixation times other quadrants
Mean fixation times to centre

3326 ms (1752)
1233 ms (1295)
2416 ms (1601)

3028 ms (1445)
1939 ms (2106)
4861 ms (2555)

Tracking
Task

Reaction times
Digit identification accuracy

661 ms (224)
.74 (.22)

585 ms (129)
.80 (.16)

Discussion
The aim of the present study was to test whether the oculomotor movement of the eyes or covert shifts of attention are necessary to facilitate memory retrieval in the looking at nothing
paradigm. To this purpose, this study manipulated whether
participants looked at or shifted their attention covertly to the
spatial area where the to-be-retrieved information was presented during encoding. A tracking task was carried out in
which a digit either appeared in the spatial area associated
with the probed information, or alternatively in the central
area of a screen or in one of the adjacent or diagonal areas.
Additionally, half the participants were instructed to look at a
tracking task whereas the other half was instructed to merely
shift their attention. Results show that the manipulation was
successful. During encoding, participants looked the longest
at the speaker symbol associating the auditorily presented information with the quadrant of the screen. Based on previous
research on the looking at nothing paradigm, it can therefore
be concluded that participants spatially indexed the location
where the fact was presented (Richardson & Spivey, 2000;
Richardson & Kirkham, 2004). During retrieval, participants
were also successful in following the instructions. Most of the
time, they looked at the digits in the eye movement condition.
In the attention shift condition participants fixated the centre
of the screen. Results do not show any significant differences
in reactions to the digits presented in the tracking task. People needed equally long to react to the digits on the screen
and reacted to about the same amount of digits. The fact that
about 20 % to 25 % of the digits were missed in both gaze
instruction conditions shows that the tracking task and the
retrieval task challenged our participants and were not easy.
The somewhat better results for the tracking task in the attention shift condition might be an indicator that participants in
this condition stuck more precisely to our instructions. Taken
together, participants in both conditions of the gaze instruction performed equally well. Although the key results are
reported here the manipulation check leaves much room for
further research and more precise examinations.
We expected participants to show higher retrieval accu-

racy when being guided towards a relevant quadrant during retrieval of verbal information than when being guided
away from such a location. This manipulation was intended
to replicate previous findings of a facilitation effect when
looking at nothing behavior is executed with verbal material
(Scholz et al., 2014). Furthermore, we did not expect differences in retrieval accuracy between participants that were
covertly attending and participants that were moving their
eyes to associated spatial locations. This was based on the
assumption that attention shift is sufficient to aid memory retrieval and that eye movements to the relevant spatial area do
not have benefits over covert shifts of attention. Our results
confirm both hypotheses. Participants in the eye movement
condition, as well as participants in the attention shift condition demonstrated increased retrieval accuracy when looking at/shifting attention to the relevant quadrant in comparison to any irrelevant quadrant. This study therefore concludes that it is not the oculomotor movement of the eyes
per se that facilitates memory retrieval whilst looking at remembered locations, but rather it is the covert shift of attention that facilitates retrieval. Facilitation does not appear to
be a consequence of mere oculomotor activity. It is important to recognize however that the wording ‘facilitation effect’
has to be used with care since we cannot present direct evidence that participants show improved retrieval performance
when looking at or shifting attention to indexed location in
comparison to freely looking at nothing. Nevertheless, if the
results of the central condition in this study are interpreted
as a control condition (due to no information being associated with the centre, and the centre always remaining at the
same spatial distance to the speaker symbols), the data of the
current study suggests a faciliatory effect when participants
looked at the relevant quadrant, as well as an impaired effect
when participants’ eye movements or focus of attention was
guided away. The current study reports evidence supporting
the close relationship between eye movements, memory and
attention (e.g., Belopolsky & Theeuwes, 2011). People build
a mental representation consisting of information they are
processing. This representation contains features like shapes

1911

and forms of objects, and it contains information concerning
spatial locations. What exactly enters the mental representation is fundamentally influenced by attention (Theeuwes et
al., 2009), and objects that are attended to are easier to process (Posner, 1980). Accordingly, the retrieval of information from memory is affected by shifts of attention to information held in memory. Simultaneously, a motor response
is executed, leading the eyes to associated spatial locations.
This process facilitates memory retrieval even if we do not
execute the programmed eye movements. A reason for this
might be that attention already shifts to the locations that have
been associated with the to-be-retrieved information during
the preceding encoding phase. Attention is the mechanism
which integrates different forms of information (visual, auditory) together to form a multimodal memory representation
(Theeuwes et al., 2010), and it is used to access this representation by orienting to the relevant information (Theeuwes et
al., 2009).
In conclusion, to access parts of our mental representation,
we direct our attention to certain features which activate the
programming of corresponding eye movements. Since the
programming of eye movements is highly connected to covert
shifts of attention, this study concludes that it is not ‘looking’
at nothing which facilitates memory retrieval but rather it is
an ‘attention shift’ to nothing.

References
Altmann, G. T. M. (2004). Language-mediated eye movements in the absence of a visual world: The ’blank
screen paradigm’. Cognition, 93(2), B79–87. Doi:
10.1016/j.cognition.2004.02.005
Andrade, J., & Meudell, P. (1993). Short report: Is
spatial information encoded automatically in memory?
The Quarterly Journal of Experimental
Psychology Section A, 46(2), 365–375.
Doi:
10.1080/14640749308401051
Belopolsky, A. V., & Theeuwes, J. (2011). Selection within
visual memory representations activates the oculomotor system. Neuropsychologia, 49(6), 1605–1610.
Deubel, H., & Schneider, W. X. (1996). Saccade target selection and object recognition: evidence for a common
attentional mechanism. Vision research, 36(12), 1827–
37.
Godijn, R., & Theeuwes, J. (2004). The relationship between inhibition of return and saccade trajectory deviations. Journal of experimental psychology. Human
perception and performance, 30(3), 538–554. Doi:
10.1037/0096-1523.30.3.538
Guérard, K., & Tremblay, S. (2011). When distractors and
to-be-remembered items compete for the control of action: a new perspective on serial memory for spatial information. Journal of experimental psychology. Human
perception and performance, 37(3), 834–843. Doi:
10.1037/a0020561
Irwin, D. E. (2004). The Interface of Language, Vission,

and Action. In J. M. Henderson & F. Ferreira (Eds.),
The interface of language, vission, and action (1st ed.,
pp. 105–131). Taylor & Francis Books, Inc.: Taylor &
Francis Books, Inc.
Jahn, G., & Braatz, J. (2014). Memory indexing of
sequential symptom processing in diagnostic reasoning.
Cognitive psychology, 68, 59–97.
Doi:
10.1016/j.cogpsych.2013.11.002
Johansson, R., & Johansson, M. (2014). Look Here, Eye
Movments Play A Functional Role in Memory Retrieval. Psychological science : a journal of the American Psychological Society / APS, 25(1), 236–242. Doi:
10.1177/0956797613498260
Laeng, B., Bloem, I. M., D’Ascenzo, S., & Tommasi, L.
(2014). Scrutinizing visual images: the role of gaze in
mental imagery and memory. Cognition, 131(2), 263–
83. Doi: 10.1016/j.cognition.2014.01.003
Posner, M. I. (1980). Orienting of attention. Quarterly Journal of Experimental Psychology, 32(1), 3–25. Doi:
10.1080/00335558008248231
Pylyshyn, Z. W. (2002). Mental imagery: in search of a
theory. The Behavioral and brain sciences, 25(2), 157–
82. Doi: 10.1017/S0140525X02000043
Richardson, D. C., & Kirkham, N. Z. (2004). Multimodal events and moving locations: eye movements
of adults and 6-month-olds reveal dynamic spatial indexing. Journal of experimental psychology. General,
133(1), 46–62. Doi: 10.1037/0096-3445.133.1.46
Richardson, D. C., & Spivey, M. J. (2000). Representation,
space and Hollywood Squares: Looking at things that
aren’t there anymore. Cognition, 76(3), 269–295.
Rosenthal, R., Rosnow, R. L., & Rubin, D. B. (2000). Contrasts and effect sizes in behavioral research: A correlational approach. New York, NY: Cambridge University Press.
Scholz, A., Helversen, B. V., & Rieskamp, J. (2015). Eye
movements reveal memory processes during similarityand rule-based decision making. Cognition, 136, 228–
246. Doi: 10.1016/j.cognition.2014.11.019
Scholz, A., Mehlhorn, K., & Krems, J. F.
(2014).
Listen up, eye movements play a role in verbal
memory retrieval. Psychological research.
Doi:
10.1007/s00426-014-0639-4
Theeuwes, J., Belopolsky, A., & Olivers, C. N. L. (2009). Interactions between working memory, attention and eye
movements. Acta psychologica, 132(2), 106–14. Doi:
10.1016/j.actpsy.2009.01.005
Theeuwes, J., Kramer, A. F., & Irwin, D. E. (2010). Attention on our mind: the role of spatial attention in visual
working memory. Acta psychologica, 137(2), 248–51.
Doi: 10.1016/j.actpsy.2010.06.011
Thomas, L. E., & Lleras, A.
(2009).
Covert
shifts of attention function as an implicit aid to
insight.
Cognition, 111(2), 168–174.
Doi:
10.1016/j.cognition.2009.01.005

1912

