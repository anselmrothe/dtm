Belief Utility as an Explanatory Virtue
Samuel G. B. Johnson, Greeshma Rajeev-Kumar, & Frank C. Keil
(samuel.johnson@yale.edu, greeshma.rajeev-kumar@yale.edu, frank.keil@yale.edu)
Department of Psychology, Yale University, 2 Hillhouse Ave., New Haven, CT 06520 USA
Abstract

should turn the ship to port, he might nonetheless act on
the starboard hypothesis, if the consequences of being
wrong are graver when the starboard hypothesis is true.
On the pragmatist view, the captain might not merely act
prudently, as though the starboard hypothesis is true, but
also believe it to be true, despite its lower probability.
More generally, in high-stakes situations where beliefs
guide our actions, we might be prudent not only in our
actions, but also in our beliefs. The decision-theoretic
view can explain the ship captain’s prudent action if he
assigns a low probability to the dangerous outcome, yet
acts to prevent that outcome because its disutility is so
high. But on the pragmatist view, this calculation might
be facilitated by the belief system itself, which may adjust
the probabilities to favor the more prudent course of
action, even if the evidence disagrees.
Although pragmatist considerations have long played a
role in psychological theories (Fiske, 1992; James,
1983/1890), this kind of radical pragmatism would be a
departure from standard approaches to decision-making.
Traditionally, biases in decision-making are explained in
terms of short-cuts in belief formation (Tversky &
Kahneman, 1974) or nonlinear utility functions relative to
probability (Kahneman & Tversky, 1979). If people both
adjusted their beliefs prudentially in light of the utility of
potential actions, and then acted prudentially given those
already-adjusted beliefs, then people would adjust twice
for the greater utility of taking the prudential action, and
may act in a way that is too conservative, relative to the
principles of rational decision theory.
We conducted three experiments to test whether people
are more likely to believe explanations that have greater
(prudential) utility. The basic paradigm was the same in
all experiments. Participants learned about some
evidence, which two potential hypotheses could explain—
either an innocuous explanation (e.g., a minor disease that
leads to soreness) or a dire explanation (e.g., a severe
disease that leads to a fatal tumor). The evidence either
favored one of the explanations or was ambiguous.
However, because one explanation necessitated action
more strongly than the other, participants should take
action as though the severe explanation is true, even when
the evidence is ambiguous (e.g., a doctor would treat the
severe disease even if a minor disease is just as likely).
Similarly, when the evidence favors the severe disease,
one should be strongly inclined to take the corresponding
action, whereas when the evidence favors the minor
explanation, one should be comparatively less inclined to
take the corresponding action because the consequences
of inaction are minor. This result would be consistent with
either the decision-theoretic or the pragmatist theory.

Our beliefs guide our actions. But do potential actions also
guide our beliefs? Three experiments tested whether people
use pragmatist principles in fixing their beliefs, by
examining situations in which the evidence is
indeterminate between an innocuous and a dire explanation
that necessitate different actions. According to classical
decision theory, a person should favor a prudent course of
action in such cases, but should nonetheless be agnostic in
belief between the two explanations. Contradicting this
position, participants believed the dire explanation to be
more probable when the evidence was ambiguous. Further,
when the evidence favored either an innocuous or a dire
explanation, evidence favoring the dire explanation led to
stronger beliefs compared to evidence favoring the
innocuous explanation. These results challenge classic
theories of the relationship between belief and action,
suggesting that our system for belief fixation is sensitive to
the utility of potential beliefs for taking subsequent action.
Keywords: Explanation;
categorization; rationality.

beliefs;

causal

reasoning;

Introduction
We are agents embedded in the world. Our cognitions
exist in part to support adaptive behavior (Fiske, 1992;
James, 1983/1890). Thus, beliefs allow us to store
information that may be useful for taking beneficial
actions later on. Here, we examine two ways that beliefs
might guide actions—and that actions might guide beliefs.
The classical decision-theoretic view asserts that our
actions are guided by our beliefs according to a set of
decision-making principles. That is, we attempt to
ascertain the truth, and on the basis of these beliefs, we
take action (Jeffrey, 1965; von Neumann & Morgenstern,
1944). Although most behavioral economists (unlike
laypeople; see Johnson & Rips, 2015) do not believe that
people are utility maximizers, most modifications to
utility-maximization theory retain the underlying
assumption that beliefs about outcome probabilities are
fixed before the decision-making process occurs (e.g.,
Kahneman & Tversky, 1979; Shafir & LeBoeuf, 2002).
An alternative, pragmatist view holds that not only do
our beliefs guide our actions, but our potential actions
also guide our beliefs, because some beliefs are more
useful to hold than others. (See Evans & Over, 1996 for a
related distinction between impersonal, logic-based and
personal, utility-based rationality.) To adapt an example
from the philosopher Charles Sanders Peirce (1997/1903),
consider the plight of a ship captain caught in a storm,
who must decide whether to put his wheel to port or to
starboard, acting on one or another hypothesis. Even if the
probabilistic evidence favors the hypothesis that he

1009

However, the decision-theoretic and pragmatist theories
diverge in their predictions for beliefs. When the evidence
is completely ambiguous, favoring neither explanation,
people should be agnostic between the two explanations
according to the decision-theoretic view—prudent actions
emerge because people take into account the greater
disutility of inaction if the severe explanation is true.
Further, if the evidence is probabilistically symmetric
when it favors either the minor or the severe explanation,
the decision-theoretic view would predict that if one has a
given degree of confidence in the minor explanation when
the evidence favors that explanation, one should have that
same degree of confidence in the severe explanation when
the evidence favors that explanation.
However, on the pragmatist view, people may favor the
severe explanation even when the evidence is ambiguous,
because this belief would facilitate prudent action. They
also may believe more strongly in the severe explanation
when the evidence favors it, compared to how strongly
they believe the minor explanation when the evidence
favors it. Such results would be in normative tension with
probability theory and expected utility theory, as well as
dominant theories of decision-making in psychology.
Our experiments varied both the vignette content and
the degree of explanatory ambiguity. In Experiment 1, we
used a medical diagnosis paradigm to test these
hypotheses, in line with previous studies of explanatory
reasoning (e.g., Khemlani, Sussman, & Oppenheimer,
2011; Lombrozo, 2007). In Experiment 2, we extended
these results to a wider range of stimuli that included both
causal explanations and categorizations. To vary
explanatory ambiguity, Experiment 3 introduced an
additional unverified prediction to one of the
explanations,
which
interferes
with
normative
probabilistic reasoning (Johnson, Rajeev-Kumar, & Keil,
2014; Khemlani et al., 2011).

disease, which differed in one symptom that was either
minor or severe:
Prasntosis causes itchy skin, spots on the face, and
mild soreness.
Hanriosis causes itchy skin, spots on the face, and a
potentially fatal tumor.
Next, participants read about treatments for the diseases:
There are effective medications for treating both
Prasntosis and Hanriosis. In a recent study, the
Prantosis medicine was effective for 80 out of 100
people, and the Hanriosis medicine was effective for
80 out of 100 people. There are no known side effects
or risks associated with taking either medicine.
Participants then learned about the patient’s symptoms. In
the Minor condition, the patient had all the symptoms of
the minor disease:
Laura has itchy skin and spots on her face. She also
has mild soreness.
In the Severe condition, the patient had all the symptoms
of the severe disease:
Laura has itchy skin and spots on her face. She also
has a potentially fatal tumor.
Finally, in the Ambiguous condition, information about
the distinguishing symptom was unavailable:
Laura has itchy skin and spots on her face. We don’t
know whether she has mild soreness or a potentially
fatal tumor.
Participants then completed an Action question and a
Belief question. For the Action question, participants were
asked to recommend a treatment (“Suppose you can only
give medicine to treat one disease. For which disease
would you administer the medicine to Laura?”) on a scale
from 0 (“Definitely Prasntosis”) to 10 (“Definitely
Hanriosis”). For the Belief question, participants were
asked to diagnose the patient’s disease (“Which disease
do you think Laura has?”) on the same scale.
The names and order of listing the two diseases was
randomized for each item. The order of mentioning the
diseases in the Ambiguous condition and the left/right
orientation of the rating scales were adjusted to match.
The order of the Action and Belief questions was
counterbalanced across participants. The assignment of
vignette (i.e., names and symptoms of the diseases) was
counterbalanced with condition using a Latin square, and
items were presented in a random order.
Two manipulation checks were also included. First,
after each item, participants were asked to rate the
seriousness of each disease. Any participant was excluded
who gave a higher seriousness rating for the minor
disease for any of the items. Second, at the end of the
experiment, participants completed a memory test for
items encountered during the experiment. These check
questions were included in order to detect participants
who might be responding at random. Any participant
answering more than one-third of these questions
incorrectly was excluded.

Experiment 1
In Experiment 1, participants gave diagnoses and
treatment recommendations for patients with various
combinations of symptoms. We measured both
participants’
preferred
actions
(i.e.,
treatment
recommendations) and beliefs (diagnoses), to distinguish
between the decision-theoretic and pragmatist theories.

Method
We recruited 200 participants from Amazon Mechanical
Turk; 32 participants were excluded from data analysis
because they failed a manipulation check (see below).
Participants were told that they would learn about some
diseases and “a patient who has one of the diseases.”
Participants completed three items, each specifying that
two diseases had equal base rates. For example:
There are two rare diseases, called Prasntosis and
Hanriosis. Both affect about 1 in 800 people during
their lifetimes.
Then, participants read about the symptoms of each

1010

than they were negative in the Minor condition [M = 3.25, SD = 1.83; t(167) = 4.34, p < .001, d = 0.34, BF10 =
436.7], indicating that their belief in the severe disease
was stronger when favored by the evidence than was their
belief in the minor disease when favored by the evidence.
Figure 1 plots this asymmetry for all three experiments,
showing mean belief in the Severe condition, and the
inverse of the mean in the Minor condition.
Second, participants could favor the severe disease even
in the Ambiguous condition, where the evidence was
equally consistent with both explanations. Figure 2 plots
this effect across all three experiments, showing the mean
belief in the Ambiguous condition. This effect was not
significant in Experiment 1 [M = 0.14, SD = 1.20; t(167)
= 1.47, p = .14, d = 0.11, BF01 = 5.6], although the mean
was in the predicted direction in all three experiments.
These results support the pragmatist position. Because
the base rates of the diseases were equal, evidence
favoring the severe disease is no more diagnostic than
evidence favoring the minor disease. Yet, participants
more strongly believed the severe explanation when
favored by the evidence than the minor explanation when
favored by the evidence. This follows participants’ greater
confidence in treating the severe disease in the Severe
condition than in treating the minor disease in the Minor
condition, and suggests that participants’ beliefs were
influenced by pragmatic considerations—taking into
account the utility of each belief.
If the pragmatist position is correct, then why did
participants not reliably favor the serious disease in the
Ambiguous condition? Various features of a medical
diagnosis task could potentially attenuate this effect. For
example, putting oneself in the position of an expert such
as a doctor might encourage the use of more deliberate
rather than intuitive judgments, which could lead one to
rely more on probability and less on belief utility. Further,
although the instructions stated that the patient had one of
the diseases, diseases are not mutually exclusive, and
participants wishing to express a degree of belief that the
patient has both symptoms might use the center of the
scale to express this belief. In Experiment 2, we used a
wider variety of items to address such possibilities.

Experiment 1
Experiment 2
Severe
Minor

Experiment 3
0

1

2
3
Strength of Belief

4

Figure 1: Asymmetry in belief strength between Severe
and Minor conditions. Bars represent 1 SE.

Results and Discussion
In reporting results of all experiments, participants’
responses were converted to a -5 to 5 scale, where
negative scores correspond to the minor explanation and
positive scores correspond to the severe explanation.1
We first analyzed responses to the Action question, to
ensure that our manipulation was successful. In the Minor
condition, participants favored the medicine for the minor
disease [M = -2.28, SD = 3.43; t(167) = -8.61, p < .001, d
= -0.66, BF10 > 1000], whereas in the Severe condition,
participants strongly favored the medicine for the severe
disease [M = 4.48, SD = 1.26; t(167) = 4.48, p < .001, d =
3.56, BF10 > 1000]. Finally, in the Ambiguous condition,
where the evidence favored neither disease, participants
favored treating the serious disease [M = 2.72, SD = 3.43;
t(167) = 15.24, p < .001, d = 1.18, BF10 > 1000]. Thus,
participants chose their actions in accord both with the
evidence (favoring actions corresponding to evidence
when unambiguous) and with prudence (favoring the
safer course of action when the evidence was ambiguous).
Further, the inclination to treat the severe disease was
much greater when the evidence favored the severe
disease, compared to the inclination to treat the minor
disease when the evidence favored the minor disease.
Our main interest was in whether these pragmatic
considerations might influence participants’ beliefs. There
are two ways in which this might occur. (Neither effect
differed in any experiment as a function of whether the
action or belief question was asked first, speaking against
the possibility of demand effects or scale biases.)
First, participants could give more extreme belief
ratings in the Severe condition than in the Minor
condition. This difference occurred: Ratings were more
positive in the Severe condition [M = 3.91, SD = 1.73]

Experiment 2
Experiment 2 had two primary goals. First, we wished to
generalize the effects of belief utility to a wider range of
stimuli. If the null effect in the Ambiguous condition of
Experiment 1 is due to task features specific to medical
diagnosis, then this effect might be detectable in different
but conceptually related tasks.
Second, we wished to test whether belief utility is used
as an explanatory virtue not only in causal explanation,
but also in categorization. Several findings suggest that
both categorization and causal explanation may rely in
part on the same mechanisms for abductive (data-tohypothesis) inference (e.g., Johnson, Merchant, & Keil,
2015; Johnson, Rajeev-Kumar, & Keil, 2015). To test

1

In addition to conventional statistics, we also report Bayes
Factors (BFs) for each test (scale factor = 1), because we wished
to be able to quantify evidence in favor of the null hypothesis
(Rouder, Speckman, Sun, Morey, & Iverson, 2009). For
example, if the evidence were three times likelier under the null
hypothesis than under the alternative, this would be denoted
‘BF01 = 3.0’. In contrast, if the evidence were six times likelier
under the alternative hypothesis than under the null, this would
be denoted ‘BF10 = 6.0’.

1011

whether belief utility might likewise be an explanatory
virtue common to both cause- and category-based
explanation, Experiment 2 included both kinds of items.

Experiment 1

Method

Experiment 2

We recruited 200 participants from Amazon Mechanical
Turk; 32 participants were excluded because they failed
more than one-third of the check questions.
Participants completed six items, in a format similar to
Experiment 1, except a variety of situations were used in
place of medical diagnosis. For example, one item read:
Imagine you are an airport security personnel in
charge of the x-ray machines. You have to determine
which bags to open and inspect based on these
signals. The x-ray machine gives the following
signals to indicate safe and danger.
A safe signal results in a square, a ringing tone, and a
check mark.
A danger signal results in a square, a ringing tone,
and an X mark.
You see a triangular bag. In your experience, about
half of triangular bags are safe and about half are
dangerous.
That is, like the scenarios of Experiment 1, one potential
explanation (“the bag is safe”) had minor consequences
whereas the other (“the bag is dangerous”) had severe
consequences, and they had equal base rates (since half of
triangular bags belong to each category).
Two items (in the Minor condition) indicated that all
features or effects of the innocuous explanation occurred:
The triangular bag gets a square and a ringing tone. It
also gets a check mark.
For two Severe items, all consequences of the grave
explanation occurred:
The triangular bag gets a square and a ringing tone. It
also gets an X mark.
Finally, for two Ambiguous items, information about the
distinguishing observation was unavailable:
The triangular bag gets a square and a ringing tone.
You can’t tell whether it got a check mark or an X
mark.
The Action question asked participants what they would
do (e.g., “Would you open and inspect the triangular
bag?”), on a scale from 0 (“Definitely No”) to 10
(“Definitely Yes”), and the Belief question asked
participants what they thought the best explanation was
(e.g., “Do you think the triangular bag got the safe signal
or the danger signal?”) on a scale from 0 (“Definitely
Safe”) to 10 (“Definitely Danger”).
Six cover stories were used. In three cases, the
explanations were causes that explained effects (airport
security, machines at a factory, and automotive repair)
and in three cases, the explanations were categories that
explained features (animals when hunting, air traffic
control signals, and railway signals).
The order of the minor and severe explanations was
pseudorandomized across items, and the assignment of

Experiment 3
0

0.25

0.5

Strength of Belief
Figure 2: Strength of belief in the severe explanation in
the Ambiguous condition. Bars represent 1 SE.
the three conditions to the six cover stories was
counterbalanced using a Latin square. The ‘seriousness’
questions from Experiment 1 were omitted to avoid
potential demand characteristics, but the memory check
was retained at the end of the experiment.

Results and Discussion
As in Experiment 1, participants’ action judgments
favored the minor explanation in the Minor condition [M
= -2.91, SD = 2.43; t(167) = -15.54, p < .001, d = -1.20,
BF10 > 1000], but more strongly favored the severe
explanation in the Severe condition [M = 3.85, SD = 1.81;
t(167) = 27.53, p < .001, d = 2.12, BF10 > 1000]. They
also favored the severe explanation in the Ambiguous
condition [M = 3.10, SD = 2.02; t(167) = 19.94, p < .001,
d = 1.54, BF10 > 1000]. Thus, participants again made
judgments in accord with the evidence and with prudence.
As in Experiment 1, participants’ beliefs were
influenced by prudential considerations. Participants in
Experiment 2 favored the severe explanation even in the
Ambiguous condition [M = 0.50, SD = 1.24; t(167) =
5.30, p < .001, d = 0.41, BF10 > 1000; Figure 2]. That is,
when the evidence favored neither explanation,
participants favored the explanation corresponding to the
prudential course of action. The asymmetry between the
strength of belief in the Severe and Minor conditions was
not significant in Experiment 2 [M = 3.82, SD = 1.71 and
M = -3.68, SD = 1.73; t(167) = 0.91, p = .36, d = 0.07,
BF01 = 10.9; Figure 1], although it was in the predicted
direction in all experiments.
These results once again support the pragmatist view,
uncovering an even more flagrant violation of decisiontheoretic norms than in Experiment 1. When the evidence
is indeterminate between two explanations with equal
base rates, the explanations have equal probability. Yet,
participants’ beliefs favored the severe explanation when
the evidence was indeterminate, as predicted by the
pragmatist position. These results both generalize effects
of belief utility to a broader set of items, and show that
belief utility is used as an explanatory virtue both in
cause-based and category-based explanations.

1012

Experiment 3

factors were counterbalanced as in Experiment 2. The six
items were presented in a random order

In our previous experiments, the evidence in the
Ambiguous condition was indeterminate between the two
explanations because a critical piece of diagnostic
information was unavailable. In general, people distrust
explanations that make unverified or latent predictions
(Khemlani et al., 2011). Although this bias cannot explain
the results of Experiments 1 and 2 because both
explanations made latent predictions in the Ambiguous
condition, we took advantage of this bias to introduce
greater perceived ambiguity in Experiment 3. In addition
to the latent predictions already made in the Ambiguous
condition, we added an additional latent prediction to
either the minor or severe explanation in all three
conditions. Because explanatory intuitions are less stable
when latent predictions are introduced, we anticipated that
this added ambiguity could make more room for belief
utility to affect explanatory judgments.

Results and Discussion
There were no differences between items for which the
latent symptom was diagnostic of the minor or severe
explanation, so we collapse across this factor.
Once again, action judgments were as expected,
favoring the minor explanation in the Minor condition [M
= -1.22, SD = 3.15; t(90) = -3.66, p < .001, d = -0.39, BF10
= 40.0] and the severe explanation in the Severe condition
[M = 3.68, SD = 1.68; t(90) = 20.76, p < .001, d = 2.19,
BF10 > 1000] and the Ambiguous condition [M = 2.97, SD
= 2.01; t(90) = 14.01, p < .001, d = 1.48, BF10 > 1000].
These prudential considerations again affected belief
judgments. First, beliefs favored the severe explanation
more in the Severe condition [M = 2.96, SD = 1.90] than
they favored the minor explanation in the Minor condition
[M = -2.28, SD = 2.36; t(89) = 2.58, p = .011, d = 0.27,
BF10 = 2.0; Figure 1]. Second, beliefs favored the severe
explanation even in the Ambiguous condition, where the
evidence was equally consistent with either possibility [M
= 0.45, SD = 1.12; t(89) = 3.82, p < .001, d = 0.40, BF10 =
66.9; Figure 2].
These are the most robust findings in favor of the
pragmatist position, since participants’ judgments
significantly violated decision-theoretic principles in both
possible ways. This seems to have occurred because the
additional ambiguity introduced by the latent evidence
enhanced the effect of belief utility on explanatory
judgments. Compared with Experiment 2, the effect in the
Ambiguous condition was of similar size [t(256) = 0.33, p
= .74, d = 0.04, BF01 = 9.2], but the asymmetry between
the Minor and Severe conditions was marginally larger
[t(256) = 1.92, p = .056, d = 0.25, BF01 = 2.6]. Future
research should follow up on this result to investigate the
role of evidence ambiguity in the use of belief utility.

Method
We recruited 100 participants from Amazon Mechanical
Turk; 10 participants were excluded because they failed
more than one-third of the check questions.
Experiment 3 was identical to Experiment 2, except that
it incorporated the latent evidence as follows.
For three (Minor-Latent) items, the innocuous
explanation had an additional consequence (e.g., “a brown
dot”), which was unobserved. For example:
A safe signal results in a square, a ringing tone, a
brown dot, and a check mark.
A danger signal results in a square, a ringing tone,
and an X mark.
For the other three (Severe-Latent) items, the grave
explanation had the additional unobserved consequence:
A safe signal results in a square, a ringing tone, and a
check mark.
A danger signal results in a square, a ringing tone, a
brown dot, and an X mark.
Within each set of three items, there was one item for
which there was strong evidence for the Minor
explanation:
The triangular bag gets a square and a ringing tone. It
also get a check mark. You can’t tell whether it gets a
brown dot or not.
There was one item for which there was strong evidence
for the Severe explanation:
The triangular bag gets a square and a ringing tone. It
also gets an X mark. You can’t tell whether it gets a
brown dot or not.
Finally, there was one item for which the evidence was
ambiguous between the Minor and Severe explanations:
The triangular bag gets a square and a ringing tone.
You can’t tell whether it got a check mark or an X
mark. You also can’t tell whether it gets a brown dot
or not.
Which three vignettes were in the Minor-Latent or
Severe-Latent condition was counterbalanced. The other

General Discussion
Our actions are shaped by our beliefs. Here, we showed
that our beliefs are also shaped by the potential courses of
action they entail—that people take account of prudential
considerations in their explanatory judgments, in two
ways. First, evidence favoring a dire explanation was
taken as more diagnostic than evidence favoring an
innocuous explanation (Experiments 1 and 3). Second,
completely ambiguous evidence was taken to favor a dire
explanation (Experiments 2 and 3).
Several alternative explanations should be considered,
which could be consistent with the classic decisiontheoretic view. First, could the evidence have been seen
as more diagnostic when it favored the severe
explanation? That is, when a person has a tumor, that
might be seen as overwhelming evidence favoring one
explanation, whereas when a person has soreness, that
may be seen as less convincing evidence favoring the
other. Although this could potentially account for the

1013

References

difference between the Minor and Severe conditions, it
would not predict any effect in the Ambiguous condition,
since the evidence is the same for both explanations.
Further, this explanation would not account for
Experiments 2 and 3, where the evidence would not seem
to differ in diagnosticity (e.g., an X mark or check mark).
Second, could the severe explanations be more salient
than the minor explanations, leading them to be more
believable? Because imagining something to be true
makes it seem more likely to be true (Koehler, 1991),
perhaps participants found the more vivid severe
explanations (tumors, dangerous bags) to be more
believable than the more pallid minor explanations
(soreness, safe bags). In one sense, this may be more a
mechanism for instantiating prudential considerations,
rather than an alternative hypothesis. At the same time,
though, this account would seem to predict that making
the evidence more ambiguous should make the
explanations less vivid, and hence would attenuate the
effects. Experiment 3 found evidence for the opposite.
Finally, could participants have been interpreting the
belief questions (“Which disease do you think Laura
has?”) as asking about pretense (“Which disease would
you act as though Laura has?”)? Although this
interpretation would indeed lead to the same pattern of
results that we found, we think it is unlikely that this is
driving our results. These belief questions were asked on
the same page as a question asking directly about which
course of action the participant would take (“For which
disease would you administer the medicine to Laura?”),
which should pragmatically discourage participants to
encourage the belief question as asking about their
actions. Further, the order of asking the belief and action
questions did not make any difference, as it should have if
the belief question was found to be ambiguous.
These results have broad implications for reasoning,
judgment, and decision-making. Although expected utility
theory has been widely challenged (Shafir & LeBoeuf,
2002), the underlying assumption that beliefs are
computed independently of the utility of actions has
remained unchallenged. These findings complicate this
picture, showing that the potential courses of actions
implied by our beliefs feed back to affect those beliefs.
To the extent that our decision-making capacity might
‘double correct’ for what is prudent, this could result in
overly conservative behavior, such as extreme risk
aversion. Further, belief utility seems to exert a stronger
effect when people are less able to estimate precise
probabilities—a situation that may be all too common in
day-to-day life. Yet, interventions might be devised to
encourage people to form their beliefs in accordance with
the evidence rather than their utility. Such interventions
await empirical support.

Evans, J.St.B.T., & Over, D.E. (1996). Rationality and
reasoning. East Sussex, UK: Psychology Press.
Fiske, S.T. (1992). Thinking is for doing: Portraits of
social cognition from daguerreotype to laserphoto.
Journal of Personality and Social Psychology, 63, 877–
889.
James, W. (1983). The principles of psychology.
Cambridge, MA: Harvard University Press. (Original
work published 1890.)
Jeffrey, R. C. (1965). The logic of decision. New York,
NY: McGraw-Hill.
Johnson, S.G.B., Merchant, T., & Keil, F.C. (2015).
Predictions from uncertain beliefs. Proceedings of the
37th Annual Conference of the Cognitive Science
Society. Austin, TX: Cognitive Science Society.
Johnson, S.G.B., Rajeev-Kumar, G., & Keil, F.C. (2014).
Inferred evidence in latent scope explanations.
Proceedings of the 36th Annual Conference of the
Cognitive Science Society. Austin, TX: Cognitive
Science Society.
Johnson, S.G.B., Rajeev-Kumar, G., & Keil, F.C. (2015).
Sense-making under ignorance. Manuscript under
review.
Johnson, S.G.B., & Rips, L.J. (2015). Do the right thing:
The assumption of optimality in lay decision theory and
causal judgment. Cognitive Psychology, 77, 42–76.
Kahneman, D., & Tversky, A. (1979). Prospect theory:
An analysis of decision under risk. Econometrica, 47,
263–291.
Khemlani, S. S., Sussman, A. B., & Oppenheimer, D. M.
(2011). Harry Potter and the sorcerer’s scope: Latent
scope biases in explanatory reasoning. Memory &
Cognition, 39, 527–535.
Koehler, D.J. (1991). Explanation, imagination, and
confidence in judgment. Psychological Bulletin, 110,
499–519.
Lombrozo, T. (2007). Simplicity and probability in causal
explanation. Cognitive Psychology, 55, 232–257.
Peirce, C.S. (1997). Pragmatism as a principle and
method of right thinking: The 1903 Harvard lectures on
pragmatism. Albany, NY: SUNY Press.
Rouder, J.N., Speckman, P.L., Sun, D., Morey, R.D., &
Iverson, G. (2009). Bayesian t tests for accepting and
rejecting the null hypothesis. Psychonomic Bulletin &
Review, 16, 225–237.
Shafir, E., & LeBoeuf, R.A. (2002). Rationality. Annual
Review of Psychology, 53, 491–517.
Tversky, A., & Kahneman, D. (1974). Judgment under
uncertainty: Heuristics and biases. Science, 185, 1124–
1131.
Von Neumann, J., & Morgenstern, O. (1944). Theory of
games and economic behavior. Princeton, NJ:
Princeton University Press.

Acknowledgments
We thank Laurie Santos, Josh Knobe, and the members of
the Cognition and Development Lab for discussion.

1014

