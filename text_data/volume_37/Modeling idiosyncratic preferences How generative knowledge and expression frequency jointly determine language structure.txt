   Modeling idiosyncratic preferences: How generative knowledge and expression
                               frequency jointly determine language structure
                                               Emily Morgan (eimorgan@ucsd.edu)
                                                    Roger Levy (rlevy@ucsd.edu)
                   Department of Linguistics, UC San Diego, 9500 Gilman Drive, La Jolla, CA 92093-0108 USA
                              Abstract                                     Specifically, we identify two reasons why such a model is
                                                                        advantageous:
   Most models of choice in language focus on broadly applica-
   ble generative knowledge, treating item-specific variation as        1. Models identify both rules and exceptions.
   noise. Focusing on word order preferences in binomial ex-
   pressions (e.g. bread and butter), we find meaning in the            One intrinsic reason that modeling idiosyncrasies is advan-
   item-specific variation: more frequent expressions have more         tageous is because identifying exceptions can help identify
   polarized (i.e. frozen) preferences. Of many models consid-
   ered, only one that takes expression frequency into account can      rules. In a traditional linguistic setting (e.g. identifying rules
   predict the language-wide distribution of preference strengths       for past tense formation), we rely upon intuition to deter-
   seen in corpus data. Our results support a gradient trade-off in     mine what is the grammatical rule and which verbs should
   language processing between generative knowledge and item-
   specific knowledge as a function of frequency.                       be treated as exceptions. In the case of binomial expressions,
                                                                        we likewise expect there to be exceptions to the rules, partic-
   Keywords: Bayesian modeling; binomial expression; fre-
   quency; word order                                                   ularly for frequent expressions. For example, there is in gen-
                                                                        eral a strong constraint to put men before women; however,
                                                                        ladies and gentlemen is preferred over the reverse due to its
                          Introduction                                  conventionalized formal use. But compared with past tense
A pervasive question in language processing research is how             formation, the rules that determine binomial ordering are far
we reconcile generative knowledge with idiosyncratic prop-              more complex and gradient, such that using traditional lin-
erties of specific lexical items. In many cases, the generative         guistic analysis to determine the full set of rules is not viable.
knowledge is the primary object of study, while item-specific           In this case, we require our model not only to identify what
idiosyncrasies are treated as noise. For instance, in mod-              the rules are but simultaneously to determine which expres-
eling the dative alternation, Bresnan, Cueni, Nikitina, and             sions must be treated as exceptions. Having such a model is
Baayen (2007) take care to demonstrate that effects of ani-             useful for empirical cognitive science, e.g. for disentangling
macy, givenness, etc. on structure choice hold even after ac-           the effects of people’s generative knowledge from effects of
counting for biases of individual verbs. But the verb biases            their item-specific linguistic experience on language process-
themselves are not subject to any serious investigation. Here           ing (Morgan & Levy, 2015).
we present evidence that patterns within the item-specific
variation are meaningful, and that by modeling this variation,          2. Models relate cognitive representations to
we not only obtain better models of the phenomenon of in-               language-wide structure.
terest, we also learn more about language structure and its             As a further benefit, models can help us understand how
cognitive representation.                                               structural properties of the language relate to people’s cog-
   Specifically, we will develop a model of word order prefer-          nitive linguistic representations. In particular, let us look at
ences for binomial expressions of the form X and Y (i.e. bread          the distribution of preferences for binomial expressions taken
and butter preferred over butter and bread). Binomial order-            from a subset of the Google Books corpus (described later in
ing preferences are in part determined by generative knowl-             Creating the Corpus.) Each binomial can be assigned a pref-
edge of violable constraints which reference the semantic,              erence strength corresponding to how frequently it appears in
phonological, and lexical properties of the constituent words           alphabetical order, from 0 (always in non-alphabetical order)
(e.g. short-before-long; Cooper & Ross, 1975; McDonald,                 to 0.5 (perfectly balanced) to 1 (always alphabetical). Bino-
Bock, & Kelly, 1993), but speakers also have idiosyncratic              mials which always or nearly always appear in one order are
preferences for known expressions (Morgan & Levy, 2015;                 said to be frozen. The distribution of preference strengths is
Siyanova-Chanturia, Conklin, & van Heuven, 2011). Bino-                 shown in Figure 1. Preferences have a multimodal distribu-
mial expressions are a useful test case for modeling idiosyn-           tion with modes at the extremes as well as around 0.5. This
cracies because their frequencies can be robustly estimated             distribution poses a challenge to standard models of binomial
from the Google Books n-grams corpus (Lin et al., 2012).                preferences. As we will show later, standard models predict
Here we will demonstrate that explicitly modeling these ex-             only a single mode around 0.5. In other words, the true distri-
pressions’ idiosyncrasies both produces a better predictive             bution of binomial expressions includes more frozen binomi-
model for novel expressions and also constrains our theory              als than standard models predict. As we develop a model that
of these expressions’ cognitive representations.                        accounts for this multimodal distribution, we will see that this
                                                                    1649

language-structural fact puts constraints on our theories of in-      Power The more powerful or culturally prioritized word
dividuals’ cognitive representations of binomial expressions.         comes first, e.g. clergymen and parishioners.
   In the remainder of this paper, we first describe how we           Iconic/scalar sequencing Elements that exist in sequence
developed a new corpus of binomial expressions. We then               should be ordered in sequence, e.g. achieved and maintained.
explore a variety of models with differing levels of ability to       Cultural Centrality The more culturally central or common
model item-specific idiosyncrasies. Finally, we return to the         element should come first, e.g. oranges and grapefruits.
issue of how these models inform us about cognitive repre-            Intensity The element with more intensity appears first, e.g.
sentations of language.                                               war and peace.
                            Histogram of binomial types               The metrical constraints, Length and No final stress, were
                  1.5                                                 automatically extracted from the CMU Pronouncing Dictio-
                                                                      nary (2014), augmented by manual annotations when neces-
                                                                      sary. Word frequency was taken from the Google Books cor-
               Density
                                                                      pus, counting occurrences from 1900 or later. Semantic con-
                                                                      straints were hand coded by two independent coders (drawing
            0.5       1.0                                             from the first author and two trained research assistants). Dis-
                                                                      crepancies were resolved through discussion.
                                                                         For each binomial, we obtained the number of occurrences
                  0.0                                                 in both possible orders in the Google Books corpus from 1900
                 0.0 0.2 0.4 0.6 0.8 1.0
            Proportion of occurrences in alphabetical order
                                                                      or later. Items containing proper names, those with errors
                                                                      in the given parses, those whose order was directly affected
Figure 1: Binomial preferences are multimodally distributed           by the local context (e.g. one element had been mentioned
in corpus data                                                        previously), and those with less than 1000 total occurrences
                            Creating the Corpus                       across both orders were excluded from analysis, leaving 594
                                                                      binomial expression types.
We extracted all Noun-and-Noun binomials from the
parsed section of the Brown corpus (Marcus, Santorini,                                           Models
Marcinkiewicz, & Taylor, 1999) using the following Tregex
(Levy & Galen, 2006) search pattern:                                  We will develop four models of binomial ordering prefer-
                                                                      ences: a standard logistic regression, a mixed-effects logis-
/ˆN/=top < (/ˆNN/ !$, (/,/ > =top) .
                                                                      tic regression, and two hierarchical Bayesian beta-binomial
((CC <: and > =top) . (/ˆNN/ > =top)))
                                                                      models. All are based on the idea of using logistic regres-
This pattern finds all Noun-and-Noun sequences dominated              sion to combine the constraints described above in a weighted
by a Noun Phrase which are not preceded by a comma (to                fashion to produce an initial preference estimate for each bi-
exclude the final pair in lists of more than two elements), a         nomial. The models differ in whether and how they explic-
total of 1280 tokens.                                                 itly model the fact that true preferences will be distributed id-
   Binomials were coded for a variety of constraints, origi-          iosyncratically around these estimates. The standard logistic
nally described by Benor and Levy (2006) but restricted to            regression includes no explicit representation of item-specific
the subset determined to be most relevant for predicting or-          idiosyncrasies. The mixed-effect logistic regression includes
dering preferences by Morgan and Levy (2015):                         random intercepts which account for item-specific idiosyn-
Length The shorter word (in syllables) comes first, e.g.              crasies, but which are constrained to be distributed normally
abused and neglected.                                                 around the initial prediction. The two Bayesian models as-
No final stress The final syllable of the second word should          sume that item-specific preferences are drawn from a beta
not be stressed, e.g. abused and neglected.                           distribution whose mean is determined by the initial predic-
Lapse Avoid unstressed syllables in a row, e.g. FARMS and             tion. In the first of these models, the concentration of the beta
HAY-fields vs HAY-fields and FARMS                                    distribution is fixed, while in the second, it varies with the
Frequency The more frequent word comes first, e.g. bride              frequency of the binomial in question.
and groom.
Formal markedness The word with more general meaning                  Evaluation
or broader distribution comes first, e.g. boards and two-by-          One obvious criterion for evaluating a model is how well it
fours.                                                                predicts known binomial preferences (i.e. the corpus data).
Perceptual markedness Elements that are more closely                  For this, we report R2 (X, X̂) as well as mean L1 error,
                                                                       1 N
connected to the speaker come first. This constraint encom-           N Σi=1 |x̂i − xi |, where x̂i is the model prediction for how of-
passes Cooper and Ross’s (1975) ‘Me First’ constraint and in-         ten binomial i occurs in a given order, and xi is the true corpus
cludes numerous subconstraints, e.g.: animates precede inan-          proportion.
imates; concrete words precede abstract words; e.g. deer and             In addition to considering model predictions for each in-
trees.                                                                dividual item, we want to consider the overall distribution of
                                                                   1650

preferences within the language. As we will see, a model can         we take the sample median for each item, which optimizes
provide good predictions for individual items without cor-           the L1 error.
rectly capturing the language-wide multimodal distribution              Including random intercepts improves neither our point es-
of these expressions’ preference strengths. Thus our second          timates nor our language-wide distribution prediction. Appar-
desideratum will be the shape of the histogram of expression         ently, the normal distribution of the random intercepts is not
preferences.                                                         well suited to capturing the true distribution of binomial pref-
                                                                     erences. In particular, for a given item, the normality of ran-
Logistic regression
                                                                     dom effects in logit space leads to predictions that are skewed
Logistic regression is the standard for modeling syntactic al-       towards the extremities of probability space.1
ternations, both for binomial expressions specifically (e.g.
Benor & Levy, 2006; Morgan & Levy, 2015) as well as                  Hierarchical Bayesian beta-binomial model
other syntactic alternations (e.g. Bresnan et al., 2007; Jaeger,
2010). Thus we begin by constructing a baseline logistic re-         Having seen that normally distributed random intercepts do
gression model. Benor and Levy have argued that one should           not adequately capture the distribution of item-specific pref-
train such a model on binomial types rather than binomial to-        erences, we introduce the beta distribution as a potentially
kens because otherwise a large number of tokens for a small          better way to model this distribution. The beta distribution,
number of overrepresented types can skew the results. While          defined on the interval [0, 1], has two parameters: one which
agreeing with this logic, we note that to train only a single        determines the mean of the draws from the distribution, and
instance of each type is to ignore a vast amount of data about       one which determines the concentration, i.e. whether draws
the gradient nature of binomial preferences. As a compro-            are likely to be clustered around the mean versus distributed
mise, we instead train a model on binomial tokens, using to-         towards 0 and 1. For example, for a beta distribution with
ken counts from the Google Books corpus, with each token             a mean of 0.7, a high concentration implies that most draws
weighted in inverse proportion to how many tokens there are          will be close to 0.7, while a low concentration implies that
for that binomial type, i.e. a type with 1000 tokens will have       roughly 70% of draws will be close to 1 and 30% of draws
each token weighted at 1/1000. In this way, we preserve the          will be close to 0. When we treat the output of the beta dis-
gradient information about ordering preferences (via the di-         tribution as a predicted binomial preference, a high concen-
versity of outcomes among tokens) while still weighting each         tration corresponds to a pressure to maintain variation while
type equally. The constraints described above are used as pre-       a low concentration corresponds to a pressure to regularize.
dictors. Outcomes are coded as whether or not the binomial              In order to incorporate the beta distribution into our model
token is in alphabetical order.                                      of binomial preferences, we combine the logistic regression
   For this and all future models, predictions are generated for     and the beta distribution in a hierarchical Bayesian model
all training items using 20-fold cross validation. Results for       (Gelman et al., 2013), as shown in Figure 3. For each item,
all models can be seen in Figure 2. While the logistic regres-       the model determines a mean µ via standard logistic regres-
sion model does a reasonable job of predicting preferences           sion, using the same predictors as before. The model also
for individual items, it does not capture the multimodal dis-        fits a concentration parameter ν. These two parameters deter-
tribution of preference strengths seen in the corpus data. We        mine a beta distribution from which the binomial preference π
proceed to consider models in which item-specific idiosyn-           is draw. Observed data is drawn from a binomial distribution
crasies are modeled explicitly.                                      with parameter π.
Mixed-effects regression                                                We fit this model using the rjags package in R (Plum-
                                                                     mer, 2003). After a burn-in period of 2000 iterations, we
By far the most common method in language modeling for               run for 2000 more iterations sampling every 20 iterations. In
accounting for item-specific idiosyncrasies is mixed-effects         order to predict novel data, we fix the point estimates for the
regression models (Jaeger, 2008). Formally, this model as-           regression coefficients βˆ and the concentration parameter ν.
sumes that idiosyncratic preferences are distributed normally        We then sample 1000 draws of π for each item. As with
(in logit space) around the point estimate given by the fixed-       the mixed-effects model, the histogram in Figure 2(c) shows
effects components of the regression model.                          the full sample distribution, while point estimates (the sample
   We train a mixed-effect logistic regression on binomial to-       median) are used to calculate L1 error and R2 (Figure 2(b)).
kens using the lme4 package in R. We use as predictors the
                                                                        This model performs better on L1 and R2 than the mixed-
same fixed effects as before, plus a random intercept for bino-
                                                                     effects model, but still worse than the initial logistic regres-
mial types. As described above, the fitted model now predicts
                                                                     sion. The predicted histogram shows hints of the multimodal
a distribution, rather than a single point estimate, for a novel
                                                                     distribution seen in corpus data, but is overall too flat.
binomial. To make predictions for our (cross-validated) novel
data, we sampled 1000 times from this distribution for each
                                                                         1 An alternative method of prediction for novel items would be to
item. The histogram in Figure 2(c) shows the full sample dis-
                                                                     take the median random intercept in logit space, i.e. to set all random
tribution across all items. In order to generate point estimate      intercepts to 0. This method yields results that are very similar to—
predictions for computing L1 and R2 (shown in Figure 2(b)),          but all-around slightly worse than—the original regression model.
                                                                 1651

     θβ                                                                                                   θβ                                           θα,β
     β̂      X̂                θν                                                                         β̂      X̂                     Mn            α, β
                                     N: # unordered binomial types
                                     Mn : Frequency of binomial n
             µ                 ν                                                                                   µ                           ν
                                     X̂: Predictors (i.e. generative
                                     constraints)
                                     ˆ Regression coefficients
                                     β:
             π                                                                                                          π
                                     θ: Uninformative priors
                                              1
                                     µ=            ˆ
             D                            1 + e−X̂·β                                                                    D
                                     ν ∼ exp(θν )                                                                                   Mn
                  Mn
                                     π ∼ Beta(µ, ν)                                                                                                N
                       N
                                     D ∼ Binomial(π, Mn )
                                                                          Figure 4: Hierarchical Bayesian beta-binomial model with
Figure 3: Our initial hierarchical Bayesian beta-binomial                 variable concentration parameter
model. The set of nodes culiminating in µ implements a stan-
dard logistic regression. The output of this regression deter-
                                                                               Sparsity parameter ν                     Sparsity parameter ν
mines the mean of the beta distribution (with ν determining
the concentration) from which π and finally the observed data
                                                                                 2      4   6    8                        2      4   6    8
itself is drawn.
Beta-binomial with a variable concentration
parameter                                                                               0
                                                                                                      -18 -16 -14 -12
                                                                                                                                 0
                                                                                                                                                   -18 -16 -14 -12
                                                                                                        Log frequency                                Log frequency
A crucial fact that we have not taken into account in previ-
ous models is the role of frequent reuse in shaping expres-               Figure 5: Concentration parameter ν as a function of fre-
sions’ preferences. In particular, the degree to which an ex-             quency with 95% confidence intervals. (Left) Parameteriza-
pression takes on a polarized preference may depend upon its              tion given in Eq. 1. (Right) Alternate parameterization with
frequency. We build upon the beta-binomial model in the pre-              cubic splines, for comparison.
vious section by parameterizing the concentration parameter
by the frequency of the (unordered) binomial expression:                  This model also indicates that more frequent binomials are
                           ν = exp(α + β · log(Mn ))               (1)    on average more polarized.
                                                                             This modeling finding supports Morgan and Levy (2015)’s
where Mn is the total number of occurrences of binomial n in              claim that generative knowledge and item-specific direct ex-
both orders. Training and testing of the model are identical to           perience trade off gradiently in language processing, such that
above.                                                                    processing of novel or infrequent items relies upon generative
   We find that β = −0.26 is significantly different from 0               knowledge, with reliance upon item-specific experience in-
(t99 = −94; p < 2.2 × 10−16 ), indicating that the concentra-             creasing with increasing frequency of exposure. Morgan and
tion parameter changes significantly as a function of fre-                Levy support this claim with behavioral data, showing that
quency: less frequent expressions have more dense distribu-               empirical preferences for binomials which are completely
tions while more frequent expressions have more polarized                 novel depend on generative constraints while preferences for
distributions, as shown in Figure 5. We find that this model              frequent expressions depend primarily on frequency of expe-
generates the best predictions of all our models, produc-                 rience with each order. Our modeling results augment this ar-
ing a marginally significant improvement in both L1 (t593 =               gument by demonstrating that this trade-off is likewise neces-
1.86; p = 0.06) and R2 (by fold t19 = 1.76; p = 0.09) relative            sary in order to predict the language-wide distribution of pref-
to the initial logistic regression. Moreover, it correctly pre-           erence strengths. In particular, we can conceive of generative
dicts the multimodal distribution of expression preferences.              knowledge as providing a prior for ordering preferences. Un-
                                                                          der our final model, the logistic regression component serves
                                   Discussion                             an estimate of generative knowledge, which generates pref-
Overall, we found that all models made approximately simi-                erences clustered unimodally around 0.5. The amount of di-
larly good best-guess predictions for binomials they weren’t              rect experience one has with an expression then modulates
trained on, but the frequency-sensitive beta-binomial model               whether it conforms to this prior or whether it deviates. Items
was clearly superior in predicting the language-wide distribu-            with low frequency have a high concentration: they maintain
tion of idiosyncratic binomial-specific ordering preferences.             their variability and continue to contribute to the mode around
                                                                       1652

0.5. Items with high frequency have a low concentration: they               A Probabilistic Analysis of English Binomials. Lan-
are more likely to regularize and contribute to the modes at 0              guage, 82(2), 233–278.
and 1. Crucially, the inclusion of expression frequency as a         Bresnan, J., Cueni, A., Nikitina, T., & Baayen, R. H. (2007).
predictor of the concentration of the beta distribution is nec-             Predicting the dative alternation. Cognitive foundations
essary in order to achieve this effect in the model, demon-                 of interpretation, 69–94.
strating that expressions are indeed relying differentially on       The CMU Pronouncing Dictionary . (2014). Carnegie Mellon
generative knowledge versus direct experience depending on                  University.
their frequency.                                                     Cooper, W. E., & Ross, J. R. (1975). World Order. In
   This finding fits with previous models of cultural transmis-             R. E. Grossman, L. J. San, & T. J. Vance (Eds.), Pa-
sion in which, in general, preferences gravitate towards the                pers from the parasession on functionalism (pp. 63–
prior (Griffiths & Kalish, 2005), but with sufficient expo-                 111). Chicago: Chicago Linguistics Society.
sure, exceptions can be learned (e.g. irregular verbs; Lieber-       Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Ve-
man, Michel, Jackson, Tang, & Nowak, 2007). However,                        htari, A., & Rubin, D. B. (2013). Bayesian Data Anal-
this raises a question which is not answered by our or oth-                 ysis, Third Edition. CRC Press.
ers’ models: why don’t all expressions converge to their prior       Griffiths, T. L., & Kalish, M. L. (2005, May). A Bayesian
preferences eventually? We present two possibilities.                       view of language evolution by iterated learning. Pro-
   One possibility is that people’s probabilistic transmission              ceedings of the 27th annual conference of the cognitive
behavior differs at different frequencies. Convergence to the               science society, 827–832.
prior relies upon probability matching: people must repro-           Hudson Kam, C. L., & Newport, E. L. (2009). Getting it right
duce variants in approximately the proportion in which they                 by getting it wrong: When learners change languages.
have encountered them. However, this is not the only possi-                 Cognitive Psychology, 59(1), 30–66.
ble behavior. Another possibility is that people preferentially      Jaeger, T. F. (2008). Categorical data analysis: Away from
reproduce the most frequent variant they have encountered,                  ANOVAs (transformation or not) and towards logit
to the exclusion of all other variants, a process known as reg-             mixed models. Journal of Memory and Language,
ularizing. If people’s tendency to probability match versus                 59(4), 434–446.
regularize is dependent on the frequency of the expression           Jaeger, T. F. (2010). Redundancy and reduction: Speakers
in question (with more regularizing at high frequencies), this              manage syntactic information density. Cognitive Psy-
could produce the pattern of more polarized expressions at                  chology, 61(1), 23–62.
higher frequencies seen in our data. Another possibility is that     Levy, R., & Galen, A. (2006). Tregex and Tsurgeon. 5th
there is some other unspecified exogenous source of pressure                International Conference on Language Resources and
towards regularization, as for instance seems to be the case in             Evaluation (LREC).
child language acquisition (Hudson Kam & Newport, 2009).             Lieberman, E., Michel, J.-B., Jackson, J., Tang, T., & Nowak,
This pressure might be weak enough that it is overwhelmed                   M. A. (2007). Quantifying the evolutionary dynamics
by convergence towards the prior at lower frequencies, but                  of language. Nature, 449(7163), 713–716.
can be maintained for items with high enough frequencies to          Lin, Y., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman,
have sufficient exposure to deviate from the prior. Further                 W., & Petrov, S. (2012). Syntactic Annotations for the
work is necessary to disentangle these explanations.                        Google Books Ngram Corpus. Proceedings of the 50th
   In addition to contributing to our understanding of bino-                Annual Meeting of the Association for Computational
mial expression processing, we have demonstrated the value                  Linguistics, 169–174.
of modeling the distribution of idiosyncratic preferences in         Marcus, M., Santorini, B., Marcinkiewicz, M. A., & Taylor,
two ways. First, it has improved our ability to predict pref-               A. (1999). Treebank-3. Linguistic Data Consortium.
erences for novel items, by better differentiating the rule-         McDonald, J., Bock, K., & Kelly, M. (1993). Word and
following training data from the exceptions. Second, this                   world order: Semantic, phonological, and metrical de-
model turns an observation about language-wide structure                    terminants of serial position. Cognitive Psychology, 25,
(the multimodal distribution of preferences) into a constraint              188–230.
on our theory of the cognitive representation and processing         Morgan, E., & Levy, R. (2015). Abstract knowledge ver-
of language (more polarization at higher frequencies).                      sus direct experience in processing of binomial expres-
                                                                            sions. Manuscript submitted for publication.
                     Acknowledgments                                 Plummer, M. (2003). JAGS: A program for analysis of
We gratefully acknowledge support from research grants NSF                  Bayesian graphical models using Gibbs sampling.
0953870 and NICHD R01HD065829 and fellowships from                   Siyanova-Chanturia, A., Conklin, K., & van Heuven, W. J. B.
the Alfred P. Sloan Foundation and the Center for Advanced                  (2011). Seeing a phrase “time and again” matters: The
Study in the Behavioral Sciences to Roger Levy.                             role of phrasal frequency in the processing of multi-
                                                                            word sequences. Journal of Experimental Psychology:
                         References                                         Learning, Memory, and Cognition, 37(3), 776–784.
Benor, S., & Levy, R. (2006). The Chicken or the Egg?
                                                                 1653

                    Intercept                                      ●
                                                                   ●
                                                                                                                                                                     1.0
                      Length                                                       ●       ●
                                                                                                                                                      Model proportion
                      Stress                                                                                       ●   ●
                                                                                                                                                                     0.8                                             2.5
                      Lapse                            ●
                                                       ●
                                                                                                                                                                                                                     2.0
                                                                                                                                                                     0.6
                                                                                                                                                                                                               Density
                        Freq                                               ●       ●
                                                                                                                                                                                                                     1.5
                                                                                                                                                                     0.4
                       Form                                                                    ●   ●
                                                                                                                                                                                                                     1.0
 Logistic            Percept                                                                           ●   ●
                                                                                                                                                                     0.2
                                                                                                                                                                                                                     0.5
 regression           Power                                                                                            ●   ●
                                                                                                                                                                     0.0                                             0.0
                        Icon                                                                                                   ●
                                                                                                                               ●
                                                                                                                                                                           0.0   0.2   0.4   0.6   0.8   1.0                                       0.0   0.2   0.4   0.6   0.8   1.0
                                                                                                                                                                                 Corpus proportion                                                 Proportion of occurrences
                     Culture                                                                                   ●   ●
                                                                                                                                                                                                                                                     in alphabetical order
                     Intense                                               ●
                                                                                                                                                      L1= 0.169 (0.006)
                                                                               ●
                                                                   0                                       1
                                                                                           Parameter estimate                                         R2 = 0.368 (0.021)
                    Intercept                  ●   ●
                                                                                                                                                                     1.0
                      Length                           ●   ●
                                                                                                                                                      Model proportion
                      Stress                                                           ●                                                                             0.8
                                                                                                                                                                                                                     1.2
                                                                                               ●
                      Lapse        ●       ●
                                                                                                                                                                     0.6
                                                                                                                                                                                                               Density
                        Freq                           ●   ●
                                                                                                                                                                                                                     0.8
                                                                                                                                                                     0.4
                       Form                                                ●   ●
 Mixed-              Percept                                               ●       ●
                                                                                                                                                                     0.2
                                                                                                                                                                                                                     0.4
 effects              Power                                                                                    ●
                                                                                                               ●
                                                                                                                                                                     0.0                                             0.0
 regression             Icon                                                                                                         ●
                                                                                                                                     ●
                                                                                                                                                                           0.0   0.2   0.4   0.6   0.8   1.0                                       0.0   0.2   0.4   0.6   0.8   1.0
                                                                                                                                                                                 Corpus proportion                                                 Proportion of occurrences
                     Culture                                                                           ●
 with random
                                                                                                       ●
                                                                                                                                                                                                                                                     in alphabetical order
                     Intense                           ●
                                                                                                                                                      L1= 0.173 (0.006)
                                                           ●
 intercept
                            −0.5           0.0                                     0.5     1.0      1.5                            2.0          2.5
                                                                                    Parameter estimate                                                R2 = 0.355 (0.022)
                    Intercept                  ●
                                               ●
                                                                                                                                                                     1.0
                      Length                           ●
                                                       ●
                                                                                                                                                      Model proportion
                      Stress                                               ●       ●
                                                                                                                                                                     0.8                                             1.0
                      Lapse        ●
                                   ●
                                                                                                                                                                                                                     0.8
                                                                                                                                                                     0.6
                                                                                                                                                                                                               Density
                        Freq                           ●
                                                       ●
                                                                                                                                                                                                                     0.6
                                                                                                                                                                     0.4
                       Form                                        ●   ●
                                                                                                                                                                                                                     0.4
 Beta-               Percept                                               ●
                                                                           ●
                                                                                                                                                                     0.2
                                                                                                                                                                                                                     0.2
 binomial             Power                                                                                    ●
                                                                                                               ●
                                                                                                                                                                     0.0                                             0.0
 model                  Icon                                                                                                         ●
                                                                                                                                     ●
                                                                                                                                                                           0.0   0.2   0.4   0.6   0.8   1.0                                       0.0   0.2   0.4   0.6   0.8   1.0
                                                                                                                                                                                 Corpus proportion                                                 Proportion of occurrences
                     Culture                                                                           ●
                                                                                                       ●
                                                                                                                                                                                                                                                     in alphabetical order
                     Intense                           ●
                                                                                                                                                      L1= 0.170 (0.003)
                                                           ●
                                   0.0                                                       0.5          1.0                            1.5
                                                                                           Parameter estimate                                         R2 = 0.367 (0.020)
                    Intercept                  ●   ●
                                                                                                                                                                     1.0
                      Length                               ●   ●
                                                                                                                                                                                                                     0.0 0.2 0.4 0.6 0.8 1.0 1.2
                                                                                                                                                      Model proportion
                      Stress                                                   ●       ●
                                                                                                                                                                     0.8
                      Lapse        ●   ●
                                                                                                                                                                     0.6
                                                                                                                                                                                                               Density
                        Freq                           ●       ●
                                                                                                                                                                     0.4
                       Form                                        ●
                                                                   ●
 Beta-               Percept                                           ●   ●
                                                                                                                                                                     0.2
 binomial             Power                                                    ●       ●
                                                                                                                                                                     0.0
 model with             Icon                                                                                                        ●●
                                                                                                                                                                           0.0   0.2   0.4   0.6   0.8   1.0                                       0.0   0.2   0.4   0.6   0.8   1.0
                                                                                                                                                                                 Corpus proportion                                                 Proportion of occurrences
                     Culture                                                                           ●
 variable con-
                                                                                                           ●
                                                                                                                                                                                                                                                     in alphabetical order
                     Intense                   ●
                                                                                                                                                      L1= 0.166 (0.003)
                                                   ●
 centration
                                       0.0                                                     0.5           1.0                               1.5
                                                                                           Parameter estimate                                         R2 = 0.381 (0.021)
                                                                                                       (a)                                                                             (b)                                                                     (c)
Figure 2: For each of our four models, we display: (a) Parameter estimates for the logistic regression component. Dots show
point estimates with bars indicating standard errors. (b) Predictions for each item, as well as mean by-type L1 error and R2 with
by-fold standard errors. (c) Language-wide predicted distribution of preference strengths.
                                                                                                                                         1654

