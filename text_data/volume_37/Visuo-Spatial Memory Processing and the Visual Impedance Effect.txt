              Visuo-Spatial Memory Processing and the Visual Impedance Effect
                                         Rebecca Albrecht (albrechr@cs.uni-freiburg.de)
                                            Center for Cognitive Science, University of Freiburg
                                    Holger Schultheis (schulth@informatik.uni-bremen.de)
                                                  Cognitive Systems, University of Bremen
                                                      Wai-Tat Fu (wfu@illinois.edu)
                            Department of Computer Science, University of Illinois at Urbana-Champaign
                              Abstract                                  Barkowsky, 2013, for an in-depth discussion of this point)
                                                                        remains largely to be determined.
   Models of spatial reasoning often assume distinct visual and
   spatial representations. In particular, the visual impedance ef-        In this paper we argue that previous research has not suf-
   fect – slower response time when more visual details are rep-        ficiently considered the role of memory when studying and
   resented in three-term series spatial reasoning tasks – has been
   taken as evidence for the distinctive roles of visual and spa-       comparing visual and spatial representations. We demon-
   tial representations. In this paper, we show that a memory           strate our case by presenting a memory model that explains
   model of spreading activation based on the ACT-R architec-           the visual impedance effect without the assumption that vi-
   ture can explain the visual impedance effect without the as-
   sumption of distinct visual and spatial representations. Using       sual and spatial representations have distinctive functional
   the same memory representation, varying levels of visual fea-        roles in spatial reasoning. Instead, the model assumes that vi-
   tures associated with an object are represented in the model.        sual and spatial information can both be represented similarly
   The visual impedance effect is explained by the spreading ac-
   tivation mechanism of ACT-R. The model not only provides             as memory items. However, the model predicts that some-
   a more parsimonious explanation to the visual impedance ef-          times additional visual details may slow down the mainte-
   fect, but also leads to testable predictions of a wide range of      nance of the memory representations of the informaton. This
   memory effects in spatial reasoning.
                                                                        is different from the argument by Knauff and Johnson-Laird
   Keywords: Visual impedance, memory processing, scalable              Knauff and Johnson-Laird (2002), who argued that visual
   representation, spreading activation, ACT-R, relational reason-
   ing, mental model theory.                                            representations of information may slow down the reason-
                                                                        ing process. As a result, contrary to their arguments Knauff
                          Introduction                                  and Johnson-Laird (2002), the visual impedance effect does
Processing visual and spatial information is among the most             not provide any support to the claim that visual and spatial
crucial human abilities, because it permeates virtually every-          relations are represented distinctively, nor does it imply that
thing we do (imagine moving in / through the environment                an abstract spatial mental model can lead to a faster reason-
without being able to process the visual and spatial informa-           ing process. Our model not only provides a more parsimo-
tion available from you surroundings).                                  nious explanation to the visual impedance effect, but it also
   In a seminal paper Ungerleider and Mishkin (1982) argue              has the advantages of having more generality and continuity
that in the primate brain two separate pathways are responsi-           with other theories in cognitive sciences.
ble for processing visuo-spatial information: The what path-
                                                                        Visual Impedance
way and the where pathway which are associated with the
temporal and parietal lobe, respectively. The what pathway              Three-term series problems (P. N. Johnson-Laird, 1972) have
mainly processes information related to object identification           played a prominent role in investigating spatial and visual
and recognition (e.g., color), while the where pathway mainly           representations (e.g., Shaver, Pierson, & Lang, 1975; Knauff
processes spatial information (e.g., object location or move-           & Johnson-Laird, 2002; Rauh, Hagen, Kuss, Schlieder,
ment). This distinction has subsequently received additional            & Strube, 2005; Schultheis, Bertel, & Barkowsky, 2014;
support from many behavioural and neuroscientific studies               Schultheis & Barkowsky, 2013; Sima et al., 2013). A three-
(e.g., Milner & Goodale, 2008; Klauer & Zhao, 2004).                    term series problem constitutes a deductive relational reason-
   The existence of these two distinct neural pathways has              ing problem in which the relation between two objects, A and
given rise to the assumption that visuo-spatial information             C, has to be inferred given the relations between objects A
processing in humans draws on two distinct types of men-                and B as well as the relation between B and C. For example,
tal representations: Visual and spatial representations. Al-            given the information that (a) the dog is left of the cat and
though this assumption is shared by the two main theories of            (b) the mouse is left of the dog, participants may be asked to
visuo-spatial information processing, the mental model the-             verify the statement that the mouse is left of the cat. Simi-
ory (P. Johnson-Laird, 1998) and the theory of mental im-               larly, knowing that (a) the dog is dirtier than the cat and (b)
agery (Kosslyn, Thompson, & Ganis, 2006), the nature of                 the mouse is dirtier than the dog, one can verify the statement
the representations and, in particular, the relation between            that the mouse is dirtier than the cat.
the two types of representations (see Sima, Schultheis, &                  Knauff and Johnson-Laird (2002) conducted experiments
                                                                     72

in which they compare participants performance in solving                          relation-type                          more-than
such three-term series for different types of relations. Specif-
ically, Knauff and Johnson-Laird (2002) distinguish between                        o1       p   o2                      o1     p   o2
visual, visuo-spatial, and control relations: Visual relations
are relations that are easy to envisage visually (e.g., dirtier);
visuo-spatial relations are relations that are easy to envisage          A              content      B            hat        left           tie
spatially and visually (e.g., to the left of); control relations
                                                                     (a) Abstract (i.e. not instan-            (b) Instantiated relation with
are relations that are hard to envisage both spatially and vi-       tiated) representation of rela-           relation-type more-than and
sually (e.g., better). The main finding reported by Knauff           tional information.                       property (p) left.
and Johnson-Laird (2002) is that reasoning about visual rela-
tions takes significantly more time than reasoning about ei-         Figure 1: Uniform representation of relational information. A
ther visuo-spatial or control relations. This comparatively          relation consists of a relation-type, two objects (o1 , o2 ), and a
poor performance of reasoning with visual representations            property (p).
has been termed the visual impedance effect.
   The explanation provided for the visual impedance effect          sentation with ACT-R’s spreading activation mechanism. We
assumes that for all types of relations the actual reasoning         illustrate how our model explains the visual impedance ef-
process involves (spatial) mental models (Knauff, Fangmeier,         fect and present an ACT-R implementation and simulation of
Ruff, & Johnson-Laird, 2003). For visuo-spatial relations            our model. We conclude in highlighting the contribution of
and control relations the given information is directly repre-       our modeling work as well as interesting questions for future
sented in such a spatial mental model and, thus, can imme-           work.
diately be used for reasoning. For visual relations, however,
the given information is initially represented by a visual rep-                               Memory Representation
resentation (e.g., a visual mental image) that does not sup-
                                                                     In order to understand the dependency between a represen-
port reasoning. To solve the reasoning problem, an additional
                                                                     tation of relational information in memory on the one hand
step for building a spatial mental model is required (Knauff,
                                                                     and a reasoning process on the other we will first introduce a
2009). Note that this explanation of the visual impedance ef-
                                                                     scalable, abstract representation for relational information. In
fect assumes that the comparatively poor performance with
                                                                     particular, our abstract representation distinguishes between
visual relations is due to problems associated with the rea-
                                                                     the relation in a mathematical sense, i.e., as it is deemed suit-
soning process. Against this background it seems remark-
                                                                     able for reasoning, and the meaning of a relation.
able that all available computational models that formalise
                                                                         To abstractly represent relational information of the type
reasoning with spatial mental models do not explain the vi-
                                                                     employed in three-term series problems, we consider more-
sual impedance effect (Krumnack, Bucher, Nejasmic, Nebel,
                                                                     than relation types. A more-than relation type consists of
& Knauff, 2011; Ragni & Knauff, 2013; Khemlani, Trafton,
                                                                     three different pieces of information, two objects (o1 , o2 ) and
Lotstein, & Johnson-Laird, 2012).
                                                                     a property (p), i.e. more-than(o1 , p, o2 ). An example is de-
   We propose that the visual impedance effect is not a rea-         picted in Figure 1). Intuitively, our representation can be un-
soning effect, but a memory effect. Following Schultheis and         derstood as “object o1 has more of a property p than object
Barkowsky (2011), we assume that spatial and visual repre-           o2 ”. Or more concretely, “the hat is more left than the tie”
sentations are not two distinct, qualitatively different types       for the visuo-spatial relation “left” and “the hat is more dirty
of representations, but that visuo-spatial representations lie       than the tie” for the visual relation “dirty” (cf. Figure 1).
along a continuum that characterizes how specifically amodal             We further assume that for a concrete relational statement
/ spatial or modality-specific / visual a representation is. De-     each of the three arguments is associated with features in
pending on the current task context, representations can flex-       memory that represent the arguments’ meaning. We define
ibly scale along this continuum (i.e., become more or less vi-       the content of an object (or property) as the tuple of fea-
sual) without a need to modify the reasoning processes work-         tures necessary to represent the object o, i.e. content(o) =
ing on the representations. Given such scalable representa-                                                                        p           p
                                                                     ( f1o , . . . , fno ), and property p, i.e. content(p) = ( f1 , . . . , fn ),
tions, we assume that visual relations give rise to more com-        in memory. Figure 2 shows graphically how relational state-
plex representations, because more visual details are repre-         ments and their contents are represented. This representa-
sented. Specifically, due to spreading activation of memory          tion includes an abstract representation suitable for reasoning
items, the additional visual details slower down the access to       (Figure 2, above the red line) and the memory representation
the information that needs to be processed during the later          defined by the content of the relational information (Figure 2,
reasoning process; and, thus, slows down the overall reaction        below the red line).
times when solving the three-term series problem.                        Scalability of this representation is defined in terms of the
   In the following we first introduce a scalable, abstract rep-     number of features involved in representing the relational
resentation for relational information that supports reason-         statement. In particular, the representation can scale to be-
ing. We then present our model that combines this repre-             come more or less visual depending on how much visual fea-
                                                                  73

                                     more-than                                               more-than
                           o1             p             o2                        o1              p               o2
              A                       relation                      B                         relation                           C
             hat                        dirty                      tie                          dirty                          shoe
     f1hat   ...     fnhat      dirty
                               f1        ...      dirty
                                                           f1tie   ...      fntie       dirty
                                                                                       f1                  dirty
                                                                                                                     f1shoe     ... fnshoe
                                                fn                                                        fn
Figure 2: Representation of relational information ’the hat is dirtier than the tie’ and ’the tie is dirtier than the shoe’ in memory
assuming a hierarchical representation of information. Above the red line is a representation which supports reasoning with
relations. Below the red line is the memory representation of the objects and property used to elaborate the content of relational
information. Scalability is defined in terms of the number of features necessary to represent relational information.
tures are associated with the relational statement. For exam-          d which are a part of the current working memory state (i.e.,
ple, the relation “dirtier” may be associated with features like       assigned to a buffer). Formally, the signal strength between
“dirt”, “mud”, “black dots”, etc. and, thus yield a more visual        chunk c and a chunk d is computed as Sd,c = S − ln( f and ).
representation than the relation “to the left of”, which may           The signal strength depends on the number of outgoing con-
only be associated with a single feature “position”.                   nections of chunk d, a concept which has been termed f an of
                                                                       chunk d. The signal strength additionally depends on a global
                      Cognitive Model                                  constant S which has been interpreted psychologically as an
In this section we describe an ACT-R model that explains the           approximation of the declarative memory size. The complete
visual impedance effect. Employing the above described rep-            spreading activation of a chunk c is calculated by 1
resentation, the model explains the effect as a memory phe-
nomenon arising from spreading activation.                                                 sa(c) =            ∑           Sd,c
                                                                                                     d in working memory
ACT-R Spreading Activation
                                                                          The time to retrieve a chunk c from declarative memory
ACT-R realises working memory as a structured set of buffers
                                                                       is defined with respect to the activation of chunk c, in our
(Anderson, 2007). Buffers hold declarative information, so-
                                                                       case RT (c) = a · e−sa(c) , where a is a constant. The higher
called chunks. A chunk is a set of key-value (or slot-value)
                                                                       the activation of a chunk the lower the response time. For
pairs. For example, a chunk representation of the introduced
                                                                       spreading activation a greater fan implies a lower activation
’more-than’ relation has three slots (o1 , p, o2 ) to which values
                                                                       and, thus, a higher response time.
(often also chunks) can be assigned. Behaviour in ACT-R is
produced by the repeated application of production rules that
fit a current working memory state and change the working              Example. It may be helpful to more closely consider
memory state according to their definition. Changes to the             how the spreading activation mechanism explains the visual
working memory come about by requests to modules that are              impedance effect. The visual impedance effect is measured
associated with buffers. Modules process requests by updat-            in the time to verify a given conclusion. When a conclusion
ing the chunks contained in their buffers. This processing is          needs to be verified, information from the first and second
associated with a time cost and, in some cases, has an uncer-          relational statement have already been integrated in a mental
tain outcome.                                                          representation. Depending on assumptions stated in a reason-
    The ACT-R declarative module (sometimes called declar-             ing theory this mental representation may, for example, be a
ative memory) holds all declarative information known to a             mental model (Ragni & Knauff, 2013) or a relational infer-
model such as, for example, the complete representation de-            ence, i.e., a relational statement, (Braine & O’Brien, 1998).
picted in Figure 2. The time it takes the declarative module to        In either case, this mental representation needs to be retrieved
process requests depends on the activation values assigned to          from declarative memory in order to verify the conclusion. In
candidate chunks. While a number of mechanisms can influ-                  1 For the sake of representation simplicity we assume that spread-
ence the chunks’ activations, we focus our analysis to spread-         ing activation is enabled for every buffer and that all buffers are as-
ing activation.                                                        signed the same weight, which sum up to a total of 1. Addition-
                                                                       ally, in our scenario the working memory holds the same number of
    The spreading activation of a chunk c in ACT-R is defined          chunks in every request. Thus, we leave out the weight of the ACT-R
in terms of a signal strength S between chunk c and all chunks         spreading activation equation.
                                                                    74

the following, we will use a mental model as the mental repre-                          f1h
                                                                                                                            f1h
sentation. The argument for a relational inference is analogue                 hat     ...
                                                                                                                           ...
                                                                                                                   hat
and the simulation results for the retrieval time only differ by                        fnh
                                                                                                                            fnh
a constant factor due to the different representation.
    Consider our scalable representation defined for relational                         f1h     model:
                                                                                                                                    model:
                                                                                                pos1 : hat
information, i.e., more-than(o1 , p, o2 ). When a mental model               dirtier   ...      pos2 : tie
                                                                                                                                    pos1 : hat
                                                                                                                 left of    f1      pos2 : tie
is requested from the declarative module the conclusion is                              fnh
                                                                                                pos3 : shoe
                                                                                                                                    pos3 : shoe
                                                                                                axis: dirtier
represented in the model’s working memory (e.g., the goal                                                                           axis: le f to f
buffer). Thus, o1 , o2 and p are potential sources for spread-                          f1h
                                                                                                                            f1h
ing activation (cp. Figure 3). The signal strength between the                shoe     ...
                                                                                                                  shoe     ...
mental model chunk (model) and the content of a relation p
                                                                                        fnh
is then S p,model = S − ln( f an(p)) + r. The fan is influenced                                                             fnh
by the number of features associated with the relation, i.e.              (a) Fan for chunks in work-
 f an(p) = content(p), and the constant r approximating the               ing memory for conclusion re-       (b) Fan for chunks in work-
                                                                          lation ”dirtier”. The content       ing memory for conclusion re-
fan associated with reasoning representation (e.g. a second                                                   lation ”to the left of”. The con-
                                                                          of relation ”dirtier” needs to be   tent of relation ”to the left of”
mental model in declarative memory). The signal strength                  represented by more than one
can thus be calculated as S p,model = S − ln(content(p)). Con-            feature (e.g., dirty, mud, black    can be represented by a single
                                                                          spots, etc.                         feature (e.g., the position).
sequently, the more features are necessary to represent the
content of a relation, the higher the retrieval time of a target
                                                                          Figure 3: Example illustration of memory representations for
chunk (due to the higher fan).
                                                                          visual (a) and visual-spatial relations. Chunks active in the
    Now consider the concrete relations introduced for the vi-
                                                                          working memory are ”hat”, ”shoe”, and ”dirtier” (3a), and
sual impedance effect, that is, visual relations like “dirtier”,
                                                                          ”to the left of” (3b). The target chunk is a mental model that
visuo-spatial relations like “to the left of” and control rela-
                                                                          needs to be retrieved in order to verify the conclusion. Due to
tions like “better”. If we assume that the visuo-spatial relation
                                                                          the higher fan the target chunk (here a mental model) receives
“to the left of” can be represented using one feature (e.g., the
                                                                          more spreading activation for the conclusion relation ”to the
position), the fan is f an(left of) = content(left of) = 1 (Fig-
                                                                          left of” than for the conclusion relation ”dirtier”. Thus, the
ure 3b). For visual relations like “dirtier”, on the other hand,
                                                                          retrieval time is higher for the conclusion relation ”dirtier”.
more features need to be represented (e.g., dirt, mud, etc.)
Therefore, the fan associated with “dirtier” is higher than
the fan associated with “to the left-of”, i.e. f an(dirtier) >            Brüssow, 2010), which assumes that exactly one retrieval is
 f an(left of), and the signal strength between chunks “dirtier”          necessary to verify a given conclusion.
and model is lower than between “to the left of” and model,                  For a prototypical task such as “the hat is dirtier than the
i.e. Sdirtier,model < Sleft of,model (Figure 3a). Assuming that the       tie”, the “tie is dirtier than the shoe”, “is the hat dirtier than
features representing objects o1 and o2 are the same in both              the shoe?” we define the mental representation as either a
cases2 , the spreading activation that the mental model chunk             mental model chunk or a relational inference chunk which is
receives from the visual relation “dirtier” is given by                   stored in declarative memory, e.g.,
          sa(model) = S − ln(k) + S − ln(n) + S − ln(l)                   • (r1 ISA model pos1 hat pos2 tie pos3 shoe rel dirtier)
                          | {z } | {z } | {z }
                            Shat,model  Sdirtier,model Sshoe,model        • (r2 ISA inference o1 hat o2 shoe rel dirtier)
                                                                             Additionally we represent the features associated with ob-
and from the spatial relation “to the left of” is given by
                                                                          jects and the property of a relation as content chunks, e.g.,
          sa(model) = S − ln(k) + S − ln(1) + S − ln(l)                   • (l1 ISA content id left-of feature l1)
                          | {z } | {z } | {z }
                            Shat,model  Sleft of,model Sshoe,model        • (d1 ISA content id dirtier feature dd)
Obviously, the mental model chunk receives more spreading                    Source of spreading activation is a representation of the
activation for visuo-spatial relations. Therefore, the retrieval          conclusion in the goal buffer, e.g.,
time is lower (see Figure 4).                                                (rel1 ISA more-than o1 hat p dirtier o2 shoe)
                                                                             We define one production rule which requests a mental
ACT-R implementation. Our ACT-R model is based on                         representation chunk (model or inference) from declarative
the ACT-R implementation of PRISM (Ragni, Fangmeier, &                    memory. This request does not specify any restrictions on
                                                                          slot values other than the type being either a mental model
    2 Instead of keeping the features of the objects fixed and varying    or an inference. The time it takes the declarative module to
the features of the relation, it would also be possible to represent      answer this request depends on the fan associated with the
the objects by more of less features depending on the relation type.
This would not impact the explanatory power of our model w.r.t the        objects and the property of the relation, that is, the number of
visual impedance effect.                                                  content chunks associated with the objects and the property.
                                                                       75

                                                                           Our work shows that combining the concept of scalable
                                                                        representation structures with spreading activation provides
                                                               ●
                                                                        a more parsimonious explanation to the visual impedance
                      560
                                                                        effect, as the proposed model does not assume distinct vi-
                                                           ●            sual and spatial representations or a specific reasoning pro-
                      550
Retrieval Time (ms)
                                                                        cess. The current model uses memory representation of ob-
                      540
                                              ●                         jects and memory processes that have been used to explain a
                                                                        wide range of memory effects (e.g., in previous ACT-R mod-
                      530                                               els of memory tasks). The current model therefore has the
                                ●
                                                                        potential to lead to a wide range of testable predictions on
                      520
                                                                        the effects of memory in spatial reasoning, such as effects of
                      510   ●                                           individual differences in working memory capacity, interfer-
                            1   2             3            4   5
                                                                        ence effects, or effects of memory decay. In addition to the
                                                                        original visual impedance effect (Knauff & Johnson-Laird,
                                      Number of Features
                                                                        2002), our modeling work also explains moderations of the
                                                                        effect that have been reported. If the visual impedance ef-
Figure 4: Retrieval time for a mental representation chunk              fect is due to memory processing as assumed in the proposed
(here mental model chunk) when a conclusion needs to be                 model, it should scale with the model’s ability and necessity
verified. The response time increases independently of a con-           to represent specific features in order to maintain a represen-
crete task or reasoning theory with the number of visual fea-           tation suitable for reasoning. Consistent with our model, re-
tures associated with the objects and the relation.                     search shows that blind people show no visual impedance ef-
                                                                        fect (Knauff, 2009)— perhaps because they are less inclined
                                                                        to represent objects with visual features, or the number of
   All ACT-R parameters are set to their default values. We             visual features tend to be lower for blind people. Further-
approximate parameter S by the logarithm of the average size            more, people who have a higher tendency to visualize object
of the model’s declarative memory (i.e. S = 3). Figure 4                details show a stronger visual impedance effect (Castañeda
shows how the retrieval time increases with the number of               & Knauff, 2013), because they tend to represent more visual
features associated with the content of a relation.                     features as other groups.
   Accordingly, our model accounts for the visual impedance                The proposed cognitive model investigates the impact of
effect by predicting that verification of conclusions for three-        the memory representations of visual features on spatial rea-
term series problems involving visual relations such as “dirt-          soning. The model, however, does not make any assump-
ier” take more time than for problems involving visuo-spatial           tion on the reasoning process, as the reasoning tasks are the
relations. Interestingly, our model predicts that the visual            same across the different conditions in the studies on vi-
impedance effect should not be restricted to the use of visual          sual impedance (Knauff & Johnson-Laird, 2002). In other
relations, but should arise whenever the reasoner is inclined           words, the explanation of the visual impedance effect by our
to associate multiple (visual) features with a relational state-        model is independent of the reasoning process. For exam-
ment. As discussed below, existing evidence supports this               ple, if we apply the reasoning process in the PRISM model
prediction.                                                             (Ragni et al., 2010), in which the main difference in level
                                                                        of difficulty in spatial reasoning tasks is characterized by the
                                    Conclusion                          number of focus operations on the represented objects, we
                                                                        will have the same number of focus operations in each con-
Knauff and Johnson-Laird argued that the reason why items
                                                                        dition, and the only difference is how quickly the model can
that could easily be envisaged would lead to slower response
                                                                        asscess the objects represented in memory as the focus op-
times was that visual representations of irrelevant features
                                                                        erations are applied. However, we should point out that the
slowed down the reasoning process. We provided an alter-
                                                                        PRISM model by itself does not seem to be able to explain
native explanation: the easily envisaged items took longer to
                                                                        the visual impedance effect. On the other hand, our model
be accessed in memory because they were associated with
                                                                        can be used with other reasoning theories (e.g., (Krumnack
more visual features, which slowed down their access time
                                                                        et al., 2011; Braine & O’Brien, 1998)) to explain the visual
as predicted by the spreading activation mechanism. Con-
                                                                        impedance effects. In other words, our model suggests that
trary to the argument by Knauff and Johnson-Laird, we did
                                                                        the visual impedance effect can be explained by memory pro-
not find that the visual impedance effect provided any sup-
                                                                        cesses rather than reasoning processes.
port to the claim that easily envisaged items were represented
by a visual representation that was funcational different from             The goal of this paper is to show that the visual impedance
a (spatial) mental model, nor did the results support the claim         effect can be explained without committing to a unique spa-
that “visual imagery as the medium for reasoning would be               tial representation that is distinct from visual representation.
implausible” (Knauff & Johnson-Laird, 2002).                            This is consistent with the idea that the long debate about the
                                                                   76

role of visual imagery in spatial reasoning can be resolved by         Johnson-Laird, P. N. (1972). The three-term series problem.
considering the visuo-spatial representation as a continuum              Cognition, 1, 57–82.
Schultheis and Barkowsky (2011), with varying levels of vi-            Khemlani, S., Trafton, J. G., Lotstein, M., & Johnson-Laird,
sual (and spatial) features represented in memory. Another               P. (2012). A process model of immediate inferences. In
advantage of this approach is that by utilizing memory repre-            Proceedings of the 11th international conference on cogni-
sentations and mechanisms, the model is more readily com-                tive modeling (p. 151).
pared and tested against a wide range of cognitive phenomena           Klauer, K. C., & Zhao, Z. (2004). Double dissociations in
beyond spatial reasoning. We believe that our modeling ap-               visual and spatial short-term memory. Journal of Experi-
proach and results constitute an important first step towards            mental Psychology: General, 133(3), 355–381.
studying the impact of memory processing in human reason-              Knauff, M. (2009). A neuro-cognitive theory of deductive
ing and the nature of spatial and visual representations. While          relational reasoning with mental models and visual images.
previous theories and studies mostly restricted considerations           Spatial Cognition & Computation, 9(2), 109–137.
to the reasoning process or the representation, we define a            Knauff, M., Fangmeier, T., Ruff, C. C., & Johnson-Laird,
link between these concepts. As a result, our approach also              P. N. (2003). Reasoning, models, and images: Behav-
highlights promising avenues for future work, both empirical             ioral measures and cortical activity. Journal of Cognitive
and computational, to shed more light on aspects of reasoning            Neuroscience, 15(4), 559-573.
processes and representations.                                         Knauff, M., & Johnson-Laird, P. (2002). Visual imagery can
   Empirically, we propose experiments that explicitly con-              impede reasoning. Memory & Cognition, 30(3), 363–371.
trol the number of represented features, both of objects and           Kosslyn, S. M., Thompson, W. L., & Ganis, G. (2006). The
relations. Such an experiment would yield valuable results               case for mental imagery. New York: OUP.
on the effect size with respect to the number of features nec-         Krumnack, A., Bucher, L., Nejasmic, J., Nebel, B., & Knauff,
essary to represent concepts. A possible approach is using a             M. (2011). A model for relational reasoning as verbal
high and low similarity conditions similar to Folk and Luce              reasoning. Cognitive Systems Research, 11, 377-392.
(1987), that is, where more or less features need to be rep-           Milner, A., & Goodale, M. (2008). Two visual systems re-
resented in order to draw conclusions (e.g., “the red hat is             viewed. Neuropsychologia, 46(3), 774 - 785.
dirtier than the tie” vs. “the red hat is dirtier than the red tie”    Ragni, M., Fangmeier, T., & Brüssow, S. (2010). Deductive
vs. “the red hat is dirtier than the blue tie”).                         spatial reasoning: From neurological evidence to a cogni-
   Computationally, assuming visual impedance is in fact an              tive model. In Proceedings of the 10th international con-
effect in memory processing our results can be used to further           ference on cognitive modeling (pp. 193–198).
examine reasoning theories. The ACT-R implementation of                Ragni, M., & Knauff, M. (2013). A theory and a compu-
the PRISM model represents the complete mental model in                  tational model of spatial reasoning with preferred mental
one chunk and approximates focus operations as a constant                models. Psychological review, 120(3), 561.
factor. However, according to the ACT-R theory information             Rauh, R., Hagen, C., Kuss, T., Schlieder, C., & Strube, G.
is usually represented as linked lists. Thus, if a mental model          (2005). Preferred and alternative mental models in spatial
was represented as a linked list a focus operation would in fact         reasoning. Spatial Cognition & Computation, 5(2), 239–
be a request to declarative memory. In this case, our memory             269.
model would predict a linear increase in the response time for         Schultheis, H., & Barkowsky, T. (2011). Casimir: an archi-
visuo-spatial relations with the number of focus operations.             tecture for mental spatial knowledge processing. Topics in
                                                                         Cognitive Science, 3(4), 778–795.
                          References                                   Schultheis, H., & Barkowsky, T. (2013). Variable stability
                                                                         of preferences in spatial reasoning. Cognitive Processing,
Anderson, J. R. (2007). How can the human mind occur in                  14(2), 209 - 211.
   the physical universe? Oxford University Press.                     Schultheis, H., Bertel, S., & Barkowsky, T. (2014). Modeling
Braine, M. D., & O’Brien, D. P. (1998). Mental logic. Psy-               mental spatial reasoning about cardinal directions. Cogni-
   chology Press.                                                        tive Science, 38(8), 1521–1561.
Castañeda, L. E. G., & Knauff, M. (2013). Individual dif-             Shaver, P., Pierson, L., & Lang, S. (1975). Converging evi-
   ferences, imagery and the visual impedance effect. In                 dence for the functional significance of imagery in problem
   M. Knauff, M. Pauen, N. Sebanz, & I. Wachsmuth (Eds.),                solving. Cognition, 3(4), 359 - 375.
   Proceedings of the 35rd annual conference of the cognitive          Sima, J. F., Schultheis, H., & Barkowsky, T. (2013). Dif-
   science society. Austin, TX: Cognitive Science Society.               ferences between spatial and visual mental representations.
Folk, M. D., & Luce, R. D. (1987). Effects of stimulus                   Frontiers in psychology, 4.
   complexity on mental rotation rate of polygons. Journal             Ungerleider, L., & Mishkin, M. (1982). Two cortical systems.
   of Experimental Psychology: Human Perception & Perfor-                Analysis of Visual Behavior, 549–586.
   mance, 13, 395 - 404.
Johnson-Laird, P. (1998). Imagery, visualization, and think-
   ing. Perception and cognition at centurys end, 441–467.
                                                                    77

