         A Bayesian Framework for Learning Words From Multiword Utterances
                                           Stephan C. Meylan (smeylan@berkeley.edu)
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                           Department of Psychology, University of California, Berkeley, CA 94720 USA
                              Abstract                                 the pattern <referential entity> is X-ing, in which X describes
   Current computational models of word learning make use of           some activity for that referential entity. Another regularity in
   correspondences between words and observed referents, but as        English suggests that in the Y <referential entity>, Y is prob-
   of yet cannot—as human learners do—leverage information             ably a word that describes that following referential entity;
   regarding the meaning of other words in the lexicon. Here we
   develop a Bayesian framework for word learning that learns          hence, the color of the animal in each scene is a good candi-
   a lexicon from multiword utterances. In a set of three sim-         date for the meanings of the words wub and zek. In this way,
   ulations we demonstrate this framework’s functionality, con-
   sistency with experimental work, and superior performance in        learning the meaning of a single word may result in a cascade
   certain learning tasks with respect to a Bayesian word lean-        of further word learning.
   ing model that treats word learning as inferring the meaning of         Existing word learning models are well-suited to explain
   each word independently. This framework represents the first
   step in modeling the potential synergies between referential        how a learner might infer the meaning of the word “garp” in
   and distributional cues in word learning.                           the above scenes. Learners may use hypothesis elimination
   Keywords: word learning; Bayesian inference; artificial lan-        (Siskind, 1996) or more graded co-occurrence information
   guage learning; distributional learning                             (Smith & Yu, 2008) to discover the regular mapping from
                                                                       word to referent or concept. Bayesian models are particu-
                          Introduction
                                                                       larly powerful in that they can use implicit negative evidence
Among the many feats that comprise first language learning,            for this purpose. For example, Xu and Tenenbaum (2007)
discovering the meaning of many tens of thousands of words             showed that kids can learn words related by a taxonomic hi-
is among the most impressive. Indeed the size and richness of          erarchy in which a hypernym like “animal” is never incorrect
human vocabularies is one of the major points of distinction           for referring to a category member like a cat. Such models
between the linguistic capacities of humans and those of non-          also provide a formal framework for the integration of non-
human primates (Pinker & Jackendoff, 2005). Learners start             linguistic cues in word learning (Frank, Goodman, & Tenen-
early on this task: long before they utter their first words, tod-     baum, 2008), as well as additional category information (e.g.
dlers develop a substantive receptive vocabulary (Bergelson            a property-vs.-kind distinction) that learners may bring to the
& Swingley, 2012). How precisely young learners assemble               problem (Gagliardi, Bennett, Lidz, & Feldman, 2012).
this knowledge so quickly remains an active area of investi-               In contrast with the above models, distributional models
gation.                                                                are naive as to the correspondence between a word like “garp”
   One possibility is that learners are able to concurrently use       and entities or states in the world, and instead proceed from
both correspondences between 1) words and referents and 2)             the observation that the co-occurrence statistics of words—
words and other words in order to formulate and assess hy-             even in absence of referents—can encode rich information
potheses regarding word meaning. For example, consider the             about latent structure in language. As implicated in the above
two scenes and utterances with five novel words presented in           example, a word’s immediate context (previous word and fol-
Figure 1. In this example, a learner could use the reliable co-        lowing word) constitutes strong evidence of its grammatical
occurrence of garp and a particular referent (the depicted ani-        category (Mintz, 2003). Other models such as the connec-
mal) across the two scenes to infer its meaning. Having a rea-         tionist network of Elman (1990) and the technique of Latent
sonable hypothesis regarding the meaning of garp in turn en-           Semantic Analysis in Landauer and Dumais (1997) show how
ables several consequent inferences on the basis of structural         relationships of synonymy can be extracted from large cor-
regularities in English. Establishing that garp is a referential       pora by means of dimensionality reduction.
entity (a noun within the adult syntactic system, though the
                                                                           In the present work we examine how learners may use in-
child learner may have somewhat different provisional lexi-
                                                                       formation regarding other words in the lexicon to guide the
cal categories) means that both utterances are consistent with
                                                                       learning of word-to-referent mappings. We begin by outlin-
                                                                       ing a word learning model that learns the referent of a single
                                                                       word, then show how this procedure can be generalized for
                                                                       learning the referents of many different words concurrently
                                                                       from multiword utterances.
         t
         hewubgar   pi sleebi ng        hezebgar
                                        t          pi ssl   ng
                                                         eppi
                                                                                         Modeling Framework
Figure 1: Referential word learning (in this case “garp”) helps        To introduce our modeling framework, we first summarize a
the learner identify additional regularities, which in turn sup-       previous Bayesian word learning model and then generalize
port further word learning.                                            it to multiword utterances.
                                                                   1583

Bayesian Word Learning                                              the model supports unintuitive—though logically possible—
The Bayesian word learning model introduced by Xu and               meanings for both. The framework then treats the problem of
Tenenbaum (2007) focused on the learning of nouns. The              word learning as one in which the learner must find the best
learner observes a particular object x being given a word la-       lexicon to explain a set of observed world-states and corre-
bel w, and considers hypotheses h that correspond to the sets       sponding utterances.
of objects that could be given that label. The posterior proba-        A lexicon H consists of one or more word-level hypotheses
bility of each h is given by                                        {h1 , . . . , hn }, each of which is a mapping from a word w to a
                                 p(x|h, w)p(h)                      set of world-states {xn , . . . , xm }. The posterior probability of
              P(h|x, w) =                               ,    (1)    a lexicon given a set of utterances U and a set of observed
                             ∑h ∈H p(x|h, w)p(h0 )
                               0
                                                                    scenes X can be calculated according to Bayes’ rule:
corresponding to the normalized product of the likelihood
p(x|h, w) and the prior p(h).                                                                           p(X|H, U)p(H)
                                                                                     p(H|X, U) =                             .       (5)
   The likelihood term p(x|h, w) reflects whether the observed                                     ∑H0 ∈H p(X|H0 , U)p(H0 )
                         (h)                                           An observation from the above language consists of an ut-
concept x is in the set Sw identified by word w given hypoth-
esis h,                                                             terance u and a world-state x. Assuming the conditional in-
                             
                              (h) 1
                                         if x ∈ Sw
                                                  (h)               dependence of the observed utterance/world-state pairs, the
                p(x|h, w) =      Sw                   .      (2)    likelihood for a lexicon is the product of the probabilities of
                             
                               0         otherwise                  observing the world-state xi for the corresponding utterance
                                                                    ui for a given hypothesized lexicon H:
   The model’s likelihood employs the reciprocal of the set
size picked out by the current word – the size principle – cor-                            p(X|H, U) = ∏ p(xi |H, ui ).              (6)
                                                                                                         i=1
responding to assuming that objects are sampled uniformly              The likelihood term reflects whether the world-state xi can
at random. The model can accommodate multiple indepen-              be referred to by utterance ui under the lexicon H. If the
dent observations, in this case an ordered set of objects X and     world-state x is in with the set of world-states picked out by
an ordered set of words W, by modifying the likelihood to           the utterance give the current lexicon, the likelihood is calcu-
become                                                              lated as the reciprocal of the number of world-states that are
                  P(X|h, W) = ∏ p(xi |h, wi ).               (3)
                                  i=1                               picked out. Otherwise, the likelihood term is near zero. To
   The prior reflects the expectations of the learner about         prevent overfitting, a small portion of the probability mass (ε)
which hypotheses are more likely to be true. The simplest           is spread evenly across all hypotheses, yielding
                                                                                                                               (H)
                                                                                           
prior is one in which each hypothesis regarding the word-to-                                          1
                                                                                           (1 − ε) (H)        1
                                                                                                          + ε |S| if any xi ∈ Sui
concept mapping (the power set of concepts) is considered                                           Sui
                                                                       p(xi |H, ui ) =                                             , (7)
equally likely, p(h) = 1/2s , where s is the size of the hypoth-                           ε 1
                                                                                              |S|                 otherwise
esis space which that word could refer to.                                       (H)
                                                                    where Sui is the set of world-states picked out by the utter-
   Putting these pieces together, the probability that the word     ance given the current lexicon. The framework is itself agnos-
applies to a new object is given by                                 tic as to how the utterances and the lexicon pick out a partic-
                 P(y ∈ Sw |x) =      ∑     p(h|x, u),        (4)    ular set of world-states; depending on the assumptions about
                                       (h)                          the semantics, the lexicon may specify different sets of world-
                                  h:y∈Sw
being the sum of the posterior probabilities of those hypothe-      states given an utterance. In Simulation 1 we describe one
ses under which y would be a member of the corresponding            such function that picks out a particular set of world-states
set of objects.                                                     given an utterance and a lexicon. Rather that the exact form
                                                                    of this compositional function, the critical contribution of this
Multiword Utterances                                                framework is that of casting the problem of word learning as
We now generalize this model for learning the individ-              one in which all hypothesized word meanings that comprise
ual word-to-referent mappings for nouns to learning several         a lexicon can be used in the assessment of the likelihood or
word-to-referent mappings for different classes of words con-       prior for a particular word-to-referent mapping.
currently from a set of utterances. This requires changing two         The prior probability of the lexicon, p(H) is the product of
features of the modeling approach. First, rather than individ-      the prior probabilities of the hypotheses h that comprise the
ual words referring to sets of objects, we treat each word as       lexicon H, ∏h∈H p(h). In the current case, the prior is unin-
referring to a subset of possible states of the world, or world-    formative: each mapping from a word to a set of world-states
states. In a world in which there is an object that is 1) ei-       is equally likely. Here the prior p(H) = 1/2s×n , where s is the
ther red or black and 2) either round or square, there would        number of world-states and n is the number of words in the
be exactly four possible world-states. Second, we treat the         lexicon. A more informative prior, such as a preference for
referential content of an utterance as the set of world-states      cluster distinctiveness in taxonomic hierarchies (Xu & Tenen-
picked out by some compositional function operating over            baum, 2007) or a concept prior reflecting higher-level knowl-
the relevant word-to-referent mappings in the lexicon. By de-       edge of word categories (Gagliardi et al., 2012), could also be
lineating both word and utterance meaning in terms of sets,         implemented within this same framework.
                                                                1584

   The probability that a novel world-state y can be referred        that posits that wag refers to objects that move up and down
to by utterance u (consisting of one or more words) can be           has a likelihood of 0 because that world-state is not seen con-
computed by generalizing Equation 4,                                 sistently with that utterance. The lexicon that posits that wag
                p(y ∈ Su |u) = ∑ p(H|X, u),                   (8)    refers to things that move side to side and those that are black
                                    (H)
                               H:y∈Su
                                                                     receives a higher probability than the first lexicon because it
being the sum of the posterior probabilities p(H|X, U) for all       is consistent with the observed data, but the likelihood is rel-
lexicons in which y is in the set of world-states picked out for     atively low in that the hypothesis picks out a larger number
utterance u.                                                         of world-states. The lexicon that posits that wag refers to
                                                                     objects that move side to side receives the highest posterior
                         Simulations
                                                                     probability, in that it is the most specific hypothesis that is
We present three simulations to demonstrate the function and         consistent with the observed data. Probabilities of general-
utility of the new modeling framework. In Simulation 1, we           ization for each utterance to each world-state are presented in
show how the model finds the optimal lexicon in a toy world          Figure 2.
in which an utterance specifies a set of world-states via a sim-
                                                                     Simulation 2: Kersten and Earles (2001)
ple compositional function. In Simulation 2, we show that the
                                                                     In the second simulation, we show how a model developed
framework generates predictions that are consistent with ex-
                                                                     within the framework presented here is capable of learn-
perimental work in which adults learn the meaning of words
                                                                     ing word meanings in an existing artificial language learn-
from multiword utterances (Kersten & Earles, 2001). Finally,
                                                                     ing paradigm. Kersten and Earles (2001) describe a set of
in Simulation 3 we describe a word learning task in which the
                                                                     experiments in which participants are presented with one-
new framework significantly outperforms the basic Bayesian
                                                                     to three-word utterances coupled with simple visual scenes.
word learning model.
                                                                     Utterances encode some variable aspects of the scenes (e.g.
Simulation 1: Word learning in a simple toy world                    the type and manner of motion of insects depicted in the
First, we demonstrate how the above framework allows us              scene), while many other aspects of each scene vary ran-
to learn the best lexicon for a simple toy language under the        domly. To investigate the effects of hearing only partial ut-
assumption of intersective semantics. Whereas the likelihood         terances on language learning, participants in one condition
function in the simple Bayesian word learner only depends            heard a complete set of 72 three-word utterances, while those
on whether a world-state is in the set picked out by a word,         in the other condition heard 24 single-word utterances, then
some compositional function is needed to pick out the set of         24 two-word utterances, and ultimately 24 three-word utter-
world-states given an utterance. Here we assume that this set        ances. All words were marked with a consistent morphologi-
is specified by the intersection of world-states selected by the     cal marker, corresponding with the sentence position (e.g. all
words that comprise that utterance:                                  sentence-final words, which describe the manner of motion,
                          (H)    \ (h)                               terminate in the particle –tig).
                        Sui =        Sw .                     (9)       After a training period of 72 utterances and scenes, a bat-
                                w∈ui
                                                                     tery of two alternative forced-choice tests was used to assess
Other, more elaborate semantic functions may be substituted
                                                                     the degree to which participants had learned the meanings of
(e.g. a fully compositional semantics) within the framework;
                                                                     words and utterances. In the 12 isolated test trials, each par-
in the current case we use intersective semantics as the ba-
                                                                     ticipant chose between two scenes which was the better ex-
sis for a simple demonstration of the framework that can
                                                                     ample of the single-word utterance test item. In 12 embedded
nonetheless capture aspects of previous work on artificial lan-
                                                                     test trials, each participant chose between two scenes which
guage learning.
                                                                     was the better example of a three-word utterance.
   In the toy world, world-states vary along three binary di-
                                                                        This study is an appealing task to model within our new
mensions: an object is either a square or a circle (pu or du),
                                                                     framework for two reasons. First, it involves learning corre-
which is either filled or unfilled (li or ri), and which moves
                                                                     spondences between words and many possible candidate fea-
either side-to-side or up and down (wag or div). There are
                                                                     tures in each scene. For example, participants must infer that
thus eight possible states of the depicted in the scene, and
                                                                     the background of a scene is not encoded by any words in the
eight utterances of three words length (e.g. the utterance pu
                                                                     lexicon. Second, Kersten and Earles assumed an intersective
li wag would be accompanied by a world-state of a black
                                                                     semantics for their artificial language, making their experi-
square moving side-to-side). A complete set of utterances
                                                                     ment straightforward to model.
and world-states are shown in Figure 2 along the vertical and
horizontal axes respectively.                                        Memory Noise We use a noise model to simulate a
   To demonstrate the operation of the model, consider the           learner’s imperfect memory or limited attention in observing
posterior probability of three different lexicons, each of           which words were said. Each word in the set of observed ut-
which maps the word wag to a different set of world-states,          terances U is switched with an alternative word that appears
after seeing eight sentences and corresponding world-states.         in the same sentence position at rate η, between 0 and 1. Ed-
While each lexicon has the same prior probability under the          its can be attributed to any mixture of attentional deficit (the
model, they are distinguished by their likelihood. The lexicon       learner did not attend to a feature, resulting in an edit) or noisy
                                                                 1585

                                 Single Word Utterances                                   Multiword Utterances        Probability
                       pu                                             du ri div                                          1.0
                                                                      pu ri div
                       du                                                                                                0.8
                                                                      du li div
          Utterances
                       li                                             pu li div                                          0.6
                       ri                                             du ri wag
                                                                                                                         0.4
                                                                      pu ri wag
                       wag                                                                                               0.2
                                                                      du li wag
                       div                                            pu li wag                                          0.0
                             ↔   ↔     ↔    ↔   ↕   ↕     ↕   ↕                   ↔   ↔     ↔   ↔    ↕   ↕    ↕   ↕
                                     State of the World                                     State of the World
Figure 2: Probability that each single word utterance (left) or multiword utterance (right) can refer to each of eight world-states
given the lexicon learned by the model in Simulation 2. Colors represent the probability of generalization, or the probability
that a given world-state can referred to by an utterance.
memory. Inference then proceeds over the set of utterances                 ples, then collect 5000 samples and thin to every fifth sample.
with noise imposed, U0 .                                                   Convergence was assessed by assessing the log likelihood on
Inference To provide for maximum generality in possible                    repeated simulations. The posterior over the full hypothesis
word meanings, the framework specifies that any word in the                space was estimated using likelihood-weighted samples from
model can refer to the power set of world-states. The hypoth-              the prior. Likelihood weighting is a special case of impor-
esis space for the lexicon is thus a very large discrete space             tance sampling in which the importance distribution is the
even for the small language presented in Kersten and Earles                prior. To compensate for sampling from the prior distribution
(2001): there are 26×8192 possible lexicons (a binary can re-              rather than the posterior, probabilities are adjusted by weight-
fer/can’t refer indicator for 8,192 possible world-states, for             ing by the likelihood and normalizing.
each of 6 words.) We use a hybrid approximation strategy
to approximate the posterior in this large space by both sam-                 The two sampling techniques outlined above have com-
pling from a subset of “structured” hypotheses using Gibbs                 plementary weaknesses: Markov chain Monte Carlo over
sampling as well as taking likelihood-weighted samples from                the structured hypotheses omits the unstructured hypotheses,
the full space.                                                            while the likelihood weighting—in that is sampled from the
                                                                           prior—finds relatively few high-value hypotheses. We mix
   Structured hypotheses are those that consistently refer to a            samples from the two distribution with weights 1 − α and α.
feature that is shared across states of the world (e.g. all in-            Including the inference procedure, the model for Simulation
sects that have square bodies). Unstructured hypotheses ad-                2 thus has three free parameters: the per-word error rate η
ditionally include heterogenous combinations of world states               for the stored utterances, noise in the likelihood function ε,
as potential word meanings, including complex meanings like                and mixing weight α. For the simulations reported here, we
“bugs traveling upwards so long as the legs move back and                  take 1000 samples from the prior and set α to .1 and under
forth, and also bugs with oval bodies.” Treating meaning as                the assumption that these hypotheses constitute a relatively
denotation—a mapping from a word to a set of states of the                 small proportion of the overall mass, ε to .05, and test a range
world—permits the representation of both kinds of hypothe-                 of η values between 0 and 1 and intervals of .01. We ran-
ses within the same formalism.                                             domly generate 50 experimental setups of the sort described
   Even the structured set alone contains 26×26 hypotheses.                in Kersten and Earles (e.g. different training data and test data
Consequently we use Gibbs sampling (Gelman et al., 2013)                   in each case) for each level of noise, collect 2 sets of samples
to approximate the posterior on the structured set by sampling             using MCMC and likelihood weighting for each setup, and
from the full conditional distribution according to a Markov               assess each set of samples against 10 instances of the testing
chain on hypotheses. We use a burn-in period of 2500 sam-                  battery.
Table 1: Example utterances and scene descriptions from the artificial language learning paradigm in Kersten and Earles (2001),
modeled in Simulation 2. Scene vary randomly along five additional dimensions.
 Utterance                                 Body and Legs (“Object”)    Path of Movement                      Manner of Movement
 “geseju elnugop doochatig”                light oval                  towards stationary character          legs angled forward and back
 “mogaju ontigop neematig”                 dark rectangle              away from stationary character        side-to-side movement
 “geseju elnugop neematig”                 light oval                  towards stationary character          side-to-side movement
                                                                      1586

                                 η = 0.1              η = 0.15             η = 0.2                η = 0.25         Adult Performance
                       1.0
                                                                                                                        Full Exposure
                       0.9
                       0.8
                       0.7
                                                                                                                                        Trial Type
     Percent Correct
                       0.6
                       0.5                                                                                                                   Embedded
                       1.0
                                                                                                                      Staged Exposure        Isolated
                                                                                                                   Staged Exposure
                       0.9
                       0.8
                       0.7
                       0.6
                       0.5
                             Object Manner Path   Object Manner Path   Object Manner Path     Object Manner Path   Object Manner Path
                                                                          Word Type
Figure 3: Model performance at four levels of noise compared with adult performance found in Experiment 1 of Kersten
and Earles (2001). Participants/models choose between two scenes for a given word (isolated test trials) or a given utterance
(embedded test trials). Error bars indicate standard error of the mean.
Results Test scores from Simulation 2 (Figure 3) indicate                                   and ending with single word utterances.
that the Bayesian word learning model presented here, like                                     The model performance diverges from human behavior in
human participants, is fully capable of learning correspon-                                 two notable ways. The model performs substantially better
dences between words and world-states from multiword ut-                                    on isolated test trials—in which utterances consist of a sin-
terances. The model performs at or near ceiling at low levels                               gle word—than on embedded ones (β = .774, SE = .032,
of memory noise (η = 0 to η = .1), while it demonstrates                                    z < .001). At higher levels of noise, isolated test trials ex-
levels of performance in the range achieved by human partic-                                hibit higher levels of performance than embedded test trials,
ipants at moderate levels (η = .1 to η = .3). Memory noise                                  as evinced by the trial type × memory noise interaction term
levels beyond η = .3 result in performance near chance.                                     in the model (β = 0.918, z < .001). In contrast, Kersten
                                                                                            and Earles found no significant difference in people’s perfor-
    To further explore the effects of staged vs. full exposure,
                                                                                            mance on the two test trial types (p > .1). The model also
memory noise, word type, and test trial type we constructed
                                                                                            predicts only minor differences in performance across word
a logistic regression model to predict the outcome of indi-
                                                                                            types (object, manner, and path), whereas Kersten and Ear-
vidual forced-choice trials (predicting correct vs. incorrect
                                                                                            les found that participants who saw staged input learned ob-
choices). Manner words, embedded trials, and complete ex-
                                                                                            ject and path words significantly better (74.5% and 77% for
posure are treated as the reference levels for the categori-
                                                                                            those who saw partial input, and 60% and 69.5% for those
cal predictors. The model scores consistently higher on the
                                                                                            who saw complete) than manner words (55% for partial and
testing battery when trained on the partial training utterances
                                                                                            49.5% for complete exposure). The explanation for this dis-
(β = .118; z < 0.001; intercept = 7.04). Furthermore there is
                                                                                            sociation is straightforward: the model presented here has no
an interaction with memory noise such that the model’s per-
                                                                                            information that would substantively distinguish word types
formance given partial exposure is higher at higher levels of
                                                                                            from one another. Performance for manner and path are lower
noise (β = 5.584; z < 0.001). Like participants in Experiment
                                                                                            under staged exposure because the model observes a manner
1 in Kersten and Earles (2001), the Bayesian word learning
                                                                                            word in 2/3 of cases and a path word in just 1/3 of cases.
model presented here performs better when trained on staged
exposure. This result, like Kersten and Earles’s observation                                Simulation 3: Multiple Objects Per Scene
of the empirical phenomenon, is intriguing in that a partici-                               A simple Bayesian word learning model that learns the mean-
pant/model in the full exposure condition should be able to                                 ing of words independently performs equally well on the
achieve the same results as one in the partial exposure con-                                first two simulations. In the final simulation, we demon-
dition by selectively attending to just a subset of the data. It                            strate a case in which a model using intersective semantics
appears that the model entertains more inclusive hypotheses                                 within the new framework significantly outperforms the sim-
for individual words and two-word phrases than three-word                                   ple Bayesian model. Inference, testing procedure, and set of
phrases, which consequently helps the model to avoid over-                                  observed utterances are the same as Simulation 2, though we
fitting the lexical hypotheses. In effect, memory noise leads                               set η = 0 for simplicity. The critical change is that rather than
the model to prefer lower-complexity lexicons, which then                                   a single world-state, the model observes four different world-
generalize better upon exposure to novel test data. This con-                               states along with each utterance. The likelihood functions
clusion leads to the empirically-testable prediction that the                               in Equations 2 and 7 are altered such that they assess whether
same higher performance for partial exposure would be ob-                                   any of the observed world-states is in the set picked out by the
served among human participants if the order of staged pre-                                 utterance, following from the possibility the utterance could
sentation were reversed—starting with three-word utterances                                 refer to any of the world-states depicted. Additionally, the
                                                                                     1587

                               1.0                     Trial Type        plicitly defining this set. We make the additional simplifying
             Percent Correct
                               0.9                         Embedded      assumption of intersective semantics: “black cats” would re-
                               0.8                         Isolated      fer to the set of things in the intersection of things that are
                               0.7
                                                                         cats and things that are black. Rich compositional semantics,
                                                                         rather than intersective semantics as demonstrated here, will
                               0.6
                                                                         better approximate real-world word meanings.
                               0.5                                          Despite the shortcomings, we believe that this work is an
                                     Intersective Non-intersective
                                              Model                      essential first step in understanding how learners flexibly use
Figure 4: Model performance for the language learning ex-                information form the entire lexicon in the process of word
periment presented in Simulation 3.                                      learning. Future work will require 1) a more elaborate model
                                                                         of semantics 2) the formulation of priors that constrain the
four world-states presented in each learning trial are chosen            space of preferred lexicons 3) the development of inference
to be very similar to one another: instead of being drawn at             methods that operate over this large hypothesis space. Fully
random from the entire set of world-states, an observation               integrating lexical distributional information will require non-
consists of a world-state and a corresponding veridical utter-           trivial formal machinery for identifying structural categories
ance in the language, as well as three world-states that are             of words and relating them to dimensions of similarity among
consistent with utterances that differ by only one word from             world-states. However, by recasting the problem of word
the veridical utterance.                                                 learning as one of lexicon inference–and one in which the
                                                                         whole utterance can be used—we take the necessary first
Results In this case, the intersective model significantly
                                                                         steps in bridging the gap between referential and distribu-
outperforms the base model. In the standard Bayesian model,
                                                                         tional models of word learning.
the set of world-states consistent with a given utterance are
those that are identified by each independent word-level hy-             Acknowledgments.This material is based upon work supported by
pothesis. The intersective learner is more choosy: it only               the National Science Foundation Graduate Research Fellowship un-
considers world-states that are picked out as the intersection           der grant number DGE-1106400 and by grant number FA9550-13-
of all word-level hypotheses. In this way, the intersective              1-0170 from the Air Force Office of Scientific Research.
learner leverages information about other words in the lexi-
con to identify a single world-state consistent with the entire
                                                                                                   References
                                                                         Bergelson, E., & Swingley, D. (2012). At 6–9 months, human in-
utterance. Both models perform well if the objects in a scene               fants know the meanings of many common nouns. Proceedings
are highly dissimilar because alternative word-level hypothe-               of the National Academy of Sciences, 109, 3253-3258.
ses receive little support from the data. However, if the set            Elman, J. L. (1990). Finding Structure in Time. Cognitive Science,
                                                                            14, 179–211.
of observed world-states in each scene are all very similar to           Frank, M. C., Goodman, N., & Tenenbaum, J. B. (2008). A
one another, the base model entertains many hypotheses as                   Bayesian framework for cross-situational word-learning. In
consistent with the data that the intersective model avoids be-             J. Platt, D. Koller, Y. Singer, & S. Roweis (Eds.), Advances in
                                                                            Neural Information Processing Systems 20 (pp. 457–464). Cur-
cause they do not describe any one world-state in the scene.                ran Associates, Inc.
Performance drops to near chance on the test set for the sim-            Gagliardi, A., Bennett, E., Lidz, J., & Feldman, N. (2012). Chil-
                                                                            dren’s inferences in generalizing novel nouns and adjectives. In
ple model, while the intersective word learner is still able to             Proceedings of the 34th Annual Conference of the Cognitive Sci-
infer much of the lexicon (Figure 4).                                       ence Society (pp. 354–359).
                                                                         Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D., Vehtari, A., &
                                       Discussion                           Rubin, D. B. (2013). Bayesian Data Analysis. London: Chapman
                                                                            and Hall.
We demonstrate a powerful, extensible, and versatile                     Kersten, A. W., & Earles, J. L. (2001). Less really is more for adults
                                                                            learning a miniature artificial language. Journal of Memory and
Bayesian framework for learning word-to-referent mappings                   Language, 44, 250–273.
from multiword utterances. By assuming an underlying sim-                Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s
ple compositional semantics, an utterance can be treated as                 problem: The latent semantic analysis theory of acquisition, in-
                                                                            duction, and representation of knowledge. Psychological Review,
more than a collection of words with independent denota-                    104, 211–240.
tions. Instead, as we demonstrate in Simulation 3, the rich              Mintz, T. H. (2003, November). Frequent frames as a cue for gram-
information contained in multiword utterances can be lever-                 matical categories in child directed speech. Cognition, 90, 91–
                                                                            117.
aged to guide the word learning process.                                 Pinker, S., & Jackendoff, R. (2005). The faculty of language: What’s
   The model presented here makes use of strong simplifying                 special about it? Cognition, 95, 201–236.
                                                                         Siskind, J. M. (1996). A computational study of cross-situational
assumptions regarding the nature of word meanings and the                   techniques for learning word-to-meaning mappings. Cognition,
formalism underlying semantic composition. Word meaning                     61, 39–91.
is treated here as denotation, or the selection of world-states,         Smith, L., & Yu, C. (2008). Infants rapidly learn word-referent
                                                                            mappings via cross-situational statistics. Cognition, 106, 1558–
and leaves the matter of connotation unaddressed. For En-                   68.
glish, this is analogous to saying that word “cat” means the             Xu, F., & Tenenbaum, J. B. (2007, April). Word learning as
set of things in the world that are cats, whereas criteria like             Bayesian inference. Psychological Review, 114, 245–72.
“four-legged,” “predatory,” and “mammal” are taken as im-
                                                                      1588

