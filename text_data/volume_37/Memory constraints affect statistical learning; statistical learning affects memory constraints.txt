                                 Memory constraints affect statistical learning;
                                 statistical learning affects memory constraints.
                                          Joshua R. de Leeuw (jodeleeu@indiana.edu)
                           Department of Psychological and Brain Sciences & Program in Cognitive Science
                                                     Bloomington, IN 47405 USA
                                         Robert L. Goldstone (rgoldsto@indiana.edu)
                           Department of Psychological and Brain Sciences & Program in Cognitive Science
                                                     Bloomington, IN 47405 USA
                               Abstract                               explicitly in the form of chunks (e.g. Fiser & Aslin, 2001,
   We present evidence that successful chunk formation during a
                                                                      2002).
   statistical learning task depends on how well the perceiver is         A key part of statistical learning research is identifying
   able to parse the information that is presented between            the conditions under which chunking occurs. The
   successive presentations of the to-be-learned chunk. First, we     foundational work focused on learning based on transitional
   show that learners acquire a chunk better when the                 probabilities (Aslin, Saffran, & Newport, 1998; Saffran,
   surrounding information is also chunk-able in a visual             Aslin, & Newport, 1996), and much subsequent research has
   statistical learning task. We tested three process models of       explored different constraints and biases that affect learning.
   chunk formation, TRACX, PARSER, and MDLChunker, on
   our two different experimental conditions, and found that only     A key theme from this research is that previous learning
   PARSER and MDLChunker matched the observed result.                 experience alters how new information is processed.
   These two models share the common principle of a memory            Learners form expectations about the kind of structure that
   capacity that is expanded as a result of learning. Though          is present in an information stream from previous exposure
   implemented in very different ways, both models effectively        to other streams (Lew-Williams & Saffran, 2012). This can
   remember more individual items (the atomic components of a         cause them to fail to learn structures that are in conflict with
   sequence) as additional chunks are formed. The ability to
                                                                      their expectations (Gebhart, Aslin, & Newport, 2009). Prior
   remember more information directly impacts learning in the
   models, suggesting that there is a positive-feedback loop in       learning can also improve subsequent learning. For
   chunk learning.                                                    example, acquiring non-adjacent dependencies is easier after
                                                                      first learning the adjacent dependencies (Lany & Gómez,
   Keywords: statistical learning; chunking; memory
                                                                      2008).
                                                                         Memory constraints are an important factor in
                           Introduction                               determining the success of learning new chunks. Frank and
   The formation of chunks is hypothesized to be a crucial            Gibson (2011) showed that statistical rule learning is
aspect of cognition, perception, and learning (Gobet et al.,          improved in a variety of experimental paradigms when
2001). Chunks are a means of creating compressed                      memory constraints are alleviated by presenting examples
encodings for frequently co-occurring inputs. The concept             concurrently instead of sequentially. They hypothesize that
of chunking has been used to explain a wide range of                  this is because learners need to be able to remember enough
psychological phenomena, including the advantages that                items in order to extract the statistical regularities. However,
expert chess players have in remembering the position of              it is unknown what functional role the memory constraints
chess pieces on a board (Chase & Simon, 1973; Gobet &                 might play.
Simon, 1998), differences in the speed of retrieving                     Models of statistical learning vary on whether they
successive letters of the alphabet (Klahr, Chase, &                   include memory constraints and how such constraints are
Lovelace, 1983), and the ability to remember more words               implemented. Models with memory constraints, either in
when the words are part of familiar phrases (Simon, 1974).            terms of a limit on the number of input items that can be
A core aspect of chunking is that it increases the number of          remembered or a limit on the number of internal states that
items that can be stored in memory: It is possible to                 the model can track, tend to fit human performance on word
remember more individual letters if they are chunked into             segmentation tasks better than models without such
words, and more words if they are chunked into sentences.             constraints (Frank, Goldwater, Griffiths, & Tenenbaum,
   Statistical learning paradigms are well suited for                 2010). However, previous models have not explored how
investigating the conditions under which chunks are learned           the learning process and the memory constraints might
(Perruchet & Pacton, 2006). In a typical statistical learning         interact. Since statistical learning is hypothesized to involve
task, a novel information stream containing latent structure          chunk formation, and chunks are more efficient memory
is presented to a subject for a moderate length of time, and          structures for encoding information, learning may have a
the subject is tested on how well they are able to learn the          cyclical effect: learning to chunk may reduce the memory
structure that generated the stream. Often this structure is          constraints of encoding a sequence, allowing people to
                                                                  530

remember more items and more easily extract the
regularities. We tested this hypothesis in a simple
experiment.
                             Method
We replicated and extended a classic result from temporal
visual statistical learning (Fiser & Aslin, 2002). In the
original experiment, subjects were exposed to a sequence of
shapes, presented one at a time, with no overt task.
Unbeknownst to the subjects, the sequence was formed by
grouping the shapes into sets of three items (triples) and
presenting the triples in a random order. We replicated the
original experiment as a control condition, and also tested
subjects’ ability to learn an individual triple when the other
triples were scrambled. In both conditions, the target triple
appears equally often and with equal frequency throughout
the sequence. If learning chunks makes it easier to learn
other chunks, then learners should show improved learning           Figure 1: Shape stimuli used in experiment 1. In the four-
for the target triple in the condition with more triples.
                                                                    triples condition, stimuli were grouped into four triples
                                                                    (illustrated with solid boxes). In the one-triple condition,
Participants
                                                                    one triple appeared with the shapes in the same order
41 people participated in the study via Mechanical Turk.            throughout the sequence (solid box), and the rest of the
Subjects were paid $1.25 for participation. Subjects were           stimuli appeared in groups of three but with a random order
randomly assigned into either the four-triples (N = 21) or          of the shapes inside the box during each appearance
one-triple (N = 20) condition.                                      (dashed boxes)
Procedure                                                           four triples was always presented as a consistent triple,
Subjects completed the experiment in a web browser of their         maintaining its original order. In addition, three impossible
choice. The experiment was developed using the jsPsych              triples were created for testing purposes. Impossible triples
software library (de Leeuw, 2015).                                  contained one shape from each of the three randomized
   The experiment consisted of an exposure phase and a test         triples. When the sequence was constructed, shapes that
phase. During the exposure phase, subjects viewed a                 occurred in the same impossible triple could not occur
sequence of 300 images with the instructions to simply              sequentially. This constraint allowed for a comparable test
observe the shapes because they would be asked questions            in both conditions: a triple that was seen could be paired
about what they saw. The sequence consisted of 12 unique            with a triple that was never observed.
shapes, modeled after the shapes depicted in (Fiser & Aslin,           In the test phase, subjects were sequentially presented
2002). The sequence was shown as an animation with                  with two three-item sequences and asked to report which
shapes oscillating horizontally, moving behind an occluding         triple occurred more often during the exposure phase. Each
rectangle in the center of the screen (see Fiser & Aslin, 2002      three item sequence was presented in the same manner as
for a visual depiction). It took one second from the point          the sequence during the exposure phase. There was a 1
that a part of the shape appeared to the point that the shape       second gap between the two test sequences. Subjects were
was completely occluded again. The entire sequence lasted           required to choose one of the sequences, even if they were
five minutes.                                                       unsure. There were 32 test pairs. In the four-triples
   In the four-triples condition, the shapes were grouped into      condition, four impossible triples were created, where the
four triples, with each shape belonging to one triple (figure       probability of each item in the triple appearing adjacent to
1). The assignment of particular shapes to triples was              the other items during the exposure phase was 0. Each triple
randomized for each subject. The sequence was created by            was tested against each impossible triple twice, once with
randomly ordering the triples, with the constraints that: (1) a     the triple first and once with the triple second. In the one-
triple could not occur twice in a row, (2) a triple could not       triple condition, we also created four impossible triples, as
occur more than twice before every other triple occurred at         well as three low-probability triples. The impossible triples
least once, and (3) all triples occurred exactly 25 times.          never occurred in the sequence, and the low-probability
   In the one-triple condition, the sequence was created in a       triples occurred rarely. We did not use any data from the test
similar way, except that the order of three of the triples was      pairs that contained low-probability triples; they were
randomized for each presentation of the triple. Thus, if one        merely created to make the testing phase the same length in
of the randomized triples was ABC, it would randomly                both conditions, and to ensure that the frequency of
                                                                    individual shapes was identical in the testing phase. There
appear as ABC, ACB, BAC, BCA, CAB, and CBA. One of the
                                                                531

were 24 trials containing low-probability triples, and 8
containing the single triple compared with one of the four
impossible triples.
                             Results
   Subjects in the four-triples condition had an overall
accuracy of 73.4% at identifying the triple they had seen
before in the forced-choice tests, while subjects in the one-
triple condition were only 58.8% accurate (Figure 2). Thus,
subjects in the four-triple condition were 14.8% more
accurate at identifying the target triple, on average.
   We used a Bayesian data analysis model to estimate the
difference in probability of a correct response between the
two conditions. There are numerous reasons to favor
Bayesian data analysis over conventional null-hypothesis
significance testing (Kruschke, 2011), but a significant
advantage in this particular application is the ability to
naturally account for the different number of critical trials in
each condition (32 for the four-triples condition and 8 for
the one-triple condition). Each subject’s responses were
treated as being generated from a binomial distribution with
probability p and number of samples N. For subjects in the
four-triples condition, N=32, and for subjects in the one-
triple condition, N=8. We estimated p as the sum of two                 Figure 2: Experiment results. Top: Mean accuracy for the
random variables: pbaseline and pdifference. The baseline               two conditions in experiment 1. Error bars show one
component estimated the overall mean probability of a                   standard error of the mean. The y-axis begins at chance
correct response across conditions, and the difference                  performance (50%). Bottom: Posterior distribution of the
component estimated the magnitude of the difference                     estimated difference in the probability of a correct answer
between conditions. The prior on pbaseline was a beta                   between conditions. Positive values indicate samples from
distribution with both shape parameters equal to 1, and the             the posterior in which the four-triples group was more
prior on pdifference was a normal distribution with the mean            accurate than the one-triple group. The 95% HDI is shown
equal to 0 and the standard deviation equal to 1. These                 in black, with the limits labeled.
parameters represent vague priors that are appropriate to the
scale of the data. We used MCMC sampling with the
runjags R package to find the posterior distribution. The               Model descriptions
95% highest-density interval (HDI)1 for pdifference was 6.39%
                                                                           We tested three models: PARSER, MDLChunker, and
to 22.9%, with a mode of 13.4%. Thus, the model finds
                                                                        TRACX. We chose these models because they are process
strong evidence that the four-triple group did indeed learn
                                                                        models that represent different approaches to sequence
the triples better than the one-triple group2.
                                                                        segmentation and chunk learning, and they all had
                                                                        publically available implementations that we could use.
                           Modeling                                     Importantly, the three models all deal with memory
   The experiment found evidence that chunk learning is                 constraints in different ways. Here we briefly summarize
influenced by more than just the repeated presentation of a             each model to provide an intuition for how they work. Due
consistent set of items. The target triple was learned                  to space constraints, please refer to the original source
significantly better when the surrounding information was               material listed in the heading for a more detailed
also generated from a triple-based structure. We tested three           explanation of each of the models.
well-established process models of statistical learning to see
if they predicted the difference in learning that we observed.          PARSER (Perruchet & Vinter, 1998). PARSER
                                                                        constructs an internal lexicon through an online chunk
                                                                        formation process. Candidate chunks are created through a
   1
                                                                        random process as the model processes the input: PARSER
     The range of parameter values containing 95% of the posterior      selects a percept length of 1, 2, or 3 units (with the default
where each value inside the HDI is more probable than those
                                                                        parameter set). This percept becomes a candidate chunk.
outside it. The HDI represents the most likely parameter values for
the model given the data.
                                                                        Frequently seen chunks are reinforced, while candidate
   2
     A t-test of the difference in means also reached the same          chunks that are encountered rarely are forgotten. When the
conclusion of a significant difference in accuracy, t(39)=2.2855, p     strength of an individual chunk rises above a threshold, then
= 0.028.                                                                incoming information is shaped by the presence of the
                                                                    532

chunk. For example, if the incoming sequence is ABCD and
PARSER selects a percept length of 2 and has no chunks,
then the model will form a candidate chunk of AB. But if
PARSER already has the chunks AB and CD and selects a
percept length of 2, then the input sequence ABCD will be
processed as AB/CD. This will result in both the AB and CD
chunks being reinforced, as well as the formation of a
candidate ABCD chunk.
MDLChunker (Robinet, Lemaire, & Gordon, 2011).
MDLChunker also creates an explicit internal lexicon, but it
uses the minimum description length principle (Rissanen,
1978) to guide the formation of new chunks. As
MDLChunker processes a sequence, it checks to see if
recoding the sequence using chunks would decrease the
number of bits required to encode the sequence.
Importantly, adding chunks increases the number of bits
required to store the lexicon, and MDLChunker will only
add a new chunk if the cost of adding the chunk to the
lexicon is outweighed by the overall reduction in coding
complexity of the sequence. We used the memory-
constrained version of MDLChunker (see section 7.3 of
Robinet et al., 2011). Without memory constraints,
MDLChunker checks the cost of adding a new chunk
against all of the input that it has previously seen. The
memory constraint imposes a limit, expressed in bits, for
how much of the previous input can be retained by the
model (and thus used in the calculation for adding a new
chunk). Importantly, the memory cost is calculated based on
the lexicon. Thus as the model gets more efficient at              Figure 3: Model results. Each of the three models has a
encoding the input, the absolute number of items in memory         different way of indicating how well the target chunk was
will grow.                                                         learned, indicated on the y-axis. The distributions of the
                                                                   measurement values are shown in grey. PARSER produced
TRACX (French, Addyman, & Mareschal, 2011).                        a bimodal distribution in the one-triple condition, showing
TRACX is a connectionist model of chunk learning. The              that the target triple was learned only some of the time. The
core of TRACX is an auto-associative network. The input            box-and-whisker overlay is provided to show a
layer represents two adjacent items (called the left- and          representation of the central tendency. The dark line is the
right-hand items, with the left-hand item occurring                median, the boxes represent the range of values between the
temporally before the right-hand item) from the sequence,          25th and 75th percentile of the distribution, and the whiskers
the hidden layer forms a compressed representation of the          show the range of data that is within the inter-quartile range
input, and the output layer recreates the input. Back-             (height of the box) times 1.5. Values outside this range are
propagation is used to adjust the weights so that the output       plotted as individual dots.
better matches the input. The key innovation is that the
network will use the hidden layer as the left-hand item in the     http://perruchet.jimdo.com/u-learn/.                       For
next input when the error in reconstruction is low. Low            TRACX, we used a JavaScript version of the model from
reconstruction error occurs when the input is very familiar        https://github.com/YourBrain/TRACX-Web.
to the network, and thus is a candidate chunk. The                 We made no modifications to the model code.
distributed pattern of activity on the hidden layer is a
representation of the chunk. Initially, TRACX will learn           Procedure. We converted the sequences seen by
only two-item chunks, but as these chunks are learned and          participants in the experiment into strings of letters, with
subsequently become part of the input, then longer chunks          each shape being represented by a unique letter. The strings
can also be learned.                                               were 300 characters long. We used the exact same
                                                                   sequences seen by participants in the experiment. Each
Method                                                             model was tested with 20 different four-triple sequences and
Model implementations. We used publicly available                  20 different one-triple sequences. We used the default
implementations of each of the three models. For PARSER            parameters for all models.
and MDLChunker, we used the U-LEARN software from
                                                               533

   Each of the three models generates a different kind of            We tested three process models on this task, and found that
output. PARSER and MDLChunker both construct lexicons                two of them, PARSER and MDLChunker, predicted a
containing explicit chunks. PARSER assigns a weight to               difference in learning between the two conditions, while the
each chunk, with higher scores being chunks that have                third, TRACX, did not.
greater weight. MDLChunker reports the number of bits                   Why did PARSER and MDLChunker both match the
needed to encode each chunk; smaller bit lengths represent           direction of the effect, while TRACX showed equivalent
chunks that are more strongly encoded. TRACX produces a              performance in both conditions? The key difference seems
network recognition error score for any given input chunk,           to be the way that memory constraints are implemented in
but because the chunks are represented as distributed                the models. PARSER and MDLChunker both share a
patterns there is no list of known chunks produced by the            common feature: As the models learn to chunk the input
model. Instead, the model is queried with a particular chunk         sequence, the relative strength of the memory encoding for
to see what the error rate is. Since our main interest was           individual chunks increases. In both models, this effectively
seeing if any of the models could fit the qualitative pattern        leads to a longer lasting memory for previously seen
and this only requires within-model comparisons, we did              chunks. The longer memory span improves learning for
not attempt to equate these different output values between          individual chunks, as they seem to be more frequent from
models.                                                              the perspective of the memory-limited model. We’ll
   It was unclear how to link the various model’s outputs to         illustrate this by walking through each model.
performance on the forced-choice test. TRACX provided a                 PARSER processes a sequence in sets of 1, 2, or 3 units at
relatively straightforward option, since the recognition error       a time. The number of units is randomly selected at each
for any particular chunk can be tested. However, both                model step. Consider the sequence ABCGHIDEFABC. If
PARSER and MDLChunker will never learn the foil items                PARSER contains no chunks, and randomly selects to see 3
from the forced-choice test, since the transitional probability      items, then the input on this step will be A/B/C. But, if
for each pair of shapes in a foil triple was 0. Thus, we             PARSER has already learned the chunks ABC, GHI, and
decided it was best to investigate how well the target triple        DEF, then the input would be ABC/DEF/GHI. In both
was learned, rather than looking at relative learning between        cases, the chunk ABC will be reinforced, increasing its
the target triple and a foil triple that was impossible for two      weight in memory. However, on the next step, the version
of the three models to have any sort of false confidence in.         with no chunks will see the input G (supposing that
                                                                     PARSER randomly chooses 1 unit as the input), and the
Results. PARSER and MDLChunker both showed better
                                                                     ABC chunk will decay slightly in memory. The version with
learning of the target triple in the four-triple condition than
                                                                     chunks will see ABC again, since it has already processed
in the one-triple condition (PARSER: t(38) = 2.79, p =
0.008; MDLChunker: t(38) = 3.14, p = 0.003). TRACX                   the first nine items in the sequence, reinforcing ABC even
showed equivalent performance in both conditions, t(38) =            further. When PARSER is able to chunk the input sequence,
0.05, p = 0.96. Figure 3 shows the distribution of model             it can process the input in fewer model steps, as shown by
outputs for each condition.                                          this toy example. This has the effect of accelerating the
   While both PARSER and MDLChunker matched the                      exposure rate of chunks. Since the decay rate of items in
direction of the effect, PARSER’s performance seems to               memory is fixed to the number of model steps, an individual
match the experimental data better. MDLChunker learned               chunk will experience less decay between successive
the target triple in every single run of the model, though the       presentations when the intermediate sequence is chunk-able.
average bit length was lower in the four-triple condition.           This process could equivalently be thought of as decreasing
PARSER showed greater variability: PARSER learned the                the decay rate of stored items when the incoming items are
target triple in only 11 of 20 runs in the one-triple condition,     chunks. PARSER, in essence, behaves like it has a longer
but in 19 of 20 runs in the four-triple condition. PARSER’s          lasting memory when the input sequence is chunk-able than
occasional lack of learning maps onto the forced-choice data         when it is not.
a bit more naturally than MDLChunker’s varying degrees of               MDLChunker ends up with functionally similar behavior,
learning. PARSER might genuinely predict uncertainty                 but through a different kind of memory limitation. In
between the target and foil triple when the target is not            MDLChunker, the minimum description length (MDL) is
learned, but MDLChunker always learned the target to some            calculated on a set of two components: the set of chunks the
degree.                                                              model has stored in its lexicon, and the input sequence
                                                                     coded in terms of the chunks in the lexicon. The bit length
                         Discussion                                  of an individual chunk depends on the relative frequency of
                                                                     that chunk in memory. In the one-triple condition, the
We presented results from an experiment designed to                  optimal encoding would be one triple and nine singletons,
investigate how the learning of a chunk is influenced by the         so the relative frequency of the triple will be, on average,
presence or absence of other chunk-able information. We              1/10. In the four-triple condition, the optimal encoding
found that a chunk was better learned when it was                    would be four triples, and the relative frequency of the
embedded in a sequence that was also chunk-able than when            target triple would be 1/4. Since the bit length of an
it was embedded in a more randomly generated sequence.               individual chunk depends on its frequency in memory, the
                                                                 534

bit length of the target chunk is smaller when the                       statistical word segmentation. Cognition, 117, 107–
surrounding sequence also contains chunks. If we take bit                125. doi:10.1016/j.cognition.2010.07.005
length to indicate relative strength of encoding, then the         French, R. M., Addyman, C., & Mareschal, D. (2011).
target chunk will have a stronger encoding in the four-triple            TRACX: a recognition-based connectionist
condition, due to an increase in relative frequency.                     framework for sequence segmentation and chunk
   TRACX, in contrast, has no explicit memory storage nor                extraction. Psychological Review, 118(4), 614–636.
any explicit forgetting parameter. TRACX also processes a                doi:10.1037/a0025255
sequence at a rate of one item per step regardless of              Gebhart, A. L., Aslin, R. N., & Newport, E. L. (2009).
previous learning. Memory constraints in TRACX will                      Changing structures in midstream: Learning along the
depend on interference in learning connection weights.                   statistical garden path. Cognitive Science, 33, 1087–
Thus, TRACX lacks the kind of mechanism that we                          1116. doi:10.1111/j.1551-6709.2009.01041.x
hypothesize might be responsible for the observed effect.          Gobet, F., Lane, P. C., Croker, S., Cheng, P. C. H., Jones,
   This interpretation of the model results makes a key                  G., Oliver, I., & Pine, J. M. (2001). Chunking
prediction: The reinforcement schedule necessary for                     mechanisms in human learning. Trends in Cognitive
successful chunk learning depends on the complexity,                     Sciences, 5(6), 236–243.
defined in terms of the perceiver’s internal                       Gobet, F., & Simon, H. A. (1998). Expert chess memory:
model/representation, of the information that is seen in                 revisiting the chunking hypothesis. Memory, 6(3),
between successive presentations of the chunk. When the                  225–255. doi:10.1080/741942359
information between successive presentations of a chunk is         Klahr, D., Chase, W. G., & Lovelace, E. A. (1983).
highly compressible, then less frequent presentations are                Structure and process in alphabetic retrieval. Journal
necessary to support chunk formation. However, when the                  of Experimental Psychology: Learning, Memory, and
information between sequences is unpredictable, then more                Cognition, 9(3), 462–477.
frequent presentations of the chunk are necessary in order         Kruschke, J. K. (2011). Doing Bayesian Data Analysis: A
for learning to take place. This prediction can be tested                Tutorial with R and BUGS (1st ed.). Academic Press.
empirically in future work.                                        Lany, J., & Gómez, R. L. (2008). Twelve-month-old infants
                                                                         benefit from prior experience in statistical learning.
                    Acknowledgments                                      Psychological Science, 19(12), 1247–1252.
This material is based on work that was supported by a             Lew-Williams, C., & Saffran, J. R. (2012). All words are
National Science Foundation Graduate Research Fellowship                 not created equal: Expectations about word length
under Grant No. DGE-1342962.                                             guide infant statistical learning. Cognition, 122, 241–
                                                                         246. doi:10.1016/j.cognition.2011.10.007
                                                                   Perruchet, P., & Pacton, S. (2006). Implicit learning and
                         References                                      statistical learning: one phenomenon, two approaches.
                                                                         Trends in Cognitive Sciences, 10(5), 233–238.
Aslin, R. N., Saffran, J. R., & Newport, E. L. (1998).                   doi:10.1016/j.tics.2006.03.006
       Computation of Conditional Probability Statistics by        Perruchet, P., & Vinter, A. (1998). PARSER: A Model for
       8-Month-Old Infants. Psychological Science, 9(4),                 Word Segmentation. Journal of Memory and
       321–324. doi:10.1111/1467-9280.00063                              Language, 39, 246–263. doi:10.1006/jmla.1998.2576
Chase, W. G., & Simon, H. A. (1973). Perception in chess.          Rissanen, J. (1978). Modeling by shortest data description.
       Cognitive Psychology, 61, 55–61.                                  Automatica, 14(5), 465–471.
de Leeuw, J. R. (2015). jsPsych: A JavaScript library for          Robinet, V., Lemaire, B., & Gordon, M. B. (2011).
       creating behavioral experiments in a Web browser.                 MDLChunker: A MDL-based cognitive model of
       Behavior Research Methods, 47(1), 1–12.                           inductive learning. Cognitive Science (Vol. 35, pp.
       doi:10.3758/s13428-014-0458-y                                     1352–1389). doi:10.1111/j.1551-6709.2011.01188.x
Fiser, J., & Aslin, R. N. (2001). Unsupervised statistical         Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
       learning of higher-order spatial structures from visual           Statistical learning by 8-month-old infants. Science,
       scenes. Psychological Science, 12(6), 499–504.                    274(5294), 1926–1928.
Fiser, J., & Aslin, R. N. (2002). Statistical learning of                doi:10.1126/science.274.5294.1926
       higher-order temporal structure from visual shape           Simon, H. A. (1974). How big is a chunk? Science, 183,
       sequences. Journal of Experimental Psychology:                    482–488. doi:10.1126/science.183.4124.482
       Learning, Memory, and Cognition, 28(3), 458–467.
       doi:10.1037/0278-7393.28.3.458
Frank, M. C., & Gibson, E. (2011). Overcoming Memory
       Limitations in Rule Learning. Language Learning and
       Development, 7, 130–148.
       doi:10.1080/15475441.2010.512522
Frank, M. C., Goldwater, S., Griffiths, T. L., & Tenenbaum,
       J. B. (2010). Modeling human performance in
                                                               535

