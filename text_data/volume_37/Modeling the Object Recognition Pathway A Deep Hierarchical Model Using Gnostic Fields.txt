     Modeling the Object Recognition Pathway: A Deep Hierarchical Model Using
                                                               Gnostic Fields
                                                    Panqu Wang (pawang@ucsd.edu)
                       Department of Electrical and Computer Engineering, University of California San Diego
                                                9500 Gilman Dr 0407, La Jolla, CA 92093 USA
                                                    Garrison Cottrell (gary@ucsd.edu)
                        Department of Computer Science and Engineering, University of California San Diego
                                                9500 Gilman Dr 0404, La Jolla, CA 92093 USA
                                               Christopher Kanan (ckanan@caltech.edu)
                                        Jet Propulsion Laboratory, California Institute of Technology
                                              4800 Oak Grove Drive, Pasadena, CA 91101, USA
                                Abstract                                  is aimed to model cognitive phenomena such as the develop-
                                                                          ment of hemispheric lateralization (Wang & Cottrell, 2013)
   To recognize objects, the human visual system processes in-
   formation through a network of hierarchically organized brain          and experience moderation effect of face and object recog-
   regions. Many neurocomputational models have modeled this              nition (Wang, Gauthier, & Cottrell, 2014), and the input is
   hierarchical structure, but they have often used hand-crafted          processed by a Gabor-PCA system, which is used as input to
   features to model early visual areas. According to the linear ef-
   ficient coding hypothesis, the goal of the early visual pathway        a multi-layer perceptron neural network.
   is to capture the statistical structure of sensory stimuli, remov-        One limitation of these models is that they all use hand-
   ing redundancy, and factoring the input into independent fea-
   tures. In this work, we use a hierarchical Independent Compo-          crafted features to simulate visual processing done by the
   nents Analysis (ICA) algorithm to automatically learn the vi-          retina, LGN, or primary visual cortex: DoG filters in Vis-
   sual features that account for early visual cortex. We then con-       Net, Gabor filters in TM and S1 units of HMAX, and the pre-
   tinue modeling the object recognition pathway using Gnostic
   Fields, a theory for how the brain does object categorization, in      set parameter of S2 units in HMAX. In mammals, this is not
   which brain regions devoted to classifying mutually-exclusive          how the early visual system develops its representations. It
   categories exist near the top of sensory processing hierarchies.       develops visual features as visual experience is acquired, and
   The whole biologically-inspired model not only allows us to
   develop representations similar to those in primary visual cor-        these visual representations are likely learned in a mostly un-
   tex, but also to perform well on standard computer vision ob-          supervised manner, since these features are universal across
   ject recognition benchmarks.                                           visual tasks. In this paper, we describe a model that uses hi-
   Keywords: object recognition; Independent Component Anal-              erarchical Independent Components Analysis (ICA) to learn
   ysis; Gnostic Fields; deep model
                                                                          a hierarchy of visual filters that can extract diagnostic visual
                                                                          features from images. To recognize objects, we combine the
                            Introduction
                                                                          learned ICA filters with gnostic fields, which simulates IT.
Over the years, researchers have built many models of ob-
ject recognition that are informed by findings in neuro-                     The ICA algorithm is an implementation of Barlow’s linear
science. Four of the best known models are the O’Reilly                   efficient encoding hypothesis (Barlow, 1961), which hypoth-
and Munakata model (OReilly & Munakata, 2000), HMAX                       esizes that the goal of early vision is to reduce the redundancy
(Riesenhuber & Poggio, 1999), VisNet (Wallis & Rolls,                     of the input, from which the statistical structure can be cap-
1996), and The Model (”TM”, Dailey and Cottrell (1999)). In               tured. One-layer ICA has been used to explain the very first
O’Reilly and Munakata, the visual input is first transformed              layer of information processing in the visual cortex (Bell &
by a center-surround transformation, followed by the pro-                 Sejnowski, 1997; Olshausen & Field, 1996). More recently,
cessing of V1, V2, V4 and PFC, with realistic neural con-                 Shan, Zhang, and Cottrell (2006) proposed a recursive im-
straints and increasingly large receptive fields. The HMAX                plementation of ICA, which captures the higher order struc-
model uses a hierarchical structure with alternating layers               ture. Our hierarchical model adopts the basic idea of recur-
of units that are selective for complex features (S units) and            sive ICA, but with different implementation.
units that have increasing tolerance to position and scale (C                ICA is not sufficient for the system to output labels for each
units), with the top layer of the hierarchy containing view-              category; supervised learning is needed. To do this, we use
tuned and task-related units that correspond to processing                Gnostic Fields, a state-of-the-art algorithm for object recog-
done by IT and PFC. VisNet shares a similar structure with                nition (Kanan, 2013a, 2014). Gnostic Fields are based on
HMAX, but the input is filtered by Differential of Gaussian               based on Konorski’s theory (Konorski, 1967) for how the
(DoG) filters before feeding into the 4 hierarchical competi-             brain recognizes objects. In this theory, Gnostic Fields are
tive network that correspond to V2, V4, PIT and AIT, and the              brain regions that exist near the top of the sensory informa-
synapse weights are learned using Hebbian learning rule. TM               tion processing hierarchy, and they are responsible for object
                                                                      2601

recognition. A gnostic field is composed of competing gnos-          Sejnowski, 1997) to decorrelate the stimulus and normalize
tic sets, with each set representing one category. Each gnostic      the variance. This transformation can be written as:
set contains multiple gnostic neurons, and they encode par-                                                            1
ticular properties of an object while maintaining a degree of                              WC = (Dc + δI)− 2 ΦTc ,                            (3)
invariance to scale, location, or appearance. fMRI studies
suggest that the brain does develop regions that are especially      where Φc contains the eigenvectors of the covariance matrix
active during object recognition tasks, such as the fusiform         of the input stimulus, Dc is the diagonal eigenvalue matrix,
face area (FFA) for face recognition (Kanwisher, McDermott,          and I is the identity matrix. The regularization parameter δ is
& Chun, 1997) and the visual word form area for recognizing          set to be 0.01 in all experiments.
words (McCandliss, Cohen, & Dehaene, 2003).                             As the neural signal S is assumed to be sparse and indepen-
   In the next sections, we give implementation details for our      dent, the filter response, which is the input for next layer’s
model, an analysis of visual filters learned by the model, re-       ICA, is not Gaussian. As suggested by Shan et al. (2006)
sults on benchmark computer vision datasets, and we discuss          and Kanan (2013a), we take the absolute value of the fil-
future directions for the model.                                     ter response and apply the cumulative distribution function
                                                                     (CDF) of the exponential distribution to the response to effi-
                            Methods                                  ciently increase the discriminative power of ICA filters. We
Figure 1 depicts the structure of our model, which consists          then whiten the filter response again to form the input for the
of image pre-processing inspired by the retina, visual filters       second layer ICA.
learned using hierarchical ICA, and then a gnostic field that           The above process is repeated again for the second layer
performs object classification.                                      ICA. This two-layer hierarchical structure can efficiently cap-
                                                                     ture the statistical property of the input stimulus. As the fea-
Image Preprocessing                                                  tures are learned gradually from the images, this process can
While most object recognition systems start from modeling            simulate the formation of the early visual areas in our brain,
with V1, we begin with pre-cortical processing done by the           which is believed to develop and mature in our early life and
retina. We first resize the input image so that its smallest di-     are shared components in the entire visual pathway.
mension is 128 pixels, with the other dimension resized to
preserve the aspect ratio. Subsequently, we convert the image        Gnostic Fields
from standard RGB (sRGB) color space to LMS colorspace               In our model, the unsupervised hierarchical ICA algorithm
(Fairchild, 2013), which simulates the retina’s long, medium,        simulates the early visual pathway. In order to model the
and short wavelength cone photoreceptors’ responses. We              full object recognition pathway, supervision is needed as we
then apply a cone-like nonlinearity to the LMS pixels, which         need information to distinguish between different object cat-
helps the model deal with changes in brightness (Caywood,            egories. We use Gnostic Fields to model the higher visual
Willmore, & Tolhurst, 2004). The formula we use is                   pathway. Below is a brief review of the necessary informa-
     0
                  
                     log(θ + 1) − log(IC (z) + θ)
                                                                    tion to implement a gnostic field. Please see Kanan (2013b)
    IC (z) = max                                  + 1, 0 , (1)       and Kanan (2014) for additional details.
                    (log(θ + 1) − log(θ))(θ − 1)
                                                                        The feature response vector for a given ICA layer (chan-
where IC (z) is the image for LMS channel C at location z. θ         nel) c and location t is gc,t . We then augment this descriptor
controls the normalization strength and θ = 0.01 in all of our       by adding a vector that contains the feature’s spatial infor-
experiments.                                                         mation: lc,t = [xt , yt , xt2 , yt2 , 1]T , where (xt , yt ) is the location
                                                                     of gc,t normalized by the dimension of the input image. lc,t
Hierarchical ICA
                                                                     is then also normalized, and the appended new vector ĝc,t is
The two central assumptions of hierarchical ICA are: 1) dif-         whitened by Wc , which is learned by a collection of training
ferent brain regions share similar anatomical structures and         images of the task. This whitening step can also be served
work under similar computational principles; 2) the input            as dimensionality reduction, and the whitened feature vectors
should follow generalized Gaussian distribution in order to          are made unit length. The final whitened and normalized fea-
let the statistical structure of the system be captured. In the      ture vector fc,t is given by
formulation of ICA, the observed data X is assumed to be
generated by underlying neural signal source S:                                                               Wc ĝc,t
                                                                                                 fc,t =                  .                    (4)
                           X = AS + ε,                       (2)                                            k Wc ĝc,t k
where A is the ICA basis matrix and ε is the Gaussian noise             In our model, a gnostic field for channel c is composed of
term.                                                                K gnostic sets, and each set represents one object category.
   In the first layer of ICA, in order to form a Gaussian-like       Each gnostic set contains a different number of gnostic units
input, the stimulus X has to be whitened. In the visual sys-         mk,c , which is given by
tem, the input is whitened in retina and LGN before trans-
mission to V1. Here we use whitened PCA (WPCA) (Bell &                          m(k, c) = min(db(log(nk,c ) + 1)2 e, nk,c ),                  (5)
                                                                 2602

Figure 1: A high-level description of our model. Hierarchical ICA learns features of lower visual areas, and the visual infor-
mation projects to gnostic sets, with units in the target gnostic set responding strongest. The competitive normalization step
suppresses non-relevant set responses, and the information for each feature adds to the model’s beliefs. The linear classifier
makes the final prediction using information from all categories and layers.
where nk,c is the total number of feature vectors from category            The next step is categorical evidence accumulation, which
k and channel c, and b regulates how many units learned in              simply sums the activation of βc,k,t across all time steps or
the category (here b = 10 in all experiments). Since the num-           locations of the input:
ber of feature vectors is directly proportional to the number
                                                                                                          T
of examples, the number of gnostic units increases logarith-
                                                                                                  ψc,k = ∑ βc,k,t ,                  (10)
mically in the number of training examples.                                                              t=1
    A gnostic set measures how similar the input feature vector
 fc,1 , ..., fc,T is to the previous examples (memory) of that cat-     and the evidence accumulated from all category and channels
egory. The output vector of the given category is generated             of the gnostic field forms the vector Ψ, which is made mean
by:                                                                     zero and unit length.
                         ac,k,t = max j (vc,k, j · fc,t ),      (6)        Finally, a linear multi-category classifier is used to decode
where vc,k, j denotes the weight vector for each gnostic unit           the activity of all units and deal with confused categories. The
 j in category k, and is learned by spherical k-means unsu-             model’s predicted category is given by k̃ = argmaxk wk · Ψ,
pervised clustering algorithm for unit length data (Dhillon &           where wk is the weighting vector of category k. In our ex-
Modha, 2001). The max operation is taken across all units in            periments, the weights were learned with the LIBLINEAR
the category, so we can treat it like a max pooling step, which         toolbox (Fan, Chang, Hsieh, Wang, & Lin, 2008) using multi-
enables the gnostic set to activate strongly to any stimuli that        class Support Vector Machine (SVM) formulation by Cram-
matches previous observations of that category.                         mer and Singer (Crammer & Singer, 2001), and the SVM cost
    The above ”max pooling” step is performed on every gnos-            parameter was set to be 0.0001.
tic set in the whole gnostic field. Subsequently, inhibitive               In our model, the gnostic field sits on top of the low-level
competition is used to suppress the response of least active            visual information processing produced by hierarchical ICA,
gnostic sets. To implement this on all K sets of channel c,             simulating the fact that the gnostic sets sit near top of a sen-
their outputs are first attenuated by half-way rectification, i.e.,     sory processing hierarchy (vision in our model), as hypoth-
                                                                        esized in Konorski’s Gnostic Fields theory. In general, our
                        qc,k,t = max(ac,k,t − θc,t , 0),        (7)     biological-inspired hierarchical model not only develops the
where θc,t = K1 ∑k0 ac,k0 ,t . The non-zero responses are then          low-level visual features, but also possesses the capability of
normalized using                                                        learning categorical information necessary to perform well in
                                                                        the high-level object recognition task. We show the results in
                              βc,k,t = νc,t · qc,k,t ,          (8)
                                                                        the next section.
where
                                       ∑k0 qc,k0 ,t                                               Experiments
                       νc,t =                              ,    (9)
                                (K + ∑k0 q2c,k0 ,t )3/2
                                    −1
                                                                        In this section, we will first analyze the ICA filters learned by
which acts as a form of divisive normalization and has been             hierarchical ICA, and then show the object recognition ex-
reported crucial in obtaining good object recognition accu-             periment results on computer vision datasets using the whole
racy in Kanan (2013b).                                                  model.
                                                                    2603

                                                                      Figure 3: Visualization of 6 randomly selected examples of
                                                                      second-layer ICA filters. Each sub-figure shows a filter with
Figure 2: The 600 basis function learned from first layer ICA.        the dimensions of top 6 × 6 entropy values. Dimension for
The learned Gabor-like filters share the three color opponency        each patch is 7 × 7.
characteristics (dark-light, yellow-blue, and red-green) of V1
neurons.                                                              marily respond to edges with different frequencies and ori-
                                                                      entations (a,c and f), and they show some degree of spatial
                                                                      invariance (same filter pattern appears at different location of
Feature Learning using Hierarchical ICA                               the patch), like V1 complex cells. In addition, there are also
Just like the early visual cortex matures by gradually per-           filters that correspond to contours (b and d), like the cells in
ceiving the surrounding environment, our model develops its           V2 do. In general, the second-layer ICA features are good
representation for unsupervised feature learning by using the         representation for higher layers of early visual cortex.
natural image dataset. We learned the ICA filters from 625
                                                                      Object Recognition Using Gnostic Fields
images from the Mcgill color image dataset (Olmos et al.,
2003). Each image was preprocessed using the method de-               Given the feature learned by Hierarchical ICA, we assessed
scribed in the previous section. For each image, 300 patches          the performance of our object recognition system using two
were randomly selected for 25 × 25 patch size. Prior to ICA,          major computer vision datasets: Caltech-101 (Li, Fergus, &
all patches were whitened using WPCA and the dimensional-             Perona, 2007) and Caltech-256 (Griffin, Holub, & Perona,
ity was reduced to 600, which preserves about 98.86% of the           2007). Caltech-101 dataset contains 9146 images of 101 dis-
total variance. Next, we learned the filters using the Efficient      tinct object categories (and one background). Caltech-256
Fast ICA algorithm (Koldovsky, Tichavsky, & Oja, 2006).               dataset is the successor of Caltech-101, and it contains a
The learned matrix A = [a1 , ..., an ]T consists of ICA basis row     broader number (256) of object categories and more varied
vector ai , and the learned filters are shown in Figure 2.            images, so the task is more difficult. All results below are re-
   From Figure 2, we can see a population of filters that re-         ported as the mean-class accuracy over five cross-validation
spond to both chromatic and achromatic features. All features         runs.
are Gabor-like, and they share all three color opponency char-            On Caltech-101 dataset, we analyzed the contribution of
acteristics of V1 neurons: dark-light, blue-yellow and red-           each component in our model to the overall performance
green (Caywood et al., 2004; Lee, Wachtler, & Sejnowski,              of object recognition task. Specifically, we aim to explore
2002).                                                                whether we will benefit from the hierarchical structure, and
   In the second layer of ICA, the filter responses were first        how much performance will boost using the gnostic field. The
processed by a 2 × 2 max pooling operation, which enables             results are shown in Figure 4.
the system to gain invariance for small translation. The patch            As can be seen from Figure 4, the recognition accuracy
size at this layer is 7 × 7, which make the visual feature            benefits a lot from adding the gnostic field. This is because if
learned in this layer account for a larger receptive field. Next,     learning only involves unsupervised approach, ICA features
the filter responses were processed through the non-linearity         themselves are not very discriminative, just like we cannot
and the dimensionality was reduced to 300 by WPCA. To vi-             recognize objects relying only on V1 and V2. On the other
sualize this layer’s features, we obtained the row vector for         hand, the performance of using two-layer ICA is better than
each ICA basis that can be reshaped to a 49 × 600 matrix,             using only one-layer ICA. This indicates that a hierarchical
then ranked each column of the matrix based on the entropy            system not only better models the early visual cortex, but also
of the learned filters. Examples of the learned second layer          generates the feature that is more discriminative than that a
ICA filters are shown in Figure 3.                                    single-layer model, although both are unsupervised. Consid-
   From Figure 3, we can see that the second layer filters pri-       ering the “deep” models, no matter unsupervised (Le, 2013)
                                                                  2604

Figure 4: Mean per-class accuracy on Caltech-101 dataset
using different model structures. Left: Recognition accuracy
as a function of number of training instances. Right: Com-
parison of using only one-layer ICA and two-layer ICA, with
different model structure: 1) ICA: gather the filter reponse for
each image, downsample to uniform size, vectorize and apply          Figure 5: Mean per-class accuracy of Caltech-256 dataset
SVM as classifier; 2) ICA+GF: Apply gnostic field on top of          as a function of number of training images. Griffin et al.
ICA, use the naive argmax classifier; 3) ICA+GF+SVM: re-             (2007) provides baseline results. Deep convolutional neural
place argmax by SVM.                                                 networks do not perform well when trained solely on Caltech-
                                                                     256, because they overfit without additional training on sig-
or fully supervised (Krizhevsky, Sutskever, & Hinton, 2012),
                                                                     nificantly larger datasets, e.g., ImageNet (Zeiler & Fergus,
they all benefit from the rich representation that the inter-
                                                                     2014). Sohn et al. (2011) used features learned from a convo-
mediate layer could develop, which helps the model reach
                                                                     lutional RBM. Kanan (2013a) used CSIFT features and multi-
very good object recognition performance. Finally, we can
                                                                     channel gnostic fields, and we achieved almost the same ac-
observe that combining information of both ICA layers and
                                                                     curacy. Chance is 0.0039.
adding linear classifier generates the best performance, which
predicts that the categorizer in IT or PFC may need the in-          respectively. We learned V1 and V2-like filters automatically
formation from all previous visual layers to make the more           from natural images using a hierarchical ICA algorithm, that
reliable decision.                                                   simulates the development and maturation of early visual cor-
   On Caltech-256 dataset, using the same settings on                tex. With Gnostic fields, we achieve good performance on
Caltech-101 dataset, we measured the mean per-class accu-            two object recognition tasks. We suggest that the Gnostic
racy of up to 50 test images using 15, 30, 45, 60 training           field models the categorization process in areas IT and PFC.
images, respectively. The results, as well as the comparison         Overall, our biologically-inspired model provides an end-to-
with other methods, are shown in Figure 5.                           end model of the human object recognition pathway.
   From Figure 5, we can see that our model’s performance is            Recently, deep convolutional neural networks (CNNs)
very competitive when compared with other methods from               have emerged as a powerful machine learning tool for ob-
computer vision. Our model achieves the recognition ac-              ject classification, particularly when millions of labeled im-
curacy of 51.8% when using 60 training images, and out-              ages are available (Krizhevsky et al., 2012). A deep CNN has
performs all other methods mentioned in the figure. The              multiple convolutional layers followed by multiple fully con-
closest performance is achieved by Kanan (2013a), but they           nected layers. In our model, ICA layers are stacked twice, and
used manually-designed CSIFT feature, as opposed to our              the Gnostic Fields serve as the classification layer. One no-
biologically-inspired learned ICA features. One thing to note        table difference between our model and deep networks is that
is that for deep convolutional neural network, using a pre-          our model is composed of both unsupervised and supervised
trained network on big dataset usually yields a better perfor-       learning, while state-of-the-art CNNs are fully supervised.
mance than training on Caltech-256 alone. This suggests that
                                                                        One question is whether more layers of ICA will help or
our hierarchical model is more competent on learning from a
                                                                     not. The learning method is highly computationally inten-
medium-sized dataset.
                                                                     sive, so we were unable to add a third layer of ICA process-
                                                                     ing. It is possible that more invariance would arise in a deeper
                          Discussion                                 unsupervised network. The question is whether the loss of in-
In this paper, we proposed a biologically-inspired deep hier-        formation through dropping the sign and spatial pooling will
archical model for visual object recognition. We combined            lead to beneficial invariants being learned. Also, it is possi-
unsupervised feature learning (hierarchical ICA) and a super-        bly the case that deeper layers, which are closer to the tem-
vised learning algorithm (Gnostic Fields) to account for the         poral pole, receive more category information, so that using
processing of early visual cortex and higher visual pathways,        strictly unsupervised learning might not be appropriate. In
                                                                 2605

future work, we would like to apply this model to video to in-      Konorski, J. (1967). Integrative activity of the brain.
vestigate whether it can learn useful spatiotemporal features         Chicago.
and achieve good performance in tracking and activity recog-        Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-
nition.                                                               agenet classification with deep convolutional neural net-
                                                                      works. In Advances in Neural Information Processing Sys-
                    Acknowledgments                                   tems (pp. 1097–1105).
This work was supported in part by NSF Science of Learning          Le, Q. V. (2013). Building high-level features using large
Center grants SBE-0542013 and SMA-1041755 to the Tem-                 scale unsupervised learning. In Acoustics, Speech and Sig-
poral Dynamics of Learning Center, NSF grant IIS-1219252              nal Processing (ICASSP), 2013 IEEE International Con-
to GWC. PW was supported by a fellowship from Hewlett-                ference on (pp. 8595–8598).
Packard. This work was initiated while CK was affiliated with       Lee, T.-W., Wachtler, T., & Sejnowski, T. J. (2002). Color op-
UCSD.                                                                 ponency is an efficient representation of spectral properties
                                                                      in natural scenes. Vision Research, 42(17), 2095–2103.
                         References                                 Li, F., Fergus, R., & Perona, P. (2007). Learning generative
Barlow, H. B. (1961). Possible principles underlying the              visual models from few training examples: An incremental
  transformation of sensory messages. Sensory Communica-              bayesian approach tested on 101 object categories. Com-
  tion, 217–234.                                                      puter Vision and Image Understanding, 106(1), 59–70.
Bell, A. J., & Sejnowski, T. J. (1997). The independent com-        McCandliss, B. D., Cohen, L., & Dehaene, S. (2003). The
  ponents of natural scenes are edge filters. Vision Research,        visual word form area: expertise for reading in the fusiform
  37(23), 3327–3338.                                                  gyrus. Trends in Cognitive Sciences, 7(7), 293–299.
Caywood, M. S., Willmore, B., & Tolhurst, D. J. (2004).             Olmos, A., et al. (2003). A biologically inspired algorithm for
  Independent components of color natural scenes resemble             the recovery of shading and reflectance images. Perception,
  v1 neurons in their spatial and color tuning. Journal of            33(12), 1463–1473.
  Neurophysiology, 91(6), 2859–2873.                                Olshausen, B. A., & Field, D. J. (1996). Emergence of
Crammer, K., & Singer, Y. (2001). On the algorithmic im-              simple-cell receptive field properties by learning a sparse
  plementation of multiclass kernel-based vector machines. J          code for natural images. Nature, 381, 607–609.
  Machine arning Research, 2, 265-292.                              OReilly, R., & Munakata, Y. (2000). Neurocomputational
Dailey, M. N., & Cottrell, G. W. (1999). Organization of face         models of face processing. In Computational Explorations
  and object recognition in modular neural network models.            in Cognitive Neuroscience. Cambridge, MA: MIT Press.
  Neural Networks, 12, 1053–1073.                                   Riesenhuber, M., & Poggio, T. (1999). Hierarchical models
Dhillon, I. S., & Modha, D. S. (2001). Concept decompo-               of object recognition in cortex. Nature Neuroscience, 2,
  sitions for large sparse text data using clustering. Machine        1019–1025.
  learning, 42(1-2), 143–175.                                       Shan, H., Zhang, L., & Cottrell, G. (2006). Recursive ICA.
Fairchild, M. D. (2013). Color appearance models. John                In Advances in Neural Information Processing Systems (pp.
  Wiley & Sons.                                                       1273–1280).
Fan, R., Chang, K., Hsieh, C., Wang, X., & Lin, C. (2008).          Sohn, K., Jung, D. Y., Lee, H., & Hero, A. O. (2011). Effi-
  LIBLINEAR: A library for large linear classification. J             cient learning of sparse, distributed, convolutional feature
  Machine Learning Research, 9, 1871-1874.                            representations for object recognition. In Computer Vision,
Griffin, G., Holub, A., & Perona, P. (2007). Caltech-256              2011 IEEE International Conference on (pp. 2643–2650).
  object category dataset.                                          Wallis, G., & Rolls, E. T. (1996). A model of invariant object
Kanan, C. (2013a). Active object recognition with a space-            recognition in the visual system. Prog. Neurobiol, 51, 167–
  variant retina. ISRN Machine Vision, 2013, 138057.                  194.
Kanan, C. (2013b). Recognizing sights, smells, and sounds           Wang, P., & Cottrell, G. W. (2013). A computational model
  with gnostic fields. PLoS ONE, e54088. doi: 10.1371/jour-           of the development of hemispheric asymmetry of face pro-
  nal.pone.0054088                                                    cessing. In Proceedings of the 35th Annual Conference of
Kanan, C. (2014). Fine-grained object recognition with gnos-          the Cognitive Science Society. Austin, TX: Cognitive Sci-
  tic fields. In Proceedings of the IEEE Winter Applications          ence Society.
  of Computer Vision Conference (WACV-2014).                        Wang, P., Gauthier, I., & Cottrell, G. (2014). Experience
Kanwisher, N., McDermott, J., & Chun, M. M. (1997). The               matters: Modeling the relationship between face and object
  fusiform face area: a module in human extrastriate cor-             recognition. In Proceedings of the 36th Annual Conference
  tex specialized for face perception. The Journal of Neu-            of the Cognitive Science Society. Austin, TX: Cognitive
  roscience, 17(11), 4302–4311.                                       Science Society.
Koldovsky, Z., Tichavsky, P., & Oja, E. (2006). Efficient vari-     Zeiler, M. D., & Fergus, R. (2014). Visualizing and un-
  ant of algorithm fastica for independent component analy-           derstanding convolutional networks. In Computer Vision–
  sis attaining the cramer-rao lower bound. Neural Networks,          ECCV 2014 (pp. 818–833). Springer.
  IEEE Transactions on, 17(5), 1265–1277.
                                                                2606

