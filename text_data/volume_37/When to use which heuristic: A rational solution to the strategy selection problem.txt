 When to use which heuristic: A rational solution to the strategy selection problem
            Falk Lieder (falk.lieder@berkeley.edu)                   Thomas L. Griffiths (tom griffiths@berkeley.edu)
                   Helen Wills Neuroscience Institute                                  Department of Psychology
           University of California at Berkeley, CA, USA                 University of California at Berkeley, CA, USA
                                Abstract                                     This paper proposes a rational solution to the strategy se-
                                                                          lection problem: in analogy to model-based reinforcement
   The human mind appears to be equipped with a toolbox full
   of cognitive strategies, but how do people decide when to use          learning (Dolan & Dayan, 2013) our theory posits that peo-
   which strategy? We leverage rational metareasoning to derive           ple learn a mental model that enables them to predict each
   a rational solution to this problem and apply it to decision mak-      heuristic’s accuracy and execution time from features of the
   ing under uncertainty. The resulting theory reconciles the two
   poles of the debate about human rationality by proposing that          problem to be solved and choose the heuristic with the best
   people gradually learn to make rational use of fallible heuris-        predicted speed-accuracy tradeoff.
   tics. We evaluate this theory against empirical data and exist-           In the remainder of this paper we first review previous
   ing accounts of strategy selection (i.e. SSL and RELACS). Our
   results suggest that while SSL and RELACS can explain peo-             strategy selection theories and introduce our new theory of
   ple’s ability to adapt to homogeneous environments in which            strategy-selection. We then test our theory against those pre-
   all decision problems are of the same type, rational metarea-          vious accounts by fitting published data on multi-attribute de-
   soning can additionally explain people’s ability to adapt to het-
   erogeneous environments and flexibly switch strategies from            cision making, conducting a novel experiment, and demon-
   one decision to the next.                                              strating that our theory can account for people’s adaptive flex-
   Keywords: Strategy Selection; Decision Making; Heuristics;             ibility in risky choice. We close with a discussion of the im-
   Bounded Rationality; Cognitive Control; Learning                       plications of our results for the debate about human rational-
                                                                          ity and directions for future research.
                           Introduction
Many of our decisions and judgments systematically violate                                 Models of Strategy Selection
the laws of logic and probability theory (Tversky & Kahne-                According to previous theories of strategy selection we learn
man, 1974). These violations are known as cognitive biases.               to choose the strategy that works best on average across all
Cognitive biases have been interpreted as evidence that peo-              problems in an environment (Rieskamp & Otto, 2006; Erev &
ple do not reason by the rules of logic and probability theory            Barron, 2005) or category (Shrager & Siegler, 1998). This ap-
but by simple yet fallible heuristics. Whether or not these               proach ignores that every problem has distinct characteristics
findings prove that humans are irrational has been debated                that determine the strategies’ effectiveness. After reviewing
for decades (Stanovich, 2009). While some view heuristics                 these context-free theories, we propose a model that chooses
and biases as a sign of human irrationality, recent work sug-             strategies based on the features of individual problems.
gests that some heuristics can be understood as rational strate-
                                                                          Context-free strategy selection learning
gies once computational costs are taken into account (Lieder,
Griffiths, & Goodman, 2013; Griffiths, Lieder, & Goodman,                 According to the SSL model (Rieskamp & Otto, 2006) the
2015; Lieder, Hsu, & Griffiths, 2014). Furthermore, Gigeren-              probability that strategy s will be chosen (P(St = s)) in trial t
zer and colleagues argue that having a toolbox of simple                  is proportional to its reward expectancy qi :
heuristics that are well adapted to the structure of our envi-
                                                                                                    P(S = s) ∝ qt (s),                   (1)
ronment makes us smart (Gigerenzer & Todd, 1999).
   Yet, being a skilled carpenter requires more than a tool-              where qt (k) is the sum of the rewards obtained when strategy
box: you also have to know when to use which tool. Todd and               k was chosen prior to trial t plus the initial reward expectancy
Gigerenzer (2012) postulate that we choose heuristics that are
well-adapted to our current situation (i.e. ecologically ratio-                                   q0 (k) = rmax · w · βk ,               (2)
nal), but they do not explain how we are able to do so. Empir-
ical evidence suggests that people do indeed choose heuristics            where rmax is the highest possible reward, w is the strength
adaptively (Payne, Bettman, & Johnson, 1988; Bröder, 2003;               of the initial reward expectancy, and β1 , · · · , βN ∈ [0, 1] are
Pachur, Todd, Gigerenzer, Schooler, & Goldstein, 2011). De-               the agent’s initial relative reward expectancies for strategies
spite some progress, the computational principles of strategy             1, · · · , N and sum to one.
selection remain unclear (Marewski & Link, 2014). Previ-                     The RELACS model (Erev & Barron, 2005) chooses strate-
ous theories of strategy selection, namely SSL (Rieskamp &                gies according to their recency-weighted average payoffs
Otto, 2006), RELACS (Erev & Barron, 2005), and SCADS                                             (
(Shrager & Siegler, 1998) predict the formation of rigid men-                                     α · rt + (1 − α) ·Wt (k) if St = k
                                                                                      Wt+1 (k) =                                         (3)
tal habits that always pursue the same strategy, whereas peo-                                     Wt+1 (k) = Wt (k)        else
ple are more flexible (Lieder, Plunkett, et al., 2014; Payne et                                       Wt (k)
al., 1988).                                                                           P(St = k) ∝ eλ·  Vt                                (4)
                                                                      1362

where the parameters α and λ determine the agent’s learn-              sampled value (ties are broken at random). This proposal is
ing rate and decision noise respectively, and Vt is the agent’s        line with behavioral (Vul, Goodman, Griffiths, & Tenenbaum,
current estimate of the payoff variability.                            2014) and neural evidence (Fiser, Berkes, Orbán, & Lengyel,
   The SCADS model (Shrager & Siegler, 1998) presupposes               2010) for sampling as a cognitive mechanism.
that each problem has been identified as an instance of one or
more problem types and assumes associative learning mech-              Learning when to use fast-and-frugal heuristics
anisms similar to those of SSL.
                                                                       Fast-and-frugal heuristics perform very few computations
Feature-based strategy selection learning                              and use only a small subset of the available information
In this section, we present a theory according to which people         (Gigerenzer & Gaissmaier, 2011). For instance, the Take-
learn a mental model predicting the effectiveness of cognitive         the-Best (TTB) heuristic for multi-attribute decision making
strategies from features of the problem to be solved.                  chooses the option with the highest value on the most pre-
   Strategy selection is a metacognitive decision with uncer-          dictive attribute that distinguishes the options and ignores
tain consequences. We therefore leveraged rational metarea-            all other attributes. This strategy works in so-called non-
soning – a decision-theoretic framework for choosing com-              compensatory environments in which the attributes’ predic-
putations (Russell & Wefald, 1991) – to develop a rational             tive validities fall off so rapidly that the recommendation of
model of strategy selection that is theoretically sound, com-          the most predictive attribute cannot be overturned by ratio-
putationally efficient, and competitive with state-of-the-art al-      nally incorporating other attributes. Yet it can fail miserably
gorithm selection methods (Lieder, Plunkett, et al., 2014).            in compensatory environments in which no single attribute
   Rational metareasoning chooses the strategy s? with the             reliably identifies the best choice by itself.
highest value of computation (VOC) for the problem speci-                 Bröder (2003) found that people use Take-the-Best more
fied by input i:                                                       frequently in non-compensatory environments than in com-
                                                                       pensatory environments. Rieskamp and Otto (2006) con-
                     s? = arg max VOC(s, i),                   (5)     ducted an experiment suggesting that this adaptation might
                                s∈S
                                                                       result from reinforcement learning: Two groups of partici-
where S is the set of the agent’s cognitive strategies. The            pants made 168 multi-attribute decisions with feedback in a
VOC of executing a cognitive strategy s is the expected net in-        compensatory versus a non-compensatory environment. Over
crease in utility over acting without deliberation. If the strat-      time, the choices of participants in the non-compensatory en-
egy chooses an action and the utility of the available actions         vironment became more consistent with TTB, whereas the
remains approximately constant while the agent deliberates,            choices of participants in the compensatory environment be-
then the VOC can be approximated by the expected reward                came less consistent with TTB and more consistent with
of the resulting action minus the opportunity cost of the strat-       the weighted-additive strategy (WADD) that computes the
egy’s execution time T :                                               weighted average of all attributes.
                                                                          These findings raise the question how people learn when
             VOC(s; i) ≈ E [R|s, i] − E [TC(T )|s, i] ,        (6)
                                                                       to use TTB. This problem could be solved either by learn-
where R is the increase in reward and TC(T ) is the opportu-           ing how well TTB works on average, as postulated by SSL
nity cost of running the algorithm for T units of time. The re-        and RELACS, or by learning to predict the performance of
ward R can be binary (correct vs. incorrect output) or numeric         TTB and alternative strategies for individual problems as sug-
(e.g., the payoff). Equations 5-6 reveal that near-optimal             gested by rational metareasoning.
strategy selection can be achieved by learning to predict the             As a first test of our model we demonstrate that it can ex-
strategies’ expected rewards and execution times from fea-             plain the findings of Experiment 1 from Rieskamp and Otto
tures f(i) of the input i that specifies the problem to be solved.     (2006). This experiment was structured into seven blocks
These predictions can be learned by Bayesian linear or logis-          comprising 24 trials each. In each trial participants chose
tic regression as described in Lieder, Plunkett, et al. (2014).        between two investment options based on five binary at-
   Equation 5 is optimal when the VOC is known, but when               tributes whose predictive validities were constant and explic-
the VOC is unknown the value of exploration should not                 itly stated. To apply our general rational metareasoning the-
be ignored. To remedy this problem, we employ Thomp-                   ory to multi-attribute decision making, we manually selected
son sampling (Thompson, 1933) – a near optimal solution                a small set of simple features that are highly informative
to the exploration-exploitation dilemma (May, Korda, Lee, &            about the relative performance of TTB versus WADD: The
Leslie, 2012). Concretely, each strategy s is chosen (S = s)           first feature predicts the performance of TTB by the validity
according to the probability that its VOC is maximal:                  of the most reliable discriminative attribute ( f1 ), the second
                                                                     and the third feature measure the potential for WADD to per-
            P(S = s) ∝ P s = arg max VOC(s; i) .               (7)     form better than TTB by the gap between the validity of the
                                        s
                                                                       most reliable attribute favoring the first option and the most
This is implemented by drawing one sample from each strat-             reliable attribute favoring the second option ( f2 ) respectively
egy’s VOC model and picking the strategy with the highest              the absolute difference between the number of attributes fa-
                                                                   1363

voring the first option and the second option respectively                                                      Rational Metareasoning
                                                                                                      0.8
( f3 ). The simulated agent’s toolbox contained two strategies:
                                                                                                      0.7
Take-The-Best (s1 = TTB) and the weighted-additive strat-
egy (s2 = WADD). The probability that strategy s leads to                                             0.6
                                                                              Prob. of choosing TTB
the correct decision was modeled by                                                                   0.5                       Noncompensatory Env. (RM)
                                                                                                                                Noncompensatory Env. (People)
                                     1                                                                0.4                       Noncompensatory Env. (SSL)
             P(R = 1|s) =                          .         (8)                                                                Compensatory Env. (RM)
                            1 + exp(−f · αs + bs )                                                    0.3                       Compensatory Env. (People)
                                                                                                                                Compensatory Env. (SSL)
                                                                                                      0.2
To accommodate people’s prior knowledge about the strate-
gies’ performance we parameterized its prior distribution by                                          0.1
                                                                                                       0
              P(αs ) = N µ = 0, Σ−1 = τ · I
                                            
                                                          (9)                                           1   2     3        4
                                                                                                                      Block Number
                                                                                                                                     5        6         7
                                   (s)
                P(bs ) = N (µ   = b0 , σ−2   = τ).          (10)
                                                                      Figure 1: Rational metareasoning explains experimental find-
where b0 and τ are free parameters and 0 and I are the zero           ings by Rieskamp and Otto (2006).
vector and the identity matrix respectively.
   We created compensatory and non-compensatory environ-
                                                                      iment is needed to determine if strategy selection learn-
ments similar to those used by Rieskamp and Otto (2006): In
                                                                      ing is context-free as postulated by SSL and RELACS or
the non-compensatory environment TTB always makes the
                                                                      feature-based as postulated by rational metareasoning. We
Bayes-optimal decision, and in the compensatory environ-
                                                                      thus investigated under which conditions rational metar-
ment WADD always makes the Bayes-optimal decision. In
                                                                      easoning predicts different strategy choices than SSL and
both environments TTB and WADD make the same decision
                                                                      RELACS: We evaluated the performance of context-free ver-
on exactly half of the trials. To determine the Bayes-optimal
                                                                      sus feature-based strategy selection learning in 11 environ-
decision and generate payoffs, we computed the probability
                                                                      ments with p ∈ {0%, 10%, 20%, · · · , 100%} compensatory
that option A is superior to option B by Bayesian inference
                                                                      problems (in which WADD makes the right and TTB makes
under the assumption that their order is uninformative and
                                                                      the wrong decision) and 1 − p ∈ {100%, 90%, 80%, · · · , 0%}
that positive and negative ratings are equally common. Pay-
                                                                      non-compensatory problems (in which TTB makes the right
offs were sampled from the posterior distribution given the
                                                                      and WADD makes the wrong decision). Each environment
attributes’ values and validities. Furthermore, we assumed
                                                                      comprised 168 decision problems in random order. We eval-
that the participants’ opportunity cost c corresponded to $5
                                                                      uated the performance of rational metareasoning with b0 = 0
per hour at 1 computation per second. Since the initial bias
                                                                      and τ = 1, SSL with β1 = β2 = 0.5 and w = 1, and RELACS
in strategy selection only depends on the difference between
                                                    (TTB)             with α = 0.1 and λ = 1. These parameters correspond to a
the prior beliefs about the two strategies, we set b0     to zero     weak bias towards using both strategies equally often, but this
               (WADD)      (TTB)
and fit ∆b0 = b0       −b0      and τ to the average frequency        is not critical since any bias is eventually overwritten by expe-
with which Rieskamp and Otto’s participants used TTB ver-             rience. Our simulations revealed that the variance p · (1 − p)
sus WADD in each block by minimizing the mean squared er-             of the problems’ compensatoriness has qualitatively different
ror using grid search. For each grid point we simulated trial-        effects on the performance of feature-based versus context-
by-trial learning and decision making and averaged the choice         free strategy selection learning; see Figure 2. Concretely, the
frequencies within each block across 1000 simulations. The            performance of context-free strategy selection learning drops
resulting parameter estimates were ∆b̂0 = 0.14 and τ = 80.            rapidly with the variance in the environment’s compensatori-
    We found that rational metareasoning can explain people’s         ness: As the ratio of compensatory to non-compensatory
ability to adapt to compensatory as well as non-compensatory          problems approaches 50/50 the performance of SSL and
environments (see Figure 1): When the environment was non-            RELACS drops to the chance level. The performance of ra-
compensatory rational metareasoning learned to use TTB, but           tional metareasoning, by contrast, is much less susceptible to
when the environment was non-compensatory rational metar-             the variance and stays above 70%. The reason for this dif-
easoning learned to avoid TTB and use WADD instead. Our               ference is that rational metareasoning learns to use TTB for
simulation results show that rational metareasoning captured          non-compensatory problems and WADD for compensatory
that people gradually adapt their strategy choices to the deci-       problems whereas SSL and RELACS learn to always use the
sion environment. The fits of rational metareasoning and the          same strategy. Rational metareasoning outperforms RELACS
fit of SSL reported by Rieskamp and Otto (2006) were about            across all environments. In purely (non)compensatory envi-
equally good (MSE: 0.0050 vs. 0.0048; see Figure 1).                  ronments SSL and rational metareasoning perform equally
                                                                      well, but as the environment becomes variable the perfor-
    Strategy selection in mixed environments                          mance of SSL drops below the performance of rational metar-
Since both SSL and rational metareasoning can explain                 easoning. Thus context-free and feature-based strategy selec-
the results of Rieskamp and Otto (2006), a new exper-                 tion make qualitatively different predictions about people’s
                                                                   1364

performance in mixed environments. We can therefore de-
termine if people use context-free or feature-based strategy                                                 100
                                                                                                                                 Rational Metareasoning
                                                                                                                                 SSL
                                                                                                                                 RELACS
selection by measuring their performance in a mixed envi-                                                                        People
                                                                          Optimal Choices in % with 95% CI
ronment with the following experiment:                                                                        90
Methods                                                                                                       80
We recruited 120 participants on Amazon Mechanical Turk.                                                      70
Each participant was paid 50 cents for about five minutes of
work. The experiment comprised 30 binary decisions. Par-                                                      60
ticipants played the role of a banker deciding which of two
companies receives a loan based on the companies’ ratings                                                     50
on six criteria. The criteria and their success probabilities                                                      0   20        40          60           80   100
                                                                                                                            Compensatory Trials in %
were the same as in Rieskamp and Otto’s first experiment.
On each trial, the two companies’ ratings on these criteria
were presented in random order, and the criterias’ validities     Figure 2: Rational metareasoning outperforms SSL and
were stated explicitly. After choosing company A or com-          RELACS–especially when the environment is heterogeneous.
pany B participants received stochastic binary feedback gen-
erated according to the cue validities. The relative frequency    trials in a simple environment, whereas our participants per-
of positive feedback was 75.96% following the correct re-         formed only 30 trials and the environment was very complex.
sponse and 25.04% following the incorrect response. The           Future experiments will employ a larger number of trials and
decision problems were chosen such that TTB and WADD              more reliable feedback to explore whether people learn in the
make opposite decisions on every trial. In half of the trials,    mixed decision environment too. In conclusion, people’s per-
the decision of TTB was correct and in half of the trials the     formance in homogeneous decision environments is consis-
decision of WADD was correct. Thus, always using TTB,             tent with feature-based and context-free strategy selection,
always using WADD, choosing one of the two strategies at          but context-free strategy selection is insufficient to explain
random, or context-free strategy selection would result in an     human performance in heterogeneous environments whereas
accuracy of 50%; see Figure 2.                                    feature-based strategy selection can account for it. Thus our
Results and Discussion                                            results suggest that human strategy selection is feature-based.
People chose the more creditworthy company in 64.6% of
                                                                       Adaptive flexibility in strategy selection
the trials. Assuming a uniform prior on people’s perfor-
mance, the 99% highest-posterior density credible interval        People adapt their strategy not only to reoccurring situations,
is [62.5%; 66.6%]. We can thus be more than 99% confi-            but they can also flexibly switch strategies as soon as the sit-
dent that people’s average performance is above 62.5% and         uation changes. This flexibility has been empirically demon-
conclude that they performed significantly better than chance     strated in decision making under risk: Payne et al. (1988)
(p < 10−15 ). This is qualitatively consistent with feature-      found that people adaptively switch decision strategies in the
based strategy selection but inconsistent with context-free       absence of feedback.
strategy selection; see Figure 2. Consistent with Rieskamp           To determine whether rational metareasoning can account
and Otto (2006) performed worse on non-compensatory tri-          for this adaptive flexibility we simulated Experiment 1 from
als than on compensatory trials. Thus the deviation from op-      Payne et al. (1988). This experiment presented ten instances
timal performance is not due to saving mental effort by using     of each of four types of decision problems in random order.
the simpler TTB strategy when the more demanding WADD             The four problem types were defined by the time constraint
strategy would be required. To measure learning we regressed      (15 seconds vs. none) and the dispersion of the outcomes’
our participants’ average performance on the trial number and     probabilities (low vs. high). In each decision problem par-
a constant. We found that the increase in our participants’       ticipants chose between four gambles with four possible out-
performance was not statistically significant (95% CI of the      comes. The four gambles assigned different payoffs to the
slope: [−0.1%, 0.2%]). The observation that participants nev-     four outcomes but they shared the same outcome probabil-
ertheless performed above chance, suggests that they entered      ities. The payoffs (range 0–999 cents) and their probabili-
the experiment with moderately high strategy selection skills.    ties were stated numerically. To see an outcome probability
Alternatively, participants could have used a single, more        or payoff participants had to click on the corresponding out-
complex strategy that works for both kinds of problems, but       come. This allowed Payne et al. (1988) to infer which kind
note that with a larger number of trials systematic changes in    of strategies their participants were using. They measured the
strategy use have been demonstrated in a similar task; see Fig-   use of fast-and-frugal attribute-based heuristics, that is TTB
ure 1. The absence of evidence for learning is not necessarily    and elimination-by-aspects (EBA; Tversky (1972)), by the
incompatible with previous findings, because Rieskamp and         proportion of time participants spend processing the options’
Otto (2006) found small changes in performance after 168          payoffs for the most probable outcome. For the compensatory
                                                               1365

                                                                                                                                     People                                                                         Rational Metareasoning
expected value strategy WADD this proportion is only 25%,                                                            50                                                                                       60
                                                                                                                                                             Predicted Use of TTB or EBA in % (with 95% CI)
                                                                                                                              No Time Pressure
but for TTB and EBA it can be up to 100%. When the dis-                                                                       High Time Pressure
                                                                             Time on Most Important Attribute in %
                                                                                                                     45                                                                                       50
persion of outcome probabilities was high, people focused
                                                                                                                     40                                                                                       40
more on the most probable outcome. Time pressure also in-
creased people’s propensity for such selective and attribute-                                                        35                                                                                       30
based processing; see Figure 3. Thus, people seem to use                                                             30                                                                                       20
non-compensatory strategies such as TTB and EBA more fre-
                                                                                                                     25                                                                                       10
quently when time is limited or some outcomes are much
more probable than others.                                                                                           20
                                                                                                                          Low Dispersion   High Dispersion
                                                                                                                                                                                                              0
                                                                                                                                                                                                                   Low Dispersion   High Dispersion
   To simulate this experiment we applied rational metarea-
soning to choosing when to use which of the ten strategies
considered by Payne et al. (1988). These strategies included        Figure 3: Rational metareasoning predicts the increase in se-
WADD as well as fast-and-frugal heuristics such as TTB and          lective attribute-based processing with dispersion and time
EBA. To simulate the effect of the time limit we counted            pressure observed by Payne et al. (1988).
each strategy’s elementary operations according to Johnson
and Payne (1985), assumed that each of them takes one sec-             Rational metareasoning correctly predicted that time-
ond, and returned the strategy’s current best guess when it         pressure and probability dispersion increase people’s propen-
exceeded the limit as described by Payne et al. (1988). Ra-         sity to use TTB or EBA; see Figure 3. SSL and RELACS, by
tional metareasoning represented each risky-choice problem          contrast, predict that there should be no difference between
by five simple and easily computed features: the number             the four conditions. This is because SSL and RELACS can-
of attributes, the number of options, the number of inputs          not learn to choose different strategies for different kinds of
per available computation, the highest outcome probability,         problems. Their strategy choices only change in response to
and the difference between the highest and the lowest pay-          reward or punishment but the experiment provided neither.
off. These features were manually chosen to capture highly          In conclusion, rational metareasoning can account for adap-
predictive dimensions along which decision problems were            tive flexibility in decision making under risk but SSL and
varied by Payne et al. (1988). Our rational metareasoning           RELACS cannot.
model of strategy selection in risky choice learns to predict
each strategy’s relative reward                                                                                                      General Discussion
                                                                    We have proposed a rational solution to the strategy selection
                                 V (s(D), o)                        problem: feature-based strategy selection by rational metar-
                    rrel (s) =                ,           (11)
                                 max V (c, o)                       easoning. We have previously shown that feature-based strat-
                                  c
                                                                    egy selection can – but context-free strategy selection cannot
where s(D) is the gamble that strategy s chooses in decision        – account for people’s adaptive choice of sorting strategies
problem D, V (c, o) is the payoff of choice c if the outcome is     (Lieder, Plunkett, et al., 2014). Here we have extended this
o, and the denominator is the highest payoff the agent could        conclusion to decision-strategies that operate on internal rep-
have achieved given that the outcome was o. The priors on all       resentations. The theoretical significance of rational metar-
coefficients in the score and execution time models of rational     easoning is twofold: First, it reconciles the two poles of the
metareasoning were standard normal distributions.                   debate about human rationality by proposing that people learn
   We performed 1000 simulations of people’s strategy               to make rational use of fallible heuristics. Our theory formal-
choices in this experiment. In each simulation, we modeled          izes the strategy selection principle of ecological rationality
people’s prior learning experiences about risky choice strate-      (Todd & Gigerenzer, 2012) and specifies the computational
gies by applying rational metareasoning to ten randomly gen-        mechanisms of learning and cognitive control through which
erated instances of each of the 144 types of decision problems      it could be realized. Second, strategy selection by rational
considered by Payne et al. (1988). We then applied ratio-           metareasoning completes the resource-rational approach to
nal metareasoning with the learned model of the strategies’         cognitive modeling: the principle of resource-rationality can
performance to a simulation of Experiment 1 from Payne              be used not only to derive heuristics (Lieder et al., 2013; Grif-
et al. (1988). Since their participants received no feedback,       fiths et al., 2015; Lieder, Hsu, & Griffiths, 2014) but also to
our simulation assumed no learning during the experiment.           predict when people should use which heuristic. Strategy se-
Outcome distributions with low dispersion were generated by         lection by rational metareasoning thereby provides the con-
sampling unnormalized outcome probabilities independently           ceptual glue necessary to build integrated theories out of our
from the standard uniform distribution and dividing them by         scattered bricks of knowledge about tens or hundreds of cog-
their sum. Outcome distributions with high dispersion were          nitive strategies.
generated by sampling the outcome probabilities sequentially           If the brain had sufficient computational power and sophis-
such that the second largest probability was at most 25% of         tication to perform near-optimal strategy selection, then why
the largest one, the third largest probability was at most 25%      would it rely on simple heuristics to make economic deci-
of the second largest one, and so on.                               sions? The reason might be that real-life decisions are much
                                                                 1366

more complex than the decisions modeled above: The com-              Johnson, E. J., & Payne, J. W. (1985). Effort and accuracy in
putational complexity of optimal decision making increases             choice. Management science, 31(4), 395–414.
with the size of the decision problem but the computational          Lieder, F., Griffiths, T. L., & Goodman, N. D. (2013). Burn-
complexity of strategy selection does not. Thus, as decision           in, bias, and the rationality of anchoring. In P. Bartlett,
problems become more complex normative decision strate-                F. C. N. Pereira, L. Bottou, C. J. C. Burges, & K. Q. Wein-
gies take prohibitively long, but the time required for strategy       berger (Eds.), Adv. neural inf. process. syst. 25.
selection remains the same. Therefore, rational metareason-          Lieder, F., Hsu, M., & Griffiths, T. L. (2014). The high avail-
ing is not only tractable but also worthwhile in complex real-         ability of extreme events serves resource-rational decision-
life decisions where the normative solution is intractable and         making. In Proc. 36th ann. conf. cognitive science society.
applying the wrong heuristic can have dire consequences.               Austin, TX: Cognitive Science Society.
   The exact conditions under which the benefits of strategy         Lieder, F., Plunkett, D., Hamrick, J. B., Russell, S. J., Hay,
selection by rational metareasoning offset its cost will be de-        N., & Griffiths, T. (2014). Algorithm selection by rational
termined in future work. In addition, rational metareasoning           metareasoning as a model of human strategy selection. In
can be a used as a computational level theory (Marr, 1983) of          Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, &
metacognition–just like Bayesian inference is a used as com-           K. Weinberger (Eds.), Adv. neural inf. process. syst. 27 (pp.
putational level theory of inductive reasoning and learning.           2870–2878). Curran Associates, Inc.
   In conclusion, strategy selection by rational metareasoning       Marewski, J. N., & Link, D. (2014). Strategy selection: An
is a promising framework for cognitive modeling. It could,             introduction to the modeling challenge. Wiley Interdisci-
for instance, be used to explain paradoxical inconsistencies in        plinary Reviews: Cognitive Science, 5(1), 39–59.
risky choice by identifying why people use different heuris-         Marr, D. (1983). Vision: A Computational Investigation into
tics in different contexts. Furthermore, our model of strategy         the Human Representation and Processing of Visual Infor-
selection learning can be applied to education and cognitive           mation. W. H. Freeman.
training: First, our model could be used to optimize problem         May, B. C., Korda, N., Lee, A., & Leslie, D. S. (2012). Op-
sets for helping students learn when to apply which procedure          timistic Bayesian sampling in contextual-bandit problems.
(e.g. in algebra) rather than drilling them on one procedure at        J. Mach. Learn. Res., 13.
a time. Second, our model could be used to design cognitive          Pachur, T., Todd, P. M., Gigerenzer, G., Schooler, L. J., &
training programs promoting adaptive flexibility in decision           Goldstein, D. G. (2011). The recognition heuristic: a re-
making and beyond. Future work will also explore learn-                view of theory and tests. Frontiers in psychology, 2.
ing the VOC of elementary information processing operations          Payne, J. W., Bettman, J. R., & Johnson, E. J. (1988). Adap-
(Russell & Wefald, 1991) as a model of strategy discovery.             tive strategy selection in decision making. J. Exp. Psychol.-
                                                                       Learn. Mem. Cogn., 14(3), 534.
Acknowledgments. This work was supported by ONR MURI                 Rieskamp, J., & Otto, P. E. (2006). SSL: A theory of how
N00014-13-1-0341 and grant number N00014-13-1-0341 from the            people learn to select strategies. J. Exp. Psychol. Gen.,
Office of Naval Research.                                              135(2), 207–236.
                          References                                 Russell, S., & Wefald, E. (1991). Principles of metareason-
                                                                       ing. Artificial Intelligence, 49(1-3), 361–395.
Bröder, A. (2003). Decision making with the” adaptive tool-         Shrager, J., & Siegler, R. S. (1998). SCADS: A model of
   box”: Influence of environmental structure, intelligence,           children’s strategy choices and strategy discoveries. Psy-
   and working memory load. J. Exp. Psychol.-Learn. Mem.               chological Science, 9(5), 405–410.
   Cogn., 29(4), 611.                                                Stanovich, K. E. (2009). Decision making and rationality in
Dolan, R. J., & Dayan, P. (2013). Goals and habits in the              the modern world. Oxford University Press, USA.
   brain. Neuron, 80(2), 312–325.                                    Thompson, W. R. (1933). On the likelihood that one un-
Erev, I., & Barron, G. (2005). On adaptation, maximiza-                known probability exceeds another in view of the evidence
   tion, and reinforcement learning among cognitive strate-            of two samples. Biometrika, 285–294.
   gies. Psychological review, 112(4), 912–931.                      Todd, P. M., & Gigerenzer, G. (2012). Ecological rationality:
Fiser, J., Berkes, P., Orbán, G., & Lengyel, M. (2010). Statis-       Intelligence in the world. New York: Oxford University
   tically optimal perception and learning: from behavior to           Press.
   neural representations. Trends Cogn. Sci., 14(3), 119–130.        Tversky, A. (1972). Elimination by aspects: A theory of
Gigerenzer, G., & Gaissmaier, W. (2011). Heuristic decision            choice. Psychological review, 79(4), 281.
   making. Annual Review of Psychology, 62(1), 451–482.              Tversky, A., & Kahneman, D. (1974). Judgment under un-
Gigerenzer, G., & Todd, P. M. (1999). Simple heuristics that           certainty: Heuristics and biases. Science, 185(4157), 1124–
   make us smart. New York: Oxford University Press.                   1131.
Griffiths, T. L., Lieder, F., & Goodman, N. D. (2015). Ratio-        Vul, E., Goodman, N., Griffiths, T. L., & Tenenbaum, J. B.
   nal use of cognitive resources: Levels of analysis between          (2014). One and done? optimal decisions from very few
   the computational and the algorithmic. Topics in Cognitive          samples. Cognitive science, 38(4), 599–637.
   Science, 7(2), 217-229.
                                                                 1367

