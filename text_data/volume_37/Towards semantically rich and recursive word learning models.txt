                 Towards semantically rich and recursive word learning models
                                          Francis Mollica (fmollica@bcs.rochester.edu)
                                     Steven T. Piantadosi (spiantadosi@bcs.rochester.edu)
                Department of Brain and Cognitive Sciences, University of Rochester, Rochester, NY 14627 USA
                              Abstract                                 and referents. Variants of cross-situational models have been
                                                                       couched in terms of connectionist networks (Plunkett, Sinha,
   Current models of word learning focus on the mapping be-
   tween words and their referents and remain mute with regard         Møller, & Strandsby, 1992), deductive hypothesis testing
   to conceptual representation. We develop a cross-situational        (Siskind, 1996), hypothesis competition (Regier, 2005) and
   model of word learning that captures word-concept mapping           probabilistic inference (Yu & Ballard, 2007; Frank, Good-
   by jointly inferring the referents and underlying concepts for
   each word. We also develop a variant of our model that incor-       man, & Tenenbaum, 2009). These prior theories remain mute
   porates recursion, which entertains the idea that children can      about concept-word mappings or assume words label pre-
   use learned words to aid future learning. We demonstrate both       existing concepts, an assumption that is inherently problem-
   models’ ability to learn kinship terms and show that adding
   recursion into the model speeds acquisition.                        atic. These theories leave unanswered both how conceptual
   Keywords: word learning; cross-situational learning; lan-           representations develop and how the word-concept mapping
   guage acquisition                                                   interacts with the word-referent mapping.
                                                                          An alternative approach is to treat conceptual development
                          Introduction                                 and word learning as a joint inference problem. This is the
Most contemporary research on word learning examines how               approach that we develop in this paper building on general
children solve the word-to-referent mapping problem—i.e.,              versions of ideas proposed in prior cross-situational learning
how children who are presented with multi-word utterances in           models. Our work provides a cross-situational word learning
multi-referent contexts learn which objects, events, or prop-          model that aims to integrate the three aspects of word learn-
erties a word refers to. Real-life word learning is actually           ing: word labels, object referents, and abstract concepts. It
much more interesting than discovering these simple corre-             works by combining a plausible (though simplified) semantic
spondences; discovery of the meaning of words often goes               representation with cross-situational evidence of words and
hand-in-hand with true conceptual learning. Waxman and                 referents. Our model also captures the idea that children do
Markow (1995) argue that, for kids, encountering a new word            not just learn concepts but organize conceptual information
provides an “invitation” for a conceptual distinction to be            to form intuitive theories about the world (Carey, 1985; Well-
made. It has even been shown in adults that having novel               man & Gelman, 1992). There have been advances in compu-
words improves category learning (Lupyan, 2006). In this               tational modeling on learning how these theories might be
light, it is not surprising that having conceptual knowledge           structured (Tenenbaum, Griffiths, & Kemp, 2006) and ac-
influences how children extend novel words (Booth & Wax-               quired (Ullman, Goodman, & Tenenbaum, 2012).
man, 2002).                                                               Intuitive theories likely influence how a child approaches
   Here, we aim to extend formalized theories of cross-                word learning. For example, a child that has learned an ab-
situational word learning to capture not just the salient physi-       stract conceptual structure might approach learning words as
cal referents that are present, but the more abstract conceptual       denoting relationships across the structure; this kind of be-
meanings that adults posses. A useful framework for under-             havior is likely relevant in number word learning, where the
standing our approach is the semiotic triangle put forth by            structure of the count list (“one”, “two”, “three”, etc.) likely
Peirce (1868):                                                         provides a placeholder structure for their meanings (Carey,
                             concept                                   2009). Of course, such structures can be more complex than
                                                                       lists: a child’s theory of kinship as a family tree might shape
                   word                   referent                     how they approach the task of learning kinship terms. For in-
Competent word users must know each link—the mappings                  stance, a child might need to have the right conceptual struc-
from words to referents and words to abstract concepts, and            ture (e.g. a tree) with the referents in their particular family
the relationship between abstract concepts and their referents.        members in the right place in order to correctly determine ab-
In learning, children must use observed co-occurrences of,             stract relations like uncle.
say, the word “accordion” and physical accordions to infer
this mapping, as well as the abstract concept ACCORDION                                       Our Approach
that can, in principle, refer to infinitely many physical objects.     Here, we study the domain of kinship as an example of cross-
   Cross-situational models of word learning capitalize on the         situational word learning that requires abstract conceptual
fact that a word is often heard when its referent is in the im-        knowledge. Kinship is an ideal domain because it lends itself
mediate context. Repeated exposure of words and their cor-             to straightforward logical representation and it is one of the
responding referents in multiple contexts provides the basis           early domains available to children, building on their initial
for the statistical learning of the association between words          learning of terms like mom and dad. The domain is complex
                                                                   1607

enough to need interesting cognitive mechanics, but simple
enough to be computationally and representationally tractable
(Katz, Goodman, Kersting, Kemp, & Tenenbaum, 2008). In
our demonstration, we focus on learning the word-concept-
referent mappings for following kinship terms of English:
parent, child, spouse, grandparent, sibling, uncle/aunt1 and
cousin.
   We start by considering a cross-situational learning setup,
meaning that children and the model observe both words
and immediately available referents (e.g., parent spoken by
“Rose” to refer to “Brandy”). The model formalizes a se-
mantic space that includes the possibility of learning individ-
ual referents for each word (as in traditional word-learning           Figure 1: Family tree serving as the context for our model.
models), or more abstract logical concepts. This represen-             Bold lines signify spouse relationships.
tation can be thought of as a function that, given a context,
returns a set of referents in that context. The simplest hy-           for our kinship model was based on the family tree shown in
potheses explicitly “memorize” the set of referents for each           Figure 1. All members of the family tree were seen by the
word; however, the model also allows logical hypotheses that           model as potential referents. We assume the learner has de-
implicitly define this set. For instance, a word like parent           veloped the abstract structure of a family tree, including the
might return the pairs X,Y such that X is the parent of Y . The        primitive relations between entities. Future research will at-
model is cross-situational because any instance of parent will         tempt to integrate learning the tree structure and primitives
occur with only one particular X and Y (e.g. “Brandy” and              into the model.
“Rose”). The learner must aggregate information across us-                The likelihood function gives the probability of a word Wi
ages in order to both figure out the more abstract, productive         correctly being mapped onto a referent Yi conditioned on the
form of the meaning, and learn that parent does not refer to a         speaker Xi , the context Ci , and the current hypotheses for each
particular parent (e.g. “Brandy”).                                     word, i.e. the hypothesized lexicon L. We assume a noisy
   Our learning model uses two components, both of which               likelihood process, where a correct word-referent pair is ob-
have been used in previous models of conceptual and lan-               served with probability α and an incorrect word-referent pair
guage learning (e.g. Goodman, Tenenbaum, Feldman, &                    is observed with probability 1-α. Here, we fix α = 0.9.
Griffiths, 2008; Ullman et al., 2012; Piantadosi, Tenenbaum,              The PCFG and the likelihood function specify a probabil-
& Goodman, 2013, 2012): a simplicity prior over semantic               ity model for all possible lexicons. With this model we can
representations and a size principle likelihood specifying how         rank the probability of a hypothesized lexicon conditioned on
well any hypothesized representation explains the observed             observed word-referent mappings in a given context with a
data.                                                                  given speaker according to Bayes Rule:
   Our prior takes the form of a Probabilistic Context Free                      P(L|W, X,Y,C) ∝ P(L) · ∏ P(Wi ,Yi |L, Xi ,Ci )       (1)
Grammar (PCFG) which specifies how learners may combine                                                      i
our assumed semantic primitives and entities in the context               Here, P(L) is the probability of L under the PCFG and
(see Table 1).                                                         P(Wi ,Yi |L, Xi ,Ci ) gives the likelihood of the word-referent
                                                                       mappings under the hypothesized lexicon and the observed
      Table 1: The PCFG used for learning Kinship terms.               data. The PCFG prior penalized complex lexicons, meaning
  START → SET                            SET → parents(SET)            that this builds in a simplicity bias, a natural assumption for
  SET → union(SET,SET)                   SET → children(SET)           learners (Feldman, 2003) especially for the kinship domain,
  SET → intersection(SET,SET)            SET → spouses(SET)            where it has been shown that kinship systems are the opti-
  SET → set difference(SET,SET) SET → male(SET)                        mal trade-off between simplicity and informativity (Kemp &
  SET → complement(SET)                  SET → female(SET)             Regier, 2012). Thus, learners “score” any hypothesized lex-
  SET → specific referent                SET → X                       icon (mapping of words to meanings) L by considering (i)
                                                                       how complex L is and (ii) how well L explains the observed
   Our PCFG for learning kinship terms included the set-               word-referent usages.
theoretical primitives, union, intersection, set-difference and
complement, and primitives specific to the kinship domain,                                          Methods
parents, children, spouses, male and female. All entities in           Using Equation 1 to determine the most likely lexicons given
the context were potential sets. Additionally, the speaker X           the data is a complex inference problem because there are, in
was included in the grammar as a potential set. The context            principle, infinite possible lexicons generated from the PCFG.
    1 For simplicity, we do not distinguish gender here, although      Here, we solve the problem using sampling—Markov-Chain
there is nothing to suggest the model could not handle it with the     Monte-Carlo (MCMC)—methods. MCMC provide samples
addition of gender primitives.                                         from the posterior distribution (in this case P(L|W, X,Y,C) )
                                                                   1608

                 Figure 2: Expected proportion of corrected hypotheses by the number of observed data points.
by walking around the space of hypotheses. In the limit, these                                 Model Results
walks provably draw samples from the true posterior distri-
                                                                    The model learned the correct hypothesis for each word2 (see
bution. Because our hypothesis space is discrete, this method
                                                                    Table 2). As can be seen in Figure 2, the model learns the
primarily allows us to determine the most likely lexicon given
                                                                    correct hypotheses for simple concepts, such as PARENT and
the observed data.
                                                                    GRANDPARENT, faster than it learns the correct hypotheses
                                                                    for more complex concepts, such as SIBLING and COUSIN.
                                                                    The logistic shape of the growth curves suggests that accurate
                                                                    performance is not gradual. Therefore, the model predicts
   We ran the model on simulated data constructed by sam-
                                                                    that after observing a certain amount of data, a child should
pling data from a correct lexicon with the specified noise pa-
                                                                    be able to learn the correct concept for each word.
rameter α. Note that by sampling based on the true word-
referent mappings in the tree, the data selected is biased to-         The posterior-weighted average of each hypothesis’ re-
wards the more common relationships in the specific tree used       call and precision3 can be used to identify patterns of over-
as context. Additionally, we did not fix the speaker to a spe-      generalization and under-generalization of a word’s refer-
cific person. If we had fixed the speaker, the data would be        ents. The posterior-weighted average recall represents the
egocentric with regard to the speaker, meaning that the model       proposed hypotheses ability to select the correct referents. A
would be attempting to learn the referents of kinship terms for     recall of one means that on average each hypothesis selected
that person as opposed to the underlying concepts for kinship       all of the correct referents. The posterior weighted average
terms in general.                                                   precision reflects the hypotheses’ specificity in selecting only
                                                                    the correct referents. Recall greater than precision suggests
                                                                    that a word is being over-extended to incorrect referents as
                                                                    can be seen for siblings, cousins and uncles/aunts.
   We varied the amount of data the model received from                As expected by the pattern of precision and recall, the in-
10 data points to 250 data points at ten point intervals and        correct hypotheses in the finite hypothesis space (see Table 2)
ran 16 sampling chains for one million steps at each data           tend to over-generalize terms corresponding to complex con-
amount. For each chain, we saved the 100 lexicons with the          cepts. The most common incorrect hypotheses for siblings,
highest posterior score, and used the union of these sets as        uncles/aunts and cousins are over-extensions of the terms to
a finite hypothesis space representing the posterior distribu-      respectively include the speaker herself, the speaker’s parents,
tion (Piantadosi et al., 2012). We used the finite hypothesis       or everyone in the speakers generation including the speaker.
space to calculate the learning trajectory for each word as the         2 For readability, all hypotheses presented in the paper have been
amount of data observed increases. Given that the hypotheses        transformed into their simplest semantically equivalent form—i.e.,
were generated based on varying amounts of data, their pos-         the shortest composition of primitives denoting the same set of ref-
terior score and likelihood per data point were normalized by       erents.
                                                                        3 Recall is the amount of correct referents proposed by the hy-
recalculating them on a set of 1000 data points. The growth
                                                                    pothesis divided by the total amount of correct referents for the
trajectory is then represented as the posterior-weighted aver-      word. Precision is the total amount of correct referents proposed by
age of all lexicons’ accuracy.                                      the hypothesis divided by all referents proposed by the hypothesis.
                                                                1609

                          Table 2: Correct hypothesis and most common incorrect hypothesis for each word.
      Word                                      Correct Hypothesis                                       Most Common Incorrect Hypothesis
    Children                                       children(X)                                                   children(spouses(X))
     Parents                                        parents(X)                                                   spouses(parents(X))
  Grandparents                                 parents(parents(X))                                           parents(spouses(parents(X)))
    Spouses                                        spouses(X)                                                     male(spouses(X))
    Siblings                         set difference(children(parents(X)), X)                                     children(parents(X))
  Uncles/Aunts           union(set difference(children(parents(parents(X))), parents(X)),                union(children(parents(parents(X))),
                          spouses(set difference(children(parents(parents(X))), parents(X))))             spouses(children(parents(parents(X)))))
     Cousins        children(union(set difference(children(parents(parents(X))), parents(X)),      children(spouses(children(parents(parents(X)))))
                         spouses(set difference(children(parents(parents(X))), parents(X)))))
Interestingly, the most common mistakes made learning the                 dicts roughly matches the qualitative pattern observed
simpler concepts tend to under-generalize a term by imposing              in this data: mommy/daddy (parent) is learned quickly,
an additional constraint. For children, parent and grandpar-              grandma/grandpa (grandparent) is learned somewhat less
ent, the incorrect hypotheses would be correct if every child             quickly, and brother/sister (sibling) and uncle/aunt take much
had two married parents, represented by spouse relationships.             more time. Intuitively, the model explains this acquisition tra-
The incorrect hypothesis for spouse places an unnecessary                 jectory by penalizing complex hypotheses. When the model
male constraint on the correct hypothesis. At face value, these           has not observed much data, it relies heavily on its prior,
mistakes seem like plausible mistakes a child learning kinship            which is biased to favor simplicity. If you compare the com-
terms could make.                                                         plexity of the correct hypotheses the model learns (see Table
   The model also demonstrated that under simple assump-                  2), PARENT is a single semantic primitive, GRANDPARENT
tions, rational statistical learners will not learn specific refer-       requires two primitives, and SIBLINGS and UNCLES / AUNTS
ents but prefer abstract logical hypotheses. For instance, the            are much more complex.
posterior probability of a hypothesis containing any specific                 However, beyond the relative difficulty of words, the gen-
referent was at most 10−25 by 5 data points. The pressure for             eral shape of the model predictions do not closely match chil-
abstraction occurs because any particular referent is unlikely            dren’s trajectory. The child data suggest a gradual acquisi-
in the prior and the speaker varies, meaning that accurate                tion of kinship terms. As discussed in Ullman et al. (2012),
word-referent maps are hard to create4 . For example, propos-             one possible explanation for this discrepancy is that the model
ing that sibling refers to rose is only true when spoken by               predicts an individual child’s learning trajectory; whereas, the
Rose’s siblings. To avoid under-generalizing, the model con-              MCDI data is an average growth trajectory over children. If
structed abstract hypotheses that over-generalized the word               we consider that children might differ in the rate at which
to many referents. As the model observed more data, it nar-               they observe data, curves that are individually logistic might
rowed down the set of potential referents in an attempt to find           suggest gradual learning when averaged together.
the simplest correct hypothesis.                                              To further explore the relationship between children’s be-
                                                                          havior and the model, we considered transforming both learn-
Comparison to Child Data Our model’s predicted learn-
                                                                          ing curves to relate performance to the number of instances
ing trajectories can, in principle, be compared to child ac-
                                                                          of each word; however, there is no directly analogous way to
quisition data. We used the Words and Gestures MacArthur-
                                                                          convert the child data. We tried fitting a logistic regression
Bates Communicative Development Inventory (MCDI) data
                                                                          for each word as a function of age in months and dividing
from Word Bank5 to calculate child learning trajectories for
                                                                          the coefficient for age by an estimate of the number of in-
parents, grandparents, siblings and uncle/aunts. The MCDI
                                                                          stances of that word a child hears in a month. In doing so,
data is a parent report measure of their child’s understanding
                                                                          we assumed children hear 360,000 words per month and esti-
of a specific list of words. As our model did not differentiate
                                                                          mated instances of a specific word using frequency data from
gender, we averaged over the gendered terms in the MCDI for
                                                                          CHILDES6 (see Figure 3b).
each concept in our model (e.g., mommy/daddy corresponds
                                                                              Interestingly, this transformation suggests that higher fre-
to parents). While the MCDI is the best data set available, the
                                                                          quency words need more data to be learned. For instance,
child’s understanding of a word is based on parental report
                                                                          mommy, which is learned very quickly, is also extremely fre-
rather than experiments and, thus, might incorrectly capture
                                                                          quent. Its learning curve, therefore, stretches out, showing
children’s true understanding. For example, without proper
                                                                          that children require many instances to learn. Conversely, un-
controls, over-generalization can be mistaken for correct un-
                                                                          cle/aunt, which is learned slowly, is relatively infrequent and
derstanding.
                                                                          its learning curve suggests that children require very few in-
   Figure 3a shows smoothed growth curves fit to MCDI                     stances to learn.
data.      The order of acquisition that the model pre-                       However, this pattern in the transformed data may not re-
                                                                          flect what happens with children. The word frequency es-
   4 When the model is provided with ego-centric data, hypotheses         timates from corpus data may overestimate the amount of
with specific referents are more likely.
   5 Retrieved from wordbank.stanford.edu on 2015-01-20.                       6 Data retrieved on 2015-01-20 using ChildFreq (Bååth, 2010).
                                                                     1610

   Figure 3: (a) Growth trajectories of MCDI data by age. (b) Growth trajectories from MCDI data by number of instances.
data actually considered by the learner. At the same time, if       ten times.
the pattern of learning trajectories from the transformed child        As with the non-recursive model, we varied the amount of
data is accurate, there is a discrepancy with the model. Ac-        data given to the model from 5 data points to 200 data points
cording to the child data, the more complex concepts such as        at five point intervals and ran 16 sampling chains for 500,000
UNCLE / AUNT require relatively few data points and the sim-        steps at each data amount. Again, we created a finite hypoth-
ple concepts require many data points. This is the exact oppo-      esis space and calculated the learning trajectories using the
site prediction from our model and any other with a simplicity      same method as before (see Figure 4).
bias. One possibility for this discrepancy is that while word          For the words with simple hypotheses, there is no substan-
learning is a joint learning problem, the learning of concept-      tial difference in allowing the model to recurse; however, for
word mappings and word-referent mappings might not occur            the more complex hypotheses, allowing recursion decreases
on the exact same time scale. In this domain, children might        the number of data points that need to be observed. This is
learn word-referent mappings before they learn word-concept         because some word meanings can be expressed more con-
mappings. As a result, the transition from associative link         cisely by referencing other word meanings. For instance,
to abstract conceptual representation might require additional      COUSIN becomes easier to learn once UNCLE is known be-
data points. Whether or not word-referent mappings must             cause COUSIN can be expressed as children(uncle(X)), in-
precede word-concept mappings is an empirical question.             stead of the more arduous form in Table 2 above.
   Another possible explanation is that in kinship and other           As our model shows, acquisition of this kind of recursive,
semantic domains, a conceptual representation’s dependence          interrelated theories is possible through essentially the same
on an abstract structural representation (e.g., tree or taxon-      mechanisms as non-recursive theories. By creating a repre-
omy) limits a child’s ability to learn word-concept mappings        sentation language that permits recursion and doing inference
until the abstract structural representation has been devel-        over that language with cross-situational data, we are able to
oped. This explanation could plausibly explain the transfor-        learn word meanings that are richly interconnected. A general
mation requiring more words for simple concepts. If you con-        prediction of this system is that permitting recursion of the
sider the learner using early relationships, such as PARENT         referents of concepts to each other will speed learning of cer-
and SPOUSE, to construct a family tree, the high instance re-       tain types of meanings by allowing them a much more concise
quirement might reflect the development of the kinship tree.        representation. This exact mechanism might also account for
Future research will explore these possibilities.                   how complex semantic primitives develop from simple prim-
Recursive vs. Non-Recursive                                         itives.
One of the most interesting aspects of children’s theory learn-
                                                                                              Discussion
ing is that theories can be rich, interconnected systems of
ideas and concepts. Our model allows us to explore learn-           In this paper, we have provided a possible mechanism for
ing interconnected representations in the domain of kinship         simultaneously learning a conceptual representation and the
by potentially allowing words to be defined in terms of other       mapping of that representation to a word. This model dif-
words, a capacity essentially for recursion.                        fers from previous models attempting to learn word-concept
   In a second version of the model, we built recursive rules       mappings (e.g. Fazly, Alishahi, & Stevenson, 2010) in that
into the PCFG. For instance, the recursive grammar allows           we focus on the learning of concepts requiring abstract re-
a function like SET → uncle(SET), giving the referents that         lations between entities. We offer this model as a first step
are considered “uncles” by the model’s current hypothesized         in suggesting that children could learn hierarchical concepts
lexicon (which may or may not be correct). To prevent infinite      and their corresponding words jointly. We expect that this
recursion, hypotheses were restricted to recurse maximally          mechanism will generalize to other hierarchical domains.
                                                                1611

                               Figure 4: Growth trajectories of the model with and without recursion.
   Here, we focused on conceptual development as the for-               Frank, M. C., Goodman, N. D., & Tenenbaum, J. B. (2009). Using
mulation and refinement of hypotheses about the relationship              speakers’ referential intentions to model early cross-situational
                                                                          word learning. Psychological Science, 20(5), 578–585.
between speakers and referents on a pre-existing kinship tree.          Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths, T. L.
Future work will attempt to integrate the tree construction               (2008). A rational analysis of rule-based concept learning. Cog-
into the model.                                                           nitive Science, 32(1), 108–154.
                                                                        Katz, Y., Goodman, N. D., Kersting, K., Kemp, C., & Tenenbaum,
   Both our model and other models of cross-situational                   J. B. (2008). Modeling semantic cognition as logical dimension-
concept-word mapping learning rely heavily on the seman-                  ality reduction. In Proceedings of thirtieth annual meeting of the
tic primitives or features posited to represent concepts. The             cognitive science society.
                                                                        Kemp, C., & Regier, T. (2012). Kinship categories across languages
scalability of our model depends on uncovering what prim-                 reflect general communicative principles. Science, 336(6084),
itives people use when constructing conceptual representa-                1049–1054.
tions. Ongoing research is focused on discovering the primi-            Lupyan, G. (2006). Labels facilitate learning of novel categories. In
                                                                          The sixth international conference on the evolution of language
tives people use and future research will investigate both the            (pp. 190–197).
development of complex primitives from simple primitives                Peirce, C. S. (1868). On a new list of categories. In Proceedings of
and the time course of primitive development.                             the american academy of arts and sciences (Vol. 7, pp. 287–298).
                                                                        Piantadosi, S. T., Tenenbaum, J. B., & Goodman, N. D. (2012).
                                                                          Bootstrapping in a language of thought: A formal model of nu-
                           Conclusion                                     merical concept learning. Cognition, 123(2), 199–217.
                                                                        Piantadosi, S. T., Tenenbaum, J. B., & Goodman, N. D. (2013).
We have developed a cross-situational word learning model                 Modeling the acquisition of quantifier semantics: a case study in
that captures richer semantic representations than associative            function word learnability. Under review.
                                                                        Plunkett, K., Sinha, C., Møller, M. F., & Strandsby, O. (1992). Sym-
links. Instead, it captures the acquisition of abstract semantic          bol grounding or the emergence of symbols? vocabulary growth
relations in the context of a rich theory of the world. We                in children and a connectionist net. Connection Science, 4(3-4),
developed two variants of the model: one which permitted                  293–312.
                                                                        Regier, T. (2005). The emergence of words: Attentional learning in
recursion and one which did not. We show that the recursive               form and meaning. Cognitive Science, 29(6), 819–865.
model not only can be made to work—explaining how word                  Siskind, J. M. (1996). A computational study of cross-situational
learning may interface with rich semantic theories—but also               techniques for learning word-to-meaning mappings. Cognition,
                                                                          61(1), 39–91.
speeds acquisition.                                                     Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-based
                                                                          bayesian models of inductive learning and reasoning. Trends in
                           References                                     Cognitive Sciences, 10(7), 309–318.
                                                                        Ullman, T. D., Goodman, N. D., & Tenenbaum, J. B. (2012). Theory
Bååth, R. (2010). Childfreq: An online tool to explore word fre-        learning as stochastic search in the language of thought. Cognitive
   quencies in child language.                                            Development, 27(4), 455–480.
Booth, A. E., & Waxman, S. R. (2002). Word learning is smart: Evi-      Waxman, S. R., & Markow, D. B. (1995). Words as invitations
   dence that conceptual information affects preschoolers’ extension      to form categories: Evidence from 12-to 13-month-old infants.
   of novel words. Cognition, 84(1), B11–B22.                             Cognitive Psychology, 29(3), 257–302.
Carey, S. (1985). Conceptual change in childhood. MIT press.            Wellman, H. M., & Gelman, S. A. (1992). Cognitive development:
Carey, S. (2009). The origin of concepts. Oxford University Press.        Foundational theories of core domains. Annual Review of Psy-
Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilistic          chology, 43(1), 337–375.
   computational model of cross-situational word learning. Cogni-       Yu, C., & Ballard, D. H. (2007). A unified model of early word
   tive Science, 34(6), 1017–1063.                                        learning: Integrating statistical and social cues. Neurocomputing,
Feldman, J. (2003). The simplicity principle in human concept             70(13), 2149–2165.
   learning. Current Directions in Psychological Science, 12(6),
   227–232.
                                                                    1612

