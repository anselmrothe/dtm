                     Constraint-Based Parsing with Distributed Representations
                                                Peter Blouw (pblouw@uwaterloo.ca)
                                           Chris Eliasmith (celiasmith@uwaterloo.ca)
                                      Centre for Theoretical Neuroscience, University of Waterloo
                                                      Waterloo, ON, Canada N2L 3G1
                              Abstract                                 constraints defined over symbols and operations defined over
                                                                       vectors. At a theoretical level, a technique for performing this
   The idea that optimization plays a key role in linguistic cog-
   nition is supported by an increasingly large body of research.      mapping has been proposed (Smolensky, 2006), but it is pri-
   Building on this research, we describe a new approach to pars-      marily defined using tensor product representations that grow
   ing distributed representations via optimization over a set of      in dimensionality in proportion to symbol structure depth. In
   soft constraints on the wellformedness of parse trees. This
   work extends previous research involving the use of constraint-     practice, the main existing implementation of this technique
   based or “harmonic”’ grammars by suggesting how parsing             uses localist rather than distributed representations (Hale &
   can be accomplished using fully distributed representations         Smolensky, 2006).1
   that preserve their dimensionality with arbitrary increases in
   structural complexity. We demonstrate that this method can             Our aim is to build on this previous work by implement-
   be used to correctly evaluate the wellformedness of linguis-        ing a constraint-based parser that operates on fully distributed
   tic structures generated by a simple context-free grammar, and      representations in a vector space of fixed dimensionality at all
   discuss a number of extensions concerning the neural imple-
   mentation of the method and its application to complex parsing      structural depths. There are two benefits to performing these
   tasks.                                                              extensions. First, the use of fully distributed representations
   Keywords: natural language processing; parsing; optimiza-           can allow for parsing behaviour that is sensitive to graded de-
   tion; harmonic grammar; holographic reduced representations;        grees of similarity between structures; such graded sensitivity
   semantic pointer architecture                                       to processing constraints is arguably necessary to account for
                                                                       many kinds of linguistic phenomena (Rogers & McClelland,
                          Introduction                                 2014). Second, the use of a fixed vector space for encod-
One of the most impressive features of human cognition is              ing structures of all degrees of complexity provides a natural
the ability to rapidly process vast numbers of linguistic ex-          way to account for the graceful saturation of processing capa-
pressions. To help explain this ability, many cognitive scien-         bilities as structural complexity is increased. Accounting for
tists adopt the view that humans possess implicit knowledge            such saturation and related processing errors is an important
of the grammatical properties that characterize well-formed            goal for research on linguistic cognition.
phrases and sentences. An influential approach to describing              In what follows, we first describe existing approaches to
this knowledge involves postulating sets of interacting con-           describing grammatical knowledge in terms of sets of vio-
straints that favour and penalize the co-occurrence of certain         lable constraints and briefly motivate this approach in favour
structural features in the representation of a linguistic expres-      of more conventional approaches that postulate a set of sym-
sion (Smolensky & Legendre, 2006). Applications of this ap-            bolic rewrite rules capable of generating all and only the sen-
proach have resulted in a number of important insights con-            tences of a particular language. We then describe an existing
cerning the nature of language processing in cognitive sys-            method for computing grammatical forms using constraints
tems (Prince & Smolensky, 1997).                                       and describe a novel variation of this method that can allow
   A significant benefit of describing grammatical knowledge           for the implementation of a parser that operates on distributed
in terms of violable constraints is that processes sensitive to        representations in a vector space of fixed dimension. We con-
such constraints are naturally computed in neural systems              clude with a discussion of the current limitations of our ap-
(Smolensky & Legendre, 2006; Rogers & McClelland, 2014).               proach, along with some promising extensions.
Given as much, an important area of research concerns the
development of techniques for (a) encoding structured rep-
                                                                                           Harmonic Grammars
resentations into continuous vector spaces of the sort used            A fairly widespread view amongst contemporary cognitive
to describe neural systems (Plate, 2003; Smolensky, 2006;              scientists is that strictly rule-based accounts of linguistic phe-
Eliasmith, 2013), and (b) defining operations on these vectors         nomena are empirically inadequate. Since the late 1980’s,
spaces that perform constraint-sensitive computations of the           considerable research has been directed towards the devel-
sort required to account for linguistic phenomena (Smolensky           opment of probabilistic and constraint-based frameworks in
& Legendre, 2006). To date, there has been comparatively               which certain properties of linguistic structures are favoured
more success solving the first of these problems than the sec-         in a violable manner (Rogers & McClelland, 2014). Through-
ond.                                                                   out this paper, we adopt the formalism of harmonic grammar
   One challenge facing efforts to connect constraint-based            (Smolensky & Legendre, 2006), in which the wellformedness
accounts of grammatical knowledge and vector-based ac-                     1 In a distributed representation, each unit of a neural network
counts of neural computation concerns the mapping between              participates in the encoding of numerous representations.
                                                                   238

of a linguistic structure is evaluated against a set of soft con-                    S                            S
straints that take the following form:
                                                                              A           B                        -
   The co-occurence of structural constituents ci and c j                                                          -
   should be favoured (or disfavoured) to degree wi j .                       a           b                      ++
Once a constraint set of this sort has been defined, it is pos-                                          +                  +
sible to assign a scalar measure of well-formedness to every
                                                                                                   A     -                   -    B
possible structure built from a fixed inventory of constituents.                                         -                   -
Formally, the scalar measure or “harmony” value, E, is a pair-                                             +                   +
wise sum over the set of constituents that make up particular                 S       AB
structure, s:
                                                                              A       a               +                   +
                       H(s) = ∑ H(ci , c j )                    (1)           B       b            a     -                   -    b
                                i≤ j
where H(ci , c j ) evaluates to wi j if both ci and c j are present     Figure 1: Harmonic grammar constraints for a small set of
in s. Importantly, the value H assigns an ordering to the set           context-free rewrite rules, adapted from Hale and Smolensky
of structures containing a set of input constituents for which a        (2006). The constraints defining the grammar are organized
parse is being sought. The maximum element of this ordering             such that each constituent is penalized for occurring alone to
is the structure corresponding to the optimal parse of the in-          a degree equal to the number of parents and children it re-
put. Or put more intuitively, the optimal parse is the structure        quires.These penalties are exactly cancelled out by the pos-
containing the input constituents that minimizes the overall            itive values associated with each co-occurrence of a correct
degree of constraint violation.                                         parent/child pair.
   The expressive power of a grammar defined in terms
of simple pairwise constraints is considerable. Hale and
Smolensky (2006), for example, prove that harmonic gram-                   Now, it is possible to convert the rules into pairwise con-
mars can be constructed to describe formal languages in all             straints of the form “B should be favoured as a right-child of
classes of the Chomsky hierarchy. One important caveat of               S to degree to x” or “S should be favoured as parent to A to
this result is that the resource and processing requirements            degree y”, where x and y are derived from a constraint holding
associated with these grammars are often substantial. For this          between constituents. The key to defining the grammar for a
reason, our goal of implementing a parser whose performance             language is to organize these constraints such that the struc-
degrades gracefully with increased structural complexity is an          tures permitted by the language are assigned energy values of
important one.                                                          0 via (1), while all other structures are assigned positive en-
   Considering the case of a simple context-free grammar                ergy values (Hale & Smolensky, 2006). Figure 1 illustrates
helps to illustrate how Hale and Smolensky are able to de-              this constraint assignment in an intuitive manner.
scribe arbitrary formal languages with large collections of
pairwise constraints. A context-free grammar, to explain, is a               Encoding Grammars in Neural Networks
grammar in which each production rule takes on the follow-              At this point, it might seem that constraint-based grammars
ing form:                                                               are simply complex re-descriptions of more conventional
                              S→Φ                               (2)     grammars. The key advantage of adopting the constraint-
where S refers to any non-terminal symbol and Φ refers to               based approach is that it can be used to easily translate a
a set of terminal and non-terminal symbols. To convert a                measure of the wellformedness of a particular linguistic struc-
context-free grammar into a harmonic grammar, the gram-                 ture into a measure of the wellformedness in a neural system
mar is first translated into Chomsky normal form, in which              that encodes this structure. This translation occurs via a map-
all production rules take one of two forms:                             ping from (1) to an equivalent measure of wellformedness
                                                                        that only refers to the state of the neural system in question.
                       S → AB         A→a                       (3)        Before describing the translation in more detail, it is nec-
where A and B are non-terminal symbols, and a is a terminal             essary to briefly discuss how symbol structures can be en-
symbol. Notice that each production rule in (3) adds either a           coded into the vectors that describe neural network states.
binary or unary branch to a parse tree while also specifying            The first step of the encoding involves performing a role de-
which symbols can be placed in parent-child relationships on            composition on a set of symbol structures that allows each
a given branch. This observation can be used to break each              structure to be represented as a combination of role/filler pairs
binary branching rule into two parts, each of which refers to           (Smolensky, 2006). This role decomposition defines a func-
only a pair of symbols:                                                 tion β : S → P (F × R) that maps each symbol structure to a
                                                                        collection of role/filler pairs. F × R refers to the Cartesian
                      S → A–         S → –B                     (4)     product of the set of all possible fillers and the set of all pos-
                                                                    239

sible roles. The roles in question are typically tree nodes, and         given that E(a) = −H(s) by virtue of the mapping provided
the fillers are symbols.                                                 in (5).2
   The second step of the encoding involves mapping a collec-               With these preliminaries out of the way, it is possible to
tion of role-filler pairs into a superposition of states in a neural     introduce the central contribution of this paper, which is to
network. This mapping is achieved by assigning a vector to               show how to evaluate E using a network whose represen-
each role and filler, and by defining a binding operation that           tational state maintains a fixed dimensionality regardless of
joins a role and filler into a pair. Following Plate (2003), we          parse complexity.
adopt circular convolution as binding operation and use vec-
tor addition for constructing combinations of bound items.                     Parsing while Preserving Dimensionality
These operations can be used to map symbol structures onto               For simplicity, we initially construct our parser using the fol-
vectors as follows:                                                      lowing set of context-free rewrite rules:
                  a = υ(β(s)) =        ∑        Fi ~ Ri          (5)        S→A B           A→a
                                   Fi /Ri ∈β(s)                             A→C S           B→b           C→c
where a is vector, and υ indicates a mapping from a col-                 These abstract rules are chosen because they form a small
lection of role/fillers pairs onto a sum of role/filler bindings.        set exhibiting the potential for boundless recursion, since an
This mapping generates what Plate refers to as a “holographic            arbitrary number of branches that stem from the symbols A
reduced representation” (HRR), which importantly is not a                and S can be included in a tree. Next, we assign a random
lossless encoding the of original symbol structure s.The con-            512-dimensional unit vector to each symbol present in these
volution operation ensures that each role/filler binding resides         rules, and to each of branch position up to some maximum
in the same vector space as the vectors for each role and filler,        depth assuming ternary branching (i.e. left, right, middle). By
but also it results in sums of bindings that correspond to com-          binding together vectors for symbols and branch positions,
pressed approximations of symbol structures.                             we can construct HRRs that encode arbitrary tree structures
   Translating (1) into the language of HRRs is now straight-            (Smolensky, 2006). For example, the tree in Figure 1 might
forward because each structural constituent is simply a                  be encoded as follows:
role/filler binding. As such, the wellformedness of an HRR
can be assessed by summing over the constraint magnitudes                          a = S + rL ~ A + rR ~ B + rLM ~ a + rRM ~ b             (7)
associated with all pairs of role/filler bindings encoded in the
HRR.                                                                     where rL , rR , etc. are role vectors indicating the locations of
   If the constraints defining a harmonic grammar are en-                tree nodes relative to the (unlabelled) tree root.
coded into the connection weights of a neural network, then it              As before, the rules defining the grammar can be refor-
possible to calculate the wellformedness of the network state            mulated as a set of pairwise constraints. The first rule states
through a simple algebraic operation that ranges over all pair-          that the symbol S should have a A as its left-child and B as
wise connections in the network:                                         its right child. In the language of our encoding scheme, this
                                                                         means that if an HRR includes an S bound into a particular
                E(a) = −aT Wa = − ∑i j aiWi j a j                (6)     tree position, then it should also include a B bound into the
                                                                         next left-branching position. For each case in which one of
where a is an activation vector describing the state of the
                                                                         these constraints is satisfied, the magnitude of the constraint
network, and W is the network’s weight matrix. Intuitively,
                                                                         is added to the value of E(a).
if two units connected by a positive weight are active, the
                                                                            We can now construct a weight matrix that assigns the
wellformedness of the activation vector increases (i.e. fewer
                                                                         proper energy values to those vectors that encode wellformed
constraints are violated). If the weight is instead negative,
                                                                         parse trees. Following Smolensky (2006), we design the
the wellformedness decreases (i.e. more constraints are vio-
                                                                         weight matrix such that value of aT Wa is equal to a sum of the
lated). (6) is what is often referred to as an “energy” function,
                                                                         magnitudes of all constraints that hold between pairs of con-
and a number of results concerning the optimization of these
                                                                         stituents encoded in a. For example, if a = S + rL ~ A and the
functions in neural networks can accordingly be applied here
                                                                         constraint favouring A as a left child of S has a magnitude of
(Smolensky & Legendre, 2006). The most important of these
                                                                         2, then −aT Wa should equal −2. It is possible to obtain this
results entails that, assuming certain constraints on the con-
                                                                         result exactly if the vectors for all constituents in a are lin-
nectivity of W , a large class of networks implement dynamics
                                                                         early independent (Smolensky, 2006), but our use of an HRR
that push the activation vector a towards a steady state that is
                                                                         based encoding scheme is not guaranteed to oblige this condi-
a local optimum of E(a). Thus, if the weight matrix W en-
                                                                         tion. It precisely the point of this paper to show how parsers
codes the constraints that define a harmonic grammar, and a
                                                                         might be designed without enforcing a linear independence
is initialized to encode a set constituents for which a parse
                                                                         condition that requires the dimensionality of a to scale as a
is being sought, then the network dynamics will shift a to a
point the minimizes E(a). Minima of E(a), by definition,                     2 The minus sign in (6) is a convention: energy is equal to nega-
encode structures that correspond to (locally) optimal parses            tive harmony and vice versa.
                                                                     240

function of the number of constituents in a. We predict that                to by W (different constituents are mapped to scaled versions
as the bindings encoding different constituents become more                 of u to account for constraints of differing magnitudes). If u
and more dependent, the measured value of E(a) will provide                 is included in the activation vector a, then each unary con-
a progressively worse approximation to the ideal value E(a)                 straint defined over a constituent present in a will have the
(i.e. the value calculated assuming linear independence).                   desired effect on E(a), since E(a) = −aT Ea and −aT Wa will
   To specify the exact form of W , it helps to consider the                vary with the presence or absence of constituents in a that are
simplest case in which W encodes only a single constraint. If               mapped by W to u.
a encodes the two constituents used in the previous example,                    In the following simulations, we define a weight matrix W
and the constraint holding between these constituents still has             that is a sum of circulant matrices, each of which performs a
a value of 2, then there is a readily available expression in               mapping of the sort described in (8). The circulant matrices
which W occurs as the only unknown quantity:                                that make up W are derived by converting the simple grammar
                                                                            defined at the beginning of this section into a set of constraints
               −2 = −(S + rL ~ A) ·W (S + rL ~ A)                   (8)     using the methods of Hale and Smolensky (2006). Because
                                                                            the rewrite rule grammar produces a infinite set of trees of the
Because matrix multiplication is linear, it is possible to break            form {cn−1 abn | n > 0}, we cap the depth of the trees under
W into components that “look for” the correct parent or child               consideration at 4. This leads to a grammar that defines two
of a particular constituent. For example, one component                     trees (see Figure 2).
might look for an S in the position that is a parent to the po-
sition at which A is bound. (8) can therefore be split into                             S     AB     A        a              S
two components that each search for a parent and child, re-
                                                                                        A     CS     B        b
spectively. The first of these components seeks a parent for
rL ~ A, and must therefore be mapped by Wa to a vector that                             C     c                      A             B
has an expected dot product of 1 with S and 0 with all other
constituents. This result is accomplished by observing that                                   S                                    b
                                                                                                               C           S
(S ~ A−1 ~ rL−1 ) ~ (rL ~ A) ≈ S and that S · S ≈ 1. As such, the
matrix Wa simply needs to perform a mapping that is equiva-                             A          B            c
lent to convolving its input by (S ~ A−1 ~ rL−1 ). Plate (2003)                                                     A            B
demonstrates that a fixed convolution operation is equivalent                           a          b
to multiplication by a circulant matrix, so it is straightfor-                                                      a            b
ward to derive Wa as a product of circulant matrices. Note
that Wa maps other constituents to meaningless terms such as                Figure 2: The two trees generated by the depth-capped gram-
(S ~ A−1 ~ rL−1 ) ~ S ≈ ~S ~ rL ~ A−1 ~ S, which are treated                mar under consideration.
as noise (Plate, 2003).3
   To account for the component of W that seeks a proper                        Given W and an HRR encoding some particular sum of
child for S, the process just described is repeated to generate             role-filler bindings, we can compute the wellformedness of
a matrix Wb that performs a mapping equivalent to convolv-                  the HRR via (6). Furthermore, we can compare the well-
ing an input with S−1 ~ A ~ rL . Any vector containing S is                 formedness of various HRRs that encode different collections
mapped by Wb to a vector that has an expected dot product                   of parse tree constituents. In the simulations below, compar-
of 1 with rL ~ A and 0 with all other constituents. Adding                  isons of the wellformedness of different HRRs are used to
together Wa and Wb to yields the matrix W that was originally               illustrate how any network that optimizes (6) will map any
sought to satisfy the equality described in (8), since aT Wa a              vector residing in a specific region of space to a new vector
and aT Wb a are both equal to one and together sum to 2 (i.e.               than encodes one of the trees defined by our grammar.
the constraint value specified by begin with).                                  A challenge that arises when computing the wellformed-
   A final technicality concerns the fact that some of the con-             ness of an HRR via (6) is that the circulant matrices that com-
straints in a harmonic grammar are defined with respect to a                prise W introduce considerable amounts of noise into the cal-
single constituent (e.g. a tree containing A is disfavoured to              culation of E(a). To explain, a single circulant matrix “looks
degree x). This sort of constraint requires a matrix W that                 for” for a particular role-filler binding in a, and if this bind-
maps a constituent back to itself. Adding the identity matrix               ing is present, the circulant matrix returns a different binding
to W is possible way to satisfy this requirement, but doing                 (i.e. a binding that is a parent or child of the binding that
so has the unintended consequence of reproducing the entire                 was being sought). Every other constituent encoded in a is
input to W in its output. To avoid this, we instead define a vec-           also mapped to an output by the circulant matrix, but these
tor u that every constituent with unary constraint gets mapped              outputs are highly dissimilar to the role-filler bindings of in-
                                                                            terest and are therefore treated as noise. If, however, there
    3 The −1 in these expressions denotes the pseudo-inverse of a vec-      are n circulant matrices added into W , then each each role-
tor with respect to convolution; a vector convolved with its pseudo-
inverse is approximately equal to I, which in the context of convolu-       filler binding present in a will add n − 1, n − 2 or n − 3 noise
tion is a vector whose first element is 1 while the rest are zeros.         terms into the desired output of Wa (since only 1, 2, or 3 of
                                                                        241

n matrices seeks out a given binding by virtue of the branch-         1) and again multiplied by the vectors the subnetwork’s con-
ing structure of the trees). The overall noise added to E(a)          straints are designed to produce, yielding “clean” versions of
has a mean of zero, but a variance that is proportional to the        these vectors. This sort of clean-up function is frequently
number of individual noise terms included in the computa-             implemented in models that make use of HRRs (Eliasmith,
tion of E(a). To avoid this problem in our simulations, we            2013; Plate, 2003). The clean vectors are subsequently added
break up the network defined by W into a set of subnetworks           together, and the dot product of this sum and an is calculated
{W1 ,W2 ,W3 , ...Wn }, where n is the number of locally well-         to produce a value for E(an ). In other words, we compute
formed trees (i.e. those trees comprised of a single parent           E(an ) = −aTn C(Wn an ) in each subnetwork n, where C refers
and its immediate children) permitted by the grammar, and             to the noise cleanup operation. E(a) is computed by summing
Wn is the sum of the circulant matrices that encode the con-          the energy values in each subnetwork.
straints corresponding to the nth local tree. Then, we break
apart the vector a into a set of HRRs, each of which contains
the bindings present in a that are acted on by the constraints
in a given subnetwork. We compute the energy value in each
subnetwork, adding together all such values to obtain the total
energy of a. Put simply, we factor the overall energy function
in order to compute it more accurately. With 512 dimensional
vectors, the overall level noise is minimal. We use a simple
thresholding operation (described below) to filter this noise.
However, some unavoidable noise remains due to the fact that
the vector dot product used to compute −aTn Wan performs
pairwise comparisons between vectors that are only approxi-
mately orthogonal (see the discussion below Eq. 8)
                         Simulations
To illustrate the optimization behaviour that we propose to
take advantage of to perform parsing, we compare the well-
formedness of HRRs encoding various subsets of the set of
role/filler pairs defined by our grammar. The point of this
demonstration is to show that HRRs encoding the two parse             Figure 3: Comparison of well-formedness for different tree
trees in Figure 2 have lower energy values than all nearby            structures. The minimum in the graph corresponds to a struc-
HRRs. Specifically, removing a needed constituent or adding           ture that encodes the 5 constituents present in the first tree
a superfluous constituent is shown to typically increase en-          in Figure 2. Points to the left of the minimum correspond
ergy; from this we can conclude that an optima of the energy          to structures that are missing constituents in the tree, while
function E(a) is obtained when is a is very close to a vector         points to the right of the minimum correspond to structures
that encodes a well-formed parse tree.                                that have extra constituents not found in the tree. Each plotted
   In each simulation, we randomly generate vectors for all           value is the mean of 250 trials involving randomly generated
roles and fillers that can be used in the encoding of the parse       role and filer vectors of dimension D. Error bars indicate 95%
trees in Figure 2. Specifically, we use random unitary vec-           confidence intervals.
tors, which have equal exact and approximate inverses (Plate,
2003). From the random role and filler vectors, we gener-                Next, we perform a comparison of wellformedness across
ate a set of circulant matrices that perform linear transforma-       a number of linguistic structures. In the comparison shown
tions that are equivalent to convolutions (e.g. S ~ A−1 ~ rL−1 ).     in Figure 3, we evaluate structures that progressively include
The circulant matrices corresponding to the constraints defin-        more constituents from the first tree shown in Figure 2. The
ing a local tree are added together and included in the set           wellformedness measure E(a) minimizes when all five con-
{W1 ,W2 ,W3 , ...}.                                                   stituents of the tree are present, and increases as additional
   For a given vector a we compute E(a) by first breaking             constituents are added. This result suggests that the well-
apart a into subsets of bindings that each correspond to one          formedness of a structure can be accurately tracked even
of the subnetworks used to factor the energy function. Next,          though it only encodes a compressed approximation of the
for each subnetwork, we compute En (an ) by multiplying an            symbol structure that the wellformedness measure is defined
by Wn , the weight matrix of subnetwork n. The result of this         over. In the comparison shown in Figure 4, we perform
multiplication is a vector, and we take the dot product of this       an analogous evaluation with structures that include progres-
vector and the vectors that the subnetwork’s constraints are          sively more constituents from the second tree shown in Fig-
are designed to produce (i.e. certain child and parent bind-          ure 2. All simulations are run using 128, 256, and 512 di-
ings). The dot products are then thresholded (typically to 0 or       mensional vectors. As expected, performance degrades with
                                                                  242

lower dimensional vectors.                                             function using simulated neurons. Extracting subsets of the
                                                                       bindings in a may also incur additional computational costs
                                                                       in a neural network implementation.
                                                                          One other important limitation concerns the need to scale
                                                                       our methods to more complex trees involving large numbers
                                                                       of bindings. Preliminary empirical tests indicate that this
                                                                       is possible (e.g. using trees including on the order of 100
                                                                       bindings), and theoretically, good scaling is to be expected,
                                                                       since the values of the noise terms introduced in the calcu-
                                                                       lation of E(a) are dependent on the complexity of the local
                                                                       trees evaluated in each subnetwork, which have at most only
                                                                       two direct children. Finally, on a more speculative front, it
                                                                       is worth noting that constraint-based parsers need not be re-
                                                                       stricted to computing syntactic transformations. In general,
                                                                       a parser of this sort finds a local solution to an optimization
                                                                       problem, so it is conceivable that many constraint-sensitive
                                                                       cognitive processes can be modelled using the framework
                                                                       proposed here. Another interesting idea concerns integrat-
                                                                       ing this sort of parsing into the Semantic Pointer Architec-
                                                                       ture (Eliasmith, 2013), a recently proposed framework for
Figure 4: Comparison of well-formedness for different tree             describing the functional organization of neurobiological sys-
structures. The minimum in the graph corresponds to a struc-           tems. Pursuing these ideas over the long term will hope-
ture that encodes the 11 constituents present in the second tree       fully lead to the development of scalable and robust models
in Figure 2. Points to the left of the peak correspond to struc-       of constraint-based language processing.
tures that are missing constituents in the tree. The second
                                                                                           Acknowledgments
minimum likely arises as due to structures with fewer con-
stituents having a greater degree of similarity to the first tree.     This research was supported by CFI, OIT, SSHRC, NSERC
Each value is the mean of 250 trials and error bars indicate           Discovery Grant 261453, AFOSR Grant FA8655-13-1-3084,
95% confidence intervals.                                              and the Canada Research Chairs program.
                                                                                                References
   Overall, the key implication of these results is that a lossy       Eliasmith, C. (2013). How to build a brain: An architecture
encoding of a set of symbol structures into a vector space                    for neurobiological cognition. New York, NY: Oxford
seems to preserve the properties of an energy function de-                    University Press.
fined over the much larger space that is required to encode            Eliasmith, C., & Anderson, C. (2003). Neural engineering:
the structures exactly.                                                       Computation, representation, and dynamics in neuro-
                                                                              biological systems. Cambridge, MA: MIT Press.
          Conclusions and Future Directions                            Hale, J., & Smolensky, P. (2006). Harmonic grammars and
The main purpose of this work is to extend earlier research on                harmonic parsers for formal languages. In The har-
harmonic grammars and optimization in linguistic cognition.                   monic mind: From neural computaton to optimality-
One limitation of our current work is that we have not dis-                   theoretic grammar (p. 393-415). MIT Press.
cussed the problem of selecting a starting state for the parsing       Plate, T. (2003). Holographic reduced representations. Stan-
network given an input expression (e.g. ’the dog ran’). Meth-                 ford, CA: CSLI Publications.
ods for solving this problem described by Hale and Smolen-             Prince, A., & Smolensky, P. (1997). Optimality: From neural
sky (2006) could likely be incorporated into our framework.                   networks to universal grammar. Science, 275(5306),
   It is also unknown whether our use of factoring and thresh-                1604-1610.
olding to eliminate noise will have any impact on whether              Rogers, T., & McClelland, J. (2014). Parallel distributed pro-
or not a neural network can be easily designed to optimize                    cessing at 25: Further explorations in the microstruc-
E(a). As mentioned, if filtering is used, then the expression                 ture of cognition. Cognitive Science, 38(1024-1077).
for E(a) changes slightly to −aT C(Wa) where C denotes a               Smolensky, P. (2006). Tensor product representations: For-
noise clean-up function. One approach to solving this prob-                   mal foundations. In The harmonic mind: From neural
lem involves the use of the Neural Engineering Framework                      computaton to optimality-theoretic grammar (p. 271-
(Eliasmith & Anderson, 2003), a theory that describes meth-                   344). MIT Press.
ods for computing arbitrary functions in networks of spiking           Smolensky, P., & Legendre, G. (2006). The harmonic mind:
neurons. If it is possible to define a function that optimizes                From neural computaton to optimality-theoretic gram-
−aT C(Wa), then it should also be possible to compute this                    mar (Vol. 1). Cambridge, MA: MIT Press.
                                                                   243

