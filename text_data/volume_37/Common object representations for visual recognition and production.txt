                Common object representations for visual recognition and production
                        Judith E. Fan                                  Daniel L. K. Yamins                        Nicholas B. Turk-Browne
                    Department of Psychology                  McGovern Institute for Brain Research                Department of Psychology
                      Princeton University                    Massachusetts Institute of Technology                  Princeton University
                     jefan@princeton.edu                              yamins@mit.edu                                 ntb@princeton.edu
                                       Abstract                                       the external world. We test this hypothesis by evaluating two
                                                                                      predictions it makes: (1) that recognition of both objects and
  What is the relationship between recognizing objects and
  drawing objects? We examine the possibility that both func-                         human drawings can be achieved by a common visual fea-
  tions are supported by a common internal representation.                            ture representation; and (2) that training on a novel drawing
  First, we show that a model of ventral visual cortex only                           task can alter this representation, just as training on visual
  optimized to recognize objects in photographs generalizes to
  drawings of objects, suggesting that the capacity for visual                        recognition tasks can alter object representations (Goldstone,
  abstraction is rooted in the functional architecture of the visual                  1998).
  system. Next, we tested whether practice drawing objects
  might alter how those and other objects are represented. On                             Part One: Recognizing Pictures of Objects
  each trial, participants sketched an object. The model then
  guessed the identity of the sketched object, providing real-                        People can recognize objects in the face of enormous vari-
  time feedback. We found that repeatedly sketched objects were                       ation in pose, size, position, lighting, and other sources of
  better recognized after training, while sketches of unpracticed
  but similar objects worsened. These results show that visual                        noise, a fact which belies the computational difficulty of this
  production can reshape the representational space for objects:                      feat (Pinto, Cox, & DiCarlo, 2008). This ability is supported
  by differentiating trained objects and merging other nearby                         by a set of hierarchically organized brain regions known as
  objects in the space.
                                                                                      the ventral visual stream (Malach, Levy, & Hasson, 2002),
Keywords: communication; drawing; learning; perception                                by which simple visual features (e.g., orientation, spatial fre-
and action; computer vision                                                           quency) encoded in the lowest area, V1, are successively
                                                                                      combined and transformed in such a way as to support read
                               Introduction                                           out of abstract object properties (e.g., category, identity) at
                                                                                      the top level in the hierarchy, inferior temporal (IT) cortex
Although the retinal images cast by physical objects and line
                                                                                      (Hung, Kreiman, Poggio, & DiCarlo, 2005).
drawings differ dramatically, humans effortlessly recognize
objects in either format. How does the brain accomplish                                                                  OBJECT
this feat of visual abstraction? Moreover, with just a few                                              sailboat submarine elephant   dog     piano ...
well-placed strokes, humans are able to communicate abstract
                                                                                             sketch
ideas (e.g., object identity) by drawing. What are the mecha-
nisms that underlie the ability to produce a sketch that repre-
                                                                                       MODALITY
sents an object?
                                                                                             photo
                     idea
                                                                                             3D model
                                                 Figure 1: Visual recogni-
                                                 tion entails mapping an
      recognition
                                                 image onto a specific idea
                                                 (e.g., object identity based
                                    production
                                                 on a photograph).        Vi-         Figure 2: Multi-domain imageset containing sketches, photographs,
                                                 sual production entails ex-          and 3D-rendered images of 147 real-world objects.
                                                 pressing a specific idea in
                                                 an image (e.g., identifiable            Supplementing natural visual inputs from objects in the
                                                 sketch based on an object
                                                 concept). (Art credit: Jef-          environment, humans have also devised a wide range of
                            image                fery Thompson)                       technologies for producing pictures that represent objects.
                                                                                      The most ancient among these is drawing, whereby lines
   Here we examine the possibility that the ability to recog-                         and marks are made on a surface by manipulating a stylus
nize objects and produce drawings of objects are linked by a                          (Clottes, 2008). Despite large differences between drawings
common internal substrate — a generalized object represen-                            of objects and visual inputs from physical objects (or photore-
tation. This premise is plausible, given the reciprocal func-                         alistic images of objects), they are just as effective at evoking
tions of recognition and production (Fig. 1). Specifically, vi-                       the real-world object (Biederman & Ju, 1988).
sual recognition entails mapping an image from the external                              What commonalities across line drawings and photoreal-
world onto a specific idea in mind; visual production entails                         istic images (e.g., photographs, 3D-renderings) allow people
expressing a specific idea in mind as an image residing in                            to recognize the same object depicted in such different ways?
                                                                                650

Discovering the computational principles that underlie such
                                                                                                                        Operations in Linear-Nonlinear Layer                                            Behavioral Tasks
robust recognition is a challenge that lies at the heart of hu-                                                          ⊗Φ    1
                                                                                                                                                                                                        e.g., Trees vs. Non-Trees
                                                                                                                         ⊗Φ
man visual abstraction. Here we present a computational ap-
                                                                                                                               2
                                                                                                                          ...
                                                                                                                         ⊗ Φk                                                  ...                                                                  ...
                                                                                                                        Filter      Threshold         Pool Normalize
proach to quantifying such commonalities (Fan, Yamins, Di-
Carlo, & Turk-Browne, 2014; Yamins et al., 2014).                                                                       LN                      LN                                        1. Optimize Model for Task Performance
                                                                                                 Spatial Convolution                                              LN
                                                                                                                                                LN
                                                                                                 over Image Input       LN                                                              LN
                                                                                                                                                                   ...                   ...
                                                                                                                         ...                    ...                                     LN                  layer 4
Methods                                                                                                                                         LN
                                                                                                                                                                  LN                 layer 3
                                                                                                                        LN
                                                                                                                                          layer 1               layer 2
                                                                                                                                                                               2. Test Per-Site Neural Predictions
Imageset We first assembled a multi-domain imageset
containing sketches, photographs, and 3D-rendered images
(Fig. 2). From an existing sketch corpus (Eitz, Hays, & Alexa,                                                   100ms
                                                                                                                                        V1
                                                                                                                                                         V4
                                                                                                                 Visual
2012), we obtained ∼12,000 sketches of 147 common, real-                                                      Presentation
                                                                                                                                                                                                  ...                         ...
                                                                                                                                            V2                   IT
world objects. These sketches were produced by human par-                                                                                                                                      Neural Recordings from IT and V4
ticipants on Amazon Mechanical Turk, who were prompted                                                                               Layer 1                                                            Top Layer
                                                                                                                        sa
                                                                                                                                        e                                                      sa                            nt
                                                                                                                                                            nt                                 suilbo
on each trial with a randomly chosen entry from a list of 250                                                           suilbo
                                                                                                                           bm at
                                                                                                                              arin
                                                                                                                                                do
                                                                                                                                                eleg
                                                                                                                                                   p
                                                                                                                                                          ha
                                                                                                                                                                 pian
                                                                                                                                                                         o                        bm at
                                                                                                                                                                                                     arin e
                                                                                                                                                                                                                       d
                                                                                                                                                                                                                      eleog
                                                                                                                                                                                                                          ph a
                                                                                                                                                                                                                                    pi
                                                                                                                                                                                                                                      o
                                                                                                                                                                                                                                      an
basic-level object categories to sketch on a digital drawing                                             bmsa
                                                                                                           ar ilbo                                                                                                                                t ine
canvas. From the annotated Imagenet database (Deng et al.,                                                   in at                                                                                                                              oa ar
                                                                                                                e                                                                                                                            ilb m
                                                                                     sketch
                                                                                                                                                                                                                                           sa sub
2009), we acquired ∼200K photographs of the same 147 ob-
jects, depicting diverse exemplars from each object class em-                                              ha dog
                                                                                                             nt                                                                                                                              g ep
                                                                                                                                                                                                                                           do el
                                                                                          su
                                                                                                         ele                                                                                                                                   ha
bedded in their natural backgrounds. Finally, using 3D mesh                                                 p
                                                                                                                                                                                                                                                 nt
                                                                                                               o                                                                                                                           pi
models, we rendered ∼200K synthetic images of these same                                                  pi
                                                                                                            an                                                                                                                              an o
objects in highly variable positions, sizes, and poses against                                                                  ρ=0.07                                                             ρ=0.58
randomly selected real-world backgrounds.                                                                 m sai
                                                                                                           ar lb                                                                                                                                  t ine
                                                                                                             in oa
                                                                                                               e t                                                                                                                              oa ar
                                                                                                                                                                                                                                             ilb m
                                                                                                                                                                                                                                           sa sub
                                                                                     photo
Neurally Predictive Model of Object Recognition We                                                       ub
                                                                                                                                                                             ρ=0.56
then applied a recently developed deep convolutional neural                                              ele
                                                                                                            ph do                                                            ρ=0.11                                                          g eph
                                                                                                              an g                                                                                                                         do el
network model that was inspired by the functional architec-                              s
                                                                                                                t                                                                                                                               an  t
ture of the ventral visual stream in order to extract features                                             pi
                                                                                                              no
                                                                                                              a
                                                                                                                                                                                                                                           pi
                                                                                                                                                                                                                                             ano
from these images (Yamins et al., 2014; Fig. 3a). This model                                                                       ρ=0.13                                                        ρ=0.55
had been identified using hierarchical modular optimization                                              ub s
                                                                                                                                                                                                                                                  t ine
                                                                                     3D model
                                                                                                           m ail                                                                                                                                oa ar
                                                                                                            ar bo
(HMO), a procedure for efficiently searching among mixtures                                                   in at                                                                                                                          ilb m
                                                                                                                 e                                                                                                                         sa sub
of convolutional neural networks for candidate hierarchical
                                                                                                            ha dog                                                                                                                            g eph
model architectures that achieve high performance on basic-                                              ele
                                                                                                              nt
                                                                                                                                                                                                                                            do el
                                                                                                                                                                                                                                                 an
                                                                                          s
                                                                                                             p
level object recognition tasks. The HMO procedure was per-
                                                                                                                                                                                                                                                     t
                                                                                                               o                                                                                                                            pi
                                                                                                            an                                                                                                                               ano
formed on an independent imageset containing photographs                                                  pi
only, with no objects in common with the multi-domain im-                 Figure 3: (a) Feature extraction using a neurally predictive, deep
ageset described above. In addition to achieving human-level              convolutional neural network model optimized for performance on
                                                                          challenging object recognition tasks. (b) Correlation matrices for
performance on these tasks, the higher layers of the resulting            each image domain, displaying the overall layout of objects in high-
model are also quantitatively predictive of neural population             dimensional feature space. Each entry shows correlation distance
responses in high-level visual cortex (e.g., V4 and IT). As               (1-ρ) between feature vectors for a pair of objects.
such, it was an attractive candidate for investigating the visual
invariants that support recognition across image domains.
                                                                                                   0.6 sketch 3D
                                                                           Cross-Domain Similarity (ρ)
Results                                                                                                sketch photo
                                                                                                   0.5 photo 3D
The model uses a fixed, but large number of feature dimen-
sions to represent all images. Each image elicits a pattern                                        0.4
of feature values at every layer in the model, which may be
expressed as a vector in this high-dimensional feature space.                                      0.3
For a given image domain, we computed average feature vec-
                                                                                                   0.2
tors within an object class, then derived correlation matri-
ces based on these feature vectors. This procedure was per-                                        0.1
formed at each of the five layers of the model. Each matrix
entry represents the proximity between the average feature
                                                                                                                       1                                  2                3                                            4                      Top
vectors from the model for a pair of objects (Kriegeskorte
                                                                                                                                                                       Model Layer
et al., 2008). Higher values (cooler colors) reflect relatively
                                                                          Figure 4: Cross-domain similarity (Spearman’s ρ) between image
proximal pairs of objects, whereas smaller values reflect more            domains increases as a function of model layer.
distant object pairs. Each 147x147 matrix provides a compact
                                                                    651

visualization of the layout of objects in the high-dimensional          realistic images and hand-drawn sketches of objects. Insofar
feature space inherent to each layer of the model, for each             as both the ability to recognize sketches and to produce recog-
image domain (see Fig. 3b for first and top-layer matrices).            nizable sketches recruit a common internal representation, we
   All matrices individually show clear block-diagonality, in-          hypothesized that practice drawing some objects (e.g., horse,
dicating the presence of higher-order structure due to cluster-         cow) might affect the way that those and other, related objects
ing of objects with similar features.                                   (e.g., sheep) are subsequently represented.
   The matrices computed based on top-layer output also                    Since recognizability is a key attribute of successful draw-
show striking cross-domain similarities, both visually and as           ings, a natural starting point for examining learning is to iden-
quantified by Spearman rank correlation comparisons (Fig.               tify objects for which untrained participants have trouble pro-
3b). This indicates an underlying commonality in the fea-               ducing clearly recognizable drawings — that is, that are fre-
ture representations for the three image modalities at the top          quently confused with drawings of other objects. The most
layer in the model, the layer whose output has been previ-              confusable objects are likely to be objects whose drawings
ously shown to be highly predictive of neural population re-            share many visual features, even if the objects themselves are
sponses in IT cortex.                                                   not semantically related, per se (e.g., bell and pear). To de-
   By contrast, cross-domain similarities are negligible at the         fine groups of related objects (i.e., ‘visual categories’), we
lowest layer in the model, the layer approximating the lo-              exploit pre-existing object clusters revealed by the model in
cal/simple features encoded in V1. This shows that low-level            the original sketch corpus collected by Eitz et al. (2012).
image statistics (e.g., edge fragments) are insufficient to ex-
plain robust recognition across image modalities, especially               a               airplane      bed        bell
                                                                                                                        categories
                                                                                                                               cat   banana ﬂoor lamp   cactus    SUV
under conditions of high image variation.                                                   blimp      bench    frying-pan    cow   dolphin    fork      crab   bicycle
                                                                                          crocodile     chair       hat    elephant   duck    guitar    giraffe   bus
                                                                                objects
   We found that the strength of cross-domain similarities in-                                ﬁsh      couch        pear     horse  mosquito hammer    lobster motorbike
                                                                                          helicopter    harp       shoe    kangaroo mouse microphone palm tree race car
creased over successive layers in the model (Fig. 4), con-                                    ship     ladder      socks      pig    seagull  shovel  pineapple  train
sistent with the understanding that the ventral visual stream                              trumpet     laptop   tablelamp rabbit      shark   snake      tiger   truck
computes progressively more abstract properties of objects                 b                 violin     table     teapot    sheep     swan    spoon     zebra     van
over successive processing stages.                                                         trained                 pre                training                 post
                                                                                           near                                       {         }
   In sum, our results show that a hierarchical neural net-                                                                                x5
                                                                                           far
work model only optimized to recognize photorealistic im-
ages of objects generalized to abstract drawings of objects,            Figure 5: (a) Stimuli: Objects belonged to eight visual categories,
                                                                        each containing eight items. (b) Design: Each participant was ran-
having produced congruent object-similarity ‘maps’ across               domly assigned two of these categories. During training, partici-
image domains based on the same visual feature represen-                pants drew four randomly selected objects in one category (Trained)
tation. These results suggest that the capacity for visual ab-          multiple times. Before and after training, participants drew the other
                                                                        four objects in that category (Near), as well as the objects in the
straction may be rooted in the functional architecture of the           second category (Far), once each.
visual system.
   Part Two: Producing Drawings of Objects                              Methods
How does learning refine object representations? For exam-              Participants Six hundred and fifty-one participants were
ple, although people tend to label an object at the basic level         recruited via Amazon Mechanical Turk (AMT) for the draw-
(Mervis & Rosch, 1981), domain-specific expertise (e.g.,                ing experiment, with sixty excluded for failing to complete
knowledge of dogs) makes subordinate-level names (e.g.,                 the session. Participants were paid a base amount of $1.50
‘schnauzer’) as accessible (if not more accessible) than basic-         and up to $3.00 bonus for high task performance. Three hun-
level names for objects in the domain of expertise (Tanaka &            dred and twenty-seven additional participants were recruited
Taylor, 1991). This suggests that initially similar stimuli can         (via AMT) to provide labels for the sketches from the drawing
become more differentiated with practice. Expertise can also            experiment, and were paid $0.85 for their participation. All
lead to unitization of features that were initially processed           provided informed consent in accordance with the Princeton
separately. For instance, dog experts are worse at recognizing          IRB.
inverted images of dogs than non-experts (Diamond & Carey,              Stimuli and Design In order to identify groups of objects
1986), suggesting that extensive experience with an object              that are drawn similarly prior to training, we applied a cluster-
can lead to automatic binding of features into a viewpoint-             ing algorithm (affinity propagation with damping=0.9; Frey
specific functional unit.                                               & Dueck, 2007) to the features extracted from the 147-
   Such findings suggest that training on recognition tasks can         object sketch corpus described above (Eitz et al., 2012). This
alter object representations. Here we ask whether training on           yielded 16 clusters containing between 3 and 20 objects each.
visual production tasks can also alter representations of ob-           Among clusters containing at least 8 objects, we defined 8
jects. In the previous section, we found that computations              visual categories containing 8 objects each (Fig. 5a). Each
approximating those performed by the ventral visual stream              participant was randomly assigned two of these categories.
produced congruent object similarity ‘maps’ for both photo-             During training, participants sketched 4 randomly selected
                                                                  652

objects in one category (Trained) multiple times. Before and                       Feedback We trained a 64-way support vector machine
after training, participants sketched the other 4 objects in that                  (SVM) linear classifier on model responses to photographs
category (Near), as well as the objects in the second category                     of the objects used in this study, but no sketches. (Thus,
(Far), once each (Fig. 5b).                                                        sketch-classification during the experiment reflects pure gen-
Task The sketching task was performed in the context of a                          eralization across image domains.) On each trial, top-layer
game (‘Guess My Sketch’) in which participants teamed up                           model features were extracted from the submitted sketch in
with two avatars (red, blue) in order to earn points. At the                       real time, which were passed to the 64-way classifier to de-
start of each trial, only the red avatar was onscreen. This                        termine feedback. The classifier returned a list of 64 margin
avatar cued participants with either an image (N=324) or                           values, corresponding to the level of confidence that the test
word (N=267) that referred to a target object for them to                          image belonged to each object class. The three objects with
sketch (Fig. 6). After cue offset, the blue avatar appeared,                       the most positive margin values (highest confidence) were re-
prompting the participant to begin sketching. Upon sketch                          turned to the participant as guesses. In the verbal-cue version
submission, the blue avatar listed its top three guesses as to                     of the task, when none of the three top guesses were correct,
the identity of the drawn object, thus providing participants                      the rank of the target in this ordered object list was also re-
with immediate feedback about the quality of their sketch.                         turned to the participant (e.g.,“Too bad...‘giraffe’ would have
These guesses were listed in order of confidence. Participants                     been my 9th guess.”). Because this target rank value provides
earned points if any of these guesses were correct, and more                       a consistent measure of the ‘goodness-of-fit’ of the submit-
points the earlier the correct guess was in the list.                              ted sketch to the target object representation in the model,
                                                                                   this value served as our primary measure of task performance.
                                                                                   Since the criteria for recognition by the model were fixed, we
             start of                                                              interpret changes in task performance as reflecting changes to
                trial                                                              the participants’ internal object representations.
                                                                                   Validating Model Representation Because the conditions
                                                                                   used by Eitz et al. (2012) to collect sketches differ somewhat
   image cue                                    verbal cue                         from our own (e.g., only verbal cues were used, each partic-
                                                                                   ipant sketched an object only once, and could apply ‘undo’,
                                                                                   ‘redo’, ‘clear’ on their sketches), we first sought to assess the
                                                                                   similarity between their sketch corpus and the sketches col-
                                                                                   lected for this experiment. To accomplish this, we extracted
                                                            1   horse
                                                                                   features of sketches from the verbal-cue and image-cue ver-
      (3s)
                                                                cow
                                                target
                                                            2
                                                            3   sheep
                                                                                   sions of the task using top-layer output from the model.
                                                  rank
                                                            4   elephant
       sketching                                            5   pig
            task
                                                            6   helicopter
                                                            7   truck
                                                                                                               5.0
     (until submission)                                     8   bench
                                                                                                                        r = 0.649***
                                                            9   shoe
                                                                                                                        (p < 0.001)
                              submit                       10   dolphin
                                                                …
                                                                                                               4.0
                                                           55   zebra
                                                                                                 computer d’
                                                           56   trumpet
                                                           57   bell                                           3.0
                                                           58   teapot
                                            classifier     59   couch
                                            feedback       60   shovel
                                                                                                               2.0
                                            (3 guesses
                                                           61   motorcycle
                                            displayed)
                                                           62   frying-pan
                                                           63   mouse
                                                                                                               1.0
                                                           64   cactus
Figure 6: Task: On each trial, participants were prompted with an                                              0.0
                                                                                                                  0.0       1.0       2.0   3.0   4.0   5.0
image (N=324) or word (N=267) that referred to a target object for
them to sketch. The computer guessed the identity of the drawn                                                                        human d’
object in real time, providing participants immediate feedback about               Figure 8: An independent cohort of human participants guessed
the quality of their sketch. The rank of the target in the list of all 64          the identity of objects depicted in drawings from the image-cue ex-
guesses (ordered by confidence) returned by the computer was used                  periment. Human and computer recognition performance (d’) was
to track changes in performance across trials.                                     highly consistent across objects (r=0.649).
   In the image-cue version of the task, unique photographs                           For each version of the task, we computed the average fea-
were used as cues on every trial in order to discourage overly                     ture vectors for all sketches within an object class, then de-
stereotyped sketches. Participants were instructed to “make a                      rived correlation matrices on these feature vectors. In both
sketch in which someone else is likely to recognize the object                     the image-cue and verbal-cue datasets, we found that sketches
depicted,” but were informed that the sketch did not have to                       of objects within a category were highly similar, validating
exactly depict what was in the photo. Other than the lack of an                    category assignments. The two matrices were also highly
image cue, the verbal-cue version of the task was identically                      similar to each other (Spearman’s ρ=0.890), suggesting that
structured.                                                                        this feature representation successfully captured object iden-
                                                                             653

                                                                              bus                                         giraffe                palm tree                       kangaroo                        horse
                                                           TARGET
                                                           GUESS
                                                                              bus                                         giraffe                  crab                             pear                         horse
                                                               Figure 7: Sample sketches from the experiment, with target label and model’s top guess.
tity despite low-level task differences. Moreover, both matri-                                                                                          We then performed the same type of ANOVA as was used
ces were highly similar to the original sketch corpus (image-                                                                                           for the pre-test analysis, which revealed a highly significant
original: ρ=0.715; verbal-original: ρ=0.708). An indepen-                                                                                               difference among conditions (F 2,1178 =12.7, p<0.001). There
dent cohort of human participants (N=327) provided three la-                                                                                            was no main effect of cue-type (F 1,589 =0.213, p=0.644), and
bels to each sketch from the image-cue experiment, in order                                                                                             no interaction with condition (F 2,1178 =0.365, p=0.695), so
of confidence, from the set of 64 object labels. We found that                                                                                          in subsequent analyses we collapsed across cue-type. A
human and model recognition performance (d0 ) was highly                                                                                                follow-up t-test revealed that sketches of Trained objects were
consistent across objects (Spearman’s ρ=0.649, Fig. 8).                                                                                                 better recognized by the model following training (∆rank <
                                                                                                                                                        0:t590 =3.89, p=0.0001), and this improvement was also sta-
Consequences of Drawing Practice Since the assignment
                                                                                                                                                        tistically reliable when compared with performance on Far
of objects to condition was randomized across participants,
                                                                                                                                                        objects (∆rank,trained < ∆rank, f ar : t590 =3.05, p=0.002). By con-
no differences in performance (target rank) on Trained, Near,
                                                                                                                                                        trast, model performance for sketches of Near objects wors-
and Far objects were predicted during the pre-test. To test
                                                                                                                                                        ened after training relative to baseline (∆rank < 0: t590 =2.03,
this, we computed the mean target rank in each condition for
                                                                                                                                                        p=0.04) and relative to control Far objects (∆rank,near <
each participant (Trained=9.92, Near=9.24, Far=9.57), which
                                                                                                                                                        ∆rank, f ar : t590 =2.15, p=0.03). Recognition of Far objects
we then analyzed using a 3-condition (Trained, Near, Far)
                                                                                                                                                        did not change significantly relative to baseline (∆rank < 0:
x 2 cue-type (image, verbal) repeated-measures ANOVA.
                                                                                                                                                        t590 =0.751, p=0.453). This was true when computing the tar-
There was no main effect of condition on pre-test perfor-
                                                                                                                                                        get rank among all distractors (Fig. 9a), as well as when clas-
mance (F 2,1178 =1.70, p=0.184). Cue type did have an effect
                                                                                                                                                        sification was restricted to objects within the target category
(F 1,589 =19.7, p<0.001), but did not interact with condition
                                                                                                                                                        (Fig. 9b).
(F 2,1178 =0.258, p=0.773).
   Our main hypotheses concerned changes in performance                                                                                                    These results show that visual production reshaped par-
due to focused drawing practice on the Trained objects.                                                                                                 ticipants’ representational space for objects: by differentiat-
Specifically, we predicted that repeatedly sketching a subset                                                                                           ing trained objects and merging other objects nearby in the
of the objects in one category would affect how other similar                                                                                           space (Fig. 10). More broadly, these findings suggest that the
objects belonging to the same category were drawn, but that                                                                                             outward expression of visual concepts can itself bring about
such practice would not affect how unrelated objects were                                                                                               changes to their internal representation.
drawn.
                                                                                                                                                          a                trained       near              far                  b
      a                                across all objects
                                                                                 b                             within target category
                                                                                                                                                                           pre   post   pre   post   pre         post
                                                                                                                                                                                                                                       trained
                                                                                                       0.3                                                                                                                             near
                                                                                                                                                           trained
                                                                                                                                                                post pre
                                                           *                                                                                                                                                             0.16          far
                             1.0                     *
                                                                                                                                                                                                                                pre
                                                                           Δ target rank (post-pre)
                                                                                                       0.2
                                                                                                                                     *                                                                                   0.14
 Δ target rank (post-pre)
                             0.5                                                                                               **
                                                                                                                                                                post pre
                                                                                                                                                                                                                         0.12
                                                                                                                                                           near
                                                                                                       0.1
                             0.0
                                                                                                                                                                                                                         0.10
                                                                                                       0.0
                            − 0.5                                   n.s.                                                                                                                                                 0.08
                                                                                                      − 0.1                               n.s.                                                                           0.06
                            − 1.0                                                                                                                               pre
                                                                                                                                                           far                                                           0.04   post
                            − 1.5                                                                     − 0.2
                                      ***                                                                       ***                                             post
                                                                                                                                                                                                                         0.02
                                                     **                                               − 0.3                    **
                            − 2.0             ***                                                                       ***
                                    trained         near            far                                       trained         near       far                                                                             0.00
Figure 9: Changes in performance between post-test and pre-test for
each condition: (left) when computing target rank among all 63 dis-                                                                                     Figure 10: (a) Representational similarity between conditions across
tractors and (right) when computing target rank relative to remain-                                                                                     phases (averaged over object identity): low values in the large off-
ing 7 objects within the target category only. Error bars represent 1                                                                                   diagonal blocks correspond to larger average correlation distances
s.e.m. *p<0.05, **p<0.01,***p<0.001                                                                                                                     between objects in different categories. More subtle changes un-
                                                                                                                                                        derlying the effects shown in Fig. 9 are reflected in the smaller di-
                                                                                                                                                        agonal and off-diagonal blocks. (b) Visualization of changes to the
  To evaluate this prediction, we calculated the change in tar-                                                                                         representation using multidimensional scaling on the correlation dis-
get rank for each item (∆rank = rank post − rank pre ), then aver-                                                                                      tances between objects in each condition.
aged these ∆rank values for each condition within-participant.
                                                                                                                                                  654

                          Discussion                                   Diamond, R., & Carey, S. (1986). Why faces are and are
                                                                         not special: an effect of expertise. Journal of Experimental
Humans draw for many reasons: to depict, to record, to plan,
                                                                         Psychology: General, 115(2), 107.
to explain, to create (Tversky, 2011). Drawn images predate
                                                                       Eitz, M., Hays, J., & Alexa, M. (2012). How Do Humans
the historical record (Clottes, 2008), are pervasive in human
                                                                         Sketch Objects? ACM Transactions on Graphics (TOG),
culture (Gombrich, 1989), and are often produced prolifically
                                                                         31(4), 44.
in childhood (Kellogg, 1969). Moreover, drawing is a pow-
                                                                       Fan, J. E., Yamins, D., DiCarlo, J., & Turk-Browne, N. B.
erful tool for communication — with just a few strokes it is
                                                                         (2014). Mapping core similarity among visual objects
possible to convey the identity of a face (Bergmann, Dale,
                                                                         across image modalities. In ACM Siggraph 2014 Posters
& Lupyan, 2013) or express an intention (Galantucci, 2005).
                                                                         (p. 67).
Just as investigations of both verbal comprehension and pro-
                                                                       Fay, N., Garrod, S., Roberts, L., & Swoboda, N. (2010).
duction are indispensable to theories about linguistic com-
                                                                         The interactive evolution of human communication sys-
munication, a more complete understanding of visual com-
                                                                         tems. Cognitive Science, 34(3), 351–386.
munication will entail examining how visual recognition and
                                                                       Frey, B. J., & Dueck, D. (2007). Clustering by passing mes-
production interact to achieve our goals.
                                                                         sages between data points. Science, 315(5814), 972–976.
   Although we interpret our results as supporting the idea            Galantucci, B. (2005). An experimental study of the emer-
that training had reshaped participants’ internal represen-              gence of human communication systems. Cognitive Sci-
tation of objects, another possibility is that they had only             ence, 29(5), 737–767.
adapted their responses based on classifier feedback. Exam-            Goldstone, R. L. (1998). Perceptual learning. Annual Review
ining the generality of these effects across different tasks in          of Psychology, 49(1), 585–612.
subsequent studies will be helpful for teasing apart these two         Gombrich, E. (1989). The story of art. Phaidon Press, Ltd.
accounts. Specifically, future experiments will examine how            Gureckis, T. M., & Markant, D. B. (2012, September). Self-
learning to draw objects affects how these objects are later             directed learning: A cognitive and computational perspec-
perceived, to further evaluate the idea that visual production           tive. Perspectives on Psychological Science, 7(5), 464–
alters a generalized object representation that supports both            481.
recognition and production. In addition, we plan to investi-           Hung, C. P., Kreiman, G., Poggio, T., & DiCarlo, J. J. (2005).
gate how visual learning achieved via active production dif-             Fast readout of object identity from macaque inferior tem-
fers from that achieved through passive observation (Gureckis            poral cortex. Science, 310(5749), 863–866.
& Markant, 2012), involving close examination of how sen-              Kellogg, R. (1969). Analyzing children’s art. National Press
sory feedback (e.g., visual, tactile) and social interaction (Fay,       Books Palo Alto, CA.
Garrod, Roberts, & Swoboda, 2010) influence learning. Ul-              Kriegeskorte, N., Mur, M., Ruff, D. A., Kiani, R., Bodurka,
timately, inquiries into the psychological basis of visual pro-          J., Esteky, H., . . . Bandettini, P. A. (2008). Matching cate-
duction may shed new light upon the origins of symbolic writ-            gorical object representations in inferior temporal cortex of
ing systems for communication, and the very nature of our                man and monkey. Neuron, 60(6), 1126–1141.
ability to apprehend abstract meanings from visual artifacts.          Malach, R., Levy, I., & Hasson, U. (2002). The topogra-
                                                                         phy of high-order human object areas. Trends in Cognitive
                    Acknowledgments                                      Sciences, 6(4), 176–184.
This work was supported by NSF GRFP DGE-0646086                        Mervis, C. B., & Rosch, E. (1981). Categorization of natural
(J.E.F), NIH R01 EY021755 (N.B.T.-B.), and the David A.                  objects. Annual review of Psychology, 32(1), 89–115.
Gardner ’69 Magic Project at Princeton University.                     Pinto, N., Cox, D. D., & DiCarlo, J. J. (2008). Why is real-
                                                                         world visual object recognition hard? PLoS Computational
                         References                                      Biology, 4(1), e27.
                                                                       Tanaka, J. W., & Taylor, M. (1991). Object categories and
Bergmann, T., Dale, R., & Lupyan, G. (2013). The impact                  expertise: Is the basic level in the eye of the beholder? Cog-
   of communicative constraints on the emergence of a graph-             nitive Psychology, 23(3), 457–482.
   ical communication system. In Proceedings of the 35th               Tversky, B. (2011). Visualizing thought. Topics in Cognitive
   Annual Conference of the Cognitive Science Society (pp.               Science, 3(3), 499–535.
   1887–1992).                                                         Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seib-
Biederman, I., & Ju, G. (1988). Surface versus edge-based                ert, D., & DiCarlo, J. J. (2014). Performance-optimized hi-
   determinants of visual recognition. Cognitive Psychology,             erarchical models predict neural responses in higher visual
   20(1), 38–64.                                                         cortex. Proceedings of the National Academy of Sciences,
Clottes, J. (2008). Cave art. Phaidon London.                            201403112.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei,
   L. (2009). Imagenet: A large-scale hierarchical image
   database. In Computer Vision and Pattern Recognition,
   2009. (pp. 248–255).
                                                                   655

