                         Assessing a Bayesian account of human gaze perception
                                             Peter C. Pantelis (PCPANTEL@indiana.edu)
                                                 Daniel P. Kennedy (DPK@indiana.edu)
                          Indiana University–Bloomington, Department of Psychological and Brain Sciences
                                               1101 E. 10th Street, Bloomington, IN 47405 USA
                               Abstract                                    gaze perception may employ a similar mechanism, this re-
                                                                           mains untested. Thus, we here ask whether a Bayesian model
   Although gaze can be directed at any location, different lo-
   cations in the visual environment vary in terms of how likely           that incorporates a visual saliency map as a prior can account
   they are to draw another person’s attention. One could there-           for actual human subjects’ performance better than one which
   fore weigh incoming perceptual signals (e.g., eye cues) against         ignores this information, and uses only the eye cues.
   this prior knowledge (the relative visual saliency of locations
   in the scene) in order to infer the true target of another person’s        Our experimental subjects view photographs of another
   gaze. This Bayesian approach to modeling gaze perception has            person gazing at various locations on a partially transparent
   informed computer vision techniques, but we assess whether              surface situated between him and the camera. The subjects
   it is a good model for human performance. We present sub-
   jects with a “gazer” fixating his eyes on various locations on          are instructed to indicate where on this surface this other per-
   a 2-dimensional surface, and project an arbitrary photographic          son is looking; we define this task computationally as the
   image onto that surface. Subjects judge where the gazer is              inference of the location [x, y] where the photographed indi-
   looking in the image. A full Bayesian model, which takes im-
   age saliency information into account, fits subjects’ gaze judg-        vidual is gazing within the continuous 2-dimensional plane
   ments better than a reduced model that only considers the per-          (Gx,y ) given the gaze directional cue from the eyes of the per-
   ceived direction of the gazer’s eyes.                                   son (D) and the image presented in that plane (I). Bayes’ rule
   Keywords: gaze; Bayesian modeling; social perception; so-               yields the posterior probability distribution, continuous over
   cial attention; visual saliency                                         the 2-dimensional hypothesis space:
                           Introduction                                                     p(Gx,y |D) ∝ p(D|Gx,y )p(Gx,y ).             (1)
Another person’s gaze direction is a strong cue for where this                In our treatment, the prior—p(Gx,y )—is equivalent to the
person may be directing his or her visual attention, and there-            relative visual saliency of location [x, y] within image I, where
fore helps one to infer what may be on his or her mind. Ad-                saliency is some model of where people are a priori likely to
ditionally, because people (and other animals) tend to direct              direct their visual attention and fixation.
their visual attention to the informative and behaviorally rele-              One example of how human gaze perception incorporates
vant areas of the environment (Mackworth & Morandi, 1967),                 prior information under conditions of uncertainty is that peo-
the ability to infer attention also provides hints as to the im-           ple show a prior bias that another person’s gaze is directed to-
portant things that may be happening in the immediate envi-                ward them (Ken, 1990; Mareschal, Calder, & Clifford, 2013).
ronment (Byrne & Whiten, 1991). Given that the direction                   This empirical finding makes sense given basic intuitions
of another person’s eye fixation is a basic cue for tracking               about human nature. That is, other people’s faces (including
gaze (and therefore, attention), the human visual system has               one’s own) would naturally be regions of interest in a coun-
evolved to process this perceptual signal with remarkable ac-              terpart’s visual scene (Yarbus, 1967), and even the most mun-
curacy and efficiency (Cline, 1967).                                       dane face is surely more interesting than, say, the empty space
   Nevertheless, the perceptual signal extracted from another              immediately to the left and right of it.
person’s eyes is noisy and ambiguous. As such, other cues                     However, it should be clear that indeed all of the locations
like head position (Wallaston, 1824; Ken, 1990; Langton,                   in the counterpart’s visual environment (including one’s own
2000) also inform the judgment of gaze direction. But ad-                  face) are salient to varying degrees—that is, a priori more
ditionally, if one has reliable intuitions about where in the vi-          or less likely to capture the other person’s visual attention.
sual scene another person is likely to direct his or her gaze—a            Thus, we predict that that prior considerations with respect to
priori of perceiving the signal from his or her eyes—then this             presumed visual saliency should, in the general case, factor
prior information could potentially be integrated with the eye             into human gaze perception.
cue to improve the inference of gaze direction.
   This basic approach—combining perceptual cues from the                                              Methods
target person’s eyes (or head position, etc.) with the visual              Subjects
saliency of the scene—has been exploited in computer vi-                   23 undergraduates at Indiana University received course
sion to improve machine performance both in the discrim-                   credit for their participation in the experiment.
ination of gaze direction (Hoffman, Grimes, Shon, & Rao,
2006; Yücel et al., 2013) and in the related task of identify-            Stimuli
ing where another person is pointing (Schauerte, Richarz, &                Photographs of the “gazer” We took a set of photographs
Fink, 2010). And although it has been speculated that human                of a young man (the “gazer”) seated behind a glass surface.
                                                                       1811

                Block 1                                                                              Blocks 2-5
                                      1400 ms                                         1400 ms
                                            500 ms                                500 ms
                                                           Wait for click
Figure 1: After the presentation of a fixation cross for 1400 ms, the scene appeared. After 500 ms, a mouse cursor appeared
as a red square at a random location within the projected image (this image was a photograph in block 1, and uniform gray in
blocks 2-5). The subject indicated with a mouse click where he or she thought the gazer was looking. After the subject clicked,
the next trial began. (Note: The fixation crosses and red mouse cursors are enlarged in this figure to be more visible.)
In each photograph, the gazer fixated his eyes on a differ-          perimposed on the photograph. When the gazer had been
ent location on the glass surface, where a grid of points had        photographed, he had always fixated on locations that would
been marked (later, these marks were digitally removed from          have fallen within this gray frame. Either an image (for block
the photographs, leaving no observable trace). Though other          1) or uniform gray (for blocks 2-5) was presented within the
cues (such as head position) can also be exploited to infer          rectangular gray frame in each presented scene, and alpha
the target of gaze, for this experiment we aimed only to vary        blended (at alpha = 180, where 0 is fully transparent and 255
the eye cues among these photographs. Therefore, the gazer           is fully opaque) with the background photograph of the gazer
maintained minimal head and body movement as he fixated              (see Fig. 1). For the subject, this created a perceptual effect
on the various locations on the glass surface.                       akin to the subject and gazer being on opposite sides of a par-
   The height of the origin of this grid of points, the camera       tially transparent surface, with the gazer’s silhouette faintly
lens, and the center point between the gazer’s eyes was 125          visible through it. Only a tight ellipse around the gazer’s eyes
cm. The glass surface was 115 cm from the gazer’s face, and          was fully visible through the image, with the area around the
the glass surface was 160 cm from the camera. The gazer’s            eyes smoothly transitioning to greater opacity. Thus, in either
face was lit from above, both from the left and right, so as to      condition (projected image, or uniform gray), the gazer’s eyes
avoid casting heavy shadows on his face. The photographs             were made fully visible to the subject, and presented simulta-
were taken with a Canon EOS Digital Rebel XT camera, a 50            neously with the supposed target of his gaze.
mm lens, 1/125 s exposure time, and no flash. The original           Projected images For the first block of trials, images were
resolution of these photographs was 3456×2304 pixels.                projected onto the plane upon which the gazer had fixated.
   Thirty-three photographs were used in the experiment, cor-        The 165 color images (provided by Judd, Ehinger, Durand,
responding to when the gazer had been fixating on 32 respec-         & Torralba, 2009) included a wide range of indoor and out-
tive points in a lattice spread over 7 rows and 9 columns (the       door scenes, some containing and some not containing peo-
first row had 5 dots spaced at even 10 cm intervals, the second      ple. These images were originally 768×1024 pixels, but were
row had 4 dots with the same spacing but shifted 5 cm, the           resized to fit the presented 550×733 frame.
third row had 5 dots, and so on), plus the origin (i.e. straight
ahead, and directly into the camera).                                Procedure
   The experiment was presented on a 2560×1440 pixel dis-
play. One of the 33 photographs of the gazer appeared in ev-         The experiment consisted of 5 blocks, each consisting of 165
ery trial of the experiment, within a 1200×800 pixel window          trials. The subject took a 5 minute break after the 3rd block.
at the center of the display. The unused, background portion            Before the first trial of each block, four photographs were
of the display (falling outside of the edges of the 1200×800         displayed in succession, each for 1 s. In these four pho-
pixel window) was made gray.                                         tographs, the gazer was fixated on four respective locations
   For every trial, a rectangular gray frame (inner dimensions:      (marked with 8 × 8 pixel black squares) near the four respec-
550×733 pixels; outer dimensions: 570×753 pixels) was su-            tive corners of the gazed-upon glass surface. This was a “cal-
                                                                 1812

ibration” of sorts for the subject, who could get a sense of         personalized likelihood functions. Each of these probabilistic
how the gazer’s eyes were positioned when he had been pho-           2-dimensional likelihood maps was renormalized to sum to
tographed fixating on the extremes of the glass surface.             1. For an example of a likelihood map derived for one subject
   Each trial began with a black fixation cross, presented at        and one of 33 directional cues from the eyes, see Figure 2.
the center of the screen for 1.4 s against a gray background.
The subject was then presented with a static scene. Over the
course of each block, these scenes featured each of the 33
photographs of the gazer (fixating on 33 respective locations)
5 times, with these 165 total trials being randomly shuffled.
   For the first block, one of 165 color images (from the Judd
et al., 2009 set) was randomly assigned to each of these 165
trials and projected into the frame in front of the gazer; thus,
the projected image and the direction of the subject’s gaze
in the photograph were randomly paired. For the 2nd-5th
blocks, the frame in front of the gazer was filled with a uni-
form gray.
   500 ms after the presentation of this scene, a 10×10 red
square appeared at a random location within the frame, and
could be controlled by the mouse. After the time when this
red cursor appeared, the subject could indicate with a mouse
click where, within the frame, he or she believed that the           Figure 2: Likelihood. Subjects indicated where they thought
gazer was looking. There was no enforced time limit for this         the person in the photo was looking, within a uniform gray
task, and the entire scene remained on the screen until the          area. The “gazer” was shown fixating on each of 33 target lo-
subject responded. After the subject clicked, the next trial be-     cations within the frame, 20 times per subject. Here, the white
gan. The experimental procedure for each trial is illustrated        dots represent the 20 locations selected by one actual subject
in Figure 1.                                                         (via mouse click) when presented with this same scene. We
Bayesian Model                                                       fit a Gaussian ellipse to these 20 points (superimposed here
                                                                     on the scene), and this ellipse enters into the computational
The likelihood: Using eye cues Computational treatments
                                                                     model as the likelihood function with respect to this particu-
of the problem of discriminating the target of another per-
                                                                     lar directional cue from the eyes of the gazer.
son’s gaze from eye and head cues (e.g., Kim & Ramakrishna,
1999; Hoffman et al., 2006; Yücel et al., 2013; Gao, Harari,
Tenenbaum, & Ullman, 2014) often model gaze as a vector or           The prior: Using saliency information We hypothesized
blurry cone emanating from the gazer’s face and intersecting         that it would be expedient for the human visual system to ex-
with surfaces in the environment. A complete, self-contained         ploit a model of where people are a priori likely to look in
algorithm for judging another person’s gaze would employ             a scene. Many computer vision models have already been
one of these rigorous computer vision approaches in order            developed to serve precisely this function—predicting where
to compute what we here define as the likelihood function:           human observers are likely to fixate their visual attention in a
L(Gx,y |D).                                                          given image (e.g., Itti, Koch, & Niebur, 1998; Harel, Koch, &
   We instead derive the likelihood function empirically from        Perona, 2006; Rezazadegan, Rahtu, & Heikkilä, 2011)—and
each subject’s gaze judgments recorded during blocks 2-5             the performance of many of these models has been systemat-
(These were the trials for which the gazer was presented as          ically benchmarked at saliency.mit.edu.
viewing a uniform gray surface). We associate each photo-                We used the saliency model put forth by Judd et al. (2009),
graph of the gazer—associated with the gazer’s eyes being            because they make freely available a.) MATLAB code for
fixated in 1 of 33 directions—with a 2-dimensional likeli-           their saliency model, b.) a set of images against which their
hood function, which we assume to be elliptical (a bivariate         saliency algorithm has been validated, and against which
Gaussian distribution). This assumption of an elliptical shape       other algorithms have been tested for comparison, and c.)
makes sense if one imagines a cone of gaze emanating from            pre-computed saliency maps corresponding to these images.
the gazer’s eyes, because the intersection of this cone with         The Judd et al. algorithm incorporates low-level visual fea-
the gazed-upon planar surface would be elliptical in shape           tures (e.g., intensity and color contrast), higher-level features
(indeed, this the geometric definition of an ellipse, one of the     (e.g., face detection), and a prior bias toward the center. We
basic types of conic section).                                       selected a subset of 165 images they provide, on the basis
   After collecting responses from each subject as he or she         that they were all of a consistent size (768 × 1024 pixels).
cycled 20 times through the complete set of 33 eye direc-            We incorporate these images’ corresponding, pre-computed
tions, we estimated the mean (µ) and 2 × 2 covariance matrix         saliency maps as the prior in our Bayesian model of human
(Σ) of all 33 Gaussian ellipses comprising a complete set of         gaze perception. See Figure 3 for an example of a saliency
                                                                 1813

Figure 3: Prior. During the first block of the experiment,            Figure 4: Posterior. The posterior probability outputted by
images were projected into the frame, and subjects indicated          the Bayesian model (superimposed here on a screenshot from
where in the picture they thought the person in the photo was         the experiment) is a multiplication of the likelihood function
looking. Here, we superimpose the saliency map correspond-            (given this gaze direction) and prior (given this image). For
ing to this particular image, a continuous 2-dimensional func-        this particular trial, we present one possible location a sub-
tion that enters into the computational model as the prior.           ject may have clicked, as a small white bullseye. We assess
                                                                      the model’s performance on a given trial as the likelihood of
                                                                      the subject’s gaze judgment given the model’s posterior pre-
map corresponding to one of these 165 images.                         diction map.
The posterior: Combining the eye cue with image saliency
The posterior distribution outputted by the Bayesian model
                                                                      1.6 proved to be optimal. For only one very atypical subject,
(given a photograph of the gazer fixating in a particular direc-
                                                                      we were unable to validate the likelihood model—that is, no
tion, and a particular gazed-upon image with a correspond-
                                                                      parameterization of the likelihood model trained on any three
ing saliency map), is the pixel-by-pixel multiplication of the
                                                                      of the subject’s blocks was at all predictive of the subject’s
likelihood map (p[D|Gx,y ]) and saliency map (p[Gx,y ]). After
                                                                      responses on the remaining test block. We therefore excluded
this multiplication, the posterior distribution is renormalized
                                                                      this subject from subsequent analyses.
to sum to 1. The resulting map is a hybrid of the two maps
giving rise to it. The full Bayesian model indeed exploits the        Model assessment and comparison
saliency map, but typically only within the neighborhood of
                                                                      We compared the full Bayesian model (outputting a posterior
locations where the gazer may have plausibly been looking,
                                                                      distribution over the image) with a more basic model that only
given the direction of his eyes.
                                                                      relied on the perceptual signal from the eye cues of the gazer
                            Results                                   (i.e., the likelihood model, not multiplied with the saliency
                                                                      map). We tested the relative performance of these two models
Validation of the likelihood model                                    in predicting the gaze judgments of subjects during the first
Our model of the likelihood function—33 ellipses fit to gaze          block of the experiment (These were the trials in which the
judgments in blocks 2-5—was first validated for each subject.         gazer was presented with a projected photograph—unlike in
This approach was cross-validated by fitting ellipses to the          blocks 2-5, in which the gazer was presented with a uniform
subject’s responses during three of these blocks, and testing         gray surface). Because the likelihood function (a component
how well they predicted responses on the fourth block. This           of both models) was independently validated and optimized
leave-one-out cross-validation was done each of the four pos-         for each subject with respect to the four other blocks of the
sible ways (leaving each of the four blocks out as the test set).     experiment, neither the full Bayesian model nor the reduced
The main diagonals of the covariance matrices Σ of all 33             model fit any free parameters to the responses of the subjects
ellipses were multiplied by one additional parameter, which           in the first block.
was optimized for each subject via this same cross-validation            The relative performance of these two models was first as-
procedure. This multiplication procedure increased the vari-          sessed in terms of log likelihood ratio (LLR). For a given
ances of the likelihood function ellipses in a manner that in-        trial, the gaze judgment made by the subject had a likeli-
creased their predictive power.                                       hood given the prediction maps of either model (e.g., as in
   The cross-validated performance of the likelihood model            Fig. 4). Over the subject’s 165 trials, the predictions of the
was good and remarkably consistent across subjects. For               two models were compared via their cumulative likelihood
most subjects, multiplying the variances of the fit ellipses by       ratio. The natural logarithm of this ratio was computed for
                                                                  1814

                                                                                    A                                        B
                                                                 50
                   Log Likelihood Ratio (Model 1 vs. Model 2)
                                                                 25
                                                                  0
                                                                −25
                                                                −50
                                                                      Full Bayesian vs. Eye Cues Only     Full Bayesian vs. Mismatched Bayesian
Figure 5: These box plots show the relative performance of the full Bayesian model compared to two other candidate models,
assessed for all 22 subjects via log likelihood ratio (LLR). The red line represents the mean LLR of these 22 subjects, the
blue box represents the 25th and 75th percentiles, and the black whiskers represent the entire range of LLRs. Left: For most
subjects, and for the average subject, the full Bayesian model outperforms the reduced model that only relies on eye cues.
Right: Consistently across subjects, the full Bayesian model outperforms a mismatched Bayesian model, computed with an
inappropriate saliency map.
each subject, with positive values favoring the full Bayesian                                           true Bayesian model was a better fit for 18 out of 22 subjects
model and negative values favoring the reduced “eye cues                                                (cumulative LLR = 161.2; see Fig. 5B),
only” model. 16 out of 22 subjects’ judgments (73%) favored                                                Finally, although most subjects (and the average sub-
the full Bayesian model (see Fig. 5A), and the cumulative                                               ject) showed the predicted effect, we by no means wish
LLR across all subjects very strongly favored the full model                                            to gloss over the individual differences we observed (see
(209.5). For each subject, we also calculated the percentage                                            Fig. 5A). Not all subjects incorporated saliency informa-
of trials for which the full Bayesian model was a better fit.                                           tion into their strategy; indeed, 5 (out of 22) subjects ap-
For 18 out of 22 subjects (82%), the Bayesian model was the                                             parently ignored this cue such that the reduced model pro-
better fit on the majority of trials. And for the average sub-                                          vided a far better fit to their gaze judgments (individ-
ject, the full Bayesian model was preferred on significantly                                            ual LLRs = [−23.7, −14.5, −12.8, −11.1, −8.1]). The full
more than half of trials (M = 61%, SD = 14%, t[21] = 3.69,                                              Bayesian model accounted poorly for the performance of
p = .001).                                                                                              these 5 subjects, and fit their judgments hardly any better than
   These data confirmed our hypothesis that subjects would                                              a mismatched Bayesian model would have (mean LLR = 1.5).
exploit prior information about the relative saliency of loca-
tions in the gazed-upon image, in addition to using the direc-                                                       Discussion and Conclusions
tional signal computed from the gazer’s eye cue.                                                        In this paper, we developed a Bayesian model for gaze per-
   To provide additional context for the assessment of these                                            ception, which takes into account both cues from the gazer’s
two models, we reran the full Bayesian model, but instead of                                            eyes and prior saliency information present in the visual
feeding the model the appropriate saliency map correspond-                                              scene. Via a quantitative model comparison, we demon-
ing to the gazed-upon image in a given trial, we mismatched                                             strated that the performance of most subjects is better ex-
each image with a saliency map corresponding to one of the                                              plained by this full Bayesian model than a reduced model
other 164 images in the set. The motivation for the assess-                                             that only takes the eye cues into account. The full Bayesian
ment of this mismatched Bayesian model was to examine                                                   model also easily outperforms a model that incorporates in-
whether the true Bayesian model had improved the perfor-                                                correct saliency information. We consider these data to be
mance of the reduced “eye cues only” model for some su-                                                 strong preliminary support for a Bayesian account of gaze
perficial reason that was not specific to features of the par-                                          perception, and of closely related social processes like shared
ticular image. If the true Bayesian model were a truly better                                           attention, gaze following and joint attention.
model of subjects’ performance, it would systematically out-                                               We emphasize that we do not mean to present this paper as
perform the mismatched Bayesian model. And indeed, the                                                  a study of how gaze perception relates to “saliency” (defined
                                                                                                   1815

in any one particular way, via a specific algorithm), as a visual     Hoffman, M. W., Grimes, D. B., Shon, A. P., & Rao, R. P. N.
feature in itself. Rather, we use computed saliency (according           (2006). A probabilistic model of gaze imitation and shared
to one algorithmic approach) as a simplified stand-in (that is,          attention. Neural Networks, 19, 299–310.
a model) for the predictive computation of which locations in         Itti, L., Koch, C., & Niebur, E. (1998). A model of saliency-
a scene are expected to draw another person’s visual attention.          based visual attention for rapid scene analysis. IEEE
Most subjects’ judgments revealed that they were at least im-            Transactions on pattern analysis and machine intelligence,
plicitly sensitive to these a priori expectations, which were            20(11), 1254–1259.
apparently correlated with the output of the saliency model           Judd, T., Ehinger, K., Durand, F., & Torralba, A. (2009).
we employed.                                                             Learning to predict where humans look. In IEEE Interna-
   The data also appear to indicate that a subset of subjects            tional Conference on Computer Vision (ICCV).
(20-25%) utilized only the cues from the eyes of the gazer.           Ken, M. (1990). Perception of where a person is looking:
These individual differences in strategy raise many questions            Overestimation and underestimation of gaze direction. To-
to be addressed in future experiments: Is the tendency to                hoku Psychologica Folia, 49, 33–41.
use one strategy over the other relatively stable to the indi-        Kim, K., & Ramakrishna, R. S. (1999). Vision-based eye-
vidual? Was this saliency algorithm we used a poor model                 gaze tracking for human computer interface. In 1999 IEEE
for where some subjects expect other people will look in the             international conference on systems, man, and cybernetics
scene? What experimental conditions would favor the use of               (Vol. 2, pp. 324–329).
one strategy over the other?                                          Langton, S. R. H. (2000). The mutual influence of gaze and
   A Bayesian account of this social perceptual process makes            head orientation in the analysis of social attention direction.
several specific predictions. For example, the noisier the per-          The Quarterly Journal of Experimental Psychology, 53(3),
ceptual signal, the more the observer should rely on prior               825–845.
information. This was indeed the result Mareschal, Calder,            Mackworth, N. H., & Morandi, A. J. (1967). The gaze se-
Dadds, and Clifford (2013) observed in their study of gaze               lects informative details within pictures. Perception & Psy-
perception; subjects’ prior bias toward direct eye contact was           chophysics, 2(11), 547–552.
modulated by the amount of noise the experimenters added              Mareschal, I., Calder, A. J., & Clifford, C. W. G. (2013).
to the observed eyes. We expect that manipulations like                  Humans have an expectation that gaze is directed toward
this could also be applied to the basic experimental frame-              them. Current Biology, 23, 717–721.
work presented in this paper, with analogous results. Besides         Mareschal, I., Calder, A. J., Dadds, M. R., & Clifford,
adding noise to the gazer’s eyes (e.g., via blurring), one could         C. W. G. (2013). Gaze categorization under uncertainty:
manipulate the amount of time the observer is given to view              Psychophysics and modeling. Journal of Vision, 13(5), 1–
the stimulus, the amount of time the observer is given to re-            10.
spond, the size or contrast of the stimulus, or the distance          Rezazadegan, T. H., Rahtu, E., & Heikkilä, J. (2011). Fast
between the gazer and the gazed-upon surface in the scene.               and efficient saliency detection using sparse sampling and
The perceptual consequences of each of these manipulations               kernel density estimation. Image Analysis, 666-675.
could then be interpreted within the context of this Bayesian         Schauerte, B., Richarz, J., & Fink, G. A. (2010). Saliency-
treatment, providing additional insight into the nature of hu-           based identification and recognition of pointed-at objects.
man gaze perception.                                                     In 2010 IEEE/RSJ International Conference on Intelligent
                                                                         Robots and Systems (pp. 4638–4643).
                     Acknowledgments                                  Wallaston, W. H. (1824). On the apparent direction of eyes in
This research was supported by faculty start-up funding at               a portrait. Philosophical Transactions of the Royal Society
Indiana University. Special thanks to Sebastian Kagemann                 of London, 114, 247–256.
for his help with the creation of stimuli.                            Yarbus, A. L. (1967). Eye movements and vision. New York:
                                                                         Plenum Press.
                          References                                  Yücel, Z., Salah, A. A., Meriçli, C., Meriçli, T., Valenti, R.,
Byrne, R., & Whiten, A. (1991). Computation and min-                     & Gevers, T. (2013). Joint attention by gaze interpolation
   dreading in primate tactical deception. In A. White (Ed.),            and saliency. IEEE Transactions on Cybernetics, 43(3),
   Natural theories of mind. Oxford: Basil Blackwell.                    829–842.
Cline, M. G. (1967). The perception of where a person is
   looking. The American Journal of Psychology, 80(1), 41–
   50.
Gao, T., Harari, D., Tenenbaum, J., & Ullman, S. (2014).
   When computer vision gazes at cognition (Tech. Rep.).
   Center for Brains, Minds, & Machines.
Harel, J., Koch, C., & Perona, P. (2006). Graph-based vi-
   sual saliency. Advances in Neural Information Processing
   Systems, 545–552.
                                                                  1816

