                              What defines a category? Evidence that listeners’
                                      perception is governed by generalizations
                                            Rachael R. Richardson1 (rachaelr@umd.edu)
                                                 Naomi H. Feldman1,2 (nhf@umd.edu)
                                                 William Idsardi1 (idsardi@umd.edu)
                               1 Department of Linguistics and 2 Institute for Advanced Computer Studies
                               1401 Marie Mount Hall, University of Maryland, College Park, MD 20742
                               Abstract                                     tial stop consonants (Lisker & Abramson, 1964). VOT is de-
   Listeners draw on their knowledge of phonetic categories when            fined as the amount of time between the release of a stop con-
   identifying speech sounds, extracting meaningful structural              sonant and the onset of glottal phonation. In voiceless stops,
   features from auditory cues. We use a Bayesian model to                  phonation substantially lags the release, whereas in voiced
   investigate the extent to which their perceptions of linguistic
   content incorporate their full knowledge of the phonetic cate-           stops, phonation closely follows the release.
   gory structure, or only certain aspects of this knowledge. Sim-             Although VOT serves primarily as a cue to voicing, it
   ulations show that listeners are best modeled as attending pri-          varies as a result of other factors as well. For example, place
   marily to the most salient phonetic feature of a category when
   interpreting a cue, possibly attending to other features only in         of articulation affects the distribution of VOT: Consonants ar-
   cases of high ambiguity. These results support the conclusion            ticulated at the back of the mouth (e.g. velars) have signifi-
   that listeners ignore potentially informative correlations in fa-        cantly longer VOTs, whereas consonants articulated in the
   vor of efficient communication.
                                                                            front of the mouth (e.g. labials) have shorter VOTs. This
   Keywords: speech perception; categorization; voice onset
   time; speaking rate                                                      pattern is largely owed to phonetic universals, but there is
                                                                            enough cross-linguistic variation to require language-specific
                           Introduction                                     components in a complete account (Cho & Ladefoged, 1999).
Identifying a category presumes a structural knowledge of the                  Speaking rate also affects the distribution of VOT, as it af-
category, i.e., of the perceptual cues associated with the cat-             fects all durational cues. However, unlike place of articula-
egory and of how these perceptual cues relate to its more ab-               tion and voicing, speech rate is not an intrinsic cue to category
stract features. In the domain of speech sounds, the phonetic               membership. Previous analyses have shown that listeners’ ad-
category is a latent, linguistic variable explaining the observ-            justments to speech rate variation are robust when measured
able variation in the signal. Knowing the structure of pho-                 at different scales, with variation in both target syllable rate
netic categories enables listeners to extract a message from                and target sentence rate contributing to changes in internal
the speech they hear. These categories are thought to be struc-             category structure (Wayland, Miller, & Volaitis, 1994).
tured in terms of features, such as voicing, place, or manner,                 We test three models of category encoding on their ability
which facilitate generalization (e.g., Cristià & Seidl, 2008;              to predict listeners’ perception of stop consonant categories
Maye, Weiss, & Aslin, 2008). This paper explores the way in                 from a single acoustic cue. In the first model, All Available
which listeners use phonetic features during perception.                    Features (AAF), the likelihood function is generated from
   We probe listeners’ categorization of English stop con-                  Gaussians jointly conditioned on both place and voicing fea-
sonants, which are typically characterized by their voicing                 tures. The second model encodes categories as a distribu-
and place features. Voiced stops /b,d,g/ differ from voice-                 tion conditioned on a single feature: Voicing Only (VO). The
less stops /p,t,k/ in the voicing feature,1 whereas labials /b,p/,          third model is designed for Effective Ambiguity Resolution
alveolars /d,t/, and velars /g,k/ differ in the place feature. Cat-         (EAR), and conditions recruitment of the place feature on the
egorical perception has been found along acoustic dimensions                amount of uncertainty that remains after taking account of
relevant to both voicing and place (Liberman, Harris, Hoff-                 the voicing feature. Simulations show that listeners’ behav-
man, & Griffith, 1957; Wood, 1976), suggesting that both                    ior is better fit by a model that defines phonetic categories
types of features contribute to the intrinsic identity of a cat-            according to a single feature: voicing, with possible recruit-
egory. Knowing a stop consonant category entails knowing                    ment of the place feature when the distribution of cues de-
both its voicing and its place.                                             fined by voicing is maximally uninformative. We argue that
   We employ a computational model to assess the explana-                   this behavior is consistent with that of an optimal listener who
tory power of different possible category encodings of these                partitions perceptual space to maximize their ability to com-
features during a perception task. Our focus is on perception               municate efficiently.
of a durational cue, voice onset time (VOT), that is widely at-                We begin by reviewing data from Volaitis and Miller
tested cross-linguistically as a cue to voicing contrasts in ini-           (1992) showing that differences in VOT distributions arise
    1 We refer to these categories as voiced and voiceless, despite the     from variation not only in voicing, but also in speaking rate
fact that word-initially, they are better characterized phonetically as     and place of articulation. The following section introduces
being voiceless unaspirated and aspirated stops, respectively.              our category encoding models. We then present simula-
                                                                        1979

                                                                     along the voicing continuum. These continua were synthe-
                                                                     sized to have identical onsets but overall durations of 125 ms
                                                                     and 325 ms: a fast and slow condition, respectively. Two of
                                                                     the continua were synthesized as velar stop consonants, /gi/
                                                                     and /ki/, while the other two were synthesized as labial stop
                                                                     consonants, /bi/ and /pi/. Participants were given options of
                                                                     identifying the stimuli as either /gi/ and /ki/ (for velar con-
                                                                     tinua) or /bi/ and /pi/ (for labial continua),2 and were asked
                                                                     to identify the sound they heard. Results confirmed a large
                                                                     and reliable effect of syllable duration on the location of the
                                                                     category boundary, with longer syllables more often evoking
                                                                     voiceless responses.
                                                                        Volaitis and Miller conclude from this evidence that the
                                                                     perceptual mapping between acoustic structure and phonetic
                                                                     category is comprehensively altered with changes in speech
                                                                     rate. However, they did not ask whether differences in place
                                                                     of articulation alter the perceptual mapping between VOT and
                                                                     phonetic category identity in the same way. If place of artic-
                                                                     ulation and speaking rate behave similarly, we would expect
                                                                     that examining listeners’ behavior with respect to changes in
Figure 1: Digitized plot of selected production study data           place of articulation would reveal the same changes in listen-
from Volaitis and Miller (1992)                                      ers’ internal category structures as were found in response to
                                                                     changes in speaking rate. On the other hand, if extrinsic fac-
tions showing that, excepting near the category boundary, the        tors such as speaking rate behave differently from intrinsic
model that attends preferentially to the voicing feature pro-        features such as place of articulation, we might expect differ-
vides a better account of listeners’ perceptual behavior. We         ent patterns of behavior in each case.
conclude by summarizing our findings and discussing impli-              Our simulations of these data test three hypotheses. Our
cations for theories of speech perception.                           first hypothesis is that to complete the forced choice listening
                                                                     task, listeners recruit all available information. In this case,
           Volaitis and Miller’s Experiments                         they should jointly infer both available features to stop con-
                                                                     sonant identity: place of articulation and voicing, and inter-
Volaitis and Miller (1992) investigated whether internal cat-        pret the cue with respect to both at all places along the VOT
egory structure could be considered context independent, ex-         continuum. Our second hypothesis is that we can more ac-
ploring the effect of syllabic speaking rate on category struc-      curately describe listeners as preferentially inferring only the
ture in two experiments.                                             single most salient feature in this task. Listeners’ responses
   In a production experiment, participants were recorded            are uniformly voiced for VOT below a threshold of about 35
speaking six syllables: three beginning with voiced stop con-        ms, and universally voiceless for those above 80 ms. The
sonants, /b,d,g/ and three beginning with their voiceless coun-      place feature may therefore only be useful in categorizing
terparts /p,t,k/. All ended with the vowel /i/. Each participant     stimuli between these values. Our third simulation tests the
produced six instances of each syllable in the order /bi/, /pi/,     hypothesis that listeners use information adaptively, inferring
/gi/, /ki/, /di/, /ti/ at eight different speech rates. This was     the featural knowledge that will best help them solve the task
repeated four times for each participant. To examine the ef-         effectively and efficiently, depending on the position in the
fect of rate categorically, the syllables were then divided into     VOT continuum.
three duration categories: 100-299 ms, 300-499 ms, and 500-             Toscano and McMurray (2010) investigated a similar ques-
799 ms. Volaitis and Miller found that VOT systematically            tion using a learning model to categorize word-initial stop
increased with syllable duration for all three places of artic-      consonants. They found that discrepancies between produc-
ulation. All speakers showed the same pattern of increasing          tion and perception data can be described as resulting from
VOT with increasing syllable duration. In addition, all speak-       preferential down-weighting of cues which are less informa-
ers showed a pattern of increased VOT for stops articulated          tive at a given position in the voicing continuum. Here we
farther back in the mouth. Aggregated data for these four            investigate a more abstract characterization of listeners’ be-
speakers from eight of these conditions are shown in Figure 1.       havior, investigating categorization using a single cue with
   Volaitis and Miller then conducted a perception experiment
to investigate how listeners adjust to this apparent systematic          2 Participants were presented with three options: they could la-
variation in VOT. A new group of participants was presented          bel the sound as the voiced category, the voiceless category, or a
                                                                     third unnatural voiceless category with an extremely long VOT. In
with a forced-choice categorization task. Participants were          our analysis we have collapsed the natural and unnatural voiceless
tested on four synthetically generated series of consonants          categories, counting both as corresponding to a voiceless response.
                                                                 1980

multiple possible abstract featural specifications.                    To apply our model to the Volaitis and Miller experimen-
                                                                    tal data, we estimate Gaussian distributions for each category
            Model of Sound Categorization                           from their production data. Given the heights of bars in their
Our model adopts a framework introduced by Nearey and               histograms, h, and the VOT values corresponding to those
Hogan (1986) and used in several recent models of speech            bars, x, we can compute maximum likelihood values for the
perception (e.g., Clayards, Tanenhaus, Aslin, & Jacobs, 2008;       mean and variance of a category as
Feldman, Griffiths, & Morgan, 2009; Kleinschmidt & Jaeger,
2015; Sonderegger & Yu, 2010). The model characterizes                                               ∑ xi × hi ,
                                                                                                     i
perception of speech sounds as a statistical inference prob-                                  µ̂cr =                              (5)
lem. The goal of listeners, in perceiving a speech sound, is                                           ∑ hi
                                                                                                        i
to infer the category of the sound using the information avail-
able to them from the speech signal and their prior knowledge                                    ∑(xi − µ̂cr )2 × hi ,
                                                                                                   i
of phonetic categories.                                                                 σ̂2cr =                                   (6)
   Following previous literature, we define phonetic cate-                                             ∑ hi
                                                                                                        i
gories as Gaussian distributions. In producing a VOT, speak-
ers first select a stop consonant category and a speech rate,       where i ranges over all productions at a given speaking rate
then articulate a production. If, at rate r, phonetic category c    and a given voicing value and, in AAF, a given place of ar-
has mean µcr and variance σ2cr , speakers generate production       ticulation. We then compute the posterior distribution over
x from that phonetic category with probability                      category labels for each stimulus according to Equation 2 and
                                                                    compare it to the data from their perception experiment.
                       x|c, r ∼ N (µcr , σ2cr )              (1)
                                                                                               Simulations
   Inverting production data to produce perception data, we         We apply the models outlined above to the data from the
apply Bayes’ rule. The posterior probability of perceiving a        Volaitis and Miller experiments. Data from different speaking
particular speech sound category from a given VOT at a par-         rates are modeled separately, based on previous data that lis-
ticular rate is equal to the probability that the VOT was pro-      teners compensate perceptually for changes in speaking rate.
duced by that category and rate, weighted by the prior proba-          We are primarily interested in the effect of differences in
bility of that category occurring, normalized according to the      category structure on our model’s ability to accurately por-
probability of that VOT occurring,                                  tray the relationship between production and perception. The
                                  p(x|c, r)p(c)                     model which best preserves this relationship can be consid-
                    p(c|x, r) =                              (2)    ered the better representation of listeners’ phonetic cue pro-
                                      p(x)
                                                                    cessing.
   We begin by contrasting the All Available Features (AAF)
model, which defines a phonetic category as a Gaussian dis-         Simulation 1: Joint Contributions of Place and
tributions over equally weighted voicing and place features,        Voicing
with the Voicing Only (VO) model, which defines categories          Our first hypothesis is that variation in VOT caused by both
using only the voicing cue. The difference in the number of         place of articulation and voicing will make significant con-
features affects the likelihood function given in Equation 1,       tributions to listeners’ inferences about linguistic content. To
which becomes                                                       test this hypothesis, we use the AAF model.
                                                                       Results are shown in Figures 2A and 2B. Dashed lines
                     x|v, p, r ∼ N (µvpr , σ2vpr )           (3)    give the empirical data, and solid lines give the model pre-
                                                                    dictions. The overall effects of speech rate and of place of
for AAF and
                                                                    articulation are evident in both model and data, with slower
                                                                    speech rates more often eliciting voiced category responses.
                       x|v, r ∼ N (µvr , σ2vr )              (4)
                                                                    However, the model predicts a much larger effect of place
for VO.                                                             of articulation than is evinced in the perception data, with
   For VOTs far from the categorical boundary, judgments are        widely divergent category boundary predictions for the labial
uniformly either voiced or voiceless. We investigated the pos-      and velar sounds. For labials, the model predicts the category
sibility that for such values, a listener can be described as       boundary at a shorter VOT than evinced by the behavioral ex-
relying on the voicing feature alone, recruiting the less reli-     periments, whereas for velars, it predicts the category to be at
able place feature only when necessary to resolve a significant     a larger VOT. This may also be true of rate, which is predicted
level of ambiguity between categories. Accordingly, our third       to have a larger effect than actually occurs in the perception
model, Efficient Ambiguity Resolution (EAR) is an interpo-          data. However, the speaking rates in the perception experi-
lation the AAF and VO models. EAR uses the uncertainty              ment were far to one end of the range of speaking rates in
in the posterior distribution from the VO model to gate the         the production data, and this prevents us from drawing strong
recruitment of the AAF place-specific category knowledge.           conclusions about listeners’ use of rate information.
                                                                1981

    Figure 2: Empirical data and predictions of models evaluated with labial, velar data: AAF = A,B VO = C,D EAR = E,F
Simulation 2: Generalizing across Places                             vides a better fit to the empirical data.
Our second hypothesis is that one feature, voicing, is over-
                                                                     Simulation 3: Efficiently Resolving Ambiguity
whelmingly more informative to listeners, and that they pri-
marily define their categories in terms of this feature when         Although listeners’ inferences about linguistic content are
performing the forced choice task. To test this hypothesis, we       generally dominated by voicing, variation in VOT caused by
use the VO model. Under this model, listeners are able to            place of articulation makes a significant contribution which
generalize with respect to the place feature, recruiting infor-      is most apparent near the category boundary. We hypothe-
mation about VOT variation at all other places of articulation       size that the degree of recruitment of place features can be
to solve the inference problem.                                      best described in terms of uncertainty about category identity.
   Results are shown in Figures 2C and 2D. Dashed lines              This strategy would facilitate efficient communication, allow-
give the empirical data, and solid lines give the model pre-         ing the listener to preferentially process less ambiguous cues
dictions. Because the model’s predictions are independent of         using a simpler representation than required for more am-
place of articulation, it predicts the same identification func-     biguous cues. Such an account is compatible with theoretical
tion for both labials and velars. The solid black line shows         accounts of listeners only recruiting additional information
the model’s estimate of the empirical data for faster sounds,        regarding category membership, including lexical status and
while the red line gives the model predictions for the slower        visemes, as necessary to resolve ambiguous members (Green
sounds.                                                              & Miller, 1985) and models of online perception of phonemes
   Omitting place as a relevant feature predicts that the shift      as underspecified lexical forms (e.g., Lahiri & Reetz, 2002).
due to rate will be uniform across places of articulation. This      To test this hypothesis, we use a blend of the AAF and VO
is borne out in the empirical data and model results, though         models. For each step along the continuum, we calculate the
the model predicts a substantially larger difference between         entropy of the category distribution as defined only by voic-
rates than is seen empirically. The model also underestimates        ing. If the entropy in this distribution is below a specified
the slope of the categorical boundary, likely due to the in-         threshold, then the categorization task is performed using the
creased variance of the likelihood functions that were esti-         voicing distribution (VO). If the entropy is above that thresh-
mated from combined labial and velar data. Nevertheless,             old, then results are calculated using a joint distribution on
our quantitative comparison will show that the VO model pro-         voicing and place (AAF).
                                                                 1982

   Results for the threshold which produced the best fit to the      Using the mixed strategy of EAR reduces the average cross
perception data (0.68 bits) are shown in Figures 2E and 2F.          entropy of our model to 0.013 bits, a roughly 6% reduction
Dashed lines give the empirical data, and solid lines show           over the voicing-only strategy.
the model predictions. While the model relies exclusively               Although we do not have access to the raw data that would
on voicing for the slower continuua, for faster sounds in the        enable us to compute the log likelihood of these models di-
40-50 ms VOT range, the place-specific model provides a              rectly, note that the cross entropy for a binomial distribution
slightly better fit. This improvement, however, is entirely          is closely related to its log likelihood, with the negative log
owed to a change in a single point in this part of the VOT           likelihood being equal to the cross entropy in nats multiplied
continuum, for voiced sounds only. This change is enough to          by the number of trials in the participants’ data. Thus, the dif-
effect a change in boundary slope, successfully portraying a         ferences in cross entropy between AAF and VO found here
reduced effect of the place cue near the boundary compared           are likely to translate into non-negligible changes in log like-
to what would be predicted on the basis of speakers’ produc-         lihood when taking into account the fact that each point in
tions.                                                               each of the four continua represents responses across 15 tri-
                                                                     als for each of 12 participants.
Quantitative comparison
We compare the success of the models using cross entropy.                                      Discussion
Entropy is a measure of information: as the model more suc-
cessfully predicts listeners’ perceptions from production data,      This paper used a Bayesian model to explore the relation-
fewer bits will be required to encode the perception data us-        ship between categorical effects and the weighting of intrinsic
ing an optimal code derived from the production data. We             phonetic category features in the context of stop consonants.
thus hold the category encoding which produces the lower             Using production data to model perception of voicing con-
cross entropy to be a closer match to the way in which human         trasts at different speech rates, we compared three hypothe-
listeners perform the perceptual task.                               ses as to the relationship of the linguistic features. The first
   Cross entropy is computed as                                      simulation, attributing equal explanatory power to distinct in-
                                                                     trinsic features, predicted an exaggerated effect of place. The
                     1                                               second simulation, although it entirely omits place of articu-
         H( p̂) = −       ∑ pdata (c|vi ) log pmodel (c|vi ) (7)
                     n∑ i c
                                                                     lation, provides a more accurate description of the relative ef-
                                                                     fect of rate changes on the location of the category boundary.
where i ranges over VOT values, c ranges over the voiced             The third simulation provided the most accurate account, with
and voiceless categories, and n is the number of steps in the        listeners relying solely on the voicing feature for most stim-
continuum.                                                           uli, but recruiting the place feature to disambiguate sounds
   For each model, our definition of c will vary according           near the category boundary.
to our hypothesis concerning the internal category structure.           The VO model substantially outperforms the place-
The AAF model posits a joint distribution conditioned on             dependent AAF, yet completely fails to account for listeners’
voicing and place features, while the VO model represents            ability to discriminate stop consonants with different places
the category as a distribution conditioned on the voicing fea-       of articulation. The EAR model provides a first attempt at
ture alone.                                                          balancing the efficacy of generalizing across similar cate-
                                                                     gories, with the acuity of a model that recruits all available
                         Labial              Velar                   structural information in interpreting the available cue. Fu-
                     Fast     Slow       Fast     Slow               ture work should explore whether combining these models
           AAF      0.061 0.023         0.014 0.044                  using a different mechanism, such as weighted averaging,
            VO      0.012 0.018         0.014 0.012                  could produce a more accurate description of the categorical
           EAR      0.010 0.018         0.013 0.012                  listener’s behavior.
                                                                        Our simulations suggest that overall, listeners exhibit a
Table 1: Average Cross Entropy of Perception Data by Model
                                                                     preferential treatment of the voicing feature when perform-
   Results are given in Table 1. The average cross entropy for       ing the forced choice task, ignoring the place feature for most
AAF is 0.035 bits and for VO it is 0.014. Thus, on average,          stimuli, despite the apparently meaningful variation in cate-
we find that VO is about 60% more efficient than AAF, requir-        gories it reflects. Rather than than ignoring specific cues, or
ing an average of 0.021 fewer bits to encode the distribution in     physical aspects of the speech signal, listeners appear to be
the perception data. The model thereby captures the observa-         systematically ignoring specific abstract structural aspects of
tion that participants’ performance is fairly homogenous be-         the categories whose identities they are inferring.
tween places of articulation, with place information appear-            This work inherits a limitation of the previous study:
ing to only play a significant role in processing for sounds         Volaitis and Miller presented participants with a forced choice
near the category boundary. This pattern emerges despite the         between two sounds both belonging to the same place of
existence of potentially useful invariant auditory cues distin-      articulation. Therefore, cues specific to place, while avail-
guishing place of articulation (Stevens & Blumstein, 1978).          able, could be deemed irrelevant to the task compared to cues
                                                                 1983

which distinguish VOT across places. However, the results                chophysics, 38(3), 269-276.
of Simulation 3 suggest that listeners may not be ignoring             Kleinschmidt, D. F., & Jaeger, T. F. (2015). Robust speech
this irrelevant dimension entirely, but rather interpreting vari-        perception: Recognize the familiar, generalize to the simi-
ation in VOT due to place of articulation for a specific range           lar, and adapt to the novel. Psychological Review, 122(2),
of stimuli. To further test whether these perceptual patterns            148-203.
extend to situations in which there ambiguity as to the place          Lahiri, A., & Reetz, H. (2002). Underspecified recognition.
of articulation, we could design a new behavioral study re-              Laboratory phonology, 7, 637–676.
quiring participants to make judgments between sounds with             Liberman, A. M., Harris, K. S., Hoffman, H. S., & Griffith,
different place features. Given a task which directed the lis-           B. C. (1957). The discrimination of speech sounds within
tener’s attention to both place and voicing during identifica-           and across phoneme boundaries. Journal of experimental
tion, listeners may be forced to rely on a smaller, more spe-            psychology, 54(5), 358-368.
cialized set of exemplars in their decision process, resulting         Lisker, L., & Abramson, A. S. (1964). A cross-language
in more interaction between these two features. Nevertheless,            study of voicing in initial stops: Acoustical measurements.
our findings suggest that listeners are able to privilege some           Word, 20, 384-422.
features while ignoring others in perceptual tasks, providing          Maye, J., Weiss, D. J., & Aslin, R. N. (2008). Statistical
support for featural representations. This type of representa-           phonetic learning in infants: facilitation and feature gener-
tion could also benefit listeners by allowing them to recognize          alization. Developmental Science, 11(1), 122-134.
underlying phonation categories in the absence of significant          Nearey, T. M., & Hogan, J. T. (1986). Phonological con-
cues, creating a perceptual system which retains robust recog-           trast in experimental phonetics: Relating distributions of
nition even in severe noise.                                             production data to perceptual categorization curves. In
   Although it appears that listeners are actually making use            J. J. Ohala & J. J. Jaeger (Eds.), Experimental phonology
of less information than they have available to them, perhaps            (p. 141-162). Orlando, FL: Academic Press.
by treating the categorization task as less specialized, and re-       Sonderegger, M., & Yu, A. (2010). A rational account of
lying on exemplars from multiple places of articulation, they            perceptual compensation for coarticulation. In S. Ohlsson
are actually increasing the amount of information available              & R. Catrambone (Eds.), Proceedings of the 32nd Annual
to them in the decision task. Generalizing – attributing ob-             Conference of the Cognitive Science Society (p. 375-380).
served variation to as few features as possible – allows the             Austin, TX: Cognitive Science Society.
listener to posit that the preferred feature is not only most in-      Stevens, K. N., & Blumstein, S. E. (1978). Invariant cues for
formative, but on its own, informative enough. This powerful             place of articulation in stop consonants. The Journal of the
assumption would not only endow the perceptual system with               Acoustical Society of America, 64(5), 1358-1368.
the ability to withstand noisy input, but to effectively encode        Toscano, J. C., & McMurray, B. (2010). Cue integration with
ambiguity. Effective resolution of ambiguity is a key prop-              categories: Weighting acoustic cues in speech using un-
erty of linguistic processing systems, reflecting an optimiza-           supervised learning and distributional statistics. Cognitive
tion of cue interpretation in accordance with communicative              Science, 34, 434-464.
pressures.                                                             Volaitis, L. E., & Miller, J. L. (1992). Phonetic prototypes:
Acknowledgments We thank the Probabilistic Modeling group and            Influence of place of articulation and speaking rate on the
the Language Science Lunch group for valuable discussion and feed-       internal structure of voicing categories. The Journal of the
back. This work was supported in part by NSF IGERT grant DGE-            Acoustical Society of America, 92(2), 723-735.
0801465 and NSF grant BCS-1320410.
                                                                       Wayland, S. C., Miller, J. L., & Volaitis, L. E. (1994). The
                                                                         influence of sentential speaking rate on the internal struc-
                          References
                                                                         ture of phonetic categories. The Journal of the Acoustical
Cho, T., & Ladefoged, P. (1999). Variation and universals in             Society of America, 95(5), 2694–2701.
   VOT: evidence from 18 languages. Journal of Phonetics,              Wood, C. C. (1976). Discriminability, response bias, and
   27(2), 207–229.                                                       phoneme categories in discrimination of voice onset time.
Clayards, M., Tanenhaus, M. K., Aslin, R. N., & Jacobs, R. A.            The Journal of the Acoustical Society of America, 60(6),
   (2008). Perception of speech reflects optimal use of proba-           1381–1389.
   bilistic speech cues. Cognition, 108(3), 804-809.
Cristià, A., & Seidl, A. (2008). Is infants learning of sound
   patterns constrained by phonological features? Language
   Learning and Development, 4(3), 203227.
Feldman, N. H., Griffiths, T. L., & Morgan, J. L. (2009). The
   influence of categories on perception: Explaining the per-
   ceptual magnet effect as optimal statistical inference. Psy-
   chological Review, 116(4), 752-782.
Green, K. P., & Miller, J. L. (1985). On the role of visual
   rate information in phonetic perception. Perception psy-
                                                                   1984

