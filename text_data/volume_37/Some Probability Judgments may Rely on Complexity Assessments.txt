                 Some Probability Judgments may Rely on Complexity Assessments
                                Antoine Saillenfest (antoine.saillenfest@telecom-paristech.fr)
                              Jean-Louis Dessalles (jean-louis.dessalles@telecom-paristech.fr)
                                                    Telecom ParisTech, 46 rue Barrault,
                                                            F-75013 Paris, France
                              Abstract                                   solve these issues, but to show that complexity should not be
                                                                         ruled out as a candidate to account for uncertainty
   Human beings do assess probabilities. Their judgments are             judgments, and that contrary to a common opinion, it is
   however sometimes at odds with probability theory. One                cognitively plausible, perhaps no less than probability itself.
   possibility is that human cognition is imperfect or flawed in the
   probability domain, showing biases and errors. Another                   We will first consider the notion of complexity and show
   possibility, that we explore here, is that human probability          how it can be turned into a cognitive notion. Then we will
   judgments do not rely on a weak version of probability                see how notions like condition, independence and subjective
   calculus, but rather on complexity computations. This                 probability are reformulated in the framework of Simplicity
   hypothesis is worth exploring, not only because it predicts some      Theory. We will use an example, a story of plagiarism, to
   of the probability ‘biases’, but also because it explains human       show that individuals are sensitive to complexity. Lastly, a
   judgments of uncertainty in cases where probability calculus
                                                                         small experiment based on this example will be presented.
   cannot be applied. We designed such a case in which the use of
   complexity when judging uncertainty is almost transparent.            Its purpose is to test some predictions of the theory.
   Keywords: Probability, Kolmogorov complexity, simplicity,                                Cognitive Complexity
   unexpectedness.
                                                                         The mathematical notion of complexity, known as
                          Introduction                                   Kolmogorov complexity, emerged in the last fifty years to
                                                                         deal with issues such as randomness, induction in learning
Human beings have a natural intuition of ‘probability’. They             and computability. The complexity of a situation is the size of
use it, not only to anticipate risks, but much more often                its shortest summary. Or, in other words, its size when it has
when they get the point of narratives based on unexpected                been maximally compressed. This definition can be made
events (Dessalles, 2008a). For instance, people are very                 formal by coding situations as binary strings and by finding
good at noticing all factors that control unexpectedness in              computer programs that generate them. The complexity C(s)
coincidences (Griffiths & Tenenbaum, 2001 ; Dessalles,                   of s is the length of the shortest program that outputs s.
2008b) and in near-miss experiences (Teigen, 2005;                          The transposition to cognitive science seems straightforward
Dessalles, 2010). Even if unexpectedness does not always                 (Chater, 1999). It is indeed known since Gestalt Theory that
match the presence of low probability (Teigen & Keren,                   human individuals are sensitive to simplicity. The use of
2003; Maguire & Maguire, 2009), the two notions are                      complexity in cognitive science has however been hindered by
strongly linked, in a way that will be explored here.                    an obvious objection: it is not computable. It is easy to prove
   This definite and consistent ability to assess probability            that no program can output C(s) when s is given as input. Ideal
appears quite mysterious in the light of the many apparent               compression is well-defined, but cannot be computed. This
‘biases’ that have been revealed in the past decades. For                observation led to the conclusion that human minds have no
instance, people wrongly assign low probability to non-                  access to complexity. Some authors decided to abandon it
representative sequences of similar events (Kahneman &                   altogether in favour of statistical inference (Griffiths &
Tversky, 1972; Tenenbaum & Griffiths, 2001). Let’s                       Tenenbaum, 2003), while others attempted to consider
mention some other errors of judgments: the gambler’s                    computable alternative measures of complexity, such as pattern
fallacy (Terrell, 1994), the base-rate fallacy (Bar-Hillel,              complexity or Boolean complexity (Simon, 1972; Feldman,
1980), the conjunction fallacy (Tversky & Kahneman,                      2004). We just need, however, to consider a bounded-resource
1983) or the simplicity bias in causal explanations                      version of complexity (Chater, 1999). Human beings do have
(Lombrozo, 2007). What kind of computation can be so                     computational power that allows them to detect, for instance,
wrong that it fails on basic tests and yet is so precise when it         pattern repetition. They are therefore able to perform some
comes to judging uncertainty for everyday purposes?                      compression on perceived situations. For instance, anyone who
   This issue, quite surprisingly, has not been considered a             knows about numbers can detect a pattern in the series
priority. In many psychology experiments, for instance in                122333444455555, namely “n repeated n times”, which leads
decision theory, probabilities are provided as input to                  to significant compression.1 More generally, any detection of
participants, with the tacit hypothesis that they are able to
process them directly. Could it be that judgments of
uncertainty are spontaneously achieved through a
                                                                            1
                                                                              Using concatenation, the series can be written nn, n < 5. This
fundamentally different form of computation? Could it be                 is much more compact than the independent specification of 15
                                                                         numbers: two instructions (loop and repeat) and an upper limit on
that probability is no more than a mathematical notion with
                                                                         the one hand, 15log2(10) = 50 bits on the other hand. If we spare
no cognitive counterpart? The purpose of this paper is not to
                                                                         three bits to designate each instruction in the number sequence
                                                                     2069

structure achieves a compression. Let’s call Ci,t(s) the size of          Note that this definition of generation complexity provides a
the best compression that an individual i has been able to                simple notion of independence. Two situations s1 and s2 are
produce within time t. This notion is, by definition, computable          independent iff Cw(s1&s2) = Cw(s1) + Cw(s2).
as soon as a computable cognitive model is available. In what                ST defines unexpectedness U(s) as the difference between
follows, C(s) will be used to designate Ci,t(s). In this sense, C(s)      generation and description complexity.
is computable.
                                                                                                  U(s) = Cw(s)  C(s).                    (1)
                Description vs. Generation                                   This definition is congruent with Teigen and Keren’s
Links between complexity and probability have been                        observation that surprise corresponds to contrasts between
noticed from the outset (Solomonoff, 1964). The basic idea                actual outcomes and expectations (Teigen & Keren, 2003;
is that simpler patterns are more probable. Algorithmic                   Saillenfest & Dessalles, 2014). Here, expectations correspond
Information Theory (AIT) offers several definitions of                    to Cw(s) and outcomes to C(s). The above definition of
algorithmic probability, including p(s) = 2C(s), which                    unexpectedness aims at capturing exactly what people regard
amounts to converting each complexity bit into the flip of a              as surprising, as unlikely, as ‘improbable’ (in the naïve
fair coin.2 This definition matches the subjective uncertainty            sense). The correspondence with probability is explored now.
attached to explanatory scenarios: complicated explanations
involving many choice points are perceived as less likely.                    Simplicity Theory and Probability ‘Biases’
   This definition cannot be the answer, however, as                      The main hypothesis explored in this paper is that human
subjective probability sometimes functions the other way                  beings, in many judgments about uncertainty, rely on
around. Lottery draws such as 1-2-3-4-5-6 or 5-10-15-20-                  unexpectedness rather that on probability. Let’s consider the
25-30 are intuitively felt as much more improbable than 17-               above mentioned ‘fallacies’ in turn to see if they are
19-24-35-38-43, not because the former are more complex,                  compatible with this hypothesis.
but on the contrary because they are less complex                            In the gambler’s fallacy (Terrell, 1994), people are reluctant
(Dessalles, 2006; Maguire et al., 2013). AIT accounts for                 to bet on recently drawn numbers. This behavior is deviant in
this effect by introducing a new notion, randomness                       the eyes of Probability Theory (PT). Why would a memory-
deficiency (Li & Vitányi, 1994). Within the framework of                  less lottery avoid recent numbers? If 571 was drawn four
Simplicity Theory (ST), randomness deficiency is a special                weeks ago in a weekly lottery, people behave as if they
case of complexity drop (Dessalles, 2008a).                               considered the probability that 571 be drawn, not twice at a
   Why is improbability sometimes attached to complexity                  four week distance, but twice within a four week interval. PT
(as for causal explanation) and sometimes to simplicity (as               explains neither the phenomenon nor the ‘within’ hypothesis
for lottery draws)? According to ST, unexpectedness does                  required to account for its fading with time. According to ST,
not correspond to one measure of complexity, but to the                   gamblers bet on the least unexpected outcome. If 571 was
difference between two measures of complexity: generation                 recently drawn, it is much simpler to describe than log2(1000) 
complexity and description complexity. The latter matches                 10, which is the number of bits required to distinguish among
the usual definition of C(s). Note that each individual is                the 1000 options in the lottery studied by Terrell. The simplest
regarded as a different computing ‘machine’: if a lottery                 description of 571 now amounts to 2 bits, as it consists in giv-
draw matches an individual’s telephone number, it will be                 ing its rank in the list of past winning numbers. The gambler’s
very simple for her, but not for other people.                            fallacy results from a decrease in C(s), while Cw(s) remains
   Generation complexity, on the other hand, is defined as                constant. This makes recent winning numbers too unexpected
the simplest causal scenario that the individual can figure               to be bet on. The effect lasts as long as the complexity of
out to explain a situation. In a lottery, all numbers are                 locating 571 in the winning list remains small enough.
believed to be generated by equally complex causal                           As observed by H. Simon (1972), simplicity accounts for
processes. Generation complexity Cw(s) can be measured by                 biases of representativeness as well. People would consider
the number of choice points and the number of options in                  a series of eight births like GGGGBBBB, where four girls
the minimal scenario that generates s. For instance, most                 and then four boys are born in the family, as less ‘probable’
individuals consider that the presence of a famous actor in               than a more complex pattern like GGBGBBGB. Here also,
their kitchen would require a complex causal scenario.3                   Cw(s) is kept constant while C(s) varies. The effect is a
                                                                          mystery for PT, but it is again predicted by ST if we
context, the first code would need only 23+log2(5) < 9 bits.             suppose that individuals are sensitive to unexpectedness. As
   2
     This definition is sometimes regarded as problematic. If we code     (1) shows, simple structures make U(s) larger, hence the
situations s as numbers ns, then for most situations, C(s)  log2(ns),    feeling of improbability. This simplicity effect can be
and p(s) =  instead of 1. This problem is avoided, either by            quantified and matches experimental results (Simon, 1972).
considering prefix-free codes, or by regarding numbers like 297 and       For the same reason, remarkable lottery draws like 1-2-3-4-
2971 as non exclusive (as the latter contains the former).                5-6 are regarded as virtually impossible by most people
   3
      See www.simplicitytheory.org for further details. The site
answers some frequently asked questions about ST, including why
a situation that is the most complex in its class turns out to be         a common window, turns out to be complex, as it requires a
simple for that reason; or the converse: why a standard object, like      lengthy description to be distinguished from all other windows.
                                                                      2070

(Dessalles, 2006). Note that contrary to PT, ST does not                 distributions. (2) shows that the phenomenon can be
invoke here any ad hoc notion such as representativeness.                parsimoniously analyzed in terms of complexity
   In the conjunction fallacy (Tversky & Kahneman, 1983),                exclusively. The detour through probability is unnecessary.
people find it less probable that Paul, a former Green                      If individuals judge uncertainty based on complexity
activist, would drive a big SUV rather than if he drived a               rather than on probability, several other ‘fallacies’ are no
big SUV functioning with LPG (SUV are known to waste                     longer problematic. Simplicity bias in causal explanations
more energy than standard cars and LPG is known to be less               (Lombrozo, 2007) and base-rate neglect (Bar-Hillel, 1980)
polluting than gasoline). Within the set-theoretic framework             rely on experiments in which probabilities are provided as
of PT, this seems absurd, as the set of SUV drivers includes             numbers (percentages) to participants. While educated
the set of LPG-SUV drivers. How does ST account for this                 individuals may be able to translate unexpectedness into
phenomenon? By noticing that the causal generation of the                probability estimates, it is a too strong assumption to
SUV case is more complex than for the LPG-SUV case.                      suppose that the converse might be true.
   The two situations: Paul driving a SUV (s1) and Paul
driving a LPG-SUV (s2), differ both by their description and                 Unexpectedness and Subjective Probability
their generation. Assuming that Paul is already in the                   ST has been developed to account for the human ability to
context, situation s1 can be described using the concept of              assess the unexpectedness of events after they have
green activist (G) and the concept of SUV (f1).                          occurred. Ex post probability6 is defined as:
                      C(s1) = C(G) + C(f1|G).                                                        p(s) = 2U(s).                       (3)
   The vertical bar in C(a|b) denotes conditionality. It means              Formula (3) explains why a simple sequence like 1-2-3-4-
that b is available to describe a. For instance, C(a|a) = 0. s2          5-6 is felt as much more improbable than a complex one
requires an additional feature f2 = ‘LPG’ to be described.               like 17-19-24-35-38-43. It also explains why events that are
              C(s2) = C(G) + C(f1|G) + C(f2|G, f1).                      rare, unique or extreme according to a simple criterion are
                                                                         perceived as improbable when they occur; it explains why
   (we ignored other features, such as ‘drive’, that are                 rare events (like a fire) are regarded as less probable when
common to s1 and s2). Here, concepts4 are prototypical                   they occur in the vicinity; it explains recency effects in the
situations evoked by words. Considering prototypes instead               news; it also explains why coincidences are exaggeratedly
of sets is presented as a human flaw by Tversky and                      perceived         as     improbable       (Falk,       1983)    (see
Kahneman (1983). But the hypothesis that words be                        simplicitytheory.org for a review). In all these examples,
associated with sets rather than prototypes is a constraint              probability judgments are performed ex-post, after the fact.
imposed by PT and has little cognitive support.5                            In all the above mentioned classical studies on probability
   If we abandon PT’s extensional constraint, we can compute             bias, individuals were asked whether a situation was more
how s1 and s2 differ on the generation side. s1 evokes a typical         ‘probable’ than another. This corresponds to an ex-ante
situation, i.e. a gasoline SUV. Since LPG is supposed to be              judgment. Ideally, from the ex-ante perspective, s is already
more Green-friendly than standard gasoline, the contradiction            determined: C(s) = 0, and ex-ante unexpectedness is Ua(s) =
with Paul’s past as Green activist is less flagrant in s2 than in        Cw(s). The central thesis of this paper is that people translate
s1. This means that the minimal causal scenario explaining s2            the word ‘probable’ by considering both Ua(s) and U(s). In
is less complex than the minimal causal scenario leading to s1.          our lottery examples, Ua(s) is constant and only U(s) varies.
In other words: Cw(s2) < Cw(s1). We get:                                 In the SUV example, Ua(s) and U(s) vary in the same
        U(s1)  U(s2) = Cw(s1)  Cw(s2) + C(f2|G, f1) > 0.        (2)    direction. But only for a relevant feature like LPG.
                                                                            ST predicts that individuals’ behavior will be different if
   ST correctly predicts that s1 will appear more unexpected,            f2 is a neutral feature such as ‘the SUV was red’. Suppose
and therefore less probable, than s2. This prediction matches            there are 16 possible SUV colors. One needs C(f2|f1) = 4 bits
the so-called ‘conjunction fallacy’. Note that this account,             to designate the actual color. On the generation side, 4
derived from ST, is not unrelated to Maguire et al.’s (2013)             additional bits are also required in a causal scenario to orient
explanation. These authors introduce different ‘models’,                 the choice among the 16 colors: Cw(s2) = Cw(s1) + 4. We get:
which correspond to the causal scenarios underlying Cw(s1)               U(s2) = U(s1), and s2 will be judged no more unexpected
and Cw(s2). Though adopting a complexity-based approach,                 than s1. In a narrative like: “Remember Paul, the former
their description is expressed in terms of probability                   Green activist?”, the two mentions “I saw him driving a big
                                                                         SUV” and “I saw him driving a big red SUV” offer exactly
   4
     For any concept x, C(x) can be approximated as C(x)  log2(r),      the same unexpectedness. The detail about the color is
where r is the rank of a word expressing x in a list of words sorted     irrelevant,7 if relevance is defined as: ‘contributing to
by frequency of occurrence in a corpus. Assuming Zipf’s law, r
can also be the frequency itself, or the relative number of hits on a
                                                                            6
Web search engine (Cilibrasi & Vitányi, 2007). A specific corpus              Technically, ex-post probability p(s) as defined by (3)
can be used to refine estimates for given individuals.                   corresponds to Prob(happens(s) | s). It is not itself a probability
   5
     The set of all SUVs is a mathematical abstraction which is not      measure: the sum of p(si) for different events si may exceed 1.
                                                                            7
computable, either objectively or cognitively.                                Unless it is used to emphasize that the story is true.
                                                                     2071

unexpectedness’ (Dessalles, 2013). However, Ua(s2) is                           Can we predict how parameters S, S1, S2, p1, p2 and the two
larger than Ua(s1) by 4 bits. This may lead most people to                   options influence plagiarism probability? PT has something
regard ‘red-SUV’ as less probable than mere ‘SUV’, thus                      to say about this. It will predict that the probability of T1
respecting the conjunction axiom this time. Note that PT is                  appearing in B2 by chance would decrease with S and increase
unable to take the relevance of the feature into account.                    with S1 and S2. But in the absence of any specific knowledge
                                                                             about the borrowing mechanism, it would assume uniform
                    The Plagiarism Story                                     probability for p1 and p2, and their value would be irrelevant.
To make the case of complexity even stronger, we searched                    Comparing options 1 and 2 would be somewhat tedious. One
for a situation in which individuals make a definite                         would need to imagine all ways of splitting T1 into several
judgment of uncertainty that cannot be explained by                          pieces to determine the probability that T1 would end up in
probability calculus. Consider the following story.                          three, instead of one, two or more than three pieces.
                                                                                In the ST framework, plagiarism is blatant when the
   Story 1: Ms S. is accusing Mr D. of having stolen her manuscript          coincidence between the content of B1 and of B2 is too
   and of having published it under his name. Fortunately, she hid           unexpected. There is coincidence if these contents are
   her name in the book. Her name is (option n1: Sami); (option n2:
                                                                             supposed to be independent:
   Schildget). It can be retrieved by taking (option r1: the first letter
   of each chapter); (option r2: the first or the second letter of each                       Cw(B1&B2) = Cw(B1) + Cw(B2).                 (5)
   chapter, depending on the chapter’s parity).
                                                                                On the description side:
   We could not find anyone, in informal inquiries among
students, who chose options other than n2 and r1 when asked                                    C(B1&B2) < C(B1) + C(B2|B1).                (6)
to maximize Ms S.’s chances to win her case. Why do the                         Following (1), the unexpectedness of the coincidence,
more complex name and the simpler retrieving algorithm                       U(B1&B2), corresponds to the complexity drop between
make plagiarism so obviously more likely in this story?                      generation (5) and description (6). Assuming that neither B1
   PT would merely predict that a shorter name is more                       nor B2, as sequence of words, is unexpected by itself8
likely to ‘occur’ by chance in the book, without any                         (Cw(Bi) = C(Bi)), we get:
precision about what ‘occur’ means. It is unable to account
for the role of the algorithm used to retrieve the name.                                       U(B1&B2) > C(B2)  C(B2|B1).                (7)
   ST’s explanation is straightforward: plagiarism is                           The right-hand side of (7) is the compression of B2 allowed
probable if the co-occurrence by chance of Ms S.’s name                      by the knowledge of B1. This compression is due to the
(N) and Mr D.’s book (B) is highly unexpected, i.e. if                       resemblance between T1 and T2. Ideally, C(T2) could be
U(N & B) is large. ‘By chance’ here means that N and B are                   spared in the description of B2. There is a tax to pay, however,
supposed to be independent. By definition of independence,                   which is the complexity of the procedure needed to get T2
Cw(N & B) = Cw(N) + Cw(B). On the description side,                          from B1. To compute a lower bound of the compression, we
C(N & B) < C(B) + C(N|B). The algorithm A provided by                        may compute the complexity of the following procedure:
Ms S. to retrieve her name in the book gives an upper bound                  locate T1 in B1; use algorithm A to transform T1 into T2;
of C(N|B): C(N|B) < C(A). If we assume that neither B nor N                  determine target location in B2; insert T2. From (7), we get:
is unexpected by itself, we get by applying (1):
                                                                                        U(B1&B2) > C(T2)  (C(l1) + C(A) + C(l2)).         (8)
                     U(N & B) > C(N)  C(A).                          (4)
                                                                                l1 and l2 designate the precise locations of T1 and T2 in B1
   ST thus explains why a complex (i.e. long) name and a                     and B2. If there are about n words in a page, then:
simple algorithm make plagiarism more probable in story 1.
The complexity of the retrieving algorithm, A, is directly                       U(B1&B2) > C(T2)  (C(p1) + C(A) + C(p2) + 2 log2(n)). (9)
understood to play a crucial role. A probability-based model
                                                                             In option 1, if texts are strictly identical, then A1 is a mere
could not account for this effect without many ad hoc
                                                                             copy and C(A1) = 0. In option 2, C(A2) has a definite value, as
assumptions. It is more parsimonious to consider that
                                                                             it requires at least four numbers: two cut points in T1 to make
individuals have direct access to complexity assessments.
                                                                             the three pieces, and the size of two gaps to specify how to
   We designed a small experiment as a first attempt to
                                                                             insert the pieces in the target text. To make things concrete,
explore the Plagiarism story in more details and try to test
                                                                             we may say that C(A2) ~ 4 log2(n). Formula (9) allows us to
finer grain phenomena. This time, we introduce new var-
                                                                             draw the following predictions. Plagiarism is more likely if:
iables, such as the size of the book. Here is the second story.
   Story 2: Ms Schmidt is accusing Mr Durand of having
                                                                              [1] T2 is a long excerpt, making C(T2) large
   plagiarized in his book B2 a passage T2 of size S that is almost           [2] A is simple
   identical to a passage T1 from her book B1. The sizes of the two           [3] Sizes Si are small, as C(pi) < log2(Si), while n is not too
   books are S1 and S2. T1 is located at page p1 of B1 and T2 at                    large.
   page p2 in B2. T2 is found in one piece (option 1) / in three
   pieces distributed over three paragraphs in p2 (option 2).
                                                                                8
                                                                                  One way to be unexpected in this way would be to be more
                                                                             compressible than exepected, as Georges Perec’s Grand palindrome.
                                                                         2072

                              Figure 1: Answers for each alternative of the 6 propositions in Story 2
 [4] pi is small (while n is not too large), as in this case         binomial tests. Results indicate a significant effect (p <
      C(pi) ~ log2(pi) < log2(Si).                                   0.05) for the number of pages of the books, the size of the
                                                                     passage, its location in Mr Durand’s book and the
   If B1 is not known in advance, C(B1) is augmented by the
                                                                     complexity of the algorithm that leads from one text to
determination of Ms Schmidt and by the determination of B1
                                                                     another. The location of the passage in Ms Schmidt’s book
in her works. We could then add the two following
                                                                     did not lead to a significant effect (p  0.11). These results
predictions:
                                                                     agree with our predictions [1], [2], [3] and [4]. Note that a
 [5] Ms Schmidt is a famous author                                   probabilistic model would predict [1] and [3], perhaps [2],
 [6] She has written few books.                                      but not [4].
                                                                       The non-significant result for the localization in Ms
   Note that prediction [1] has a massive effect, as compared
                                                                     Schmidt’s book is due in part to its small expected
with predictions [2]-[4]. Each word in T2 contributes by
                                                                     contribution in comparison with [1]. Moreover, an informal
log2(N) bits to unexpectedness, where N is the size of the
                                                                     inquiry among additional participants suggests that some
lexicon (if one compares text creation to a uniform lottery
                                                                     individuals may have performed second-order reasoning:
among words). In comparison, [4] may spare 6 bits only if
                                                                     borrowing a passage from Ms Schmidt’s first pages would
p1 = 3 and S1 = 250. The following small experiment is an
                                                                     be too conspicuous and would make plagiarism not rational
attempt to put predictions [1]-[4] to the test.
                                                                     and therefore less likely. Due to its design that did not
                                                                     anticipate this reaction, Story 2 turns out to be less
Experiment
                                                                     convincing than Story 1.
Participants were presented Story 2 (in French) with the               In further work, we plan to test predictions [5] and [6], as
following options:                                                   the contribution is larger (~ log2(A) where A is the number
o Ms Schmidt’s book is a 154/654 -page book.                         of authors), and also because probability calculus would not
o Mr Durand’s novel is a 162/443 -page book.                         naturally take the author’s celebrity into account.
o The passage mentioned by Ms Schmidt is located page
  number 3/43 in her collection of short stories.                                            Conclusion
o The passage mentioned by Ms Schmidt is 9/19 lines long.            The first aim of this paper is to question the human ability to
o The passage mentioned by Ms Schmidt can be found in                process probability as such. Despite the existence of
  one part / spread over 3 paragraphs in Mr Durand’s novel.          numerous human ‘biases’, the fact that uncertainty
o The passage mentioned by Ms Schmidt can be located                 judgments rely on some form of probability calculus is often
  page number 5 / 122 in Mr Durand’s novel book.                     taken for granted without questioning its cognitive
A total of 352 individuals (aged from 16 to 63, mean 28.64           plausibility.
(std. dev. 6.66), 276 females, 91 males, 15 unknown gender)            Our second aim is to put forward the possibility that
participated to the test online. Participants were recruited via     complexity can be directly assessed by individuals.
social networks and billposting. We manually checked the             Complexity has not been sufficiently considered as a good
answer files for individuals who provided incomplete results         candidate to explain judgments of uncertainty, due to
or whose response time was less than 30 seconds.                     prejudices     concerning       its   non-computability,     to
                                                                     misconceptions about how it should be computed (see
Results and discussion                                               note 3), and to the (wrong) belief that probability itself is
Percentages of answers for each alternative proposed are             always computable (see note 5). Notions which are quite
presented in Figure 1. We tested for significance using              intricate when expressed in probabilistic terms are more
                                                                 2073

intuitively defined in terms of complexity: independence        Falk, R. & Macgregor, D. (1983). The surprisingness of
and causal indeterminism are captured by generation               coincidences. In O. Svenson & A. Vári (Eds.), Analysing
complexity; conditionality and simplicity come naturally          and aiding decision processes, 489-502. Budapest:
with description complexity.                                      Adadémiai Kiadó.
   We hypothesized that unexpectedness, as defined in           Feldman, J. (2004). How surprising is a simple pattern?
Simplicity Theory as the difference between generation and        Quantifying 'Eureka! Cognition, 93, 199-224.
description complexity, is used by individuals to judge         Griffiths, T. L. & Tenenbaum, J. B. (2001). Randomness
about uncertainty. This hypothesis has two advantages. (1)        and coincidences: Reconciling intuition and probability
It offers new and parsimonious accounts of the various            theory. In J. D. Moore & K. Stenning (Eds.), 23rd annual
cognitive ‘biases’; (2) It accounts for situations in which       Conf. of the Cognitive Science Soc., 370-375. Edinburgh.
probability theory would be partially silent. We designed       Griffiths, T. L. & Tenenbaum, J. B. (2003). Probability,
two versions of the Plagiarism story to make up cases in          algorithmic complexity, and subjective randomness. In R.
which judgments of uncertainty seem to rely on complexity         Alterman & D. Kirsh (Eds.), 25th annual Conf. of the
rather than on probability.                                       Cognitive Science Soc., 480-485. L.E.A.
   Simplicity Theory has been designed in an unrelated          Kahneman, D. & Tversky, A. (1972). Subjective
context: to account for interest and relevance in narratives      probability: A judgement of representativeness. Cognitive
(Dessalles, 2008; Saillenfest & Dessalles, 2014). Quite           Psychology, 3, 430-454.
remarkably, as we showed here, it can be successfully           Li, M. & Vitányi, P. (1994). Statistical properties of finite
applied with no modification to judgments of uncertainty.         sequences with high Kolmogorov complexity.
No ad hoc hypotheses, such as recency avoidance,                  Mathematical Systems Theory, 27, 365-376.
representativeness or randomness deficiency, have been          Lombrozo, T. (2007). Simplicity and probability in causal
introduced. This invites us to consider complexity as a           explanation. Cognitive Psychology, 55, 232-257.
plausible dimension of cognitive processing.                    Maguire, P. & Maguire, R. (2009). Investigating the
                                                                  difference between surprise and probability judgements.
Acknowledments                                                    In N. A. Taatgen & H. van Rijn (Eds.), 31st Annual Conf.
This study is supported by grants from the programme              of the Cognitive Science Soc. Amsterdam.
Futur&Ruptures and from the “Chaire Modélisation des            Maguire, P., Moser, P., Maguire, R. & Keane, M. T. (2013).
Imaginaires, Innovation et Création”.                             A computational theory of subjective probability. In M.
                                                                  Knauff, M. Pauen & N. Sebanz (Eds.), 35th Annual Conf.
                         References                               of the Cognitive Science Soc., 960-965. Austin, TX.
                                                                Simon, H. A. (1972). Complexity and the representation of
Bar-Hillel, M. (1980). The base-rate fallacy in probability
                                                                  patterned sequences of symbols. Psychological Review,
   judgments. Acta Psychologica, 44 (3), 211-233.
                                                                  79 (5), 369-382.
Chater, N. (1999). The search for simplicity: A fundamental
                                                                Solomonoff, R. J. (1964). A Formal Theory of Inductive
   cognitive principle? The Quarterly Journal of
                                                                  Inference. Information and Control, 7 (1), 1-22.
   Experimental Psychology, 52, 273-302.
                                                                Saillenfest, A. & Dessalles, J-L. (2014). Can Believable
Cilibrasi, R. & Vitányi, P. (2007). The Google similarity
                                                                  Characters Act Unexpectedly?. Literary & Linguistic
   distance. IEEE Transactions on Knowledge and Data
                                                                  Computing, 29 (4), 606-620.
   Engineering, 19 (3), 370-383.
                                                                Teigen, K. H. (2005). When a small difference makes a big
Dessalles, J-L. (2006). A structural model of intuitive
                                                                  difference - Counterfactual thinking and luck. In D. R.
   probability. In D. Fum, F. Del Missier & A. Stocco
                                                                  Mandel, D. J. Hilton & P. Catellani (Eds.), The
   (Eds.), seventh International Conf. on Cognitive
                                                                  psychology of counterfactual thinking, 129-146. Oxon,
   Modeling, 86-91. Trieste, IT: Edizioni Goliardiche.
                                                                  UK: Routledge.
Dessalles, J-L. (2008a). La pertinence et ses origines
                                                                Teigen, K. H. & Keren, G. (2003). Surprises: low
   cognitives. Paris: Hermes-Science Publications.
                                                                  probabilities or high contrasts? Cognition, 87, 55-71.
Dessalles, J-L. (2008b). Coincidences and the encounter
                                                                Tenenbaum, J. B. & Griffiths, T. L. (2001). The rational
   problem. In B. C. Love, K. McRae & V. M. Sloutsky
                                                                  basis of representativeness. In J. D. Moore & K. Stenning
   (Eds.), 30th Annual Conf. of the Cognitive Science Soc.,
                                                                  (Eds.), 23rd annual Conf. of the Cognitive Science Soc.,
   2134-2139. Austin, TX: Cognitive Science Soc.
                                                                  1036-1041. Edinburgh.
Dessalles, J-L. (2010). Emotion in good luck and bad luck:
                                                                Terrell, D. (1994). A test of the gambler's fallacy: Evidence
   predictions from simplicity theory. In S. Ohlsson & R.
                                                                  from pari-mutuel games. Journal of risk and uncertainty,
   Catrambone (Eds.), 32nd Annual Conf. of the Cognitive
                                                                  8 (3), 309 - 317.
   Science Soc., 1928-1933. Austin, TX.
                                                                Tversky, A. & Kahneman, D. (1983). Extensional versus
Dessalles, J-L. (2013). Algorithmic simplicity and
                                                                  intuitive reasoning: The conjunction fallacy in probability
   relevance. In D. L. Dowe (Ed.), Algorithmic probability
                                                                  judgment. Psychological Review, 90 (4), 293-315.
   and friends - LNAI 7070, 119-130. Berlin, D: Springer
   Verlag.
                                                            2074

