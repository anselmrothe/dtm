                                    Causal relations from kinematic simulations
                                 Sangeet Khemlani1, Geoffrey P. Goodwin2, and Phil Johnson-Laird3,4
                         sangeet.khemlani@nrl.navy.mil, ggoodwin@psych.upenn.edu, phil@princeton.edu
                                     1
                                       US Naval Research Laboratory, Washington, DC 20375 USA
                                        2
                                          University of Pennsylvania, Philadephia PA 19104 USA
                                               3
                                                 Princeton University, Princeton NJ 08540 USA
                                            4
                                              New York University, New York, NY 10003, USA
                              Abstract                                     What is less controversial is the centrality of mental
  Reasoners distinguish between different types of causal
                                                                           simulation in causal thinking and reasoning: people
  relations, such as causes, enabling conditions, and                      construct small-scale simulations of possibilities to make
  preventions. Psychologists disagree about the representations            predictions of outcomes (Kahneman & Tversky, 1981), to
  that give rise to the different relations, but agree that mental         understand mechanisms (Hegarty, 2004) and physical
  simulations play a central role in inferring them. We explore            scenes (Battaglia, Hamrick, & Tenenbaum, 2013), to resolve
  how causal relations are extracted from mental simulations.              inconsistencies and contradictions (Khemlani & Johnson-
  The theory of mental models posits that people use a                     Laird, 2012, 2013; Park & Sloman, 2014), to deduce the
  kinematic simulation to infer possibilities. It predicts that
  causes should be easier to infer than enabling conditions, and           consequences of algorithms (Khemlani et al., 2013), and to
  that the time it takes to infer a causal relation should correlate       reason about counterfactual scenarios (Byrne, 2005;
  with the number of moves in a mental simulation. To test                 Galinsky & Moskowitz, 2000).
  these two predictions, we adapted a railway domain designed                 One challenge for theories of causality is how different
  to elicit mental simulations, and we devised problems in                 causal relations are extracted from a mental simulation.
  which reasoners had to infer causal relations from simulations           People distinguish between causal relations such as cause,
  of the movements of cars in this domain. Two studies
                                                                           enable, and prevent. For example, they recognize that the
  corroborated the model theory's predictions. We discuss the
  results in light of recent theories of causation and mental              meaning of,
  simulation.
                                                                              1. The button caused the missile to launch.
  Keywords: causal reasoning, mental models,                 mental
  simulation, railway domain, enabling conditions.                         is distinct from,
                          Introduction                                        2. The button enabled the missile to launch.
  A man presses a button on his computer that switches on
a missile control system. His partner, a woman, presses a                  Theorists have appealed to the transmission of force (Wolff,
button on the missile control system to launch the missile,                2007), causal model structure (Sloman et al., 2009), and
and a few minutes later the missile is launched. Did the first             mental models of possibilities (Goldvarg & Johnson-Laird,
man cause the missile to launch? Or did he merely enable its               2001), to explain the differences in meaning between causal
launch? Reasoners often have to make judgments of the                      relations (for a review, see Khemlani, Barbey, & Johnson-
distinction between causing and enabling, and in doing so                  Laird, 2014). But there exists no account of how causal
they may rely on mental simulations of a sequence of events                relations are inferred from simulations, and as such no one
(as in the example above). Many people may conclude that                   has specified an algorithm that can carry out the task.
the man caused the launch, because the temporal contiguity                    Our goal in the present article is to report studies that
of two events is often all that is required to infer causality             should help to solve this problem. We begin by illustrating
(Bramley, Gerstenberg, & Lagnado, 2013; Lagnado &                          how causal relations can be inferred from kinematic models
Sloman, 2006; Rottman & Keil, 2012). In the case of the                    by introducing the general tenets of mental model theory.
missile launch, the causal inference may be unwarranted,                   We used a domain designed to elicit kinematic mental
because the description is consistent with alternative                     simulations, and so we introduce that domain and describe
possibilities, such as one in which the woman decides not to               its characteristics. We then describe two studies in which
press the button she controls. Suppressing the initial causal              participants’ inferences about causal relations depended on
inference requires mental simulation too, because reasoners                the number of discrete steps in mental simulations. Finally,
may engage in a search for alternative possibilities                       we evaluate the results in the context of current theories of
consistent with the description (Frosch & Johnson-Laird,                   causal reasoning.
2011).
  Causation is controversial; it has vexed scholars for                                      Models of possibilities
centuries, and psychologists disagree on its underlying                       The mental model theory – the “model” theory, for short
mechanisms (Ahn & Bailenson, 1996; Cheng, 1997; Hilton                     – applies to reasoning across many domains, including
& Erb, 1996; Sloman, 2005; White, 2014; Wolff, 2007).                      reasoning based on sentential connectives, such as if, or, and
                                                                      1081

and (Johnson-Laird & Byrne, 1991), reasoning based on                (Goldvarg & Johnson-Laird, 2001, Experiment 1).
quantifiers (Khemlani, Lotstein, Trafton, & Johnson-Laird,           However, unless otherwise prompted to do so, reasoners
2015) and reasoning about temporal, spatial, causal, and             build models in accordance with the principle of truth. As
abstract relations (Goodwin & Johnson-Laird, 2005). Three            such, causes and enabling conditions have a single mental
main principles underlie the theory (Johnson-Laird, 2006).           model representing one possibility:
First, mental models represent discrete possibilities: each
model captures a distinct set of possibilities. Second, mental                        button     missile-launch
models are iconic as far as that is possible: the structure of
the model corresponds to the structure of what it represents         Hence, individuals often fail to distinguish enabling from
(see Peirce, 1931-1958, Vol. 4), and so kinematic models             causing (Goldvarg & Johnson-Laird, 2001, Experiment 5).
that unfold in time can represent a temporal sequence of                When individuals observe or envisage a sequence of
events (Johnson-Laird, 1983; Khemlani et al., 2013). But,            events, such as a button being pressed before a missile is
models can also include abstract symbols, e.g., the symbol           launched, they can infer a causal relation between them. A
for negation (Khemlani et al., 2012). Third, the model               correct inference depends on not only observing the factual
theory posits a principle of “truth”: mental models represent        mental model of the relation (e.g., the button being pressed,
only what is true and not what is false.                             then the missile launching) but also envisaging the
   Inferences that require more models are more difficult            counterfactual possibilities to which the relation refers (i.e.,
than those that require fewer models. As a consequence,              the set of fully explicit models). If reasoners can envisage
reasoners take longer to draw such inferences and are more           possibilities that correspond to the first conjunctive set
likely to err, particularly by overlooking possibilities that        above (e.g., the missile launching after the button press, and
render a given statement false. As such, they often represent        either launching or not in the absence of the button press)
only possibilities that render a statement true – their mental       then they should infer that pressing the button caused the
models – though they can flesh out those mental models to            missile-launch. In contrast, inferring an enabling condition
include additional possibilities to build fully explicit models.     should be more complex. The difference in difficulty
We illustrate the difference for the case of causal reasoning.       depends on the assumption that at least one of the following
   The model theory resolves the differences in                      causal relations holds between the button and the missile
interpretation between causal relations by distinguishing the        launch: causes, enables, or prevents. Reasoners observe
sets of possibilities to which those relations refer (Goldvarg       what happens given that the button is pressed. If the missile
& Johnson-Laird, 2001), i.e., fully explicit models. A causal        launches in this case, then they are likely to infer that the
assertion such as (1) above refers to a conjunction of three         button causes the missile launch, and they will be correct if
separate models of possibilities, depicted in this schematic         indeed it does. They will even be correct if they also
diagram:                                                             consider the counterfactuals of what happens given that the
                                                                     button is not pressed. The only way that they are likely to
                 button      missile-launch                          infer that the button enables the missile launch is if they can
               ¬ button      missile-launch                          envisage a counterfactual possibility in which the missile
               ¬ button ¬ missile-launch                             does not launch when the button is pressed. The asymmetry
                                                                     arises because the button press and the missile launch are
Each row in the diagram represents a different temporally            both possible given either causes or enables, but the button
ordered possibility, e.g., the first row represents the              press and missile not launching is possible only with
possibility in which the button is pushed and the missile            enables. Hence, the theory predicts that when reasoners
launches. In other words, the model theory posits that               draw causal conclusions from mental simulations, it should
causality rules out those situations in which the button is          be more difficult to infer enabling conditions, e.g., that
pushed and the missile doesn’t launch, as well as those              pressing the button enabled the missile launch.
situations in which the missile launch precedes the button              When individuals need to simulate a sequence of events
push. In contrast, an enabling assertion, such as the one            in a kinematic model, the theory makes a direct prediction:
specified in (2), refers to a different conjunction of               the more events that occur in the sequence, the harder the
possibilities:                                                       inference should be – it should take longer and be more
                                                                     likely to yield an error.
                 button      missile-launch                             In order to test the theory’s predictions we adapted an
                 button ¬ missile-launch                             experimental domain used to elicit kinematic mental
               ¬ button ¬ missile-launch                             simulations in order to study causal inferences. We describe
                                                                     that experimental domain in the next section.
i.e., to say that pushing the button enabled the missile to
launch is to assert that the missile may or may not launch                 Kinematic models in a railway domain
(the first two possibilities above). The enabling condition is
inconsistent with the possibility in which the missile                  We sought to investigate how individuals without any
launches without the button being pushed. Reasoners list             formal training in logic, probability, or causal reasoning
these possibilities for assertions such as (1) and (2)               were able to infer causal relations by carrying out mental
                                                                 1082

simulations. Accordingly, we developed a domain based on                     Reasoners can mentally simulate such a sequence of
the railway environment shown in Figure 1. The                             moves. Indeed, a reasoner who carried out the steps above
environment is composed of a railway track and a siding,                   to reverse the train ABCDE may have made causal
and recent studies demonstrate its ability to elicit kinematic             inferences in passing. Consider the first move in the
simulations underlying deductive and abductive inferences                  sequence:
(Khemlani et al., 2013). The environment is simple enough
for children to understand and to reason about (Bucciarelli                  ABCDE[          ]
et al., under review). Cars are prohibited from moving from                         A[BCDE]
the siding directly to the right track and vice versa, and they
are prohibited from moving from the right track back to the                those who envision moving car B from the left track to the
left track. In other words, there are only three legal moves in            siding might recognize that doing so caused cars C, D, and
the environment: i) a move from the left track to the siding,              E to move along with it. The inference can be drawn
ii) a move from the siding to the left track, and iii) a move              because the causation relation refers to three separate
from the left track to the right track. Multiple cars can be               possibilities in a situation, such as Figure 1:
moved at once such that any move of a selected car applies
to all cars in front of it. In Figure 1, if you moved the D car                    B moves-to siding         C moves-to siding
to the right track, then the E car would move along in front                    ¬ B moves-to siding          C moves-to siding
of it. Because both the siding and the left track function as                   ¬ B moves-to siding        ¬ C moves-to siding
stacks in an automaton, the environment in principle has the
computational power of a universal Turing machine. To                      In reversing the order of the cars, individuals can simulate
restrict the environment to a single stack, cars could move                the first possibility. They might also simulate counterfactual
from the siding only to the output on the right track.                     possibilities, e.g., if B hadn’t moved to the siding, C may or
                                                                           may not have moved to the siding.
                                                                             Reasoners can infer enabling conditions in a similar
                                                                           fashion. For example, reasoners can infer that moving B to
                                                                           the siding enabled A to move alone to the right track on a
                                                                           subsequent move. How might reasoners infer the relation?
                                                                           They may envisage that if B moves to the siding then A can
Figure 1. The railway domain with an example of an initial                 move alone to the right track:
configuration in which a set of cars is on the left side of the track,
the siding can hold one or more cars while other cars are moved to                 B moves-to siding         A moves-to right track
the right side of the track.
                                                                           Some reasoners may therefore infer that B’s move causes
   Consider the problem of reversing the order of five cars,               A’s move. But, the inference is erroneous. To make the
ABCDE, i.e., to produce the sequence, EDCBA on the right                   correct inference, reasoners need to consider two
track. The environment can be depicted in this diagram:                    counterfactual moves:
   ABCDE[          ]                                                            ¬ B moves-to siding          A moves-to right track
                                                                                ¬ B moves-to siding        ¬ A moves-to right track
where the brackets denote the siding, the area to the left of
the brackets denotes the left track, and the area to the right             In other words, A could have remained on the left track
of the brackets denotes the right track. This sort of notation             even if B had moved to the siding; but, A could not have
is used in a computer program that solves such problems                    moved alone to the right track if B hadn’t moved to the
and infers algorithms for solving them (Khemlani et al.,                   siding. The three models together suffice to infer the
2013). To reverse the order of the train, a reasoner can move              enabling relation: moving B to the siding enabled A to move
the cars as follows:                                                       to the right-track alone. But, the inference should be more
                                                                           difficult than inferring that A’s move to the right track
        A[BCDE]           Move all but A to the siding.                    caused B to move there too.
         [BCDE]A          Move A to the left track.                          We carried out two studies in which reasoners made such
        B[CDE ]A          Move B to the left track…                        inferences about the railway environment. The model theory
                                                                           makes two main predictions about errors and latencies:
         [CDE ]BA …then to the right track.
        C[DE ]BA Repeat for all the remaining cars.
                                                                             1.    Inferences that one event causes another should
         [DE ]CBA
                                                                                   be easier than inferences that one event enables
        D[E      ]CBA
                                                                                   another for the reasons we explain above.
         [E      ]DCBA                                                       2.    The number of moves required to carry out the
        E[       ]DCBA                                                             simulation should predict the difficulty of an
         [       ]EDCBA                                                            inference.
                                                                       1083

                      Experiment 1                                  experiment recorded the latency from the appearance of the
                                                                    premise and question to when participants clicked one of the
Experiment 1 aimed to test the two predictions, and thereby
                                                                    buttons.
to corroborate the model theory’s account of causal
meanings and the role of kinematic models. On each trial,
participants saw a picture of three cars on the railway track,
                                                                    Results and discussion
such as one corresponding to the situation:                         The participants’ inferences were more accurate for causal
                                                                    relations than for enabling relations (66% vs. 59%,
           ABC[       ]                                             Wilcoxon test, z = 1.74, p = .04, one-tailed, Cliff’s δ = .07),
                                                                    which corroborated the model theory’s first prediction.
They then had to understand a supposition, such as: Suppose         Figure 2 presents the proportion of correct responses (left
B has just moved to the empty siding. In this case, they have       panel) and the Winsorized latencies of all of the
to simulate a single move: A[BC]. Finally, they had to              participants’ responses, both correct and incorrect (right-
answer a question, such as: Did that move cause C to move           panel) as a function of the number of moves in the
to the siding? In this case, the theory predicts that they          simulation. Accuracies did not differ as a function of the
should respond: Yes. The experiment manipulated the                 number of moves in simulation (Page’s trend test, z = .03, p
number of moves required in the simulation in order to              = .98). But, latencies yielded a significant trend depending
respond to the question: 0, 1, 2, or 3, whether the question        on the number of moves (Page’s trend test, z = 3.98, p <
referred to “cause” or “enable”, and whether the predicted          .0007). The effect was more pronounced when isolating
answer should be “yes” or “no”.                                     only correct responses (Page’s trend test, z = 4.10, p <
                                                                    .0001). This result corroborated the model theory’s second
Method                                                              prediction.
                                                                       An immediate concern is whether participants were in fact
Participants. Thirty-six students at the University of
                                                                    simulating moves in the environment, or whether the
Pennsylvania completed the experiment for partial course
                                                                    significant trend in latency is attributable to the number of
credit. All of the participants were native English speakers,
                                                                    words in each of the problems. To address the issue, we
and none had had any prior training in formal logic.
                                                                    conducted a linear regression analysis on log-transformed
                                                                    latencies that included the number of words and the moves
Design. Participants acted as their own controls and carried
                                                                    required as predictors. Both were significant predictors, but
out 16 problems in a fully repeated measures design, which
                                                                    number of words had a lower regression coefficient (B =
manipulated the number of moves in a simulation (4), the
                                                                    .09, p = .02) than moves required (B = .13, p = .007). We
causal or enabling relation (2), and the correct answer (2).
                                                                    likewise conducted a hierarchical analysis in which two
The 16 problems in the study are in the Appendix. The
                                                                    regression models were contrasted against one another:
study measured the accuracy of participants’ responses to
                                                                    Model 1 regressed the latencies on the number of words
the questions and their latencies.
                                                                    alone, while Model 2 regressed latencies on the number of
                                                                    words in the problem as well as the number of moves
Procedure. The instructions explained the railway
                                                                    required. An analysis of deviance showed a significant
environment, and that moving a car moved all of the cars in
                                                                    increase in model fit from Model 1 to Model 2 (Model 1 R2
front of it too. As a screening procedure, participants had to
                                                                    = .46 vs. Model 2 R2 = .67, F = 10.23, p = .007).
manipulate cars in the environment to reverse the order of a
                                                                       The results corroborated the model theory’s prediction
five-car train. All the participants passed this test. They
                                                                    that causes should be easier to envisage from simulation
were told that all of the descriptions of moves that they
                                                                    than enabling conditions. The results also corroborated the
would receive were a result of making the fewest moves
possible. They were also told (bold text in the original):
                                                                                                                                     20
                                                                        Proportion correct
  “When we ask about whether one move causes another,                                        0.6
                                                                                                                                     15
                                                                                                                       Latency (s)
  we are concerned with whether the first move makes
                                                                                             0.4
  the second move occur. When we ask about whether                                                                                   10
  one move enables a second, we are concerned with
                                                                                             0.2                                      5
  whether that second move immediately becomes
  possible as a result of the first.”                                                        0.0                                      0
                                                                                                   0     1   2   3                        0   1   2   3
   On each trial in the experiment, participants saw an image
of the empty railway track. After a 1000 ms delay, cars in a
                                                                                                       Number of moves in simulation
specified arrangement appeared on the track. After another
2000 ms delay, a premise and a question appeared in the             Figure 2. The proportion of correct responses (left-panel) and the
                                                                    response latencies from all responses (right-panel) in Experiment 1
center of the screen, below the railway track, together with        as a function of the number of moves required to carry out a
two buttons marked “Yes” and “No”, which participants               simulation.
clicked with the mouse to record their responses. The
                                                                 1084

prediction that latencies should correlate with the number of
steps in a simulation needed to infer a causal relation. Both                              0.8
                                                                                                                                   20
                                                                      Proportion correct
of the results suggest that participants extracted causal and
                                                                                           0.6
                                                                                                                     Latency (s)
enabling relations by carrying out and inspecting mental                                                                           15
simulations. As the Figure shows, problems calling for zero                                0.4                                     10
moves in a simulation were not reliably easier than those
calling for one move. A simple explanation is that                                         0.2                                      5
individuals nevertheless envisaged the move described in
the supposition even though its effects were depicted in the                               0.0                                      0
picture of the railway.                                                                          0     1    2   3                       0   1   2   3
   One shortcoming of the present experiment was that the
tenses in the verb phrase of the stimuli were held constant                                          Number of moves in simulation
across the problems. This constancy eliminated some               Figure 3. The proportion of correct responses (left-panel) and the
confounds, but introduced an oddity: for problems that            response latencies (right-panel) in Experiment 2 as a function of
require at least one or more simulations, the premise used        the number of moves required to carry out a simulation.
the past tense to refer to moves that had yet to occur. For
example, consider the first 1-move problem. Participants
saw the a track corresponding to:                                 Results and discussion
                                                                  Experiment 2 measured participants’ accuracies and
          ABC[        ]                                           latencies. Their responses were again more accurate for
                                                                  causal relations than for enabling conditions (69% vs. 57%,
and they were told:                                               Wilcoxon test, z = 1.71, p = .04, one-tailed, Cliff’s δ = .12).
                                                                  Figure 3 presents the proportion of correct responses (left
  Suppose B moved to the empty siding.                            panel) and the Winsorized latencies of all of the
  Did that move cause C to move to the siding? [Y/N]              participants’ responses (right-panel) depending on the
                                                                  number of moves in the simulation. As in Experiment 1,
The premise uses the past tense of “move” to refer to a           accuracies did not reflect the number of moves in a
situation to be simulated. A more natural formulation would       simulation (Page’s trend test, z = .64, p = .52), but latencies
use premises and questions that fit the temporal constraints      did for all responses and correct responses (Page’s trend
of the simulation, e.g.,                                          tests, z > 2.95, p < .003).
  Suppose B moves to the empty siding.                                                                     General discussion
  Does that move cause C to move to the siding?
                                                                     Experiments 1 and 2 corroborated the model theory’s two
Experiment 2 accordingly replicated Experiment 1 using the        predictions: causes were easier to infer than enabling
more natural present tense when appropriate.                      conditions, and the number of moves in a mental simulation
                                                                  predicted response latencies.      The distinction in the
                                                                  meanings of “causes” and “enables” depends on a semantics
                      Experiment 2                                for causal relations capable of building discrete
  Experiment 2 used the same task and design as the               representations, i.e., one that is deterministic (Frosch &
previous study, however it used slightly modified materials.      Johnson-Laird, 2011; Khemlani et al., 2014). This
That is, the descriptions used the present tense to refer to      distinction cannot be captured in a probabilistic account
arrangements in the environment that reasoners had to             (pace, e.g., Suppes, 1970; Cheng & Novick, 1991).
simulate.                                                            Previous studies have suggested that mental simulation
                                                                  underlies reasoning about mechanical systems (Hegarty,
Method                                                            2004), about instabilities in physical systems (Battaglia et
Participants. Twenty-one participants were recruited from         al., 2013), and about the consequences of algorithms
the same subject pool as in the previous study, and they          (Khemlani et al., 2013). The present studies also
completed the study for course credit. All of them were           demonstrate for the first time that the number of discrete
native English speakers; all of them passed the screening         steps in a simulation has a direct effect on the time that it
described in Experiment 1; and none of them had received          takes individuals to make inferences. This result
any training in formal logic.                                     corroborates the use of kinematic mental models in
                                                                  reasoning.
Design and procedure. Same as Experiment 1.
                                                                                                           Acknowledgements
Materials. Modifications to the materials are shown in the        We thank Paul Bello, Monica Bucciarelli, Ruth Byrne,
Appendix.                                                         Philipp Koralus, David Lobina, Robert Mackiewicz,
                                                               1085

      Salvador Mascarenhas, Adam Moore, Alex Doumas, and                                Johnson-Laird, P.N. (2006). How we reason. New York: Oxford University
                                                                                           Press.
      Greg Trafton for advice. This work was supported by a
                                                                                        Johnson-Laird, P. N., & Byrne, R.M.J. (1991). Deduction. Hillsdale, NJ:
      Jerome and Isabella Karle Fellowship from the Naval                                  Erlbaum.
      Research Laboratory (to S.S.K.) and by National Science                           Kahneman, D., & Tversky, A. (1998). The simulation heuristic. In (Eds.)
      Foundation Grant SES 0844851 (to P.N.J-L.) to study                                  D. Kahneman, P. Slovic, A. Tversky. Judgment under uncertainty:
                                                                                           heuristics and biases. Cambridge: Cambridge University Press.
      deductive and probabilistic reasoning.
                                                                                        Khemlani, S., Barbey, A., & Johnson-Laird, P. N. (2014). Causal reasoning
                                                                                           with mental models. Frontiers in Human Neuroscience, 8, 849.
                                     References                                         Khemlani, S. & Johnson-Laird, P. N. (2012). Hidden conflicts:
                                                                                           Explanations make inconsistencies harder to detect. Acta Psychologica,
      Ahn, W., & Bailenson, J. (1996). Causal attribution as a search for
                                                                                           139, 486–491.
         underlying mechanism: An explanation of the conjunction fallacy and
                                                                                        Khemlani, S., & Johnson-Laird, P. N. (2013). Cognitive changes from
         the discounting principle. Cognitive Psychology, 31, 82−123.
                                                                                           explanations. Journal of Cognitive Psychology, 25, 139–146.
      Battaglia, P. W., Hamrick, J. B., & Tenenbaum, J. B. (2013). Simulation as
                                                                                        Khemlani, S., Lotstein, M., Trafton, J.G., & Johnson-Laird, P. N. (2015).
         an engine of physical scene understanding. Proceedings of the National
                                                                                           Immediate inferences from quantified assertions. Quarterly Journal of
         Academy of Sciences, 110(45), 18327-18332.
                                                                                           Experimental Psychology.
      Bramley, N. R., Gerstenberg T. & Lagnado, D. A. (2014). The order of
                                                                                        Khemlani, S., Mackiewicz, R., Bucciarelli, M., & Johnson-Laird, P. N.
         things: Inferring causal structure from temporal patterns. In P. Bello, M.
                                                                                           (2013). Kinematic mental simulations in abduction and deduction.
         Guarini, M. McShane, & B. Scassellati (Eds.), Proceedings of the 36th
                                                                                           Proceedings of the National Academy of Sciences, 110, 16766–16771.
         Annual Conference of the Cognitive Science Society. Austin, TX:
                                                                                        Khemlani, S., Orenes, I., & Johnson-Laird, P.N. (2012). Negation: a theory
         Cognitive Science Society.
                                                                                           of its meaning, representation, and use. Journal of Cognitive
      Bucciarelli, M., Mackiewicz, R., Khemlani, S., & Johnson-Laird, P. N.
                                                                                           Psychology, 24, 541-559.
         (under review). Simulations and gestures in children's formulations of
                                                                                        Lagnado, D. A., & Sloman, S. A. (2006). Time as a guide to cause. Journal
         algorithms. Manuscript under review.
                                                                                           of Experimental Psychology: Learning, Memory, and Cognition, 32(3),
      Byrne, R.M.J. (2005). The rational imagination: How people create
                                                                                           451–60.
         alternatives to reality. Cambridge, MA: MIT.
                                                                                        Park, J., & Sloman, S. (2014). Causal explanation in the face of
      Cheng, P. W. (1997). From covariation to causation: a causal power theory.
                                                                                           contradiction. Memory & Cognition, 42, 806-820.
         Psychological Review, 104, 367–405.
                                                                                        Pearl, J. (2009). Causality: Models, reasoning, and inference (2nd ed). NY:
      Cheng, P. W., & Novick, L. R. (1991). Causes versus enabling conditions.
                                                                                           Cambridge University Press.
         Cognition 40, 83–120.
                                                                                        Peirce, C.S. (1931-1958). Collected papers of Charles Sanders Peirce. 8
      Frosch, C.A., & Johnson-Laird, P.N. (2011). Is everyday causation
                                                                                           vols. C. Hartshorne, P. Weiss, and A. Burks, (Eds.). Cambridge, MA:
         deterministic or probabilistic? Acta Psychologica, 137, 280–291.
                                                                                           Harvard University Press.
      Galinsky, A., & Moskowitz, G. (2000). Counterfactuals as behavioral
                                                                                        Rottman, B. M., & Keil, F. C. (2012). Causal structure learning over time:
         primes: Priming the simulation heuristic and consideration of
                                                                                           observations and interventions. Cognitive Psychology, 64(1), 93–125.
         alternatives. Journal of Experimental Social Psychology, 34, 384-409.
                                                                                        Sloman, S. A. (2005). Causal models: How people think about the world
      Goldvarg, Y., & Johnson-Laird, P.N. (2001). Naive causality: a mental
                                                                                           and its alternatives. Oxford University Press, USA.
         model theory of causal meaning and reasoning. Cognitive Science, 25,
                                                                                        Sloman, S.A., Barbey, A.K. & Hotaling, J. (2009). A causal model theory
         565−610.
                                                                                           of the meaning of “cause,” “enable,” and “prevent.” Cognitive Science,
      Goodwin, G.P., & Johnson-Laird, P.N. (2005). Reasoning about relations.
                                                                                           33, 21-50.
         Psychological Review, 112, 468-493.
                                                                                        Suppes, P. (1970). A probabilistic theory of causality, Amsterdam: North-
      Hegarty, M. (2004). Mechanical reasoning as mental simulation. Trends in
                                                                                           Holland Publishing.
         Cognitive Sciences, 8, 280-285.
                                                                                        White, P.A. (2014). Singular cues to causality and their use in human
      Hilton, D.J., & Erb, H-P. (1996). Mental models and causal explanation:
                                                                                           causal judgment. Cognitive Science, 38, 38–75.
         Judgements of probable cause and explanatory relevance. Thinking &
                                                                                        Wolff, P. (2007). Representing causation. Journal of Experimental
         Reasoning, 2, 273−308.
                                                                                           Psychology: General, 136, 82−111.
      Johnson-Laird, P.N. (1983). Mental models. Cambridge: Cambridge
         University Press. Cambridge, MA: Harvard University Press.
      Appendix. The 16 problems used in Experiments 1 and 2. Where relevant, changes to the stimuli between Experiment 1 and
      Experiment 2 are marked in bolded text.
 # of   Initial                                                                                                                                      Causal     Correct
                    Premise                                                    Question
moves    conf.                                                                                                                                        reln.      answer
  0     A[BC]       Suppose B has just moved to the empty siding.              Did that move cause C to move to the siding?                           Cause       Yes
  0     A[BC]       Suppose B has just moved to the empty siding.              Did that move enable C to move to the siding?                         Enable        No
  0     A[BC]       Suppose B has just moved to the empty siding.              Did that move cause A to stay on the left track?                       Cause        No
  0     A[BC]       Suppose B has just moved to the empty siding.              Did that move enable A to stay on the left track?                     Enable       Yes
  1     ABC[]       Suppose B moved/moves to the empty siding.                 Did/Does that move cause C to move to the siding?                      Cause       Yes
  1     ABC[]       Suppose B moved/moves to the empty siding.                 Did/Does that move enable C to move to the siding?                    Enable        No
  1     ABC[]       Suppose B moved/moves to the empty siding.                 Did/Does that move cause A to stay on the left track?                  Cause        No
  1     ABC[]       Suppose B moved/moves to the empty siding.                 Did/Does that move enable A to stay on the left track?                Enable       Yes
  2     ABC[]       Suppose A moved/moves to be alone on the right track.      Did/With that move, does B cause C to move to the siding?              Cause       Yes
  2     ABC[]       Suppose A moved/moves to be alone on the right track.      Did/With that move, does B enable C to move to the siding?            Enable        No
  2     ABC[]       Suppose A moved/moves to be alone on the right track.      Did/Does that move cause B to move to the left track?                  Cause        No
  2     ABC[]       Suppose A moved/moves to be alone on the right track.      Did/Does that move enable B to move to the left track?                Enable       Yes
  3     [ABC]       Suppose C moved/moves to be alone on the left track.       Did/With that move, does A cause B to move to the right track?         Cause       Yes
  3     [ABC]       Suppose C moved/moves to be alone on the left track.       Did/With that move, does A enable B to move to the right track?       Enable        No
  3     [ABC]       Suppose C moved/moves to be alone on the left track.       Did/Does that move cause C to move to the right track?                 Cause        No
  3     [ABC]       Suppose C moved/moves to be alone on the left track.       Did/Does that move enable C to move to the right track?               Enable       Yes
                                                                                   1086

