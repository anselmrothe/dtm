          Organizing Metacognitive Tutoring Around Functional Roles of Teachers
                                           David A. Joyner (david.joyner@gatech.edu)
                                                Ashok K. Goel (goel@cc.gatech.edu)
                       Design & Intelligence Laboratory, Georgia Institute of Technology, Atlanta, GA 30338
                               Abstract                                help develop metacognitive skills. However, it is not yet
                                                                       clear how to facilitate metacognitive tutoring, especially in
   Metacognitive skills are critical in learning but difficult to
   teach. Thus the question becomes how can we facilitate              exploratory learning environments for open-ended tasks
   metacognitive tutoring? We present an exploratory learning          such as inquiry-driven modeling: we still need to identify
   environment called MILA-T with embedded metacognitive               organizing principles for metacognitive tutoring.
   tutors imitating five functional roles of teachers in                 One way to organize metacognitive tutoring is along the
   classrooms. We tested MILA–T in a controlled experiment             functional roles of teachers in science classrooms. Thus we
   with 237 middle school students. We examine the impact of           developed a categorization of some of the functional roles of
   MILA–T on the models of a natural phenomenon constructed
   by the students. We find that students with access to MILA–T
                                                                       teachers: guide, critic, mentor, interviewer, and observer.
   wrote better evidential justifications for their models, and        Next, we developed an exploratory learning environment for
   thus, deliver better-justified models for the phenomenon. We        inquiry-driven modeling (the Modeling & Inquiry Learning
   also find that these improvements persisted during a transfer       Application, or MILA), and a metacognitive tutoring system
   task. These results lend support for organizing metacognitive       (MILA-T) consisting of metacognitive tutors imitating the
   tutoring around the functional roles of teachers for supporting     roles of a guide, critic, mentor, interviewer, observer. Then,
   inquiry-driven modeling.                                            to evaluate if MILA-T scaffolded inquiry-driven modeling,
   Keywords: Functional roles of teachers; intelligent tutoring        we introduced the tutoring system to 237 middle school
   systems, metacognition, metacognitive tutoring; scientific          science students engaged in modeling a natural ecological
   inquiry; scientific modeling.                                       phenomenon. We found that engagement with MILA–T led
                                                                       teams to developer better-justified models of scientific
                          Introduction                                 phenomena than teams without a metacognitive tutoring
   Both learning scientists and emerging educational                   system. In addition we tested the efficacy of above learning
standards assert the need to teach authentic science to                on a transfer task. We also found that the improvements in
students from an early age (e.g. National Research Council             model quality persisted even after the tutoring system has
1996; Edelson 1997). Research in cognitive science                     been disabled, showing that access to MILA–T actually
describes scientific discovery as an iterative process                 improves students' inquiry-driven modeling and that those
entailing four related but distinct phases (Darden 1998;               improvements transfer to a new task.
Nersessian 2008): model construction, use, evaluation, and
revision. Thus, a model is first constructed to explain some                   MILA: Modeling & Inquiry Learning
observations of a phenomenon. The model is then used to                                         Application
make predictions about other aspects of the phenomenon.
                                                                       MILA is an exploratory learning environment in which
The model’s predictions next are evaluated against actual
                                                                       students working in small teams can participate in an
observations of the system. Finally, the model is revised
                                                                       authentic process of scientific modeling and inquiry. Teams
based on the evaluations to correct errors and improve the
                                                                       investigate a natural phenomenon, posing multiple
model’s explanatory and predictive efficacy. Research in
                                                                       hypotheses that could explain the phenomenon, constructing
cognitive science also relates this process of scientific
                                                                       models to provide mechanisms to those explanations, and
inquiry and modeling to metacognition (e.g. Clement 2008;
                                                                       supplying evidence to support those hypotheses and
Nersessian 2008; Schwarz et al. 2009; White & Frederiksen
                                                                       mechanisms. The MILA window is illustrated in Figure 1.
1998): scientific inquiry and modeling is the metacognitive
                                                                         Models in MILA are made of nodes and edges. Each node
ability to reason over one's own understanding of a
                                                                       in a MILA model represents a changing trend: for example,
scientific phenomenon, construct an evidence-backed model
                                                                       the first node in the chain shown in Figure 1 is that the
of one's understanding, and use that model to inform further
                                                                       Quantity of Fertilizer is Increasing, a trend in the system.
investigation into the system. This suggests the use of
                                                                       These nodes are then linked together in a causal chain that
metacognitive tutoring to scaffold learning about inquiry-
                                                                       explains some phenomenon: in Figure 1, the increasing
driven modeling.
                                                                       fertilizer quantity leads to an increase in the quantity of
   However, metacognitive skills are generally difficult to
                                                                       phosphorus in the system. This leads to an increase in the
teach (Roll et al. 2007), and teaching inquiry-driven
                                                                       population of algae, which leads to an increased
modeling is no different. In past, exploratory learning
                                                                       concentration of oxygen, which leads to a decreased
environments (e.g. van Joolingen et al. 2005) and intelligent
                                                                       population of fish. Note that this model is incomplete:
tutoring systems (e.g. Azevedo et al. 2009, 2010) have been
                                                                       further mechanism is needed to show how the increased
successful in enhancing students' metacognition at least to a
                                                                       quantity of oxygen leads to a decreased fish population.
limited degree, indicating that it is in principle feasible to
                                                                   1033

Figure 1: MILA and MILA–T. In MILA, teams describe a phenomenon (top left), pose multiple hypotheses (middle left), and
 construct models that explain how a hypothesis could have actually caused a particular phenomenon (the causal chain in the
middle). Within these models, teams provide evidence in support of their model (top right). Teams in the Experimental group
   receive feedback from MILA–T, a metacognitive tutoring system. Here, the Interviewer is waiting to give feedback in the
    bottom left (as indicated by the light bulb), and the Guide (bottom right) is currently giving feedback in the bottom right.
   In constructing these models, teams are asked to supply
evidence in support of their claims. Evidence annotates the                    MILA–T: Metacognitive Tutoring
individual connections in the model. The box in the top
                                                                     MILA is augmented with a metacognitive tutoring system
right of Figure 1 shows the evidence that the team is
                                                                     that is the primary object of analysis for this paper. The
supplying in support of the claim that the increased fertilizer
                                                                     metacognitive tutoring system, MILA–T, is comprised of
quantity leads to an increase in phosphorus concentration.
                                                                     five distinct agents: a Guide, a Critic, an Interviewer, a
Justifying this connection demands three ideas: showing
                                                                     Mentor, and an Observer. Each of these tutors is defined by
that fertilizer did increase, showing that phosphorus did
                                                                     a functional role that a teacher typically plays in the
increase, and showing that the two increases are causally
                                                                     classroom, tied in part to Grasha's (1996) model of teaching
linked. Evidential justifications are further annotated with
                                                                     styles, and based partly on our observations of teaching and
categories: teams can choose from one of seven categories
                                                                     learning in science classrooms (Goel et al. 2013). This
that describe their evidence derived from the epistemic
                                                                     differentiates MILA–T from other metacognitive tutoring
cognition community and our prior research: logical
                                                                     initiatives; whereas systems like MetaTutor define the
explanations, expert information, non-expert information,
                                                                     functional roles of the agents with respect to the target skill
direct observations, controlled experiments, simulation
                                                                     (Azevedo et al. 2009, 2010), MILA–T defines the functional
observations, and similar system observations (the first five
                                                                     roles according to the pattern of interaction between the
derived from Goldin, Renken, Galyardt & Litkowski 2014,
                                                                     student and the agent.
the remaining two added based on our activities). These
                                                                        The five tutors are classified into two broad categories:
evidence categories are associated with scores to reflect the
                                                                     proactive and reactive. The proactive tutors (the Mentor, the
desirability of different types of evidence in defending a
                                                                     Interviewer, and the Observer) continuously monitor the
hypothesis: it is, for example, preferable to rely on
                                                                     team's modeling process and intervene where necessary.
established scientific theories and observations from similar
                                                                     The reactive tutors (the Guide and the Critic) wait for the
systems than to rely on logical explanations and novice
                                                                     team to solicit feedback. In this way, the proactive tutors
information. These scores lead to the calculation of the
                                                                     mimic a teacher moving around the classroom, observing
evidence strength metrics described under data analysis later
                                                                     teams' progress, and intervening where necessary. The
in this paper. (Joyner 2015 provides more details of MILA.)
                                                                1034

reactive tutors mimic a teacher sitting at the front of the         testing; the third and sixth days were spent on laboratory
room waiting for teams to approach him or her for approval          exercises. The remaining five days of the unit were spent
or guidance about how to proceed. These functional roles            interacting with the exploratory learning environment MILA
are further differentiated by the specific type of interactions     in teams of two or three. Throughout the first four days of
that each tutor facilitates; the Critic, for example, critiques     interaction with MILA, teams of students were asked to
the current status of the team's explanation, while the Guide       develop an explanation of a massive fish kill that occurred
anticipates and answers the team's questions. Similarly, the        in a nearby lake a few years earlier; this is dubbed the
Mentor monitors for mistakes or increasing aptitude, while          "Learning" project because it is during this time that teams
the Interviewer waits for critical moments and asks students        are learning the skills associated with scientific modeling
to reflect on their thought process.                                and inquiry. On the fifth day of interaction with MILA (the
   MILA–T is defined as a metacognitive tutoring system,            eighth day of the unit overall), teams are asked to develop
rather than a cognitive tutoring system, because the target of      an explanation of the record-high temperatures taking place
MILA–T is students' internal inquiry-driven modeling                in Atlanta over the past decade; this is dubbed the
process. Thus, MILA–T is metacognitive in two ways: first,          "Transfer" project because students are transferring the
it itself reasons over students' thought processes, meaning         skills they learned to a new phenomenon. At the conclusion
that MILA–T thinks about students' thinking; and second,            of each project, students submit their final explanation of
the skill it attempts to teach students is metacognitive.           the phenomenon. Thus, each team submits two models
Inquiry-driven modeling is a metacognitive skill that               during the unit: a model of the fish kill and a model of
operates on the learner's current understanding of a system,        Atlanta's high temperatures.
and thus the target of inquiry-driven modeling is the                 The controlled variable in this study is access to MILA–T.
learner's own knowledge and understanding, meeting the              Both the Control and Experimental groups interact with
common definition of a metacognitive skill (e.g., Veenman,          MILA and complete the nine-day curriculum described
Hout-Wolters & Aflerbach 2006).                                     above. The Control group never sees MILA–T. The
   The tutors of MILA–T take a three-prong approach to              Experimental group receives MILA–T during the Learning
teaching metacognition: emphasize, instruct, and                    project online; MILA–T is disabled during the Transfer
demonstrate. First, the tutors anticipate that students will        project. In this way, we may analyze whether students
deemphasize metacognition, and thus anticipate questions            improve in their inquiry-driven modeling while receiving
that students might ask; they then take those opportunities to      feedback from MILA–T by comparing the Control and
turn students' attention toward the metacognitive skill. For        Experimental during the Learning project, and we may also
example, the Guide anticipates students may ask what the            analyze whether any improvements persisted after the
right answer to the system is, and reacts to that question by       feedback from MILA–T was disabled by comparing the
describing to students how the "right" answer in science is         groups during the Transfer project.
an explanation they construct rather than an answer they              Entire classes were assigned to either the Control or
receive. Second, the tutors attempt to explicitly instruct          Experimental group in order to prevent Control group teams
metacognition in their feedback. The Critic, for example,           from being aware of the existence of MILA–T. Given the
gives students feedback on what kind of evidence they rely          significant differences in teaching style between the two
on in their explanations, but augments this feedback with           teachers, classes were assigned to one group or the other
notes on why certain kinds of evidence are considered               within the teacher; thus, each teacher taught three classes in
preferable and how one ought to evaluate an argument                the Experimental group and two classes in the Control
grounded in certain types of evidence. Third, the tutoring          group. As a result, 84 total teams completed the Learning
system, especially the Mentor and the Interviewer, attempt          project with 50 teams in the Experimental group and 34
to demonstrate proper metacognition to the students. The            teams in the Control group. 81 teams completed the
Interviewer, for example, will react to certain critical            Transfer project with 47 teams in the Experimental group
decisions that students make by asking students to explain          and 34 teams in the Control group. Teachers assigned
the reasoning that led to their decision, and then respond          students to teams without any direction from the
with an example of the reasoning she would have used to             researchers. Researchers were present in the classroom to
arrive at the same decision, thus demonstrating the desirable       provide technical support, but avoided interacting with
metacognitive process. (Joyner 2015 provides more details           students on the project itself. Teachers taught identical
of MILA-T.)                                                         material to classes in the Control and Experimental groups
                                                                    with no direction from the researchers to interact differently
                   Experimental Design                              with the two groups of classes.
The experiment with MILA-T was conducted with two
middle school science teachers together teaching ten total                                 Data Analysis
classes. Participation in the experiment took place during          This data analysis focuses on the models that teams of
nine consecutive regular school days, with each class               students constructed to explain the two phenomena; other
participating for 45 minutes per day. The first and last days       analyses have examined the impact on students’ dispositions
of this nine-day unit were spent on content and attitude            toward science (Joyner & Goel 2014), process of model
                                                                1035

construction (Joyner & Goel 2015), and content knowledge           Any pieces of evidence that were coded as Miscategorized
(Joyner 2015), As described previously, while constructing         were scored with the category assigned to them during the
models in MILA, teams annotate their models with                   evidence coding process. These metrics were then analyzed
evidential justifications for their explanation. These             using a multivariate analysis of variance to determine
evidential justifications are the primary target for analysis      whether any differences existed in any of the metrics based
here: how well do teams justify their explanations? To             on the experimental condition.
answer this, we performed two analyses: coded evidence
analysis and quantitative model analysis.                                                     Results
                                                                   Both these analyses demonstrate the same conclusion: teams
Coded Evidence Analysis                                            in the Experimental group outperformed teams in the
1301 total pieces of evidence were supplied in support of          Control group in both the Learning and Transfer projects.
the 165 models. First, a subset of these pieces of evidence
was randomly drawn and subjected to grounded analysis.             Coded Evidence Analysis Results
Notes were taken on whether each piece of evidence was             In order to examine the difference between the Control and
acceptable as a justification for the explanation, and if not,     Experimental groups as a result of this evidence coding
for what reason the piece of evidence was unacceptable.            process, a Χ² analysis was performed to determine whether
These notes were then processed into a coding scheme with          the distributions of two samples were identical. Χ² analysis
seven categories: Acceptable, Miscategorized, Redundant,           of the results of the coding process for the Learning project
Gibberish, Irrelevant, Insufficient, and Not Evidence. The         demonstrated a statistically significant difference between
first two categories are generally noted as 'Acceptable'           the Control and Experimental groups (χ² = 52.423, p ≈ 0.0).
evidence (as Miscategorized evidence still supports the            The Experimental group outperformed the Control group:
explanation, but simply is annotated with the wrong                59.00% of the pieces of evidence written by the Control
category), and the last five categories are noted as               group teams were coded as Acceptable, while 72.83% of the
'Unacceptable' evidence.                                           pieces of evidence written by the Experimental group teams
   After establishing this coding scheme, all 1301 pieces of       received that positive designation. This improvement results
evidence were run through three rounds of coding by a              in significantly lower proportions of the Experimental
single rater with three weeks between coding sessions.             group's evidence falling into several negative categories, as
Intrarater reliability was established as very good between        documented in Table 1.
every pair of rounds of coding (Cohen's Kappa of 0.890
between the two most similar coding rounds). The results of          Table 1: Observed and Expected counts of evidence coded
this coding process were then tested using a Χ² analysis to           into each category in the Experimental group during the
examine whether the results were different between the                  Learning project, as predicted by the percentages of
Control and Experimental conditions. This was performed               evidence coded into each category in the Control group.
separately on the results of the Learning and Transfer                  Column labels show the seven categories assigned in
projects.                                                           coding: Acceptable, Miscategorized, Redundant, Gibberish,
                                                                               Irrelevant, Insufficient, Not Evidence.
Model Analysis
Given the results of that evidence coding process, five                           A       M       G        I    N      R      S
metrics were calculated for each model. Each model's "total
                                                                     Control   59.0% 12.6% 5.8% 8.4% 6.1% 5.3% 2.6%
evidence strength" was calculated by summing the strengths
of all pieces of evidence supplied in support of the model;        Expected      271      27      39      58    28     25    12
strengths were assigned on a scale of 1 (weak) to 3 (strong)
based on the category given to the evidence by the student.        Observed      335       9      25      58     9     21     3
Each model's "average model strength" was calculated by
dividing the total strength by the size of the model to              Experi-
                                                                               72.8% 12.6% 2.0% 5.4% 2.0% 4.5% 0.6%
analyze how well the team justified each individual claim of         mental
the model. Each model's "average evidence strength" was               Χ² analysis of the results of the coding process for the
calculated by dividing the total strength of the model by the      Transfer project also demonstrated a statistically significant
number of pieces of evidence to analyze the strength of the        difference between the Control and Experimental groups (χ²
individual pieces of evidence supplied by the team. Each           = 42.720, p ≈ 0.0). The Experimental group again
model's "total evidence" was calculated simply by counting         outperformed the Control group: 55.41% of the pieces of
the number of pieces of evidence without regard to strength.       evidence written by the Control group teams were coded as
Each model's "model complexity" was calculated by                  Acceptable, while 67.91% of the pieces of evidence written
counting the number of nodes and edges in the model.               by the Experimental group teams received that positive
   All of these metrics were calculated with the evidence          designation. This improvement results significantly lower
that resulted from the coded evidence analysis. Any piece of       proportions of the Experimental group's evidence falling
evidence that was coded as Unacceptable were not counted.          into several negative categories, as documented by Table 2.
                                                               1036

 Table 2: Observed and Expected counts of evidence coded             average strength of the justification supplied for each
   into each category in the Experimental group during the           individual claim in the model. The improvement was
Transfer project, as predicted by the percentages of evidence        approximately 50% across each of the three metrics,
        coded into each category in the Control group.               meaning that the models that the models produced by teams
                                                                     in the Experimental group were approximately 50%
               A       M      G       I     N        R      S        stronger (as measured by these metrics) than models
                                                                     produced by teams in the Control group. Thus, during the
 Control    55.4% 11.7% 5.2% 8.2% 7.8% 10.9% 0.9%                    Learning project, teams in the Experimental group
                                                                     constructed better-justified models than teams in the Control
Expected     193       18     29     41     27      38      3        group.
                                                                        During the Transfer project, there again existed a
Observed     237       10     17     43     18      16      8        statistically significant effect of the experimental condition
 Experi-                                                             (F = 4.2, p < 0.01). Follow-up univariate analysis showed a
            67.9% 12.3% 2.9% 4.9% 5.2% 4.6% 2.3%                     statistically significant influence of Condition on all five
  mental
                                                                     variables during the Transfer project: total model strength (F
   Between the Learning and the Transfer project, both the           = 12.0, p < 0.001), average model strength (F = 6.0, p <
Control and the Experimental groups experienced a slight             0.05), total evidence (F = 4.2, p < 0.05), average evidence
but statistically significant decrease in the overall                strength (F = 12.2, p < 0.001), and model complexity (F =
acceptability of their evidence (χ² = 15.62, p < 0.05 for the        4.9, p < 0.05). This means that the Experimental group
Control group; χ² = 37.94, p ≈ 0.0 for the Experimental              teams outperformed the Control group teams in several
group), likely based on the reduced time available for the           ways: they supplied more evidence, the individual pieces of
Transfer project. The Experimental group experienced a               evidence they supplied were stronger, and the combination
larger decrease than the Control group, however, suggesting          of these strengths led to stronger justifications of each claim
that the tutoring system had played a role in improving the          in their models and their models as a whole. This
Experimental group's models during the Learning project.             improvement was approximately 70% across each of these
Despite this larger decrease in evidence acceptability,              metrics; thus, teams in the Experimental group produced
however, the Experimental group teams still outperformed             models that were approximately 70% stronger than models
the Control group teams in the Transfer project.                     produced by teams in the Control group.
   In terms of the learning goals for the project, these results        In terms of the learning goals for this project, these results
show that teams in the Experimental group have an                    show that the previously documented increased ability to
improved ability to use evidence in support of their                 use evidence in support of arguments and explanations has
arguments and explanations compared to teams in the                  increased the strength of the final arguments that these
Control group. It is thus reasonable to say that participation       teams produce. It is thus reasonable to say that participation
with MILA–T during the unit improved teams' inquiry-                 with MILA–T during the unit improved teams' inquiry-
driven modeling by improving their ability to use evidence           driven modeling as demonstrated by improvements seen in
in support of their claims.                                          the strength of the models that these teams produced.
                                                                        These analyses were also run on the raw evidence prior to
Model Analysis Results                                               the coding process, and while the results differed, the
The results of this evidence coding process were used to             Experimental group teams outperformed the Control group
score teams' models according to the five metrics described          teams in similar ways.
previously. A multivariate analysis of variance was then
performed for the Learning and Transfer projects to discern                                   Conclusions
whether there was a difference in teams' performance along           The above results indicate that the teams who received the
these metrics based on participation in the Experimental             metacognitive tutoring system MILA–T outperformed teams
condition. If the multivariate analysis of variance revealed         that did not receive the tutoring system during the same
an effect of the condition, a follow-up univariate analysis          activity. More significantly, however, this superior
was conducted on each of the five variables to discern               performance also carried through to a new activity where
which variables were impacted.                                       MILA–T was no longer available, demonstrating that the
   During the Learning project, there existed a statistically        feedback that teams received from MILA–T was
significant effect of the experimental condition (F = 3.3, p <       internalized and transferred to a new task. This suggests that
0.01). Follow-up univariate analysis showed a statistically          access to MILA–T improved teams' inquiry-driven
significant influence of Condition on three variables during         modeling, and thus improved the quality of the models the
the Learning project: total model strength (F = 10.9, p <            teams generated. These improvements were seen both in the
0.01), average model strength (F = 7.5, p < 0.01), and total         individual evidence that teams provided and in the total
evidence (F = 5.9, p < 0.05). This means that teams in the           strength of the justifications for the teams' models as a
Experimental group constructed better-justified models in            whole.
terms of both the overall strength of the justification and the
                                                                 1037

  However, it is important to note that some of these                 Azevedo, R., Witherspoon, A., Chauncey, A., Burkett, C., & Fike,
improvements are difficult to attribute directly and solely to          A. (2009). MetaTutor: A MetaCognitive tool for enhancing self-
the metacognitive tutoring system. During both the Learning             regulated learning. In Procs. AAAI Fall Symposium on Cognitive
and Transfer projects, teams in the Experimental group                  and Metacognitive Educational Systems.
                                                                      Clement, J. (2008). Creative Model Construction in Scientists and
demonstrated a decreased propensity to supply unacceptable              Students: The Role of Imagery, Analogy, and Mental Simulation.
evidence. The acceptability of evidence is determined solely            Dordrecht: Springer.
by the text that the team provides, and MILA–T is unable to           Darden, L. (1998). Anomaly-driven theory redesign:
read this text; it can only give feedback on the categories             computational philosophy of science experiments. In T. W.
that teams choose for their evidence. Thus, despite MILA–               Bynum, & J. Moor (Eds.), The Digital Phoenix: How Computers
T's inability to give feedback on the actual text of the                are Changing Philosophy (pp. 62–78). Oxford: Blackwell.
justifications that teams provide, the text nonetheless               Edelson, D. (1997). Realizing authentic scientific learning through
improved.                                                               the adaptation of scientific practice. In K. Tobin & B. Fraser
  So how can one explain this improvement among teams                   (Eds.), International Handbook of Science Education.
                                                                        Dordrecht, NL: Kluwer.
receiving MILA–T without MILA–T giving direct feedback
                                                                      Goel, A., Rugaber, S., Joyner, D., Vattam, S. Hmelo-Silver, C.,
on the quality of this text? One explanation is that MILA–T             Jordan, R., Sinha, S., Honwad, S., & Eberbach, C. (2013)
supplied information on what makes a good justification,                Learning Functional Models of Aquaria: The ACT project on
and this information was internalized by teams even without             Ecosystem Learning in Middle School Science. In International
receiving feedback on their own present justifications; this            Handbook on Meta-Cognition and Self-Regulated Learning, R.
explanation, however, relies on very significant                        Azevedo & V. Aleven (editors), pp. 545-560, Springer.
improvement without any targeted feedback. Another                    Goldin, I., Renken, M., Galyardt, A., & Litkowski, E. (2014).
explanation is that teams in the experimental group felt                Individual Differences in Identifying Sources of Science
observed given the visual presence of the tutoring system               Knowledge. In Open Learning and Teaching in Educational
                                                                        Communities. Springer International Publishing.
and thus were more likely to engage more earnestly, leading
                                                                      Grasha, A. (1996). Teaching with Style. Pittsburgh: Alliance
to better justifications; this explanation relies on teams              Publishers.
carrying over these good habits into the Transfer project             Joyner, D. (2015). Metacognitive Tutoring for Inquiry-Driven
even after the tutoring system has been disabled.                       Modeling. Ph.D. Dissertation, School of Interactive Computing,
  The third, and we posit the most likely, explanation is               Georgia Institute of Technology, Atlanta, Georgia.
corroborated by teacher feedback. In exit interviews,                 Joyner, D. & Goel, A. (2014). Attitudinal Gains from Engagement
teachers in the experiment reflected that in Experimental               with Metacognitive Tutors in an Exploratory Learning
classrooms, they were not needed as often to answer basic               Environment. In Proceedings of the 12th International
low-level questions because the tutoring system took care of            Conference on Intelligent Tutoring Systems, Honolulu, Hawaii.
                                                                      Joyner, D. & Goel, A. (2015). Improving Inquiry-Driven Modeling
these simple feedback opportunities; they, then, were able to
                                                                        in Science Education through Interaction with Intelligent
focus on high-level and complex feedback that went beyond               Tutoring Agents. In Proceedings of the 20th International
the scope of what the tutoring system could provide. This               Conference on Intelligent User Interfaces. Atlanta, Georgia.
reflects aninterplay between individual- or team-level                National Research Council. (1996). National Science Education
tutoring systems and classrooms as a whole; in many ways,               Standards. Washington, DC: National Academy Press.
intelligent tutoring systems can be seen as approaches to             Nersessian, N. (2008). Creating Scientific Concepts. Cambridge,
offloading responsibilities from the teacher to allow the               MA: MIT Press.
teacher to more effectively direct their time toward feedback         Roll, I., Aleven, V., McLaren, B., & Koedinger, K. (2007).
that only a human can provide.                                          Designing for metacognition—applying cognitive tutor
                                                                        principles to the tutoring of help seeking. Metacognition in
  This also lends evidence in support of our proposal for
                                                                        Learning 2(2).
organizing metacognitive tutoring for inquiry-driven                  Roll, I., Aleven, V., & Koedinger, K. R. (2010). The invention lab:
modeling around the functional roles of teachers in science             Using a hybrid of model tracing and constraint-based modeling
classrooms. We plan to conduct additional research to                   to offer intelligent support in inquiry environments. In Procs.
examine whether the more and the better the metacognitive               International Conference on Intelligent Tutoring Systems.
tutors can imitate the functional roles of teachers, (1) the            Berlin: Springer.
higher is the efficacy of scaffolding learning about inquiry-         van Joolingen, W. R., de Jong, T., Lazonder, A. W., Savelsbergh,
driven modeling, and (2) larger is the offloading of                    E. R., & Manlove, S. (2005). Co-Lab: Research and
responsibilities from the science teacher to the                        development of an online learning environment for collaborative
                                                                        scientific discovery learning. Computers in Human Behavior,
metacognitive tutors.
                                                                        21(4), 671-688.
                                                                      Veenman, M., Van Hout-Wolters, B., & Afflerbach, P. (2006).
                          References                                    Metacognition and learning: conceptual and methodological
Azevedo, R., Johnson, A., Chauncey, A., & Burkett, C. (2010).           considerations. Metacognition and Learning, 1, 3–14.
   Self-regulated learning with MetaTutor: Advancing the science      White, B. & Frederiksen, J. (1998). Inquiry, modeling, and
   of learning with MetaCognitive tools. In M. Khine & I. Saleh         metacognition: Making science accessible to all students.
   (Eds.), New science of learning: Computers, cognition, and           Cognition and Instruction, 16(1). 3–118.
   collaboration in education (pp. 225-247). Amsterdam: Springer.
                                                                  1038

