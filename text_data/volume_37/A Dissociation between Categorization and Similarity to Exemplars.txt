               A Dissociation between Categorization and Similarity to Exemplars
                                              Nolan Conaway and Kenneth J. Kurtz
                                          Department of Psychology, Binghamton University
                                                      Binghamton, NY 13905 USA
                             Abstract                                 learners categorize based on proximity to reference points
   Research in category learning has been dominated by a
                                                                      associated with category responses. More specifically,
   ‘reference point’ view in which items are classified based on      reference point models assume that classification decisions
   attention-weighted similarity to reference points (e.g.,           are based on computing the similarity of a presented cue to
   prototypes, exemplars, clusters) in a multidimensional space.      stored reference points, typically following an inverse
   Although much work has attempted to distinguish between            exponential function of geometric distance (Shepard, 1987).
   particular types of reference point models, they share a core         An important feature of these models is that similarity can
   design principle that items will be classified as belonging to     be attentionally-mediated, but the inescapable commitment
   the category of the most proximal reference point(s). In this
   paper, we present an original experiment challenging this          is to stimulus generalization (Nosofsky, 1986). Although
   distance assumption. After classification training on a            legitimate concerns have been raised about the validity of
   modified XOR category structure, we find that many learners        this distance assumption in psychological models (Medin,
   generalize their category knowledge to novel exemplars in a        Goldstone, & Gentner, 1993; Rips, 1989), reference point
   manner that violates the distance assumption. This pattern of      accounts have remained leaders in the field of category
   performance reveals a fundamental limitation in the reference      learning due to superior quantitative fits to behavioral data
   point framework and suggests that stimulus generalization is
                                                                      (e.g., Kruschke, 1992; Medin & Schaffer, 1978; Nosofsky et
   not a reliable foundation for explaining human category
   learning.                                                          al., 1994a). Indeed, there exist few examples of empirical
                                                                      phenomena in the artificial classification learning paradigm
   Keywords: categorization, generalization, formal modeling          that are not well described in terms of distance to reference
                                                                      points.
                          Introduction
The history of research on human category learning is rich                               The Current Study
and complex. Whereas early studies explored the capacity              We report an original experiment challenging the idea that
for human learners to acquire rule-defined concepts (i.e.,            people make classification responses using distance to
Bruner, Goodnow, & Austin, 1956), current research and                stored reference points. Our experiment is based on specific
theory is now largely centered around a ‘reference-point’             predictions made by two contemporary models: ALCOVE
framework, where learners are thought to master categories            (Kruschke, 1992), and DIVA (Kurtz, 2007). ALCOVE is an
by learning to associate stored perceptual referents (e.g.,           adaptive network model that straightforwardly embodies the
prototypes, exemplars) with individual categories.                    central tenets of the reference point framework: ALCOVE
Reference point models of categorization (e.g., Kruschke,             uses error-driven learning to optimize attention weights
1992; Love et al., 2004; Nosofsky, 1986; Smith & Minda,               mediating the similarity computation and association
2000) have enjoyed wide success in explaining human                   weights between the exemplar-based reference points and
behavior, and are widely considered a definitive account of           category nodes. ALCOVE has been tested thoroughly for its
how categories are learned, represented, and applied.                 ability to account for behavioral data (Kruschke, 1992,
   Although specific reference point models differ from one           1993; Nosofsky et al., 1994a) and has remained a leading
another in a variety of ways, they tend to make generally             account of human category learning since its publication.
similar representational and process assumptions. Chiefly,               DIVA (Kurtz 2007) offers a similarity-based alternative
all reference point models assume that categories are                 to the reference point framework by representing statistical
represented by one or more points in a psychological space.           models of categories in a DIVergent Autoencoder. Rather
On the extremes, a prototype model represents each                    than learning to associate reference points with category
category in terms of its central tendency, i.e., the average          responses, DIVA learns how to correctly reconstruct
across known members, while an exemplar model would                   presented cues on their category channels. Classification
represent the category in terms of the individual items               decisions are made based on the reconstructive error
themselves. Successful reference point models employ a                observed across category channels – if one of DIVA’s
selective attentional mechanism that allows them to weight            channels is able to reconstruct a cue without much
the importance of each stimulus dimension (Medin &                    distortion, then the cue will likely be classified as a member
Schaffer, 1978; Kruschke, 1992).                                      of that category.
   Discrepancies between reference point models have been                Both models rely on a form of similarity to guide
the subject of extensive debate (e.g., Homa, 1984;                    classification (ALCOVE uses attention-weighted distance to
Nosofsky, 1992; Smith & Minda, 2000), but at present we               exemplar reference points; DIVA uses a more implicit form
are interested in their common design principle: that                 of similarity in that inputs are more likely to be successfully
                                                                  435

reconstructed if they are like known category members),             example, Type II learning was faster when learners are
and accordingly their predictions are often very much alike.        provided instructions that encourage rule formation and
However, the models differ in their commitment to                   when stimulus dimensions are more easily verbalizable.
distance-based classification. Since DIVA does not use              Kurtz et al. (2013) argue for a revision of the general SHJ
reference points to represent categories, its responses can         ordering along with recognition that models should be able
diverge from predictions made by distance-based accounts            to account for systematic variability in Type II acquisition
of categorization. We can therefore apply the two models to            To date, nearly all work on XOR has represented the
generate      predictions    about     human     classification     categories using binary stimulus dimensions. These stimulus
performance relative to the distance assumption.                    sets, however, are limited by the lack of a generalization set.
                                                                    Consequently, it is difficult to satisfactorily address the role
A Priori Simulations                                                of distance in classification. Instead, we employ a two-
In a set of a priori simulations using DIVA and ALCOVE,             dimensional, continuous adaptation of the XOR structure
we compared generalization predictions after training on a          (see Figure 1) that maintains the overall logical structure of
variant of the well-known exclusive-OR (XOR) category               the categories while providing a generalization set.
structure. XOR categories are commonly studied in learning             In our simulations, DIVA and ALCOVE were tested for
and machine learning research alike, and are defined by a           generalization performance after training on the continuous
logical rule operating across two or more dimensions. For           XOR categories. We observed that the two models
example, one category might consist of white squares (00)           generalized similarly and the predicted classification
and black circles (11), and the contrast category would then        responses for both models were consistent with the distance
consist of black squares (10) and white circles (01).               assumption. However, we also tested a novel variation in
   XOR categories have played an important role in the              which the trained set included a partial version of one of the
literature on human category learning. In a classic study,          categories (i.e., one of the four quadrants was left untrained,
Shepard, Hovland, & Jenkins (1961) measured the number              as in Figure 1) along with a standard version of the other
of errors made during classification training on six                category. We found that DIVA often generalizes as if it had
elemental category types comprised of eight exemplars that          been trained on the full version of XOR. That is, DIVA
vary in three binary dimensions. The second category type           often makes the prediction that the ‘one-quadrant’ category
(Type II) represents the logical XOR structure over two             generalizes to exemplars in the untrained quadrant. This
dimensions, with a third irrelevant dimension. Shepard et al.       prediction is particularly interesting because the critical test
found that Type II was learned second quickest of the six           items are closer to the members of the ‘two-quadrant’
elemental category types—it was even learned more quickly           category: the central exemplar in the untrained quadrant is,
than the Type IV categories which are linearly separable and        on average, 1.67 city blocks away from members of the
adhere to a minimal version of family-resemblance (Rosch            two-quadrant category, and 3 blocks away from members of
& Mervis, 1975). Formal modeling work later explained the           the one-quadrant category (consistent results arise using a
ease of acquisition differences in terms of selective attention     Euclidean metric, though we used a cityblock metric for the
(Nosofsky et al., 1994a): by learning to ignore the irrelevant      current study). Accordingly, reference point models like
dimension, ALCOVE was the only model that could predict             ALCOVE have no ability to produce this pattern of
a strong Type II advantage. Subsequently, fitting the               results—they instead predict that generalization will be
observed Type II advantage has been a leading benchmark             based on the more proximal exemplars belonging to the
for formal models of category learning.                             two-quadrant category.
   In a detailed investigation of the Type II advantage, Kurtz
et al. (2013) found that the ease of Type II acquisition varies     Behavioral Experiment
markedly based on a number of methodological factors. For           We designed a straightforward study to test the predictions
       Figure 1. Left. Sample stimuli. Middle. Continuous, two-dimensional XOR categories. Right. Partial XOR categories.
                                                                436

made by DIVA and ALCOVE about generalization                     known about how presentation frequency affects
performance after learning the partial-XOR structure.            generalization. After training, participants completed 49
Specifically, we were interested in the extent to which          generalization trials consisting of items sampled at 7
human learners would generalize the one-quadrant category        positions on each dimension. All of the training examples
to an untrained area of the stimulus space that is spatially     were included (intermixed).
closer to members of the two-quadrant category.                     Participants were informed that there would be test trials
   It is worth noting that our goal here is not just another     prior to beginning the experiment. The instructions did not
example of the ‘which model wins’ approach. Instead, we          encourage learners to engage in hypothesis testing to
are using these models to test assumptions about how             discover a rule. On each trial, a single stimulus was
classification decisions are reached: whereas ALCOVE             presented on a computer screen and learners were prompted
presumes that responses are exclusively distance-based,          to make a classification decision by clicking one of two
DIVA is not theoretically committed to the distance              buttons (labeled ‘Alpha’ and ‘Beta’). During the training
assumption. We therefore use the models to evaluate the          phase, learners were given feedback on their selection.
validity of the distance assumption in the psychology of         Feedback was not provided during the generalization phase.
category learning.
                                                                 Results. One participant was excluded from analysis due to
Participants and Materials. 61 undergraduates from               experimenter error leaving 30 participants in each condition.
Binghamton University participated in fulfillment of a              Not surprisingly, the two category structures differed in
course requirement. Stimuli were squares varying in shading      terms of ease of acquisition (see Figure 2). Specifically, the
and size (see Figure 1 for samples). These dimensions were       partial XOR categories were learned more quickly than the
chosen in order to maintain compatibility with ‘standard’        full XOR categories, t(58) = 4.06, p<0.001, d=1.03.
materials used in experiments involving XOR categories           However, most of the learners in both conditions showed
(e.g., Nosofsky et al., 1994a; Shepard et al., 1961).            evidence of mastery of the categories by the end of training.
Exemplars were automatically generated at 7 positions on            Our primary focus is on the generalization data. Each
each dimension (7 shading * 7 line spacing = 49 examples).       participant’s set of responses in the test phase yields a 7x7
The assignment between perceptual and conceptual                 generalization gradient of classification performance. These
dimensions was randomly counterbalanced across                   data revealed a variety of individual differences in
participants.                                                    classification strategies. To formally profile each learner’s
                                                                 generalization responses, we compared each gradient to a
Procedure. Each participant was randomly assigned to             set of templates or idealized gradients representing idealized
receive training on either the full or partial XOR category      patterns of responses under different possible classification
structure. In both conditions, participants completed 96         strategies. Each learner was profiled based on finding the
training trials (12 blocks consisting of the 8 training          template that best matched their performance according to a
examples). In order to equate block size in the partial          mean-squared error, MSE, metric.
condition, the one-quadrant category exemplars were                 In the full XOR condition, we identified two prevalent
presented twice within each block. This way of handling the      generalization profiles: 1) systematic XOR responses,
unbalanced category structure raises the issue of exemplar       reflecting mastery of the categories (Learners), and 2)
presentation frequency (Nosofsky, 1988), though little is        random responses, reflecting failure to master the categories
                                                                 (Non-Learners). This dichotomy fits nicely with evidence
                                                                 from Kurtz et al. (2013) that XOR learning is bimodal—
                                                                 most learners either fully master the categories or do not
                                                                 figure them out at all.
                                                                    We identified four profiles of partial XOR generalization:
                                                                 1) Extrapolation-Based generalization in which the one-
       Accuracy                                                  quadrant category is extended to the untrained quadrant, 2)
                                                                 50/50 generalization, in which the learner randomly
                                                                 classifies exemplars in the untrained quadrant, but has
                                                                 mastered the categories otherwise, 3) Proximity-Based
                                                                 generalization, in which the learner extends the proximal
                                                                 two-quadrant category to the untrained quadrant, and 4)
                                                                 Non-learner generalization, reflecting the random
                                                                 performance of a non-learner.
                                                                    The profile distribution for each category type is
                                Training Block                   displayed in Table 1. We observed a substantial number of
                                                                 non-learners in the full XOR condition. These learners are
          Figure 2. Aggregate training accuracy for the          interesting in that they achieved a reasonably high level of
               partial and full XOR categories.                  accuracy (81%) in the last block of training, but their poor
                                                             437

                                                                    about generalization following training on the full and
                                                                    partial XOR categories. It is important to note that these
                                                                    models are conventionally applied to explain aggregated
                                                                    data. In this case, however, we are testing the models on
                                                                    their ability to match an individual differences
                                                                    distribution—we are interested in whether either model can
                                                                    explain the distribution of generalization profiles that was
                                                                    observed behaviorally (Table 1).
                                                                    Procedure. For both models, we generated a large number
                                                                    of predictions using a wide range of parameter values. We
     Figure 3. Aggregate generalization gradients for the
                                                                    searched over the predictions made by different parameter
  extrapolation (left) and proximity (right) profiles in the
                                                                    values using a ‘grid-search’ method where each model was
                   partial XOR condition.
                                                                    initialized 30 times (corresponding to the number of
                                                                    participants in each condition) at each search point. We fit
generalization performance suggests that a memorization             DIVA over four parameters: number of hidden units,
strategy may have been used (particularly given the small           learning rate, initial weight range, and a focusing parameter,
size of the training set). An intriguing implication is that        β (Conaway & Kurtz, 2014). Likewise, we fit ALCOVE
there exists a type of exemplar memorization that is                over its specificity constant, association learning rate,
effective during learning, but that fails to support systematic     attention learning rate, and response mapping constant. Note
generalization. It is not clear how stimulus generalization         that although we allowed both models to use attentional
would account for this pattern.                                     mechanisms, all dimensions are equally relevant in the
   Nearly all of the successful learners in the partial XOR         categories we tested. By profiling the predictions made by
condition exhibited ether extrapolation-based (9/30) or             each initialization, we can create a distribution of predicted
proximity-based (19/30) generalization. This distribution is        generalization profiles that are linked to particular
of great interest: the presence of extrapolation-based              parameterizations. We are then able to assess the quality of
generalization suggests that classification responses are not       each parameterization’s predictions relative to our
always reached by comparing a presented cue to known                behavioral findings.
reference points. In other words, these learners generalize            Our training and generalization procedure was identical to
with blatant disregard for proximity to exemplars.                  the one we used in our behavioral data. After training on full
Aggregated generalization gradients for the extrapolation-          XOR, each initialization was profiled based on its match to
based and proximity-based profiles are depicted in Figure 2.        the Learner and Non-Learner profiles. After training on
                                                                    partial XOR, we profiled each generalization gradient based
Summary. To begin, we found that the partial XOR                    on its match to the Extrapolation-Based, 50/50, Proximity-
category structure was acquired more easily than full XOR.          Based, and Non-Learner profiles.
This result is interesting given previous work showing that
linearly separable classifications are not always more              Results. Both ALCOVE and DIVA provided a full account
quickly acquired that non-linearly separable ones (Medin &          of the Full XOR data. In particular, both models were able
Schwanenflugel, 1981). In this experiment, the linearly             to match the rate of non-learners identified at the
separable partial XOR categories were learned more                  generalization phase. Both models were able to do so under
quickly.                                                            a wide range of parameterizations.
   Our result of primary interest was that many learners who           The models, however, diverged substantially when they
were trained on the partial XOR categories generalized the          were trained on the partial categories. Notably, and as
one-quadrant category to the untrained quadrant. The                predicted, we were unable to find any parameterization of
presence of this extrapolation-based generalization shows           ALCOVE that could produce extrapolation-based
that people do not universally make classification decisions        generalization. Instead, ALCOVE commonly produced
based on distance to stored reference points—exemplars in           proximity-based and 50/50 generalization gradients.
the untrained quadrant are more proximal to members of the          ALCOVE’s generalization was highly dependent on its
two-quadrant category. Since this is the central tenet of           parameter values: for example, the model was more likely to
reference point theories of categorization, these results pose      produce 50/50 gradients when the specificity value was
a significant challenge. In the following section, we               large.
formally evaluate DIVA and ALCOVE for their ability to fit             These results confirm our earlier simulations and
these behavioral results.                                           theoretical analysis. ALCOVE can only produce
                                                                    classification responses based on distance to exemplar
Simulations                                                         reference points, so the model will never be able to extend
The overall goal of the following simulations is to evaluate        the one-quadrant category to exemplars that are more
the range of predictions made by DIVA and ALCOVE                    proximal to the two-quadrant category. Accordingly,
                                                                438

ALCOVE provides a sharply limited and unsatisfactory               of learners (9/30) generalized according to the pattern of full
account of generalization in the partial XOR condition.            XOR. That is, these learners extrapolated the one-quadrant
  As expected, DIVA commonly predicted extrapolation-              category to novel exemplars that were actually more
based gradients after training on the partial categories. The      proximal, or similar, to members of the two-quadrant
model was most likely to generalize from the one-quadrant          category. In doing so, these extrapolation-based learners
category to the untrained area with a moderate to larger           violated the distance assumption, showing that classification
number of hidden units (3–10), a small to moderate learning        is not always based on distance to known reference points.
rate (<= 0.5), and a large initial weight range (>= 1.5). No          We used formal simulations with ALCOVE (Kruschke,
parameterization produced a strong majority of                     1992) to show that traditional reference point models could
extrapolation-based gradients: in our simulations DIVA             not account for the presence of the extrapolation-based
predicted a maximum of 16/30 extrapolation-based                   generalization profile in our data. Because these models are
gradients. Nonetheless, DIVA’s ability to produce this             limited to classification based on distance to stored
generalization profile sets the model apart from traditional       reference points, they are unable to extend the partial
reference point models that are limited to distance-based          category into the untrained quadrant. As such, we expect
classification. Table 1 displays the best predictions made by      this limitation to be shared by any standard reference point
each model (minimizing the mean-squared error, MSE,                account including prototype models (Smith & Minda, 2000)
between the observed and predicted frequencies of each             and adaptive cluster models (Love, Medin, & Gureckis,
generalization profile).                                           2004). The extrapolation-based generalization we observed
                                                                   is therefore a significant challenge to the assumptions
    Table 1. Observed and predicted generalization profile         underlying reference point models.
                         frequencies.                                 We contrasted ALCOVE’s simulations with predictions
                                                                   made by DIVA (Kurtz, 2007), which is not a reference point
                               Obs.     ALCOVE        DIVA         model and is therefore not limited to classification based on
    Full     Learner             19         19         19          distance. We observed that DIVA could produce
    XOR      Non-Learner         11         11         11          extrapolation-based generalization after training on the
             Extrapolation        9          0          8          partial categories, providing a full account of the
  Partial    50/50                1          0          0          generalization data.
    XOR      Proximity           19         23         17
             Non-Learner          1          7          5          Learning More Than Reference Points
                                                                   Many of our partial XOR learners do not appear to have
Summary. Both models were able to accurately predict               represented the categories using reference points in the input
generalization after training on the full XOR categories.          space (e.g., prototypes, exemplars, adaptive clusters).
However, ALCOVE was unable to explain extrapolation-               Further work will be needed to determine exactly what these
based generalization following training on the partial             individuals did learn, though their generalization responses
categories. DIVA provides a good account of the full               indicate that, through training on the partial categories, they
distribution of generalization profiles observed behaviorally.     acquired a category representation that is consistent with the
This calls into question the reduction of human category           logical structure of XOR (i.e., white squares and black
learning to stimulus generalization that is inherent in            circles versus black squares and white circles). That is, our
reference point models.                                            learners may have represented the partial categories using a
                                                                   rule, rather than similarity.
                         Discussion                                   Given this interpretation, it is possible that rule-based
                                                                   models of category learning (i.e., RULEX; Nosofsky et al,
Theoretical work in human category learning has been               1994b; Nosofsky & Palmeri, 1998) can provide an account
closely tied to a reference point framework. Although              of the generalization we observed in the partial XOR
reference point models differ from one another in a variety        condition. However, the simplest rules that characterize the
of ways, they share a common assumption: classification            two categories would not appear to offer a systematic basis
decisions are reached by comparing presented cues to one or        for generalizing to the untrained quadrant. Hybrid or
more reference points. That is, all reference point models         multiple-systems models that incorporate rule-based
assume that classification decisions are based on distance.        learning components (Ashby et al., 1998; Erickson &
  In this paper, we reported an experiment that directly           Kruschke, 1998) might be able to produce extrapolation-
tested the distance assumption. Learners were given                based generalization, but this would have to be despite the
classification training on one of two versions of the              use of a category structure that requires information
exclusive-Or (XOR) categories. In the full XOR condition,          integration.
the XOR categories were represented in a continuous, two              Alternatively, our simulation results with DIVA suggest
dimensional stimulus space. In the partial XOR condition,          that models may not need to represent rules explicitly in
one of the four quadrants was left untrained.                      order to capture the patterns of generalization observed
  Most importantly, after classification training on the           behaviorally. Although DIVA does not explicitly learn
partial XOR categories, we found that a sizable proportion
                                                               439

rules, the model was able successfully to produce                       Love, B. C., Medin, D. L., & Gureckis, T. M. (2004).
extrapolation-based generalization—it is therefore possible               SUSTAIN: A network model of category learning.
that the model is able to approximate rule knowledge                      Psychological Review, 111, 309-332.
through training on the partial XOR categories. Clearly,                Medin, D. L., & Schaffer, M. M. (1978). A context theory
more work is needed to formally describe the knowledge                    of classification learning. Psychological Review, 85, 207-
that DIVA acquires through training on partial XOR.                       238.
   Finally, it is important to note that the critical result arises     Medin, D. L., & Schwanenflugel, P. J. (1981). Linear
in only a subset of the sample (9/30). However, we believe                separability in classification learning. Journal of
it is important for models to provide an explanation of the               Experimental Psychology: Human Learning and Memory,
full set of commonly occurring profiles, and preferably                   7, 355-368.
provides a basis for explaining the variability. While                  Medin, D. L., Goldstone, R. L., and Gentner, D. (1993).
reference point models can account for the majority result                Respects for similarity. Psychological Review, 100, 254-
(proximity-based generalization), we found that DIVA can                  278.
correctly predict the occurrence of both proximity and                  Nosofsky, R. M. (1986). Attention, similarity, and the
extrapolation-based generalization. These results are best                identification-categorization relationship. Journal of
explained outside of the reference point framework. In sum,               Experimental Psychology: General, 115(1), 39-57.
by measuring generalization, removing the complicating                  Nosofsky, R. M. (1988). Similarity, frequency, and category
factor of attention, avoiding aggregated outcomes, and                    representations. Journal of Experimental Psychology:
putting a novel twist on an old favorite (XOR), we provide a              Learning, Memory, and Cognition, 14(1), 54-65.
clear demonstration of the need to look beyond similarity to            Nosofsky, R. M. (1992). Exemplars, prototypes, and
reference points in explaining human categorization.                      similarity rules. In A. F. Healy, S. M. Kosslyn, & R.
                                                                          Shiffrin (Eds.), Essays in honor of William K. Estes (pp.
                         References                                       149 –167). Hillsdale, NJ: Erlbaum.
Ashby, F., Alfonso-Reese, L., Turken, A., & Waldron, E.                 Nosofsky, R. M., Gluck, M. A., Palmeri, T. J., & McKinley,
   (1998). A neuropsychological theory of multiple- systems               S. C. (1994a). Comparing models of rule-based
   in category learning. Psychological Review, 105, 442–                  classification learning: A replication and extension of
   481.                                                                   Shepard, Hovland, and Jenkins (1961). Memory &
Bruner, J. S., Goodnow, J. J., & Austin, G. A. (1956). A                  Cognition, 22(3), 352-369.
   study of thinking. New York: Wiley.                                  Nosofsky, R. M., & Palmeri, T. J. (1998). A rule-plus-
Conaway, N. B., & Kurtz, K. J. (2014). Now you know it,                   exception model for classifying objects in continuous-
   now you don't: Asking the right question about category                dimension spaces. Psychonomic Bulletin & Review, 5(3),
   knowledge. Proceedings of the Thirty-Sixth Annual                      345-369.
   Conference of the Cognitive Science Society. (pp. 2062-              Nosofsky, R. M., Palmeri, T. J., & McKinley, S. C. (1994b).
   2067). Quebec City, Canada: Cognitive Science Society.                 Rule-plus-exception model of classification learning.
Erickson, M. A. & Kruschke, J. K. (1998). Rules and                       Psychological Review, 101(1), 53-79.
   exemplars in category learning. Journal of Experimental              Rips, L. J. ( 1989). Similarity, typicality, and categorization.
   Psychology: General, 127, 107-140.                                     In S.Vosniadou & A.Ortony ( Eds.), Similarity and
Homa, D. (1984). On the nature of categories. In G. H.                    analogical reasoning. Cambridge, England: Cambridge
   Bower (Ed.), Psychology of learning and motivation (Vol.               University Press.
   18, pp. 49-94). New York: Academic Press.                            Rosch, E., & Mervis, C. B. (1975). Family resemblances:
Kruschke, J. K. (1992). ALCOVE: An exemplar-based                         Studies in the internal structure of categories. Cognitive
   connectionist model of category learning. Psychological                Psychology, 7, 573–605.
   Review, 99(1), 22-44                                                 Shepard, R. N. (1987). Toward a universal law of
Kruschke, J. K. (1993). Human category learning:                          generalization for psychological science. Science, 237,
   Implications for backpropagation models. Connection                    1317-1323.
   Science, 5(1), 3-36.                                                 Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961).
Kurtz, K. J. (2007). The divergent autoencoder (DIVA)                     Learning      and     memorization       of    classifications.
   model of category learning. Psychonomic Bulletin &                     Psychological Monographs: General and Applied, 75, 1–
   Review, 14, 560 –576.                                                  42.
Kurtz, K. J., K. R. Levering, R. D. Stanton, J. Romero, and             Smith, J. D., & Minda, J. P. (2000). Thirty categorization
   S. N. Morris (2013). Human learning of elemental                       results in search of a model. Journal of Experimental
   category structures: revising the classic result of Shepard,           Psychology: Learning, Memory, & Cognition, 26, 3-27.
   Hovland, and Jenkins (1961). Journal of Experimental
   Psychology: Learning, Memory, and Cognition, 39(2),
   552-572.
                                                                    440

