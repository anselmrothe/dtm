                  A Resource-Rational Approach to the Causal Frame Problem
                 Thomas F. Icard, III (icard@stanford.edu), Noah D. Goodman (ngoodman@stanford.edu)
                                   Departments of Philosophy and Psychology, Stanford University
                              Abstract                                with regard to causal knowledge. Assuming a person’s causal
                                                                      knowledge can be represented (at least implicitly) in terms
   The causal frame problem is an epistemological puzzle about
   how the mind is able to disregard seemingly irrelevant causal      of a very large directed graphical model (or Bayes net), the
   knowledge, and focus on those factors that promise to be use-      causal frame problem arises because computations involv-
   ful in making an inference or coming to a decision. Taking         ing the entire model promise to be intractable. Somehow the
   a subject’s causal knowledge to be (implicitly) represented in
   terms of directed graphical models, the causal frame problem       mind must focus in on some “submodel” of the “full” model
   can be construed as the question of how to determine a rea-        (including all possibly relevant variables) that suffices for the
   sonable “submodel” of one’s “full model” of the world, so as       task at hand and is not too costly to use. In as far as a proper
   to optimize the balance between accuracy in prediction on the
   one hand, and computational costs on the other. We propose a       submodel may nonetheless neglect relevant causal informa-
   framework for addressing this problem, and provide several il-     tion, this may lead to inaccuracy. We suggest that perhaps
   lustrative examples based on HMMs and Bayes nets. We also          the mind tolerates local and occasional inaccuracy in order to
   show that our framework can account for some of the recent
   empirical phenomena associated with alternative neglect.           achieve a more global efficiency. To substantiate this claim,
   Keywords: frame problem, bounded-resource-rationality,             we need a better understanding of what it is for a submodel
   causal reasoning, alternative neglect.                             to be more or less apt for a task, from the perspective of a
                                                                      reasoner with bounded time and resources. It is clear that hu-
                          Introduction                                man reasoners cannot consult an indefinitely detailed mental
To any inference or decision problem there is no a priori             model of the world for every inference task. So what kind of
bound on what aspects of a person’s knowledge may be use-             simpler model should a reasoner consult for a given task?
fully, or even critically, applied. In principle, anything could          This work follows a line of research in cognitive science
be related to anything. This challenge is sometimes referred          concerned with bounded or resource rationality (Simon 1957;
to as the frame problem, characterized by Glymour (1987)              Gigerenzer and Goldstein 1996, inter alia), and specifically
as: “Given an enormous amount of stuff, and some task to be           in the context of probabilistic models, and approximations
done using some of the stuff, what is the relevant stuff for the      thereto (Vul et al., 2014). In addition to inherent interest, it
task?” (65). The question is foundational to reasoning and ra-        has recently been suggested that considerations of bounded
tionality. Part of what makes people so smart is the ability to       rationality may play a methodological role in sharpening the
solve the frame problem, ignoring those aspects of the world          search for reasonable accounts of the cognitive processes
(and one’s knowledge of it) that are irrelevant to the problem        underlying inductive inference (Griffiths et al., 2014; Icard,
at hand, thereby simplifying the underlying reasoning task,           2014). However, in this tradition there has been more of a
turning an intractable problem into a tractable one.                  focus on the algorithm used for inference in a given model,
   Not all of the psychological literature paints a picture of        and less attention paid to questions of model selection.
human reasoners as so adept at disregarding only the irrele-              In this largely programmatic paper we offer a framework
vant, however. In the literature on causal reasoning, there is        for addressing the causal frame problem by selecting rational
a robust empirical finding that subjects often neglect causal         submodels, provide several illustrative examples, and address
variables, including those that are in principle accessible to        some of the empirical findings concerning alternative neglect.
the subject, which would sometimes allow the subject to
make better, more accurate inferences. So called alterna-                           Resource-Rational Submodels
tive neglect is an especially well documented phenomenon,             Let P(X) be a joint probability distribution over random vari-
in which subjects ignore alternative possible causes of some          ables X = X1 , X2 , . . . , and define a query to be a partition
event (Fischhoff et al. 1978; Klayman and Ha 1987; Fern-              hXq ; Xl ; Xe i of X into query variables, latent variables, and
bach et al. 2011, inter alia), even when doing so leads to            evidence variables, respectively. A typical query task is to
incorrect inferences. More generally, at least in the causal          find values of Xq that maximize the conditional probability
domain, subjects seem to consider “smaller” models of the             P(Xq | Xe = v), marginalizing over Xl . Clearly, the difficulty
world than would be relevant to the task at hand, given the           of this and related tasks scales with the number of variables.
subject’s knowledge and reasoning abilities. This has led             We will be interested in smaller models with fewer variables:
many to criticize the behavior as normatively objectionable.          a sublist X∗ of X with associated distribution P∗ (X∗ ), and par-
Perhaps people are ignoring too much of their knowledge.              tition hXq ; X∗l ; X∗e i, so that only latent and evidence variables
   We would like to suggest that alternative neglect and re-          are ignored. The intention is for P∗ to be close in structure
lated phenomena may be natural consequences of a general              to P but without the neglected variables. In each of the cases
mechanism for sifting the most pertinent information from             considered here (HMMs and Noisy-Or Bayes nets), there will
all other knowledge—that is, for solving the frame problem            be a canonical way of choosing P∗ given X∗ .
                                                                  962

   Given P(X) and P∗ (X∗ ), there are at least two kinds of                   second example, of a causal Bayes net, shows that under a
questions we would like to ask. The first of these captures                   sampling scheme for decision making, the submodel actually
how well an agent will fare by using the approximate sub-                     outperforms the “ideal” full model in many cases, even with-
model, as compared with the full model, holding fixed a pro-                  out taking costs into account. Finally, the third example re-
cedure for using this model to choose an action. For instance,                veals that certain kinds of inferences may be subject to greater
the agent might use this distribution to compute expected util-               information loss resulting from neglect than others. Recent
ity, or to sample from the model in order to approximate ex-                  empirical literature shows that people respect this difference,
pected utility (see, e.g,. Vul et al. 2014). The second question              suggesting that there may indeed be an element of resource
asks how far off the approximate model is from the “true”                     rationality in alternative neglect behavior.
model in its probabilistic predictions.1
                                                                                                        Hidden Markov Models
1. Given a decision problem with action space A and utility
                                                                              A Hidden Markov Model is given by a time-labeled sequence
   function U : Xq × A → R, and assuming fixed a (stochastic)
                                                                              of state variables . . . , X−1 , X0 , X1 , . . . , with transition proba-
   choice rule ΨQ taking a distribution Q over Xq to a distri-
                                                                              bilities P(Xt+1 | Xt ), and a sequence of evidence variables
   bution on actions, what are the respective expected utilities
                                                                              . . . ,Y−1 ,Y0 ,Y1 , . . . , with emission probabilities P(Yt | Xt ). In a
   of using P and P∗ under (assumed “true”) distribution P?
                                                                              typical inference task, after observing values of Y (blue), we
   That is, how great is the following difference ∆P,P∗ ?
                                                                              are interested in the value of Xt+1 (beige) at time t + 1:
   ∆P,P∗ = Ex∼P EA∼ΨP U(x, A) − Ex∼P EA∼ΨP∗ U(x, A)
                                                                                               ...
2. How far is P∗ from P in information distance, for the vari-
   ables Xq of interest? That is, what is the Kullback-Leibler
   (KL) divergence between P and P∗ with respect to Xq ?
                                              P(Xq = x | Xe = v)
KL(P || P∗ ) = ∑ P(Xq = x | Xe = v) log
                 x                           P∗ (Xq = x | X∗e = v∗ )          For instance, variables X might be whether there is high or
                                                                              low air pressure, while observations Y are of sun or clouds.
In general we will expect that KL(P || P∗ ) > 0, and ∆P,P∗ > 0,
                                                                              While in principle determining X0 —today’s weather—could
indicating that the full model yields more accurate results.
                                                                              depend on indefinitely earlier observations and states at times
However, in line with other work on resource rationality, as-
                                                                              t = −1, −2, . . . , one has the intuition that “looking back” only
suming the requisite computations using distribution P come
                                                                              a few days should be sufficient for typical purposes.
with a greater cost than when using P∗ , this difference in cost
                                                                                 For a first illustration, consider a simple HMM with binary
may well be worth the difference in accuracy or utility.
                                                                              variables Xt and Yt , and probabilities as follows:
   Suppose we have a cost function c : P → R+ , assigning a
real cost to each approximation P∗ ∈ P . For instance, c may                        P(Xt+1 = 1 | Xt = 1) = P(Xt+1 = 0 | Xt = 0) = 0.9
simply charge a constant amount for each variable used in
the associated submodel, or may be proportional to a graph                             P(Yt = 1 | Xt = 1) = P(Yt = 0 | Xt = 0) = 0.8
property such as tree width of the corresponding graphical
model. Given a set P of approximations, we can then ask for                   Our class P of approximate distributions includes all trun-
the resource-optimal approximation in either of the above two                 cations of the model at variable Xt−N , in which case we as-
senses. For instance, with KL-divergence the distributions of                 sume the distribution P∗ (Xt−N ) is uniform. In Figure 1 is a
interest include any P̃ that optimally trades off cost against                graph showing the KL-distance between the full model and
KL-distance from the true distribution P:                                     a submodel with only N previous time steps included. We
              P̃ = argmin KL(P || P∗ ) + c(P∗ ) .                (1)                          0.2
                       P∗ ∈P
Notice the immediate result that any node X that is screened-
off from Xq by Xe should be eliminated: doing so will not
reduce the KL, but will improve the efficiency of inference.                                  0.1
                                                                                         KL
   In what follows we illustrate these ideas with three exam-
ples using familiar graphical models. The first example, of
an HMM, demonstrates an extreme case of the frame prob-
lem in which the initial model is infinite. We show that the
resource-optimal submodel is not only finite, but often quite                                  0
                                                                                                    1    2   3   4        5       6    7   8   9   10
small, and in many instances includes just a single node. The                                                        Number of nodes
    1 Strictly speaking, the second can be seen a special case of the
                                                                                Figure 1: Dropoff in KL as function of number of nodes.
first, with a logarithmic scoring rule (Bernardo and Smith, 1994).
                                                                        963

chose this particular model for illustration because the KL-                                   worth the cost to include more than a single state variable in
distance is relatively high for the submodel with only one                                     the model (see Fig. 3). This is perhaps not surprising, given
node. Nonetheless, even for this model, the value drops off                                    the low KL-values in Fig. 2. However, even in models with
rather dramatically with only a few additional nodes.                                          significantly higher KL-distances in general, it does not pay
   This model has a low mixing rate, as measured by the sec-                                   to include more than one or two additional nodes. As we
ond eigenvalue (λ2 ) of the transition matrix for the underly-                                 increase or decrease the cost c of a node, the graph becomes
ing Markov model (the transition probabilities). In general, a                                 less flat and flatter, respectively. For instance, provided that
higher λ2 value means a lower mixing rate, which means the                                     c > 0.148, the optimal number is 1 for all these values of λ2 =
past provides more information about the present. One might                                    0.1, . . . , 0.9. Decreasing the cost would increase the optimal
expect that in such cases it is more detrimental to ignore pre-                                number for larger values of λ2 , but for any c > 0 this number
vious state variables. If we look at the graph (Figure 2) of                                   is of course still finite.
KL-distances as a function of λ2 , holding fixed the observa-                                     As Fig. 3 indicates, the optimal number of nodes to include
tion probabilities as above, we see that this model (for which                                 in an HMM is not only finite, but can typically be quite small,
λ2 = 0.8) is indeed near the higher end.                                                       in line with ordinary intuition.
                     0.3
                                                                                                          Neglecting Alternative Causes
                                                                                               We next consider a simple causal model under a so called
                                                                                               Noisy-Or parameterization (Cheng, 1997), in which each
                                                                                               cause has independent causal power to bring about the effect.
                                                                                               Suppose we have binary causal variables X, taking on values
               0.15                                                                            0 or 1, and conditional probabilities given in terms of weights
         KL
                                                                                               θY,X codifying the influence of parent Y on a variable X—in
                                                                                               particular θY,X gives the probability of Y causing X when Y is
                                                                                               active—and a “background bias” parameter β:
                                                                                                                                                        
                                                                                                                    = 1 − (1 − β) ∏ (1 − θY,X )Y
                                                                                                                
                      0
                           0.1   0.2   0.3    0.4      0.5       0.6   0.7   0.8   0.9
                                                                                                  P X | pa(X)
                                             second eigenvalue                                                                        Y ∈pa(X)
Figure 2: KL-distance for an approximate model with only                                       This model has the convenient property that deleting nodes
one state variable, as a function of second eigenvalue λ2 .                                    from the graph still leaves us with a well-defined distribution.
                                                                                               Hence the family of submodels is immediate from the full
                                                                                               model (without having to introduce a proxy uniform distribu-
   If we now factor in the cost of including more nodes in the                                 tion as in the previous example).
approximate HMM, we can determine, for different values of                                        Suppose in particular we have variables A, B,C, D with
λ2 , and for different assumptions about cost of a node, what                                  weights θ1 , θ2 , and θ3 , as depicted on the left in Figure 4.
the optimal number of nodes to include will be, in line with
Equation (1) above. To give one (arbitrary, but illustrative)
example, let us assume the cost of an additional node to be                                                        C       A                C
                                                           1
0.02, i.e., that this cost is equivalent to the utility of 50 more                                            θ1       θ2 θ3           θ1       θ2
bits of information. For relatively low values of λ2 , it is not
                                                                                                             B         D              B          D
                      4
                                                                             3     3                  Figure 4: Full Model versus Partial Submodel
                                                                                               Imagine, for instance, a simple scenario in which these vari-
                                              2         2        2     2
          nodes
                      2                                                                        ables correspond to:
         (optimal)
                            1    1     1
                                                                                                 A: “Mary has lost her usual gloves”
                                                                                                 B: “Mary has her bicycle with her”
                      0
                                                                                                 C: “Mary is going cycling”
                          0.1    0.2   0.3   0.4       0.5       0.6   0.7   0.8   0.9
                                             second eigenvalue                                   D: “Mary is wearing cycling gloves”
Figure 3: Optimal number of nodes, given a cost of 0.02 per                                    Observing that Mary is wearing cycling gloves makes it more
node, as a function of second eigenvalue λ2 .                                                  likely that she is going cycling, and therefore that she has
                                                                                               her bike with her. But this is attenuated by the alternative
                                                                                         964

possible cause, that she lost her other gloves. Our question in                     P(C)    P(A)     θ1    θ2     θ3     ∆P,P∗
this case is, how much worse will a reasoner fare by ignoring                        0.5     0.5     1      1     1     −0.097
alternative cause A (that Mary has lost her gloves), that is, by                     0.5     0.5     1     0.5   0.9    −0.074
using the smaller submodel in Figure 4, on the right?                                0.5     0.5     1     0.5   0.5    −0.087
   To assess this question, we can look at both the difference                       0.5     0.5     1     0.9   0.5    −0.096
in expected utility and the KL-divergence between using the                          0.3     0.1     1      1     1     −0.073
“true” distribution P and the approximate distribution P∗ , for                      0.5     0.5    0.9    0.1   0.5    −0.007
B given D = 1. Table 1 below presents example KL-values for                          0.3     0.1    0.9    0.1   0.5     0.011
different settings of model parameters: the priors on C and A,                       0.5     0.5    0.1    0.1   0.1     0.006
and the three weights θ1 , θ2 , and θ3 (here and throughout this
subsection, we set β = 0.05).                                                      Table 2: Example EU-values, with β = 0.05.
         P(C)     P(A)    θ1      θ2    θ3     KL(P || P∗ )
          0.5      0.5     1      1      1         0.594                 cess probability. In fact, as shown in Table 1, for many pa-
          0.5      0.5     1     0.5    0.9        0.467                 rameter settings the agent actually fares better by using the
          0.5      0.5     1     0.5    0.5        0.270                 simpler model (∆P,P∗ < 0). Take the first parameter setting,
          0.5      0.5     1     0.9    0.5        0.261                 for example. In this case P(B = 1 | D = 1) ≈ 0.673, whereas
          0.3      0.1     1      1      1         0.121                 P∗ (B = 1 | D = 1) ≈ 0.955. The submodel drastically overes-
          0.5      0.5    0.9    0.1    0.5        0.081                 timates the probability of B by ignoring the alternative cause,
          0.3      0.1    0.9    0.1    0.5        0.023                 as reflected in the very high KL-value in Table 1. However,
          0.5      0.5    0.1    0.1    0.1        0.000                 insofar as the true probability is significantly above 0.5, if
                                                                         the subject is going to make a decision by drawing a single
          Table 1: Example KL-values, with β = 0.05.                     sample from this distribution, such overestimation turns out
                                                                         to be advantageous, since the subject is more likely to choose
   Table 1 in fact shows settings near the higher end. We can            the more probable outcome. Such advantages would only be
also calculate the approximate average KL-divergence, over               compounded by the reduction in computational cost resulting
values of P(C) and P(A) at 0.1 intervals (0.1, 0.2, etc.), and           from the simpler model.
0.01 intervals for θ1 , θ2 , θ3 (thus, over 7 million parameter             We can tentatively conclude from this small case-study
settings): for this model, it is 0.060. Averaging over param-            that alternative neglect—even when it results in less accu-
eters with P(A) and P(C) fixed at 0.5 gives a similar average            rate judgments, which it certainly does—can still be a very
KL-divergence of 0.059. Thus, the KL-value is typically well             reasonable, indeed resource-rational, strategy.
under one-tenth of a bit of information lost. Nonetheless, if
                                                                               Predictive versus Diagnostic Reasoning
confidence in estimation is important, or if very fine-grained
decisions are called for, using the submodel may be detrimen-            Resource-rational analysis of submodel choice and alterna-
tal in this case.                                                        tive neglect predicts these phenomena to occur, at least to
   However, following question 1 from above, we may also                 a first approximation, when they would result in an optimal
consider how detrimental using a submodel will be for action             balance of outcome expected utility and computation cost, as
choice in specific decision problems. For the EU calculation,            outlined above. To what degree is this prediction born out by
suppose our agent is making a guess based on a single sample             empirical data on alternative neglect?
from either distribution P or P∗ ,2 and that utility 1 is achieved          One of the more robust recent findings in the causal reason-
for a correct response, 0 for incorrect. Example calculations            ing literature is that subjects tend to neglect alternatives to a
are summarized in Table 2. As with KL, we can also com-                  much greater extent in predictive reasoning than in diagnos-
pute the (approximate) average difference in EU, which for               tic reasoning (Fernbach et al., 2011; Fernbach and Rehder,
this model is 0.024. That is, on average over all parameter              2013). Most of the experiments in this work evince three
settings, a sampling agent will only suffer about 50   1
                                                          of a utile     variables A, B,C as in Figure 5. The left diagram depicts a
by using the simpler submodel. The cost of including the ad-
ditional variable A would therefore need to be extremely low,                           C       A                   C       A
relative to utility in the given decision problem, to merit its                       θ1                          θ1
                                                                                             θ2                          θ2
presence in the model.
   Evidently, when making a binary, sample-based decision,                               B                          B
using the smaller submodel does not greatly reduce one’s suc-
    2 This assumption is more apt in more complicated models,                    Figure 5: Predictive versus Diagnostic Inference
where computing exact estimates would be harder. We consider this
kind of rule simply for illustration, and contrast with information
distance, which would be more closely aligned with an agent (non-        predictive inference, where effect B is queried given evidence
noisily) maximizing expected utility across decision problems.           that cause C is active. On the right is a diagnostic inference,
                                                                     965

where the causal variable C is queried given evidence B. In                      P(C)     θ1     θ2      ∆ pred     ∆diag
this simple model, we can fold P(A) and θB,A into a single pa-                    0.1     0.3    0.9    1.083       1.345
rameter θ2 , so that A effectively has prior probability 1, and                   0.5     0.9    0.9    0.176      −0.041
the resulting conditional probabilities can be simplified to:                     0.5     0.5    0.1    0.010      −0.144
                                                                                  0.2     0.3    0.2    −0.034      0.345
                  P(B | C) = θ1 + θ2 − θ1 θ2                                      0.1     0.3    0.1    −0.036      0.550
                                                          
  P(C | B) = 1 − 1 − P(C) θ2 / P(C)θ1 + θ2 − P(C)
                                                                     Table 3: Differences in expected utility between the true and
The finding in Fernbach et al. (2011) is that subjects routinely     approximate distributions, for predictive and diagnostic infer-
ignore variable A in predictive inference tasks, and thereby         ences (with β = 0.05 for diagnostic cases).
consistently make low estimates of P(B | C). In diagnostic in-
ference tasks, however, subjects show sensitivity to strength
and number of alternatives, consequently making more accu-
                                                                     Indeed, the average ∆ pred value, over 0.01 intervals for all pa-
rate judgments. Indeed, there is a longer reaction time for
                                                                     rameters, is 0.198—relatively large, and significantly greater
diagnostic than for predictive inferences, and only in the di-
                                                                     than ∆diag , whose average is 0.044. Again, this means that a
agnostic condition is there dependency of reaction time on
                                                                     subject drawing a single sample will on average lose about 15
number of alternatives (Fernbach and Darlow, 2010). Fern-
                                                                     of a utile for predictive inferences on this graph, versus only
bach and Rehder (2013) verified that this asymmetry between                  1
                                                                     about 25  for diagnostic inferences.
diagnostic and predictive reasoning is robust; in particular, it
seems not to be due to availability or memory limitations.              This is in sharp contrast to an agent that decides based
   In other words, subjects seem to be reasoning with a sub-         on exact inference (or equivalently, based on many samples).
model (ignoring variable A) in the predictive case, but not          Based on the earlier results for KL-distance, we can compare
in the diagnostic case. How detrimental would it be to ne-           action selection directly for agents that accurately maximize
glect A for these two types of inference? Consider first KL-         expected utility from the full- or sub-model. It turns out that
divergence (question 2). Without a background bias term (as          for such agents the average difference in expected utility over
in the previous example), for the diagnostic case ignoring           all parameter settings is indeed significantly greater in the di-
variable A will lead to the conclusion that C has probability        agnostic case (0.378, versus 0.174 in the predictive case). If
1, since it is the only possible cause. In that case, the KL-        resource rationality is the correct explanation of these cases
divergence is infinite. With a positive bias term β, we can          of alternative neglect, we would then have to conclude that
make the KL-divergence finite, but it will still be large if the     participants are not using a single sample but rather many
bias is small. For instance, with 1% chance of B happening           (or more generally are using some algorithm for computing
spontaneously (β = 0.01), the average value of KL(P || P∗ )          closer approximations to the exact probabilities). Indeed, the
for the diagnostic inference is already 1.740, extremely high.       models in Fig. 5 are rather simple and one might expect that
With β = 0.05, it is 0.916.                                          inference for these models is relatively easy. At any rate,
   By contrast, the average value of KL(P || P∗ ) in the predic-     positing that one can well-approximate the true probability
tive case (even without a bias term, which would further de-         for a given model, if one were to ignore causal variables rou-
crease the average KL value) is only 0.357. This is a general        tinely in one type of case (diagnostic or predictive) but not
observation about this particular small causal graph, which is       the other, it would be most rational to do so in the predictive
the one implicated in many studies of “elemental causal in-          case, as subjects in fact do.
duction.” While it may be difficult to assess these KL-values           We might therefore tentatively conclude that, at a certain
absolutely, we can confirm that there is a substantial differ-       level of grain, subjects are ignoring variables in a reasonable
ence between the two types of inference. Indeed, on average          way. However, this does not yet say anything about resource-
one can expect to make much worse predictions in the di-             rationality at the level of individual inferences. In fact, Fern-
agnostic case than in the predictive case; an agent balancing        bach and Rehder (2013) have shown that subjects exhibit ne-
computation cost with accuracy would then be expected to             glect even in cases where the mistake is rather serious, result-
neglect the alternative more in predictive reasoning than in         ing in egregiously wrong predictions. One possible expla-
diagnostic reasoning.                                                nation is that subjects are optimizing grain of representation
   How does this look from the perspective of expected utility,      only at a high level of abstraction, in terms of general features
again assuming a single-sample-based agent? Table 3 shows            of the inference problem (e.g., whether it is predictive or di-
the differences in expected utility for several parameter set-       agnostic). This hypothesis merits further empirical investiga-
tings between the EU of using the true distribution and the          tion, as well as further theoretical consideration, for instance,
approximate distribution (ignoring variable A), for both the         by incorporating elements of metalevel control (Icard, 2014;
predictive and diagnostic cases. In some cases, ∆ pred is in-        Lieder et al., 2014) into the framework. The analysis offered
deed smaller than ∆diag , meaning that the agent suffers less        here, we believe, promises a useful starting point for under-
in expected utility when using the smaller submodel. How-            standing how rational submodels might be selected online,
ever, there are also cases where ∆ pred is greater than ∆diag .      and for addressing this question of level of grain.
                                                                 966

          Further Questions and Directions                            Meeting of the Cognitive Science Society, pages 1088–
The resource-rational submodel is that submodel of the true,          1093.
larger model which an agent ought to use for a given pur-          Fernbach, P. M., Darlow, A., and Sloman, S. A. (2011).
pose, balancing costs with accuracy. As we have seen, this            Asymmetries in predictive and diagnostic reasoning. Jour-
approach to the causal frame problem does already shed light          nal of Experimental Psychology: General, 140:168–185.
on a number of phenomena: it agrees with our pretheoretical        Fernbach, P. M. and Rehder, B. (2013). Cognitive shortcuts in
intuition that only a few previous time steps should matter in        causal inference. Argument and Computation, 4(1):64–88.
an HMM; it interacts in subtle ways with different assump-         Fischhoff, B., Slovic, P., and Lichtenstein, S. (1978). Fault
tions about choice rules, witness the single-sampling agent in        trees: Sensitivity of estimated failure probabilities to prob-
the second example; and it retrodicts general alternative ne-         lem representation. Journal of Experimental Psychology:
glect patterns observed in human causal reasoning.                    Human Perception and Performance, 4(2):330–344.
   The claim we would like to make is that, somehow or other,      Fodor, J. A. (1987). Modules, frames, fridgeons, sleeping
the mind is able to focus attention on the predicted submod-          dogs, and the music of the spheres. In Pylyshyn, Z. W.,
els for the task at hand. Our analysis, however, is from a            editor, The Robot’s Dilemma: The Frame Problem in Arti-
“god’s eye” point of view, and leaves open how is the mind            ficial Intelligence, pages 139–149. Ablex.
able to select the right submodel online, at inference time.
                                                                   Gigerenzer, G. and Goldstein, D. G. (1996). Reasoning the
There is relevant empirical work which might start to pro-
                                                                      fast and frugal way: Models of bounded rationality. Psy-
vide a process-level explanation of submodel selection. For
                                                                      chological Review, 103(4):650–699.
example, across many situations subjects seem to “add” a
new causal variable when faced with an apparent contradic-         Glymour, C. (1987). Android epistemology and the frame
tion (Park and Sloman, 2013). At the same time, work in AI            problem. In Pylyshyn, Z. W., editor, The Robot’s Dilemma:
suggests useful heuristics for similar resource optimization          The Frame Problem in Artificial Intelligence, pages 65–75.
problems (e.g., Wick and McCallum, 2011).                             Ablex.
   A number of extensions to our analysis suggest themselves.      Griffiths, T. L., Lieder, F., and Goodman, N. D. (2014). Ratio-
One could consider other types of submodels, e.g., not by             nal use of cognitive resources: Levels of analysis between
eliminating nodes, but by cutting links. One could also con-          the computational and the algorithmic. Topics in Cognitive
sider more seriously the interaction of submodel choice with          Science. forthcoming.
inference algorithm choice. As hinted in the previous sec-         Icard, T. F. (2014). Toward boundedly rational analysis. In
tion, it would be useful to consider what the optimal combi-          Bello, P., Guarini, M., McShane, M., and Scassellati, B.,
nation of submodel together with number of samples will be            editors, Proceedings of the 36th Annual Meeting of the
for a given model (thus combining the analysis here with that         Cognitive Science Society, pages 637–642.
in Vul et al. 2014). Assuming smaller submodels will allow         Klayman, J. and Ha, Y.-W. (1987). Confirmation, disconfir-
for more samples per unit of time (or energy), and given that         mation, and information in hypothesis testing. Psycholog-
more samples lead to more accurate predictions with respect           ical Review, 94(2):211–228.
to that model, there is a substantive question of what combi-      Lieder, F., Plunkett, D., Hamrick, J. B., Russell, S. J., Hay,
nation is optimal in any case.                                        N. J., and Griffiths, T. L. (2014). Algorithm selection by
   While many questions remain open, we hope to have made             rational metareasoning as a model of human strategy selec-
some progress toward illuminating a general solution to the           tion. In Advances in Neural Processing Systems.
causal frame problem. We believe the question is central to
                                                                   Park, J. and Sloman, S. (2013). Mechanistic beliefs deter-
our understanding of human reasoning. As Fodor remarked,
                                                                      mine adherence to the Markov property in causal reason-
“The frame problem goes very deep; it goes as deep as the
                                                                      ing. Cognitive Psychology, 67:186–216.
analysis of rationality” (Fodor, 1987).
                                                                   Simon, H. A. (1957). Models of Man. Wiley.
Acknowledgements                                                   Vul, E., Goodman, N. D., Griffiths, T. L., and Tenenbaum,
Thanks to Andreas Stuhlmüller and Long Ouyang for com-               J. B. (2014). One and done? Optimal decisions from very
ments on a draft of this paper.                                       few samples. Cognitive Science, 38(4):699–637.
                                                                   Wick, M. L. and McCallum, A. (2011). Query-Aware
                         References                                   MCMC. In 25th Conference on Neural Information Pro-
Bernardo, J. M. and Smith, A. M. (1994). Bayesian Theory.             cessing Systems, pages 2564–2572.
   John Wiley and Sons.
Cheng, P. W. (1997). From covariation to causation: A causal
   power theory. Psychological Review, 104:367–405.
Fernbach, P. M. and Darlow, A. (2010). Causal conditional
   reasoning and conditional likelihood. In Ohlsson, S. and
   Catrambone, R., editors, Proceedings of the 32nd Annual
                                                               967

