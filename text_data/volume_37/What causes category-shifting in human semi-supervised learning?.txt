               What causes category-shifting in human semi-supervised learning?
                  Bryan R. Gibson (bgibson@wisc.edu)                        Timothy T. Rogers (ttrogers@wisc.edu)
                       Department of Computer Sciences                                 Department of Psychology
                               1210 W. Dayton St.                                       1202 W. Johnson Street
                           Madison, WI 53706 USA                                        Madison, WI 53706 USA
                 Chuck W. Kalish (cwkalish@wisc.edu)                          Xiaojin Zhu (jerryzhu@cs.wisc.edu)
                    Department of Educational Psychology                           Department of Computer Sciences
                            1025 W. Johnson Street                                        1210 W. Dayton St.
                           Madison, WI 53706 USA                                        Madison, WI 53706 USA
                              Abstract                                    along a single dimension. Following a short supervised learn-
   In a categorization task involving both labeled and unlabeled
                                                                          ing experience with a single item from each category, partic-
   data, it has been shown that humans make use of the underlying         ipants acquired a category boundary approximately midway
   distribution of the unlabeled examples. It has also been shown         between the two labeled items. Subsequently they classified
   that humans are sensitive to shifts in this distribution, and will     a large number of additional items sampled from a bimodal
   change predicted classifications based on these shifts. It is not
   immediately obvious what causes these shifts – what specific           distribution along the single stimulus dimension, without re-
   properties of these distributions humans are sensitive to. As-         ceiving any feedback. This unlabeled distribution was se-
   suming a parametric model of human categorization learning,            lected so that the trough between the two modes lay some
   we can ask which parameters or sets of parameters humans fix
   after exposure to labeled data and which are adjustable to fit         distance from the learned boundary between classes. After
   subsequent unlabeled data. We formulate models to describe             exposure to this distribution, participants had shifted their be-
   different parameter sets which humans may be sensitive to and          liefs about the location of the category boundary, aligning it
   a dataset which optimally discriminates among these models.
   Experimental results indicate that humans are sensitive to all         with the trough in the unlabeled distribution – a behavior pre-
   parameters, with the closest model fit being an unconstrained          dicted by a simple parametric SSL model.
   version of semi-supervised learning using expectation maxi-
   mization.                                                                 Subsequent work has shown that such category shifts –
   Keywords: Categorization; Semi-Supervised Learning; Cog-               changes to beliefs about category structure arising from un-
   nitive Modeling                                                        labeled learning experiences – can be quite dramatic. For in-
                                                                          stance in one study, a majority of participants ended up mis-
                          Introduction                                    classifying the very item that had been directly taught during
The ability of human beings to learn and generalize category              the initial supervised learning phase, after exposure to a dra-
structure has been of perennial interest to cognitive science.            matically shifted unlabeled distribution (C. Kalish, Kim, &
This ability has often been studied using supervised learning             Young, 2012). Other work has shown that the temporal or-
experiences, where the learner is provided only with labeled              dering of unlabeled items can also change the acquired cate-
examples – that is, with correct information about category               gory structure (Zhu et al., 2010); that young children are more
membership in each learning trial.                                        susceptible to influences from unlabeled data (C. W. Kalish,
   Real-world category learning is somewhat different: while              Zhu, & Rogers, 2014); that exposure to unlabeled distribu-
we may learn an item’s category membership directly on oc-                tions can lead to acquisition of quite counter-intuitive cat-
casion, in most experiences we simply observe objects in the              egory structures (Gibson, Zhu, Rogers, Kalish, & Harri-
environment and make implicit inferences about their cate-                son, 2010); and that, despite receiving no feedback, people
gory membership. That is, most of our worldly experience                  will revise a (completely accurate) classification rule learned
is unlabeled. The joint use of labeled and unlabeled data is              on the fully labeled data after exposure to unlabeled exam-
sometimes called semi-supervised learning (SSL), and is a                 ples (C. W. Kalish, Rogers, Lang, & Zhu, 2011; Lake & Mc-
topic of considerable interest in machine learning where a                Clelland, 2011).
range of different approaches have been developed for var-                   Note that, while SSL has been observed in many differ-
ious learning problems (Zhu & Goldberg, 2009). A key in-                  ing scenarios, there have been instances where the addition
sight from this work has been that combined use of labeled                of unlabeled information has not impacted behavior (Vandist,
and unlabeled examples can produce quite different category               De Schryver, & Rosseel, 2009; McDonnell, Jew, & Gureckis,
structures, and in many cases more accurate structures, than              2012). Clearly more work is necessary to fully understand
learning from the labeled items alone.                                    how humans make use of combinations of labeled and unla-
   The last few years have provided substantial evidence that             beled data during category learning.
human category learning in the lab can be strongly influenced                In this paper we consider the causes behind the category-
by the distribution of unlabeled examples. In a seminal study,            shifts observed in semi-supervised learning studies of the
Zhu, Rogers, Qian, and Kalish (2007) had participants clas-               kind initially described by Zhu et al. (2007) – that is, in stud-
sify a set of novel, visually complex objects lying varying               ies where initial category structures are learned from fully
                                                                      794

supervised experience, then those structures are observed to           following exposure to unlabeled examples, as documented in
change after exposure to unlabeled examples. We consider               prior work. The effort is nontrivial, insofar as it requires us
two general hypotheses.                                                to design a SSL study under which the different heuristic hy-
   Under the first, the shifts happen because, during the initial      potheses and the SSL hypothesis all make different predic-
supervised phase, participants notice and track one or more            tions about how initial category structures should change fol-
parameters of the distribution from which the labeled items            lowing unsupervised learning. To achieve this goal, we first
are sampled, then seek to maintain a category structure that           formalize the nature of the learning task and describe a se-
preserves the noticed parameter. For instance, in Zhu et al.’s         ries of computational SSL models, each representing one of
(2007) study, the supervised phase involved learning about             the hypotheses under consideration. Using simulations with
just two examples (one from each category), each presented             the different models, we next discern a particular combina-
10 times with the order randomized. This experience poten-             tion of supervised and unsupervised learning experiences that
tially provides the learner with important information about           are expected to produce quite different learning outcomes un-
the two classes that she may then seek to preserve when ex-            der the different hypotheses. Finally, we report the results
posed to the unlabeled distribution. The learner may notice            of behavioral studies with human subjects exposed to these
that members of each category occur about equally frequently           learning experiences, and consider how their behaviors align
during the supervised phase, for example. In the unsuper-              with predictions of the different learning models. The results
vised phase, she may then select a category boundary that              of these studies allow us to clearly determine what is causing
divides the unlabeled items approximately in half, preserving          category-shifts in human SSL.
this frequency information. Alternatively, the learner might
notice that the two categories both have approximately equal               Cognitive Models and Experimental Design
variance, and so might learn category structures that preserve         To address the question posed above we formulate a set of
roughly equal variation between members of the category.               models and then attempt to determine which model or models
   Since the unlabeled distribution in the original study was          best fit human behavior on a classification task.
bimodal, symmetrical about the trough with peaks of equal                 The task we will be using for investigation is a 1D bi-
width, either of these strategies would lead the learner to shift      nary classification task (feature values x ∈ [0, 1] with la-
the boundary to this trough. Indeed, there are many elements           bels y ∈ {0, 1}). We make the strong, yet common, as-
of the unsupervised and supervised distributions that differed         sumption that humans are making use of a Gaussian Mix-
in this study, any one of which might account for the observed         ture Model (GMM). Formally, we define the parameters of
changes in categorization behavior.                                    a two-component GMM as θ = {w0 , µ0 , σ20 , µ1 , σ21 }, and let
   The first hypothesis, then, is that learners are trying to pre-     Θ = {θ}, the set of all parametrization of this model. The
serve specific parameters of the item and label distribution           learner is presented first with a set of labeled items: L =
learned during the initial supervised phase. We refer to this          {(xi , yi )}, i = 1 . . . nL , drawn from a 2-component GMM de-
as the heuristic hypothesis, since there is no principled reason       fined by θL , followed by a set of unlabeled items U = {(x j )},
for choosing to preserve a particular parameter from the la-            j = nL + 1 . . . nL + nU drawn from another GMM with differ-
beled distribution. Moreover, note that there are several pos-         ent parameters θU .
sible variants of the heuristic hypothesis: participants may try          We assume that, when training on L, humans find the
to preserve the relative frequencies of the two categories, their      maximum likelihood estimate (MLE) denoted θ̂SL ∈ Θ. The
variances, their distance from the boundary, and so on.                learner is then presented with a new set of unlabeled data
   The second hypothesis is that human beings are true semi-           U which may be drawn from a different distribution than L.
supervised learners – that is, they learn the category struc-          Learning from U amounts to performing a search in Θ for a
tures most likely to have generated all of the observations,           set of parameters that best fit the observed stimuli. Under the
labeled and unlabeled, subject to particular implicit assump-          heuristic hypotheses, humans search some subspace of Θ for
tions about the relation between labeled and unlabeled exam-           the new optimum, while under the SSL hypothesis, humans
ples. In the semi-supervised mixture model described by Zhu            search in the whole of Θ.
et al. (2007), the assumptions are that (i) items are sampled             We also assume the learner uses some form of expectation-
from a distribution in the feature space that is a mixture of          maximization (EM) as the search procedure to find this op-
Gaussian components and (ii) items sampled from the same               timum, the MLE on U, with θ̂SL as the starting point for
component of the mixture receive the same category label.              the search (Dempster, Laird, & Rubin, 1977; Bishop, 2007).
With these assumptions, it is possible to estimate, from all           Note that, as an optimization procedure, EM can be applied
labeled and unlabeled items, the most likely components of             even when labeled and unlabeled items come from different
the mixture (and their parameters) and the most likely labels          distributions. Although unusual in machine learning, EM
associated with each component. We refer to this as the SSL            used on non-iid data is plausible as a mechanism for how
hypothesis.                                                            humans adapt. Under this assumption, participants are not
   The remainder of this paper attempts to adjudicate which            focused on matching or maintaining particular aspects of the
of these hypotheses best explains category-shifts that occur           labeled distribution, but are trying to find a parametric model
                                                                   795

that jointly “explains” the labeled and unlabeled distributions.                  All remaining models correspond to our heuristic models.
   For example, humans might be only willing to change the                     They are all similar to θ̂SSL , but assume that learning is be-
proportion of one class to another(ŵ0 ) leaving the rest of the              ing done by fixing one of the GMM parameters to the values
learned parameters µ̂0 , µ̂1 , σ̂20 , σ̂21 fixed as they were in θ̂SL .        learned on L while allowing all others to vary:
Or, they may update both ŵ0 and the peaks of the learned dis-
tribution (µ̂0 , µ̂1 ), but                                                    constrained means (θ̂µ ) : Means µˆ0 and µˆ1 are fixed at the
                           remain insensitive to changes in spread,
or variance σ̂20 , σ̂21 , of the data. This behavior might be in-                 initialization values learned on L using (1). It is as though
terpreted as the human learner “hanging on” to some beliefs                       two prototypes are formed at the modes of the labeled dis-
learned on L.                                                                     tribution and retained when exposed to U. At each EM iter-
                                                                                  ation t, the values of µ̂ at t − 1 are simply copied forward.
Formalized Cognitive Models                                                       The variances σˆ0 2 , σˆ1 2 , weight wˆ0 and responsibilities γi
With this task in mind we describe the cognitive models under                     are updated using (5), (6) and (7) respectively.
consideration as models of human behavior.
                                                                               fixed standard deviations (θ̂σ ) : The standard deviations σˆ0
unconstrained SL (θ̂SL ) : This model is a purely supervised                      and σˆ1 are fixed at the initialization values learned on L
   learner defined by the parameters θ̂SL . This model esti-                      using (2). Here, it is the spread of the labeled data that
   mates the GMM parameters using the MLE over the la-                            is considered important, and is maintained. Again at each
   beled set L alone and holds them fixed over the unlabeled                      EM iteration the values of σˆ0 and σˆ1 are simply copied
   test data, in effect ignoring the unlabeled data. It is in-                    forward. Updates for means, weight and responsibilities
   cluded as comparison, as we know humans are affected by                        are the same as in (4), (6) and (7).
   U. Updates are made using
                                                                               fixed ratio of standard deviations (θ̂r ) : At initialization,
                                     nL                                           the ratio of standard deviations learned on L using (2) is
                               1
                  µ̂0     =         ∑ 1 {yi = 0} xi
                               n0 i=1
                                                                       (1)        calculated as r = σ̂0 /σ̂1 . Again, the spread is considered
                                                                                  most important, but now the spread of each class is allowed
                                                                                  to vary only so long as the ratio between the two is main-
                               1 nL                                               tained. As the parameters σ̂0 and σ̂1 are now tied, the pa-
                 σ̂20     =         ∑ 1 {yi = 0} (xi − µ̂0 )2
                               n0 i=1
                                                                       (2)
                                                                                  rameter set becomes θ̂r = {w0 , µ0 , µ1 , σ}. Reformulating
                               n0                                                 the optimization function and solving for σ we find the new
                 ŵ0      =                                            (3)        update equations
                               nL
                                                                                                   "                                                           #
                                                                                                         n
   with n0 = ∑ni=1       1 {yi = 0} (µ̂1 , σ̂1 are defined similarly).                          1                                                            
                    L                                                                   2                                       2      2                    2
                                                                                     σ̂   =             ∑          γi (xi − µ0 ) + r (1 − γi )(xi − µ1 ) (8)
                                                                                               nU    i=nL +1
unconstrained SSL (θ̂SSL ) : We define the SSL model, de-                                                           w0 N (xi ; µ̂0 , σ̂2 )
   fined by the parameters θ̂SSL , before the heuristic models                        γi  =                         2
                                                                                                                                                          .     (9)
                                                                                               w0 N   (xi ; µ̂0 , σ̂ ) + (1 − w0 )N (xi ; µ̂1 , (σ̂/r)2 )
   as all other models are derived from this unconstrained ver-
   sion. Consideration must be given as to whether to perform                     Updates for means and weight are the same as in (4)
   EM on the full data set (L + U) or to use θ̂SL , the MLE                       and (6).
   on L, as initialization and perform EM on U alone. We
   choose the latter as it more closely approximates the situa-                constrained weight (θ̂w ) : The weight parameter ŵ0 is fixed
   tion faced by human learners in the task: initially exposed                    at the initialization value learned on L. In this case it is the
   to L but with no additional feedback as they classify U. For                   frequency of each class which is considered most important
   each M-step of EM, the MLE estimates become                                    to retain from the labeled data. All other updates remain
                                                                                  unchanged.
                                     ∑ni=nL +1 γi xi
                        µˆ0   =                                        (4)        The above models each fix one property. We also consider
                                      ∑ni=nL +1 γi
                                                                               cognitive models which constrain multiple parameters. For
                                     ∑ni=nL +1 γi (xi − µˆ0 )2                 example, the model θ̂σ,w has only two parameters which are
                      σˆ0 2   =                                        (5)
                                           ∑ni=nL +1 γi                        free to vary: {µ̂0 , µ̂1 }, with ŵ0 , σ̂20 and σ̂21 fixed. This results
                                      1     n                                  in 5 additional models: {θ̂σ,w , θ̂r,w , θ̂µ,w/σ , θ̂µ,w/r , θ̂µ,σ , θ̂µ,r }.
                       wˆ0    =            ∑      γi                   (6)     The model θ̂µ,w/σ is constrained in means and weight while
                                     nU  i=nL +1
                                                                               standard deviations are allowed to vary. The model θ̂µ,w/r
   n = nL + nU , responsibilities γi calculated at each E-step as              is constrained in means and weight while ratio of standard
                                                                               deviations is allowed to vary.
                               ŵ0 N (xi ; µ̂0 , σ̂20 )                           The final cognitive model we examine (propL) is not prob-
        γi =                                                           (7)
             ŵ0 N (xi ; µ̂0 , σ̂20 ) + (1 − ŵ0 )N (xi ; µ̂1 , σ̂21 )         abilistic. In this model, the learner simply calculates the pro-
                                                                               portion of negative to positive items seen in L. When the
   and µ̂1 and σ̂1 calculated similarly using (1 − γi ).                       learner is then presented with U, they attempt to place a
                                                                           796

boundary in feature space such that this proportion of neg-
ative to positive items is preserved. If the distribution gen-
erating unlabeled items is different from that generating the
labeled items, the boundary learned on the labeled data will
not necessarily be the same one applied to the unlabeled data.
This model, θ̂ propL has only a single parameter n0 /nL , with               Figure 1: Stimuli at x = 0, 0.25, 0.75 and 1 respectively.
the boundary b̂ induced from this ratio:
                             j   n0
               b̂ = x( j) :    = ,          j ∈ [1, nU ]         (10)     Participants and Procedure
                            nU   nL
where b ∈ [0, 1] and {x(1) , x(2) , ..., x(nU ) } are the items in U,     Using this chosen dataset, we performed a human experi-
sorted by feature value. Note that this model is related to               ment where 49 undergraduate students, participating for par-
the cognitive models which preserve the GMM weight w0 .                   tial course credit, were asked to learn a timed classification
However, since this is not a GMM and classification is simply             task. The 1D stimuli used were Gabor patch images vary-
performed by a step function placed at the learned boundary               ing in only the frequency dimension, with fixed rotation (Fig-
b, the resulting behavior may be different.                               ure 1). Each participant was asked to classify the nL = 50
   With these cognitive models in hand we now discuss how                 labeled images, each classification followed by feedback in-
to compare their performance to human behavioral data in                  dicating whether they were correct or incorrect. The partici-
order to assess which may be a better match.                              pant was then asked to classify the nU = 300 unlabeled stim-
                                                                          uli, with no feedback given. All participants classified the
Human Experiment Design                                                   same set of stimuli, each a randomized ordering.
We design an experiment which attempts to discriminate
which of our proposed models is a best fit to human behavior              Evaluation Criteria
in a 1D classification task. An important aspect of this design           We call the measurement we use to evaluate our models
is the construction of the dataset.                                       “agreement”. This refers to how well a cognitive model’s
   A dataset must be found which will maximally discrimi-                 classification predictions agree with observed human behav-
nate predictions made by our various models, so that it is as             ior. Each participant k ∈ {1, . . . , K} is asked to classify the
clear as possible which model most strongly matches human                 set of labeled and unlabeled items in a randomized ordering
behavior. This step is similar in flavor to the machine teach-            (L,U)(k) . For each participant k we consider the first 50 +
ing task proposed in (Zhu, 2013). In that setting, a teacher                                                      (k)
                                                                          200 items as a training set (L,U)train and the remaining 100
attempts to design an optimal dataset to teach a (potentially                                   (k)
unknown) learner a target hypothesis. The difference here is              items as a test set Utest . Though there is certainly no reason
that we do not have a target we wish our learners to learn, but           to assume that humans will not continue learning on the test
instead would simply like our learners to differ as much as               set, we do make the assumption that after 200 unlabeled ex-
possible in their resulting predictions. The similarity is in the         amples, the learned boundary will have stabilized.
search over potential datasets.                                              Each of our proposed models m is then trained on
                                                                                 (k)
   To find a good dataset, first a labeled set L of nL = 50 la-           (L,U)train producing θ̂(m,k) . For the GMM models we use
beled pairs were drawn from θL = {w0 = 0.75, µ0 = 0.4, σ0 =               the constrained versions of EM described above while propL
0.12, µ1 = 0.8, σ1 = 0.06}. A heuristic search was then made              is calculated directly. We can then calculate the predicted
over a sparse grid of parameter settings θU , varying in all              boundary b̂(m,k) for each trained model on each dataset. For
parameters. At each setting a potential unlabeled set Ũ of               each of these model m nand dataset k opairs we can then make
nU = 300 was drawn. All cognitive models were then trained                predictions ŷ(m,k) = 1 xi ≤ b̂(m,k) , i = 201, . . . , 300 and
                                                                                                       (k)
on L and predictions made on that Ũ. We heuristically se-
                                                                          calculate:
lected the dataset L + Ũ with the aim to produce the largest
combined pairwise difference between predictions, and there-                                             1  ntest    n               o
                                                                                                            ∑1
                                                                                                                        (m,k)    (k)
fore largest discriminative power. Additionally, parameters                      agreement(m, k) =                    ŷi     = yi     (11)
                                                                                                      ntest i=1
which produced more than one decision boundary in the tar-
get range x ∈ [0, 1] were avoided.
   In the end the parameters selected from which U was                    and total mean-agreement for each model over all K partici-
drawn were θU = {w0 = 0.25, µ0 = 0.3, σ0 = 0.05, µ1 =                     pants:
0.6, σ1 = 0.1}. Plots of the chosen underlying distributions
are shown in Figure 2. Importantly note that the labeled and                                               1 K
                                                                                mean-agreement(m) =           ∑ agreement (m, k)       (12)
unlabeled distributions vary in all parameters. Figure 2 also                                              K k=1
shows the estimated distributions and boundaries resulting
from training each of the cognitive models on the selected                The mean agreement scores are then used to determine which
dataset.                                                                  model is the best fit over all.
                                                                      797

                                        0.77          0.63              0.62                  0.69                0.70              0.71
                             0    0.5     1    0   0.5    1   0     0.5     1     0     0.5        1   0   0.5      1    0   0.5      1
p(x|y,θ)
                                 SL                SSL              µ                   σ                   r                w
                                        0.79       0.39                 0.61                0.55                0.61                0.70
           0   0.5     1
                x
                             0    0.5     1    0   0.5    1   0     0.5     1     0     0.5        1   0   0.5      1    0   0.5      1
                                 σ, w              r, w           µ, w/σ              µ, w/r               µ, σ              µ, r
Figure 2: On the left, the ground truth labeled distributions (in blue and red) and unlabeled distribution (in black). On the right
the trained models and most central prediction boundary indicated by a dotted line. The boundary for propL falls at 0.65.
                                                                                                       Discussion
                                                                          The question we set out to answer was what causes the cat-
                                                                          egory shifts seen in many semi-supervised learning studies?
                                                                          The two hypotheses were 1) heuristic: that humans notice
                                                                          and track some properties or set of parameters of the distri-
                                                                          bution from which labeled items are sampled, and then seek
                                                                          to preserve these properties when integrating information de-
                                                                          rived from unlabeled items and 2) SSL: that humans are true
                                                                          semi-supervised learners, sensitive to all properties.
                                                                             In this particular categorization task, our results support the
                                                                          latter hypothesis: humans are sensitive to all parameters
                                                                          and do not constrain their search of the parameter space.
                                                                          They are sensitive to all changes in the unlabeled data distri-
                                                                          bution as they try to find the category structure most likely to
                                                                          have generated all observations, labeled and unlabeled.
                                                                             This result should be of interest to both the cognitive psy-
                                                                          chology and machine learning communities. From the cogni-
                                                                          tive psychology perspective we can compare these results to
Figure 3: Top: mean agreement scores calculated for each                  those regarding the distinction between generative and dis-
model. Bottom: number of participants for which each model                criminative learning (Hsu & Griffiths, 2010). Recall that
is the best match (highest agreement).                                    to perform categorization, a generative learner attempts to
                                                                          model the full generating distribution p(x, y) while the dis-
                                                                          criminative learner only attempts to learn a discriminating
                           Results                                        function p(y | x). Several studies have shown that humans
                                                                          are capable of both types of learning (Rips, 1989; Smith &
                                                                          Sloman, 1994; Hsu & Griffiths, 2010). In our task where the
Using the method described above, we found that the maxi-                 underlying generating distribution is important due to its non-
mum mean-agreement score is 0.7 for the completely uncon-                 iid nature, the generative learning model is preferred. Our re-
strained model θ̂SSL , simply standard SSL (Figure 3, top). A             sults argue that humans do in fact use a generative model for
repeated measures one-way ANOVA shows significant dif-                    this particular task, as the SSL model is a better fit than the
ference between model agreements per subject, F(12, 624) =                propL model, a discriminative model. It may be that in other
26.68, p = 2 × 10−16 . Additionally, the unconstrained SSL                tasks, where discrimination between hypothesized models, or
model, θ̂SSL , is a significantly better fit to human behavior            models not in the GMM family, is still possible, this result
than all other models (post-hoc multiple comparison test with             may not be the case. Additional investigation is required to
Holm correction, p ≤ 0.05), save one, SSL constrained by                  confirm that our conclusion generalizes to other situations.
ratio of standard deviations (θ̂r , p = 0.11).                               From the machine learning perspective this result matches
   If we look at which model has the best agreement per par-              the intuition that, for best performance on transfer learning,
ticipant, unconstrained SSL θ̂SSL is the clear winner, having             the learner should not be constrained a priori without specific
the highest agreement on 71% of participants (Figure 3, bot-              knowledge of the relation between the source domain and the
tom).                                                                     target domain. The learner should be allowed to explore the
                                                                   798

full parameters space when attempting to find the best fit ap-      Zhu, X., Gibson, B. R., Jun, K., Rogers, T. T., Harrison, J.,
proximation.                                                          & Kalish, C. (2010). Cognitive models of test-item effects
   Finally, though the evidence points to the unconstrained           in human category learning. In The 27th international con-
hypothesis dominating over all, no significant difference was         ference on machine learning (ICML).
found between it and the model constrained by ratio of stan-        Zhu, X., & Goldberg, A. B. (2009). Introduction to semi-
dard deviations. The difference here is subtle and additional         supervised learning. Morgan & Claypool.
work is necessary to distinguish whether this model is in fact      Zhu, X., Rogers, T. T., Qian, R., & Kalish, C. (2007). Hu-
a good approximation of human behavior or just an artifact of         mans perform semi-supervised classification too. In Pro-
the current study.                                                    ceedings of the 21st conference on artificial intelligence
                                                                      (AAAI).
                        References
Bishop, C. M. (2007). Pattern recognition and machine
  learning. Springer.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maxi-
  mum likelihood from incomplete data via the em algorithm.
  Journal of the Royal Statistical Society. Series B (Method-
  ological), 39(1), 1–38.
Gibson, B. R., Zhu, X., Rogers, T. T., Kalish, C. W., & Har-
  rison, J. (2010). Humans learn using manifolds, reluc-
  tantly. In Advances in neural information processing sys-
  tems (NIPS) (Vol. 24).
Hsu, A. S., & Griffiths, T. E. (2010). Effects of generative
  and discriminative learning on use of category variability.
  In 32nd annual conference of the cognitive science society.
Kalish, C., Kim, S., & Young, A. (2012). How young children
  learn from examples: Descriptive and inferential problems.
  Cognitive Science, 36, 1427–1448.
Kalish, C. W., Rogers, T. T., Lang, J., & Zhu, X. (2011). Can
  semi-supervised learning explain incorrect beliefs about
  categories? Cognition, 120(1), 106–118.
Kalish, C. W., Zhu, X., & Rogers, T. T. (2014). Drift in chil-
  dren’s categories: when experienced distributions conflict
  with prior learning. Developmental Science.
Lake, B. M., & McClelland, J. L. (2011). Estimating the
  strength of unlabeled information during semi-supervised
  learning. In Proceedings of the 33rd annual conference of
  the cognitive science society.
McDonnell, J. V., Jew, C. A., & Gureckis, T. M. (2012).
  Sparse category labels obstruct generalization of category
  membership. In Proceedings of the 34th annual conference
  of the cognitive science society.
Rips, L. J. (1989). Similarity, typicality, and categorization.
  In S. Vosniadou & A. Ortony (Eds.), Similarity and ana-
  logical reasoning (pp. 21–59). New York, NY: Cambridge
  University Press.
Smith, E., & Sloman, S. (1994). Similarity-versus rule-based
  categorization. Memory & Cognition, 22(4), 377–86.
Vandist, K., De Schryver, M., & Rosseel, Y. (2009). Semisu-
  pervised category learning: The impact of feedback in
  learning the information-integration task. Attention, Per-
  ception, & Psychophysics, 71(2), 328–341.
Zhu, X. (2013). Machine teaching for bayesian learners in
  the exponential family. In Advances in neural information
  processing systems (NIPS).
                                                                799

