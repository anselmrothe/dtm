      Teaching with Rewards and Punishments: Reinforcement or Communication?
                                                Mark K Ho (mark_ho@brown.edu)
                          Department of Cognitive, Linguistic, and Psychological Sciences, 190 Thayer St.
                                                       Providence, RI 02912 USA
                                         Michael L. Littman (mlittman@cs.brown.edu)
                                           Computer Science Department, 115 Waterman St.
                                                       Providence, RI 02912 USA
                                          Fiery Cushman (cushman@fas.harvard.edu)
                                 Department of Psychology, 1484 William James Hall, 33 Kirkland St.
                                                       Cambridge, MA 02138 USA
                                     Joseph L. Austerweil (joseph_austerweil@brown.edu)
                          Department of Cognitive, Linguistic, and Psychological Sciences, 190 Thayer St.
                                                       Providence, RI 02912 USA
                              Abstract                                that evaluative feedback will be treated as face value
   Teaching with evaluative feedback involves expectations
                                                                      rewards and punishments. Positive responses are
   about how a learner will interpret rewards and punishments.        pleasurable, rewarding outcomes of behavior to be
   We formalize two hypotheses of how a teacher implicitly            maximized, while negative responses are painful, punishing
   expects a learner to interpret feedback – a reward-maximizing      outcomes to be minimized. The learner is expected to
   model based on standard reinforcement learning and an              interpret feedback like any other valenced stimulus that
   action-feedback model based on research on communicative           results from acting on the environment, such as a ripe apple
   intent – and describe a virtual animal-training task that          having fallen from a shaken branch or a burnt finger having
   distinguishes the two. The results of two experiments in
   which people gave learners feedback for isolated actions           touched a hot stove. On this view, when Eve’s parents want
   (Exp. 1) or while learning over time (Exp. 2) support the          to teach her about using the toilet and not the living room,
   action-feedback model over the reward-maximizing model.            they intend the sticker to serve as an incentive. “Eve loves
                                                                      stickers,” they reason, “so she will want to use the toilet
   Keywords: pedagogy; reward; punishment; reinforcement
   learning; feedback; evaluative feedback; communication             again”.
                                                                         In contrast, peoples’ understanding of communicative
                          Introduction                                intent when learning new concepts from examples (Sperber
                                                                      & Wilson, 1986; Csibra & Gergely, 2009; Shafto et al.,
   Imagine Eve, a 4-year-old toddler, who uses the toilet for         2014) suggests an action-feedback model in which teachers
the first time. Her proud parents might give her a hug and            expect learners to treat responses communicatively or as
some stickers for the accomplishment. Or consider Fido, the           commentary about an action. Rewards signal to the learner
chocolate labrador puppy, who ignores the paved walkway               that the action just performed was correct given the
leading to the house and tramples over a freshly planted bed          circumstances, whereas punishments signal that the action
of flowers. Fido’s owner, who spent the last two months               was wrong or incorrect. Teachers further expect such a
tending to his lawn, scolds Fido harshly in a firm and                learner to be motivated to perform correct actions and avoid
imposing voice. In both cases, a teacher (Eve’s parents,              incorrect ones in a given state. From this perspective, when
Fido’s owner) attempts to modify another agent’s behavior             toilet training Eve, her parents intend the sticker to serve as
(a child or a dog) using valenced stimuli (stickers, scolding).       a signal that she is doing the right thing. “Eve knows we
This type of interaction – teaching via evaluative feedback –         don’t give stickers out for nothing,” they might reason, “so
occurs frequently between parents and their children (Owen            she’ll learn that she should be using the toilet.”
et al., 2012) as well as between humans and other species                In this paper, we first formalize these two hypotheses of
such as dogs.                                                         teaching via evaluative feedback in the framework of
   Here, we explore how teachers implicitly expect a learner          Markov Decision Processes. Teachers who teach according
to interpret rewards and punishments intended to modify               to a reward-maximizing model expect learners to treat
behavior. That is, we examine how teachers provide                    positive feedback as desirable rewards to be maximized and
evaluative feedback in response to the actions of a learner.          negative feedback as undesirable punishments to be
   Reinforcement learning models of human and animal                  minimized. In contrast, those who teach based on an action-
learning based on operant conditioning (Sutton & Barto,               feedback model expect learners to treat positive and
1998; Dayan & Niv, 2008) assume that learners are reward-             negative stimuli as signals for correct or incorrect behavior.
maximizing. Teachers might share this assumption, namely,             We then describe a novel teaching task that qualitatively
                                                                  920

distinguishes these two models. Finally, we present results                Learner Models
from two experiments that show the majority of people do                   Learning consists of changes in an agent’s policy over time,
not teach in accord with a reward-maximizing account, but                                          , resulting in a learned policy,       .
instead broadly follow the predictions of the action-
                                                                           When teaching by evaluative feedback, a teacher expects a
feedback model.                                                            learner to learn from the feedback signal produced by .
                                                                           Thus, each of our models characterizes the functional
                               Model                                       relationship between a learned policy,          , and feedback
We first describe an interaction model of the teacher-learner              function, .
dynamics during teaching with evaluative feedback. Second,                    The reward-maximizing agent treats teacher-feedback
we propose two learner models (reward-maximizing and                       from a feedback function as a face value reward to be
action-feedback) that capture a teacher’s expectation of how               maximized over the long term – exactly like the reward
the learner interprets feedback. Finally, we show how the                  signal found in standard reinforcement learning (RL)
models can be distinguished in a novel sequential-teaching                 (Sutton & Barto, 1998). That is, a reward-maximizing agent
paradigm.                                                                  calculates the cumulative long-term value of each available
                                                                           action in the current state , under the current policy .
Interaction Model                                                          Call this the action-value,             , from a state with a
To model the interaction between a teacher and a learner,                  policy:
we use the Markov Decision Process formalism (Bellman,
1957). On each timestep , the teacher observes a learner
interacting with an environment composed of states (                 )
by performing any one of the actions available in a state
(             ). Each action is generated from a learner’s
behavioral repertoire at a given time step, represented as a
policy, , where          is a mapping from states to available             Importantly, future rewards may be treated as less rewarding
                                                                           than immediate ones, so we include a discount parameter
actions (                     ).1
                                                                                        As its name suggests, the reward-maximizing
   After observing the agent’s current state, action, and
                                                                           agent is interested in eventually learning a policy,      , that
subsequent state (                ), the teacher responds to the
                                                                           maximizes the action-value in all states. Thus, such an agent
learner with a positive or negative feedback signal of a finite
                                                                           learns the policy:
magnitude (                  ). The function that takes an
observation of the learner and returns feedback we call the
feedback function:
                                                                           for all        . Note that the model is agnostic about the
                                                                           precise learning mechanism that updates           as long as it
The pattern of rewards and punishments that constitute this                converges on the reward-maximizing policy                . For
feedback function is determined by the target policy ( )                   instance,         could be learned via explicit planning or
that the teacher wants the learner to acquire. The interaction             trial-and-error learning – e.g. model-based learning vs.
then continues into the next timestep (Figure 1).                          model-free Q-learning (Dayan & Niv, 2008).
                                                                              The action-feedback agent treats feedback as a direct
                                                                           signal for the correctness or incorrrectness of an action. A
                                                                           positive teacher response indicates that the action matches
                                                                           the corresponding action in the target policy, while a
                                                                           negative response indicates it does not match. Thus, teacher
                                                                           responses map directly onto whether an action should or
                                                                           should not be done, and we can define the action-
                                                                           correctness,         , from the present state as:
 Figure 1: Teacher-learner interaction model. The learner’s
 current policy, , takes in the current state and returns
 an action , leading to the next state                . A feedback         Then for all        , the action-feedback agent will learn the
                                                                           policy:
 function, , observes this and gives feedback                 to the
 learner, resulting in the modified policy          .
   1
     For simplicity, we assume that state transitions and policies are
deterministic (e.g.                      ), however, this can be
generalized to stochastic environments and policies.
                                                                       921

 Figure 2: The task faced by Fido’s owner. Tiles enclosed by double lines are the garden; unenclosed tiles are the path. The
 owner wants to teach Fido . The two rows show two possible feedback functions                  and     (solid blue arrows are
 rewards, dotted red arrows are punishments) as well as the policies learned by the two models. A reward-maximizing
 learner will not learn the target policy under       because of the positive cycle (big grey arrows). Note that a feedback
 function may not yield a unique an action-feedback policy.
How do teachers teach?
When do the two models diverge? That is, when does
              for a feedback function ? Furthermore, when
does a reward-maximizing learner or an action-feedback
learner acquire the target policy, ?
   For the learner models, the reward-maximizing discount
parameter, , must be sufficiently large. Otherwise, the
learner’s estimate of an action’s correctness and its value
coincide                                 , and               .
This means the two can only diverge when the reward-
maximizing learner cares about future feedback.
   For feedback functions, the learned policies of the models
can diverge given positive cycles: state-action-feedback
sequences where the learner returns to an initial state,            Figure 3: Fido’s Garden-Path task. On each trial, a dog
(                              ), but receives a net positive       moves and then participants give their feedback. (Demo at:
reward,                                                             http://research.clps.brown.edu/mkho/gardenpath/task.html)
(Ng, Harada, & Russell, 1999).                                     presence of which would indicate that people expect action-
   For example, consider what happens if Fido is punished          feedback but not reward-maximizing learners. Dogs were
for going into the garden but rewarded for getting on the          chosen because people are unlikely to attribute sophisticated
path or heading towards the house. Suppose Fido heads              cognitive capacities to them (unlike with human children)
towards the house along the path, gains rewards, and stops         but are likely to be familiar with them (unlike robots).
at the door. At this point, Fido could enter the house and get     Experiment 1 investigated peoples’ teaching patterns for
a final, perhaps large, reward. But, if Fido is a reward-          isolated actions taken by a learner. Experiment 2
maximizing learner who values future rewards, he could             investigated how people teach a single learning agent over
double back through the garden, take the punishments,              time.
follow the path to the house again, and gain even more
rewards. If the tradeoff between punishments and rewards is             Experiment 1: Teaching Isolated Actions
a net gain, this is a positive cycle. Figure 2 illustrates the
predicament of Fido’s owner in a simplified gridworld.             In Experiment 1, participants provided feedback to learners
   We designed a dog-training paradigm, the Garden-Path            who performed isolated actions in the Garden-Path task,
task, reminiscent of the one faced by Fido’s owner (Figure         allowing us to map out their feedback functions over the
3) to determine whether people produce positive cycles, the        entire state-action space.
                                                               922

 Figure 4: Results of Experiment 1. (a) Average teaching function of all participants. (b) Net value of responses on cycle
 trials by individual. (c) Results of hierarchical clustering of participants’ responses with the average teaching function of the
 two largest clusters. These correspond to an action-feedback function and a “state-training” function (see text).
                                                                      whether a dog would prefer 2 scoldings followed by 4
Method                                                                praises or prefer receiving nothing at all.
Participants and materials Thirty-nine people from
MTurk participated. On each trial the dog starts at a tile,           Results
rotates to face one of the four cardinal directions, and then         Positive Cycles We first analyzed whether participants’
walks onto the adjacent tile (3000ms). After viewing the              stationary feedback functions had positive cycles that could
dog’s movement, participants provide feedback ranging                 be discovered by a reward-maximizing learner. Figure 4a
continuously from highly negative to highly positive: “a              graphs the average feedback function, where the response
mild but uncomfortable shock” to scolding the dog (“Bad               scale was coded as between -1 and +1. The aggregated
Dog”) to “doing nothing” to praising the dog (“Good dog!”)            pattern of feedback reveals that starting from the lower left-
to “a few delicious treats”. The instructions explicitly stated       hand corner and performing the action sequence <up, up,
that the scale should be seen as ‘balanced’ such that                 right, down, down, left> yields a net positive feedback. This
distances from the midpoint of the scale were equivalently            positive cycle had an average value of +1.20, SE=0.20 (t(38)
positive or negative.                                                 = 5.99, p < .001). Furthermore, individual-level responses
                                                                      had positive cycles. Figure 4b is a histogram of net cycle
Procedure We told participants that they would help train a           values and clearly demonstrates that 36 out of 39
school of 24 distinct dogs to “go into the house by staying           participants delivered a net positive reward along this route.
along the path and staying out of the garden” and that the
goal of training is for each dog to be able to do this                Feedback Function Types Previous work has shown that
independently. The entire task consisted of 24 trials that            people adopt different ‘training strategies’ when giving RL
covered all possible initial locations, actions, and final            agents rewards and punishments (Loftin et al., 2014). To
locations. Trial order was randomized under the constraint            identify individual differences in feedback functions, we
that no trial began where the previous trial had ended. We            performed a hierarchical clustering analysis. Individual
told participants to imagine they had placed the dog in that          feedback functions were represented as 22-dimensional
location at the beginning of the trial. They had to answer            vectors of responses between -1 and 1 (actions from the
several comprehension questions completely correctly to               terminal state were not included), and we calculated a
start the task.                                                       Euclidean-distance dissimilarity matrix. Clusters were
   To evaluate participants’ perceptions of whether the dogs          identified using a complete linkage method.
value future rewards, we asked several questions about dog               Results (Figure 4c) reveal two large, homogeneous
preferences after the task. For example, one question asked           clusters (n=17 each) and a single small, heterogenous
                                                                 923

 cluster (n=5). The first large cluster (left) closely matches                Experiment 2: Teaching Over Time
 the action-feedback model that rewards correct actions and
                                                                      Experiment 1 examined responses to isolated actions.
 punishes incorrect ones. The two subclusters in this cluster
                                                                      Experiment 2 had participants teach a single dog over time.
 reflect response magnitude differences. The second large
                                                                      This allows us to test whether positive cycles still arise and
 cluster (right), reveals a feedback pattern distinct from either
                                                                      whether teachers can properly track a learner’s policy.
 the reward-maximizing or action-feedback model.
 Participants gave rewarding responses based on the general           Method
 permissibility/impermissibility of state-types, even if they
 were not correct for the specific task being trained. For            Participants and materials The same interface was used.
 example, if the dog stayed on the path but walked away               Thirty-seven people trained a single dog over 8 game days.
 from the door, a “state training” teacher would still give a         Each day, the dog began in the lower left corner and
 reward. This leads to even worse positive cycles that could          movements on each day were predetermined. Apparent
 be exploited by a reward-maximizing agent who simply                 performance improved over the course of the first 5 days,
 walks back and forth along the path. Importantly, only 5 of          were optimal on the 6th and 7th days, and on the 8th day the
 the 17 state training participants did not mention ‘going to         dog proceeded on the positive cycle steps identified in
 the house’ in a pre-task free-response question, suggesting it       Experiment 1. Except for the final day, the dog’s behavior
 is not due to a misunderstanding of the task. Noticably, only        on days 1 through 7 was generated by choosing the optimal
 one participant (found in the small ‘other’ cluster) showed a        action in a given state with a probability       or any of the
 ‘reward-maximizing’ pattern of feedback.                             actions with a probability                    . was 1.0, 1.0,
                                                                      0.45, 0.1, 0.1, 0.0, and 0.0 for days 1 to 7 respectively.
 Feedback Value Participants perceived that the dog would             Unless the dog made it to the door, at which point that day
 assign a positive net value to the future expected rewards in        ended, each day was 6 steps long. We showed all
 the positive cycle (i.e.       is sufficiently large). 92% of        participants the same pre-determined set of actions.
 participants responded that the dog would prefer 2 scoldings
 (-.5 twice) followed by 4 praisings (.5 four times) to nothing       Procedure We told participants that they would train a
 (0), indicating that                                                 single dog over the course of 8 game days and that at the
                        (i.e.       ). Most participants (85%)        end of the experiment, we would test the dog, on its own, 3
 used rewards greater than or equal to punishments on cycle           times at the beginning of the path. A bonus was contingent
 trials, indicating that most would expect a reward-                  on the dog’s performance (but everyone won). Between
 maximizing agent to prefer the identified cycle at measured          each game day, participants answered questions regarding
 values. Additional questions confirmed that the scale itself         the dog’s current ability and its improvement since the last
 was interpreted symmetrically, however, we will not discuss          day (only after days 2-8). After the task, we asked
 them due to space limitations.                                       participants about the responsiveness of the dog to feedback
                                                                      on a 1-5 scale. All other details of the task, including post-
                                                                      task questions, were otherwise the same as in Experiment 1.
                                                                      Results
                                                                      All participants rated dog responsiveness above 1 = not
                                                                      responsive at all (mean=3.45, SE=.11). Additionally,
                                                                      preference judgments were similar to those in Experiment 1.
                                                                      Positive Cycles and Diminishing Rewards When teaching
                                                                      a single learner over time, most participants’ feedback
                                                                      functions showed positive cycles. The final day in the dog
                                                                      training task had the dog take the 6 steps corresponding to
                                                                      the positive cycle identified in Experiment 1. Although
                                                                      smaller, the average total reward for these 6 steps was still a
                                                                      positive value: +0.67, SE=0.19 (one-sided t-test: t(36)=3.53,
                                                                      p < .001). As compared to Experiment 1, however,
                                                                      fewer participants had a net positive cycle value on day 8
                                                                      (24 out of 37, Figure 5a).
                                                                         Consistent with smaller and fewer positive cycle values
                                                                      on the final day, rewards for correct steps declined but
                                                                      remained positive over days 3 to 8. A repeated measures
Figure 5: Experiment 2 (a) Responses on final game day still          ANOVA of responses with Day and Action as factors
have positive cycles (b) Rewards for correct actions decrease         showed both main effects (Day: F(1,36) = 15.69, p < 0.001;
but stay positive even when the learner is trained (days 6-7).        Action: F(3, 108) = 47.0, p < 0.001) and an interaction (Day
rewards delivered for each correct along the path on days 3 to
8.
                                                                  924

x Action: F(3, 108) = 4.78, p < 0.01). This suggests that                 Additionally, we mainly looked at teacher expectations in
although people do produce positive cycles consistent with             light of learning outcomes and bracketed the question of
action-feedback expectations, some teachers attempt to                 how specific learning mechanisms interact with patterns of
‘wean’ the learner off of rewards (Figure 5b).                         feedback online. Since these studies deliberately hold
                                                                       learner behavior constant, we will compare how different
Tracking Learner Ability and Improvement Participants                  teaching strategies interact with different algorithms (e.g.
only have access to the learner’s interactions with the                model-based RL, model-free RL, or online action-feedback)
environment, and so can only infer its policy indirectly.              during learning.
Despite this, judgments of the dog’s ability at the task                  More broadly, if human teachers do not naturally expect
following each day tracked the value of                 extremely      to interact with reward-maximizers but rather something
closely (mean Pearson correlation = 0.93, SE=0.008;                    akin to an action-feedback learner, one question is whether
t(36)=119.67, p < .001). Similarly, judgments of the dog’s             human learners (or other agents) meet those expectations. If
improvement tracked day-to-day changes in                   (mean      so, this may suggest that rewards and punishments delivered
Pearson correlation = 0.85, SE=0.014; t(36) = 62.39, p <               communicatively from another agent are processed
0.001). Thus, when teaching via evaluative feedback,                   distinctly from those delivered otherwise or from the
teachers infer the current state of the learner’s policy and           environment. Future work should investigate mechanisms of
track changes to that policy over time as our interaction              teaching with and learning from reward and punishment, as
model assumes.                                                         well as their interaction.
                         Discussion                                                        Acknowledgments
   Teachers often use reward and punishment to modify the              This material is based upon work supported by the National
behavior of other agents such as children and animals. In              Science Foundation Graduate Research Fellowship under
two experiments, we examined how teachers expect learners              Grant No. (DGE-1058262).
to interpret evaluative feedback. Our results demonstrate
that when giving feedback for isolated actions (Exp. 1) and                                     References
when training a single learner over time (Exp. 2), people’s
                                                                       Bellman, R. E. (1957). A Markovian decision process.
patterns of reward and punishment produce positive cycles.
                                                                          Journal of Mathematics and Mechanics, 6, 679–684.
That is, people deliver rewards in a manner that a reward-
                                                                       Csibra, G., & Gergely, G. (2009). Natural pedagogy. Trends
maximizing agent (of the variety found in standard RL)
                                                                          in Cognitive Sciences, 13(4), 148–153.
would discover and capitalize on (Dayan & Niv, 2008;
                                                                       Dayan, P., & Niv, Y. (2008). Reinforcement learning: The
Sutton & Barto, 1998).
                                                                          Good, The Bad and The Ugly. Current Opinion in
   These results can be explained by the action-feedback
                                                                          Neurobiology, 18(2), 185–196.
model, which is based on work on communicative intent
                                                                       Loftin, R., MacGlashan, J., Peng, B., Taylor, M. E.,
(Sperber & Wilson, 1986; Csibra & Gergely, 2009; Shafto
                                                                          Littman, M. L., Huang, J., & Roberts, D. L. (2014). A
et al. 2014).2 On this view, teacher feedback is not just a
                                                                          strategy-aware technique for learning behaviors from
face value reward but instead a signal about an action’s
                                                                          discrete human feedback. Proceedings of the 28th AAAI
correctness. This allows an action-feedback learner to learn
                                                                          conference on artificial intelligence.
the desired policy even in the presence of positive cycles.
                                                                       Ng, A. Y., Harada, D., & Russell, S. J. (1999). Policy
   At the same time, our current action-feedback model does
                                                                          Invariance Under Reward Transformations: Theory and
not completely account for teachers’ communicative use of
                                                                          Application to Reward Shaping. Proceedings of the 16th
rewards and punishments. For instance, it does not entirely
                                                                          international conference on machine learning (pp. 278-
explain state-training teaching functions (Exp. 1) or
                                                                          287).
diminishing rewards (Exp. 2). State training could reflect
                                                                       Owen, D. J., Slep, A. M., & Heyman, R. E. (2012). The
teachers’ attempts to teach intermediate policies, while
                                                                          effect of praise, positive nonverbal response, reprimand,
diminishing rewards may involve consideration of the
                                                                          and negative nonverbal response on child compliance: a
history of the learner. Our model could be extended to
                                                                          systematic review. Clinical Child and Family Psychology
include teacher’s inferences about what the learner has
                                                                          Review, 15(4), 364–385.
learned so far.
                                                                       Shafto, P., Goodman, N. D., & Griffiths, T. L. (2014). A
   Relatedly, the domains considered here are simple
                                                                          rational account of pedagogical reasoning: Teaching by,
gridworlds, and we do not assume that learners generalize
                                                                          and learning from, examples. Cognitive Psychology, 71,
information learned about one tile token to another of the
                                                                          55–89.
same type (e.g. two path tiles). The presence of state-
                                                                       Sperber, D., & Wilson, D. (1986). Relevance:
training suggests teachers may not make this assumption
                                                                          Communication and Cognition. Cambridge, MA: Harvard
and instead expect shared knowledge of state types.
                                                                          University Press.
                                                                       Sutton, R. S., & Barto, A. G. (1998). Reinforcement
   2
     Note that whereas Shafto et al. (2014) look at using examples        learning: An introduction. Cambridge, MA: MIT Press.
to teach concepts, we examine using feedback to teach behavior.
                                                                   925

