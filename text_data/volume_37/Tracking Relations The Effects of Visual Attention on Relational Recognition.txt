       Tracking Relations: The Effects of Visual Attention on Relational Recognition
                                             Katherine A Livins (klivins @ucmerced.edu)
    University of California at Merced, Department of Cognitive and Information Sciences, 5200 North Lake Road, Merced,
                                                                   CA, 95343
                                      Leonidas A.A. Doumas (ldoumas@staffmail.ed.ac.uk)
    University of Edinburgh, School of Philosophy, Psychology, and Language Sciences, Dugald Stewart Building, 3 Charles
                                                          St., Edinburgh, EH8 9AD
                                               Michael J. Spivey (spivey@ucmerced.edu)
    University of California at Merced, Department of Cognitive and Information Sciences, 5200 North Lake Road, Merced,
                                                                   CA, 95343
                              Abstract                                      types of relational processing. These steps include access,
                                                                            mapping, transfer, and evaluation (e.g., see Holyoak,
Relational recognition is the process by which relational
representations get recognized (i.e., representations that specify an       Gentner, & Kokinov, 2001). Broadly, access involves
actor and a patient, and are role sensitive). This process is currently     retrieving a source analog from long-term memory given a
poorly understood, but is an important aspect of relational                 particular target (Hummel & Holyoak, 1997), mapping
cognition (Livins & Doumas, 2014). This paper presents two                  involves finding structural correspondences between that
experiments that investigate the degree to which visuospatial               source and target (Hummel & Holyoak, 1997), transfer
factors influence it. The first is an exploratory eye-tracking study        allows that mapping to be used to draw inferences by
that shows that first fixations are correlated with what object gets        applying information about the base analog to the target,
bound to the actor role, while the second uses priming to show that         (Spellman & Holyoak, 1996), and evaluation involves
such fixations can alter which relation is recognized.
                                                                            adapting those inferences for the constrains and
Key Words: relational recognition,             relational   reasoning,      requirements of the problem at hand (Holyoak, Gentner &
embodiment, eye-tracking, priming                                           Kokinov, 2001). Learning is also sometimes included as
                                                                            final step in which new information, categories, and
   The statements “the author enjoyed writing the paper” and                schemas can be added to memory based on the completed
“the chef enjoyed cooking the meal” have something in                       analogy (Holyoak, Gentner & Kokinov, 2001).
common—they both rely on relational representations.                           However, these accounts miss an important and
Strictly speaking, relations are functions that assign some                 fundamental step. Livins and Doumas (2014) pointed out
truth-value to an ordered k-tuple (see Gentner, 1989), which                that while they provide a detailed account of how one may
boils down to saying that relations can take arguments, and                 reason about relations, they are silent as to how relations are
that the order in which those arguments are specified is                    recognized in the first place. Relational recognition has had
important. So, one can say, “I enjoy writing papers”, or “I                 limited study dedicated to it, but existing research suggests
enjoy cooking meals”, but saying that those things “enjoy                   that it can be quite challenging. For instance, Gick and
me” is simply confusing.                                                    Holyoak (1980, 1983) pointed out that people often fail to
   Relational representations are powerful: their focus on                  notice relations unless they are explicitly directed to do so
roles means that objects that look nothing alike can be                     and that, if people fail to do so, then the entire reasoning
compared based on what they are doing. Thus, while the                      process may never get off the ground (also see Doumas, et
two aforementioned statements might involve completely                      al., 2008). As a result, it seems important to account for
different elements (authors and chefs do not look alike, and                relational recognition.
hopefully neither do papers and meals), they can be                            While little is known about how relational recognition
understood and compared based on a common relational                        works, research has begun by studying the types of factors
structure, and the roles that such a structure affords (i.e.,               that might influence it. For instance, Livins & Doumas
creators and their creations). Thus, relational cognition (i.e.,            (2014) found that, unlike mapping, relational recognition
cognition that works with these sorts of representations) has               may be improved by increased amounts of relational
been implicated in a number of cognitive functions. For                     complexity and an integrated relational structure. Likewise,
instance, analogy-making (Gentner, 1983; Doumas &                           while evidence is currently limited, there are empirical
Hummel, 2005), inductive reasoning (Hummel & Holyoak,                       indications that visuospatial factors might also be important.
2003), and many forms of language-use (e.g., Gentner &                         First, visual cues can boost performance in participants
Namy, 2006) have all been shown to rely heavily on                          solving problem-solving tasks that involve the recognition
relational processing. As a result, it is perhaps unsurprising              of a relational concept. For example, Pedone, et al., (2001)
that great effort has been spent attempting to account for it.              showed that priming the concept of convergence with a
   While these efforts differ in their details, they have                   schematic animation can improve the likelihood of then
converged on a general set of steps that occur during many                  solving the Duncker Radiation Problem (a famously
                                                                        1416

difficult insight problem that relies on the recognition of a         encode roles and fillers. So, the lowest level is a set of
“convergence” relationship). Grant and Spivey (2003)                  distributed features (much like those found in traditional
further explored how people solve this same problem by                connectionist networks). One layer up, localist nodes
presenting a diagram of the problem and recording the eye-            combine sets of those features to code for objects and
movement patterns associated with the generation of                   relational roles, which are then temporarily bound to create
successful solutions. They found that moving one’s eyes               more complex relational structures (e.g., see Figure 1). As a
around the diagram in such a way as to spatially simulate             result, relations are not actually represented as wholes, but
the solution was correlated with success, and that evoking            instead as combinations of their roles and fillers. So, for
those same patterns with an augmented display improved                example, chases is not represented by a chases node, but by
performance (see also Thomas & Lleras, 2007).                         the combination of chaser and chased, which can be
   The relational priming literature also hints at the                temporarily bound to things like cat and dog to create
importance of visuospatial factors. For example, Livins,              something like chaser(dog) + chased(cat). Eventually, these
Doumas, and Spivey (2014) showed that ocular movements                role-bindings are combined to create full relational
might affect the learning of spatial relational categories like       statements like chased(dog, cat). Importantly though DORA
“next-to” and “above” in the presence of ambiguous stimuli.           binds through temporal asynchrony—in other words, by
Specifically, they primed ocular movements congruent with             tracking when units fire and the sequence by which they do
the underlying representational directionality of one of those        so. So, chased(dog, cat) would be represented by firing
relations (horizontal for “beside” or vertical for                    chaser, then dog, then chased, then cat, and chaser and dog
“above/below”); they found that participants were more                would be bound by firing them in immediate temporal
likely to recognize and learn whatever relational category            proximity. Thus, the model requires the subsequent firing of
had been primed. Interestingly though, the manipulation did           each relational role—the actor, and then the patient. As a
not affect performance after learning, which suggests that it         result, the model predicts that one must encode both roles
might have affected the recognition of one relation over the          (and the objects that fill them) independently in order to
other. Likewise Livins, Doumas, and Spivey (submitted)                process a relation, and that the order in which things fire is
showed that crossmapping analogy problems are more likely             representationally important.
to be solved with a relational mapping instead of a featural
mapping if problem-solving is immediately preceded by
ocular movements congruent with the relational content in
the problem. This result suggests that visuospatial priming
may not only alter which relation is recognized or learned,
but whether one is recognized at all.
   While this literature is suggestive, it is not conclusive: the
existing research has hinged on participants’ ability to
recognize a relation, but has not tested relational recognition
in specific. Thus, a rigorous test of the role of visuospatial
factors in relational recognition is necessary, along with a
theoretical account of why it might be important.                     Figure 1: An example of how DORA represents relations. A shows the
                                                                      model firing a role, and B shows it firing a filler directly after in order to
   To date, there has been one existing effort towards this           represent chasing(dog, cat).
project. Franconeri et al. (2012) looked at the role of vision
in the processing of spatial relations (such as “to the left of”         Thus, it maybe be the case that attention should not just
or “to the left of”). They argued that the visual system will         be necessary for recognizing a relation, but also for
register such relations, not holistically, but by processing          specifying which relation is recognized: if one attends to one
each relationally-relevant object sequentially. This “shift”          object playing one role before another object playing
account predicts that attention and ocular shifts (saccades)          another role, then the former might be designated as the
between objects help to encode these relations, and that at           actor, while the later the patient. Visual processing may
least one shift is necessary for recognition. For example, if         guide which is attended to first, and so which is fired first,
one is looking at a scene with a series of shapes, one might          and so which is designated as the actor (thereby affecting
recognize that “one shape is to the left of another” by               relational recognition). This paper will test this possibility.
looking at the left one, then making a saccade to one on the
right. Franconeri et al. (2012) used eye-tracking and a                                          Experiment 1
relational judgment task to confirm that such movements                  The general objective of this experiment is to determine
occurred prior to making relational judgments.                        whether eye-movement patterns can predict relational
   Interestingly, at least one model of relational reasoning          recognition. It will use a paradigm similar to that found in
predicts that this type of sequential attentional-shift-based         Gleitman et al. (2007), which used eye-tracking to show that
processing is necessary for thinking about all relations (not         gaze can shape the structure of a sentence used to describe a
just spatial ones). The DORA model (Doumas et al., 2008)              given scene. For example, it showed participants an image
encodes relations across layers of nodes, and uses time to            of a dog chasing a man and asked whether it could be
                                                                  1417

described by statements like, “The man chases the dog” or                     descriptions that differed depending on which object was
“The dog flees from the man”. They found that the selected                    bound to which role (i.e., which object was designated as
structure was influenced by the item that was looked at first.                the actor and which was designated as the patient). For
However, the study looked only at sentence structure, along                   example, chasing(x,y) might be described as escaping(y,x).
with the actor/patient designations in a given verb. They did                 These relations were depicted such that two of the image’s
not, however, look at whether this designation could change                   primary objects (the ones engaged in the primary relation)
what verb or relation was represented/identified entirely.                    began equidistant from the center of the screen on the x-axis
The current work addresses this issue. Thus, it will test                     (see Figure 3). The full list of these relations can be found in
whether the first object that one fixates on in a scene can                   Table 1. Second, filler items were chosen because they were
predict not only which object is treated as the actor or                      also expressible as two-place relations, but had a more
patient (e.g., Griffin & Bock, 2000), but also what relation                  prominent single relation (see Table 2). These relations were
is explicitly recognized and identified. First fixations were                 not depicted with the prominent relational items in the
used because they are a measure of visual attention.                          center of the screen since their inclusion was intended to
   This experiment hinges on the fact that the scenes used,                   ensure that participants did not develop trained biases to one
and the real world in general, depict numerous relations at                   location or side of the screen.
any given time. For example, a picture of a mother feeding a
child might depict a feeding relationship, as well as an
eating relationship between the child and the food, a sitting
relationship between the mother and a chair, and any
number of spatial relationships (next to, beside, etc.; see
Figure 2). Relational recognition involves, at some level,
prioritizing one relation over the others, and so                             Figure 3: An example of a scene that possess multiple relations, but in
understanding relational recognition will mean asking why                     which one (a “kissing” relationship) might be more prominent than the
                                                                              others.
that prioritization might occur and what factors might cause
it. Thus, the research question asked here is whether initial
visual fixation within a scene is one such factor.
                                                                              Figure 4: An example of a key stimulus in which the two relationally-
                                                                              engaged objects begin an equal distance away from the image-center.
Figure 2: An example of a scene that might be described as “feeding”, but
which also depicts an “eating” relationship, as well as numerous spatial           Possible           Possible Relational              Objects
ones.                                                                             Relation                Description 2           Used In Stimuli
Participants: Participants were 58 University of California                    Description 1
                                                                                 Chasing                 Escaping                boy, cat
Merced undergraduates. All were over 18 years of age, had
                                                                                 Talking                 Listening               woman1,woman2
normal vision to corrected-to-normal vision with contacts                        Lifting                 Hanging                 woman, monkey
(no glasses were allowed). The data from two more were                           Hunting                 Escaping                man, elephant
collected but excluded due to low eye-tracking locks.                            Kicking                 Cowering                boy, dog
Materials: Stimuli consisted of 21 pictorial scenes adapted                      Showing                 Watching                boy, woman
from Richland, Morrison, and Holyoak (2006). Each                                Dropping                Falling                 woman, baby
stimulus had six objects dispersed around a white and black,                     Pulling                 Riding                  boy, dog
drawn image. All stimuli were 720 by 450 pixels in size and                      Eating                  Feeding                 mother, child
were presented on a black background. The images were                            Pushing                 Riding                  girl, boy
centered on a computer screen such that there was a black                     Table 1: Key relations used in Experiments 1 and 2. Each relation afforded
outline around them, totaling 1440 by 900 pixels in size.                     multiple relational descriptions.
The images included both living and non-living elements.                         Stimuli were presented in a random order using the
   Every image depicted two objects engaged in a primary                      Pygame module. Pygame was interfaced with an EyeLink II
relational activity (e.g., while one person hugging a dog                     (i.e., a binocular eye-track made by SR Research) to collect
might be described as being “next-to” it, “hugging” might                     ocular fixations and saccades. Each stimulus had a small
be a more prominent relation in the scene; see Figure 3).                     text-box below it so that participants could enter an answer
Each stimulus was coded by two experimenters and when                         by typing it in and then pressing “Enter”. Possible spatial
the results were compared, 100% agreement was found.                          biases were controlled by flipping the images on their
   Overall, there were two classifications of relations. First,               horizontal axes across participants. Thus, half of the
key relations were chosen because they could be represented                   participants saw one item on the right hand side of the
as 2-place relations and were amenable to one-word                            screen, while the other half saw that same item on the left.
                                                                          1418

   Primary Relation                     Objects Used In Stimuli      “friendship”) since such answers showed a lack of
                                                                     understanding with regard to the task. Likewise, any
   Brushing                                girl, hair                responses that were either non-relational (e.g., “running”) or
   Cooking                                 man, food                 unclear with regard to which object was the actor (e.g.,
   Fighting                                boy1, boy2                “playing”) were eliminated. It is interesting to note that,
   Hoisting                                girl, monkey              despite the open nature of the responses, there was a high
   Kissing                                 girl, dog                 degree of commonality across answers. For example, for
   Opening                                 girl, gift                one stimulus “feeding” was provided 44 times, and “eating”
   Pouring                                 boy, water                was provided 6 times—no other answers were given.
   Reaching                                man, baby                 Likewise, another stimulus was described as “kicking” in 53
   Scolding                                woman, girl               out of 54 valid responses. This result suggests that each
   Towing                                  tow-truck, car            image had a “dominant” relation to participants.
           Table 2: A list of filler items used in Experiment 1         Second, ocular attention was tracked. We were
                                                                     specifically interested in the first item of fixation, which
Design: The experiment began with eye-tracker calibration.           was operationalized as the first object within an image’s
For this process, each participant was fitted with the head-         primary relation that was fixated upon. Analysis began by
mounted eye-tracker so that it was securely fastened. They           specifying square “areas of interest” around each object, and
sat approximately 36 inches from a 24-inch flat panel LCD            then checked whether a fixation was within that area. Like
monitor. Cameras were adjusted and focused, and the                  in the case of participant responses, fixations were coded as
thresholds for detecting pupils were automatically                   being “actor-” or “patient-oriented”.
calibrated. This allowed the experimenter to ensure that the                    Overall, 352 (approximately 72.43%) of responses
track was not lost at any location on the screen. A nine-            matched the item of first fixation (while 134, or
point calibration was performed before validation, which             approximately 27.57%, did not; see Figure 5). However,
ensured that there were no tracking errors. If validation            because this was a repeated measures design, we used
showed minimal error, then the experiment began.                     mixed effects logistic regression (see Jager, 2008) to further
   Participants were then told (both verbally and in text) that      interpret these results. For this analysis, assuming a
they needed to type the relational verb that they thought was        dominant relation (we used the first column of Table 1 for
most prominently depicted in each picture. A single training         this purpose) the actor/patient orientation of the participants’
trial was then given. It began with a fixation cross that was        response was treated as the criterion variable, while the first
shown for 1000ms, was white, and was centered on screen.             fixation was treated as a predictor. Given that this
A relational image was then shown, which depicted a                  experiment used a repeated-measures design, Participant ID
“playing-with” verb, but was otherwise the same as the rest          (of which there were 56) was also included in the model as a
of the stimuli. Participants were told to type an answer, and        nested factor, along with the image, which was treated as a
then shown their own answer with the possible candidate              random factor. The model is described in Table 3, and a
answer of “playing-with”. Both were shown so to ensure               likelihood ratio test was used to compare it to a null model;
that they understood what a relational verb was. Instructions        it was found that first fixation made a significant difference
were then reiterated.                                                (χ(1)=3.926, p<.05).
    Participants then began the experiment. They worked,
self-paced, through all problems (no further instructions                 Predictor	    Coef (β)	   SE(β)	       z	         p	      Odds
                                                                                                                                         Ratio	  
were given). Drift-corrects were taken every 5 trails to
                                                                          Intercept	     2.1000	    0.8983	    2.338	   0.0194	   8.165783	  
ensure that the eye-tracking lock was maintained.
Results: Two measures were collected. First, we analyzed                    First         0.7015	    0.3310	    2.120	   0.0340	   2.016787	  
                                                                          Fixation	  
participants’ responses. These were in the form of words,
and coded based on which object was bound to the actor                               Table 3: The model results from Experiment 1
role (for example, “chasing” would designate the boy in
Figure 3 as the actor). For the sake of calculations, one
relation was chosen as the default for each image (in every
case this default was the relation listed in the first column of
Table 1), and responses were coded as 1 for “actor-based”
or 0 for “patient-based”. So, for example, “chasing” was
considered the default for one image, and so a “chasing”
response was coded as “actor-based”, while “escaping” was
coded as “patient-based”.
   Given that this experiment was exploratory, and that we
wanted to determine whether there is a correlation between
looking at an item and recognizing a relation where that             Figure 5: A graphical representation of the overall number of responses
item is the actor, we had a number of exclusion criteria.            that matched the first fixation made within each stimulus.
First, any non-verb responses were eliminated (e.g.,
                                                                 1419

Discussion: The results of this study suggest that there             especially warranted given that the data from the first
exists a relationship between the item that one fixates on           experiment indicated that most stimuli had a dominant
first and the item that one designates as a relational actor.        relation that was recognized by most participants (i.e., one
As a result, they suggest that fixation is somehow related to        object that was typically bound to the actor role), and so
what relation that is recognized. However, this study was            looking for deviations seemed worthwhile.
correlational in nature, and so Experiment 2 will attempt to            First fixations were tracked and used to eliminate
direct visual attention to different objects in order to             participants. Again, given that our research question was
determine whether this relationship is causal.                       whether changing participants’ first fixations would change
                                                                     the course of the recognition process, we used fixations to
                        Experiment 2                                 ensure that participants actually fixated on the prime. Trials
   The objective of this experiment was to determine                 in which a participant initially fixated on a different object
whether the trajectory of relational recognition may be              were eliminated (this included .03% of all trials).
manipulated by visual attention. Specifically, because of the           We then used a mixed effects multinomial logistic
results of Experiment 1, it will test whether priming the first      regression model to interpret our results. Once again,
item of fixation can change what relation is identified. The         participants’ answers were treated as the criterion variable,
experiment will be almost identical to Experiment 1,                 while the prime was treated as the predictor, participant ID
however, it will direct visual attention towards a specific          was treated as a nested variable, and image as a random
object in each scene at the beginning of every trial.                variable. The model is described in Table 4, and a likelihood
Participants: Participants were 132 University of                    ratio test comparing the model to null showed that priming
California Merced undergraduates that were otherwise                 was a significant factor (χ(1)=35.343, p<.01). Specifically,
analogous to those used in Experiment 1. Four participants           it showed (again, by odds ratio) that one is 4.25 times more
were eliminated entirely due to poor eye-tracking locks.             likely to recognize a relation that uses the primed item as an
Materials: The materials were the same as those listed in            actor. The overall differences in responses by condition can
Experiment 1 with one addition. Priming was achieved by              be seen in Figure 5.
exploiting the eye-tracker’s normal calibration process.                Predictor	      Coef	      SE(β)	      z	        p	        Odds	  
Specifically, calibration involved a series of small black                                (β)	                                         Ratio	  
dots with a white center point that appeared in various                  Intercept	    2.0807	     0.8663	   2.430	   0.0151	    8.010303	  
places around the screen. It required participants to fixate on
                                                                          Primed	      1.4462	     0.2477	   5.839	   <0.0001	   4.246898	  
the center of those dots and to press “spacebar”. Thus, key
trials involved two extra “calibration dots”: one just before
an image was shown, and then one 100 to 500 ms after the                             Table 4: The model results from Experiment 2
image appeared (the exact amount of time was randomly
generated). A random number of filler trials also had extra
“calibration” dots, but the locations of the dots were
randomly generated and scattered across the screen.
Design: This experiment proceeded in almost the same way
as Experiment 1. However, during initial calibration the
experimenter emphasized that she was having trouble
getting a lock on the participant and so extra calibration
throughout the study might be required.
   Two controls were used: First, like in Experiment 1,
images were flipped on their horizontal axes for half of the
participants. Second, each relationally relevant item was            Figure 6: A graphical representation of the overall number of responses
primed for half of the participants. So, for example, if a trial     that matched the first fixation made within each stimulus.
depicted chases(boy, cat) (or escapes(cat,boy)), then half of        Discussion: The results of this study suggest that it is,
the participants were primed to initially fixate on the boy,         indeed, possible to shape relational recognition by
while the other half were primed to fixate on the cat.               manipulating which item is fixated on first. Thus, the
Results: Once again, participant responses and first                 relationship between first fixation and relational recognition
fixations were tracked. However, the coding system for the           is not just correlational, but causal.
responses changed slightly due to the research question. To
the point, our goal was to determine whether making                                           Overall Discussion
someone fixate on a specific object would change the                    When considered together, these studies suggest that
relation given. Thus, we allowed for neutral responses in            relational recognition is not only correlated with one’s
this experiment (and not just actor or patient based ones,           visual attention, but that it can also be changed by that
like in the previous experiment). For example, “conversing”          attention. Specifically, Experiment 1 showed that the first
was allowed for the “talking” stimulus, despite the fact that        item that one fixates on when scanning a scene may predict
conversing is a bidirectional relation. This approach seemed         which relation is recognized, while the second suggests that
                                                                 1420

manipulating that first fixation by directing it at one item or      Gentner, D. (1983). Structure-mapping: A theoretical
another can make it more likely that one will recognize a              framework for analogy. Cognitive Science, 7.2, 155-170.
relation that designates that object as an actor. As a result        Gentner, D. (1989). The mechanisms of analogical learning.
they have at least two theoretical implications.                       In S. Vosniadou and A. Ortony (Eds.), Similarity and
   First, very generally, they help to link visual processing to       analogical reasoning. New York: Cambridge University
relational processing—at minimum, they suggest that where              Press.
one looks affects what one attends to, which affects what            Gentner, D., & Namy, L. L. (2006). Analogical processes in
relation one recognizes, and therefore what relation one               language learning. Current Directions in Psychological
reasons about. This is an important step for embodied                  Science, 15(6), 297-301.
efforts, which have argued that the body is an important part        Gick, M.L., & Holyoak, K.J. (1980). Analogical problem
of cognitive functioning (e.g., see Spivey 2008), but does             solving. Cognitive Psychology, 12, 306-355.
not detract from the possible importance of symbolic                 Gick, M.L., & Holyoak, K.J. (1980). Schema induction and
content (which is emphasized in the relational reasoning               analogical transfer. Cognitive Psychology, 15, 1-38.
literature, e.g., see Gentner, 1983; Doumas & Hummel,                Gleitman, L.R., January, D., Nappa, R., Trueswell, J.C.
2005). That said, an interesting question to ask in the future         (2007). On the give and take between event apprehension
is whether and how this process feeds back into perceptual             and utterance formulation. Journal of Memory and
processing to create an overall trajectory of reasoning in             Language, 57(4), 544-569.
dynamic, information-rich environments.                              Grant, E.R., & Spivey, M. (2003). Eye movements and
   Secondly, these results also have implications for debates          problem solving: Guiding attention guides thought.
about mental representation—especially with regard to how              Psychological Science, 14(5), 462-466.
relations are represented. DORA and its predecessor LISA             Griffin, Z. & Bock, K. (2000). What the eyes say about
(see Hummel & Holyoak, 2003) are unique in they way that               speaking. Psychological Science, 11, 274-279.
they use role-filler bindings and time to create more                Hummel, J.E., & Holyoak, K.J. (2003). A symbolic-
complex relational structures. An important prediction of              connectionist theory of relational inference and
those representational structures is that relations are not            generalization. Psychological Review, 110, 220-264.
processed holistically, but in terms of roles, the items that        Hummel, J.E., & Holyoak, K.J. (1997). Distributed
fill them, and the temporal sequence in which they fire. This          representations of structure: A theory of analogical access
account has been supported by Franconeri et al. (2012), and            and mapping. Psychological Review, 104(3), 427-466.
extended beyond the realm of spatial relations in this work.         Jaeger, T.F. (2008). Categorical data analysis: Away from
   Finally, these studies help contribute to our understanding         ANOVAs (transformation or not) and towards logit mixed
of relational recognition as a process. Relational recognition         models. Journal of Memory and Language, 59(4), 434-
is generally not well understood, and research on it is just           446.
beginning. Livins and Doumas (2014) suggested that                   Livins, K.A., & Doumas, L.A.A. (in press). Recognizing
relational complexity is one important factor, and here                relations: What can be learned from considering
visual attention can be specified as another. That said, we            complexity. Thinking and Reasoning.
did not find the correlation between fixation and recognition        Livins, K.A., Doumas, L.A.A., & Spivey, M.J. (submitted).
to be perfect, which suggests that other factors are still yet         Shaping relations: Exploiting relational features for
to be found. One possibility is how linguistically common a            visuospatial priming
relation is over alternative descriptions (e.g., the word            Livins, K.A., Doumas, L.A.A., & Spivey, M.J. (2014). The
“kicking” is used more commonly than the word                          effects of visuospatial priming on category learning.
“cowering”). Future research will look at such factors in              Proceedings of the 36th Annual Conference of the
order to provide a more cohesive account of relational                 Cognitive Science Society. Quebec City: Cognitive
recognition in general. Ultimately, the goal will be to                Science Society.
describe recognition in such a way as to incorporate it into         Pedone, R., Hummel, J.E., & Holyoak, K.J. (2001). The use
the overall relational-reasoning process.                              of diagrams in analogical problem solving. Memory &
                                                                       Cognition, 29, 214–221.
                         References                                  Richland, L.E., Morrison, R.G., & Holyoak, K.J. (2006).
Doumas, L. A. A. & Hummel, J. E. (2005). Modeling                      Children’s development of analogical reasoning: Insights
   human mental representations: The Cambridge handbook                from scene analogy problems. Journal of Experimental
   of thinking and reasoning (pp. 73-91). Cambridge, UK:               Child Psychology, 94, 249-271.
   Cambridge University Press.                                       Spellman, B.A., Holyoak, K.J. (1996). Pragmatics in
Doumas, L.A.A., Hummel, J.E., & Sandhofer, C.M. (2008).                analogical mapping. Cognitive Psychology, 31, 307-346.
   A theory of the discovery and prediction of relational            Thomas, L.E. & Lleras, A. (2007). Moving eyes and
   concepts. Psychological Review, 115(1), 1-43.                       moving thought: On the spatial compatibility between eye
Franconari, S.L., Scimeca, J.M., Roth, J.C., Helseth, S.A., &          movements and cognition. Psychonomic Bulletin and
   Kahn, L.E. (2012). Flexible visual processing of spatial            Review, 14(4), 663-668.
   relationships. Cognition, 122, 210-227.
                                                                 1421

