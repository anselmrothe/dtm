      On the interplay between spontaneous spoken instructions and human visual
                                        behaviour in an indoor guidance task
                                          Nikolina Koleva (nikkol@coli.uni-saarland.de)
                          Embodied Spoken Interaction Group, Saarland University, Saarbrücken, Germany
                                             Sabrina Hoppe (shoppe@mpi-inf.mpg.de)
                  Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany
                              Mohammad Mehdi Moniri (Mohammad Mehdi.Moniri@dfki.de)
                               German Research Center for Artificial Intelligence, Saarbrücken, Germany
                                           Maria Staudte (masta@coli.uni-saarland.de)
                          Embodied Spoken Interaction Group, Saarland University, Saarbrücken, Germany
                                            Andreas Bulling (bulling@mpi-inf.mpg.de)
                  Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany
                               Abstract                                 a “signal” and feedback channel to the speaker, who can then
                                                                        modify and adapt their next utterance. A listener may even
   We present an indoor guidance study to explore the interplay
   between spoken instructions and listeners’ eye movements.            consciously use her gaze, similar to a pointing gesture, for
   The study involves a remote speaker to verbally guide a lis-         instance in order to point when her hands are full or for other
   tener and together they solved nine tasks. We collected a            reasons unavailable.
   multi-modal dataset consisting of the videos from the listen-
   ers’ perspective, their gaze data, and instructors’ utterances.         This reciprocal nature of gaze during spoken interactions
   We analyse the changes in instructions and listener gaze when        is not captured in most interactive studies so far, also because
   the speaker can see 1) only the video, 2) the video and the gaze     it is difficult to assess. Eye movements may be considered
   cursor, or 3) the video and manipulated gaze cursor. Our re-
   sults show that listener visual behaviour mainly depends on ut-      both a dependent variable (symptom, as an indicator for com-
   terance presence but also varies significantly before and after      prehension processes) and an indirect independent variable
   instructions. Additionally, more negative feedback occurred          (signal, affecting utterance content). The aim of the present
   in 2). While piloting a new experimental setup, our results
   provide indication for gaze reflecting both: a symptom of lan-       study is to shed light onto this dual role of gaze and to quan-
   guage comprehension and a signal that listeners employ when          tify to which extent listener eye movements depend on the
   it appears useful and which therefore adapts to our manipula-        speaker’s utterance and vice versa.
   tion.
   Keywords: referential situated communication; specific task
                                                                           We designed an exploratory experiment that involves spon-
   guidance; mobile eye tracking; visual behaviour analysis;            taneous spoken instructions in a real-world environment
   gaze-sensitive feedback                                              while we manipulated the availability of listener gaze (hence-
                                                                        forth GazeAvailability) in form of a cursor to the speaker.
                          Introduction                                  Specifically, one person (the speaker or “instructor”) re-
We constantly direct our gaze to different parts of the visual          motely guided another person (the listener or “walker”)
scene to be able to perceive objects of interest with high acu-         through a hall to a number of desks with distractors and tar-
ity. These eye movements can be driven internally, i.e. by              get items with which different tasks had to be performed, such
some self-initiated goal or intent, or externally by something          as assembling utensils for baking a cake. Both task and target
that attracts our visual attention (Yantis & Jonides, 1990).            items were only known to the instructor. The walker was eye-
External factors driving the attention can be salient objects           tracked and the instructor saw the output of the eye tracker’s
in the visual scene or another person’s utterances that di-             scene camera only (N O G AZE), the video overlaid with the
rect our eyes to a co-present object or event. The latter               walker’s gaze position (G AZE) or the video overlaid with the
has been exploited in many psycholinguistic studies in or-              current gaze position to which we artificially added 20% ran-
der to study language comprehension processes (for example              dom error (M AN G AZE).
see Cooper, 1974; Tanenhaus, Spivey-Knowlton, Eberhard,                    While task performance did not vary with GazeAvail-
& Sedivy, 1995).                                                        ability, the amount of feedback given by the speaker did
   Conversely, a listener’s gaze may also signal (mis-) under-          to some extent. We further found that listeners’ gaze be-
standing back to the speaker. Taking the listener’s behaviour           haviour differed as a function of whether or not an utterance
into account when planning and making utterances is an im-              was taking place, probably reflecting language comprehen-
portant aspect of collaborative, goal-oriented interaction. In          sion processes. Moreover, gaze patterns also changed with
this sense, a listener’s eye movements can be both: A re-               GazeAvailability to the speaker. In particular, we analysed
sult of a comprehension process, i.e. a “symptom”, and/or               scenes immediately before any utterance onset but also di-
                                                                    1153

rectly after utterance offset. We take the former to provide
some indication for gaze being used as signal to which a
speaker reacts, whereas the latter suggests that GazeAvail-
ability may also have an indirect influence onto the speaker’s
utterances which, in turn, have an impact on listener gaze
again.
                         Related work
Previous research has shown that listeners follow speakers’
verbal references (as well as her gaze in face-to-face sit-
uations) to rapidly identify a referent (Eberhard, Spivey-
Knowlton, Sedivy, & Tanenhaus, 1995; Hanna & Tanenhaus,
2004; Keysar, Barr, Balin, & Brauner, 2000). The reaction
of the speaker to such referential eye movements, however,
was considered in few studies. Clark and Krych (2004),
for instance, aimed to grasp this reciprocal nature of inter-
action in a study using a collaborative block building task                     Figure 1: A screenshot of instructor’s display.
and manipulating whether participants could see each other
or each other’s workspace. Their results suggested that the
joint workspace was more important than, for instance, see-            or whether the speaker would be robust towards slight impre-
ing each other’s faces. Staudte, Koller, Garoufi, and Crocker          cisions of the gaze cursor and treat it more like G AZE.
(2012) conducted a study in which users were guided by a                  Firstly, the experiment was aimed to reveal whether the
natural language generation (NLG) system through a virtual             availability of listener gaze position to the speaker would af-
world to find a trophy. The system either gave feedback to the         fect the production of verbal feedback. And secondly, if gaze
users’ eye movements, or not. This controlled setting allowed          was used as a signal, which listeners control and use delib-
the observation of dynamic and interactive (gaze) behaviour            erately, then the option to do so (and thereby evoke speaker
while maintaining control on one interlocutor (the NLG sys-            reactions) would ubiquitously change listener gaze. If gaze
tem). The results of this study suggest that it can be benefi-         was more generally a symptom of other processes and delib-
cial for task performance when listener gaze is exploited by           erate control was (too) difficult, listener gaze would rather
the speaker to give feedback. It remains unclear, however,             change with tasks or events than with GazeAvailability. Fi-
whether (human) speakers indeed provide such feedback and              nally, if gaze was used as a signal, variations of listener gaze
how the availability of listener gaze recursively affects the          behaviour should mainly occur prior to an utterance. If gaze
spoken instructions and, possibly, the gaze behaviour itself.          was a reaction to changes in the utterances (i.e. a symptom),
                                                                       gaze behaviour should rather change after an utterance.
                          Experiment
We designed an experiment that combines a dynamic, interac-
                                                                       Method
tive setting with the possibility to conduct exact and detailed
analyses, in particular on eye movement behaviour, in order
                                                                       The instructor received a plan of the route and a picture of
to assess the mutual influence of listener gaze and speech.
                                                                       the table top in which the next target object for the micro task
Naı̈ve participants either became an instructor (speaker) or a
                                                                       was highlighted, see Figure 1. To make the task sufficiently
walker (listener). The speaker instructed the listener to per-
                                                                       complex and elicit precise references to target objects, at least
form a series of tasks. These tasks consisted of a navigational
                                                                       two distractors for each target were also on the table.
part, i.e. finding the next out of nine tables in a hall, which we
call the macro task, and some object assembly at each table,              The experiment consisted of nine micro tasks, each of
the so-called micro tasks. Each pair of participants experi-           which was dedicated to some everyday life activity such as
enced all three GazeAvailability manipulations.                        office work or cooking. Office tasks included writing a letter
   The listener wore a head-mounted eye tracker through                using envelopes, pens, blocks and glue; kitchen tasks making
which the speaker could see the scene from the listener’s per-         a cake using milk, sprinkles, mixing spoons and an eggs box.
spective (N O G AZE) and additionally the listener’s exact gaze        In total, 234 every day objects were used, 36 of which were
cursor (G AZE) or a manipulated gaze point (M AN G AZE).               target objects.
The purpose of this manipulation GazeAvailability was to re-
veal whether the availability of listener gaze to the speaker          Participants Twelve pairs of participants (16 females) took
affected a) the produced utterances and b) the listener’s              part in this study. Average age was 26.6 and all but one were
gaze. The purpose of including M AN G AZE was to investi-              in the age range 18-40. All participants were German native
gate whether slightly perturbed gaze would be considered ei-           speakers and received a payment of 10 e. A session lasted
ther un-informative or even disturbing (more like N O G AZE),          between 30 and 45 minutes.
                                                                   1154

Procedure Participants were first asked about their prefer-          few other words that are not typical for feedback but had this
ence for role assignment and assigned to the walker/instructor       function in a particular context.
role accordingly. Two experimenters instructed both partici-            Lastly, we assessed the proportion of positive and neg-
pants separately from each other. Specifically, the instructor       ative feedback instances per condition. We used linear
was shown the route and tables but was not told how to refer         mixed-effects models using the lme4 package in R (Baayen,
to target objects. Then, she was led to a remote room from           Davidson, & Bates, 2008) and model selection in order to
where she guided the walker. During the experiment the               determine the influence of GazeAvailability.
instructor saw a picture of the current target object, a map of
the hall, and the scene view of the walker. Neither walker
                                                                     Eye movement analysis We first detected fixations using
nor instructor were informed about our manipulation.
                                                                     a standard dispersion-based fixation detection algorithm as
                                                                     in (Salvucci & Goldberg, 2000) that declares a sequence of
Apparatus                                                            gaze points to be a fixation if the maximum distance from
                                                                     their joint center is less than 5% of the scene camera width
We used a Pupil Pro monocular head-mounted eye tracker for           and the sequence has a minimum duration of 66 msec. Eye
gaze data collection (Kassner, Patera, & Bulling, 2014). The         movements between two fixations were considered saccades
tracker is equipped with a high-resolution scene (resolution         without further processing. Blinks were not included as
of 1280 x 720 pixels) and eye camera (640 x 360 pixels).             video-based eye trackers, such as Pupil, do not record them
We extended the Pupil software with additional functionality         by default. We then used a sliding window approach with a
needed for our study, namely to hide and display a manipu-           window size of 500 msec and step size of 250 msec to ex-
lated gaze cursor to the instructor.                                 tract eye movement features, resulting in a dataset consisting
   Two notebooks were used: one for the walker and one for           of 18841 time windows.
the instructor. The instructor notebook was connected to two            For each window, we extracted a subset of 45 features of
displays, one for the instructor and one for the experimenter.       those previously proposed for eye-based recognition of vi-
The experimenter sitting next to the instructor used a con-          sual memory recall processes (Bulling & Roggen, 2011) and
trol panel to send commands to the eye tracking software to          cognitive load (Tessendorf et al., 2011). We added 21 addi-
switch between conditions. The eye tracker was connected             tional features relating current gaze behaviour to the overall
to the walker notebook on which we recorded the incoming             gaze behaviour of the current person in the current experi-
sound, i.e. the instructions the listener heard. Both audio and      ment, e.g. the ratio of the small saccade rate in the whole
video signals were streamed using Skype. In addition, the            experiment to the small saccade rate in this time window.
walker was equipped with a presenter to signal success (find-        All features are shown in Table 1. For feature selection
ing a target object) by pressing a green button which was used       we used the minimal-redundancy-maximal-relevance crite-
later on to segment the micro tasks. The communication of            rion (mRMR) which aims to maximise the feature’s relevance
the different software components was implemented using a            in terms of mutual information between target variable and
custom client-server software but all recordings were carried        features while discarding redundant features (Peng, 2007).
out on the walker machine.                                           For our analyses we relied on data driven method and used
                                                                     the consistently top-ranked features for target variables such
Analysis                                                             as GazeAvailability, Pair or FeedbackPresence and fitted lin-
Linguistic analysis To prepare the recorded data for fur-            ear mixed-effects models to the top-ranked feature according
ther processing, we applied a standard linguistic preprocess-        to mRMR (saccade rate). Similar results can also be achieved
ing pipeline. We first transcribed the audio signal, which           based on further top-ranked features such as the ratio of small
was a manual step as the discourse collected in our study            to large saccades (where a saccade is considered small if its
was very specific and contained also ungrammatical utter-            amplitude is less than twice the maximum radius of a fixa-
ances. We then aligned the text to the audio signal by apply-        tion).
ing the forced alignment technique (Kisler, Schiel, & Sloet-
jes, 2012). We performed lemmatization and part-of-speech                                        Results
(POS) tagging followed by linguistic annotation using shal-
low syntactic analysis. These annotations are automatically          We first evaluated the time needed to solve a task (all tasks
carried out using TreeTagger (Schmid, 1995).                         were solved) in each condition to reveal whether gaze was
   Two types of feedback instances, positive and negative,           used to complete a task more efficiently. It took participants
were recognized by searching for word occurrences that ex-           64.16 seconds on average to solve a task in the G AZE con-
press feedback, e.g. “ja, genau” (yeah right) is positive while      dition, 62.96 seconds in the M AN G AZE condition, and 64.46
“nein, falsch” (no, wrong) is negative. However, in some             seconds in the N O G AZE condition. Differences were not sig-
cases those words did not express feedback but had a differ-         nificant. Since the average interaction time was generally
ent grammatical function. Therefore a manual post correction         very low, a floor effect has possibly prevented a distinction
was carried out to filter out detected instances and also to add     of the conditions.
                                                                 1155

  Fixation       rate, mean, max, variance of durations             marginally significant difference indicates that speakers make
                 mean, variance of variance within one fix.         use of the exact gaze positions of the listeners and that
  Saccades       rate, ratio of (small/large/right/left) sacc.      they utter more negative feedback to signal misunderstand-
                 mean, max, variance of amplitudes                  ings. M AN G AZE is treated somewhat in-between G AZE and
  Combined       ratio saccades / fixations                         N O G AZE.
  Wordbooks      number of non-zero entries                            Moreover, a negative feedback instance is usually followed
                 maximum and minimum entries as well as             firstly by a repair, i.e. an additional description that either pro-
                 their difference for n-grams with n ≤ 4            vides a complementary information that was not mentioned
  Ratios         all fixation, saccade and combined features        in the instruction before or an alternative description that
                 in ratio to the value over the whole trial for     describes a distractor which is usually underspecified. Sec-
                 a particular pair and condition.                   ondly, a positive feedback instance often follows to confirm
                                                                    the successful resolution of the repair. Example (1) illustrates
Table 1: Features extracted from human visual behaviour in-         that repeated pattern.
spired by Bulling et al. (2011).
                                                                    (1)      ne das andere ... Genau (no the other one ... exactly)
Linguistic analysis                                                 We further explored if these repairs differed with the avail-
Next we examined the intuition that the length of utterances        ability of gaze: We measured the length (in words) of the
would be shorter in the G AZE condition and longer in the           repairs and compared them across all conditions but found no
M AN G AZE condition. However, no significant difference            significant differences.
was found.                                                          Visual behaviour analysis
                                                                    To asses the role of listener gaze in this scenario, we ex-
                                                                    amined the interplay of utterances, listener gaze and the
                                                                    GazeAvailability manipulation.
                                                                       First, we fitted a linear mixed-effects model with a random
                                                                    intercept and random slope for pair to the data set consisting
                                                                    of all (sliding) time windows (18841 in total). We found a
                                                                    significant main effect of UtterancePresence through model
                                                                    selection (χ2 (1) = 9.54, p = .002). GazeAvailability, in con-
                                                                    trast, had no effect on model fit. We then considered feedback
                                                                    expressions which are a specific form of utterance and com-
                                                                    monly occur in situated and spoken interaction: Such phrases
                                                                    typically form a direct and closely time-locked response to
                                                                    changes in the situation or, more crucially, the listener’s be-
                                                                    haviour. Similarly to the analysis of utterances in general,
                                                                    we fitted a linear mixed-effects model, this time with Feed-
                                                                    backPresence as a factor. We observed a main effect (χ2 (1)
Figure 2: The proportion of positive and negative feedback          = 80.63, p <.001) and an interaction with GazeAvailability
instances in the different conditions. The model fitted to that     (χ2 (2) = 9.38, p =.009). The interaction suggests that the
data is the following feedbackType ∼ GazeAvailability               manipulation of gaze availability has some effect on how lis-
+ (1|Pair)                                                          teners move their eyes during verbal feedback compared to
                                                                    before or after. This observation also seems to be in line with
   We then investigated the proportion of positive and nega-        the results of the linguistic analysis according to which the
tive feedback. To test if the difference in the proportions is      proportion of positive and negative feedback instances vary
significant we constructed a generalised linear mixed-effects       in the different levels of GazeAvailability to the speaker.
model (with a logit link function) fitted to FeedbackType with         Taken together, the results from gaze behaviour in Utteran-
GazeAvailability as a fixed effect.                                 cePresence and FeedbackPresence indicate that gaze patterns
   Figure 2 depicts a graph that shows the proportion of feed-      differ with speech happening or not, i.e. when the listener
back in the different gaze conditions and gives the model           is processing speech compared to when she is not currently
specifications. The amount of data points (feedback instances       listening to an utterance, and that this is relatively indepen-
per pair) does not license the inclusion of a random slope in       dent of GazeAvailability. In light of the symptom-signal-
the model so we include only the random intercept for Pair.         distinction, this suggests that language comprehension pro-
   This model shows a difference between the G AZE and              cesses drive the ocular system (symptom) but that deliberate
N O G AZE condition that approaches significance (Coeff.            control of gaze, e.g. using it as pointer in the G AZE but not
= 0.574; SE=0.314; Wald’s Z=1.829; p = .067). This                  the N O G AZE condition (signal), hardly affects overall gaze
                                                                1156

patterns.                                                            somewhat less than natural gaze but was not ignored either.
   Furthermore, we attempted to break up the reciprocal na-             Based on the combination of gaze effects before and after
ture of the interaction between listener gaze and speech by          an utterance and the lack of such an effect on eye movements
considering the temporal order of gaze events and speech             during an utterance, we further assume that listener gaze can
events. Examining how gaze affect utterances and then, in            be seen as both a signal from listeners for conveying some
turn, how the utterances affect eye movements helps us to            sort of information to the speaker and as symptom that reflects
shed light onto the dual role of listener gaze: On the one hand,     language comprehension processes.
it can be seen as a sign that helps the walker to communicate           The tendency of speakers to produce more negative feed-
with the instructor (as the instructor can observe the walker’s      back with gaze availability also supports the role of listener
behaviour but cannot hear the walker). In this case, gaze pat-       gaze as a signal to which instructors actively react. These
terns may differ between the G AZE and N O G AZE conditions          feedback instances have the potential to quickly eliminate
before an utterance, since in the former condition gaze may          wrong beliefs by the listener about intended referents. We
be more frequently used as a signal to which the speaker re-         did not find an improvement of task performance in terms of
acts. On the other hand, gaze may be mostly a symptom that           time needed for completion in the G AZE condition but we
reflects language processing and which therefore may also re-        believe that this could be due to a ceiling effect.
flect when the speaker adapts to seeing listener gaze (G AZE            Similarly, we did not find a significant effect of GazeAvail-
condition) and produces utterances accordingly. In that case,        ability on other coarse-grained measures of the spoken ma-
gaze patterns are likely to differ with GazeAvailability imme-       terial such as utterance length (in words). However, many
diately after utterance offset.                                      words do not necessarily carry more information. Impor-
   Thus, analogously to the analyses above, we fitted linear         tantly, the salience threshold for the speech segmentation is
mixed-effects models on a subset of the data, namely the time        also a crucial parameter and can vary depending on the do-
windows immediately before the onset and after the offset            main, task and setting, e.g. whether it is an uni- or bidirec-
of an utterance. Both subsets consist of 954 instances and           tional, free or goal-oriented conversation. In addition, the
we found that the factor GazeAvailability significantly con-         word level may be too coarse to reveal qualitative differences
tributes to a better model fit, not only before an instruction       in utterances as a function of listener gaze as mentioned in
(χ2 (2) = 9.77, p = .008) but also after it (χ2 (2) = 10.89, p =     Section Linguistic analysis. Hence it may be worth examin-
.004). The same analysis was carried out for the time win-           ing whether the instructions collected in the recorded inter-
dows before and after positive and, additionally, before and         action can be distinguished on a more fine-grained linguistic
after negative feedback occurrences. However, no effect of           layer but this was beyond the scope of this paper.
GazeAvailability was observed (which may also be due to the
lower number of samples).                                            Future Work
   To conclude, we observed no significant difference in gaze
                                                                     There are several caveats in this study which motivate fu-
behaviour along with the GazeAvailability manipulation, but
                                                                     ture work. Firstly, the possibility for listeners to show their
gaze patterns were distinct from each other in the presence
                                                                     hands, make pointing gestures or hover over objects prob-
and absence of utterances in general and feedback in partic-
                                                                     ably added noise to the role of listener gaze as a feedback
ular. The analyses taking temporal aspects of the gaze and
                                                                     modality. A follow-up study in a virtual environment will
speech events into consideration showed that listener gaze
                                                                     avoid this and hopefully increase clarity of the gaze-speech
significantly differs before and after instructions. This evi-
                                                                     interaction pattern. Secondly, the experiment consisted of a
dence supports the view that listener gaze can not only be
                                                                     micro and a macro scale task, the latter of which was orig-
seen as a symptom of language comprehension but also a non
                                                                     inally intended to be more of a navigation task. The actual
verbal signal to the speaker. The latter role is comparable to
                                                                     reduction in task complexity (and therefore for the neglect in
the role of verbal deictic expression like “Do you mean this
                                                                     the analyses) lies in the significant technical challenges to set
one there?” which may have been used in a bidirectional ver-
                                                                     up a stable WLAN connection throughout a large building to
bal dialogue.
                                                                     transfer high-resolution video data, audio, and gaze data in
                                                                     real time. Thirdly, we considered relatively coarse, quanti-
                         Discussion
                                                                     tative measures for utterances so far, mostly due to the lack
In this exploratory study, we observed that the manipulation         of manpower in annotating the data. Further analyses with
of availability of listener gaze position to the speaker had a       respect to such richer annotations as well as other eye move-
main effect on listener gaze before and after an utterance, but      ment analyses (such as using smooth pursuit) are planned.
not while an instruction was being spoken. GazeAvailabil-            Lastly, we plan to classify scenes as containing confusion or
ity further affected the type and amount of feedback given           misunderstandings which would be of particular interest, for
by speakers. In particular, G AZE differed significantly from        instance, for a machine learning approach in order to detect
N O G AZE with M AN G AZE being in-between those two con-            confusion from eye movements. Measures of cognitive load,
dition with respect to the amount of negative feedback uttered       for instance, may be extracted from the available data to label
by the speaker. This suggests that manipulated gaze was used         scene segments accordingly.
                                                                 1157

Conclusion                                                           Eberhard, K. M., Spivey-Knowlton, M. J., Sedivy, J. C., &
We reported on an indoor guidance study to explore the inter-          Tanenhaus, M. K. (1995). Eye movements as a win-
play between spontaneous spoken instructions and listeners’            dow into real-time spoken language comprehension in nat-
eye-movement behaviour. We presented a study design and                ural contexts. Journal of Psycholinguistic Research, 24(6),
experimental setup to collect a multi-modal dataset of scene           409–436.
view videos from the listener’s perspective, their gaze data,        Hanna, J. E., & Tanenhaus, M. K. (2004). Pragmatic effects
and instructors’ verbal instructions. We found that listener           on reference resolution in a collaborative task: evidence
gaze itself as well as the speaker’s utterances were affected          from eye movements. Cognitive Science, 28(1), 105 - 115.
by GazeAvailability. The specific pattern of effects suggests        Kassner, M., Patera, W., & Bulling, A.                  (2014).
that listener gaze is not only a processing symptom that is af-        Pupil: an open source platform for pervasive eye
fected indirectly by the variation of GazeAvailability, but also       tracking and mobile gaze-based interaction.           In Adj.
as a signal being used deliberately as a pointing gesture.             Proc. UbiComp (p. 1151-1160).               Retrieved from
   To conclude, we have presented an exploratory study                 http://pupil-labs.com/pupil/
which aimed to shed some light on the role of listener gaze          Keysar, B., Barr, D. J., Balin, J. A., & Brauner, J. S. (2000).
(position) in an interactive, indoor guidance task. The study          Taking perspective in conversation: the role of mutual
combines an interactive and very dynamic setting with fine-            knowledge in comprehension. Psychological Science, 11,
grained data collection and analyses. The presented results            32–38.
can be seen as a first step towards understanding the recipro-       Kisler, T., Schiel, F., & Sloetjes, H. (2012). Signal process-
cal nature of gaze behaviour and speech in human interaction           ing via web services: the use case webmaus. In Proceed-
which may, for instance, help artificial interaction partners to       ings digital humanities 2012, hamburg, germany (p. 30-
exploit human gaze in making communication more efficient              34). Hamburg.
and less error-prone.                                                Peng, H. (2007). mRMR Feature Selection Toolbox for
                                                                       MATLAB, http://penglab.janelia.org/proj/mrmr/. Retrieved
                    Acknowledgements                                   from http://penglab.janelia.org/proj/mRMR/
                                                                     Salvucci, D. D., & Goldberg, J. H. (2000). Identifying fix-
We would like to thank Torsten Jachmann and Laura Faust for            ations and saccades in eye-tracking protocols. In Proc.
carrying out the annotations and Ross Macdonald for valu-              ETRA (pp. 71–78).
able discussions. Many thanks also to very helpful and en-           Schmid, H. (1995). Improvements in part-of-speech tagging
couraging anonymous reviewers. This work was funded by                 with an application to German. In In proceedings of the acl
the Cluster of Excellence on “Multimodal Computing and                 sigdat-workshop (pp. 47–50).
Interaction” of the German Excellence Initiative and by the          Staudte, M., Koller, A., Garoufi, K., & Crocker, M. W.
German Ministry of Education and Research (project MO-                 (2012). Using listener gaze to augment speech generation
BIDA—AD; grant number 01IS12050).                                      in a virtual 3D environment. In Proceedings of the 34th An-
                                                                       nual Conference of the Cognitive Science Society. Sapporo,
                          References                                   Japan.
Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008,                Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M.,
  November).         Mixed-effects modeling with crossed               & Sedivy, J. C. (1995). Integration of visual and linguistic
  random effects for subjects and items. Journal of Mem-               information in spoken language comprehension. Science,
  ory and Language, 59(4), 390–412. Retrieved from                     268, 1632–1634.
  http://dx.doi.org/10.1016/j.jml.2007.12.005                        Tessendorf, B., Bulling, A., Roggen, D., Stiefmeier, T., Feil-
  doi: 10.1016/j.jml.2007.12.005                                       ner, M., Derleth, P., & Tröster, G. (2011). Recognition of
Bulling, A., & Roggen, D. (2011). Recognition of Visual                hearing needs from body and eye movements to improve
  Memory Recall Processes Using Eye Movement Analysis.                 hearing instruments. In Proc. Pervasive (pp. 314–331).
  In Proc. UbiComp (p. 455-464).                                     Yantis, S., & Jonides, J. (1990). Abrupt visual onsets and
Bulling, A., Ward, J. A., Gellersen, H., & Tröster, G. (2011).        selective attention: Voluntary versus automatic allocation.
  Eye Movement Analysis for Activity Recognition Using                 Journal of Experimental Psychology: Human Perception
  Electrooculography. IEEE Transactions on Pattern Anal-               and Performance, 16, 121–134.
  ysis and Machine Intelligence (PAMI), 33(4), 741-753.
Clark, H. H., & Krych, M. A. (2004, January). Speak-
  ing while monitoring addressees for understanding. Jour-
  nal of Memory and Language, 50(1), 62–81.                 doi:
  10.1016/j.jml.2003.08.004
Cooper, R. M. (1974). The control of eye fixation by the
  meaning of spoken language: A new methodology for the
  real-time investigation of speech perception, memory, and
  language processing. Cognitive Psychology, 6(1), 84-107.
                                                                 1158

