            Lateral Inhibition Overcomes Limits of Temporal Difference Learning
                                                    Jacob Rafati & David C. Noelle
                                    (jrafatiheravi@ucmerced.edu, dnoelle@ucmerced.edu)
                                            Computational Cognitive Neuroscience Laboratory
                                                       University of California, Merced
                                                             5200 North Lake Road
                                                           Merced, CA 95343 USA
                              Abstract                                    it is necessary to encode the agent’s learned value function,
                                                                          mapping from sensory state features to an expectation of fu-
   There is growing support for Temporal Difference (TD) Learn-
   ing as a formal account of the role of the midbrain dopamine           ture reward, using some form of function approximator. For-
   system and the basal ganglia in learning from reinforcement.           mally, this function approximator is a parameterized equa-
   This account is challenged, however, by the fact that realistic        tion that maps from state to value, where the parameters can
   implementations of TD Learning have been shown to fail on
   some fairly simple learning tasks — tasks well within the ca-          be constructively optimized based on the experiences of the
   pabilities of humans and non-human animals. We hypothesize             agent. One common function approximator is an artificial
   that such failures do not arise from natural learning systems          neural network, with the parameters being the connection
   because of the ubiquitous appearance of lateral inhibition in
   the cortex, producing sparse conjunctive internal representa-          weights in the network. Such a network, adapted using the
   tions that support the learning of predictions of future reward.       backpropagation of error learning method (Rumelhart, Hin-
   We provide support for this conjecture through computational           ton, & Williams, 1986), was used in the previously mentioned
   simulations that compare TD Learning systems with and with-
   out lateral inhibition, demonstrating the benefits of sparse con-      Backgammon playing program (Tesauro, 1995). As illus-
   junctive codes for reinforcement learning.                             trated by this program, TD Learning with an artificial neural
   Keywords: reinforcement learning; lateral inhibition; sparse           network approximating the value function, can solve appar-
   conjunctive codes; computational cognitive neuroscience                ently complex tasks. Using a function approximator to learn
                                                                          the value function has the added benefit of potentially sup-
                           Introduction                                   porting generalization by including a bias toward mapping
Humans and non-human animals are capable of learning                      similar sensory states to similar predictions of future reward.
highly complex skills by reinforcing appropriate behaviors                   Surprisingly, some tasks that superficially appear very sim-
with reward. The midbrain dopamine system has long been                   ple cannot be perfectly mastered using this method. For ex-
implicated in reward-based learning (Schultz, Apicella, &                 ample, learning to navigate to a goal location in a simple
Ljungberg, 1993), and the information processing role of                  two-dimensional space in which there are obstacles has been
dopamine in learning has been well described by a class                   shown to pose a substantial challenge to TD Learning using
of reinforcement learning algorithms called Temporal Differ-              a backpropagation neural network (Boyan & Moore, 1995).
ence (TD) Learning (Montague, Dayan, & Sejnowski, 1996;                   Note that the proofs of convergence to optimal performance
Schultz, Dayan, & Montague, 1997). While TD Learning, by                  depend on the agent maintaining a potentially highly discon-
itself, certainly does not explain all observed reinforcement             tinuous value function in the form of a large look-up table, so
learning phenomena, increasing evidence suggests that it is               the use of a function approximator for the value function vio-
key to the brain’s adaptive nature (Dayan & Niv, 2008).                   lates the assumptions of those formal analyses. Still, it seems
   Beyond empirical support for the TD Learning account of                unusual that this approach to learning can succeed at some
biological reinforcement learning, the power of this learn-               difficult tasks but fail at some fairly easy tasks.
ing method suggests that it may be capable of explaining the                 The power of TD Learning to explain biological reinforce-
neural basis of the successful learning of even fairly com-               ment learning is greatly reduced by this observation. If TD
plex tasks (Sutton & Barto, 1998). This algorithm can learn               Learning fails at simple tasks that are well within the reach
elaborate decision making skills, such as playing the game                of humans and non-human animals, then it cannot be used to
of Backgammon at the Grand Master level (Tesauro, 1995).                  explain how the dopamine system supports such learning.
There are even proofs that TD Learning will converge to op-                  In this paper, we demonstrate how incorporating a ubiqui-
timal performance, given enough experience (Dayan, 1992).                 tous feature of biological neural networks into the artificial
   Despite these strengths, a mystery remains. There are some             neural networks used to approximate the value function can
relatively simple reinforcement learning problems for which               allow TD Learning to succeed at simple tasks that have pre-
TD Learning has been shown to fail (Boyan & Moore, 1995).                 viously challenged it. Specifically, we show that the incorpo-
These problems arise when the space of possible sensory                   ration of lateral inhibition, producing competition between
states of the learning agent is so large that it is intractable           neurons so as to produce sparse conjunctive representations,
to store the agent’s learned assessment of the value or qual-             can produce success in learning to approximate the value
ity of each state (i.e., its expectation of future reward, given          function using an artificial neural network, where only fail-
that it is in that state) in a large look-up table. In these cases,       ure had been previously found. Thus, through computational
                                                                     1931

simulation, we provide preliminary evidence that lateral in-          riety of different value function approximators, including a
hibition in the brain may help compensate for a weakness of           backpropagation network with a single hidden layer, but none
TD Learning, further buttressing the TD Learning account of           of them converged on a good solution to the problem. Indeed,
dopamine-based reinforcement learning.                                as the agent continued to explore the environment, the esti-
   In the remainder of this paper, we initially provide some          mate of future reward for locations kept changing, failing to
background concerning the reported failure of TD Learning             settle down to a fixed value function.
on fairly simple problems. We then provide details concern-              This observation suggests that the difficulty in learning this
ing our computational simulations of TD Learning with lat-            problem arises from a specific feature of reinforcement learn-
eral inhibition included. The results of these simulations,           ing. In particular, the value function that the function ap-
comparing the performance of our approach to previously ex-           proximator is trying to learn is, in a way, a moving target.
amined methods, are then described. We close with a discus-           Early in training, when the agent is unlikely to make it to
sion of these results and ideas for future work.                      the goal location, the expected future reward for a given lo-
                                                                      cation might be quite low. Later, if the agent has had some
                       Background                                     successes, the value for the same location might be higher.
Consider a very simple two-dimensional “grid world” envi-             If the value function is stored in a large look-up table, then
ronment that remains static over time. A reinforcement learn-         adjusting the value of one location has no influence on the
ing agent in this environment may be faced with the choice, at        values associated with other locations, allowing for small in-
each time step, to move a fixed distance either North, South,         cremental changes in the value function. When using a func-
East, or West. On each time step, the agent receives mild             tion approximator, however, adjusting parameters (e.g., back-
negative reinforcement, until it moves to a goal location in          propagation network connection weights) for one location
the Northeast corner of the space, at which point the agent           will likely change the value assigned to many other locations,
is relieved of negative reinforcement. Further, imagine that          potentially causing the un-learning of appropriate values for
this environment contains “puddles” through which the agent           those other locations. This is a reasonable hypothesis for the
can move, but moving into puddle locations produces ex-               observed lack of convergence.
tremely strong negative reinforcement. Finally, consider the             In the following year, Sutton (1996) showed that this task
case in which the agent can perfectly sense its location in the       could be learned by a TD Learning agent by hard-wiring the
grid environment (i.e., its Cartesian coordinates), but it oth-       hidden layer units of the backpropagation network used to
erwise has no senses. This situation is illustrated in Figure 1.      learn the value function to implement a fixed sparse conjunc-
Equipped with TD Learning, using a function approximator              tive (coarse) code of the agent’s location. The specific encod-
to learn the value function, could such an agent learn to avoid       ing used was one that had been previously proposed in the
the puddles and move rapidly to the goal location, after sub-         CMAC model of the cerebellum (Albus, 1975). Each hid-
stantial experience in the environment?                               den unit would become active only when the agent was in a
                                                                      location within a particular range of x values and within a
                    Puddle world task                                 particular range of y values. The conjoining of conditions on
            1                                                         both coordinates is what made this code “conjunctive” in na-
                                                                      ture. Also, for any given location, only a small fraction of
         0.8                                                          the hidden units displayed non-zero activity. This is what it
                                                                      means for the hidden representation to be a “sparse” code.
         0.6                                                          Locations that were close to each other in the environment
     y                                                                produced more overlap in the hidden units that were active
         0.4                                                          than locations that were separated by a large distance. By en-
                                                                      suring that most hidden units had zero activity when connec-
         0.2                                                          tion weights were changed, this approach kept changes to the
                                                                      value function in one location from having a broad impact on
            0                                                         the expected future reward at distant locations. (In the back-
                0   0.2     0.4       0.6   0.8     1
                                                                      propagation of error learning algorithm, a connection weight
                                  x                                   is changed in proportion to the activity on the sending side of
Figure 1: The agent receives −1 reward on each time step              that connection, so there is no change if there is no activity
until it reaches the goal in the Northeast corner. The agent          being sent.) By engineering the hidden layer representation,
moves a distance of 0.05 either North, South, East, or West           this reinforcement learning problem was solved.
on each time step. Entering a puddle produces a reward of                This is not a general solution, however. If the same ap-
(−400 × d), where d is the shortest distance to a puddle edge.        proach was taken for another reinforcement learning problem,
                                                                      it is quite possible that the CMAC representation would not
  Boyan and Moore (1995) provided simulation evidence                 be appropriate. Thus, the method proposed by Sutton (1996)
suggesting that such learning is impossible. They tried a va-         does not help us understand how TD Learning might flexi-
                                                                   1932

bly learn a variety of reinforcement learning tasks. This ap-         nection weight changes recommended by the backpropaga-
proach requires prior knowledge of the kinds of internal rep-         tion procedure may slightly deviate from those which would
resentations of sensory state that are easily associated with         lead to local error minimization in this network. We opted to
expected future reward, and there are simple learning prob-           ignore this discrepancy, however, trusting that a sufficiently
lems for which such prior knowledge is unavailable.                   small learning rate would keep these deviations small. Sec-
   We hypothesize that the key feature of the Sutton (1996)           ond, it is worth noting that this particular kWTA mechanism
approach is that it produces a sparse conjunctive code of the         allows for a distributed pattern of activity over the hidden
sensory state. Representations of this kind need not be fixed,        units. This provides the learning algorithm with some flex-
however, but might be learned at the hidden layers of neu-            ibility, allowing for a graded range of activation levels when
ral networks. Computational cognitive neuroscience models             doing so reduces network error. As connection weights from
have shown that a combination of feedforward and feedback             the inputs to the hidden units grow in magnitude, however,
inhibition naturally produces sparse conjunctive codes over           this mechanism will drive the activation of the top k hidden
a collection of excitatory neurons (O’Reilly & Munakata,              units closer to 1 and the others closer to 0. Indeed, an exam-
2001). Such patterns of lateral inhibition are ubiquitous in the      ination of the hidden layer activation patterns in the kWTA-
mammalian cortex (Kandel, Schwartz, Jessell, Siegelbaum,              equipped networks used in this study revealed that the k win-
& Hudspeth, 2012). Importantly, networks containing such              ning units consistently had activity levels close to the maxi-
lateral inhibition can still learn to represent input information     mum possible value, once the learning process was complete.
in different ways for different tasks (O’Reilly & Munakata,              Our reinforcement learning agent used this neural network,
2001), retaining flexibility while producing the kind of sparse       with kWTA, as an adaptive value function approximator. We
conjunctive codes that may support reinforcement learning.            used a version of TD Learning called SARSA (Sutton &
                                                                      Barto, 1998), which calculates a separate prediction of ex-
                   Simulation Methods                                 pected future reward for each action that might be taken from
                                                                      the current state. Thus, the value function can be formalized
In order to assess our hypothesis that biasing a neural network       as Q(s, a), where s is the current sensory state (i.e., the loca-
toward learning sparse conjunctive codes for sensory state in-        tion of the agent expressed as an < x , y > coordinate pair), and
puts will improve TD Learning when using a value function             a is a considered next action (i.e., one of North, South, East,
approximator, we constructed a variant of a backpropagation           and West). The value of Q(s, a) is then the expected future
network with a single hidden layer in which the number of             reward at state s when a will be the next action, and future ac-
hidden units that could be highly active at any one time was          tions will be those, at each future state, that maximize the ex-
restricted. Like some computational cognitive neuroscience            pected future reward at that state (i.e., argmaxa Q(s, a)). The
models of lateral inhibition (O’Reilly & Munakata, 2001),             expected future reward value is discounted, so that rewards
we implemented this through a k-Winners-Take-All (kWTA)               that are received soon are weighted more heavily than rewards
mechanism akin to pooled lateral inhibition. After calculat-          that are received in the distant future. The amount of dis-
ing the net input values of hidden units based on the network         counting is controlled by a discounting parameter, γ ∈ (0, 1],
inputs (i.e., the weighted sum of the inputs), we identified a        such that the expected future reward at time t is:
single scalar amount of inhibition (i.e., negatively weighted
                                                                                                m
input) that, when added to all of the hidden unit net input val-                               X
ues, would result in the k hidden units with the highest net in-                                    γ k r(t + k)
                                                                                               k =0
put values to have adjusted net input values that were positive,
while all hidden units with lower net input values would be           Here, r(t) is the instantaneous reward received at time t (see
adjusted to become negative. These adjusted net input values          below for specific values), and m is either the number of time
were transformed into unit activation values using a logistic         steps remaining until the goal is reached or, if the goal is not
sigmoid activation function (gain of 1, offset of −1), resulting      reached, a time step maximum (such that t + m = 80; about
in hidden unit activation values in the range between 0.0 and         twice the time needed to get from any point in the environ-
1.0, with the top k units having activations above 0.27 (due          ment to any other). For these simulations, temporally dis-
to the −1 offset) and the “losing” hidden units having activa-        tal rewards were fairly highly weighted by using a value of
tions below that value. The k parameter controlled the degree         γ = 0.99. The backpropagation network was expected to learn
of sparseness of the hidden layer activation patterns, with low       an approximation of the Q(s, a) function, Q̂(s, a), where the
values producing more sparsity (i.e., fewer hidden units with         state, s, is provided as input to the network, and there is one
high activations). In the simulations reported here, we set k         output for each action, a, specifying Q̂(s, a) for the pair.
to be 10% of the total number of hidden units.                           For the puddle world task, the x-coordinate and the y-
   In addition to encouraging sparse representations, this            coordinate of the current state, s, were presented to the neural
kWTA mechanism has two properties that are worthy of note.            network over two separate pools of input units. Note that
First, introducing this mechanism violates some of the as-            these coordinate values were in the range [0, 1], as shown in
sumptions needed to formally relate the backpropagation of            Figure 1. Each pool of input units consisted of 21 units, with
error procedure to gradient descent in error. Thus, the con-          each unit corresponding to a coordinate location between 0
                                                                  1933

and 1, inclusive, in increments of 0.05. To encode a coor-           backpropagation network implementing the value function.
dinate value for input to the network, a Gaussian distribu-          The network was given the input corresponding to s, and ac-
tion with a peak value of 1, a standard deviation of 0.05, and       tivation was propagated through the network. Each output
a mean equal to the given continuous coordinate value was            unit then received an error value. This error was set to zero
used to calculate the activity of each of the 21 input units.        for all output units except for the unit corresponding to the
For example, for a coordinate value of 0.15, the input unit          action that was taken, a. The selected action unit received an
corresponding to this value was set to a maximum activation          error signal equal to the TD Error, δ. These error values were
of 1 and input units corresponding to coordinate values above        then backpropagated through the network, using the standard
and below 0.15 were set to activity levels that decreased with       backpropagation of error algorithm (Rumelhart et al., 1986),
distance from 0.15 according to the Gaussian function.               and connection weights were updated (with a low learning
     The network had four output units, each corresponding to        rate, α, of 0.005). This process was then begun again, start-
one of the four directions of motion. Between the 42 in-             ing at location s 0 and taking action a 0 .
put units and the 4 output units was a layer of 220 hidden              The agent explored the puddle world environment in
units. The hidden layer was subject to the previously de-            episodes. Each episode began with the agent being placed
scribed kWTA mechanism, parameterized so as to allow 10%,            at a location within the environment sampled uniformly at
or 22, of the hidden units to be highly active. The hidden units     random. Actions were then taken, and connection weights
used a logistic sigmoid activation function on net input values      updated, as described above. The episode ended when the
that were adjusted to allow only 22 units to be highly active        agent reached the goal location or after the maximum of 80
at any one time. The output units used a linear activation           actions had been taken. At the beginning of a simulation, the
function (i.e., their activation was equal to their net input).      exploration probability, , was set to a relatively high value
There was complete connectivity between the input units and          of 0.1, and it remained at this value for much of the learning
the hidden units and between the hidden units and the out-           process. Once the average magnitude of δ over an episode
put units, with all connection weights initialized to uniformly      fell below 0.2, the value of  was reduced by 0.1% each time
sampled random values in the range [−0.05, 0.05].                    the goal location was reached. Thus, as the agent became
     Following the SARSA version of TD Learning, the rein-           increasingly successful at reaching the goal location, the ex-
forcement agent was controlled in the following way. The             ploration probability,  , approached zero. (Annealing the ex-
current location of the agent, s, was provided as input to the       ploration probability is commonly done in systems using TD
neural network, producing four output activation values. With        Learning.) The agent continued to explore the environment,
a small exploration probability, , these values were ignored,       one episode after another, until the average absolute value
and an action was selected uniformly at random from the four         of δ was below 0.01 and the goal location was consistently
cardinal directions. (The value of  is discussed further, be-       reached, or a maximum of 44, 100 episodes had been com-
low.) Otherwise, the output unit with the highest activation         pleted. (This value was heuristically selected as a function of
level determined the action, a, to be taken. The agent then          the size of the environment: (21 × 21) × 100 = 44, 100.)
moved a distance of 0.05 in the direction specified by the se-          When this reinforcement learning process was complete,
lected action, placing it in a new state, s 0 .                      we examined both the behavior of the agent and the degree
     At this point, the agent received a reward signal, r, based     to which its value function approximations, Q̂(s, a), matched
on its current location, s 0 . This value was −1 for most of the     the correct values, Q(s, a), where the correct values were de-
environment, but it had a higher value, 0, at the goal location      termined by running SARSA to convergence while using a
in the Northeast corner. If the agent was currently located          large look-up table to capture the value function.
in a puddle, the reward signal was calculated as (−400 × d),
where d is the shortest distance from the current location to                             Simulation Results
the edge of the puddle. Finally, the agent received a reward
signal of −2 if it had just attempted to leave the square envi-      We compared the performance of our kWTA neural network
ronment. Thus, the agent was “punished” for being anywhere           with that produced by using a standard backpropagation net-
except the goal location, and it was severely “punished” for         work with identical parameters. We also examined the per-
entering a puddle. This pattern of reinforcement was selected        formance of a “linear” network, which had no hidden units
to parallel that used in Sutton (1996).                              but only complete connections from all input units directly to
     The action selection process was then repeated at location      the four output units. Twenty simulations were conducted for
s 0 , determining a subsequent action, a 0 . Before this action      each of these three neural network architectures.
was taken, however, the neural network value function ap-               Figure 3 shows the value function (plotted as maxa Q̂(s, a)
proximator had its connection weights updated according to           for each location, s) for representative networks of each kind.
the SARSA Temporal Difference (TD) Error:                            Also displayed are selected actions at a grid of locations. Fi-
                                                                     nally, we show learning curves displaying the episode average
                                                                     value of the TD Error, δ, over episodes.
                                        
                 δ = r + γ Q̂(s 0 , a 0 ) − Q̂(s, a)
                                                                        In general, the linear network did not consistently learn to
The TD Error, δ, was used to construct an error signal for the       solve this problem, sometimes failing to reach the goal or
                                                                 1934

choosing paths through puddles. The backpropagation net-             sparse codes over their hidden units by including a process
work performed much better, but its value function approxi-          akin to the sort of pooled lateral inhibition that is ubiqui-
mation still contained sufficient error to produce a few poor        tous in the cerebral cortex.1 In this way, these simulation re-
action choices. The kWTA network, in comparison, consis-             sults lend preliminary support to the hypothesis that the mid-
tently converged on good approximations of the value func-           brain dopamine system does, indeed, implement a form of
tion, almost always resulting in optimal paths to the goal.          TD Learning, and the observed problems with TD Learning
                                                                     do not arise in the brain due to the encoding of sensory state
                    40            *                                  information in circuits that make use of lateral inhibition.
    MSE of Reward
                                           *
                                                                        This work was prompted by the failures of TD Learning
                    30
                                                                     on some simple machine learning tasks, reported by Boyan
                    20                                               and Moore (1995). It is interesting to note that our standard
                                                                     backpropagation network performance results are much bet-
                    10                                               ter than the analogous results reported by those researchers.
                                                                     It is not clear if this discrepancy involves subtle differences
                     0                                               in learning parameters, arises from our scheme for encoding
                         Linear       BP       kWTA                  coordinate values as inputs to the neural networks, or was
Figure 2: Averaged over 20 simulations of each network type,         caused by some other unknown factor. While our simulations
these columns display the mean squared deviation of accu-            demonstrate clear benefits from using the kWTA mechanism,
mulated reward from that of optimal performance. Error bars          the performance of our backpropagation networks was not as
show one standard error of the mean.                                 bad as what was previously reported in the literature.
                                                                        We are currently extending this work in two primary di-
   For each network, we assessed the quality of the paths pro-       rections. First, we are applying our kWTA value function
duced. For each simulation, we froze the connection weights,         approximator to other reinforcement learning problems that
and we produced an episode for each possible starting loca-          have posed difficulties for TD Learning. The first of these is
tion, except for locations inside of the puddles. The reward         the “mountain car” control problem, which involves a value
accumulated over each episode was recorded, and, for each            function with difficult non-linearities (Sutton & Barto, 1998).
episode that ended at the goal location, we calculated the sum       Second, while the work reported here focuses on improv-
squared deviation of these accumulated reward values from            ing machine learning methods, we are also interested in ap-
that produced by an optimal agent (constructed using SARSA           proaches that more closely match computational cognitive
with a look-up table for the value function). The mean of this       neuroscience models of feedforward and feedback inhibition
error measure, over all successful episodes, was recorded for        in the brain, as well as more biologically plausible learning
each of the 20 simulations run for each of the 3 network types.      procedures (O’Reilly & Munakata, 2001).
Figure 2 shows the means of these values, over 20 simula-
tions. The backpropagation network had significantly less er-                            Acknowledgments
ror than the linear network (t(38) = 4.692; p < 0.001), and the
                                                                     The authors thank the Computational Cognitive Neuroscience
kWTA network had significantly less error than the standard
                                                                     Laboratory (CCNL) at the University of California, Merced
backpropagation network (t(38) = 7.314; p < 0.001). On av-
                                                                     (UCM), as well as five anonymous reviewers, for their feed-
erage, the kWTA network deviated from optimal performance
                                                                     back. We are grateful for travel support from the UCM
by less than one reward point.
                                                                     School of Engineering, awarded to the first author.
   We also recorded the fraction of episodes which succeeded
at reaching the goal for each simulation run. The mean rate                                    References
of goal attainment, across all non-puddle starting locations,
for the linear network, the backpropagation network, and the         Albus, J. S. (1975). A new approach to manipulator con-
kWTA network were 93.3%, 99.0%, and 99.9%, respectively.               trol: The cerebellar model articulation controller CMAC.
Despite these consistently high success rates, the linear net-         Journal of Dynamic Systems, Meaasurement, and Control,
work exhibited significantly more failures than the backprop-          97(3), 220–227.
agation network (t(38) = 2.306; p < 0.05), and the backprop-         Boyan, J. A., & Moore, A. W. (1995). Generalization in rein-
agation network exhibited significantly more failures than the         forcement learning: Safely approximating the value func-
kWTA network (t(38) = 2.138; p < 0.05).                                tion. In G. Tesauro, D. S. Touretzky, & T. K. Leen (Eds.),
                                                                       Advances in neural information processing systems 7 (pp.
    Conclusions, Discussion, & Future Work                             369–376). Cambridge, MA: MIT Press.
These simulation results demonstrate that a mechanism for            Dayan, P. (1992). The convergence of TD(λ) for general λ.
learning sparse conjunctive codes for the agent’s sensory state        Machine Learning, 8, 341–362.
can help overcome learning problems observed when using                 1 Of course, sparse codes can be produced in other ways, as well,
TD Learning with a value function approximator. Artifi-              such as introducing a “sparseness constraint” regularization term to
cial neural networks can be biased toward producing such             the learning process (French, 1991; Hoyer, 2004).
                                                                  1935

                             Estimate of Value Function                          Policy map                      Mean value of δ allover each episode
       Linear                                                     1                                              0
                                                                                                                 -1
                           100
                                                                                                       mean(δ)
                                                                                                                 -2
                  −Qmax
                                                            y   0.5
                            50                                                                                   -3
                             0                          1                                                        -4
                             0                    0.5             0
                                  0.5                                                                            -5
                                                    y                 0              0.5        1                     0   10,000 20,000 30,000 40,000
                                  x       1 0                                        x                                           Episodes
                             Estimate of Value Function                          Policy map                      0
       BP                                                             1
                                                                                                                 -1
                                                                                                       mean(δ)
                                                                                                                 -2
                  −Qmax
                           100                              y    0.5
                            50                                                                                   -3
                                                        1
                             0                                                                                   -4
                             0                    0.5
                                  0.5               y                 0                                          -5
                                                                          0          0.5       1                      0   10,000 20,000 30,000 40,000
                                    x      1 0                                       x                                           Episodes
                                                                                 Policy map
                             Estimate of Value Function          1                                               Mean value of δ allover each episode
       kWTA                                                                                                      0
                                                                0.8
                                                                                                                 -1
                                                                0.6
                                                                                                       mean(δ)
                                                                                                                 -2
                   −Qmax
                                                            y
                           100
                                                                0.4
                            50                                                                                   -3
                                                        1
                             0                                  0.2
                                                                                                                 -4
                             0                    0.5
                                  0.5                            0                                               -5
                                                    y                 0              0.5           1                  0   10,000 20,000 30,000 40,000
                                   x       1 0                                       x                                           Episodes
Figure 3: The performance of various learned value function approximators may be compared in terms of their success at
learning the true value function, the resulting action selection policy, and the amount of experience in the environment needed
to learn. The approximated value functions, expressed as maxa Q̂(s, a) for each location, s, appears on the left. The ac-
tions selected at a grid of locations is shown in the middle column. The learning curve, showing the TD Error over learning
episodes, is shown on the right. The rows display results for representative neural networks of the three kinds explored: linear,
backpropagation, and kWTA, respectively, from top to bottom.
Dayan, P., & Niv, Y. (2008). Reinforcement learning: The                           sachusetts: MIT Press.
  good, the bad and the ugly. Current Opinion in Neurobiol-                      Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).
  ogy, 18, 185–196.                                                                Learning representations by back-propagating errors. Na-
French, R. M. (1991). Using semi-distributed representations                       ture, 323, 533–536.
  to overcome catastrophic forgetting in connectionist net-                      Schultz, W., Apicella, P., & Ljungberg, T. (1993). Responses
  works. In Proceedings of the 13th annual cognitive science                       of monkey dopamine neurons to reward and conditioned
  society conference (pp. 173–178). Hillsdale, NJ: Lawrence                        stimuli during successive steps of learning a delayed re-
  Erlbaum.                                                                         sponse task. Journal of Neuroscience, 13, 900–913.
Hoyer, P. O. (2004). Non-negative matrix factorization with                      Schultz, W., Dayan, P., & Montague, P. R. (1997). A neu-
  sparseness constraints. CoRR, cs.LG/0408058. Retrieved                           ral substrate of prediction and reward. Science, 275(5306),
  from http://arxiv.org/abs/cs.LG/0408058                                          1593–1599.
Kandel, E., Schwartz, J., Jessell, T., Siegelbaum, S., & Hud-                    Sutton, R. S. (1996). Generalization in reinforcement learn-
  speth, A. J. (2012). Principles of neural science (Fifth                         ing: Successful examples using sparse coarse coding. In
  Edition ed.). New York: McGraw-Hill.                                             D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.),
Montague, P. R., Dayan, P., & Sejnowski, T. J. (1996). A                           Advances in neural information processing systems 8 (pp.
  framework for mesencephalic dopamine systems based on                            1038–1044). Cambridge, MA: MIT Press.
  predictive hebbian learning. Journal of Neuroscience, 16,                      Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning:
  1936-1947.                                                                       An introduction. Cambridge, MA: MIT Press.
O’Reilly, R. C., & Munakata, Y. (2001). Computational                            Tesauro, G. (1995). Temporal difference learning and TD-
  explorations in cognitive neuroscience. Cambridge, Mas-                          Gammon. Communications of the ACM, 38(3).
                                                                              1936

