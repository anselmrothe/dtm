                           Preferred Inferences in Causal Relational Reasoning:
                                                Counting Model Operations
                                              Marco Ragni (ragni@cs.uni-freiburg.de)
                                    Foundations of AI, Technical Faculty, University of Freiburg and
                          Justus Liebig University, Experimental Psychology and Cognitive Science, Giessen
                                   Stephanie Schwenke (schwenke@neptun.uni-freiburg.de)
                                           Center for Cognitive Science, University of Freiburg
                                     Christine Otieno (otieno@psychologie.uni-freiburg.de)
                                            Department of Psychology, University of Freiburg
                               Abstract                                  tory (point B), the old tannery (point A), something even far-
   Interpreting causal relations plays an important role in every-       ther upstream? How can the townspeople determine what is
   day life, for example in scientific inquiries and text compre-        polluting their stream? What areas should the townspeople
   hension. Errors in causal reasoning can be a recipe for disaster.     investigate to glean the most information? If the new tannery
   Despite vast literature on the psychology of human causal rea-
   soning, there are few investigations into preferred inferences        (at A) caused the pollution at the park (point D), then it prob-
   in relational three-term problems. Based on a previous formal         ably also caused the duck family to leave their home (at point
   investigation about relevant causal relations we develop a cog-       C). However, the pollution is probably not to blame for the
   nitive modeling approach with mental models. The key prin-
   ciple for this approach proves to be the prediction of preferred      sheep missing from the green (point G, downstream from the
   inferences by model operations and the process of sub model           factory). Why? If you walk upstream from the park or the
   integration. Subsequent experiments test preferred inferences,        ducks’ home, you’ll eventually get to the same point. Walk-
   the number of model operations, and if concrete or generic
   problems make a difference in causal reasoning performance.           ing upstream from the green, will get you somewhere else.
   Implications of the model are discussed.                                 When reasoning about causes and effects, elements can
                                                                         have various relationships to one another. One element may
                           Introduction                                  cause another (point A comes before point D in the river ex-
Causal reasoning has often been studied in both Artificial In-           ample; represented by ≺); conversely, one element may be
telligence and with human reasoners (e.g., Russell & Norvig,             caused by another (point D comes after point A; represented
2003). How we, as humans, reason and draw inferences is                  by ) – these are called dependent relations. Another type of
quite different from classical AI approaches. Nevertheless,              dependency, like we saw with points D and C, is also think-
being able to correctly interpret causal relations is an impor-          able; we call this relationship in which two elements share
tant everyday skill. For example, a production manager must              a common ancestor or cause fork (and represent it with f).
be able to understand the complex interdependencies of sup-              Two elements can also be independent of each other (like
ply chains in order to identify delivery bottlenecks in time.            points B and D in the example, represented by ). There are
This way solutions can be found before serious supply prob-              cases in which not all dependencies are known. For example
lems occur in the production process. Consider another ex-               we can imagine a fishing hole some ways distant from our
ample: A clear stream flows through green countryside, its               stream and ask if the fishing hole is connected to the stream
route is winding going this way and that, even splitting even-           by water flowing underground? In these instances imprecise
tually into two creeks, as shown in Figure 1.                            knowledge is described by unions of basic relations (in our
                                                                         example the fishing hole is  or  of the tannery). Depen-
                                                                         dencies of probabilities (when observations depend on each
                                            D
                                                        F                other, as with our stream) are often described by a Bayesian
              A                                                          network (Pearl, 2000). Looking at the structure of the net-
                          C                    E                         work can reveal to us whether two random variables are in-
                                                                         dependent. Orders (relationships between elements) may re-
                                                                         main partly unspecified between elements when it does not
              B                   G
                                                                         matter. For example, we may not know whether the fishing
                                                                         hole is downstream, upstream or not at all connected to the
                                                                         park, but it doesn’t really matter to us either.
                  Figure 1: A river flow example.
                                                                            In other cases the specific order is vital and is necessary to
   One day people begin to notice dying cattails and reeds               deduce the hidden truth. Causal networks are not just impor-
while picnicking at the park (point D). Downstream a farmer              tant for deducing sources of pollution, complex dependencies
whose pasture (point F) is divided by the stream notices that            also play a role in identifying delivery bottlenecks in supply
several of his cows are sick. Something has polluted the wa-             chain networks, minimizing delays in railways systems, and
ter. There is much speculation about the culprit: A new fac-             in inhibiting the spread of diseases. This last example pro-
                                                                     1937

               Table 1: Relations of causal reasoning and their partial order definitions (cf. Ragni and Scivos, 2005).
                Relation                    Name                    Partial Order
                X ≺V                      X causes V                X ≤ V and not V ≤ X
                X V                   X depends on V               X ≤ V and not V ≤ X
                X fV        X and V have only a common cause ∃C C ≤ V ∧C ≤ X and neither X ≤ V nor V ≤ X
                X V              X and V are independent           neither ∃C C ≤ V ∧C ≤ X nor X ≤ V nor V ≤ X
          Note. The relation “has a common cause” requires the introduction of an additional variable which we call C.
vides a situation in which specific orders are important: To          relations and identify preferences in the next section. We will
stop a contagious disease it is desirable to identify a patient       conclude by summarizing our results and raise questions left
zero (the origin of the disease) and also know whether pa-            open.
tients have had contact with one another. However, this is not
always clear. Reasoning based on known dependencies can                 Causal reasoning, partial orders, and theories
help detect formerly unknown causes or rule out possibilities,        Causal relations can be formally described by an extension
thus limiting their number.                                           of a partial order: A partial order is a relation ≤ that satisfies
   While causal reasoning has often been examined with the            the following three properties for any x, v, z: reflexivity (x ≤ x),
assumption of base rates or probabilities, these are not always       antisymmetry (if x ≤ v and v ≤ x, then x = v), and transitivity
given; in the example above the townspeople do not know               (if x ≤ v and v ≤ z, then x ≤ z).1
how probable it is that their stream will become poisoned, that          We can base our interpretation of causal relations on the
cows will get sick because of pollution, etc. Syllogistic and         notion of partial order relations. We consider four basic re-
relational reasoning have often been studied using three term         lations, which we denote by ≺, , f, . If x, v are points
series problems. This paradigm can be extended to causal rea-         in a partial order hT, ≤i, then we define these relations in
soning. Consider the problem that emerges looking at some             terms of the partial order as follows: Semantically, a con-
of the facts the townspeople have (the letters representing the       straint v1 R v2 holds in a partial order (T, ≤) that complies
facts are used here for the sake of simplicity):                      with this definition. All relations between nodes in Fig. 1
        G and C are independent of each other.                        can be described by these basic relations. To describe trees,
        C and D have a common cause.                                  the relations {≺, , f} are sufficient. Therefore, whenever
                                                                      the relation  occurs, the graph cannot be a tree and requires
   May we assume that G and D are independent of each
                                                                      separate representations.
other? The answer(s) to this problem, and how the average
                                                                         It is possible to show that reasoning with such causal rela-
townsperson might solve it are explained in depth in the next
                                                                      tions {≺, , f, } forms a relation algebra (Ladkin & Reine-
section. Before moving on, let us consider an AI approach
                                                                      feld, 1992), i.e., it contains all unions, intersections, and com-
to the problem. In order for reasoning to be automated, a
                                                                      plements of a set of basic relations, and composition and in-
calculus that expresses the relation between a pair of nodes
                                                                      verse operations (Ragni & Scivos, 2005).
is needed to help distinguish between possibly affected areas
and those that are unaffected. Do such efficient algorithms              Transitive inferences are represented by applying the com-
exist that can detect causes and implications, discover depen-        position operator on two premises for the four causal relations
dencies and find an order compliant with a given specification        ≺, , f,  and the associated composition as shown in Table
(i.e., was it actually necessary for the townspeople to invest so     2. An interesting question is whether the relation “indepen-
much time in investigation, or could a computer have solved           dent” () provides additional complexity in comparison to
their problem for them)? One such calculus, the dependency-           classical partial orders.
calculus has been formally investigated. It can represent             A computational model
and deduce knowledge about dependencies and causal rea-
soning and can be formally grounded by extending the lan-             The graphical representation of causal relations (e.g., Fig-
guage of partial ordering. The calculus is NP-complete and            ure 1) is not sufficient to explain preferred inferences in hu-
all tractable subclasses have been identified (Ragni & Scivos,        man reasoning. A systematic approach to representing causal
2005). This calculus is useful in various applications deal-          reasoning by mental models was introduced by Goldvarg and
ing with reasoning about spatial, temporal, spatio-temporal,          Johnson-Laird (2001). They developed mental models for the
topological, competitive, and causal relations. In the next           four causal relations: causes, allows, prevents, and does not
section we will review the theory of partial orders and de-           allow. To accommodate a few more relations (like those we
fine causal relations based on them. We will also extend the          saw above), we propose the following extension found in Ta-
Theory of Mental Models to deal with preferred inferences.            ble 3:
To more thoroughly investigate the theory and its extension,              1 Note that a total order is a partial order that satisfies a fourth
we present two experiments on human reasoning with causal             property: Comparability (for any x, v, either x ≤ v or v ≤ x).
                                                                  1938

                        Table 2: The formal composition table for transitive inferences with causal relations.
                                     P1/P2            ≺                                              f
                                       ≺             ≺          ≺, , , f                         ≺, f, 
                                                ≺, , f                          , , f            , f
                                                ≺, , f                         ≺, , , f         ≺, , f
                                        f          ≺, f           , , f          , , f         ≺, , , f
Note. The leftmost column represents the relation of the first premise (abbreviated by P1, e.g., X ≺ V) and the upmost row the
relation of the second premise (abbreviated by P2, e.g., V  Z). Each intersecting cell contains all possible relations for the
given premises (e.g., X  Z). The relation are: ≺ (causes),  (caused by),  (independent), f (have a common cause).
                                                                              sense of substituting them) in both sub models (in this case
    Table 3: Causal relations and possible mental models.
                                                                              the “V”). The goal is to generate a minimal model represen-
      Relation     Name                                       Model           tation, e.g., to reduce working memory load. We call this
      X≺V          X causes V                                 X V             integration principle sub model integration. This operation
      XV          X is caused by V                           V X             leads to the following model (*), which we call the preferred
                                                                              model:
      XV          X and V are independent                         X
                                                                                                       (*)          X
                                                                   V
                                                                                                                C V
      XfV          X and V have a common cause                C X                                               C Z
                                                              C V             In this integrated representation the putative conclusion “X
                                                                              and Z are independent” holds (cf. Table 3). X and Z are not
                                                                              in a horizontal linear order (this would imply that Z, if it is
   Please note that for the relation “have a common cause”                    to the right of X, is caused by X). Instead they are in vertical
we assume2 reasoners may use an additional place holder, a                    order; this implies that they are independent of one another.
variable such as ‘C’ to represent this internally (recall that a                 From a logical perspective, however, there are other pos-
common cause was something like the tannery which polluted                    sible conclusions from the premises: “X is cause of Z” and
water both at the pasture and at the park) .                                  “X and Z have a common cause” (cf. Table 2) are both as
      (P)    Premise 1:        X and V are independent                        possible as “X and Z are independent.” These conclusions do
             Premise 2:        V and Z have a common cause                    not hold in the preferred model and so should be less likely
                                                                              chosen by participants.
             Conclusion:       X and Z are independent
                                                                                 What precisely is a preferred model? Given a reasoning
   The words in italics are only given here for representational              problem like the one above, a preferred model is reached by
purposes – they do not typically appear in an experiment.                     submodel integration. It is the easiest model to build, thus
After reading the first premise, a reasoner might construct a                 reasoners prefer it over models that take more effort to build
mental representation of the form:                                            (more on how other models may be built below). A preferred
                                     X                                        conclusion may be read from the model, in this case, that
                                     V                                        X and Z are independent. Practically, this means that we ex-
                                                                              pect participants in experiments to most often choose this pre-
which represents that X and V are independent by having                       ferred conclusion.
them in no horizontal order (cf. Table 3). According to the
                                                                                 As previously mentioned, reasoners can reach non-
representation above, a model for the second premise might
                                                                              preferred conclusions. To this end a model revision process
look like:
                                                                              might take place, i.e., the participants may try to accommo-
                                 C V                                          date the new conclusion in the model by operations. Consider
                                 C Z                                          the first conclusion: “X is cause of Z”. For this the reasoner
In a second phase – the premise integration phase – the two                   would need to revise their preferred model representation (*)
representations are combined and form a greater model. The                    such that they can now “read” this conclusion from it. Two
operations of the integration process are relevant and we hy-                 operations are necessary (delete the entry X in the first row
pothesize that reasoners try to match identical objects (in the               and add the entry X to the third row):
    2 Note that formally it is a base relation, i.e., it excludes that ad-                                 C   V
ditionally any other relation such as causes can hold at the same                                          C   X    Z
time. However, in Experiment 2 we found that when causes was the
preferred conclusion, participants also considered “have a common             If we consider the alternative conclusion “X and Z have a
cause” true significantly more often than not (W = 22, p < .001).             common cause,” we again need two operations (delete the
                                                                          1939

entry X in the first row; add the entry X to the row with V).       Scivos, 2005). All possible combinations of these four causal
                            C V X                                   relations (for two premises and one conclusion) allow for 64
                            C Z                                     different problems. Of the 64 problems 41 (64%) are correct
                                                                    conclusions (they can be found in the composition table), and
Model operations and manipulations cost time and can in-
                                                                    for 23 problems the conclusion is incorrect (36 %, these are
crease reasoning difficulty. In this case both alternatives
                                                                    all relations left out in each associated cell in Table 2). Each
should be rather neglected, and we can predict preferences.
                                                                    participant was presented with 64 problems (whereby half of
We hypothesize that there are two reasons for preferences:
                                                                    the problems were generic and half were concrete) consisting
How a reasoner integrates the sub models and the number of
                                                                    of two premises and one conclusion. The participants’ task
operations necessary to construct a model for the conclusion.
                                                                    was to determine if, given the premises, the conclusion was
The preferred model is generated by the fewest model oper-
                                                                    possible.
ations. Model representations give us an interesting advan-
tage: We can derive reasoning difficulty based on the model            Sentences were sequentially presented one by one: Par-
operations necessary to construct them. If we construct mod-        ticipants first received premise 1; after pressing the space-
els then, as outlined, the costs to integrate this information      bar the first premise disappeared and the second premise ap-
should reflect – at least to some extent – the reasoning diffi-     peared. After pressing the spacebar again participants re-
culty which is a deterministic process (cp. Frosch & Johnson-       ceived the putative conclusion. The reason for this sequen-
Laird, 2011). In other words, we can derive from the model          tial order presentation was to require participants to store the
theory a principle of sub model integration. A hypothesis is        information in working memory. This follows the separate-
that participants construct a linear ordering of the elements       stage-paradigm (Potts & Scholz, 1975). Each problem re-
only when this is the only possibility. However, once the           flected different possible causal relations and had the form of
premises allow for separate entities (cp. Problem 1 above),         problem P above. For each problem participants were asked
they are kept separately and this property is inherited to con-     to decide if the putative conclusion was possible (correct) or
nected sub models. In the next section we present a study           impossible by pressing the keys “C” (correct) or “N” (not
designed to test this hypothesis.                                   correct). Participants were asked to keep their forefingers on
                                                                    these keys.
Research Questions
• RQ 1: Do participants show a preference for linear over           Results
   non-linear orders (i.e., ≺,  over f, ) or vice versa?
• RQ 2: Can reasoning performance be explained by the con-          Research Questions 1 and 2 dealt with the overall preference
   struction of a preferred model and associated mental costs?      effect. The derived conclusions and their acceptance rate for
                                                                    each three-term series problem can be found in the composi-
• RQ 3: Do generic or content problems have an effect on            tion table (Table 4). Based on how often participants judged
   reasoning performance?                                           each conclusion to be correct, our results indicate that par-
                                                                    ticipants seem to prefer the independent relation () and the
                     Empirical Study 1                              fork relation (f) over the linear relations ≺,  (RQ1). Fur-
Participants                                                        thermore, the overall correctness was higher for a conclusion
                                                                    ‘independent’ () vs. all other relations (65.3% vs. 52%,
We tested seventy-five participants (age: M = 32.67; SD =
                                                                    Wilcoxon, W = 168, p < .05). This is precisely what the
10.51) on an online website (Amazon’s Mechanical Turk).
                                                                    Mental Model Theory predicts (the principle of sub model
They received a nominal fee for their participation.
                                                                    integration and more precisely the number of operations to
Material, Design, and Procedure                                     transform the initial model of the first two premises into one
Participants were randomly assigned to four conditions in a         which fits the conclusion). It yields a significant correlation
2 × 2 design: Type of text (with vs. without explicit causal        with the accuracy data (Spearman’s rho ρ = .801, S = 20, 000,
relations) × order of tasks (generic vs. concrete tasks first).     p < .000001).
Participants read about the effects of the Asian ladybug in            Further support comes from a central prediction of the
vineyards and the effects of the Horse-chestnut leafminer on        Mental Model Theory – namely that reasoners are better in
chestnut trees. One version of the texts included explicit          single model cases (80.25%) than in multiple model cases
causal relations identical to the ones later given in the con-      (48.25%, Wilcoxon, p < .01). In addition to overall prefer-
crete tasks while the other included all of the content infor-      ence effects RQ 3 dealt with the influence of content and task
mation without explicit causal relations. Each problem con-         order: There was a significant main effect of the order of tasks
sisted of two premises and a putative conclusion. Each prob-        on reasoning difficulty assessed by accuracy on generic prob-
lem in the abstract version used the letters X, V, and Z and        lems (F(3, 71) = 9.00, p = .004; η2 = .114) but not for con-
in each premise and conclusion we systematically varied the         crete problems (F(3, 71) = 2.00, p = .162). That is, generic
relations: cause of, caused by, is independent of, have a com-      problems were perceived as significantly more difficult when
mon cause based on a former formal analysis (cf. Ragni &            generic problems were solved first.
                                                                1940

Table 4: Composition table with human data (accuracy in percentage) for Exp 1. Relations predicted by our model are in bold.
    Premise1 / 2               ≺                                                                                       f
         ≺                   ≺: 73                                  : 58
                                                  60, : 34, f : 49,:
                                              ≺: 60                                      : 79                  ≺: 59,f    75, : 56
                                                                                                                       f : 75
                    ≺ : 50 : 50
                                50, f : 30                : 77                         : 80
                                                                                 : 40,:   80, f : 16              : 18,f
                                                                                                                          f : 50
                    ≺: 29,:: 74
                                74, f : 49                : 92                      : 71
                                                                              ≺: 28,:   71, f : 23, : 23             : 74
                                                                                                                 ≺: 27,:  74, f : 31
         f               ≺: 50,ff : 80             : 53, : 31,f
                                                                f : 55                    : 69
                                                                                   : 28,:   69, f : 43           : 64
                                                                                                             ≺: 49,: 64, f : 68, : 39
Note. The leftmost column represents the relation of the first premise (abbreviated by P1, e.g., X ≺ V) and the upmost row the
relation of the second premise (abbreviated by P2, e.g., V  Z). Each intersecting cell contains all possible relations for the
given premises (e.g., X  Z) and accuracy in percentage.
                    Empirical Study 2                                 Results
An additional important aspect in causal reasoning is that            Results from this second experiment offer some further sup-
some events can prevent others (for example, the stream be-           port for the patterns identified in Study 1 with regard to pref-
ing polluted will prevent the annual River Festival from tak-         erence effects (RQ 1). Problems in which X and Z are inde-
ing place). We conducted a second experiment to include this          pendent was the third assertion were given answers of “yes”
relation and assume the following mental model representa-            significantly more often than any other sort of problem (in
tion for X prevents V:                                                82.2% of cases; Wilcoxon: W = 588, p < .001). The second
                                                                      most popular relation in this experiment, however, proved
                    Relation         Model                            to be prevents which was accepted significantly more often
                    X prevents V     X ¬V                             than causes or have a common cause (in 64.9% of cases;
                                                                      Wilcoxon: W = 644, p = .046). Unlike above, there were
Participants                                                          no significant differences regarding preferences for common
                                                                      cause and causes (58.2% and 58.7% respectively). A sec-
30 participants were recruited and tested on Amazon’s Me-
                                                                      ond aspect of analysis dealt with preferences wrt. integration
chanical Turk. Before analysis, four datasets were eliminated
                                                                      strategies (of the first two premises). Where the sub model
because participants took an average of less than 2s per prob-
                                                                      integration strategy was possible (nine sets of the first two
lem, so that data from 26 participants was analyzed (19 fe-
                                                                      premises), results showed a clear preference for this strategy
male; mean age 41 years, SD = 12.5 years).
                                                                      (84.6% correct) over others (63.1% correct; Wilcoxon: W =
Material, Design, and Procedure                                       178, p < .001). How participants react to the presence of
                                                                      both a term and its negation in one model is an interesting
This experiment worked with the four causal relations:                question that arises from the use of prevents. Two possibili-
causes, prevents, have a common cause, and independent.               ties seemed particularly likely: 1) Reasoners would consider
Problems were presented as questions of consistency. Three            the term and its negation to be independent or 2) They would
(abstract) assertions were given:                                     perform a full negation, that is, if X prevents V and V causes
                                                                      Z, then X prevents Z. Neither of these two scenarios could
                           X causes V
                                                                      be confirmed or discarded – an almost equal number of prob-
                           V prevents Z
                                                                      lems supported each and this is an interesting matter to con-
                           X causes Z
                                                                      sider in future research. The third area of interest was the
And followed by the question: Can all three assertions be true        assessment of the difficulty of the problems. Acceptance of
at the same time? As in Study 1, all 64 combinations of the           a set of assertions as true (RQ 3) did indeed prove to cor-
four relations were presented to participants in a randomized         relate significantly with the number of operations necessary
order, in this study, however, only in abstract form (with X,         (non-parametric bootstrapping with 2000 resamples [r = -.58,
V, and Z). Before beginning the experiment, participants were         95 % CI = (-.82,-.36)]). Closer analysis furthermore revealed
given two example problems. As previously, the three asser-           that problems that required a modification of the initial model
tions and the question were given individually and self-paced         rather than the building of a completely new model, were
such that participants had to press the space-bar to receive the      answered correctly significantly more often (59% vs. 45%;
next assertion. To answer the questions, participants clicked         Wilcoxon: W = 30, p = .036). No other significant differ-
“Y” for “yes, all three assertions can be true at the same time”      ences between the model operations were found.
and “N” for “no, the three assertions cannot all be true at the
same time.” At the end of the experiment participants were                                General Discussion
asked several open-ended questions about the difficulty of the        When reading and interpreting texts and when solving logi-
problems and what they interpreted the four relations to mean.        cal problems people form mental models by merging infor-
                                                                  1941

mation at hand with prior knowledge (Graesser, Singer, &             participants add an element to a model representation they
Trabasso, 1994). In text comprehension these mental models           may think about alternative causes for events derived from
are often referred to as situation models (Kintsch, 1998). Hu-       the context and content. Similarly they may derive further
mans draw inferences and reason about causes and effects on          possible elements that prevent events. All these mental opera-
a daily basis; how we go about this is different from classi-        tions, however, may cause additional costs within our compu-
cal approaches in AI. Expanding on classical Mental Model            tational model and may explain reasoning difficulty and why
Theory, we analyzed three-term series problems of five causal        some answers are strongly preferred over others.
relations, namely causes, depends on, have a common cause,
prevents, and independent. The computational complexity of                                Acknowledgments
the associated satisfiability problem is NP-complete (Ragni          The research was supported by the DFG with a Heisenberg
& Scivos, 2005). This means, in general, the problem of              grant (RA 1934/3-1) to the first author. The authors are grate-
checking if there is a network that satisfies certain conditions     ful to Patrick Junker for developing experimental material,
is rather difficult. In many cases though, there are polyno-         Franz Dietrich for experiment implementation, Sunny Khem-
mial algorithms. Computational complexity is, however, an            lani and three anonymous reviewers for suggestions.
asymptotic measure, i.e., it makes no testable predictions for
three-term series problems and especially does not make any                                    References
predictions about differences in the preferred causal relations.     Beller, S., & Spada, H. (2003). The logic of content effects in
                                                                        propositional reasoning: The case of conditional reasoning
   How do humans draw inferences for classical three-term-
                                                                        with a point of view. Thinking & Reasoning, 9(4), 335–
series problems if they use causal relations? We derived a
                                                                        378.
prediction from the way model representations for the causal
                                                                     Chapman, L. J., & Chapman, J. P. (1959). Atmosphere effect
relations are built: The principle of sub model integration.
                                                                        re-examined. Journal of Experimental Psychology, 58(3),
Recall that sub model integration refers to a sort of “match-
                                                                        220–226.
ing” of same elements to build a minimal representation.
                                                                     Frosch, C. A., & Johnson-Laird, P. N. (2011). Is everyday
Whenever the relation “independent” appeared in a prob-
                                                                        causation deterministic or probabilistic? Acta psycholog-
lem it seemed to trump the other relations. Participants then
                                                                        ica, 137(3), 280–291.
avoided constructing minimal representations, that is total or-
                                                                     Goldvarg, E., & Johnson-Laird, P. (2001). Naive causality:
ders, although they were possible. In all other cases they
                                                                        a mental model theory of causal meaning and reasoning.
tended to perform a model integration leading to preferred
                                                                        Cognitive Science, 25(4), 565–610.
answers. The introduced computational model based on the
                                                                     Graesser, A. C., Singer, M., & Trabasso, T. (1994, July).
number of operations is a good predictor of reasoning diffi-
                                                                        Constructing inferences during narrative text comprehen-
culty. This computational model is an adaption of PRISM
                                                                        sion. Psychological Review, 101(3), 371–395.
(Ragni & Knauff, 2013) that proved useful in explaining rea-
                                                                     Kintsch, W. (1998). Comprehension : a paradigm for cogni-
soning difficulty in spatial relational reasoning.
                                                                        tion. Cambridge, U.K. ; New York: Cambridge University
   At a first glance, there is an alternative interpretation            Press.
of the findings based on the surface or the form of the              Ladkin, P., & Reinefeld, A. (1992). Effective solution of
premise, namely the classical atmosphere hypothesis effect              qualitative constraint problems. Artificial Intelligence, 57,
(Woodworth & Sells, 1935; Chapman & Chapman, 1959).                     105–124.
This effect, however, does not explain the predominance of           Pearl, J. (2000). Causality: Models, reasoning, and infer-
the two relations independent and have a common cause over              ence. Cambridge U.P.
linear relations in all but one case. From a modeling per-           Potts, G. R., & Scholz, K. W. (1975). The internal repre-
spective models can be differentiated by their compactness,             sentation of a three-term series problem. Journal of Verbal
i.e., if an order is partial or total. Although a total order           Learning and Verbal Behavior, 14, 439-452.
has a higher degree of informativity, as it allows a compar-         Ragni, M., & Knauff, M. (2013). A theory and a compu-
ison of the relations between all nodes, it is rarely chosen.           tational model of spatial reasoning with preferred mental
This is a consequence that requires further analysis. Perfor-           models. Psychological Review, 120(3), 561–588.
mance on interpreting causal relations is also influenced by         Ragni, M., & Scivos, A. (2005). Dependency Calculus: Rea-
content: If the content supports the correct conclusion it fa-          soning in a General Point Algebra. In U. Furbach (Ed.),
cilitates performance, but it may also suggest an incorrect             KI 2005: Advances in Artificial Intelligence, Proceedings
conclusion thereby impeding performance (Beller & Spada,                of the 28th Annual German Conference on AI (p. 49-63).
2003). Against the background of these potentially negative             Berlin: Springer.
content effects and also considering cognitive load principles       Russell, S., & Norvig, P. (2003). Artificial intelligence: A
it seems reasonable to train learners on generic instantiations         modern approach (2nd ed.). Prentice Hall.
first before having them solve concrete tasks. An interest-          Woodworth, R. S., & Sells, S. B. (1935). An atmosphere
ing consideration, perhaps especially in the case of content,           effect in formal syllogistic reasoning. Journal of Experi-
is what cognitive processes underly model operations. When              mental Psychology, 18(4), 451–460.
                                                                 1942

