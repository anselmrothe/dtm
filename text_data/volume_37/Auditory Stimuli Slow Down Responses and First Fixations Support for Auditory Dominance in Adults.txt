                      Auditory Stimuli Slow Down Responses and First Fixations:
                                      Support for Auditory Dominance in Adults
                                         Christopher W. Robinson (robinson.777@osu.edu)
                                       Department of Psychology, The Ohio State University Newark
                                                1179 University Dr., Newark, OH 43055, USA
                                             Wesley R. Barnhart (barnhart.135@osu.edu)
                                       Department of Psychology, The Ohio State University Newark
                                                1179 University Dr., Newark, OH 43055, USA
                                                   Samuel Rivera (rivera.162@osu.edu)
                                            Department of Psychology, The Ohio State University
                                               1835 Neil Avenue, Columbus, OH 43210, USA
                                Abstract                                    input, stimuli in one modality provide little to no details about
   Under some situations sensory modalities compete for
                                                                            information presented to another modality (e.g., a phone
   attention, with one modality attenuating processing in a second          conversation provides no information about upcoming traffic
   modality. Almost forty years of research with adults has shown           lights, location of pedestrians, etc.). Research examining
   that this competition is typically won by the visual modality.           processing of arbitrary information often shows that stimuli
   Using a discrimination task on an eye tracker, the current               presented to one modality often interferes with processing in
   research provides novel support for auditory dominance, with             a second modality (see Robinson & Sloutsky, 2010a; Sinnett,
   words and nonlinguistic sounds slowing down visual                       Spence, & Soto-Faraco, 2007; Spence, 2009 for reviews).
   processing. At the same time, there was no evidence suggesting
   that visual input slowed down auditory processing. Several eye           The current paper is primarily interested in these cross-modal
   tracking variables correlated with behavioral responses. Of              interference effects, or modality dominance effects.
   particular interest is the finding that adults’ first fixations were          There is a clear pattern within the adult literature. When
   delayed when images were paired with auditory input,                     presented with arbitrary, auditory and visual information,
   especially nonlinguistic sounds. This finding is consistent with         visual input often wins the competition (Colavita, 1974;
   neurophysiological findings and also consistent with a                   Posner, Nissen, & Klein, 1976; Sinnett, Spence, & Soto-
   potential mechanism underlying auditory dominance effects.
                                                                            Faraco, 2007). For example, in a typical Colavita task,
   Keywords: Sensory Dominance, Cross-modal Processing,                     participants are instructed to press one button when they see
   Attention                                                                a light and press a different button when they hear a tone
                                                                            (Colavita, 1974). The majority of trials are unimodal (only
                            Introduction                                    light or sound); however, some trials are cross-modal (light
Most of our experiences are multisensory in nature; however,                and sound are paired together). On these cross-modal trials,
historically most research has examined processing in a                     participants often respond incorrectly by only pressing the
single sensory modality. Over the last forty years there has                visual button, as opposed to pressing both buttons or a third
been a growing body of research examining how sensory                       button associated with cross-modal stimuli. Over the last
modalities interact while processing multisensory                           forty years visual dominance has been extended to different
information (e.g., sounds and pictures paired together).                    tasks with a variety of attentional manipulations failing to
Under some conditions, presenting information to multiple                   reverse the visual dominance effect (Ngo, Sinnett, Soto-
sensory modalities facilitates processing; whereas, under                   Faraco, & Spence, 2010; see also Spence, 2009 for a review).
other conditions, multisensory presentation hinders                              Interestingly, a different pattern can be found in the
processing. For example, amodal information such as rate,                   developmental literature, with auditory information often
tempo, etc., can be expressed in multiple sensory modalities                attenuating processing of visual input (Robinson & Sloutsky,
(e.g., rate of a hammer tapping can be seen and heard). When                2004; Sloutsky & Napolitano, 2003). For example, after
processing amodal information, multisensory presentation                    familiarizing or habituating infants to auditory-visual
often speeds up responses and facilitates learning (Fort,                   pairings, infants increase looking when the auditory
Delpuech, Pernier, & Giard, 2002; Giard & Peronnet, 1999;                   component changes at test (AUDnewVISold) but often fail to
see also Bahrick, Lickliter, & Flom, 2004 for a review).                    increase looking when only the visual component changes at
      However, there are many situations where the additional               test (AUDoldVISnew). This finding is noteworthy because
sensory information is arbitrary in nature. For example, tasks              infants discriminate the same visual images when presented
such as driving (visual) and talking on the phone (auditory)                in silence; therefore, it was concluded that the auditory input
require people to divide their attention across sensory                     overshadowed or attenuated processing of the visual input.
modalities. Furthermore, due to the arbitrary nature of the                 Auditory dominance effects are not limited to infants. When
                                                                        2009

presented with two auditory-visual pairings in a matching             stimuli, two visual stimuli, or two AV pairs were identical or
game, four-year olds often report that the two pairs are the          different. In contrast to previous research (Napolitano &
same when only the visual component changes (AUD1VIS1                 Sloutsky, 2004; Robinson & Sloutsky, 2004; Sloutsky &
→ AUD1VIS2). In contrast, adults correctly report that the            Napolitano, 2003), images were presented on an eye tracker
two pairs are different (Napolitano & Sloutsky, 2004;                 so we could examine patterns of fixations while participants
Sloutsky & Napolitano, 2003).                                         were discriminating images. Second, rather than examining
      These findings led researchers to posit that auditory input     accuracies, the current study compared response times in the
overshadows visual input early in development (Robinson &             unimodal and cross-modal conditions. Based on previous
Sloutsky, 2004; Sloutsky & Napolitano, 2003). According to            research and on a proposed mechanism underlying auditory
this account, sensory modalities share the same pool of               dominance (Robinson, Ahmar, & Sloutsky, 2010; Robinson
attentional resources and compete for these resources (see            & Sloutsky, 2010a), it was hypothesized that pairing the
Robinson & Sloutsky, 2010a for a review). Because auditory            pictures with words (Experiment 1) or sounds (Experiment 2)
stimuli are dynamic and transient in nature and are processed         would slow down processing of the visual stimulus and have
faster than visual input (Green & von Gierke, 1984), attention        no negative effect on auditory processing. Furthermore, it
may automatically be directed to this information.                    was expected that eye tracking variables such as latency of
Furthermore, due to competition for resources, processing             first fixation and mean fixation durations may also account
details of a visual stimulus may not start until the auditory         for slower response times in cross-modal conditions.
modality releases attention. While this account has received
some support in the developmental literature (Robinson &                                     Experiment 1
Sloutsky, 2007; 2010b; Sloutsky & Robinson, 2008), there is           Method
little support for auditory dominance in adult populations.
      How do modality dominance effects change across                 Participants Thirty-eight undergraduate students (M = 19.52
development? Increased resource capacity and faster                   years, 20 Females) who were enrolled in an Introductory
processing speed in adults (c.f., Kail & Salthouse, 1994) can         Psychology course at The Ohio State University at Newark
explain why under the same stimulus presentation times                participated in this experiment. Completion of the study
children only process information in one modality; whereas,           granted participants with credit that served to fulfill a course
adults have ample time to process stimuli in both modalities.         requirement. All participants provided informed consent, had
However, it is unclear how to reconcile the auditory                  normal hearing and vision (self-reported), and were debriefed
dominance account with a reversal to visual dominance. One            after completion of the study.
possibility is that the mechanism basically remains
unchanged; however, across development, visual stimuli                Apparatus Participants were centrally positioned and seated
become more salient, automatically engage attention, and              approximately 60 cm in front of an Eye Link 1000 Plus eye
attenuate encoding of auditory input. For example, it is well         tracker with desktop mount and remote camera. The eye
established that the auditory modality develops before the            tracker computed eye movements at a rate of 500 Hz, and
visual modality, with hearing beginning in the third trimester        Experiment Builder 1.10.165 controlled the timing of
of pregnancy and vision being relatively poor for the first few       stimulus presentations. Visual stimuli were presented on a
months of life. It is possible that it might take several years       BenQ XL2420 24” monitor and auditory stimuli were
for the visual modality to “catch up” to the auditory modality.       presented via Kensington 33137 headphones. Eye tracking
Alternatively, it is possible that visual input is less likely to     data were collected and stored on a Dell Optiplex 7010
engage attention than auditory input, and adults strategically        computer. Gaze fixation positions and durations were
bias their responses in favor of visual input to compensate for       identified by the Eye Link system online during the
the poor alerting abilities of this class of stimuli (Posner,         experiment and recorded for offline analysis. The eye
Nissen, & Klein, 1976). In other words, visual dominance              tracker, stimulus presentation computer, and eye tracking
may reflect a response bias rather than visual input                  computer were stationed in a quiet testing room in the High-
attenuating encoding of auditory input (Spence, 2009).                Tech lab at The Ohio State University at Newark. A trained
Following up on this idea, it is possible that auditory               experimenter oversaw the entire duration of each
dominance in children (auditory input disrupting visual               participant’s study and they manually started each trial when
encoding) and visual dominance in adults (visual response             the participants fixated on a central stimulus.
bias) co-exist (Chandra, Robinson, & Sinnett, 2011) and are
driven by different mechanisms, with many studies                     Stimuli Visual stimuli consisted of four pairs of images
overlooking auditory dominance because adults strategically           which were digitally constructed in Microsoft PowerPoint
bias their responses in favor of visual input. The goal of the        and exported as 600 x 600 bmp files (approximate size), see
current study is to test the hypothesis that auditory dominance       Figure 1 for examples of visual stimuli and Areas Of Interest
is still present in adult populations and to test assumptions         (AOI). The stimuli resembled the following real-world
underlying auditory dominance.                                        objects: cone of cotton candy, tree, globe, and rabbit, and
      Adults in the current study participated in immediate           each stimulus pair differed by two or four features. For
recognition tasks where they had to determine if two auditory         example, as can be seen in Figure 1, the diamond and circle
                                                                  2010

could be used to differentiate the two trees; thus, these two        participants’ eye measurements, a process that included a 9-
features/AOIs were considered to be relevant. The heart and          point sequence of fixations, which was followed by a 9-point
star were considered irrelevant because both trees shared            validation. The initial calibration/validation process lasted
these two features and therefore cannot be used to                   approximately 1-5 minutes. After calibration, participants
differentiate the trees. Within an individual trial, one of the      were presented with a screen that discussed the experimental
items from the pair (i.e., Target) was presented for 1 s, with       instructions. In the unimodal auditory and visual conditions
a 1 s Inter-Stimulus Interval (ISI). The second item (Test) was      they were told that they would hear two words or see two
presented until the participant made a response. Each item in        pictures and they had to press 1 if the stimuli were exactly the
the pair was equally likely to be the Target or Test item.           same and press 3 if they were different. They were also told
                                                                     to respond as quickly and as accurately as possible. There
                                                                     were 60 trials in each condition, half same trials and half
                                                                     different trials, and each trial began with drift correction (i.e.,
                                                                     central fixation stimulus). In the cross-modal condition,
                                                                     participants were told that they would see two picture-word
                                                                     pairs and they were instructed to press 1 if both the pictures
                                                                     and words were exactly the same (Aud1Vis1 → Aud1Vis1).
                                                                     They were told to press 3 if the word changed (Aud1Vis1 →
                                                                     Aud2Vis1), the picture changed (Aud1Vis1 → Aud1Vis2), or if
                                                                     both components changed (Aud1Vis1 → Aud2Vis2). There
Figure 1: Example of two visual pairs used in Experiments 1
                                                                     were 60 trials in the cross-modal condition, 15 of each of the
and 2. The circles around each feature denote the AOIs and
                                                                     trial types listed above, and each trial began with drift
were not visible during the actual experiment.
                                                                     correction. Order of condition (auditory, visual, and cross-
                                                                     modal) was randomized for each participant, and as in the
     As with visual stimuli, auditory stimuli consisted of four
                                                                     unimodal conditions, they were instructed to respond quickly
word pairs. The auditory stimuli used were one-syllable
                                                                     and accurately.
nonsense words (e.g., paf vs. dax and ket vs. yun) and two-
syllable nonsense words (e.g., lapo vs. vika and kuna vs.
                                                                     Results and Discussion
whonae). Each word was individually spoken by a female
experimenter and recorded using Cool Edit 2000. Audio files          Overall, participants exhibited high accuracy throughout the
were saved as 44.1 kHz wav files and presented to                    procedure (M = .96, SD = .19); therefore, primary analyses
participants via headphones at approximately 65-68 dB. Each          focused on participants’ response times on correct trials. In
item in the pair was equally likely to be the Target or the Test     particular, we were primarily interested in how cross-modal
item. Stimuli in the cross-modal condition were created by           presentation affected auditory and visual processing, so we
presenting images and words at the same time.                        focused on two comparisons. To quantify effects of visual
                                                                     input on auditory processing we compared how quickly
Design Each participant completed three conditions:                  participants discriminated words in the cross-modal
Unimodal Auditory (UA), Unimodal Visual (UV), and                    condition (Aud1Vis1 → Aud2Vis1) with discrimination of the
Cross-Modal (CM) conditions. In the UA and UV conditions,            same words in the unimodal condition (Aud1 → Aud2). To
participants were either presented with two words or two             quantify effects of auditory input on visual processing we
images, respectively, and they had to determine if the stimuli       compared how quickly participants discriminated visual
were exactly the same or different. In the CM condition they         images in the cross-modal condition (Aud1Vis1 → Aud1Vis2)
had to discriminate the same words and pictures; however,            with discrimination of the same images in the unimodal
the auditory and visual information were presented at the            condition (Vis1 → Vis2). The Means and Standard Errors are
same time. Discrimination in the cross-modal condition was           presented in Figure 2. Log transformed means were
compared to respective baselines. Visual dominance would             submitted to a 2 Modality (Auditory vs. Visual) x 2
be inferred if cross-modal presentation only slows down              Presentation (Unimodal vs. Cross-modal) ANOVA with both
auditory processing (compared to UA baseline), and auditory          factors manipulated within subjects. The ANOVA revealed a
dominance would be inferred if cross-modal presentation              main effect of Modality, F (1,37) = 76.89, p < .001, a main
only slows down visual processing (compared to UV                    effect of Presentation, F (1,37) = 13.97, p < .001, and the
baseline). Increased response times in both modalities in the        predicted Modality x Presentation interaction was also
cross-modal condition would suggest increased task demands           significant, F (1,37) = 10.47, p < .005. How does cross-modal
with no evidence that one modality dominated the other               presentation affect processing of visual and auditory input?
modality.                                                            Paired t-tests with a Bonferonni adjustment (p < .025)
                                                                     showed slower visual response times in the cross-modal
                                                                     condition than in the unimodal condition, t (37) = 4.74, p <
Procedure Participants were positioned to face the eye               .001. The slowdown in the auditory modality was less
tracker centrally with an approximate viewing distance of 60         pronounced, as indicated by the Modality x Presentation
cm. At the right side of each participant was the
experimenter; s/he began the experiment by calibrating
                                                                 2011

interaction, and did not reach significance when adjusting for        Finally, given short presentation times, increased fixation
multiple comparisons, t (37) = 2.20, p = .034.                        durations should be associated with fewer fixations. Latency
                                                                      of fixations, fixation durations, and number of fixations were
                                                                      derived offline from fixations identified by the Eye Link
                                                                      system with custom MATLAB and Python software
                                                                      developed by the third author. Fixations initiated before the
                                                                      stimulus presentation or after responses were excluded.
                                                                      Latencies were defined as the fixation start time relative to
                                                                      the visual stimulus onset time. Relevant fixations were those
                                                                      that occurred within either of the relevant AOIs, as depicted
                                                                      in Figure 1. As can be seen in the Table 1, latencies (delayed)
                                                                      and fixation durations (longer) were in the predicted
                                                                      direction; however, these effects did not reach significance
                                                                      when using a Bonferonni adjustment (p <.01).
                                                                      Table 1: Means, (Standard Errors), Paired t’s, and p’s across
Figure 2: Mean response times and Standard Errors in                  the unimodal and cross-modal conditions in Experiment 1.
Experiment 1.
      The same visual pairs were used across the whole
experiment; thus, it is possible that adults eventually learned
to pay attention to the relevant features. However, note that
the auditory dominance account (Robinson & Sloutsky,
2010a) argues that auditory input automatically engages
attention; therefore, knowledge of the relevant visual features
and top-down attentional control should have little effect on
how attention is automatically deployed to cross-modal
stimuli. To examine if participants could override auditory
dominance we focused on visual discrimination in the last                  We also looked at correlations between eye tracking
half of the cross-modal condition (Trials 31-60). Participants’       variables and costs of auditory input on individual response
log transformed visual response times in the cross-modal              times. To quantify the cost of auditory input on visual
condition were faster in the last 30 trials compared to the first     processing we calculated a difference score for each
30 trials, t (37) = 4.04, p < .001, suggesting that some learning     participant (Log transformed RT in cross-modal condition
occurred. However, despite this learning, the auditory stimuli        minus Log transformed RT in unimodal condition). Values
continued to slow down responses to visual stimuli, t (37) =          greater than zero suggest that the words slowed down visual
3.26, p < .005; whereas, visual input had no negative effect          processing and values less than zero indicate that the words
on auditory processing in the last half of the study, t (37) =        sped up response times. We then looked at the correlations
1.44, p = .16.                                                        between the eye tracking variables reported in Table 1 with
      According to the proposed mechanism underlying                  this difference score.
auditory dominance (Robinson & Sloutsky, 2010a), auditory
input should slow down or delay the onset of visual                   Table 2: Correlations between eye tracking variables and
processing. Preliminary support for this hypothesis comes             Difference score (Diff = RTs for discriminating visual stimuli
from a passive ERP oddball procedure where cross-modal                in cross-modal condition minus RTs in unimodal condition).
presentation sped up auditory P300s and slowed down visual            Note: “*” p < .05, “**” p < .01.
P300s (Robinson, Ahmar, & Sloutsky, 2010). To further
examine this proposal, we directly compared patterns of
fixations while participants were discriminating visual
stimuli in the unimodal and cross-modal conditions. More
specifically, we focused on variables that could potentially
account for this slowdown. For example, given increased
latency of visual P300 (Robinson, Ahmar, & Sloutsky, 2010),
it is possible that latency of first fixation and/or latency of
first fixation to a relevant AOI could be delayed. If learning
of visual input is disrupted, it is possible there will be
relatively less looking to relevant AOIs. We also examined
mean fixation times with the assumption that disrupting
visual processing would result in longer individual fixations.             As can be seen in Table 2, the number of fixations was
                                                                      negatively correlated with the difference score, suggesting
                                                                  2012

that adults who responded more slowly to changes in visual          slower to initially fixate on the relevant AOIs; however, these
images made fewer fixations.                                        effects were only marginally significant when adjusting for
      The behavioral findings from Experiment 1 are                 multiple comparisons.
consistent with auditory dominance, with cross-modal
presentation being more likely to slow down visual                  Table 3: Means, (Standard Errors), Paired t’s, and p’s across
processing than auditory processing. Is it possible that the        the unimodal and cross-modal conditions in Experiment 2.
effects are specific to human speech, a familiar class of           Note: “*” denotes that p < .015.
stimuli for adults? To address this issue, we replaced the
words with nonlinguistic sounds in Experiment 2.
                        Experiment 2
Method
Participants, Stimuli, and Procedure Twenty-nine
undergraduate students (M = 20.15 years, 21 Females)
participated in this study in exchange for course credit. The
visual stimuli and the procedure were similar to Experiment
1; however, the images in the current experiment were paired        Table 4: Correlations between eye tracking variables and
with non-linguistic sounds. Four pairs of sounds were created       Difference score (Diff = RTs for discriminating visual stimuli
using Audacity software (e.g., tones differing by 200 Hz). As       in cross-modal condition minus RTs in unimodal condition).
in Experiment 1, the nonlinguistic sounds were one second in        Note: “*” p < .05, “**” p < .01.
duration and the timing and duration of auditory, visual, and
cross-modal stimuli were identical to Experiment 1. In
contrast to Experiment 1, we focused exclusively on the
visual and cross-modal conditions because there were no
images to look at in the unimodal auditory condition. Thus,
eye tracking data from the auditory condition would have
provided no eye tracking information. The nonlinguistic
sounds and images used in Experiment 2 have been tested
without an eye tracker and cross-modal presentation slowed
down visual processing and had no negative effect on
auditory processing (Dunifon & Robinson, 2015).
Results and Discussion                                                                        Discussion
As in Experiment 1, we compared how quickly participants            The current study examined how quickly adults could
discriminated two images when presented in silence with             discriminate two pictures that were presented in silence or
discrimination of the same two images when paired with the          paired with words or sounds. While the adult literature
same sound. As in Experiment 1, participants were slower at         consistently points to visual dominance (see Sinnett, Spence,
discriminating the images in the cross-modal condition (M =         & Soto-Faraco, 2007; Spence, 2009 for reviews), the current
871 ms) than in the unimodal condition (M = 752 ms), paired         study found novel evidence that words and sounds both
t test with log transformed RTs, t (28) = 4.44, p < .001.           slowed down visual discriminations. At the same time, under
      We also examined patterns of fixations while                  similar testing conditions, the visual images did not slow
participants discriminated pictures in the unimodal and cross-      down auditory discrimination (Experiment 1; Dunifon &
modal conditions. See Table 3 for statistics. Adults in the         Robinson, 2015). This asymmetric cost in adults is a novel
cross-modal condition were slower to make their first               finding and is consistent with auditory dominance effects
fixations, mean fixation durations were longer, and latency of      reported in the developmental literature (Sloutsky &
first look to relevant AOIs were also delayed compared to the       Napolitano, 2003; Sloutsky & Robinson, 2008).
unimodal baseline.                                                       Nonsense words and nonlinguistic sounds both slowed
      Do patterns of fixations predict which adults were most       down behavioral responses; however, eye tracking variables
affected by the auditory stimulus? To address this issue we         were more predictive in the nonlinguistic sound experiment.
calculated a difference score (Log transformed RT in cross-         In particular, adults who saw images paired with
modal condition minus log transformed RT in unimodal                nonlinguistic sounds were slower to make their first fixation,
condition) and examined how eye tracking variables                  slower to make their first fixation to a relevant AOI, and
correlated with this difference score. See Table 4 for              fixated for longer durations than in the unimodal condition.
statistics. As can be seen in the Table 4, individuals who were     These findings are consistent with neurophysiological
slower at making visual responses in the cross-modal                findings where auditory input delayed visual P300s
condition made more frequent and longer fixations and were          (Robinson, Ahmar, & Sloutsky, 2010) and are consistent with
                                                                    the claim that auditory input slows down or delays visual
                                                                2013

encoding. Furthermore, consistent with previous research,              involved in audio–visual object recognition in humans.
auditory interference effects are often more pronounced                Cerebral Cortex, 12, 1031–1039.
when using unfamiliar auditory stimuli than when using               Giard, M. H., & Peronnet, F. (1999). Auditory–visual
familiar stimuli or a familiar class of auditory stimuli such as       integration during multimodal object recognition in
human speech (Robinson & Sloutsky, 2010b; Sloutsky &                   humans: A behavioral and electrophysiological study.
Robinson, 2008, but see Napolitano & Sloutsky, 2004). The              Journal of Cognitive Neuroscience, 11, 473–490.
underlying idea is that novel auditory stimuli consume more          Green, D. M., & von Gierke, S. M. (1984). Visual and
attentional resources, which results in a greater cost on visual       auditory choice reaction times. Acta Psychologica, 55, 231
processing.                                                            – 247.
     The current study provides support for auditory                 Kail, R., & Salthouse, T. A. (1994). Processing speed as a
dominance in adult populations, but two issues need to be                mental capacity. Acta Psychologica, 86, 199 – 225.
addressed in future research. First, slower visual response          Napolitano, A. C., & Sloutsky, V. M. (2004). Is a picture
times in Experiment 1 were associated with fewer fixations,            worth a thousand words? The flexible nature of modality
whereas slower responses in Experiment 2 were associated               dominance in young children. Child Development, 75,
with more fixations (see Tables 2 and 4). One possible                 1850-1870.
explanation is that adults treat words differently than other        Ngo, M. K., Sinnett, S., Soto-Faraco, S., & Spence, C. (2010).
sounds/features (c.f., Yamauchi & Markman, 2000), and the              Repetition blindness and the Colavita effect. Neuroscience
sounds and words had different effects on visual attention.            Letters, 480, 186–190.
Second, why was auditory dominance found in this study               Posner, M. I., Nissen, M. J., & Klein, R. M. (1976). Visual
while other studies show visual dominance effects? We                  dominance: An information-processing account of its
believe one key factor is that we eliminated a potential               origins and significance. Psychological Review, 83, 157-
mechanism underlying visual dominance (i.e., response bias).           171.
In contrast to most of the adult studies, auditory and visual        Robinson, C. W., Ahmar, N., & Sloutsky, V. M. (2010).
discrimination was assessed by making the same response;               Evidence for auditory dominance in a passive oddball task.
thus, participants could not bias their response in favor of           In S. Ohlsson & R. Catrambone (Eds.), Proceedings of the
visual input. Furthermore, using a similar change detection            32nd Annual Conference of the Cognitive Science
task but requiring separate responses for auditory and visual          Society (pp 2644-2649). Austin, TX: Cognitive Science
discrimination resulted in visual dominance (Chandra,                  Society.
Robinson, & Sinnett, 2011).                                          Robinson, C. W., & Sloutsky, V. M. (2004). Auditory
     While much of the adult literature suggests that visual           dominance and its change in the course of development.
input dominates auditory processing, the current study                 Child Development, 75, 1387-1401.
provides novel support for auditory dominance, with words            Robinson, C. W., & Sloutsky, V. M. (2007). Visual
and sounds slowing down responding to visual input.                    processing speed: Effects of auditory input on visual
Furthermore, sounds also delayed the latency of first fixations        processing. Developmental Science, 10, 734-740.
and increased fixation durations. These findings have                Robinson, C. W., & Sloutsky, V. M. (2010a). Development
implications on a variety of tasks that hinge on the processing        of Cross-modal Processing. Wiley Interdisciplinary
of multisensory input.                                                 Reviews: Cognitive Science, 1, 135-141.
                                                                     Robinson, C. W., & Sloutsky, V. M. (2010b). Effects of
                         References                                    multimodal presentation and stimulus familiarity on
Bahrick, L. E., Lickliter, R., & Flom, R. (2004). Intersensory         auditory and visual processing. Journal of Experimental
   redundancy guides the development of selective attention,           Child Psychology, 107, 351-358.
   perception, and cognition in infancy. Current Directions in       Sinnett, S., Spence, C., & Soto-Faraco, S. (2007). Visual
   Psychological Science, 13, 99-102.                                  dominance and attention: Revisiting the Colavita effect.
Chandra., M., Robinson, C. W., & Sinnett, S. (2011).                   Perception & Psychophysics, 69, 673–686.
   Coexistence of multiple modal dominances. In L. Carlson,          Sloutsky, V. M., & Napolitano, A. (2003). Is a picture worth
   C. Hölscher, & T. Shipley (Eds.), Proceedings of the 33rd           a thousand words? Preference for auditory modality in
   Annual Conference of the Cognitive Science Society (pp.             young children. Child Development, 74, 822-833.
   2604-2609). Austin, TX: Cognitive Science Society.                Sloutsky, V. M., & Robinson, C. W. (2008). The role of
Colavita, F. B. (1974). Human sensory dominance.                       words and sounds in visual processing: From
   Perception & Psychophysics, 16, 409-412.                            overshadowing to attentional tuning. Cognitive Science,
Dunifon, C. & Robinson, C. W. (2015, April). Pay attention             32, 354-377.
   to the pictures: Auditory dominance not under attentional         Spence, C. (2009). Explaining the Colavita visual dominance
   control. Poster presented at the Annual meeting of the              effect, Progress in Brain Research, 176, 245–258.
   Midwestern Psychological Association, Chicago, IL.                Yamauchi, T., & Markman, A. B. (2000). Inference using
Fort, A., Delpuech, C., Pernier, J., & Giard, M. H. (2002).            categories. Journal of Experimental Psychology: Learning,
   Dynamics of cortico-subcortical cross-modal operations              Memory, and Cognition, 26, 776-795
                                                                 2014

