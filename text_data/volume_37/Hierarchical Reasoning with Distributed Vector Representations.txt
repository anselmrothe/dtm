                Hierarchical Reasoning with Distributed Vector Representations
                                                 Cody Kommers (cydeko@ucla.edu)
                                   Department of Psychology, University of California, Los Angeles
                                                       Los Angeles, CA 90095 USA
                                                  Volkan Ustun (ustun@ict.usc.edu)
                                                    Institute for Creative Technologies
                                                        Playa Vista, CA 90094 USA
                                                  Abram Demski (demski@usc.edu)
                                 Department of Computer Science, University of Southern California
                                                    Institute for Creative Technologies
                                                        Playa Vista, CA 90094 USA
                                             Paul Rosenbloom (rosenbloom@usc.edu)
                                 Department of Computer Science, University of Southern California
                                                    Institute for Creative Technologies
                                                        Playa Vista, CA 90094 USA
                              Abstract
                                                                                         strawberry	  
   We demonstrate that distributed vector representations are ca-
   pable of hierarchical reasoning by summing sets of vectors rep-
   resenting hyponyms (subordinate concepts) to yield a vector
   that resembles the associated hypernym (superordinate con-
   cept). These distributed vector representations constitute a po-
                                                                                         blackberry	                    berry	  
   tentially neurally plausible model while demonstrating a high
   level of performance in many different cognitive tasks. Ex-
   periments were run using DVRS, a word embedding system
   designed for the Sigma cognitive architecture, and Word2Vec,                           raspberry	  
   a state-of-the-art word embedding system. These results con-
   tribute to a growing body of work demonstrating the various
   tasks on which distributed vector representations perform com-
   petently.                                                             Figure 1: The normalized summation of the vectors rep-
   Keywords: hierarchical reasoning; word embeddings; lan-               resenting hyponyms strawberry, blackberry, and raspberry
   guage modeling; concepts; distributed representations                 yields a vector resembling the hypernym berry.
                          Introduction
In this paper, we demonstrate that distributed vector repre-             distributed networks of activation. Intuitively, if the seman-
sentations are capable of performing hierarchical reasoning              tic discrepancies between two concepts, such as “dog” and
by inferring the appropriate category from a set of category             “cat”, can successfully be encoded as distinct patterns of neu-
members (Figure 1). This capability is one among many vari-              ral firings, then it follows that the discrepancies could also
eties of tasks that recent methods for learning distributed vec-         be encoded as distinct patterns of values in a vector (Hinton,
tor representations, also known as word embeddings, have                 1984). Furthermore, Kelly and West (2012) argue that vec-
been shown to perform competently. These capabilities in-                tor representations constitute both symbolic and subsymbolic
clude: language modeling (Bengio et al., 2006; Mikolov,                  representation, allowing distributed vector representations to
2012), natural language understanding (Collobert & Weston,               provide a comprehensive analysis of a cognitive process.
2008; Zhila et al., 2013), machine translation (Mikolov et                  An example of the utility of distributed vector representa-
al., 2013a; Zou et al., 2013), image labeling (Frome et al.,             tions as a model of cognitive phenomena is their application
2013), paragraph representation (Le & Mikolov, 2014), and                in analogical reasoning. The use of vectors to model ana-
relational extraction (Socher et al., 2013).                             logical reasoning dates back to Rumelhart and Abrahamson
   In addition to state-of-the-art performance, one major ad-            (1973). More recent examples of distributed vector represen-
vantage of these vector models is their supposed neural-                 tations for performing analogy are presented in Zhilia et al.
plausibility (Blouw & Eliasmith, 2013). At a gross level                 (2013) and Socher et al. (2013). In these examples, ana-
of abstraction, concepts are represented in the brain as dis-            logical reasoning with distributed vector representations can
tributed networks of neural activation throughout cortical               be performed by vector arithmetic. For example, the differ-
and subcortical regions (Rissman & Wagner, 2011). Dis-                   ence between vectors representing woman and man is approx-
tributed vector representations attempt to approximate these             imately the same as the difference between vectors represent-
                                                                    1171

ing queen and king:                                                    tors can represent the characteristics) to algorithmic (i.e., de-
                                                                       riving the hypernym shared among hyponyms).
                      [W ] − [M] ≈ [Q] − [K]                   (1)        The results in this paper are derived chiefly from dis-
This equation can also be interpreted as an analogy,                   tributed vector representations learned by DVRS (Ustun et al.,
woman:man::queen:king.                                                 2014), a word embedding system designed for the Sigma cog-
   Analogical reasoning with distributed vector representa-            nitive architecture (Rosenbloom, 2013). DVRS learns real-
tions can be incorporated into existing cognitive models.              valued lexical (meaning) vectors in an unsupervised manner
Upon reorganization, the equation for analogical reasoning             from large, shallow information sources based chiefly on co-
with vectors (Equation 2) successfully maps onto an excerpt            occurrence and skip-gram algorithms. DVRS is intended to
of the cognitive model of analogical reasoning presented in            be implemented within the Sigma cognitive architecture and
Holyoak (2012). [W ] − [M] maps onto the source; +[K] maps             thus strives to maximize performance while retaining its in-
onto the target; and ≈ [Q] maps onto the inference.                    tegrity as a cognitive model. As a point of comparison, re-
                                                                       sults are also presented from vectors learned by Word2Vec
                      [W ] − [M] + [K] ≈ [Q]                   (2)     (Mikolov et al., 2013b), a state-of-the-art word embedding
                                                                       system.
   In this paper, we demonstrate that distributed vector rep-             DVRS draws inspiration from BEAGLE (Jones & Me-
resentations can perform hierarchical reasoning in a similar           whort, 2007), but relies on skip-grams rather than n-grams
manner to how they perform analogical reasoning (i.e., with            and replaces circular convolution with pointwise multiplica-
simple vector arithmetic). The present demonstration con-              tion. Representations learned by BEAGLE have been hypoth-
tributes to accumulating work exemplifying the capabilities            esized to encode hierarchical information. This is suggested
of distributed vector representations. Additionally, it is un-         by the tendency of the representations to cluster hierarchically
clear to what extent distributed vector representations are ca-        (e.g., vehicles with other vehicles and birds with other birds),
pable of modeling human cognition, and hierarchical reason-            but no formal demonstration exists, to our knowledge, in the
ing, like analogical reasoning, is among the features for which        capacity demonstrated in this paper.
a comprehensive model of human cognition must account.                    To obtain the present results, vectors of 200 dimensions
Thus, the present results lend support for the ability of dis-         were trained for both DVRS and Word2Vec with their respec-
tributed vector representations to model human cognition.              tive default settings on the first 109 bytes of a Wikipedia dump
   It has previously been hypothesized that distributed vector         from March 3, 2006 (enwik9)1 . The data were preprocessed
representations can accurately represent hierarchical informa-         to convert all text to lower case, convert numbers to text, and
tion (e.g., Lenci & Benotto, 2012; Erk, 2009a; Erk, 2009b;             eliminate links and other references2 .
McDonald & Ramscar, 2001; Geffet & Dagan, 2005). Lenci
& Benotto (2012) define this as the distributional inclusion                                     Experiments
hypothesis: “if u is a semantically narrower term than v, then
a significant number of salient distributional features of u is        Four experiments were run on three different corpora of
included in the feature vector of v as well.” Previous experi-         hypernym-hyponym sets to demonstrate hierarchical reason-
ments have attempted to quantify the phenomenon of vectors             ing with distributed vector representations. The first three ex-
representing hyponyms sharing a common set of characteris-             periments measure aptitude for hierarchical reasoning with
tic features of the associated hypernym.                               distributed vector representations. The fourth experiment
   A representative example of these previous experiments              measures the number of neighbors for both hyponyms and
is Geffet and Dagan (2005), which developed an automated               their associated hypernyms to evaluate the hypothesis that
word-level feature inclusion testing method, called the Inclu-         more general concepts (i.e., hypernyms) have more neigh-
sion Testing Algorithm (ITA). For each pair of vectors rep-            bors.
resenting a hypernym and a hyponym, ITA computes a set                    In each of the first three experiments, a set of N vectors rep-
of characteristic features for the hypernym vector and tests if        resenting hyponyms were summed; the result was normalized
those features are also included in the vector of the hyponym.         and the closest M vectors in the lexicon, as measured by co-
This inclusion occurred in 86% percent of their tested pairs.          sine similarity, were considered as potential hypernyms. If
In other words, the vector that represents a category member           the appropriate hypernym was among the M closest vectors,
contains the information that characterizes it as a member of          then the trial was counted as correct. As with analogical rea-
the associated category.                                               soning, the vector calculation for hierarchical reasoning can
   The results presented in this paper derive from the same hy-        be expressed by an equation of vector arithmetic:
pothesis as the above results, but further the empirical anal-
ysis. Specifically, instead of comparing a single hyponym                                      ∑N1 [hn ]
                                                                                                           ≈ [hcategory ]             (3)
vector with a single hypernym vector, we compare sets of                                      | ∑N1 [hn ]|
hyponym vectors with the common hypernym. This is an
advance for cognitive modeling of hierarchical reasoning be-               1 Obtained from http://cs.fit.edu/mmahoney/compression/textdata.html.
cause it transitions from purely representational (i.e., that vec-         2 Script provided by Matt Mahoney with the Wikipedia dump.
                                                                   1172

   N is the number of hyponyms being summed, [hn ] is the                The number of hyponyms per hypernym varied from three
vector representing the hyponym, and [hcategory ] is the vec-         (in the case of six hypernyms) to 98 (in the case of one hyper-
tor representing the hypernym. For example, the normalized            nym, animal). For the trials labeled All in the #Hypo column,
summation of the vectors for hyponyms strawberry, black-              every hyponym was summed, regardless of number. For tri-
berry, and raspberry yields a vector resembling the hyper-            als with 3 or 10 hyponyms, hyponyms were separated into
nym berry (Figure 1). In other words, the vectors can be used         appropriately sized subsets. For example, in a trial with 10
to judge that strawberry, blackberry, and raspberry are mem-          summed hyponyms, the hyponyms of animal (with 98 total
bers of the category berry.                                           hyponyms) were separated into nine different subsets. These
   The use of three different corpora show the robustness of          sets were created by alphabetical ordering (e.g., the top 10
this effect, independent of any idiosyncrasy of the data. In ad-      in alphabetical order constituted the first set) and remaining
dition to using different corpora, three values were varied to        hyponyms were not included in the test.
show a range of effectiveness in performing the task: (1) the            For the present purposes, this data set contains a small
word embedding system under consideration (labeled System             amount of noise. These data were generated by labeling se-
in results tables), (2) N, the number of hyponyms to sum (la-         mantic features that were judged to be associated with basic-
beled #Hypo), and (3) M, the number of closest vectors in the         level concepts. That is to say, these features are not necessar-
lexicon that will be considered as potential hypernyms (la-           ily concerned with selecting the canonical hypernym of the
beled #Hyper). The table shows the number correct out of              hyponyms, merely a hypernym that is accurate. It does not
the total possible, which varies based on how the categories          necessarily follow that the labeled hypernym is the best hy-
were grouped.                                                         pernym that follows from the associated hyponyms. Thus, the
   Results are shown for both DVRS and Word2Vec in most               distributed vector representations may produce an answer that
cases. DVRS is the focus of these results, because it is              is acceptable, but merely not listed in these data. While this
most concerned with explanatory power as a cognitive model.           ambiguity does not nullify the utility of these data, it presents
Word2Vec is shown as a point of comparison, because it                an issue that may negatively affect the performance.
is a state-of-the-art word embedding system. Results from                Results are shown in Table 1. DVRS performs best, as ex-
Word2Vec are shown mostly for the trials in which the best            pected, in the case of summing 10 hyponyms and considering
performance is expected (i.e., #Hypo=10). This allows for             10 hypernyms; at 44% the performance is consistent, though
comparison of performance between DVRS and Word2Vec,                  not tremendous. DVRS significantly outperforms Word2Vec
while elaborating on several more detailed cases with DVRS            in every case. It is unclear exactly why this discrepancy in
(i.e., cases with fewer summed hyponyms).                             performance occurred.
   The number of hyponyms was varied to demonstrate that,
in principle, it is possible to derive hypernyms from a rela-         Table 1: Hierarchical reasoning results on McRae et al.
tively small set of hyponyms. There are many anecdotal cases          (2005) data.
in which the correct hypernym can be derived from only two
hyponyms. Accordingly, even though performance is weaker
with sets of fewer hyponyms, there are still examples of suc-            System        #Hypo      #Hyper     Corr.   Total    Acc.
cessful trials.                                                          DVRS          All        1          9       39       23.1%
   The number of closest vectors considered as potential hy-             DVRS          3          1          18      167      10.8%
pernyms is varied to demonstrate that, even if the targeted hy-          DVRS          3          10         54      167      32.3%
pernym vector is not the top result, it is among the top results.        DVRS          10         1          8       34       23.5%
That is, the vector resulting from the summed hyponyms con-              DVRS          10         10         15      34       44.1%
sistently resembles the vector of the hypernym, even though              Word2Vec      All        1          3       39       7.7%
it may not be the best match.                                            Word2Vec      10         1          1       34       2.9%
                                                                         Word2Vec      10         10         5       34       14.7%
Experiment 1
The first data set is from McRae et al. (2005), a corpus con-
sisting of human-generated data on semantic feature norms
for 541 basic-level concepts. Seven hundred and twenty-five           Experiment 2
participants were recruited to label semantic features for the        The second data set is derived from WordNet (Miller, 2005),
concepts. Each concept was labeled with 10 unprompted se-             a standard database of semantic relationships between words.
mantic features by at least 30 of the subjects. A semantic            One hundred and forty-seven basic-level concepts were cho-
feature was included as a norm if more than three partici-            sen as hypernyms for which WordNet supplied hyponyms.
pants included the feature for the same concept. Hypernymy            Though WordNet supplied the hyponyms, these hypernyms
was among the semantic features coded by the researchers. A           were chosen by the authors under no systematic criterion. For
subset of 535 concepts were used in the present experiment;           that reason, this set of basic-level concepts cannot be claimed
this subset consisted of basic-level concepts for which at least      to achieve the same standard of empirical disinterest as the
three shared a hypernym.                                              McRae et al. (2005) data set. However, there appears to be
                                                                  1173

no qualitative difference between the kinds of basic-level con-      in the present paradigm; that is to say, these are the results of
cepts that appear in these data versus those from the data in        a hand-selected data set and a charitable judging criterion.
McRae et al. (2005).                                                    For each set of N hyponyms, the result from the closest M
   WordNet choices for hyponyms, while empirical, contain            vectors was judged to be correct if it represented any com-
significant noise. That is to say, in many cases they do not         monality between the hyponyms. This could include a com-
represent what could be considered canonical categorizations         mon category, a common entity of which all vectors are mem-
of hypernymy as one could imagine might be judged by a hu-           bers, or a common trait. For example, a trial was considered
man. For example, schizocarp, pyxis, rowanberry, and drupe           correct if hyponyms Monterrey, Bakersfield, and Riverside
appear as hyponyms for hypernym fruit; shamanism, zoroas-            yield a hypernym such as city, the common entity of which
trianism, mithraism, and hindooism [sic] appear as hyponyms          all are members such as California, or a common attribute
for hypernym religion; thanatology, cryptanalysis, agrobiol-         such as Californian. A trial was considered incorrect if the
ogy, and architectonics appear as hyponyms for hypernym              summation yielded results such as similar category members
science. While these categorizations may not be inaccurate,          (e.g., Merced) or wholly unrelated concepts.
they do not constitute the most representative set of human             Results are shown in Table 3. While both systems demon-
judgments of hypernymy.                                              strated their best respective results, there were still failed in-
   Results are shown in Table 2. As with Experiment 1,               stances by both. These failed instances most likely do not
DVRS outperforms Word2Vec in every case, though by a                 come from a lack of examples by which a sufficient represen-
smaller margin. In the best-performance-expected trial (10           tation can be learned, but solely a failure to encode hierarchi-
hyponyms summed and 10 hypernyms considered), DVRS                   cal information. For example, with the California example
obtains 50.0% accuracy, which is comparable to its perfor-           mentioned above, DVRS got the trial incorrect because the
mance of 44.1% under the same criteria in Experiment 1.              hierarchical information was insufficiently represented, not
In contrast, Word2Vec more than doubles its accuracy for             necessarily because there were too few encounters with the
the same best-performance-expected criteria between Exper-           associated words. This claim is corroborated by a correct re-
iment 1 (14.7%) and Experiment 2 (35.4%). Results of hy-             sponse from Word2Vec in the California case.
ponym summations begin with sets of five (instead of three)             Qualitative analysis suggests that often when the answer is
to adjust for noise associated with WordNet hyponyms.                completely incorrect, the result is a another hyponym instead
                                                                     of a hypernym (i.e., a category member rather than the cat-
  Table 2: Hierarchical reasoning results on WordNet data.           egory). For example, a set of vectors representing cardinal
                                                                     directions including north, west, northwest, etc. yields south-
                                                                     eastern rather than directions or cardinal. This seemed to be
   System        #Hypo     #Hyper    Corr.    Total    Acc.          the case for Experiments 1 and 2 as well.
   DVRS          5         1         24       119      20.2%
   DVRS          5         5         36       119      30.3%         Table 3: Hierarchical reasoning results on data selected by
   DVRS          5         10        55       119      46.2%         authors. These results may be considered an upper-bound on
   DVRS          10        1         19       82       23.2%         performance for hierarchical reasoning with the present word
   DVRS          10        5         27       82       32.9%         embedding systems.
   DVRS          10        10        41       82       50.0%
   Word2Vec      10        1         10       82       12.2%
   Word2Vec      10        5         22       82       26.8%            System        #Hypo     #Hyper      Corr.   Total    Acc.
   Word2Vec      10        10        29       82       35.4%            DVRS          8         1           28      58       48.3%
                                                                        DVRS          8         10          50      58       86.2%
                                                                        Word2Vec      8         1           6       58       10.4%
Experiment 3                                                            Word2Vec      8         10          36      58       62.1%
The third data set consists of 58 sets of eight subordinate
concepts selected by the authors to constitute a data set that
would be most likely to result in correct answers from DVRS.         Experiment 4
While these data do not represent a randomly selected sam-           A subset of the WordNet hypernym-hyponym sets used for
ple, they are judged by the authors to be a data set without the     testing hierarchical reasoning in Experiment 2 were used to
ambiguity of the data from McRae et al. (2005) or the noise          compare the number of neighbors between hypernyms and
of the data from WordNet. There was no systematic crite-             hyponyms in the associated vector space. This subset con-
rion by which these data were selected; they were selected           sisted of 39 hypernym-hyponym sets which DVRS got correct
only by if the authors thought the system should be capable          in Experiment 2 (i.e., hierarchical information was success-
of producing a category shared by all members. Thus, they            fully encoded) and 39 hypernym-hyponym sets which DVRS
should be interpreted as an upper bound of the capabilities of       did not get correct in Experiment 2 (i.e., hierarchical informa-
hierarchical reasoning with distributed vector representations       tion was not successfully encoded). We hypothesized that, in
                                                                 1174

comparison between a category and a subcategory, the more               tion. The discrepancy in performance between DVRS and
general concept would have more neighbors (Figure 3). Ad-               Word2Vec suggests that not all methods of learning such vec-
ditionally, for sets in which the relevant hierarchical informa-        tors yield equally successful representations. Additionally, an
tion was shown to be successfully encoded in Experiment 2,              intriguing relationship has been uncovered between concept
the effect should be more consistent than those sets that did           generality and the number of neighbors in the associated vec-
not demonstrate success in Experiment 2. The present results            tor space.
support this hypothesis.
                                                                        Proposed Geometric Intuition
                        Philosopher                       Aristotle
                                                                        What is the intuition behind how distributed vector represen-
                                                                        tations are capable of representing this hierarchical informa-
                                                                        tion? One plausible explanation is that this effect is merely
                                                                        a function of word frequency. In this case, Philosopher has
                                                                        more neighbors than Aristotle simply because it is more fre-
                                                                        quent in the corpus on which the vectors were learned. An-
                                                                        other explanation which may also contribute is that more gen-
                                                                        eral concepts take on properties of a hyperplane on which
Figure 2: In this example, Philosopher is more general than             subordinate concepts lay (Figure 2).
Aristotle; thus, its vector would be hypothesized to have more
neighbors.
                                                                                                   Plato (1,0,0)
                                                                                                                      Philosopher (1,1,0)
   For each trial, the number of neighbors within a cosine
similarity of 0.7 was compared between the hypernym and
the average of three corresponding hyponyms. The three hy-
ponyms were randomly selected from the set of all WordNet                 Homer Simpson
hyponyms corresponding to the hypernym. If the number of
neighbors was higher for the hypernym than the average hy-                                                            Descartes    (0,1,0)
ponym, then the trial was counted as correct.
   Results are shown in Table 4. As expected, vectors repre-
senting hypernyms consistently have more neighbors within
a cosine similarity of 0.7 than their hyponyms. In the case of
                                                                        Figure 3: More general concepts may take on features of a
hypernym-hyponym sets where the hierarchical relationship
                                                                        hyperplane on which the associated subordinate concepts lay.
is demonstrated to be encoded (i.e., correct in Experiment 2),
this effect was seen in 89.7% of tested cases. If the hierar-
chical relationship was demonstrated to not be successfully                 In the 3-dimensional case depicted in Figure 3, the hy-
encoded (i.e., incorrect in Experiment 2), this effect is closer        pernym Philosopher encodes relevant feature information re-
to chance (66.7%). According to our hypothesis, if the hierar-          lated to its hyponyms Plato and Descartes. Philosopher may
chical information is not encoded, then the probability of the          be thought of as having hyperplane-like properties because
hypernym having more neighbors should be the same as in a               its vector is yielded by the summation of its hyponyms. Re-
comparison with any other word (i.e., 50%). Thus, it appears            lated concepts, such as Plato and Descartes, lay on the hyper-
that some hierarchical information is encoded in those cases            plane created by Philosophy while unrelated concepts, such
that are unsuccessful in Experiment 2, but not sufficient for           as Homer Simpson, do not. While in three dimensions this is
robust performance.                                                     an implausible scenario for a complex ontology of concepts,
                                                                        it is plausible for a high dimensional space (e.g., 200), as with
Table 4: Vectors representing hypernyms consistently have               distributed vector representations.
more neighbors within a cosine similarity of 0.7.                           This interpretation appears to be in line with the afore-
                                                                        mentioned distributional inclusion hypothesis, in which sub-
                                                                        ordinate concepts include features of their superordinate con-
   System     Hierarchy encoded?       Corr.   Total   Acc.             cepts. More results will be necessitated to explore this con-
   DVRS       Yes                      35      39      89.7%            nection.
   DVRS       No                       26      39      66.7%
                                                                                              Acknowledgments
                                                                        This work has been sponsored by the U.S. Army. Statements
                            Discussion                                  and opinions expressed do not necessarily reflect the position
The present experiments demonstrate that distributed vector             or the policy of the United States Government. No official
representations can successfully encode hierarchical informa-           endorsement should be inferred.
                                                                    1175

                        References                                 Levy, S. D., & Gayler, R. (2008). Vector symbolic architec-
                                                                     tures: A new building material for artificial general intel-
Barbu, E. (2009). Acquisition of common sense knowledge              ligence. Proceedings of the 2008 Conference on Artificial
  for basic level concepts. RANLP, 23–27.                            General Intelligence 2008: Proceedings of the First AGI
Bengio, Y., Schwenk, H., Senécal, J.-S., Morin, F., & Gau-          Conference, 414–418.
  vain, J.-L. (2006). Neural probabilistic language models.        McDonald, S., & Ramscar, M. (2001). Testing the distribu-
  Innovations in Machine Learning, 137–186.                          tional hypothesis: The influence of context on judgements
Blouw, P., & Eliasmith, C. (2013). A neurally plausible en-          of semantic similarity. Proceedings of the 23rd annual con-
  coding of word order information into a semantic vector            ference of the cognitive science society, 611-616.
  space. 35th Annual Conference of the Cognitive Science           McRae, K., Cree, G., Seidenberg, M., & Mcnorgan, C.
  Society, 1905–1910.                                                (2005). Semantic feature production norms for a large set
Collobert, R., & Weston, J. (2008). A unified architecture for       of living and nonliving things. Behavior Research Meth-
  natural language processing: Deep neural networks with             ods, 37(4), 547-559.
  multitask learning. Proceedings of the 25th international        Mikolov, T. (2012). Statistical language models based on
  conference on Machine learning, 160–167.                           neural networks. Presentation at Google, Mountain View,
Eliasmith, C., & Thagard, P. (2001). Integrating structure           2nd April.
  and meaning: A distributed model of analogical mapping.          Mikolov, T., Le, Q. V., & Sutskever, I. (2013a). Exploit-
  Cognitive Science, 25(2), 245–286.                                 ing similarities among languages for machine translation.
Erk, K. (2009a). Representing words as regions in vector             arXiv preprint arXiv:1309.4168.
  space. Proceedings of the Thirteenth Conference on Com-          Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean,
  putational Natural Language Learning, 57–65.                       J. (2013b). Distributed representations of words and
Erk, K. (2009b). Supporting inferences in semantic space:            phrases and their compositionality. Advances in Neural In-
  representing words as regions. Proceedings of the Eighth           formation Processing Systems 26, 3111–3119.
  International Conference on Computational Semantics,             Miller, G. A. (1995). Wordnet: a lexical database for english.
  104–115.                                                           Communications of the ACM, 38(11), 39–41.
Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,       Mitchell, J., & Lapata, M. (2010). Composition in distri-
  Mikolov, T., et al. (2013). Devise: A deep visual-semantic         butional models of semantics. Cognitive Science, 34(8),
  embedding model. Advances in Neural Information Pro-               1388–1429.
  cessing Systems, 2121–2129.                                      Rissman, J., & Wagner, A. D. (2012). Distributed represen-
Geffet, M., & Dagan, I. (2005). The distributional inclusion         tations in memory: insights from functional brain imaging.
  hypotheses and lexical entailment. Proceedings of the 43rd         Annual Review of Psychology, 63, 101–128.
  Annual Meeting on Association for Computational Linguis-         Rosenbloom, P. S. (2013). The sigma cognitive architecture
  tics, 107–114.                                                     and system. AISB Quarterly, 136, 4–13.
                                                                   Rumelhart, D. E., & Abrahamson, A. A. (1973). A model for
Hinton, G. (1984). Distributed representations (Tech. Rep.
                                                                     analogical reasoning. Cognitive Psychology, 5(1), 1–28.
  No. CMU-CS-84-157). Pittsburgh, PA: Carnegie Mellon
                                                                   Salton, G., Wong, A., & Yang, C.-S. (1975). A vector
  University, School of Computer Science.
                                                                     space model for automatic indexing. Communications of
Holyoak, K. J. (2012). Analogy and relational reasoning. The
                                                                     the ACM, 18(11), 613–620.
  Oxford Handbook of Thinking and Reasoning, 234–259.
                                                                   Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013).
Jones, M. N., & Mewhort, D. J. (2007). Representing word
                                                                     Reasoning with neural tensor networks for knowledge base
  meaning and order information in a composite holographic
                                                                     completion. Advances in Neural Information Processing
  lexicon. Psychological review, 114(1), 1.
                                                                     Systems, 926–934.
Kelly, M. A., & West, R. L. (2012). From vectors to symbols        Ustun, V., Rosenbloom, P. S., Sagae, K., & Demski, A.
  to cognition: The symbolic and sub-symbolic aspects of             (2014). Distributed vector representations of words in the
  vector-symbolic cognitive models.                                  sigma cognitive architecture. Artificial General Intelli-
Kiefer, M., & Pulvermüller, F. (2012). Conceptual represen-         gence, 196–207.
  tations in mind and brain: theoretical developments, cur-        Van Der Vet, P. E., & Mars, N. J. (1998). Bottom-up con-
  rent evidence and future directions. Cortex, 48(7), 805–           struction of ontologies. Knowledge and Data Engineering,
  825.                                                               IEEE Transactions on, 10(4), 513–526.
Le, Q. V., & Mikolov, T. (2014). Distributed repre-                Zhila, A., Yih, W., Meek, C., Zweig, G., & Mikolov, T.
  sentations of sentences and documents. arXiv preprint              (2013). Combining heterogeneous models for measuring
  arXiv:1405.4053.                                                   relational similarity. HLT-NAACL, 1000–1009.
Lenci, A., & Benotto, G. (2012). Identifying hypernyms             Zou, W. Y., Socher, R., Cer, D. M., & Manning, C. D.
  in distributional semantic spaces. Proceedings of the First        (2013). Bilingual word embeddings for phrase-based ma-
  Joint Conference on Lexical and Computational Semantics,           chine translation. EMNLP, 1393–1398.
  75–79.
                                                               1176

