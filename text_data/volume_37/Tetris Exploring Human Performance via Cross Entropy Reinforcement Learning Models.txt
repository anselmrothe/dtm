              Tetris™: Exploring Human Performance via Cross Entropy
                                         Reinforcement Learning Models
                                  Catherine Sibert, Wayne D. Gray, and John K. Lindstedt
                                                       Cognitive Science Department
                                                      Rensselaer Polytechnic Institute
                              Abstract                                  tertaining challenge.
   What can a machine learning simulation tell us about human               Within cognitive science, Tetris has been used to develop
   performance in a complex, real-time task such as Tetris™?            (Kirsh & Maglio, 1994; Maglio, Wenger, & Copeland, 2008)
   Although Tetris is often used as a research tool (Mayer,             and refine (Destefano, Lindstedt, & Gray, 2011) the construct
   2014), the strategies and methods used by Tetris players have        of Epistemic (or Complementary) Action. This use of Tetris
   seldom been the explicit focus of study. In Study 1, we use          is qualitatively different from other uses as the researchers
   cross-entropy reinforcement learning (CERL) (Szita & Lor-            were interested in the detailed interactions among cognition,
   incz, 2006; Thiery & Scherrer, 2009) to explore (a) the util-        perception, and action that forms the basis of interactive be-
   ity of high-level strategies (goals or objective functions) for      havior in Tetris. The scientific arguments relied on a deep
   maximizing performance and (b) a variety of features and
                                                                        analysis of the instance by instance interactions of humans-
   feature-weights (methods) for optimizing a low-level, one-
                                                                        with-zoid (i.e., the Tetris pieces), the tradeoffs made between
   zoid optimization strategy. Two of these optimization strate-
   gies quickly rise to performance plateaus, whereas two oth-          cognition in-the-head and cognition in-the-world, and how
   ers continued towards higher but more jagged (i.e., variable)        those tradeoffs changed as a function of expertise with Tetris.
   plateaus. In Study 2, we compare the zoid (i.e., Tetris piece)           The current work is well within this cognitive science tra-
   placement decisions made by our best CERL models with                dition. In Study 1, we build 8 cross-entropy reinforcement
   those made by the full spectrum of novice-to-expert human            learning (CERL) controllers that attempt to optimize Tetris
   Tetris players. Across 370,131 episodes collected from 67 hu-        performance using four different strategies (objective func-
   man players, the ability of two CERL strategies to classify hu-      tions) crossed with two different sets of features such as the
   man zoid placements varied with player expertise from 43%            landing height of the last zoid added, pits (the number of
   for our lowest scoring novice to around 65% for our three            empty cells that are covered by other zoids), and many more.
   highest scoring experts.
                                                                        Similar to genetic algorithms, each CERL model optimizes
   Keywords: Tetris, human expertise, strategies, methods,              performance by adjusting the weight given each feature in the
   cross-entropy reinforcement learning                                 feature set over several generations. In Study 2, the two best
                                                                        of these 8 controllers are used to classify each of 370,131
                           Introduction                                 episodes collected from 67 human Tetris players.
   Tetris™ is one of the most played games in the world                     Following Newell’s (1973) injunction to “accept a single
(Stuart, 2010), one of the games most used for psycholog-               complex task and do all of it” this work is part of a larger
ical studies (Lindstedt & Gray, 2015; Mayer, 2014), and a               effort that seeks to understand the acquisition of extreme ex-
favorite challenge for the machine learning community (Fa-              pertise in Tetris (Gray, Hope, Lindstedt, & Destefano, 2014).
hey, 2013; Gabillon, Ghavamzadeh, & Scherrer, 2013; Szita
                                                                        Playing Tetris
& Lorincz, 2006). The latter became interested in Tetris as a
challenging machine learning problem. The former has seen                   Players use the keyboard or special game controllers to ro-
Tetris as potentially important for its presumed side effects           tate zoids, as they are falling, into an accumulating pile of
for things as diverse as ameliorating sex differences in spa-           zoids at the bottom of the screen. When a player fills an en-
tial skills (Linn & Petersen, 1985; Okagaki & Frensch, 1994;            tire row, the row vanishes, and the score increases. Since it
Sims, 2011; Terlecki, Newcombe, & Little, 2008), relief                 is not always possible to clear rows, the pile gradually rises.
from “flashbacks for trauma” (Holmes, James, Coode-Bate,                The game ends when the pile rises above the top row in the
& Deeprose, 2009), and improving the abilities of engineer-             board. (A game in progress is shown in Figure 1.) Despite
ing students (Martin-Gutierrez, Luis Saorin, Martin-Dorta, &            Tetris’ widespread appeal, it is unwinnable. If you play it long
Contero, 2009). The world’s many game players, presumably,              enough, you will lose (Baccherini & Merlini, 2008; Kendall,
enjoy Tetris simply because it provides an enjoyable and en-            Parkes, & Spoerer, 2008; Fahey, 2013)!
                                                                   2188

2
                                                                        Table 1
    The standard Tetris board is 10 squares wide by 20 squares          Useful Tetris Features Proposed by Dellacherie
high. At the beginning of the game the zoids fall at the rate of          Feature                 Description
1.25 rows per s and would take 25 s to fall from the top to the           Landing height          Height where the last zoid is added
bottom row. This drop speed increases with the game level,                                        # of cells of the current zoid eliminated
and at level 9 the pieces fall at 10 rows per s or 2 s to fall from       Eroded zoid cells
                                                                                                  due to line clears
the top to the bottom row. Mastering decision-making and                                          # of full to empty or empty to full, row
physical placement at these rates is a significant challenge for          Row transitions
                                                                                                  transitions between cells on the board
human players.                                                            Col transitions         Same as above for vertical transitions
                                                                                                  # of empty cells covered by at least one
                                                                          Pits
                                                                                                  full cell
                                                                                                  A series of empty cells in a column such
                                                                          Wells                   that their left cells and right cells are both
                                                                                                  occupied
                                                                        with the 6 Dellacherie features as well as other features de-
                                                                        scribed in the machine learning literature. Unlike the machine
                                                                        learners, who were interested in claiming bragging rights as
                                                                        to which approach cleared the most lines, we are interested
                                                                        in human level results. Hence, for this purpose we ran each
                                                                        model on each generation until it died or until it completed
   Figure 1. Tetris board, with a falling I-Beam-zoid, the pile at
   the bottom, and a new I-Beam-zoid in the Preview Box on the
                                                                        506 Tetris episodes (i.e., where each episode is the placement
   right.                                                               of one zoid), as 506 episodes is the longest game played by
                                                                        any player in our laboratory.
    When people play Tetris, we somehow consider both the
current move and some number of future moves to deter-                  Cross Entropy Reinforcement Learning
mine where to place a zoid to maximize points and minimize
height. Data from our best human players suggest that they                  Four things are required to train the CERLs: an objective
have a web of contingency plans that span the current zoid,             function, a set of features, an assignment of weights to those
the next zoid (which is shown in the Preview box in Figure              features, and patience. Patience is required as each controller
1), and several unknown future zoids.                                   is trained for 80 generations where each generation consists
    In contrast, our CERL models are one-zoid optimizers                of 100 controllers completing one game of Tetris each.
which make move decisions by evaluating all potential zoid                  For the first generation, the starting controller sets all fac-
placements using sets of weighted features and selecting the            tor weights to zero and the standard deviation for each factor
highest scoring move. As Table 1 shows, these features are              to 100. Hence, the first 100 models for this first generation
metrics such as the total height of the pile, the number of             form a cloud around the zero starting point, with a standard
unfilled squares, or pits, and the number of lines that will be         deviation of 100. Each successive generation begins with a
cleared by the given placement. For any given game board                new starting controller defined by the mean values of the best
configuration, the feature values will differ slightly for each         performing 10 models from the prior generation. To avoid
possible placement. Ties among the highest rated zoid place-            early convergence, a constant noise factor of 4 was introduced
ments are decided randomly.                                             in each generation, meaning that for the first generation, the
                                                                        standard deviation was 104. This noise factor remains con-
                           Study One                                    stant throughout the generations, while the standard deviation
                                                                        to which it is added is adjusted and potentially converges on
    The first study explored the performance of our four dif-           an optimal feature value. As for the first generation, the new
ferent objective functions on two different feature sets. We            starting controller is used to spawn one hundred new con-
consider each objective function as one goal or strategy that           trollers that form a cloud around this new starting point. This
a human player could choose to optimize. In terms of fea-               procedure is followed until 80 generations of controllers have
ture sets, we adopted the Dellacherie set (Fahey, 2013) of 6            played Tetris, resulting in a highly optimized controller.
features that has been widely used in the machine learning                  Of course, within each run of 80 games, the definition
literature (Szita & Lorincz, 2006; Thiery & Scherrer, 2009,             of “best controller” depends on the objective function being
2009) (See Table 1). We also created our own set of 48 fea-             optimized. For our studies these objective functions were
tures. This set is composed of features developed in our prior          (a) Score, (b) total number of Lines cleared, (c) highest Level
work (Lindstedt & Gray, 2013; Lindstedt, 2013) combined                 reached, and (d) the number of episodes in which four lines
                                                                   2189

                                                            MODELING TETRIS                                                                                                           3
  Table 2
  Feature Weights of the Lines and Score Objective Functions for              To put the CERL results into a human context, we can com-
  the Dellacherie Features Set                                            pare their performance with the mean of each human’s four
 Feature                        Lines           Score                     highest scoring games. The second to fourth best humans
 Landing height                 −0.45           −0.29                     averaged scores of 93,000, 73,000, and 53,000, respectively.
 Eroded zoid cells              +0.32           −0.81                     Our very best human’s average score was 174,000.
 Row transitions                −0.26           −0.34                         Our Human High Score, shown in Figure 2, was con-
 Col transitions                −0.64           −1                        tributed by one very determined human. As, for each move,
 Pits                           −1              −0.61                     the model was allowed as much decision time as it needed
 Wells                          −0.13           −0.05                     and as the time for the model to move a piece into position
                                                                          was essentially instantaneous, our representative of humanity
                                                                          was allowed to play with gravity turned off ; that is, unless
                                                                          the human held down the drop key, the zoids did not drop.
(4Lines) were cleared at once. The first three objective func-
                                                                          This enabled our champion to score 246,186 points (see the
tions are typically displayed to human players during the
                                                                          red line towards the top of each panel in Figure 2).
game (as shown by the right-middle portion of Figure 1).
However, in all of our human games, humans were told to
optimize the first; namely, Score. The fourth objective func-                                                     Objective Function:        4Lines    Level   Lines   Score
tion, clearing four lines with one zoid, rewards 13.3 times as
many points as does using four zoids to each clear one line.                                                             Human High Score -- 246,186
Note that clearing four lines at once is called a “Tetris” and
                                                                                                      2e+05
gives the game its name. Human experts often report that the
                                                                                                                                                                                    All Factors
setting up and execution of these "Tetris" moves composes a
                                                                           Score in Thousands of Points
large part of their game strategy.
                                                                                                      1e+05
   At the end of each generation, before the next set of 100
controllers was generated, the new starting controller played
30 test games, consisting of 3 games each of 10 preselected                                           0e+00
game seeds (the game seeds produce different randomizations
                                                                                                                         Human High Score -- 246,186
of the sequence of zoids). The average score of these 30
                                                                                                                                                                                    Dellacherie's Factors
games was used to track the learning of the model over each                                           2e+05
generation. The mean scores for these 30 games is plotted,
for each of the 8 objective functions, in Figure 2.
                                                                                                      1e+05
Results
   Learning Feature Weights. Table 2 shows the final
                                                                                                      0e+00
weights (normalized) of the six Dellacherie features for the                                                  0              20                 40             60              80
Lines and Score models. Key differences between the strate-                                                                              Generations
gies employed by each model can be observed within these                                      Figure 2. Learning curve of models. (See text for discussion
numbers. For example, “eroded zoid cells” is clearly favored                                  of the Human High Score.)
by the Lines model’s moderately positive weight of +0.32,
but less emphasized by the Score model’s strongly negative                Discussion
weight of −0.81. These different weightings are character-
ized by behavioral differences between the models in that the                Study One tells us that machine learning models that focus
Lines model seeks to clear as many lines as possible (thus                on 1-zoid or 1-step optimization do pretty well compared to
eroding the zoid cells), whereas clearing lines is not as em-             humans who presumably are attempting to optimize place-
phasized in the Score model, allowing it instead to build up to           ments of two successive zoids (i.e., the current zoid and the
higher score payoffs.                                                     zoid shown in the Preview Box) while planning for longer se-
   Controller Performance. As Figure 2 shows, the                         quences, such as deliberately arranging the pile so as to clear
biggest effect on skilled performance came from the choice                off four rows with one I-Beam zoid. Likewise, the better
of objective function. Models optimized for Lines and Level               human players also work deliberately over many successive
quickly reached a score threshold of a little under 100,000,              zoids to prepare the playing board for high scoring opportu-
and then performed consistently at that threshold. Models                 nities while preventing disasterous buildups (of course, the
optimized for Score and 4Lines took longer to reach a score               CERLs do some of these things as well, see Table 1, Feature
threshold, and that threshold, while higher than those opti-              1, Landing Height).
mized for Lines and Level, was much more variable.                           Of course, our humans are working under more constraints
                                                                   2190

4
than our CERLs. Unlike humans, once a decision is made, the         tend to use them whenever available. Given the desirability
CERLs instantianeously rotate, move, and place the current          of closing such pits whenever possible, we ranked the human
zoid into the desired location. Hence, it may be easy for hu-       use of a slide as equivalent to the best move considered by the
mans to match CERL performance when the game is at level            model. (Overhang moves were made by humans 0.74% of the
one and it takes a zoid 25 s to drop from top to bottom, but        time.)
not nearly as easy when the game is at level 9 and a zoid takes
only 2 s to fall the same distance!
   These observations raise the question as to how much of
human performance could be accounted for by 1-step opti-
mizations and whether or when such optimizations need to be
superseded by other human strategies.
                          Study Two
   In Study Two, we used each of the 8 trained controllers
from Study One (two sets of factors by four objective func-
tions), to classify nearly 370,131 episodes of Tetris (all
episodes from each of our 67 human players) as to whether
the location where the human placed the zoid, matched the               Figure 3. An overhang maneuver entails placing all or part of
location that the controller would have placed the same zoid            a zoid underneath parts of the pile. In the example, the player
on the same board configuration.                                        wishes to place the falling zoid where the shadow zoid is.
Methods
                                                                    Results
   Human Gameplay. All human Tetris games were col-
lected in session one of a four session, 6-hr Tetris study.             Our 8 models were derived by crossing the four objective
Session one was “free play” as the scores obtained in ses-          functions (Lines, Levels, Score, and 4Lines) with our two sets
sion one were used to assign players to Tetris conditions           of features (ALL 48 and DELL 6). Applying these models to
for the remaining three sessions of the study. All humans           classifying human performance produces 8 statistical models
used the Meta-T (Lindstedt & Gray, 2015) experimental               of human performance. In this section, we seek to identify
task environment to play Tetris and which also collected all        the best statistical model where best is defined both in terms
keystrokes, eye data, and system events with millisecond ac-        of the model’s success at classifying human moves and by
curacy. Hence, these games can be considered as normal              parsimony; that is, the ALL 48 and DELL 8 feature sets vary
play, uninfluenced by experimental manipulations, albeit un-        in size and a trivial prediction would be that the larger feature
der laboratory conditions.                                          set fits the data better than the smaller set. Hence, as we move
   Matching Humans to Models. For each episode in the               from the realm of purely machine learning considerations, to
human dataset, the board configuration and current zoid were        models that help explain human performance, we want to be
given to each of the 8 final models from Study One. Each            sure that the models we settle on have the fewest assumptions
model evaluated a move score for all available moves, and           that are reasonable.
returned the highest scoring move. The model’s chosen move              In winnowing out our 8 models we rely on the results of
was compared to the move made by the human, and was con-            Multiple Regression modeling and the Akaike Information
sidered to have matched the human if the model chose the            Criterion (AIC). Crawley (2013), describes AIC as “penal-
same move as the human.                                             ized log-likelihood” as it weighs the fit of a model against the
   A move was also counted as a match under two additional          number of parameters used; the more parameters, the more
conditions. First, it is often the case that the model will         the model is penalized, and a better fit is required if the model
equally rank two or more moves. In this case, the model             is to be seriously considered.
breaks its tie by random choice. Therefore, in cases where              Eliminating Two Objective Functions: Lines and Lev-
the human’s move was equally ranked with the model’s, we            els. We begin by collapsing over feature sets to compute
considered that the model’s move, matched human choice.             four regression models of the form, lm(percent_match ∼
(These sitations occurred 1.39% of the time.) Second, hu-            f eature_set), and computing one AIC for each model. This
mans were capable of making one move that the models could          yields AICs for Lines and Levels of -401 and -398 and AICs
not; namely, humans could slide a zoid under an overhang            for Score and 4Lines of -446 and -452. As for AICs, smaller
left by another zoid (see Figure 3). Our current search algo-       is better, we conclude that there are sufficient differences
rithm considers these overhangs as inaccessible pits, but ex-       among our Objective Functions to justify eliminating models
perienced human players recognize these moves quickly and           with Lines and Levels from further consideration.
                                                               2191

                                                                        MODELING TETRIS                                                                        5
   Eliminating the All Features Models. We now com-                                      Score and 4Lines models, provide significant fits to the data (both
pare the remaining 48 feature, ALL models with the 6 fea-                                p0 s < .00001) with the Multiple R-Sq for Score accounting for 0.33
ture, DELL ones. Following Crawley (p. 416), we find                                     of the variance and that for 4Lines accounting for 0.32. However,
that R’s AIC function, e.g., AIC(Feature.ALL.OF.S core)                                  a clearer picture can be gained by looking at Figure 4 which plots
                                                                                         the proportion matched for each of the 67 Tetris Players based on
produces the same result as the loglinear model −2 ∗
                                                                                         the log of their criterion score (i.e., the mean of their highest 4 game
logLik(data.ALL.score) + 2 ∗ (3). Hence, we modify the log-
                                                                                         scores).
linear model by adding the number of features into the final
df term with the following results:
                                                                                         Discussion
−2 ∗ logLik(model.DELL.S core) + 2 ∗ (3 + 6) for an AIC of -237
−2 ∗ logLik(model.DELL.4Lines) + 2 ∗ (3 + 6) for an AIC of -236                             When we began the modeling work, we entertained the hypoth-
−2 ∗ logLik(model.ALL.S core) + 2 ∗ (3 + 48) for an AIC of -152                          esis that the models would do better at predicting novice than ex-
−2 ∗ logLik(model.ALL.4Lines) + 2 ∗ (3 + 48) for an AIC of -165                          pert performance, as we believed that novices tended to think only
   As lower is better for an AIC score, these comparisons led us to                      one zoid ahead whereas experts constantly planned for Tetrises and
conclude that the simpler Dellacherie models provide the better fit                      other manuevers. To our surprise, as Figure 4 shows, models match
and to drop the remaining All Features models from further consid-                       human performance from about 45 to 65 percent of the time with the
eration.                                                                                 by-player match increasing linearly from the poorest to best players.
                                                                                            Of course, a reasonable question to ask is always, “what is
                    0.70                                                                 chance?” Although we are not quite sure how to calculate chance for
                                                                                         a zoid placement, with 10 columns in which a zoid can be dropped
                    0.65
                                                                                         and 1 orientation for the Square, 2 each for the I-Beam, S, and Z,
                                                                                         and 4 each for the T, L and J, we believe that a conservative estimate
                                                                                         of chance is around 5%. Hence, we are somewhat surprised at how
                    0.60
                                                                                         good of a job these machine learning models are doing at classifying
                                                                                         human behavior.
                                                                               Score
                    0.55                                                                    Finally, we are intrigued by the two right-most points in both
                                                                                         halves of Figure 4. These are our two very best human players and
                    0.50
                                                                                         they seem to be showing a prediction plateau. Do these points rep-
                                                                                         resent the limits of prediction for models based on one-zoid opti-
                                                                                         mization? Or would our data show a continued upward slope if our
 Proportion Match
                    0.45
                                                                                         dataset included more and stronger human Tetris players?
                    0.70
                                                                                                                     Conclusion
                    0.65
                                                                                             We see four directions forward for this line of Tetris research.
                                                                                         First is the question as to how the objective functions of Score and
                                                                                         4Lines vary. It is clear for humans that clearing four lines increases
                    0.60
                                                                                         score tremendously (13.3 times more points for clearing four lines
                                                                                         with one zoid than than clearing one line with each of four zoids).
                    0.55
                                                                               4Lines    Are these two really separable or does their similiarity in matching
                                                                                         human choice imply they are measuring the same thing? Second,
                    0.50
                                                                                         we want to revisit our All Factors set to determine if some of the
                                                                                         potential power from the additional factors was wasted as subsets of
                                                                                         different factors were calculating the same outcome in some situa-
                    0.45
                                                                                         tions but different outcomes in other situations. This might result in
                                                                                         two subsets of predictors that were competing with each other rather
                           8        9             10
                                           log(criterion)
                                                             11          12              than cooperating. Third, we are very intrigued with the increasing
                                                                                         success of CERL classifications with increasing human expertise.
                Figure 4. Proportion of moves classified for each of our 67
                                                                                         Tutoring complex perceptual-motor-cognitive skills in real-time is
                human player for the best feature set (Dellacherie) and the
                                                                                         very difficult and seldom done well. We wonder whether our CERL
                two best matching Objective Functions – Highest Score and
                                                                                         models could be effectively used to provide immediate feedback to
                Number of 4Lines. Each data point in the figure represents
                                                                                         novice or intermediate human Tetris players. Fourth, it is intrigu-
                the model’s ability to predict the move that the human would
                                                                                         ing to ponder whether the two left-most points in Figure 4 represent
                make. The x-axis shows human performance in terms of the
                                                                                         a flattening of CERL powers of classification at the higher levels
                log of each human’s highest game score.
                                                                                         of human expertise or whether the slope would continue upward if
   Fit of CERL Models to Human Data. So far we have                                      more and stronger human players were found. Perhaps the appar-
been more concerned with reducing our set of models than we                              ent flattening reflects the limits of one-zoid optimization for Tetris
have been with what they suggest to us about the human data.                             and the beginning of a role for human strategies requiring extreme
Both of the remaining two models, the Dellacherie versions of the                        expertise?
                                                                                  2192

6
                      Acknowledgements                                Lindstedt, J. K. & Gray, W. D. (2015). MetaT: Tetris as an Ex-
                                                                           perimental Paradigm for Cognitive Skills Research. Be-
   Address all correspondence to Wayne Gray. <grayw@rpi.edu>.
The work was supported, in part, by grant N000141310252 to Wayne
                                                                           havior Research Methods. doi:10 . 3758 / s13428 - 014 -
D. Gray from the Office of Naval Research, Dr. Ray Perez, Project          0547-y
Officer. Thanks to Özgur Simsek for introducing the authors to        Linn, M. C. & Petersen, A. C. (1985). Emergence and char-
CERLs.                                                                     acterization of sex differences in spatial ability: a meta-
                                                                           analysis. Child Development, 56(6), 1479–1498. doi:10.
                           References                                      1111/j.1467-8624.1985.tb00213.x
Baccherini, D. & Merlini, D. (2008). Combinatorial analy-             Maglio, P., Wenger, M. J., & Copeland, A. M. (2008). Evi-
    sis of Tetris-like games. Discrete Mathematics, 308(18),               dence for the role of self-priming in epistemic action: ex-
    4165–4176. doi:{10.1016/j.disc.2007.08.009}                            pertise and the effective use of memory. Acta Psycholog-
Crawley, M. J. (2013). The R Book (Second). John Wiley &                   ica, 127(1), 72–88.
    Sons, LTD. doi:10.1002/9781118448908                              Martin-Gutierrez, J., Luis Saorin, J., Martin-Dorta, N., &
Destefano, M., Lindstedt, J. K., & Gray, W. D. (2011). Use                 Contero, M. (2009). Do Video Games Improve Spatial
    of complementary actions decreases with expertise. In L.               Abilities of Engineering Students? International Journal
    Carlson, C. Hölscher, & T. Shipley (Eds.), Proceedings                 of Engineering Education, 25(6, SI), 1194–1204.
    of the 33rd Annual Conference of the Cognitive Science            Mayer, R. E. (2014). Computer games for learning: an
    Society (pp. 2709–2014). Austin, TX: Cognitive Science                 evidence-based approach. Cambridge, MA: MIT Press.
    Society.                                                          Newell, A. (1973). You can’t play 20 questions with nature
Fahey, C. P. (2013). Tetris AI. Retrieved from http : / / www.             and win: projective comments on the papers of this sym-
    colinfahey.com/tetris/                                                 posium. In W. G. Chase (Ed.), Visual information process-
Gabillon, V., Ghavamzadeh, M., & Scherrer, B. (2013). Ap-                  ing (pp. 283–308). New York: Academic Press.
    proximate dynamic programming finally performs well in            Okagaki, L. & Frensch, P. A. (1994). Effects of video game
    the game of Tetris. In Advances in neural information                  playing on measures of spatial performance: gender ef-
    processing systems (Vol. 26, pp. 1754–1762). Retrieved                 fects in late adolescence. Journal of Applied Develop-
    from http://media.nips.cc/nipsbooks/nipspapers/paper_                  mental Psychology, 15(1), 33–58. Retrieved from http :
    files/nips26/881.pdf                                                   / / search . ebscohost . com . libproxy. rpi . edu / login . aspx ?
Gray, W. D., Hope, R. M., Lindstedt, J. K., & Destefano,                   direct = true & db = psyh & AN = 1994 - 44678 - 001 & site =
    M. (2014). Elements of extreme expertise: searching for                ehost-live&scope=site
    differences in microstrategies deployed by experts and            Sims, C. R. (2011). Internal models of embodied dynamics:
    novices. In Plenary Presentation at the 12th Biannual                  a computational theory of learning in routine interactive
    Meeting of the German Cognitive Science Society. Uni-                  behavior (Doctoral dissertation, Rensselaer Polytechnic
    versität Tübingen. Tübingen, Germany. doi:10.13140/2.                  Institute, Troy, NY).
    1.3809.1529                                                       Stuart, K. (2010). Tetris and Snake - the biggest games in the
Holmes, E. A., James, E. L., Coode-Bate, T., & Deeprose, C.                world. The Guardian. Retrieved October 20, 2012, from
    (2009). Can playing the computer game “Tetris” reduce                  http://gu.com/p/2e2kk/em
    the build-up of flashbacks for trauma? A proposal from            Szita, I. & Lorincz, A. (2006). Learning Tetris using
    cognitive science. PLoS ONE, 4(1), e4153. doi:10.1371/                 the noisy cross-entropy method. Neural Computation,
    journal.pone.0004153                                                   18(12), 2936–2941.
Kendall, G., Parkes, A., & Spoerer, K. (2008). A survey of            Terlecki, M. S., Newcombe, N. S., & Little, M. (2008).
    NP-complete puzzles. ICGA JOURNAL, 31(1), 13–34.                       Durable and generalized effects of spatial experience on
Kirsh, D. & Maglio, P. (1994). On distinguishing epistemic                 mental rotation: gender differences in growth patterns.
    from pragmatic action. Cognitive Science, 18(4), 513–                  Applied Cognitive Psychology, 22(7), 996–1013.
    549. doi:10.1016/0364-0213(94)90007-8                             Thiery, C. & Scherrer, B. (2009). Improvements on learning
Lindstedt, J. K. (2013). Identifying Expertise: Data Explo-                Tetris with cross-entropy. ICGA Journal, 32(1), 23–33.
    ration in Tetris (Master’s thesis, Rensselaer Polytechnic         Thiery, C. & Scherrer, B. (2009). Building controllers for
    Institute).                                                       tetris. ICGA Journal, 32(1), 3–11.
Lindstedt, J. K. & Gray, W. D. (2013). Extreme expertise:
    Exploring expert behavior in Tetris. (pp. 912–917).
                                                                 2193

