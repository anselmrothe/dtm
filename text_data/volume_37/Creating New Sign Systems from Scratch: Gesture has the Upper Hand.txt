Creating a New Communication System: Gesture has the Upper Hand

!

Casey J. Lister (casey.lister@research.uwa.edu.au)
School of Psychology, University of Western Australia,
35 Stirling Highway, Crawley WA 6009

!

Nicolas Fay (nicolas.fay@gmail.com)

School of Psychology, University of Western Australia,
35 Stirling Highway, Crawley WA 6009

!

T. Mark Ellison (mark.ellison@uwa.edu.au)

School of Psychology, University of Western Australia,
35 Stirling Highway, Crawley WA 6009

!

Jeneva Ohan (jeneva.ohan@uwa.edu.au)

School of Psychology, University of Western Australia,
35 Stirling Highway, Crawley WA 6009

!!

Abstract
How does modality affect our ability to create a new
communication system? This paper describes two
experiments that address this question, and extend prior
related findings by drawing from a significantly more
extensive list of concepts (over 1000) than has been used
previously. In Experiment 1, participants communicated
concepts to a partner using either gestures or non-linguistic
vocalizations (sounds that are not words). Experiment 1
confirmed that participants who gesture 1) produce more
strongly ‘motivated’ signs that physically resemble the
concepts they represent (i.e., are iconic), 2) are better able to
correctly guess the meaning of a partner’s signs, and 3) show
stronger alignment on a shared inventory of signs. Experiment
2 addressed a limitation of Experiment 1 (concurrent feedback
only in the gesture condition). In Experiment 2 concurrent
feedback was eliminated from the gesture and vocal
conditions. Gesture again outperformed vocalization on
communication effectiveness and sign alignment.
Keywords: Alignment; Gesture; Vocalization; Multimodal;
Motivated; Signs; Language Origin; Embodied Cognition

Introduction
‘What’s in a name? That which we call a rose
By any other name would smell as sweet’
William Shakespeare (trans. 1914. 2.2. 47-48)
Most of the words we use to communicate are arbitrarily
associated with their referents (Saussure, 1959). As
Shakespeare observed, the word ‘rose’ conveys its meaning
through learned convention, without which, that particular
combination of phonemes would be meaningless. How then,
do words acquire their meaning, and how did Homo sapiens
bootstrap the complex communication systems that make
our species so unique and successful?
Several competing theories of language origin have been
proposed. The proto-speech account (e.g., Cheney &
Seyfarth, 2005) suggests that language evolved out of
rudimentary vocalizations that acquired communicative
meaning over time, while the proto-sign account argues that

language evolved first from manual gestures, before shifting
to the vocal modality (Corballis, 2003; Arbib, 2005).
Because modern humans already possess complex, shared
language systems, we are unable to experimentally replicate
the context in which language arose. However, comparing
communication in the vocal and gestural modalities allows
us to make inferences about the characteristics of human
communication that equipped our ancestors to develop
complex sign systems. Fay, Lister, Ellison and GoldinMeadow (2014) compared the communicative affordances
of gesture and vocalization through a referential
communication task in which participants were prohibited
from using their shared language. Participants
communicated a set of 18 recurring concepts to a partner,
either through gestures-only, non-linguistic vocalizationsonly, or a combination of both. In line with the proto-sign
account, participants who gestured were more successful at
communicating meanings to their partner than the
participants who were restricted to the vocal modality.
Participants who gestured were also more likely to
reproduce the signs that their partner had previously used
when communicating the same concepts. This process,
known as interactive alignment (Pickering & Garrod, 2004),
underlies the development of a shared inventory of signs.
Sign alignment was also positively related to participants’
communication success. Fay et al. (2014) suggested that
gesture was a more successful mode of communication
compared to vocalization because it more naturally lends
itself to the production of ‘motivated’ signs (i.e. iconic or
indexical signs that share a direct, or non-arbitrary,
relationship with their referent). While the authors did not
directly examine sign motivation, they suggested that
participants who gestured were better able to physically
represent the concepts they wished to communicate (e.g.,
through mimicry or pantomime).
Studies such as these indicate a critical role for gesture in
communication. Theories of embodied cognition and
gesture as simulated action (e.g., Hostetter & Alibali, 2008)
suggest that our language and motor pathways are
intimately connected, both neurally and behaviourally.

1386

However, recent studies have shown that participants are
also able to produce motivated vocalizations (Perlman,
Dale, & Lupyan, 2014). Perlman et al. (2014) demonstrated
that, like gestures, motivated vocalizations can convey
meaningful information, and may also be capable of
bootstrapping human language.
These studies, like other referential communication
studies, are limited by the small number of concepts used. In
most referential communication tasks, participants
communicate the same 20 concepts (or fewer) to a partner
(e.g., Garrod, Fay, Lee, Oberlander & MacLeod, 2007; Fay,
Arbib and Garrod, 2013; Perlman et al., 2014; Fay et al.,
2014). Thus, current knowledge about the emergence of
human sign systems is limited to a small number of
experimenter-selected concepts. Experiment 1 addresses this
issue.

only condition. Hypothesis 2 is that communication success
will be higher for gesture than for non-linguistic
vocalization. Hypothesis 3 is that sign alignment will be
higher in the gesture-only condition than in the vocal-only
condition.

Method
All participants viewed an information sheet before giving
written consent to take part in Experiment 1. Information
sheets and consent forms were approved by the University
of Western Australia Ethics Committee.

!

Participants One hundred and six undergraduate students
(sixty-three females) participated in exchange for partial
course credit. Participants were placed into unacquainted
pairs, and completed the testing session in approximately
one hour. All were free from auditory, visual, speech and
motor impairments.

Experiment 1
Experiment 1 extends the Fay et al (2013, 2014) referential
communication studies by dramatically increasing the range
of concepts participants communicate. Instead of presenting
all participants with the same set of 18 recurring concepts,
the present study samples without replacement from a set of
1000 of the most common adjectives, nouns and verbs in the
English language (from the Corpus of Contemporary
American English; Davies, 2008). This is the first referential
communication study to sample from such an extensive
range of concepts, reducing the likelihood that any findings
are an artifact of a specific stimuli set.
Perlman et al. (2014) demonstrated that it is possible to
produce motivated signs through non-linguistic
vocalization, suggesting that the gesture modality is not
unique in its affordance of motivated signs. Our experiment
extends this work by providing a direct comparison between
the vocal and gestural modalities (as the authors explored
the vocal modality alone). Experiment 1 includes a gestureonly and vocal-only condition. While Fay et al. (2013, 2014)
speculated that gesture outperformed vocalization owing to
its affordance of motivated signs, they did not examine sign
motivation. Experiment 1 compares sign motivation in the
different modalities by having coders rate each sign
produced in each modality in terms of the degree of sign
motivation (ionic to arbitrary).
Pairs of participants communicated a range of different
concepts (Adjectives, Nouns, Verbs) to a partner in each
communication modality (Gesture-only, Vocal-only).
Participants communicated the same concepts repeatedly,
over 6 games. From game to game, participants
alternated roles between Directing (i.e. attempting to
communicate their list of words to their partner), and
Matching (i.e. trying to guess what words their partner was
communicating). By alternating roles across games, the
participants were able to copy (or not) features of their
partner’s signs. Participants’ signs were then rated in terms
of degree of sign motivation, and the extent to which they
copied, or aligned with, their partner’s previously produced
sign for the same meaning.
In line with the speculations made by Fay et al. (2014),
Hypothesis 1 is that sign motivation will be higher for signs
produced in the gesture-only condition than in the vocal-

!

Materials Participants tried to communicate (in pairs) a set
of 18 target concepts. The concepts were sampled without
replacement from 1000 of the most frequently used words in
American English (from the Corpus of Contemporary
American English; Davies, 2008), and fell equally into three
categories: adjectives, nouns and verbs. Participants were
also presented with six distractor concepts that were never
communicated. A different item set was communicated by
each pair.

!

Task and Procedure Participants completed two referential
communication tasks (gesture-only and vocal-only). Each
task was comprised of six separate games. During each
game, one participant (the Director) would communicate
their list of 18 recurring concepts to their partner (the
Matcher). At the end of each game, participants would swap
roles, so that the participant who had acted as Director
would become the Matcher for the next game, and the
Matcher would become the Director.
Each pair of participants communicated a different set of
concepts in the gesture-only and vocal-only modalities.
Communication modality was counterbalanced across
participants. In the gesture-only condition participants were
seated facing one another, and were only allowed to
communicate through gesture (i.e., movements of the hand,
body and face). In the vocal-only condition participants
were seated facing away from each other to eliminate the
possibility that they might communicate through gesture.
Participants in this condition communicated through nonlinguistic vocalizations (i.e., sounds that are not words, and
are made with the body or vocal chords).
iPads were used to run the experiment. During each game,
the to-be-communicated concepts would appear sequentially
on the Director’s iPad. The Matcher’s screen would display
all 18 target concepts, plus 6 distractor concepts, for the
duration of the game. Matchers would try to guess which
concept the Director was communicating, and would select
that concept using their touch screen. Following the
Matcher’s selection, the next to-be-communicated concept
would appear on the Director’s screen. Matchers were
allowed to select the same concept more than once within

1387

the same game, however every concept was only presented
to the director once in each game. Directors were allowed to
produce as many gestures or vocalizations as they wished
for each concept. Once all 18 concepts had been
communicated, that game would end, participants would
swap directing/matching roles, and begin the next game.
Sign motivation, Communication success and sign
alignment were measured.
Motivation Sign motivation was quantified using a 7-point
likert scale. Here, a rating of 0 indicates that the sign is
entirely symbolic and bares no physical resemblance to the
concept being communicated. A rating of 6 indicates that the
sign is highly motivated, and is either an icon or an index of
the concept being communicated. When directors produced
multiple gestural or vocal signs for a concept, the
motivation of each distinct sign was rated separately. These
ratings were then used to calculate a mean motivation score
for each concept, at each game.
One coder (CJL) coded all signs for sign motivation.
Signs produced by 12 participant pairs were coded for sign
motivation by a second coder who was naïve to the
experimental hypotheses. This gave 2592 separate
observations (~20% of the data). Intraclass correlations
demonstrated high reliability between the coders for signs in
the Gesture-only (82%) and Vocal-only (89%) conditions
(ps < .001).
Hypothesis 1 was that sign motivation would be higher
for gestured signs than for signs produced using nonlinguistic vocalization. For analysis, we took averages of the
sign motivation ratings at each game, across each category
of concept (Adjective, Noun, Verb). The data was entered
into a three-way repeated measures ANOVA that treated
Modality (Gestural, Vocal), Game (1-6) and Concept
(Adjective, Noun, Verb) as within-participants factors. This
returned a significant main effect of Condition, F(1, 52) =
700.33, p < .001, confirming that participants who gestured
produced more motivated signs than those who vocalized;
and Game, F(5, 260) = 39.89, p < .001, reflecting an
increase in sign motivation across games 1-6 in both
conditions. There was also a main effect of Concept, F(2,
104) = 5.73, p = .004, with pairwise comparisons showing
participants’ signs for verbs and nouns to be significantly
more motivated than those they produced for adjectives, ps
< .05. There was no difference between the motivation of
verbs and nouns, p > .05. Finally, there was also a
significant interaction between Game and Condition, F(5,
260) = 3.64, p = .003. To explore the interaction, one-way
repeated measures ANOVAs were conducted upon each
condition. These confirmed that motivation scores increased
across games 1-6 in both the Gesture and Vocalization
conditions. Difference scores between each condition were
calculated at game 1 and game 6. A paired samples t-test
revealed a greater difference in sign motivation between
conditions at game 1 (M = 2.99, SD = .84) compared to
game 6 (M = 2.77, SD = .86), t(52) = 2.17, p = .04, d = .60.

Gesture

Vocalization

100%

Identification Accuracy (%)

Results and Discussion

Communication Success Communication success was
assessed in terms of ‘identification accuracy’; the
percentage of correct guesses made by the matcher within
each game and across each concept category (Adjective,
Noun, Verb). Hypothesis 2 was that communication success
would be higher for gesture than for non-linguistic
motivation. As Figure 1 shows, participants’ identification
accuracy increased across games 1-6 in both conditions and
across all concept categories. Across all games
communication success was higher in the gesture-only
condition than in the vocal-only condition. The different
concept types were communicated equally well.

Adjectives
Nouns
Verbs

75%
50%
25%
0%

1

2

3

4

Game

5

6 1

2

3

4

5

6

Game

Figure 1: Participants’ communication success across
games 1-6 in the Gestural and Vocal modalities.
A three-way, repeated measures ANOVA that treated
Modality (Gestural, Vocal), Game (1-6) and Concept
(Adjective, Noun, Verb) as within-participants factors
confirmed these observations. There was a significant main
effect of Condition, F(1, 52) = 591.95, p < .001, confirming
that participants who gestured outperformed those who
vocalized; a significant main effect of Game, F(5, 260) =
95.63, p < .001, reflecting improvement in identification
accuracy across games in both conditions; and a significant
interaction between Game and Condition, F(5, 260) = 3.47,
p = .005. There was no main effect of Concept, F(2, 104) =
1.10, p > .05.
To examine the nature of the interaction, one way
repeated measures ANOVAs were conducted upon each
condition. These confirmed that identification accuracy
increased across games 1-6 in both conditions. Difference
scores between each condition were calculated at game 1
and at game 6. A paired samples t-test confirmed that there
was a greater difference in identification accuracy between
conditions at game 6 (M = .45, SD = .18) compared to game
1 (M = .37, SD = .17), t(52) = -2.29, p = .03, d = .64. This
reflects a greater rate of increase in identification accuracy
in the Gesture condition across games 1-6 compared to the
Vocal condition.

!

1388

Alignment Alignment was quantified using a coding
scheme that compared the similarity between the sign a
participant produced, and the sign their partner produced on
the previous game when communicating the same concept.
Ratings were made on a 7-point likert scale, where 0
indicates that the participant did not copy the sign
previously used by their partner at all, and 6 represents a
near identical copy of the partner's previous sign. A single
alignment rating was made for each concept between games
(i.e., between games 1-2, 2-3, 3-4, 4-5, 5-6).
Sign similarity was coded by one person (CJL). To
establish reliability, signs produced by 12 participant pairs
in each condition, across all games, were coded by a second
coder who was naïve to the experimental hypotheses. This
gave 2592 independent codings for sign motivation (~20%
of the data). Intraclass correlations demonstrated high
reliability between the coders for signs produced in the
Gesture-only (96%) and Vocal-only (91%) conditions (ps < .
001).
Hypothesis 3 was that alignment would be higher in the
gesture-only condition than in the vocal-only condition.
Participants’ alignment scores were entered into a three-way,
repeated measures ANOVA that treated Modality (Gestural,
Vocal), Game (2-6) and Concept (Adjective, Noun, Verb) as
within-participants factors. This returned a significant main
effect of Condition, F(1, 52) = 437.74, p < .001, confirming
that participants who gestured aligned more than those who
vocalized; and a significant main effect of Game, F(4, 208)
= 256.49, p < .001, reflecting increased alignment in both
conditions across games. There was also a significant main
effect of Concept, F(2, 104) = 3.39, p = .04, with pairwise
comparisons showing significantly more alignment upon
signs for nouns than for adjectives, p < .01. There was no
difference in alignment between adjectives and verbs, or
nouns and verbs, ps > .05.
Finally, there was a significant interaction between Game
and Condition, F(4, 208) = 4.12, p = .003. To examine the
nature of the interaction, difference scores were calculated
for alignment at game 2 and at game 6. A paired samples ttest confirmed that difference scores were significantly
higher at game 6 (M = 2.84, SD = 1.09) than at game 2 (M =
2.51, SD = .84), t(52) = -2.52, p = .02, d = .70. This
indicates that the difference in alignment between the
gesture and vocal modality increased across games.

!

identification, and greater identification accuracy leading to
increased alignment between interacting dyads.

Identification Accuracy

Identification Accuracy and Alignment
100%

Gesture
Vocal

75%
50%
25%
0%

0

1.5

3

4.5

6

Sign Alignment

Figure 2: Relationship between identification accuracy
and alignment in the Gesture and Vocal conditions.

Identification Accuracy

Sign Motivation and Identification Accuracy
100%

Gesture

Vocal

75%
50%
25%
0%

0

1.5

3

4.5

6

Sign Motivation

Figure 3: Relationship between sign motivation and
identification accuracy in the Gesture and Vocal conditions.

Experiment 2

Motivation, Communication Success and Alignment
Fay et al. (2014) speculated that greater sign motivation led
to greater communication success. They also demonstrated
that greater communication success led to increased
alignment. We conducted bivariate correlations to explore
the relationships between all three variables (collapsed
across conditions). These revealed strong positive
relationships between identification accuracy and alignment,
r(52) = .87, p < .001 (see Figure 2), identification accuracy
and sign motivation, r(52) = .79, p < .001 (see Figure 3),
and sign motivation and alignment, r(52) = .85, p < .001.
The strong relationships observed between sign motivation
and identification accuracy, and identification accuracy and
alignment, suggest that these processes are intimately
linked, with increased motivation facilitating accurate

Experiment 2 addresses a potential limitation of Experiment
1. To prevent participants in the vocal-only condition
conveying meanings to their partner through gesture,
Experiment 1 participants sat back-to-back during the
experiment. However, this may have disadvantaged
participants in the vocal-only condition by preventing them
from seeing their partner’s facial expressions of confusion
or comprehension. This type of concurrent feedback was
possible in the gesture-only condition, but not in the vocalonly condition. Experiment 2 addresses this issue. The
vocal-only and gesture-only conditions of Experiment 2 are
non-interactive, thereby eliminating all concurrent feedback.
In Experiment 2 participants try to communicate each
concept alone, and the sign they produce (vocal or gestured)
is recorded and played back to a partner who tries to identify
the intended meaning, and then tries to communicate each
of the concepts themselves (again recorded). Sign
motivation, communication success and alignment are
measured. We expect greater sign motivation, increased
identification accuracy, and greater alignment in the gestureonly condition than in the vocal-only condition. We expect
to find positive correlations between each of these measures,
as in Experiment 1.

1389

Method
All participants viewed an information sheet before giving
written consent to take part in Experiment 2. Information
sheets and consent forms were approved by the University
of Western Australia Ethics Committee.

!

Participants Sixty undergraduate students (42 females)
participated in exchange for partial course credit, and
completed the testing session in ~30 minutes. All were free
of auditory, visual, speech and motor impairments.

Paired samples t-tests showed no significant differences
between the motivation of different concepts in the
vocalization condition. However, in the gesture condition,
verbs were communicated more successfully than adjectives
and nouns ts(29) > -2.30, ps < .04, ds < .85 (see Figure 4).
There was no main effect for Game, and there were no
interaction effects, [Fs(1, 29) < .239, ps > .13].
Sign Motivation

!

Sign Motivation

6

Materials The corpus of concepts used in Experiment 2 is
the same as that used in Experiment 1. As fewer participants
took part in Experiment 2, fewer concepts were sampled
(540).

!

Task and Procedure As in Experiment 1, Experiment 2
participants produced gesture and vocal signs to
communicate lists of concepts. However, participants in
Experiment 2 took part individually, communicating signs
to a video camera instead of a partner. Consequently, testing
for Experiment 2 took place in two stages. At stage one,
participants communicated their list of concepts to a camera
(acting as Director). At stage two, their partner viewed these
recordings, and tried to guess which concepts were being
communicated (acting as Matcher). They were then
presented with the same list of concepts they had viewed in
the recording (in a different order), and asked to
communicate these concepts to the camera.
Again Directors were presented with one concept at a
time on an iPad. As opposed to waiting for a partner to make
their guess (Experiment 1), Directors clicked a button to
progress to the next concept. When Matching, participants
were presented with all 18 target concepts, plus 6 distractor
concepts, and made their guesses using the touch screen on
an iPad. All participants took part in the Gesture-only and
Vocal-only conditions. Only two games were played, as
opposed to six games in Experiment 1.

Results and Discussion
Sign motivation, communication success and sign alignment
were measured. To establish reliability, one coder rated all
signs for motivation and alignment (CJL). A second coder
rated the signs produced by 10 participant pairs in each
condition, across all games (~33% of the data). Intraclass
correlations demonstrated high inter-rater reliability for the
motivation ratings in the Gestural (84%) and Vocal (85%)
modalities, and for alignment ratings in the Gestural (92%)
and Vocal (91%) modalities, (ps < .001).

!

Motivation A three-way, repeated measures ANOVA that
treated Modality (Gestural, Vocal), Game (1-6) and Concept
(Adjective, Noun, Verb) as within-participants factors, was
run. There was a main effect of Condition, F(1, 29) =
561.28, p < .001, confirming that gestured signs were more
motivated than vocal signs, and a main effect for Concept,
F(2, 58) = 3.95, p = .03.

!

2
0

!

Adjective
Noun
Verb

4

Gesture

Vocalization

Figure 4: Motivation averaged across games 1 and 2.
Note: * p < .05

Communication Success A two-way, repeated measures
ANOVA that treated Modality (Gestural, Vocal) and
Concept (Adjective, Noun, Verb) as within-participants
factors was run on participants’ mean identification
accuracy scores at game 1. There was a main effect of
Condition, F(1, 29) = 95.99, p < .001, confirming that
gestured signs were communicated more successfully than
vocal signs. We found no effect of Concept on identification
accuracy, and no interaction effects [Fs(2, 58) < 2.82, ps > .
07].

!

Alignment A two-way repeated measures ANOVA that
treated Modality (Gestural, Vocal) and Concept (Adjective,
Noun, Verb) as within-participants factors was run on
participants’ mean alignment scores at game 2. There was a
main effect of Condition, F(1, 29) = 283.41, p < .001,
confirming that participants aligned more when gesturing
than when vocalizing. There was no effect of Concept, and
there were no interaction effects [Fs(2, 58) < 2.20, ps > .12].
Bivariate correlations were run on identification accuracy,
alignment and sign motivation (motivation scores were
averaged across game 1 and 2). In the Gesture condition,
moderate positive relationships were found between
identification accuracy and alignment, r(28) = .53, p < .001,
identification accuracy and motivation, r(28) = .64, p < .001
and motivation and alignment, r(28) = .61, p < .001. In the
Vocalization condition, a moderate correlation was found
between identification accuracy and alignment, r(28) = .59,
p < .001, and strong correlations were observed between
identification accuracy and motivation, r(28) = .75, p < .
001, and motivation and alignment, r(28) = .71, p < .001.
Experiment 2 replicated the pattern of results observed in
Experiment 1. By removing participant interaction from
both conditions, we eliminated any advantage participants
may have had from being face-to-face when gesturing in
Experiment 1. This indicates that the benefit of gesture over
non-linguistic vocalization observed in Experiment 1 is due
to the modality itself rather than concurrent feedback.

1390

General Discussion
Experiments 1 and 2 replicate and extend the findings of
similar experimental semiotic studies (e.g., Fay et al., 2014;
Perlman et al., 2014). Increasing the set of concepts
communicated eliminates the possibility that our results are
a consequence of the specific experimental stimuli used. In
line with Hypothesis 1, gestured signs were more strongly
motivated than vocal signs. Identification accuracy was also
higher in the Gesture-only condition, supporting Hypothesis
2. These results support the suggestion that participants who
gesture outperform those who vocalize because gesture
more naturally lends itself to the production of motivated
signs (Fay et al., 2014)
Hypothesis 3, that alignment would be greater in the
Gesture-only condition, was also supported. This finding
confirms that of Fay et al. (2014), and further demonstrates
the superiority of gesture over non-linguistic vocalization
when bootstrapping a shared sign system. Furthermore, the
strong correlation observed between sign motivation and
identification accuracy in the current study suggests that
motivated signs facilitate comprehension when interacting
partners are unable to draw on their common language. The
correlation between identification accuracy and alignment
supports the observations of Fay et al. (2014), who
suggested that these processes are mutually reinforcing (i.e.,
increased identification accuracy fosters greater alignment,
and vice-versa). We argue that motivated signs foster mutual
comprehension, and that comprehension promotes sign
alignment, which reinforces comprehension (Figure 5).

Motivated Sign
Production

Mutual
Comprehension
Alignment

Figure 5: Proposed relationship between sign motivation,
mutual comprehension, and alignment.
Because gesture more naturally lends itself to the
production of motivated signs than non-linguistic
vocalization, it follows that communication success and sign
alignment will be higher in the gesture modality.
Experiment 2 returned the same pattern of results
observed in Experiment 1, confirming the superiority of
gesture over vocalization even when concurrent feedback
between pairs of participants is eliminated. Interestingly, in
the Gesture condition, motivation scores from Experiment 2
differed by Concept type; signs participants produced for
Verbs were significantly more motivated than for Nouns or
Adjectives. This was not observed in Experiment 1, perhaps
because in Experiment 1 participants felt more pressured to
convey each concept as thoroughly as possible (owing to the
presence of a partner). Supporting this suggestion,
participants in Experiment 2 made fewer communicative

attempts per concept than those in Experiment 1. Under less
demanding circumstances, participants in Experiment 2 may
have made less effort to communicate more challenging
(i.e., less motivated) noun and adjective concepts. Thus, the
gestures produced for nouns and adjectives were less
motivated than for the comparatively easier verbs.
Returning to theories of language origin, our results
support an account in which gesture played a pivotal role. In
the absence of conventional language, it is likely that our
ancestors would have relied heavily upon motivated signs,
particularly gestured signs, to get their point across.

Acknowledgments
This research was supported by an ARC Discovery grant
(DP120104237) awarded to Nicolas Fay.

References
Arbib, M. (2005). From monkey-like action recognition to a
human language: an evolutionary framework for
neurolinguistics. Behavioral and Brain Sciences 28, 105–
167. doi: 10.1017/S0140525X05000038
Cheney, D. L., and Seyfarth, R. M. (2005). Constraints and
preadaptations in the earliest stages of language evolution.
Ling. Rev. 22, 135–159. doi: 10.1515/tlir.2005.22.2-4.135
Corballis, M. C. (2003). From mouth to hand: gesture,
speech, and the evolution of right-handedness. Behavioral
and Brain Sciences, 26, 199–260. doi: 10.1017/
S0140525X03000062
Davies, M. (2008). The Corpus of Contemporary American
English: 450 million words, 1990-present. Available
online at http://corpus.byu.edu/coca/.
Fay, N., Arbib, M., & Garrod, S. (2013). How to bootstrap a
human communication system. Cognitive Science, n/a-n/
a. doi:10.1111/cogs.12048
Fay, N., Lister, C. J., Ellison, T. M. & Goldin-Meadow, S.
(2014). Creating a communication system from scratch:
gesture beats vocalization hands down. Frontiers in
Psychology, 5. doi: 10.3389/fpsyg.2014.00354.
Garrod, S., Fay, N., Lee, J., Oberlander, J., & MacLeod, T.
(2007). Foundations of representation: where might
graphical symbol systems come from? Cognitive Science,
31(6), 961-987. doi:10.1080/03640210701703659
Hostetter, A. B. & Alibali, M. W. (2008). Visible
embodiment: Gestures as simulated action. Psychonomic
Bulletin & Review, 15(3), 495-514. doi: 10.3758/PBR.
15.3.495
Perlman, M., Dale, R., & Lupyan, G. (2014). Iterative vocal
charades: the emergence of conventions in vocal
communication. The Evolution of Language. doi:
10.1142/9789814603638_003
Pickering, M. J., & Garrod, S. (2004). Toward a mechanistic
psychology of dialogue. Behavioural and Brain Sciences,
27(2), 169-225. doi:10.1017/S0140525X04000056
Saussure, F. de. (1959). Course in General Linguistics. New
York, NY: Philosophical Library. (Original work
published 1922).
Shakespeare, W. (1914) Romeo and Juliette. (The Oxford
Shakespeare, Trans.). Oxford, UK: Oxford University
Press (Original work published 1564–1616).

1391

