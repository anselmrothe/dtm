            Computational principles underlying people’s behavior explanations
                              AJ Piergiovanni                                                       Alan Jern
                       piergiaj@rose-hulman.edu                                             jern@rose-hulman.edu
        Department of Computer Science & Software Engineering                   Department of Humanities and Social Sciences
                   Rose-Hulman Institute of Technology                                Rose-Hulman Institute of Technology
                              Abstract                                 vide rational support for the behaviors (Malle, 1999, 2004).
                                                                       Additionally, research has suggested that, when reasoning
   There are often multiple explanations for someone’s behavior,
   but people generally find some behavior explanations more sat-      about other people’s mental states, people expect others to
   isfying than others. We hypothesized that people prefer be-         behave generally rationally (e.g., Baker, Saxe, & Tenenbaum,
   havior explanations that are simple and rational. We present        2009; Ullman et al., 2009; Jern & Kemp, 2011). Dennett
   a computational account of behavior explanation that captures
   these two principles. Our computational account is based on         (1987) has called this expectation the intentional stance.
   decision networks. Decision networks allow us to formally              Behavior explanation is closely related to what social psy-
   capture what it means for an explanation to be simple and ra-       chologists call interpersonal attribution, the problem of at-
   tional. We tested our account by asking people to rate how sat-
   isfying several behavior explanations were (Experiment 1) or        tributing someone’s behavior to either dispositional or situ-
   to generate their own explanations (Experiment 2). We found         ational causes. However, the literature on interpersonal at-
   that people’s responses were well predicted by our account.         tribution has focused primarily on cognitive processes rather
   Keywords: behavior explanation; social cognition; decision          than computational principles (Anderson, Krull, & Weiner,
   networks
                                                                       1996; Gilbert, 1998). Similarly, although previous research
   Every day, we generate explanations for other people’s be-          on explanation generation suggests that people rely on sim-
havior. For example, suppose that you observe Bob as he                plicity and rationality when explaining other people’s behav-
arrives to a meeting. When he arrives, there are three people          ior, these principles have not been formally defined and uni-
already seated at a table, one at the far left of the table, one       fied in a computational framework (see, e.g., Keil, 2006).
in the middle, and one at the far right. Bob takes the seat            As a result, it is difficult to predict how the two principles
closest to the person on the left. Why did Bob choose to sit           will each influence people’s judgments when people consider
there? Perhaps he likes the person on the left, or he dislikes         explanations that vary in both simplicity and rationality (see
the person on the right, or perhaps he dislikes another per-           Pacer, Williams, Chen, Lombrozo, & Griffiths, 2013).
son who he knows will later sit on the right. Often, there are            In this paper, we present a computational account of be-
many possible explanations for people’s behavior. Neverthe-            havior explanation that formally characterizes what it means
less, some behavior explanations are intuitively more satisfy-         for an explanation to be simple and rational. Our account
ing than others. For example, you are likely to find any one           is based on the graphical modeling framework of decision
of the above explanations more satisfying than all of the ex-          networks1 (Howard & Matheson, 2005). Decision networks
planations combined: namely, that he likes the person on the           have been used previously to account for people’s inferences
far left, and dislikes the person already seated on the far right      about other people’s mental states (Jern & Kemp, 2011) but
and dislikes the person he knew would also sit on the right.           have not been used to account for people’s behavior explana-
   What makes some behavior explanations more satisfying               tions. As we show later, because decision networks can be
than others? The example above suggests that simpler expla-            ordered by network complexity, they can be used to provide
nations are more satisfying. However, a simple explanation             a formal definition of simplicity. And because decision net-
must first qualify as a valid explanation in order to be satis-        works incorporate a notion of choice utility, they can be used
fying. In other words, the explanation must provide rational           to provide a formal definition of rationality.
support for the behavior. For example, it would not make                  We begin by describing the basic properties of decision net-
sense to explain that Bob sat where he did because he does             works and explain how we use decision networks to define
not like the person he sat next to. Such an “explanation” is           simplicity and rationality. We then test the predictions of our
not satisfying because a rational actor who dislikes someone           decision network account in two experiments in which people
will generally avoid that person and therefore Bob’s behavior          judged or generated explanations of someone else’s behavior.
is left unexplained. This example suggests that there are two
principles that underlie people’s behavior explanations. We               Explaining behavior with decision networks
will refer to these principles as simplicity and rationality.          We will briefly introduce decision nets (short for decision
   The importance of simplicity and rationality in behavior            networks) using the example situation described at the be-
explanation is supported by previous research. When explain-           ginning of this paper. This situation can be represented by
ing causal events, people prefer explanations that posit fewer         the decision net in Figure 1a, where Bob has been replaced
causal relationships (Lombrozo, 2007). And when people                 with X. X’s seat choice is represented by the rectangular
generate explanations for intentional behaviors, their expla-
nations tend to refer to implicit beliefs and desires that pro-            1 Decision networks are sometimes called influence diagrams.
                                                                   1883

                                                                       planations will have fewer nodes, edges, and possible node
                                                                       values. This intuition can be captured using a definition of
                                                                       simplicity based on minimum description length (MDL; Ris-
                                                                       sanen, 1978). MDL has been used previously to account for
                                                                       aspects of reasoning (Fass & Feldman, 2002). For example,
                                                                       people find it easier to learn concepts that can be described
                                                                       by shorter codes (Feldman, 2000).
                                                                          Let S(N) be the simplicity of decision net N. We define
                                                                       S(N) as the inverse of a standard MDL-based definition of
                                                                       network complexity (De Campos, 2006):
Figure 1: Decision networks. (a) A decision network repre-                                                   1
senting a choice in which only A’s location affects X’s choice.                                 S(N) =                ,              (1)
                                                                                                        ∑i (xi · qi )
(b) The set of decision network explanations we considered
in our experiments. The networks differed in which of the              where xi is the number of values that node i can take on, and
dashed edges were present.                                             qi is number of values that the parents of i can take on. Ac-
                                                                       cording to this definition, simplicity increases as the number
                                                                       of nodes decreases, as the number of edges decreases, and as
choice node labeled “X’s Seat Location”. In this example,              the number of possible values of each node decreases.
X’s choice might depend on where Persons A, B, and C are
already seated. Their seat locations are represented by the            Rationality
oval nodes. The edges leading from the oval nodes to X’s               Because decision nets assume that choices are taken to in-
choice node indicate that X knows the values of these vari-            crease utility, it is straightforward to capture the principle of
ables before choosing where to sit. X’s utility is represented         rationality. Namely, a decision net explanation provides more
by the diamond utility node. The edges leading to the utility          rational support for a choice if that choice results in more util-
node indicate which variables affect X’s utility. In Figure 1a,        ity for the actor. A decision net explanation will provide com-
there are edges leading to the utility node from A’s location          plete rational support for a choice if the choice results in the
and X’s choice. This structure is consistent with X wanting to         maximum possible utility for that decision net.
sit near A and not caring about how close he is to B and C. A
                                                                       Comparing explanations
fully parameterized decision net would also include a utility
function that specifies exactly how X’s utility depends on his         We will treat the problem of judging which explanations are
seat and A’s seat, as well as conditional probability tables for       better than others as a model selection problem in which the
any probabilistic variables.                                           models under consideration are fully parameterized decision
   Decision nets assume that choices are taken to increase             nets corresponding to the different explanations. Specifically,
utility. In Figure 1a, X’s utility depends on his choice and           we compute the probability that each decision net N is the
A’s location. If we suppose that X would like to sit near A,           explanation for choice c:
X’s utility will be higher for seats that are located closer to A.                           P(N|c) ∝ P(c|N) · P(N).                 (2)
   The decision net in Figure 1a assumes that X only cares
about A’s seat location when choosing where to sit. Suppose,           In order to compute the likelihood function, P(c|N), we make
however, that you don’t know what motivated X’s choice, but            use of the decision net assumption that actors are likely to
you observed where he sat and want to explain his choice of            make choices that increase their utility. We will consider two
seat. This problem is analogous to observing the value of a            ways of instantiating this assumption: by assuming that ac-
choice node in a decision net and determining the network              tors make choices to maximize expected utility, or by making
structure that best explains the choice. Accordingly, we pro-          choices probabilistically in proportion to expected utilities.
pose that behavior explanations can be represented by deci-            We define the prior probability, P(N), to be proportional to
sion nets. We assume that each potential explanation for a             the simplicity of the decision net: P(N) ∝ S(N).
behavior corresponds to a fully parameterized decision net in             Equation 2 shows how decision nets can be used to in-
which the value of the choice node has been observed. We               corporate formal definitions of simplicity and rationality into
now show how, with this assumption, decision nets can cap-             computations about which behavior explanations are better
ture the principles of simplicity and rationality and can be           than others. Earlier, we suggested that formal definitions of
used to make probabilistic judgments about which potential             simplicity and rationality allow for predictions to be made
explanations better explain a given behavior.                          about how these two principles will collectively influence
                                                                       people’s judgments about behavior explanations. We tested
Simplicity                                                             the predictions of our decision net account in an experiment
Because decision nets are networks, we may quantify how                in which people observed someone’s choice and judged ex-
simple a decision net explanation is using standard methods            planations of the choice that varied in both simplicity and ra-
of measuring network complexity. Intuitively, simpler ex-              tionality.
                                                                   1884

    One Person     Two People           Three People                   We made this assumption because we hypothesized that, in
    Near A         Near A and B         Near A and B, Far from C       our low-stakes seating story, participants would not expect
    Near B         Near A, Far from B   Near A, Far from B and C
                                                                       people to behave completely rationally. However, we con-
    Far from A     Near A, Far from C
    Far from B     Near B, Far from C                                  sidered an alternative model that did assume that people are
    Far from C     Far from A and C                                    completely rational.
                   Far from B and C
                                                                       Alternative models
  Table 1: The set of possible explanations in Experiment 1.           We compared our decision net model to several alternative
                                                                       models that were designed to test the importance of our as-
                                                                       sumptions.
                          Experiment 1
In Experiment 1, participants read about a choice someone              Utility-maximizing model The utility-maximizing model
made and then rated how satisfying several explanations for            tested whether people only consider an explanation to be sat-
the choice were. Specifically, participants read about a meet-         isfying if it provides complete rational support for a choice.
ing in which three people, A, B and C, had already arrived             This model is identical to our decision net model but as-
and selected seats. Person X arrived last and chose a seat.            sumes that people are completely rational. In other words,
Participants were told that X likes some people, dislikes some         this model follows Equation 2 but uses a likelihood function
people, and is indifferent toward some people. Accordingly,            that is equal to 1 if an observed choice results in the maximum
all of the explanations were expressed as combinations of de-          utility under a given explanation, and is equal to 0 otherwise.
sires to sit near to, or far from, certain people. Table 1 shows       Simplicity model The simplicity model tested whether the
the complete set of explanations shown to participants. For            rationality principle is necessary to account for people’s judg-
example, the second explanation in the second column of Ta-            ments. This model is identical to our decision net model but
ble 1 identified as “Near A, Far from B” was presented to              does not take into account how probable an observed choice
participants as ”X wanted to sit near A and far from B.”               was under each explanation. In other words, this model fol-
                                                                       lows Equation 2 but sets P(c|N) = 1.
Model
All of the explanations in Table 1 can be represented by               Utility-only model The simplicity model tested whether
variations of the decision net in Figure 1b. The differences           the simplicity principle is necessary to account for people’s
between the explanations can be captured by differences in             judgments. This model is identical to our decision net model
which edges lead to the utility node (depicted by dashed lines         but does not take into account how simple explanations are.
in the figure). For example, in the decision net corresponding         In other words, this model follows Equation 2 but sets P(N) =
to the “Near A, Far from B” explanation, only the “A’s loca-           1.
tion” and “B’s location” nodes would have edges leading to
                                                                       Method
the utility node.
    Additionally, differences in explanations with identical           Participants 125 Amazon Mechanical Turk users com-
network structures, such as the “Near A” and “Far from A”              pleted the experiment. 20 were omitted for failing a manipu-
explanations, can be captured by differences in their utility          lation check described below. All were compensated.
functions. We assumed that the total utility U(s) that X as-           Design and Procedure Participants were randomly as-
signed to each seat s depended on the seat’s proximity to the          signed to one of three conditions. The conditions are depicted
people X wanted to sit near to and far from. Specifically, let         in the diagrams above the plots in Figure 2. The diagrams
ui (s) be the utility X derives from seat s’s proximity to Person      show where Persons A, B, C, and X chose to sit in a row of
i. We defined ui (s) as follows:                                       seats at a meeting.
                 
                     −kd                                                  Participants saw one of these diagrams and were instructed
                 e
                               if X wants to sit near i
                                                                       to “Rate how satisfying the following explanations are for
         ui (s) = 1 − e−kd if X wants to sit far from i        (3)     why X chose to sit there.” They were then shown the 13 ex-
                 
                    0           otherwise
                 
                                                                       planations from Table 1 and rated them on a scale from 1
                                                                       (“very bad explanation”) to 7 (“very good explanation”). The
In this equation, d is the distance, in number of seats, from
                                                                       set of 13 explanations included any explanation that would
Person i’s seat, and k is a free parameter. Equation 3 has the
                                                                       provide complete rational support for X’s seat choice in any
property that there is a larger difference in utility between the
                                                                       of the three conditions, as well as several explanations, such
more desirable seats than between the less desirable seats. We
                                                                       as “Far from A” that do not provide strong rational support in
then made a standard assumption that utilities are additive.
                                                                       any condition.
That is, X’s total utility U(s) = ∑i ui (s). Finally, we assumed
                                                                          The order of the explanations was randomized for each par-
that X would choose seats in proportion to utility. That is,
                                                                       ticipant. Additionally, half of participants in each condition
                                      U(s j )                          saw a “mirror image” of the diagrams in Figure 2. For ex-
                      P(c = s j ) =            .               (4)     ample, in the condition in the center of Figure 2, X would be
                                     ∑k U(sk )
                                                                   1885

                       0.14                                                                          7   0.14                                                                          7   0.14                                                                  7
                                                          Mean Human Ratings   Model Probabilities                                          Mean Human Ratings   Model Probabilities                 Mean Human Ratings           Model Probabilities
                       0.12                                                                          6   0.12                                                                          6   0.12                                                                  6
                       0.10                                                                          5   0.10                                                                          5   0.10                                                                  5
 Model Probabilities                                                                                                                                                                                                                                             Mean Human Ratings
                       0.08                                                                          4   0.08                                                                          4   0.08                                                                  4
                       0.06                                                                          3   0.06                                                                          3   0.06                                                                  3
                       0.04                                                                          2   0.04                                                                          2   0.04                                                                  2
                       0.02                                                                          1   0.02                                                                          1   0.02                                                                  1
                       0.00                                                                          0   0.00                                                                          0   0.00                                                                  0
                                   A   Far B    B       C                  A, Far B                                  A   Far B    B       C                  A, Far B                                  A    Far B          B       C                  A, Far B
                              Near
                               Far C   Far A   Near   A, Far
                                                                       Far B
                                                                     Near
                                                                             and C                              Near
                                                                                                                 Far C   Far A   Near   A, Far
                                                                                                                                                         Far B
                                                                                                                                                       Near
                                                                                                                                                               and C                              Near
                                                                                                                                                                                                   Far C    Far A         Near   A, Far
                                                                                                                                                                                                                                                  Far B
                                                                                                                                                                                                                                                Near
                                                                                                                                                                                                                                                        and C
                                                                           B, Far C                                                                          B, Far C                                                                                 B, Far C
                                                                     Near  A and                                                                       Near  A and                                                                              Near  A and
                                                  Near    Near         Far A and C
                                                                                  B                                                 Near    Near         Far A and C
                                                                                                                                                                    B                                                        Near    Near         Far A and C
                                                                                                                                                                                                                                                             B
                                                               Near A and                                                                        Near A and                                                                               Near A and
                                                               Near        B, Far C                                                              Near        B, Far C                                                                     Near        B, Far C
                                                                    A, Far B and                                                                      A, Far B and                                                                             A, Far B and
                                                                                  C                                                                                 C                                                                                        C
Figure 2: Comparison of decision net model predictions to people’s judgments for all explanations in all conditions of Experi-
ment 1.
shown seated closest to C instead of A. For these participants,                                                                                     complete rational support for the behavior.
the set of explanations was adjusted to reflect the different                                                                                          The predictions of the simplicity and utility-only models
seating location (e.g., “Near A” would be replaced with “Near                                                                                       are shown in Figures 3c and 3d. Figure 3c shows clearly
C”). To ensure that the participants read directions carefully,                                                                                     that people did not base their judgments on simplicity alone
we included a manipulation check at the end of the experi-                                                                                          (r = 0.02). This makes sense when you consider, for exam-
ment. The manipulation check consisted of a second page                                                                                             ple, that the decision nets corresponding to the “Near A” and
that appeared identical to the first, but instructed participants                                                                                   “Far from A” explanations have identical structure, and are
to leave all answers blank. 20 participants failed the manipu-                                                                                      therefore equally simple. However, in most cases, at most
lation check and were therefore omitted from analysis.                                                                                              one of these two explanations will be reasonable. The utility-
                                                                                                                                                    only model performs better (r = 0.34), but not nearly as well
Results                                                                                                                                             as the decision net model. The poor performance of these two
Model predictions were generated by computing P(N|c) ac-                                                                                            alternative models supports our hypothesis that people rely on
cording to Equation 2 for each explanation and normalizing                                                                                          both simplicity and rationality when explaining behavior.
the results to sum to 1. When computing S(N) using Equa-                                                                                               Figure 2 compares the decision net model predictions to
tion 1, we assumed that the utility node of each decision net                                                                                       people’s judgments for all 13 explanations in each condition.
could only take on a finite number of values equal to the                                                                                           Overall, the model accounts well for the qualitative patterns
number of available seats. Parameter k in Equation 3 was                                                                                            in people’s judgments. However, there are a few predictions
fit to the data for each model (best-fitting k for decision net                                                                                     the model gets wrong. For example, in the condition where
model: 0.249; utility-maximizing model: 0.245; simplicity                                                                                           X sat next to A (left plot), people judged the “Far from A”
model: 0.249, utility-only model: 0.253). Comparisons be-                                                                                           explanation to be about as satisfying as the “Far from A and
tween model predictions and people’s judgments for all 13                                                                                           C” explanation, while the decision net model predicted that
explanations across all conditions are shown in Figure 3.                                                                                           “Far from A and C” is more probable than “Far from A”. The
    As shown in Figure 3a, our decision net model predicts                                                                                          model’s prediction in this case is a consequence of the fact
people’s judgments quite well (r = 0.84). By contrast, as                                                                                           that it treats utilities as additive. According to the model, un-
shown in Figure 3b, the utility-maximizing model performs                                                                                           der the “Far from A and C” explanation, X’s seat provides lit-
poorly (r = 0.53). The utility-maximizing model assigns a                                                                                           tle utility for being near A, but it provides considerable utility
probability of 0 to many explanations that people assigned                                                                                          for being far from C. The sum of these two utilities is higher
high ratings to. For example, in the rightmost condition in                                                                                         than the seat’s utility under the “Far from A” explanation, so
Figure 2, both participants and our decision net model judged                                                                                       the model assigns a higher probability to the former explana-
“Far from C” to be one of the top 3 most satisfying explana-                                                                                        tion. The fact that people did not do this suggests that they
tions. However, the utility-maximizing model assigned “Far                                                                                          may have considered negative utilities, or that people may not
from C” a value of 0 because X’s seat in that condition is not                                                                                      always think of utilities as additive. Perhaps if one part of an
the optimal seat for being far from C. The poor performance                                                                                         explanation is poor, people may judge the whole explanation
of the utility-maximizing model suggests that people can find                                                                                       to be poor.
behavior explanations satisfying even if they do not provide                                                                                           Although the decision net model accounted well overall for
                                                                                                                                          1886

                                 7                                                                 7
                                 6                                                                 6
                                 5                                                                 5
            Mean Human Ratings                                                Mean Human Ratings
                                 4                                                                 4
                                 3                                                                 3
                                 2                                                                 2
                                 1                                                                 1
                             0                                                                 0
                             0.00     0.05       0.10         0.15   0.20                      0.00        0.05       0.10         0.15   0.20
                                          Model Probabilities                                                  Model Probabilities
                                     (a) Decision net model                                            (b) Utility-maximizing model
                                 7                                                                 7
                                 6                                                                 6
                                 5                                                                 5
            Mean Human Ratings                                                Mean Human Ratings
                                 4                                                                 4
                                 3                                                                 3
                                 2                                                                 2
                                 1                                                                 1
                             0                                                                 0
                             0.00     0.05       0.10         0.15   0.20                      0.00        0.05       0.10         0.15   0.20
                                          Model Probabilities                                                  Model Probabilities
                                      (c) Simplicity model                                                (d) Utility-only model
                                     Figure 3: Comparison of model predictions and human judgments in Experiment 1.
people’s judgments about explanations that we provided, it is                             was indifferent toward some. However, no guidance or con-
possible that our participants may have considered additional                             straints were placed on participants’ responses. Participants
explanations not in Table 1. Therefore, we conducted a sec-                               also completed the manipulation check from Experiment 1.
ond experiment in which participants generated their own be-                              All participants passed the check, so no participants were
havior explanations.                                                                      omitted from analysis.
                                     Experiment 2                                         Results
Experiment 2 allowed us to test whether people considered
any alternative explanations in Experiment 1 that our model                               All generated explanations were coded as one of the 13 expla-
did not account for.                                                                      nations in Table 1 or as “other”. For example, the response “X
                                                                                          doesn’t like C’ was coded as “Far from C”, while the response
Method                                                                                    “X doesn’t like anyone” was coded as “other”. We (the two
                                                                                          authors) coded independently with 96% agreement (Cohen’s
Participants 45 Amazon Mechanical Turk users completed
                                                                                          κ = 0.95). We disagreed on two responses and resolved the
the experiment and were compensated for participation.
                                                                                          disagreement by coding both responses as “other” in order to
Design and Procedure The design and procedure were                                        be as uncharitable to our model as possible. As shown in Fig-
identical to Experiment 1 except that participants generated                              ure 4, the decision net model’s top six explanations in each
their own explanations rather than rate a list of provided ex-                            condition accounted for at least 70% of participants’ gener-
planations. Participants saw one of the three cases in Fig-                               ated explanations. Overall, only 6 of the 45 generated expla-
ure 2 and were asked to provide their best explanation for                                nations were coded as “other”. These results suggest that the
why person X chose the seat. Participants were told that X                                our list of explanations in Table 1 encompasses the vast ma-
liked some of the people at the meeting, disliked some, and                               jority of explanations that participants naturally considered.
                                                                            1887

                                                                                                   ing existing ideas from the literature on social cognition and
                                1.0                                               Condition 1      explanation, and present new questions for future research.
                                0.8                                               Condition 3      Acknowledgments This work was supported by the Rose-
      Proportion of responses
                                                                                  Condition 2      Hulman Independent Projects / Research Opportunities Pro-
                                0.6                                                                gram and ArcelorMittal.
                                0.4
                                                                                                                          References
                                                                                                   Anderson, C. A., Krull, D. S., & Weiner, B. (1996). Ex-
                                0.2                                                                  planations: Processes and consequences. In E. T. Higgins
                                                                                                     & A. Kruglanski (Eds.), Social psychology: Handbook of
                                0.0                                                                  basic principles. New York, NY: Guilford.
                                   0      2       4     6      8      10     12
                                       Model's top N most probable explanations                    Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action
                                                                                                     understanding as inverse planning. Cognition, 113(3), 329–
Figure 4: The proportion of generated explanations in Ex-                                            349.
periment 2 included in the decision net model’s top predicted                                      De Campos, L. M. (2006). A scoring function for learning
explanations. Conditions 1, 2, and 3 refer to the left, middle,                                      bayesian networks based on mutual information and condi-
and right conditions in Figure 2.                                                                    tional independence tests. The Journal of Machine Learn-
                                                                                                     ing Research, 7, 2149–2187.
                                                                                                   Dennett, D. C. (1987). The intentional stance. Cambridge,
                                                    Conclusion                                       MA: MIT Press.
                                                                                                   Fass, D., & Feldman, J. (2002). Categorization under com-
Our goal was to identify the computational principles that                                           plexity: A unified MDL account of human learning of reg-
make some behavior explanations more satisfying than oth-                                            ular and irregular categories. In Advances in Neural Infor-
ers. Overall, our results support the hypothesis that people                                         mation Processing Systems 15.
rely on both simplicity and rationality when judging and gen-                                      Feldman, J. (2000). Minimization of boolean complexity in
erating explanations of other people’s behavior and that both                                        human concept learning. Nature, 407(6804), 630–633.
of these principles can be formally characterized using deci-                                      Gilbert, D. T. (1998). Ordinary personology. In D. T. Gilbert,
sion nets.                                                                                           S. T. Fiske, & G. Lindzey (Eds.), The handbook of social
   Although we considered only a narrow space of possible                                            psychology (Vol. 1). New York, NY: Oxford University
explanations that differed in utility functions, decision nets                                       Press.
can be easily adapted to account for at least one other type                                       Howard, R. A., & Matheson, J. E. (2005). Influence dia-
of explanation. Recall that some edges in decision nets (such                                        grams. Decision Analysis, 2(3), 127–143.
as the edges leading to the rectangular node in Figure 1b)                                         Jern, A., & Kemp, C. (2011). Capturing mental state reason-
represent what a person knew before making a choice. Con-                                            ing with influence diagrams. In Proceedings of the 33rd
sequently, the presence or absence of these edges can be used                                        Annual Meeting of the Cognitive Science Society.
to represent explanations that differ in what someone did or                                       Keil, F. C. (2006). Explanation and understanding. Annual
did not know.                                                                                        Review of Psychology, 57, 227–254.
   The decision net account in this paper does not provide                                         Lombrozo, T. (2007). Simplicity and probability in causal
an account of the cognitive processes involved in explain-                                           explanation. Cognitive Psychology, 55(3), 232–257.
ing behavior, but it may help to motivate future research on                                       Malle, B. F. (1999). How people explain behavior: A new
cognitive processes. For example, our models assumed that                                            theoretical framework. Personality and social psychology
the potential explanations, represented as decision nets, were                                       review, 3(1), 23–48.
already constructed and available for comparison. One im-                                          Malle, B. F. (2004). How the mind explains behavior: Folk
portant question is how those explanations are constructed to                                        explanations, meaning, and social interaction. The MIT
begin with. As another example, consider the fact that our                                           Press.
definition of simplicity takes into account each decision net’s                                    Pacer, M., Williams, J., Chen, X., Lombrozo, T., & Griffiths,
structure. In our experiments, the decision nets had relatively                                      T. L. (2013). Evaluating computational models of expla-
simple structures that could likely be fully stored in working                                       nation using human judgments. In Proceedings of the 29th
memory. For more complex decision nets, however, people                                              Conference on Uncertainty in Artificial Intelligence.
may be limited by working memory capacity. Consequently,                                           Rissanen, J. (1978). Modeling by shortest data description.
people may be unable to fully compare different explanations                                         Automatica, 14(5), 465–471.
and will cease to show a simplicity preference.                                                    Ullman, T. D., Baker, C. L., Macindoe, O., Evans, O., Good-
   Overall, our results suggest that decision nets provide a                                         man, N. D., & Tenenbaum, J. B. (2009). Help or hinder:
useful formal framework for exploring how people explain                                             Bayesian models of social goal inference. In Advances in
behavior. Decision nets provide a formal language for captur-                                        Neural Information Processing Systems 22.
                                                                                                1888

