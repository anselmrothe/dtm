          Cumulative Contextual Facilitation in Word Activation and Processing:
                                      Evidence from Distributional Modelling
                                    Diego Frassinelli (diego.frassinelli@ims.uni-stuttgart.de)
                                               Institut für Maschinelle Sprachverarbeitung
                                                             Universität Stuttgart
                                             Pfaffenwaldring 5B, 70569 Stuttgart, Germany
                                                 Frank Keller (keller@inf.ed.ac.uk)
                                           Institute for Language, Cognition and Computation
                                             School of Informatics, University of Edinburgh
                                              10 Crichton Street, Edinburgh EH8 9AB, UK
                              Abstract                                     (Vigliocco, Vinson, Lewis, & Garrett, 2004), and semantic
                                                                           priming (Lapesa & Evert, 2013).
   Information provided by the linguistic context has been shown
   to have a strong facilitatory effect on the activation and pro-            In this paper we present three studies in which we model
   cessing of upcoming words. The studies described in this pa-            several aspects of contextual processing using the bag-of-
   per aim to model the relation between context and target words          words distributional semantic model developed by Mitchell
   using a distributional semantic model. We report three mod-
   elling studies in which we show that this model can success-            (2011). DSMs represent the meaning of a word as a multidi-
   fully capture context effects in human-generated data (reading          mensional vector: each dimension of the vector corresponds
   times and association scores).                                          to a word co-occurring with the target word in a corpus. Sim-
   Keywords: word processing; contextual effects; feature over-            ilar to semantic properties, the vector dimensions describe
   lap; distributional semantics.                                          specific aspects of a word that contribute to the meaning of
                                                                           that word. In this framework, two words are similar if they
                          Introduction                                     appear in similar contexts, and, consequently, if their vector
Previous work has shown that linguistic context facilitates                dimensions overlap. Traditionally, the overlap between two
the activation and processing of upcoming words (e.g., Fe-                 words has been computed in terms of the geometrical distance
dermeier & Kutas, 1999). This facilitatory effect can be de-               between the word vectors. We will show that it is possible to
scribed in terms of feature overlap: the linguistic context ac-            describe the relation between low and high biasing words and
tivates a set of semantic features, restricting the set of possi-          the target word in terms of vector similarity scores.
ble upcoming words. The words that match these features are
pre-activated and thus processed more quickly when encoun-                                       Previous Work
tered. The feature overlap account predicts that contextual fa-            In order to manipulate the effect of context on word process-
cilitation is cumulative: the more biasing words are present in            ing in a self-paced reading study, Frassinelli et al. (2013) con-
the context, the faster the target word should be processed, as            structed linguistic materials that vary the context words that
overlapping features accumulate.                                           bias the processing of a target word. Their stimuli had the
   Frassinelli, Keller, and Scheepers (2013) studied this facili-          following structure:
tation effect by manipulating the number of contextual words
that are highly related to a target word occurring at the end of           (1)     location – actor – verb – object – target - spill-over
the sentence. They performed a self-paced reading and a vi-                        area
sual world paradigm study in which they analysed linguistic
                                                                           For each of the 24 target words, they identified three context
contextual constraints on the processing of word meaning.
                                                                           words highly related to it (high-biasing words, HB) and three
   The aim of the modelling studies in this paper is to capture
                                                                           context words unrelated to it (low-biasing words, LB). Four
the effect of context reported in Frassinelli et al. (2013) us-
                                                                           possible combinations of HB and LB context words were then
ing a distributional semantic model (DSM, Turney & Pantel,
                                                                           used to construct the sentential context, as illustrated by the
2010). The “strong version” of the Distributional Hypothesis
                                                                           following examples:
posits the cognitive validity of those models: DSMs provide
an insight into the internal representation and structure of the           (2)     Zero HB words (None): On the path, the man was
lexicon in the brain (Lenci, 2008). In recent decades, DSMs                        holding a box full of mushrooms carefully.
have become very popular in psycholinguistics where they
are used to successfully model different aspects of human lan-             (3)     One HB word (One): In the forest, the man was hold-
guage acquisition and processing such as: vocabulary acqui-                        ing a box full of mushrooms carefully.
sition (Landauer & Dumais, 1997), category-related deficits                (4)     Two HB words (Two): In the forest, the picker was
                                                                     734

                                                                                    fined as follows:
                          500
                                                    *                                                                   freqct freqtotal
                                             *                                                  posPmi = max(0, log(                     ))      (1)
                                                                                                                          freqc freqt
                          400     ●
                                                                                    where freqct is the frequency of the target t in the context
                                             ●
                                                           ●           ●
                                                                                    c; freqt is the overall frequency of t; freqc is the overall fre-
      Reading Time (ms)
                          300
                                                                                    quency of c; and freqtotal is the total frequency of all the
                                                                                    words. The substitution of negative values with zeros in the
                          200                                                       posPmi model makes this association measure more suitable
                                                                                    for dealing with sparsity and low frequency words, as they
                                                                                    occur in our dataset (Mitchell, 2011, p. 44).
                          100                                                          Moreover, we experimented with changing vector dimen-
                                                                                    sionality of the model (from 1,000 up to 50,000 dimensions).
                                                                                    We found that model performance stabilizes at 30,000 dimen-
                           0
                                                                                    sions and shows no further improvement at higher dimen-
                                 Zero       One           Two        Three          sions. In this work, we therefore report only the results ob-
                                        Number of HB Context Words
                                                                                    tained with vectors of 30,000 dimensions.
Figure 1: Plot of the reading times (with SE) averaged by the                               Study 1: Predicting Reading Times
number of HB context words (from Frassinelli et al., 2013).                         Aim In this study we test the bag-of-words DSM of
                                                                                    Mitchell (2011) by using it to predict the reading times col-
                                                                                    lected by Frassinelli et al. (2013). The authors showed that
                          holding a box full of mushrooms carefully.                the amount of time required to read the target word decreases
(5)                       Three HB words (Three): In the forest, the picker         (though not linearly) with an increase of the number of bi-
                          was holding a basket full of mushrooms carefully.         asing words in the context. In modelling terms, this means
                                                                                    that the averaged distance between the context vectors and the
The plausibility of the resulting sentences and the biasing ef-                     vector of the target word should decrease when we increase
fect exerted by the contexts were carefully tested in a series                      the number of high biasing words in the context. Conversely,
of norming studies.                                                                 when we increase the number of low biasing words in the
   Frassinelli et al. (2013) then investigated the effect of HB                     context, the distance should increase.
context words on the reading time (RT) at the target word.
Figure 1 reports RTs averaged by condition with standard er-                        Method In the self-paced reading experiment, the authors
rors (SE). Significant differences occur only between the con-                      analysed the effect of context on word meaning averaging
ditions None and Two (βTwo = −.10, p < .05) and the con-                            the RTs based on the number of HB words available (from
ditions None and Three (βThree = −.09, p < .05). Overall,                           zero up to three) (see Previous Work and Figure 1). In this
Frassinelli et al. (2013) results show that the time required                       study, we compute the distance between each context vector
to read a target word decreases with increasing amount of bi-                       and the vector of the target word (e.g., for condition Three:
asing context.                                                                      forest-mushroom, picker-mushroom, basket-mushroom), we
                                                                                    average the three resulting cosines and we average again by
                                         The Model                                  condition as for the RTs.
For the studies reported in the next sections, we use a re-                         Results Figure 2 presents the average cosine distance per
implementation (by Blacoe & Lapata, 2012) of the bag-of-                            condition. It shows that an increasing amount of biasing con-
words distributional model developed by Mitchell (2011).                            text produces a reduction in the distance between the context
   We trained this model on the lemmatised and part-of-                             and the target word vectors. Table 1 reports the coefficients
speech tagged version of ukWaC, an English corpus of two                            of a linear mixed effects (LME, version 1.1-7) model anal-
billion tokens extracted from the Web (Baroni, Bernardini,                          ysis of these data (Baayen, Davidson, & Bates, 2008). The
Ferraresi, & Zanchetta, 2009). The use of this corpus pro-                          model has the cosine distance as the dependent variable, the
vided full coverage of the experimental items.                                      contextual condition as main factor (contrast coded with the
   Mitchell presented an evaluation of four association mea-                        Zero condition as the reference level) and random slope and
sures to weight vector components: conditional probabilities,                       intercept under Item. The model shows a significant differ-
point-wise mutual information, ratios of probabilities, and                         ence between condition Zero and all the other conditions. We
positive point-wise mutual information. We compared these                           also performed a Tukey post-hoc test to compare the condi-
four measures on our data and overall positive point-wise mu-                       tions pairwise. The analysis shows a significant difference be-
tual information (posPmi) obtained the best results. It is de-                      tween all the conditions (p < .001).
                                                                              735

                                                   ***                                                                                ***
                                           ***                                                                                  ***
                                      **                                                                   1.00    ●             ●
                                                                                                                                             ●
                        1.00    ●             ●
                                                                                                                                                          ●
                                                          ●
                                                                       ●
                                                                                                           0.75
      Cosine Distance                                                                    Cosine Distance
                        0.75
                                                                                                           0.50
                        0.50
                                                                                                           0.25
                        0.25
                        0.00                                                                               0.00
                               Zero          One         Two         Three                                        Zero          One         Two         Three
                                       Number of HB Context Words                                                         Number of HB Context Words
Figure 2: Study 1: Plot of the cosine distances (with stan-                        Figure 3: Study 2: Plot of the cosine distances (with standard
dard errors) between context vector and target vector, aver-                       errors) between pairs of context words, averaged across all
aged across all items with the same number of HB words.                            items with the same number of HB words.
                               Predictor               β                                                          Predictor               β
                               (Intercept)          0.90∗∗∗                                                       (Intercept)          0.95∗∗∗
                               One                 −0.03∗∗                                                        One                 −0.01
                               Two                 −0.08∗∗∗                                                       Two                 −0.06∗∗∗
                               Three               −0.11∗∗∗                                                       Three               −0.09∗∗∗
                               ∗ p < .05, ∗∗ p < .01, ∗∗∗ p < .001                                                ∗ p < .05, ∗∗ p < .01, ∗∗∗ p < .001
  Table 1: Study 1: LME coefficients for data in Figure 2.                           Table 3: Study 2: LME coefficients for data in Figure 3.
                                                                                       Study 2: Predicting the Relation between
Discussion Figure 1 and Figure 2 allow a graphical com-                                            Context Words
parison of the trends in the reading time experiment and
                                                                                   Aim The aim of this study is to model the interaction be-
in the cosine similarity study. The modelling study shows
                                                                                   tween context words without taking into account the effect of
higher differences between conditions than those in the read-
                                                                                   the target word. We also compare the outcome of this study
ing times produced by humans. Similar results have been de-
                                                                                   with Study 1 in order to understand the relationship occurring
scribed in the semantic priming literature, where it was found
                                                                                   between context words in isolation and between those words
that Latent Semantic Analysis (LSA) predicts stronger ef-
                                                                                   and the target.
fects than those observed in humans (Hare, Jordan, Thomson,
Kelly, & McRae, 2009; Jones, Kintsch, & Mewhort, 2006).
Overall, however, the modelling study captures the RT results                      Method We computed the cosine distance between each
well: cosine distance (as the reading time) decreases with in-                     pair of context words (e.g., for condition Three: forest-picker,
creasing contextual bias.                                                          picker-basket, forest-basket) and we averaged them. The re-
                                                                                   sulting cosines were averaged again by condition.
   So far we considered only the semantic relation between
target and context words. We did not include in our analysis                       Results Figure 3 shows the average cosine distance per
the relation between context words, but HB context words are                       condition. Similarly to the previous results, the increasing
likely to also be related to each other. The relation between                      amount of biasing context produces a reduction in the dis-
context words alone (without a need for the target) could pro-                     tance between the context vectors. Table 3 reports the LME
duce a similar effect on the cosine distance as we see in this                     coefficients for these data. The model structure is the same
study. In Study 2 we test this assumption.                                         as in the previous study. The difference between condition
                                                                             736

                           Predictor                    β Model 1 β Model 2 β Model 3 β Model 4
                           (Intercept)                    2.7∗∗∗        2.5∗∗∗        2.6∗∗∗        2.7∗∗∗
                           AvgTarget                  −15.5  ∗∗∗                   −10.7 ∗∗∗      −8.5∗∗∗
                           AvgContext                               −13.4∗∗∗        −7.7∗∗∗      −10.1∗∗∗
                           AvgTarget:AvgContext                                                  −81.6∗∗∗
                                                   ∗ p < .05, ∗∗ p < .01, ∗∗∗ p < .001
                                    Table 2: LME coefficients for the model comparison study.
Zero and One is not significant, while all the other differ-              The basic assumption behind this study is that HB context
ences are strongly significant (p < .001). A post-hoc anal-            words are not only highly related to the target word but also
ysis shows significant differences between condition Two and           strongly associated to each other. In order to test this hypoth-
Three (p = .007) and all the remaining conditions (p < .001).          esis experimentally, in Experiment 1 we collected association
   A rank correlation analysis between the cosines for each            scores between context words. The outcome of this experi-
item described in Study 1 and Study 2 shows an association             ment should shed more light on the relation between contex-
of medium strength (ρ = .65, p < .05). In order to test the            tual words. Moreover, in Study 3 we model these scores using
interaction of the information captured by the two models we           the distributional model.
performed a series of LME analyses treating the contextual
condition as the dependent variable, the average cosines from             Experiment 1: Association between Context
Study 1 (AvgTarget) and Study 2 (AvgContext) as continu-                                           Words
ous predictors, random slopes and intercepts under Item. To            Aim The aim of the study is to test our assumption that
reduce collinearity in our analyses, we centered the predic-           highly biasing context words are not only related to the target
tors and we assessed the collinearity between them estimat-            word, but also to each other, thus explaining why a context-
ing the conditional number κ. As suggested in Baayen (2008),           only model (Study 2) was able to predict the reading time data
κ = 3.3 indicates the absence of collinearity between the pre-         from Frassinelli et al. (2013).
dictors.
   Four different LME analyses including different combina-
tions of these predictors were performed. Table 2 reports the          Method This experiment was performed on Amazon Me-
coefficients for each model (1–4) and shows significant ef-            chanical Turk. Subjects were required to rate how related two
fects for all the predictors. In Model 1 we included only Avg-         words are on a scale from 1 (not at all related) to 5 (very re-
Target as predictor, while in Model 2 we included only Avg-            lated). Subjects were all native speakers of English with an
Context. In Model 3 we included both the predictors but not            US account. They were paid $ 0.20 to produce 24 association
their interaction. Model 4 is the most complex one and in-             scores and were allowed to complete only one hit from the
cludes the interaction between AvgTarget and AvgContext.               same batch.
   A model selection procedure that compares the log likeli-              144 participants took part to the experiment producing a
hoods of the different models was performed in order to de-            total of 3,456 association scores. Each item was evaluated by
termine the model that provides the best fit to the data. Both         twelve subjects. Participants were asked to judge the relation
the comparison of Model 1 and Model 3 and the comparison               between each combination of LB and HB context words.
of Model 2 and Model 3 show that the inclusion of both the
predictors (as in Model 3) significantly enhances the accuracy         Results On average, two high biasing words have the high-
of the model (p < .001). On the other hand, the comparison             est association score: 3.77 (SE ±.05) out of 5. On the other
of Model 3 and Model 4 does not show any significant differ-           hand, two low biasing words obtain the lowest score: 2.81 (SE
ence (p=.08) and suggests that the inclusion of the interaction        ±.04). The associations between a high biasing and a low bi-
between the two predictors does not improve the fit signifi-           asing word (HB − LB and LB − HB) are 2.93 (SE ±.05)
cantly.                                                                and 2.96 (SE ±.05) respectively. A LME analysis was per-
                                                                       formed. The only significant difference is between LB−LB
Discussion Similarly to what we described in the previous              and HB−HB words (βHighBias = 0.94, p < 0.001). We per-
modelling study, we show a strong relationship between HB              formed a post-hoc analysis to compare pairwise the differ-
context words that produces a significant reduction in the             ent conditions. The HB−HB condition obtained significantly
average cosine distance when increasing the amount of HB               higher association scores than the other conditions (p < .001).
information provided. The model selection procedure indi-
cates that the inclusion of both contextual relations and target-      Discussion The aim of this study was to analyse the seman-
context relations is essential to improve the fit of the model to      tic relationship between pairs of context words. In this exper-
the data.                                                              iment we directly evaluated this relation without the influ-
                                                                   737

ence of the target word. The HB−HB condition obtained the                                  General Discussion
highest scores, while the LB−LB condition the lowest ones.            The studies reported in this paper aimed to test if a bag-of-
The mixed situations (HB−LB and LB−HB) are negatively                 words DSM can capture the relation between context and tar-
biased by the presence of the LB property in the pair. These          get words and, consequently, describe context effects on word
results highlights the fact that the association of a HB word         processing in terms of feature overlap. We conducted three
with a LB word is similar to the association of two LB words          studies where we analysed different aspects of the target–
even though HB words have more specific meanings than LB              context relation.
words.                                                                    In Study 1 we showed that the DSM can successfully pre-
   Overall, this study shows that the words that are highly re-       dict the RTs from Frassinelli et al. (2013). We averaged the
lated to the target word are also highly related with each other.     cosine distance between each context vector and the target
LB words, in contrast, being more general words, are less             vector and we averaged again the resulting values by condi-
strongly associated with each other and also with HB words.           tion. When increasing the amount of HB context words the
                                                                      cosine distance significantly decreases.
      Study 3: Predicting Association between
                                                                          In Study 2 we modelled the relation between context words
                      Context Words                                   without the influence of the target. The outcomes of this
Aim Here, we use the model from Study 1 and Study 2 for               model show that the inclusion of more HB information re-
the task of predicting human generated association scores be-         duces the cosine distance in the different conditions. In order
tween two contextual words (rather than RTs as in Study 2).           to test the validity of these results we collected association
From Experiment 1 it emerged that context words that are              scores between pairs of context words. As shown in Study 3
highly related to the target are also highly related to each          the model can successfully predict these scores indicating that
other. We therefore expect the semantic similarity between            words that are highly related to the target word are also highly
two high biasing context words to be higher than the similar-         related to each other. The model comparison study highlights
ity between two low biasing context words.                            the importance of including both these relations in order to
                                                                      have the best fit of our data.
Task In Experiment 1, participants had to evaluate on a                   Overall, this study provides support for feature overlap the-
scale from 1 (not related) to 5 (completely related) the as-          ory by showing that contextual facilitation is cumulative, i.e.,
sociation between two contextual words (both high and low             it increases with the number of highly biasing context words.
biasing words). The model has to correctly predict the 3,456          This theory however does not account for the context-only
human-generated association scores.                                   effect that we found in Study 2 and Experiment 1.
                                                                          We demonstrated that the accumulation of semantic fea-
                                                                      tures can be modeled as the combination of the distribu-
Method We computed the cosine similarity between pairs                tional vectors of the context words. Distributional semantics
of context words.                                                     therefore provides a computational implementation of feature
                                                                      overlap theory, with semantic features represented as vectors
Results A LME analysis was performed. The association                 components (i.e., word co-occurrences).
scores are the dependent variable, the cosine similarities
(posPmi, dim=30,000) the continuous factor, subject and item                               Acknowledgments
the random slopes and intercepts. The model shows a signifi-          We are grateful to William Blacoe for making available the
cant positive relation between word similarity and association        distributional model used in this paper. The support of the Eu-
scores (βCosine = 6.431, p < 0.001).                                  ropean Research Council under award number 203427 “Syn-
                                                                      chronous Linguistic and Visual Processing” is gratefully ac-
Discussion This study demonstrated a positive relationship            knowledged.
between the human association scores and model similar-
ity scores for context words. This indicates that the model                                     References
successfully captures the relation between context words we           Baayen, R. H. (2008). Analyzing linguistic data: A practical
found in the association study (Experiment 1). Words that are            introduction to statistics using R. New York: Cambridge
highly related to the target word are also strongly associated           University Press.
with each other because they occur in similar contexts.               Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008).
   Taken together, the results from this study and Experi-               Mixed-effects modeling with crossed random effects for
ment 1 confirm the assumptions underlying in Study 2. They               subjects and items. Journal of Memory and Language,
provide evidence that the facilitation effect found in the RT            59(4), 390–412.
data of Frassinelli et al. (2013) cannot be attributed solely to      Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E.
the relationship between the context words and the target; the           (2009). The WaCky Wide Web: A collection of very large
relationship of the context words with each other also plays a           linguistically processed Web-crawled corpora. Language
role.                                                                    Resources and Evaluation, 43(3), 209–231.
                                                                  738

Blacoe, W., & Lapata, M. (2012). A Comparison of Vector-
  based Representations for Semantic Composition. In Pro-
  ceedings of the 2012 joint conference on empirical methods
  in natural language processing and computational natural
  language learning. (pp. 546–556).
Federmeier, K. D., & Kutas, M. (1999). A Rose by Any
  Other Name: Long-Term Memory Structure and Sentence
  Processing. Journal of Memory and Language, 41(4), 469–
  495.
Frassinelli, D., Keller, F., & Scheepers, C. (2013). The Ef-
  fect of Incremental Context on Conceptual Processing : Ev-
  idence from Visual World and Reading Experiments. In
  M. Knauff, M. Pauen, N. Sebanz, & I. Wachsmuth (Eds.),
  Proceedings of the 35th annual conference of the cognitive
  science society (pp. 460–466). Berlin.
Hare, M., Jordan, M. I., Thomson, C., Kelly, S., & McRae,
  K. (2009). Activating event knowledge. Cognition, 111(2),
  151–167.
Jones, M. N., Kintsch, W., & Mewhort, D. J. (2006). High-
  dimensional semantic space accounts of priming. Journal
  of Memory and Language, 55(4), 534–552.
Landauer, T., & Dumais, S. T. (1997). A solution to Plato’s
  problem: The latent semantic analysis theory of acquisi-
  tion, induction, and representation of knowledge. Psycho-
  logical review, 104(2), 211–240.
Lapesa, G., & Evert, S. (2013). Evaluating Neighbor Rank
  and Distance Measures as Predictors of Semantic Priming.
  In Proceedings of the workshop on cognitive modeling and
  computational linguistics (pp. 66–74).
Lenci, A. (2008). Distributional semantics in linguistic and
  cognitive research. Rivista di Linguistica, 20(1), 1–31.
Mitchell, J. (2011). Composition in distributional models of
  semantics. Unpublished doctoral dissertation, University
  of Edinburgh.
Turney, P. D., & Pantel, P. (2010). From Frequency to Mean-
  ing : Vector Space Models of Semantics. Journal of Artifi-
  cial Intelligence Research, 37, 141–188.
Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett, M. F.
  (2004). Representing the meanings of object and action
  words: The featural and unitary semantic space hypothesis.
  Cognitive Psychology, 48(4), 422–488.
                                                             739

