  Upsetting the contingency table: Causal induction over sequences of point events
                                            Michael D. Pacer (mpacer@berkeley.edu)
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                                             Department of Psychology, 3210 Tolman Hall
                                                         Berkeley, CA 94720 USA
                            Abstract                                      But there is another way to model sequences of point
                                                                       events occurring in continuous-time. Here, we build a model
   Data continuously stream into our minds, guiding our learn-
   ing and inference with no trial delimiters to parse our experi-     of causal induction on the framework described in Pacer and
   ence. These data can take on a variety of forms, but research       Griffiths (2012), which uses a model of events embedded in
   on causal learning has emphasized discrete contingency data         continuous-time. It computes probabilities not in terms of the
   over continuous sequences of events. We present a formal
   framework for modeling causal inferences about sequences            frequency with which events co-occur, but directly from the
   of point events, based on Bayesian inference over nonhomo-          temporal distances between cause and effect events. Impor-
   geneous Poisson processes (NHPPs). We show how to apply             tantly, we accomplish this without needing to uniquely match
   this framework to successfully model data from an experiment
   by Lagnado and Speekenbrink (2010) which examined human             individual cause events with individual effect events. We rely
   learning from sequences of point events.                            on probabilistic graphical models to describe the structure of
   Keywords: causal inference; continuous time; stochastic pro-        causal relations. We use Poisson processes and operations
   cesses; Bayesian models                                             over these processes to describe the functional relationships
                                                                       between variables, which provide the structure by which we
                         Introduction                                  identify and organize kinds of events.
Only a single bullet is needed to end a life. One momentary               Our focus in this paper is on causal induction from se-
event quickly ruptures a slew of causal mechanisms, having             quential point processes — sequences of events that are tran-
effects that persist long after the trigger was pulled. We un-         sient moments in continuous time. This is an important case
derstand this causal inference easily enough, but how do we            for modeling causal induction, describing a wide range of
manage to arrive at that inference? More precisely, what for-          real-world settings in which people perform causal induc-
mal tools can characterize events that last an instant but leave       tion such as trying to infer which particular interaction with
profound consequences in their wake?                                   which particular other person led to coming down with the
   Learning from instantaneous events is one of the most im-           flu. Bramley, Gerstenberg, and Lagnado (2014) raise a con-
portant forms of causal induction in the real world, but is not        cern that these cases present a challenge for the continuous-
the case considered in most models of human causal reason-             time causal network approach presented by Pacer and Grif-
ing. More commonly, these models rely on the existence of              fiths (2012).
aggregated statistics often in the form of a contingency table            The paper will proceed as follows. We review Poisson
describing the frequencies with which different combinations           processes, which form the foundation for the rest of the pa-
of events occur (Griffiths & Tenenbaum, 2009). The most                per. We introduce a physical analogue of these formal struc-
traditional case is the 2 × 2 contingency table that describes         tures to provide intuition to the underlying mathematics of
two variables that are either present or not. It is out of this        nonhomogeneous Poisson processes. These processes form
statistical paradigm that many of the models of human causal           the basis of our framework when combined with probabilis-
inference and learning have arisen (Gopnik et al., 2004).              tic graphical causal models. We then successfully apply the
   The world offers more than contingency tables and fre-              framework to Experiment 2 from Lagnado and Speekenbrink
quency counts. Different kinds of data warrant different               (2010) — a paradigm for human judgments of causal struc-
kinds of inferences. For example, when Greville and Buehner            ture from sequences of point events. We then discuss the im-
(2010) coerce events embedded in continuous-time into con-             plications of this work for statistical and mechanistic theories
tingency tables, they are challenged with identifying an (as-          of human causal reasoning.
sumed) underlying trial structure. To fill a 2 × 2 table of event
pairs, one needs to slice the continuum into trials; a variable                           Formal Foundations
is counted if an event occurred in the slice and not if it did         In this section we describe formal foundations for our frame-
not. But different slicing regimens can warrant different in-          work (cf., Pacer & Griffiths, 2012). We explain a class of
ferences about the same data. This arises because of a funda-          point-process models (processes that describe the occurrence
mental asymmetry between the times at which events occur               of point-events which have a location, but no measurable du-
and when then do not occur. In a contingency table both X              ration) called Poisson processes that are defined in terms of
and ¬X are treated as the same kind of entity, but instanta-           a space and a positive rate function that gives the expected
neous events can be counted while the the continuous expanse           number of events to be found in any subspace. This rate-
of nothingness in which events are embedded by definition              function will depend upon the identity and time of events
cannot be counted, only measured.                                      that occur during the course of the processes’ activity (cf.,
                                                                   1805

Simma & Jordan, 2010; Blundell, Beck, & Heller, 2012). To                   Combining Perspectives The arrival perspective gives us a
accommodate this kind of conditional intensity function we                  probability distribution over intervals of time (i.e., intervals
use nonhomogeneous Poisson processes (NHPPs), whose rate                    defined from now until the next event arrives), while the rate-
functions are not constant.                                                 of-events perspective gives us an instantaneous measure of
   We will first describe two ways of looking at homogeneous                event likelihood which is comprehensible only in terms its
Poisson processes: arrivals and rates. After developing an                  integration over intervals of time. The former is more use-
intuition for Poisson processes with constant rate functions,               ful in cases where events are analyzed one at a time. For
we will then consider the general case of NHPPs and how                     example, when simulating dependent event sequences or cal-
to model sequences of point events in terms of a generative                 culating the probabilities of event sequences in terms of the
model that iteratively analyzes individual arrivals relative to             likelihood of each event’s occurrence given the previous rele-
the rates induced by previous arrivals.                                     vant occurrences. In our model of Lagnado and Speekenbrink
                                                                            (2010), we will use this perspective to define the likelihood of
Poisson Processes: Arrivals and Rates                                       each inter-arrival period conditional on the previous events.
                                                                               The rate-of-events perspective is useful when simulating
Poisson processes can be interpreted in a number of ways that               many events when the rate is independent of the particular oc-
lend themselves more or less easily to different applications.              currence of the events. The rate perspective is also useful for
We will describe Poisson processes in two senses: the ar-                   calculating event likelihoods, when the interval during when
rival process sense and the rate-of-events sense.1 Sequences                the events occurred is known, but the exact occurrence times
of point events can be described sequences of successive ar-                are unknown. Pacer and Griffiths (2012) use this technique to
rivals, making the arrival perspective convenient for comput-               analyze the data given to participants in Greville and Buehner
ing point event likelihoods. But it is easiest to conceive of the           (2007) in which data were presented in a tabular form that de-
causal effects of point events in terms of their altering event             scribed the day during which bacteria died but not the exact
rates. Both perspectives prove useful.                                      timing of the events. This property allows us to recover a
Arrivals The arrival sense can be understood by anyone                      trial structure from continuous-time by integrating over inter-
who has ever waited in a queue. You will have to wait some                  vals of time and treating occurrences within those intervals as
amount of time before your turn, and we can assign a prob-                  events that occurred in those trials.
ability that you will be served by time t. If you were next                    Most importantly for our uses, it is most straightforward
in the queue and it were governed by a homogeneous Pois-                    to see causes as altering the rate-of-events and then comput-
son Process with rate λ, the waiting time distribution of being             ing an expected wait-time distribution based on those altered
served by time t would be an exponential distribution with                  event rates. Describing effects in terms of rate changes will
mean λ1 (t ∼ Exp( λ1 ) ∶ p(t) = λe−λt ). Interestingly, this distri-        be the key to the causal aspect of our framework. Fortunately,
bution is memoryless, such that, regardless of how long we                  Poisson processes have two closure properties, superposition
have waited, we still expect to wait the exact same amount of               and thinning, that allow us to create continuous time analogs
time — it has no memory of how long it has been since the                   of noisy-OR and noisy-ANDNOT (for more details, see Pacer
last event. This memorylessness property does not hold for                  & Griffiths, 2012).
the general class of NHPPs.
                                                                            Superposition and Thinning in Poisson Processes
Rates Equivalently, we can count the number of events that                  This section will develop a rough physical model to aid in
occur in a measurable time-period, rather than looking at the               thinking about NHPPs as formed by functions on homoge-
delays between each event. Poisson processes define a “rate”                neous Poisson processes. Namely, we aim to provide an intu-
of events, which describes the expected count of events to                  ition for the superposition and thinning closure-properties of
occur in any interval. A homogeneous Poisson process has                    homogeneous Poisson processes from the rate-of-events per-
a constant rate, λ, and for a time interval with length ∣τ∣ we              spective. We do so by sketching a mechanistic picture of a
can expect to see event-count distribution governed by a Pois-              particle emission system that exhibits these properties.
son random variable, with mean (λ∣τ∣). In the case of nonho-                   First, consider a decaying radioactive material which re-
mogeneous Poisson processes, we will have a rate-function                   leases particles at a constant rate, λ. With a particle detector
defined over time λ(t). Integrating this function over some                 around the material you record the time-stamp at which parti-
time-interval defines how many events are expected to occur                 cles hit the detector. A particle is expected to hit the detector,
(i.e., for τ[a,b) the expected event count is ∫τ          λ(s)ds).          on average, every λ1 s. This detector will then be recording a
                                                    [a,b)
                                                                            homogeneous Poisson process with rate λ.
    1
      Poisson processes can be defined over higher dimensional                 Suppose you were to place a barrier to block some of the
spaces (e.g., R3 ) than the real line. This complicates the arrival per-    paths leading from materials to the detector (call the propor-
spective, which implicitly relies on the order that events “arrive”.        tion blocked π ∶ 0 ≤ π ≤ 1, as in the parameter associated with
The event-rate perspective is unchanged in higher dimensions; in            the orange filter in Figure 1). From the detector’s perspec-
that sense it could be said to be more “fundamental” than the ar-
rival perspective. In this paper, for simplicity we focus on processes      tive, events associated with particles blocked by a filter are
defined over time ([0, ∞)).                                                 events that never occurred. This process is known as filtering
                                                                        1806

        particle paths                                                 2012), then we have successfully managed to create a system
                    in transit
                    detected                                           of causal relations in terms of Poisson processes.
                    filtered
                                                                          It is worth noting that if we want to describe the causal
                                                                       effects of point events at all, we will need some form of in-
                                                 filter (π)
                                                                       fluence function that lasts beyond the occurrence of the point
                                                                       event. Because point events are instantaneous, if a cause’s ef-
                                                                       fects last while cause persists the effects would have to occur
                                radioactive                            at that same instant. But we are seeking to model causal re-
                                 materials                             lations that are not only embedded in a moment in time, but
                                                                       across time. We need to consider how events that occur at one
                                                                       point and time can influence events – or, more precisely, the
                                                                       underlying rate of events – at later time points and intervals.
                                                                          We need a way to describe what kinds of things exist
                               particle detector
                                                                       which things are related to one another and in what ways
Figure 1: Particle emission detector model for visualizing su-         they are related — i.e., we need an ontology, plausible re-
perpositioning and filtering Poisson processes. Color distin-          lations, and functional forms (Griffiths & Tenenbaum, 2009).
guishes particle origins prior to detection, with color lost in        Probabilistic graphical models are formal structures for ex-
detection. Filtered events are never detected.                         pressing stochastic dependencies between variables. But are
                                                                       our events valid variables? To define a set of possible graphs
the Poisson process, and if π is independent of the generating
                                                                       over variables, we usually need to know what those variables
process, filtering gives a Poisson process with rate (1 − π)λ.
                                                                       are. Given that we do not know when events will occur events
   Suppose you were to place another radioactive material in
                                                                       would make events a bad candidate for graphical nodes. Fur-
the detector, of a different kind than the original, but which
                                                                       thermore, there’s no obvious way to say whether event at
did not interact with the original radioactive material (see the
                                                                       time t0 counts as the “same” event (or even kind of event)
blue and green materials in Figure 1). From the perspective
                                                                       as that which happens at t1 . Without some other information
of the detector, there is no difference between the particles
                                                                       we would be left with a proliferation of variables equal to
hitting it from different materials — it only knows that a par-
                                                                       the number of events we observe. Instead, we will consider
ticle hits it and when it hits it. If we suppose the rate of this
                                                                       the case where events have signature identities that identify
new material’s emitting particles is constant at λ1 , then the to-
                                                                       which sequence of events each event belongs to. These se-
tal set of events would be a Poisson process with rate λ + λ1 .
                                                                       quences will be our variables. Both events and variables fea-
This is the superposition property of Poisson processes: if
                                                                       ture in our ontology, but the graphs will be defined over po-
jointly independent, the union of the events from two Poisson
                                                                       tential relations between variables which will be associated
processes will be a Poisson process whose rate is their sum.
                                                                       with sequences of point events. Events will be the compo-
   Superposition and thinning allow us to see how the rates
                                                                       nents through which variables actually affect one another.
of Poisson processes can change without altering their under-
lying structure. We can apply these transformations at par-               As in Griffiths and Tenenbaum (2009), to compute pos-
ticular times or intervals of time, thereby producing and in-          teriors over graphs, we will need a prior over the possible
crease (via superposition) or a decrease (via thinning) in the         relations between variables. Often this will be made easier
rate of events while at all times maintaining its identity as a        by knowing (from our ontology) which things are potential
Poisson process. Applying superposition or thinning as time-           causes of which effects. When all possible binary relations
dependent functions thus allows one way to create NHPPs that           could be graphs it is a challenge not because it is difficult to
nonetheless can be understood in terms of component pro-               impose a prior per se, but because of how rapidly the number
cesses and their transformations.                                      of graphs grows in terms of the number of variables. This
                                                                       is especially pernicious because by using continuous-time
Causal Graphs and Sequential Point Processes                           causal networks, we can represent directed cycles as easily as
With superposition and thinning in our tool belts, we can dis-         any other relationship, which allows even more graphs than
cuss causal point processes feeding back into themselves. By           in the traditional causal Bayes nets framework.
applying thinning and superposition dependent on time we                  For each of these relationships, we will express a func-
can build NHPPs from component processes. If we apply su-              tional form defining how variables relate to one another. In
perposition and thinning functions relative to the occurrence          particular, we need to relate the kind of relationship between
of events at particular times (e.g., by scaling their effects rel-     variables (e.g., generative, preventative) to the how the in-
ative to the distance from the events’ occurrence time), then          fluence of particular events associated with those variables
we can define NHPPs relative to the occurrence of particu-             changes over time. Our approach is to see an event(t ′ ) in the
lar events. If we then define the set of events capable of             cause-variable’s event set as inducing a NHPP on the vari-
evoking changes in the Poisson process as themselves being             able’s child nodes. For generative causes, the NHPP starts
the outcome of Poisson processes (see also Blundell et al.,            with a maximum rate(ψ) immediately when it is triggered
                                                                   1807

with the rate decaying exponentially according to a decay            However, this could either be because there is something spe-
rate(ϕ) scaled distance from the cause event ((t − t ′ )). This      cific about long delays between causes and effects, or that
                                                       ′
produces a NHPP with rate function λ(t) = ψe−ϕ(t−t ) .               longer delays allow more opportunities during which other
   We compute likelihoods for events sequences as follows.           events could occur that are not causally related, thus weak-
We compute a likelihood for an event using the rate function         ening the connection between the original two variables of
of its associated variable during its wait-time interval. We         interest.2 According to the design of the experiment – un-
then update the rate functions for variables that are effects of     known to the participant – only one of the types of wave(A)
the variable associated with the event, and iterate this process     was a cause of earthquakes, but sometimes non-causal waves
until we exhaust the event sequence. The waiting-time likeli-        would occur in the interval of time between the cause and its
hood for each event can be further broken down into waiting-         effects. This allows us to disentangle the two explanations for
time likelihoods for the Poisson process composing its vari-         reduced causal strength due to longer delays. The length of
able’s rate function at the instant the event occurs. Note that      time between the cause and its effect and the commonness of
from the superposition perspective an event’s waiting-time           mid-interval events’ sometimes were the primary differences
implies that none of the variable’s processes produced events        between the experimental conditions.
until the point when the event in question occurs.                      The experiment had a 2 × 2 structure. D ELAY-L ENGTH
   The general form of the cdf (cumulative density function)         could be L ONG (mean delay between cause and effect = 6s) or
of the waiting time distribution until the first event of a NHPP     S HORT (mean delay between cause and effect = 3s) — in both
with time-dependent rate function λ(⋅), is:                          the standard-deviation is 0.1s. The probability a non-cause
                                                                     event occurred between the cause and the effect was L OW
                                            t                        (≈ 35% of the time a non-cause event would occur between
           F(T ≤ t) = F(t) = 1 − exp( − ∫ λ(s)ds).
                                          0                          a cause and its effect) or H IGH (≈ 65%). These probabili-
                                                                     ties are approximate because the event sequences were ran-
This can be converted to a pdf by taking the derivative accord-      domly sampled and so cannot be expected to exactly match
ing to t, which (assuming that the derivative exists) becomes,       expected percentages. The authors chose delay distributions
                                       t                             between the occurrence of cause and lure events to produce
                 p(t) = λ(t)exp( − ∫ λ(s)ds).                        these probabilities in aggregate across samples.
                                     0
                                                                        They generated sixty datasets per condition that repre-
        Modeling Continuous Event Streams:                           sented the time-stamps and identities of events that occurred
                                                                     in the movie. Of these, the first twenty datasets were used,
           Lagnado & Speekenbrink, 2010
                                                                     with each of twenty participants participating in all four con-
In this section we will model Experiment 2 from Lagnado and          ditions exactly once. They were told that each animation
Speekenbrink (2010) using the formal elements described in           would last no more than 10 minutes.3
Pacer and Griffiths (2012) and above. As mentioned, Bramley             After each video participants were asked to provide judg-
et al. (2014) raised a concern that the framework described          ments about the seismic waves that they had just observed.
in Pacer and Griffiths (2012) does not address “sequences of         Participants were first asked to rate the extent to which each
point events”. Here, we apply this framework to modeling             wave was a cause of earthquakes on a scale of “0 (does not
data from real-time event streams, characterizing sequences          cause the effect) to 10 (completely causes the effect)”. This
of point events and using them for computing the same infer-         provides an “absolute” judgment of each wave’s causal prop-
ences Lagnado and Speekenbrink (2010) asked of their par-            erties since the rating provided for one of the waves did not
ticipants. Moreover, our models’ judgments closely match             constrain the rating provided for the other waves. Participants
average human responses, which suggests the framework suc-           were then asked for “comparative ratings, in which they di-
ceeds both at characterizing the sequences of point events and       vided 100 points amongst the three types of cause.”
at producing models capable of causal inference that compa-
rable to that of human beings facing the same problems.              Building the Model
                                                                     We treated the problem as one of structure induction. That is,
Experiment Description
                                                                     given the knowledge that there are three possible cause vari-
Experiment 2 of Lagnado and Speekenbrink (2010) has the              ables ({A,B,C}) of the effect in question (E) and the data D,
form of participants observing a continuous sequence of              we want to infer a posterior over the possible graphs linking
events (as a video) that represent the time-course of various
                                                                         2
kinds of seismic activity, specifically three kinds of seismic             We should note that “more opportunities” is actually somewhat
                                                                     misleading as opportunities in plural form suggests that there would
waves (which we shall refer to as A, B, C) and earthquakes           be a countable number of opportunities during which these events
(E). The goal of the participants was to infer which (if any)        could intervene. It is more accurate to say that long delays allow for
of the seismic waves were the cause of the earthquakes.              a larger, continuous amount of “opportunity”(a mass noun).
                                                                         3
   Earlier work suggests people will lessen their judgments of             Though participants did see multiple conditions, we treat each
                                                                     trial independently rather than attempting to detect order effects.
causal attribution between two variables if there is a longer(or     While we acknowledge its potential usefulness, addressing this is
more variable) delay between the occurrence of two events.           outside the scope of our analysis.
                                                                 1808

the causes to the effects. Then we will use this posterior to                            10                          Cause A          Cause B           Cause C
                                                                       Human: Absolute
compute measures analogous to those given by participants.                                8
                                                                                          6
Graphs and Parameterization We considered graphs with
any subset of three potential independent causes {A,B,C}.                                 4
All causal links were generative and non-interacting with the                             2
other causal links. Thus the total rate of effects under a graph                          0
                                                                                                Short, High          Short, Low            Long, High            Long, Low
would be the superposition of all Poisson processes induced                              1.0 r 0.9342, p =8.66 ×10−6
by the activity of cause-events according to the graph. As de-
                                                                       Model: Absolute
                                                                                         0.8
scribed in Pacer and Griffiths (2012) this is the continuous-
                                                                                         0.6
time analog to the Noisy-OR parameterization of a causal
                                                                                         0.4
graph. Because causes were independent according to all
                                                                                         0.2
graphs, the likelihood of their occurrences can be removed
from our likelihood calculations.                                                        0.0
                                                                                                       Short, High       Short, Low        Long, High            Long, Low
   In addition to a base-rate process PPλ∅ , which we assumed
was a homogeneous Poisson process with rate λ∅ , we allowed         Figure 2: Top: Mean absolute judgments, Experiment 2 of
                     [X]                                            Lagnado and Speekenbrink (2010). Bottom: mabs model.
each cause-event (td ) to initiate a NHPP with maximum rate
(ψX ) that decays exponentially(ϕX ) relative to the the distance   likelihoods of E- and B-events but eliminate A- and C-events).
                            [X]
from the cause event(∣t −td ∣).                                     Using the reduced event set ({0,t1 ,...,ti ,...,tn }), we can par-
   We sampled these parameters in a similar manner to               tition the observation interval and considering each interval
Pacer and Griffiths (2012). We use uniform random vari-             × event-identity pair τt j ,t j+1 × (X j ,X j+1 ) ≡ τ j conditioned on
ables (u ∼ U(−10.1 ,10.1 )) under a transformation (λ∅ = eu )       previous events associated with cause X (td ∀d ≤ j) we can
                                                                                                                                                        [X]
to determine our initial timescale (in seconds), which acts         calculate the log-likelihood. The total log-likelihood:
as our base-rate PP. This creates a approximate scale-free
baseline parameter (λ∅ ∼ λ1∅ ) from which other parameters                                                     ℓ(D∣Gα ,Θ) = −Λ(t0 ,tn ) + λ{t}n ,
                                                                                                                                                         0
can be sampled. We sample ψX ∼ Γ(λ∅ ,1) (the maximum
                                                                    where Λ(0,tn ) is the log-likelihood component of the intervals,
rate induced by a single event of type X occurring) and
ϕX ∼ Γ(λ∅ ,1) (the rate at which the intensity decays accord-                                                                                                                   [X]
                                                                    Λ(0,tn )= λ∅ (tn −t0 )+ ∑ [ ∑ [ ψϕXX (e−ϕX t j − e−ϕX t j+1 ) ∑ [eϕX td ]]],
ing to the distance in time from that instance) associated with                                                      τ j ∈D X,                                     [X]
                                                              [X]                                                                                                 td     ≤t j
each potential cause X ∈ {A,B,C}. Each cause instance(td )                                                                X→E∈Gα
                                                      [X]
produces a NHPP with rate function ψexp(−ϕ(t −td )).                and λ{t}n is the log-likelihood component of the point events,
                                                                                               0
   Because the baseline distribution is scale-free and defines
                                                                                                                                                                       [X]
other scales, these parameters are not “fit to the data”. A            λ{t}n = ∑ [log(λ∅ + ∑ [ψX (e−ϕX t j ) ∑ [eϕX td ]])].
                                                                           0
“misfit” baseline scale produces overflow, underflow or other                                            [E]
                                                                                                   t j ∈t j
                                                                                                                           X,                       [X]
                                                                                                                                                   td     ≤t j
                                                                                                                         X→E∈Gα
numerical and computational issues that result in model fail-
ure. But, any success can only stem from the model’s struc-           Using the likelihood estimate(L) plus the prior for each
tural commitments and the relation to the modeled data.             graph (in our case, uniform over graphs p(Gα ) ∝ 1,∀α), we
Data The data used to generate stimuli in Lagnado and               can then compute the posteriors for all graphs.
Speekenbrink (2010) are organized by the time-step (in mil-                                                                    L(D∣Gα ) × p(Gα )
liseconds) that an event occurred and the identity of the kind                                        p(Gα ∣D) =
of event (i.e., A,B,C or E). Though there were sixty gener-                                                             ∑Gα ∈G (L(D∣Gα ) × p(Gα ))
ated sequences consistent with the design principles of their       Comparison to Human Responses People judged causes,
experiment, we used only the first twenty which corresponded        not graphs; we need a way to map posterior probabilities
with the conditions that they ran in their study.                   p(Gα ∣D) to causal judgments. Lagnado and Speekenbrink
Structure Inference For each graph (Gα ∈ G) and dataset             (2010) asked participants for absolute measures (assign each
(D) we take our sampled parameters({Θ}m∈{1,...,M} ; for us,         potential cause a value on scale from 0 to 11) and comparative
M = 200000) and compute:                                            measures (assign a total of 100 points to the three causes).
                                                                       We will model judgments as statements about structure in-
                         1 M                                        ferences (not strength estimations). We interpret the absolute
            L(D∣Gα ) ≈     ∑ exp(ℓ(D∣Gα ,Θm ))                      score in terms of a probability that a particular variable is
                         M m=1
                                                                    thought to be present by marginalizing over the probabilities
                                                                    given to the graphs that include that variable is a cause. I.e.,
  We compute log-likelihoods (ℓ(⋅)) under Gα and Θm as fol-
lows. For computational efficiency, we eliminate events do                                     mabs (X ∈ N; p(G∣D)) =                       ∑             p(Gα ∣D)
not alter other events (e.g., under graph B → E, we consider                                                                          Gα ∶(X→E)∈Gα
                                                                1809

Note if the complete graph were to receive all the probabil-                           100
                                                                       Human: Comparative
                                                                                                                   Cause A           Cause B           Cause C
ity measure, then mabs (A), mabs (B), and mabs (C) would each                               80
equal 1, and so their sum would equal 3. Thus, this is not a                                60
probability measure in the usual sense because we have not                                  40
defined probabilities over causes, but over graphs.                                         20
   However, by normalizing by the sum of these measures                                      0
over all nodes, we can adapt the absolute measure to saying                                        Short, High          Short, Low        Long, High         Long, Low
                                                                                            1.0 r 0.9559, p =1.22 ×10−6
something about the comparative importance of the different
                                                                       Model: Comparative
                                                                                            0.8
nodes in producing the effect. This will sum to 1, but still
                                                                                            0.6
should not be interpreted as anything like a direct probability
of the cause being present.                                                                 0.4
                                                                                            0.2
                             ∑Gα ∶(X→E)∈Gα p(Gα ∣D)                                         0.0
   mcomp (X ∈ N; p(G∣D)) =                                                                         Short, High          Short, Low        Long, High         Long, Low
                           ∑x∈N ∑Gα ∶(x→E)∈Gα p(Gα ∣D)                                      1.0 r 0.9802, p =2.35 ×10−8
                                                                                            0.8
                                                                       Model: Unique
Finally, we could consider the comparative prompt as imply-                                 0.6
ing that there is only one cause, and so we should only con-                                0.4
sider those graphs which attribute a single cause for produc-                               0.2
ing the effect in question. In fact we can say that under the                               0.0
restriction that only one cause may exist the graph including                                        Short, High        Short, Low        Long, High         Long, Low
X as its sole cause is the measure of the comparative impor-        Figure 3: Top: Mean human comparative judgments from
tance of X (since it is the only graph with that cause).            Lagnado and Speekenbrink (2010). Middle: mcomp model.
                                                                    Bottom: munique model.
                                     p(X → E∣D)
     munique (X ∈ N; p(G∣D)) =
                                 ∑X∈{A,B,C} p(X → E∣D)              Acknowledgments. We thank Maarten Speekenbrink and David
                                                                    Lagnado for providing their stimuli and results. Support for this
Results                                                             work was provided by the National Defense Science & Engineering
                                                                    Graduate Fellowship (NDSEG) Program awarded to MP and grant
We find an excellent fit between our models’ predicted
                                                                    FA9550-3-1-0170 from the Air Force Office of Scientific Research
values and average human judgments for both absolute
                                                                    awarded to TG.
(r ≈ 0.93, p < 10−5 , see Figure 2) and comparative judgments
(mcomp ∶ r ≈ 0.96, p < 10−5 ; munique ∶ r ≈ 0.98, p < 10−7 , see                                                        References
Figure 3) of the different kinds of waves’ causal importance.       Blundell, C., Beck, J., & Heller, K. A. (2012). Modelling reciprocat-
                                                                      ing relationships with Hawkes processes. In Advances in Neural
                         Discussion                                   Information Processing Systems (pp. 2600–2608).
                                                                    Bramley, N. R., Gerstenberg, T., & Lagnado, D. A. (2014). The
Learning from streams of events unfolding in continuous time          order of things: Inferring causal structure from temporal patterns.
is an important case to consider in studying human causal in-         In Proceedings of the 36th Annual Conf. of the Cognitive Science
duction. We have shown how this case can be accommodated              Society.
                                                                    Gopnik, A., Glymour, C., Sobel, D., Schulz, L., Kushnir, T., &
within the framework of Pacer and Griffiths (2012), success-          Danks, D. (2004). A theory of causal learning in children: Causal
fully predicting mean human judgments of real-time event              maps and Bayes nets. Psychological Review, 111, 1-31.
streams used in Lagnado and Speekenbrink (2010). We know            Greville, W., & Buehner, M. (2007). The influence of temporal
                                                                      distributions on causal induction from tabular data. Memory &
of no other framework that has been brought to bear on real-          Cognition, 35, 444453.
time event streams.                                                 Greville, W., & Buehner, M. (2010). Temporal predictability fa-
   Often, statistical frameworks of theories of human causal          cilitates causal learning. Journal of Experimental Psychology:
                                                                      General, 139(4), 756.
inference are contrasted with mechanistic theories. The kinds       Griffiths, T. L., & Tenenbaum, J. B. (2009). Theory-based causal
of arguments used against statistics are that they neglect to in-     induction. Psychological review, 116(4).
clude information about temporal and spatial characteristics        Lagnado, D. A., & Speekenbrink, M. (2010). The influence of de-
                                                                      lays in real-time causal learning. The Open Psychology Journal,
of the modeled processes. We have seen success in modeling            3(2), 184–195.
causal induction that relies explicitly on the temporal aspects     Pacer, M., & Griffiths, T. (2012). Elements of a rational framework
of events. We see this as a step towards reconciling mecha-           for continuous-time causal induction. In Proc. of the 34th Conf.
                                                                      of the CogSci Society.
nism and statistics. More sophisticated graphical models can        Simma, A., & Jordan, M. (2010). Modeling events with cascades of
be built that articulate complex causal mechanisms that are           poisson processes. In International conference on uncertainty in
sensitive to particular temporal and spatial properties. This         artificial intelligence.
gives a hope for dissolving statistical-mechanistic divide and
the birth of a new synthesis with statistical inferences com-
puted over mechanistic theories.
                                                                1810

