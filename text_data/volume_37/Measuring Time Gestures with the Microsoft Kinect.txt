                    Measuring Time Gestures with the Microsoft Kinect
	  
                                        Daniel Lenzen (dlenzen@ucsd.edu)
                                         University of California – San Diego
                              Department of Cognitive Science, 9500 Gilman Drive
                                                    La Jolla, CA 92093
	                                                               press, slider) is congruent with the participant’s
                         Abstract                                time-space metaphor (Sell & Kashak, 2011).
                                                                    While these sensorimotor tasks offer
     Gestures related to time can reveal implicit                behavioral support to time-space mappings, they
     representations of the TIME is SPACE metaphor               constrain responses to single dimensions. Co-
     (Núñez & Sweetser, 2006). While past research               speech gesture, on the other hand, offers a three-
     has shown that gestures illustrate the direction of         dimensional canvas for conveying information.
     future and past on timelines, no detailed analysis
     of timelines has been possible. Using the Kinect
     depth camera and body tracking technology, we
                                                                 Time & Gesture
     tracked participants’ co-speech gestures while              Gesture can be a channel into understanding
     explaining time-related concepts. We present data           spatial or imagistic features of cognitive
     collected with novel, relatively unsupervised               representations (McNeill, 1992), making it a ripe
     Kinect-based methods that offer evidence similar
     to traditional gesture-coding methods and could
                                                                 domain for investigating abstract concepts
     provide the opportunity for novel theoretical               understood metaphorically with spatial concepts.
     findings.                                                   Studying gestures about time, for example, has
                                                                 revealed features of conceptualizations of time
     Keywords: Gesture; Motion Capture; Time;                    (see Cooperrider, Núñez & Sweetser, 2014, for a
     Metaphor                                                    review). For example, when discussing time,
                                                                 gesturers use the sagittal (front-to-back) and
                     Introduction                                horizontal (left-to-right) axes in varying ways
                                                                 depending on cultural factors. That is, members
Space-Time Metaphors                                             of a culture that has a dominant language that is
                                                                 written left to right tend to gesture about
Human cultures frequently use orientational
                                                                 sequential events with the past to the left and the
metaphors to reason and communicate about
                                                                 future to the right, while the opposite is true for
abstract concepts such as time and number. That
                                                                 users of languages that are written right to left.
is, numbers and temporal concepts are mapped
                                                                   This directional difference is also found for the
onto spatial locations (Lakoff & Johnson, 1980).
                                                                 slightly different, sagittal axis (Núñez &
These metaphors are readily available in the
                                                                 Sweetser, 2006). The Aymara tribe in the Andes
language cultures use. In particular, English
                                                                 mountains speak and gesture about the past
speakers use phrases grounded in the sagittal
                                                                 being in front of them and the future behind.
(front-to-back) axis when discussing time – e.g.
                                                                 These      studies     show   different     cultural
“Thanksgiving is behind us”. While some
                                                                 conceptualizations of time in relation to oneself
cultural artifacts such as calendars and timelines
                                                                 exist, and gesture can support and expand on
use the lateral (left-to-right) or vertical (up-
                                                                 linguistic evidence of these conceptualizations.
down) axes to visually represent time, these do
                                                                   While English has many expressions about
not typically appear in English expressions.
                                                                 time that rely on the sagittal axis but not the
Additionally, those artifacts (as well as writing
                                                                 lateral axis (i.e. an upcoming event is not “to the
direction) tend to vary more frequently across
                                                                 right”), English speakers tend to gesture along
cultures than the front-is-future, past-is-behind
                                                                 the lateral axis when discussing time naturally
representation of time.
                                                                 (Núñez & Cooperrider, 2013 for a review).
     These spatial metaphors are not just linguistic
                                                                 When directly instructed to gesture about time,
epiphenomena, but have been shown to be
                                                                 however, participants switched to primarily
embodied in physical experience. Reaction times
                                                                 using the sagittal axis (Casasanto & Jasmin,
to the presentation of time words (visual or
                                                                 2012). This intentionally communicative setting
auditory) is significantly faster when the
                                                                 mirrors the use of the sagittal axis when
direction of reaction movements (e.g. button
                                                                 discussing time in American Sign Language
                                                            1285

(ASL). ASL strictly uses the sagittal axis for           placed on the body (Lu, & Huenerfauth, 2010).
deictic references to time and the lateral axis for      Computer vision techniques for tracking gesture
sequencing events (Emmorey, 2001).                       are a promising avenue, but are computationally
  While the axis and direction in which people           intense and currently less accurate than computer
gesture for different temporal events has been           vision algorithms using the depth data offered by
documented (i.e. forward, backward, left, right),        the Kinect (Han, Shao, Xu, & Shotton, 2013).
there has been almost no work on the details of          Motion capture systems offer precise tracking of
how people gesture in these directions. Recent           body movements in a fixed laboratory setting
work has suggested that co-speech gestures               (for exception see Glowinski et al., 2013) at a
sometimes blend the sagittal and lateral timelines       greater cost of equipment, space, and setup time.
(Walker & Cooperrider, in press), but there has            The Microsoft Kinect offers a non-invasive,
been no detail in the profiles of these gestures.        cheap, mobile alternative to traditional motion
We will examine the three-dimensional location,          capture at a cost of tracking accuracy. Previous
distance and velocity of these gestures.                 research has used the Kinect to study laughter
  Time gestures could be simple binary                   (Mancini, Varni, Niewiadomski Volpe, &
movements – forward or backward, right or left           Camurri, 2014)
– as has been coded in previous research. That is,         With basic data techniques and integrative
when talking about different times in the past,          software, the Kinect can offer researchers new
co-speech gestures might move backward                   measurements of spontaneous and intentional
towards the same (or a random) location and at           gesture. This study uses the Kinect to add three-
the same (or random) speed no matter how                 dimensional, quantitative detail to our
distant the time concept may be. The information         understanding of time-related gestures.
in these gestures would be redundant with the
speech and only reinforce when the event                                      Method
happened. In this case, we would expect to find
only a difference of direction of gesture between        Participants
future and past words.
                                                         24 UCSD undergraduates participated in the
  On the other hand, if the timeline metaphors
                                                         gesture study. All participants learned English
are strongly embodied and spontaneously
                                                         before the age of 4. Three participants were
displayed, gestures along the lateral and sagittal
                                                         excluded from further analysis because of
axes related to time concepts could encode the
                                                         significant exposure to a sign language. Three
perceived distance from now (some front-center
                                                         participants were excluded for having body
location on the body) of those concepts. A
                                                         tracking data for less than 80% of frames,
gesture about the deictic concept “recently”
                                                         leaving 18 participants (13 female, 5 male; 1 left-
could have a different profile from a gesture
                                                         handed).
about “previously”, despite their common feature
                                                           Thirty participants from the same population as
of being related to the past. In this case, we
                                                         the gesture study participated in the online
would expect to find a difference in speed or
                                                         norming study.
trajectory of gesture based on perceptual distance
of the concept.                                          Materials
  New       developments      in    body-tracking
technology from contact-free sensors can add a           Twelve time-related words were selected from
level of detail to coarser measures of gesture.          those used in Lakens, Semin, & Garrido (2011).
Depth cameras can contribute yet another level –         Words were classified as either past, present, or
information in the z-axis – overcoming the 2-            future-related (e.g. “Past”, “Today”, “The day
dimensionality of normal RGB video.                      after tomorrow) and either deictic referential or
                                                         sequential (e.g. “Recently”, “Earlier”).
Body Tracking                                              Additionally, 10 spatial and 14 abstract
                                                         concepts were used as filler words (e.g. “Front”,
Most cognitive gesture research has been                 “Hero”).
conducted with video cameras and human
coding, but there have been many recent                  Norming Study
advancements tracking human motion with
computer vision techniques on RGB video                  In order to determine how far in the past or
(Song, Dimirdjian, & Davis, 2012) or measured            future the stimulus words were perceived to
directly with physical motion capture sensors            exist, an independent group of 12 participants
                                                         were given an online survey and asked to mark
                                                    1286

the position on a horizontal line with “Present”          underestimates the peaks of gestures, working
marking the midpoint and no endpoints (Lakens             against an effect we might find.
et al., 2011). Future words were rated as                   To normalize across participants, peak
significantly to the right of the midpoint (t-test p      locations were calculated as z-scores (standard
< .001; mean = 34.8/100; s.d. = 28.5). Past words         deviations) from the mean locations across all
were rated as significantly to the left of the            responses (including spatial and filler terms).
midpoint (t-test p < .001; mean = -40.2/100; s.d.            Responses in which there was little movement
= 30.6).                                                  were excluded from analyses. Lack of movement
                                                          was defined as less than 1.8m of total movement
Procedure                                                 – an average of test data segments in which we
                                                          observed no noticeable, meaningful gesture.
During the gesture tasks, participants sat on a
stool facing a Kinect v1.0 placed 1.7m away at
eye-level. Stimuli were presented using                   Average Peak Analysis
Microsoft Powerpoint on a laptop placed below             During spontaneous gesture responses, the
the Kinect. Stimuli automatically progressed,             average rightward (participant’s perspective)
and participants were not interrupted when the            wrist maximum was further to the right for the 5
next word appeared – resulting in some                    future-related phrases than the 5 past-related
variability in the length of each trial. An               words (-2.10 vs. -1.50 standard deviations from
experimenter was present and sat next to the              the participant’s mean lateral position; p < 0.01).
Kinect and laptop. Audio, RGB frames, depth                 The average leftward maximum was further to
frames, and 10 upper-body joint estimations (30           the left for the 5 past-related words than for the 5
fps) were recorded continuously during the                future related words (2.65 vs. 2.06 standard
gesture tasks using ChronoSense software                  deviations left of the mean lateral position; p <
(Weibel et al., 2015).                                    0.05).
                                                            Along the sagittal axis, the average forward
Gesture Study                                             wrist maximum was not significantly further
Participants viewed time, space or abstract words         forward for the 5 future-related phrases than the
and were asked to explain the concept to a                5 past-related words (-1.91 vs. -1.75 standard
person who did not know what they meant.                  deviations from the mean sagittal position; p =
There was no mention of speech or gesture.                0.3).
Participants had 15s to respond.                            The average backward maximum was not
  Responses were segmented in ChronoViz                   significantly further back for the 5 past-related
(Fause et al., 2011), a software tool designed to         words than for the 5 future related words (1.46
integrate Kinect data and allow for data                  vs. 1.35 standard deviations from the mean
annotation. Spontaneous gesture trials were               sagittal position; p = 0.4).
segmented based on the speech produced –                    We used gesture peaks as a proxy for activity
ending when the participant started describing            on a particular axis to compare responses to
the next stimulus.                                        deictic and sequential words. Right hand
                                                          responses to sequential words had forward
                     Results                              maxima significantly less far forward than those
                                                          for sequential words (-2.00 vs. -2.62 standard
Data Cleaning                                             deviations forward of the mean wrist position; p
                                                          <0.01). No other significant differences between
Occasionally, mostly due to occlusion, the                deictic and sequential words occurred.
Kinect is unable to track joints. In our study,
wrist estimates were present for significantly
more time points than hand estimates (89.8%
present vs. 51.1% present), so the wrist is used as
a proxy for hand location. Data segments in
which both wrists were not tracked (10.2% of
total data) were excluded from analysis.
   Wrist data was smoothed using a moving
average filter (span of 10). Note that this
smoothing limits the influence of outliers –
useful for erroneous data points, but also
                                                     1287

                                                         words than for future words (2.54m vs. 2.28m; p
                                                         > 0.25).
                                                                             Discussion
                                                         Our new methods of measuring co-speech,
                                                         spontaneous gesture supports previous hand-
                                                         coded findings while requiring less human
                                                         supervision. More nuanced analyses and more
                                                         human supervision could offer new insight on
                                                         well-known gesture findings.
                                                           We have replicated the findings of Casasanto
                                                         and Jasmin (2012) that there are more significant
                                                         differences in spontaneous, co-speech gesture
                                                         along the lateral axis than the sagittal axis.
                                                         Similar to Walker and Cooperrider (in press), we
                                                         found a relationship between more forward
Fig 1. Average lateral rightward and leftward            gestures and more rightward gestures, and a
maxima for future and past responses.                    negative relationship between rightward gestures
                                                         and further back (less far forward) gestures. This
Metaphor Blends                                          relationship does not exist for the left hand
                                                           Further analysis could examine the distance
  Based on previous research, we predicted a             traveled along the particular axes of interest and
relationship between forward gestures and                help understand how gesturers use the two axes
rightward gestures, and backward and leftward            in greater detail. We have only reported distance
gestures. Right hand maxima further to the               in general, and used peaks as a proxy for activity
participants’ right were significantly positively        along an axis – a coarse measure. We can
correlated (r = 0.25; p < 0.05) with maxima              analyze intentional, elicited gesture in the vein of
further in front of the participant and                  previous research to provide additional detail. In
significantly negatively correlated (r = -0.25;          the future, these methods could be co-opted to
p<0.05) with maxima further behind (or less far          provide novel findings for theoretical issues.
in front of) the participant. This suggests                A future study will compare these gestures to
responses with a gesture further to the right tend       more developed systems of manual movements –
to have a gesture further forward and tend to not        sign     languages.     Sign     languages     have
have a gesture further back.                             grammatical rules for locations and movement
  Right hand maxima further to the participants’         trajectories, and previous research has found the
left are significantly negatively correlated with        use of timelines in various sign languages
maxima further in front of the participant (r = -        (Engberg-Pedersen, 1993). Few attempts have
0.37; p < 0.01) and not significantly correlated         been made to understand exactly how signers use
with maxima further behind (or less far in front         the sagittal timeline.
of) the participant. This suggests responses with          Emmorey (2001) states that in ASL, points to
right hand gestures further to the left tend to          locations behind the signer can only have spatial,
have gestures less far forward, but not further          and not temporal meaning. With these methods
back from the mean position.                             we can examine if signs related to more distant
                                                         past or future events subtly encode that distance
Total Distance                                           in location or trajectory as a “gestural”
                                                         component of sign, despite the grammatical rule.
As an indicator of relative activity between
hands, we calculated cumulative distance
traveled during the response windows. Across all
                                                                        Acknowledgments
spontaneous gesture responses, the right wrist           Thanks to Seana Coulson and Ben Bergen for
travelled significantly further on average than the      project design and analysis suggestions. Thanks
left wrist (3.34m vs. 2.56m; p < 0.01). The right        to Jim Hollan, So-One Hwang, Carol Padden and
wrist did not travel significantly more for future       the UCSD Chancellor’s Interdisciplinary
words than for past words (3.07m vs. 3.20m; p >          Collaboratories      Fellowship      for    support.
0.5), and the left wrist did not further for past
                                                    1288

                   References                           Núñez, R., & Sweetser, E. (2006). With the
                                                          future behind them: Convergent evidence from
Casasanto, D., & Jasmin, K. (2012). The hands             Aymara language and gesture in the
  of time: Temporal gestures in english                   crosslinguistic     comparison       of   spatial
  speakers. Cognitive Linguistics, 23(4), 643-            construals of time. Cognitive Science, 30, 1-
  674.                                                    49.
Cooperrider, K., Núñez, R. & Sweetser, E.               Sell, A.J., & Kashak, M.P. (2011). Processing
  (2014). The conceptualization of time in                time shifts affects the execution of motor
  gesture. In Müller, C., Cienki, A., Fricke, E.,         responses. Brain and Language, 117, 39-44.
  Ladewig, S.H., McNeill, D., & Tessendorf,             Song, Y., Demirdjian, D., & Davis, R. (2012).
  S.(Eds.), Body-Language-Communication (vol.             Continuous body and hand gesture recognition
  2). New York: Mouton de Gruyter.                        for natural human-computer interaction. ACM
Emmorey, K. (2001). Language, cognition, and              Trans. Interact. Intell. Syst. 2, 1.
  the brain: Insights from sign language                Walker, E., & Cooperrider, K. (in press). The
  research. Mahwah: Lawrence Erlbaum                      continuity of metaphor: Evidence from
  Associates, Publishers.                                 temporal gestures.
Engberg-Pedersen, E. (1993). Space in Danish            Weibel, N., Rick, S., Emmenegger, C., Ashfaq,
  Sign Language: The semantics and                        S., Calvitti, A., & Agha, Z. (2015). LAB-IN-
  morphosyntax of the use of space in a visual            A-BOX: semi-automatic tracking of activity in
  language. Hamburg, Germany: Signum-                     the medical office. Personal Ubiquitous
  Verlag.                                                 Computing, 19, 317–334.
Fouse, A., Weibel, N., Hutchins, E., & Hollan,
  J.D. (2011). ChronoViz: A System for
  Supporting Navigation of Time-Coded Data.
  Proceedings of ACM Conference on Human
  Factors in Computing Systems, 299-304.
Glowinski, D., Mancini, M., Cowie, R., Camurri,
  A., Chiorri, C., & Doherty, C. (2013). The
  movements made by performers in a skilled
  quartet: a distinctive pattern, and the function
  that it serves. Frontiers in psychology, 4.
Han, J., Shao, L., Xu, D., & Shotton, J. (2013).
  Enhanced Computer Vision with Microsoft
  Kinect Sensor: A Review. IEEE transactions
  on cybernetics, 43, 5.
Huenerfauth, M., & Lu, P. (2010). Accurate and
  Accessible Motion-Capture Glove Calibration
  for Sign Language Data Collection. ACM
  Transactions on Accessible Computing, 3, 1, 2.
Lakens, D., Semin, G.R., & Garrido, M.V.
  (2011). The sound of time: Cross-modal
  convergence in the spatial structuring of time.
  Consciousness and Cognition, 20, 437-443.
Lakoff, G., & Johnson, M. (1980). Metaphors we
  live by. Chicago: University of Chicago Press.
Mancini, M., Varni, G., Niewiadomski, R.,
  Volpe, G., & Camurri, A. (2014, April). How
  is your laugh today?. In CHI'14 Extended
  Abstracts on Human Factors in Computing
  Systems (pp. 1855-1860). ACM.
McNeill, D. (1992). Hand and mind: What
  gestures reveal about thought. Chicago:
  University of Chicago Press.
Núñez, R., Cooperrider, K., (2013). The tangle
  of space and time in human cognition. Trends
  in Cognitive Sciences, 17(5), 220-229.
                                                   1289

