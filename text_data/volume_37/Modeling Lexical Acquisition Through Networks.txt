                              Modeling Lexical Acquisition Through Networks
                                         Nicole Beckage (nicole.beckage@colorado.edu)
                                       Department of Computer Science, 111 Engineering Center
                                                          Boulder, CO 80309 USA
                                           Ariel Aguilar (ariel.aguilar@microsoft.com)
                                                       Microsoft, One Microsoft Way
                                                         Redmond, WA 98052 USA
                                         Eliana Colunga (eliana.colunga@colorado.edu)
                                        Department of Psychology and Neuroscience, UCB 345
                                                          Boulder, CO 80309 USA
                              Abstract                                  Fenson, 1996). We use a graph-theoretic network representa-
                                                                        tion where the words are the nodes in the graph and the edges
   We examine the nature of phonological and semantic similar-          are based on semantic or phonological similarity. Finally, we
   ity in early language learning. We consider how the use of this
   information might change over the course of development. To          quantify the extent to which the network representation im-
   this end, we represent the lexicon as either a phonological or       proves our ability to predict which words a child will learn
   semantic network and model the growth of this network. Con-          next. We summarize the modeling results as a function of
   structing normative vocabularies from the Communicative De-
   velopment Inventory norms, we utilize a preferential attach-         age, productive vocabulary size, and language skill.
   ment growth algorithm. We predict and quantify the words
   which will be learned next, comparing the two network repre-         Growth Networks
   sentations. We consider the effect of age, total vocabulary size
   and language ability as measured through CDI percentile. Our         By assuming a network representation where the edges are
   findings suggest that the semantic representation does not out-      based on phonological or semantic similarity, we can model
   perform the baseline bag-of-words model, whereas the phono-
   logical representation conditionally does. More generally, we        the acquisition of individual words through a network growth
   show that the network representation influences the ability of       model. We turn to the work of Steyvers and Tenenbaum
   a model to capture vocabulary growth. We further offer a             (S&T) for their model of network growth in the context of
   method of analysis for testing representational assumptions in
   network models.                                                      language acquisition (2005) and adapt it to our paradigm.
                                                                        We also consider the methodology of Hills and colleagues
   Keywords: Language acquisition; word learning; lexical ac-
   quisition; network modeling; preferential attachment                 (Hills et al., 2010, 2009b) for their work of comparing net-
                                                                        work models and constructing a normative vocabulary.
                                                                           S&T considered the structure of three semantic networks,
                          Introduction
                                                                        showing that these semantic networks had similar large scale
There is much evidence to support the idea that children learn          structure, with high local clustering and short average path
words systematically. A child’s vocabulary relates to that of           lengths between words. They also found evidence of a power-
their parent’s suggesting that the environment plays an im-             law in the degree distribution within these networks. This
portant role (Weizman & Snow, 2001). The interest of the                led to the construction of a model of semantic growth where
child further influences language learning (DeLoache et al.,            words are more likely to be learned if they connect to known,
2007), and the words a child knows are useful in predicting             highly connected words in the graph. They call this model
the words that child will learn next (Beckage & Colunga,                preferential attachment because of its similarity to the growth
2013). Concrete nouns are learned earliest, as are shorter              model of Barabási and Albert (BA model, 1999). Further,
words (Gentner, 1982). However, there is still systematic-              the modeling results suggest a correlation between age of ac-
ity in learning that is not fully understood. In this paper we          quisition and global network structures of early language net-
look at the changing role of two specific sources of informa-           works.
tion that influence the learning of words – phonological or                Hills and colleges extended this work by comparing the
semantic information.                                                   content of the vocabulary, as generated by the models, to
   Here we examine the systematicity present in word learn-             a normative vocabulary constructed from the CDI norms.
ing. Specifically by focusing on whether phonological or se-            Whereas the previous work considered three different models
mantic similarity dominate early language learning. We also             of acquisition, we consider only the preferential attachment
consider how the role of semantic and phonological informa-             model since this model is also used in the work of S&T. We
tion might change over the course of development. In our ap-            also maintain the assumption that the underlying network is
proach, we model growth in the normative productive vocab-              fixed and the nodes are labeled. We extend their definition
ulary of 16 to 30 months olds, based on the MacArthur Bates             of normative language acquisition to compare a set of nor-
Communicative Development Inventory norms (CDI, Dale &                  mative vocabularies across different ages and language abil-
                                                                    190

ities. Hills and colleagues assumed a semantic network as          for the observed word acquisition data. Thus, we ask: 1)
the underlying network representation and compared differ-         Which linguistic features capture and potentially guide lexi-
ent growth models. Here we instead assume a network model          cal growth? 2) Are there different relevant linguistic features
and ask whether a semantic or phonological network repre-          at a) different points in development or b) across different de-
sentation is more predictive in modeling lexical acquisition.      velopmental trajectories?
Linguistic Information                                             Normative vocabulary networks
There is evidence to suggest both phonological and seman-          To achieve our modeling goals, we define a ’vocabulary snap-
tic aspects play a role in early language learning. First, the     shot’ to be a starting network (from a specific month) and a
phonemic pattern and the length of the word play an impor-         goal network–the network one month later. This allows us to
tant role in early word learning. Not only do length and the       test the ability of the model and representation to predict from
number of phonemes matter, but the number of words that            one month to the next. We utilize the MacArthur-Bates Com-
are phonologically similar to a given word (the phonologi-         municative Development Inventory (Dale & Fenson, 1996,
cal neighborhood) also affects learning. Words that are part       CDI) norms to compute vocabulary snapshots. The CDI is
of denser phonological neighborhoods are more likely to be         a parent report vocabulary checklist consisting of 680 words,
learned even when frequency and length are controlled for          spanning 22 semantic categories and including the most com-
(e.g., Storkel, 2009).                                             mon parts of speech. We utilize the 16-30 month produc-
   On the other hand, semantic aspects also play a role in         tion norms which aggregates productive vocabularies of 1130
early word-learning. For example, sensory-motor seman-             children of different ages through parent report. For our mod-
tic features have been shown to be available to even pre-          eling study, we focus specifically on nouns, and further con-
linguistic children (e.g., Bloom, 2002). In fact, Howell et        sider only the 352 words that are normed in both the CDI and
al. showed a significant improvement in word prediction ac-        by the Howell sensory-motor features which we use to con-
curacy by including sensory-motor features in a neural net-        struct our semantic network.
work model, suggesting that semantic features inform word             To construct normative (prototypical) vocabularies, we
learning (2005). Hills and colleges also explore the issue of      convert the norms (which include the percentage of children
semantic features by asking directly what type of semantic         of a given age who produce each word) to vocabulary snap-
edges–perceptual or conceptual–are most useful in predicting       shots. To do this, we consider a word learned if the norm for
acquisition (Hills et al., 2009b). Their results suggest that      that word and age is above a certain threshold. We can con-
perceptual features are more robust, but conceptual features       struct a variety of normative vocabulary snapshots by varying
are more discriminating and more likely to be used. With our       the percentage of the population that was reported to produce
network models, we try to understand the independent con-          a specific word. We consider thresholds between the range of
tributions of semantic and phonological information to early       the 10% of the population through 90% (indicating the rate of
word learning.                                                     production) in increments of 5. We create a range of normed
                                                                   vocabularies in the hopes of capturing the developmental dif-
                           Methods                                 ferences in vocabulary growth. We consider age, vocabulary
In this paper we utilize a network growth model to understand      size and language ability in our analysis. To assess language
and quantify the relevance of phonological and semantic fea-       ability, we use the CDI percentile which considers the size
tures on language acquisition in children. To isolate phono-       of the vocabulary for a given age as compared to their peers.
logical or semantic features from each other and other impor-      This means that a child in the 90th CDI percentile will have
tant components of language acquisition, we make a few ini-        a larger vocabulary than a child in the 20th percentile. Thus
tial assumptions. We first assume that the productive vocabu-      we use 100-threshold to approximate the CDI percentile in
lary can be represented as a network with words as nodes and       our study. We use this CDI percentile as an approximation
relations between nodes determined by similarity in phono-         of language ability. We also assume that if a word enters the
logical or semantic space. We define the exact mapping be-         vocabulary, it stays in the vocabulary–a situation that is not al-
tween edges in the network and phonological/semantic sim-          ways true when we use the norms to calculate the vocabulary.
ilarity in more detail below. Second, we assume that the           In total there are 206 vocabulary snapshots pairs (starting and
growth of this vocabulary network can be modeled through           predicted), representing 17 different thresholds for normative
a process of preferential growth that is similar to preferen-      vocabularies between 16 and 30 months.
tial attachment (Barabási & Albert, 1999; Steyvers & Tenen-
baum, 2005) and that this model captures some aspects of           Preferential Growth Model
language acquisition and language learning in children. We         We adapt the S&T model to account for a fixed edge list
finally assume that ’normative vocabularies’ as defined below      (Steyvers & Tenenbaum, 2005). The preferential attachment
represent individual children’s acquisition trends. We hold        model was tested on normative vocabularies by Hills and col-
the underlying process of network growth constant as this al-      leagues (2009b), though with a different set of edges and to
lows for direct examination of how the definition of an edge       answer different questions. Because of the prevalence and
changes the ability of the network growth model to account         use of variants of preferential attachment in the literature, we
                                                               191

assume, for this study, that lexical networks grow according              Networks
to a process similar to preferential attachment. Preferential
attachment can be seen as a growth model in which, at each                If the network representation is useful, our models will
step, a new node and some edges are added to an existing                  outperform uniform acquisition (where each unknown word
network graph. The new node attaches to already existing                  has equal probability of being learned). We also consider
nodes proportionally to the number of neighbors of the ex-                the importance of the underlying network representation by
isting node. This results in a ’rich get richer’ effect as well-          running the model on a network with the same number of
connected nodes (high degree words) in the existing graph                 edges but drawn at random. We evaluate the representation
are likely to acquire edges from new nodes, further increas-              by calculating the overlap between predicted words for
ing their connectivity and thus increasing their likelihood of            learning and the words that are actually learned, according
’preferentially attaching’ to new incoming nodes.                         to norms, in a single month’s time. These networks, and
   This model cannot be directly applied to our vocabulary                the comparison to the random models, offer a way of
networks since, in our case, we have a predefined maximal                 understanding the importance of phonological and semantic
network G0 (V 0 , E 0 )–the nodes (V 0 ) are labeled and edges (E 0 )     similarity on early language acquisition.
are fixed– such that for any network, two nodes are either con-
nected or not connected. For example, a vocabulary contain-               Phonological Network To construct a phonological
ing words dog and cat will have a node for each of these two              network, we convert the set of vocabulary words to the
words and those two nodes will always be connected in the                 international phonemic alphabet (IPA). This was done using
semantic network and always be not connected in the phono-                PhoTransEdit, a free Windows tool used to transcribe English
logical network. We are trying to understand how the network              texts to phonetic transcriptions. This transcription was used
grows over time, not only where new nodes could attach. We                to create a feature by word matrix where the full features
thus relax the definitions in the BA model to generalize it to            were all phonemes in the English language and the word
our case, as in (Hills et al., 2009b). In each iteration of this          specific features were counts of the number of times a
model, we select an attachment node from the current vocab-               given phoneme appeared in a word. The phonetic similarity
ulary graph G(V, E) ⊆ G0 (V 0 , E 0 ) proportionally to the degree        of each word was then computed as a cosine similarity
of the word in graph G (as in the original BA model). We                  of the phoneme-feature vector between two words. The
then select an unknown neighbor of the attachment node and                cosine-similarity calculation resulted in a symmetric matrix
assume that this is the newly learned word. Finally, we up-               of words by words where each cell contained a value between
date the graph such that all edges between existing words and             0 (no phonemic overlap) and 1 (complete phonemic overlap).
the newly learned word are present in the vocabulary graph.               The resulting matrix was thresholded to a binary matrix at
Under this algorithm, the probability that an unknown word j              a value of 0.6 as there was a noticeable break in similarity
is learned given the current graph G(V, E) is defined as:                 between words around this value making it a robust threshold
                                                                          for the greatest range.
                Pr(learn( j)) ∝     ∑       di I( j,i)∈E 0
                                  i∈V, j6∈V
                                                                          Semantic Networks We utilize the sensory-motor feature
where di is the degree of node i given that node i is known.              matrix of Howell and colleagues for our semantic network
This degree contributes to the probability of learning word j             (2005). In the study by Howell and colleagues, participants
if an edge exists between i and j in the full graph G0 (V 0 , E 0 ).      were asked to rate early learned nouns on a set of 97 differ-
   We run this algorithm for each vocabulary snapshot, ini-               ent features. Participants were instructed to make judgments
tializing the current vocabulary graph to the starting CDI vo-            from the perspective of a preschool aged child. These fea-
cabulary. Preferential growth runs until the network is size-             tures included aspects such as size, color, texture and other
matched to the target vocabulary. Note that we still update the           features. These ratings were collected for 352 nouns and
current graph after each iteration to include the newly learned           were averaged across at least 200 participants. These rat-
word, and all its relevant edges, such that sequential effects            ings capture population level averages indicating the extent
may appear in the model. That is, if dog gets added as an                 to which an object possesses a given feature. We compute the
attachment to cat, and puppy is already a node in the graph               semantic similarity of two words by calculating the cosine-
(i.e. a known word), the new node for dog will link with both             similarity of the feature vectors for each words This provided
cat and puppy. Further, a new word might attach to dog in the             us with a symmetric matrix that we convert to a binary matrix
next time step. We compare the words selected by a given run              by thresholding. We threshold at a level of 0.85 as there was
of the algorithm to the words that have actually been learned             a break in the cosine-similarity ratings of all vectors at that
according to the normative vocabulary snapshots. The greater              point. We chose the Howell feature norms for this analysis
the overlap, the more useful the underlying network is in                 because it specifically attempts to capture sensory-motor fea-
capturing language learning. To account for the stochastic-               tures that might be available and important to young children
ity and intractability, we simulate a model run –growing the              and, as such, is well suited for our specific research question.
vocabulary– and compare it to the predicted vocabulary 1000               Since we only have the sensory-motor features for 352 nouns,
times. We then compute the average model performance.                     we include only these words in our modeling.
                                                                      192

Table 1: Model performance: mean accuracy per word pre-
dicted, standard deviation of prediction, and performance            ation of the linguistic model and an iteration of the 1/n ran-
compared to 1/n random.                                              dom model. We find that only 31.55% (or 65 snapshots) are
                   word prob.      s.d. % better   % worse           statistically better fit by the semantic network than by the 1/n
    semantic           .178       .007   31.55       64.56           random model. In fact the 1/n random model performs statis-
    phonological       .192       .006   43.68       51.45           tically better in 64.56% (or for 133 snapshots). See Table 1
    1/n rand           .191       .008   58.00       37.62
    sem rand           .191       .013   64.56       31.55           for results. When we perform the same comparison on the
    phon rand          .191       .021   54.36       40.77           semantic network and the r-graph baseline model, we find
                                                                     the exact same results, with each snapshot either beating both
Model Comparison                                                     random models or none. Over all of our analyses, we find that
                                                                     there is no meaningful difference between the comparison of
The main point of this paper is to test which representations
                                                                     the linguistic network to the random graph that is not cap-
are useful in predicting words to be learned, and how this
                                                                     tured by considering only the 1/n bag-of-words model. Thus
may change with development. We mentioned briefly base-
                                                                     we talk only about the 1/n random model with the knowl-
line models we use for comparison. In this section we cover
                                                                     edge that the results also extend to the random graph (r-graph)
them in more detail. In our 1/n or bag-of-words model each
                                                                     model.
unknown word has an equal probability of being learned. A
bag-of-words, just learn anything model, has been shown to              Though it seems that the random model is outperforming
produce early lexical graphs with structure similar to that of       the semantic network, it could be that there is some system-
the lexical graphs of children (Beckage et al., 2011) and thus       aticity in the 31% of cases where the semantic network ac-
provides an interesting baseline model. The other random             tually outperforms the random models. For example the se-
model, our r-graph model, is based on network structure but          mantic network might outperform the random model for vo-
instead of a principled edge-list, the same number of edges          cabulary snapshots of young/older children or for vocabulary
are randomly drawn. This measures any effect that the net-           snapshots created to capture high/low language ability. To
work structure might have in isolation from the linguistic in-       explore this possibility, we cluster the results with respect to
formation present in the semantic and phonological graphs.           age, CDI percentile, and vocabulary size. Figure 1 shows the
Since we run many iterations and count the number of over-           same data as table 1 aggregated to capture possible trends in
lapping words, if we generate graphs entirely at random, our         development. The y-axis indicates the change from random,
r-graph model will approach our 1/n random model for large           with the scale varying across graphs. The x-axis aggregates
numbers of simulations. Thus we fix the random graph rep-            the data by relevant developmental features and varies across
resentation for 100 runs before drawing a new random graph.          plots. We consider effects of age, percentile, vocabulary size.
                                                                     The results for the semantic network is shown in dark grey. If
                            Results                                  performance was equal to random, this would be indicated as
                                                                     a 0 on the graph. If the performance was better than random,
Once we calculated the number of correctly predicted words
                                                                     the line would be above the 0 mark. While we do not show
for each model, we consider trends in the data. We can look at
                                                                     the results here, we also consider the number of words the
multiple levels of data analysis to shed light on different un-
                                                                     model is predicting in case there is an effect of the size of the
derlying mechanisms. The first question we ask is whether or
                                                                     prediction set.
not the collective fits across all snapshots are better than our
random baseline models. To do this, we have to normalize the            If the semantic network model captured learning of a sub-
network runs across snapshots since each snapshot captures           set of developmentally interesting and research motivated
a different number of words to be learned. Thus, we con-             snapshots, we should see this as a systematic increase of the
sider the probability that the model correctly predicts a word.      network performance over random. Instead, we find that re-
This allows us to average each snapshot equally and to com-          gardless of age, percentile, vocabulary size or the size of the
pare performance across models. The results, summarized in           prediction set, the semantic network is not performing bet-
Table 1 suggest that the probability of correctly predicting a       ter than the random models. In fact there is evidence of a
word for learning is nearly equal across models. The seman-          trend that, for higher CDI percentiles, the semantic model de-
tic network seems to be performing slightly worse than the           creases in performance and is actually worse than random. A
other models. We include the average standard deviation of           similar trend is seen for the vocabulary size as well, where
the probability of correctly choosing a word to suggest that         aside from vocabulary sizes around 450, there is a steady de-
the model converged and that there is not much variability           crease in performance of the semantic model as compared to
across runs.                                                         random for larger vocabularies.
                                                                        However, this is averages of averages, which could mask
Semantic Network Results                                             trends due to poor performance of the semantic network on
Averaging across all vocabulary snapshots, and comparing             a subset of snapshots. When we consider the proportion of
each model to the random models further suggests that the            snapshots that are better fit by the sensory-motor features than
semantic network does not outperform random. For each of             any random network, we find that, regardless of the dimen-
the snapshots, we utilize an unpaired t-test between each iter-      sion used for clustering, there is no reliable trends in the data.
                                                                 193

                                      Performance by AGE                                      Performance by PERCENTILE                                                  Performance by VOC size
                                                                                      0.01
                    0.02                                                                                                                                   0.02
diff. from random                                                 diff. from random
                                                                                      0.00
                                                                                                                                       diff. from random
                                                                                                                                                           0.00
                    0.00
                                                                                      -0.01
                                                                                                                                                           -0.02
                    -0.02
                                                                                      -0.02
                                                                                                                                                           -0.04
                                                                                              howell
                                                                                              phono
                    -0.04                                                                     random
                                                                                      -0.03                                                                -0.06
                            16   18    20    22    24   26   28                               20       40           60      80                                     0   100   200    300       400   500   600
                                             AGE                                                       PERCENTILE                                                                  VOC size
 Figure 1: Performance on snapshots compared to random aggregated by either age, percentile or vocabulary size. Dark grey
 indicates semantics and light grey indicates phonology.
 In all cases, there are some snapshots that are best fit by the                                                  words model. However, we find that the phonological net-
 semantic network but there seems to be no systematicity to                                                       work shows some systematic increase in performance over
 which snapshots they are.                                                                                        the random network and bag-of-words model. This is an in-
                                                                                                                  teresting result as it suggests that phonological features con-
 Phonological Network Results                                                                                     tain useful information in understanding how language may
 When the phonological network of overlapping phonemes is                                                         be learned. Further, we can see that this type of phonolog-
 considered, 43% of the snapshots are significantly better fit                                                    ical information and this process of growth are best able to
 by phonology than by random, and 51% are significantly                                                           capture acquisition for snapshots created for older children,
 worse fit. This suggests that a phonological representation                                                      children with larger vocabularies and normative vocabulary
 performs worse than random acquisition on average. How-                                                          snapshots that are constructed to mirror children with high
 ever, when we look at the trends across age, vocabulary size                                                     language ability.
 and language ability, we find that certain types of snapshots                                                       We interpret the conditional success of the phonological
 are reliably better fit by the phonological network than by                                                      network in the context of the failure of the semantic net-
 the random models. Figure 1 shows the performance of the                                                         work and revisit our initial assumptions as laid out in the
 phonological model (light grey) again aggregated over dif-                                                       first paragraph of the methods section in light of these results.
 ferent features of interest. We can see in the first frame that                                                  We assumed that the phonological representation of overlap-
 when we consider age, snapshots generated from norms for                                                         ping phonemes and the semantic representation of sensory
 children between 20 and 25 months are reliably better fit by                                                     motor features provided a useful initial network representa-
 the phonological preferential growth model than by the ran-                                                      tion. This is an assumption that could easily be expanded
 dom models. Similarly, there is a general trend of an increase                                                   upon or directly challenged. However, even with the limited
 in performance over random as the percentile of the snap-                                                        choice of representations, we showed two important things.
 shot increases. We also see a large increase in performance                                                      First, this result shows how the representation chosen influ-
 of the phonological model for vocabulary sizes between 200                                                       ences the ability of the model to capture vocabulary growth.
 and 550 words. Further, when we consider the proportion                                                          Further, this modeling approach suggests a way to compare
 of vocabularies that are better than the random model, similar                                                   network representations by holding the process of network
 trends emerge–we don’t see just an increase in probability but                                                   growth constant. This type of model comparison may tell us
 also an increase in the number of snapshots that are better fit                                                  about structures useful to young learners as well as about how
 by phonology than random. It is important to point out that,                                                     language itself might be structured from the perspective of a
 in general, vocabularies that are larger are also more likely                                                    young children.
 to be from normative vocabularies constructed from norms of                                                         Our assumption of a preferential growth process of acqui-
 older children and are, with our assumptions, also represen-                                                     sition was not directly explored in this paper. In future work
 tative of higher language ability. So in some sense it is not                                                    we do hope to explore this model as compared to other types
 surprising that the effect of phonology seems to increase in                                                     of network growth models to understand both the nature and
 performance in all three cases. However, the redundancy also                                                     variability of learning. But the fact that the phonological net-
 confirms that the effect is not due to random noise but is a                                                     work representation and the preferential growth model were
 property of the vocabularies. In the next section we discuss                                                     able to outperform random for a certain class of snapshots
 these findings to understand the significance of the results.                                                    suggests that this model is able to capture aspects of the pro-
                                                                                                                  cess of acquisition, if somewhat imperfectly.
                                 Discussion and future directions                                                    Our final assumption that the normative vocabulary snap-
 The results suggest that in most cases the semantic network                                                      shots captures individual behavior is the most informative.
 based on the Howell sensory-motor features is not able to out-                                                   The vocabulary snapshots do provide a way in which the vo-
 perform the random semantic network model or the bag-of-                                                         cabulary of a child may change over time but this does not
                                                                                                            194

capture the vocabulary of any individual child directly. It is                                References
a big assumption that words that are reported as learned by          Barabási, A.-L., & Albert, R. (1999). Emergence of scaling
the fewest children are also the words that early talkers learn,       in random networks. Science, 286(5439), 509–512.
for example. This assumes that word learning proceeds in a           Beckage, N. M., & Colunga, E. (2013). Using the words
systematic and predictable fashion–that late talkers are just          toddlers know now to predict the words they will learn next.
typical talkers who are older– a result that has been shown            Proc. of the 35th Conf of the Cog. Sci. Society, 163-168.
to be untrue (Thal et al., 1999; Beckage et al., 2011). The          Beckage, N. M., Smith, L. B., & Hills, T. T. (2011). Small
normative vocabularies may not capture any individual child            worlds and semantic network growth in typical and late
very well, but capture the aggregate instead. This could be            talkers. PloS one, 6(5), e19348.
particularly problematic in the domain of semantics since the        Bloom, P. (2002). How children learn the meanings of words.
averaging of multiple vocabularies and the further assumption        Dale, P. S., & Fenson, L. (1996). Lexical development norms
of creating snapshots to indicate different language ability           for young children. Behavior Research Methods, Instru-
would likely cancel out any sort of semantic consistency that          ments, & Computers, 28(1), 125–127.
wasn’t shared across the majority of children. For example,          DeLoache, J. S., Simcock, G., & Macari, S. (2007). Planes,
the vocabulary snapshots would not necessarily show prefer-            trains, automobiles–and tea sets: Extremely intense inter-
ences for animals or vehicles unless a large amount of chil-           ests in very young children. Developmental Psychology,
dren in the norming study showed such preferences at similar           43(6), 1576-1586.
times and with base rates roughly equal to their peers.              Gentner, D. (1982). Why nouns are learned before verbs: lin-
   The aggregation process may also explain why the phono-             guistic relativity versus natural partitioning. In Language
logical model was especially useful for larger vocabularies            development: Language cognition and culture.
and normative vocabularies created for older children. The           Grunwell, P. (1981). The development of phonology: A de-
CDI is a measure of productive vocabulary, meaning to check            scriptive profile. First Language(2), 161–191.
as word as ”known” the parent needs to recognize the word            Hills, T., Maouene, J., Riordan, B., & Smith, L. B. (2010).
the child is producing. It has been established that there is          The Associative Structure of Language: Contextual Diver-
much regularity in the order in which phonemes are mastered            sity in Early Word Learning. Journal of memory and lan-
in development (e.g., Sander, 1972; Grunwell, 1981). Thus              guage, 63(3), 259–273.
the vocabulary snapshots are likely able to capture general          Hills, T., Maouene, M., Maouene, J., Sheya, A., & Smith,
properties of the difficulty of production. In the case of large       L. (2009b). Longitudinal analysis of early semantic net-
vocabularies or high language ability, the phonological net-           works: preferential attachment or preferential acquisition?
work does the best. This could be due to the fact that the             Psychological science, 20(6), 729–39.
vocabulary contains a large set of phonemes which allows the         Howell, S. R., Jankowicz, D., & Becker, S. (2005). A
model to 1) distribute probability of learning over a greater          model of grounded language acquisition: Sensorimotor
set of words and 2) to implicitly model the difficulty in pro-         features improve lexical and grammatical learning. Jour-
duction of phonemes that may play a role in learning. This             nal of Memory and Language, 53(2), 258–276.
feature of phonology is likely to be more systematic across          Sander, E. K. (1972). When are speech sounds learned? Jour-
children than semantic similarity, especially with the aggres-         nal of Speech and Hearing Disorders, 37(1), 55–63.
sion procedure used to construct normative vocabularies.             Steyvers, M., & Tenenbaum, J. B. (2005). The large-scale
   The results, as they stand now, are intriguing in that we           structure of semantic networks: statistical analyses and a
have gained direct information as to the performance of the            model of semantic growth. Cognitive science, 29(1), 41–
preferential growth model within the context of language ac-           78.
quisition. We test the model and representations the model           Storkel, H. L. (2009). Developmental differences in the
uses and conclude that given this model and these data,                effects of phonological, lexical and semantic variables on
phonology outperforms the random models more often than                word learning by infants. Journal of Child Language,
semantics does. This confirms that phonological information            36(02), 291–321.
may play a role in the learning of new words or it could sug-        Thal, D. J., O’Hanlon, L., Clemmons, M., & Fralin, L.
gest instead that, in averaging, the systematic nature of se-          (1999). Vaidity of a parent report measure of vocabu-
mantics is washed out while the phonological aspects are ac-           lary and syntax for preschool children with language im-
centuated.                                                             pairment. Journal of Speech, Language, and Hearing Re-
                                                                       search, 42(2), 482–496.
                    Acknowledgments                                  Weizman, Z. O., & Snow, C. E. (2001). Lexical output as
                                                                       related to children’s vocabulary acquisition: Effects of so-
This work is an extension of Ariel Aguilar’s Computer Sci-             phisticated exposure and support for meaning. Develop-
ence senior thesis and was funded through an award from                mental Psychology, 37, 265-279.
the John Merck Scholars Fund and by NICHD grant R01
HD067315 to Eliana Colunga. The first author was funded
in part through the NSF GRFP.
                                                                 195

