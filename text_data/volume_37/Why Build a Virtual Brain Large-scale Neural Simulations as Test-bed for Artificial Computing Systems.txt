                                                 Why Build a Virtual Brain?
       Large-scale Neural Simulations as Test-bed for Artificial Computing Systems
                                              Matteo Colombo (M.Colombo@UvT.nl)
                       Tilburg Center for Logic, General Ethics and Philosophy of Science, Tilburg University
                                            P.O. Box 90153, 5000 LE Tilburg, The Netherlands
                               Abstract                                system (i.e., their representational target) not in order to
                                                                       serve as surrogates that are investigated to gain new
   Despite the impressive amount of financial resources invested
   in carrying out large-scale brain simulations, it is                knowledge about the brain. Rather, these neural simulations
   controversial what the payoffs are of pursuing this project.        imitate some features of a real neural system in order to gain
   The present paper argues that in some cases, from designing,        useful knowledge about the simulating system itself.
   building, and running a large-scale neural simulation,                While claim (1) concerns the type of knowledge one may
   scientists acquire useful knowledge about the computational         want or hope to acquire with computer simulation, claim (2)
   performance of the simulating system, rather than about the         concerns one possible representational function of computer
   neurobiological system represented in the simulation. What
   this means, why it is not a trivial lesson, and how it advances
                                                                       simulation.
   the literature on the epistemology of computer simulation are
   the three preoccupations addressed by the paper.                        Large-scale Neural Simulations: Aims and
   Keywords: Large-scale neural simulations; epistemology of
                                                                                                Prospects
   computer simulation; target-directed modeling; neuromorphic         For many large-scale neural simulations, a simulating
   technologies                                                        system implements some algorithm that finds solutions to
                                                                       mathematical equations that are believed to describe the
                          Introduction                                 dynamics and pattern of connectivity of a large number
In the last twenty years or so, several research groups have           (e.g., over a million) of neurons and synapses (for reviews
been working on large-scale brain simulations. In the face of          Brette et al. 2007; de Garis et al. 2010; Goertzel et al. 2010;
the impressive amount of financial resources invested in               Eliasmith & Trujillo 2014).
such projects, it is controversial what the payoffs are of                 A large-scale neural simulation is a type of computer
carrying out large-scale brain simulations. The present paper          simulation. Computer simulation can be characterised
explores this issue, asking: Currently, what do scientists             broadly as “a comprehensive method for studying systems,”
learn from designing, building, and running large-scale                which “includes choosing a model; finding a way of
neural simulations? One plausible answer is that at least for          implementing that model in a form that can be run on a
some such simulations scientists learn about the                       computer; calculating the output of the algorithm; and
computational performance of the simulating system.                    visualizing and studying the resultant data” (Winsberg
  Plausible as it sounds, the significance of this answer              2013). Accordingly, some real-world system should be
should not be downplayed, for at least two reasons. First,             picked as the representational target of the computer
most work in the epistemology of computer simulation                   simulation; some mathematical equations should be chosen,
overlooks or downplays the computational and material                  which are believed to model (some aspect of) the behavior
aspects of computer simulation. But learning about the                 of the target system; and an appropriate simulating system,
computational performance of a machine is far from trivial.            consisting of both hardware and software components,
Second, the kinds of neural simulations examined in this               should be used to implement the mathematical model.
paper involve an interesting set of practices that have not                In line with much of the philosophical literature, where
been adequately discussed in the epistemology of modelling             models and simulations are understood as serving as
and computer simulation.                                               representations of some system about which one wants or
  In particular, these simulations have two kinds of targets:          hopes to gain knowledge (e.g., Humphreys 2004; Parker
one target is a real neural system, which is represented in            2009; Grüne-Yanoff & Weirich 2010; Weisberg 2013),
the simulation; the other target is the computing system               Winsberg (2013) claims that the entire process constituting
itself, which is not represented in the simulation but studied         computer simulation is “used to make inferences about the
both directly and through complicated inferences. If this is           target system that one tries to model.”
correct, then two interesting conclusions follow. (1) When                 The claim also coheres with the stated aims of many
scientific models and computer simulation are employed to              large-scale neural simulations. For example, the Blue Brain
gain new knowledge, it is not always knowledge about their             Project set out to “simulate brains of mammals with a high
represented target systems that is sought. For some neural             level of biological accuracy and, ultimately, to study the
simulations, the real neural system that one tries to represent        steps involved in the emergence of biological intelligence”
is not the system about which one wants to learn. (2) Some             (Markram 2006, 153). The objective of carrying out certain
neural simulations imitate some features of a real neural              large-scale neural simulations is to understand why and how
                                                                   429

many different ion channels, receptors, neurons, and                      Three dimensions on which computational performance
synaptic pathways in the brain contribute to different brain          can be assessed are: the time it takes for the computing
functions and to emergent, intelligent behavior (158). The            system to carry out a given task, the maximum number of
aim of Izhikevich & Edelman’s (2008) simulation of a                  tasks that can be completed by the system in a given time
million spiking thalamo-cortical neurons and half a billion           interval, and the electrical power it takes for the system to
synapses was analogous. They explained that “[o]ne way to             carry out a task.
deepen our understanding of how synaptic and neuronal                     The total time required for a computing system to
processes interact to produce the collective behavior of the          complete a task is called execution time. One way to
brain is to develop large-scale, anatomically detailed models         measure the execution time of a program is in terms of clock
of the mammalian brain” (3597). Similarly, the objective of           period, which is the time length (in nanoseconds) of a cycle
Eliasmith and colleagues’ (2012) 2.5 million neuron                   of the clock built into the system that determines when
simulation was to understand why and how the robust and               events take place in the hardware. The clock rate (in hertz)
rapid flexibility of biological systems can be generated from         is the inverse of the clock period. Increasing computational
a unified set of neural mechanisms.                                   performance for a given program requires decreasing its
    Despite significant differences, the aim shared by these          execution time, which may be tackled as an engineering
projects is to use large-scale neural simulations to                  problem—viz. as the problem of reducing the clock
understanding of how and why brains’ multi-scale, complex             period—or as a computational problem—viz. as the
organization generates different brain functions and                  problem of designing a more efficient computational
emergent cognitive phenomena. This aim may be reached.                architecture or more efficient algorithms and programs.
Yet, it is far from uncontroversial that, currently, a large-             The number of tasks that can be completed per unit time
scale neural simulation is a fruitful approach to addressing          by a computing system is called throughput. If we focus on
questions about why and how neurons and synapses’                     the communication channels of a computing system, then
dynamics generate different brain functions and cognitive             the maximum throughput of a channel is often called
phenomena (Mainen & Pouget 2014).                                     bandwidth (measured in bits of data/second). The amount of
    Commenting on this approach, Carandini (2012) argues              time it takes for a communication channel to become
that, currently, “putting all of the subcellular details (most of     unoccupied so that it can allow for data transfer is called
which we don’t even know) into a simulation of a vast                 latency. The available bandwidth of a communication
circuit is not likely to shed light on the underlying                 channel is a limited resource, and should be used sparingly.
computations” (509). If the underlying neural computations            The greater the bandwidth capacity, or the lower the latency
are not understood, there is little hope to learn how and why         of the communication channels, the more likely it is that the
neural circuits generate different brain functions and                system displays better computational performance. The
cognitive phenomena. In a similar vein, Sporns (2012)                 throughput, bandwidth, and latency of a computing system
points out that the success of projects like Markram’s Blue           are a complex function of the physical medium being used
Brain “depends on knowledge about the organization of                 for communications, the system’s wiring architecture and
neurons and molecules into complex networks whose                     the type of code used for programming.
function underpins system dynamics” (168). Such                           The microprocessors of computing systems dissipate
knowledge is currently sparse and not easily incorporable             heat. Heat must be removed from a computing system; else,
into large-scale neural simulations. So, it is doubtful that,         its hardware components will overheat. Conserving power
currently, carrying out large-scale neural simulations is a           and avoiding overheating, while improving computational
fruitful approach to learn about the neurobiological systems          performance, have led computer scientists and engineers to
represented in the simulation.                                        explore novel architectures, hardware technologies, software
                                                                      solutions and programming languages for highly-efficient
     Brains and Computational Performance                             computing systems.
                                                                          There are two reasons why carrying out a computer
More plausible is that, currently, from at least some large-
                                                                      simulation of a large number of neurons and synapses can
scale neural simulations, scientists gain knowledge about
                                                                      yield non-trivial knowledge of the computational
the computational performance of the simulating system
                                                                      performance of the simulating system. First reason: brains
itself, rather than about the neural system that the simulation
                                                                      can be understood as computational systems, which can be
represents.
                                                                      used to set a real biological benchmark for artificial
    Simulating systems are computing systems comprising
                                                                      computing systems’ performance. Second reason:
both software and hardware components. They include a
                                                                      scalability, which indicates how efficient an application is
computational architecture and a set of algorithms
                                                                      when using increasing numbers of parallel processing units
formulated as computer programs that can be executed on
                                                                      or amount of computational resources.
concrete computing machines. The computational
                                                                          If the brain is a computing system, then it displays high
performance of the simulating system depends on a complex
                                                                      performance in the face of low power consumption and
combination of properties of its architecture, of the
                                                                      small size. On average, the human brain weighs around
algorithms it uses, the programs it executes, and of the
                                                                      1.3 to 1.5 Kg, is constituted by about 100 billion neurons
materials and technological devices of which it is made.
                                                                  430

and around 100 trillion synapses, and its volume is about         learn about trade-offs between memory, computation, and
1,400 ml. For carrying out its computations, it consumes          communication in a certain computational architecture.
energy at a rate of about 20 watts. Brains’ computational
architecture and style of computing are very different from        Brains, simulations, and neuromorphic devices
those of modern artificial computing systems. Modern
                                                                  Learning about the computational performance of a
artificial computing systems possess von Neumann
                                                                  computing system can be important for developing
architecture and have stored programs, which are typically
                                                                  neuromorphic technologies. Neuromorphic technologies are
implemented in digital, serial, synchronous, centralized and
                                                                  devices for information processing and data analysis that
fast microcircuits. By contrast, biological brains possess a
                                                                  aim to approximate the computational architecture and style
non-von Neumann, multiscale, network architecture; they
                                                                  of computing of biological brains. Such technologies
have distributed computational units, which carry out
                                                                  include vision systems, auditory processors, multi-sensor
mixed-mode analog-digital, parallel, asynchronous, slow,
                                                                  integrators, autonomous robots, and tools for handling and
noisy, computations (Montague 2007; Piccinini & Bahar
                                                                  analysing large amount of data (Indiveri & Horiuchi 2011).
2013).
                                                                      SyNAPSE (Systems of Neuromorphic Adaptive Plastic
    Available information about general computational
                                                                  Scalable Electronics) is an on-going research program
features of biological brains can provide one basis for
                                                                  funded by the U.S. Defense Advanced Research Projects
benchmarking the performance of artificial computational
                                                                  Agency (DARPA). “The vision for the SyNAPSE program
systems along some dimension of interest like power
                                                                  is to develop electronic neuromorphic machine technology
consumption or scalability. Comparing the computational
                                                                  that scales to biological levels” (DARPA BAA08-28). This
performance of the simulating system in a large-scale neural
                                                                  research program aims to develop electronic technology
simulation to that of its neurobiological target along some
                                                                  with similar computational performance to the mammalian
dimension of interest allows scientists to learn about why
                                                                  brain in terms of size, speed, and energy consumption.
and how certain features of the simulating system (e.g., its
                                                                      Under the SyNAPSE program, Preissl and colleagues
network architecture, its materials) impact its performance
                                                                  (2012) carried out a computer simulation of a very large
relative to that dimension.
                                                                  neural circuit with the ultimate goal of exploring how
    What about scalability? Although it is problematic to
                                                                  closely one can “approximate the function, power, volume
precisely define ‘scalability,’ the term is generally used in
                                                                  and real-time performance of the brain within the limits of
computer science to denote the capacity of a multiprocessor
                                                                  modern technology” (10). The representational target
parallel computing system to accommodate a growing
                                                                  system of their simulation was a network comprising 65
number of processing units or to carry out a growing
                                                                  billion neurons and 16 trillion synapses, which imitated the
volume of work gracefully (Hill 1990). Scalability is a
                                                                  largest known wiring diagram in the macaque monkey’s
desirable feature of a computing system because it allows
                                                                  brain. This biological target was modelled as a network of
for hardware or software components to be added in the
                                                                  neurosynaptic cores containing digital integrate-leak-and-
system without outgrowing it. Two more specific notions,
                                                                  fire neurons.
helpful to assess the performance of a large-scale
                                                                      The simulating system involved a 16-rack Blue Gene/Q
simulation, are those of strong scaling and weak scaling,
                                                                  supercomputer of 16,384 to 262,144 CPUs and 256 TB of
which denote respectively the capacity of a system to reduce
                                                                  main memory, and Compass, a multi-threaded, massively-
execution time for solving a fixed-size problem by adding
                                                                  parallel software, which enabled the simulation of billions
processors, and the capacity to keep execution time constant
                                                                  of neurosynaptic cores operating in a parallel, distributed,
by adding processors so as to accommodate additional
                                                                  and semi-synchronous fashion.
workload. Assessing strong scaling is particularly relevant
                                                                      The modelling choices of Preissl and colleagues were
to learning about why some program takes a long time to
                                                                  congenial to the pursuit of an engineering goal. The
run (something that is CPU-bound). Assessing weak scaling
                                                                  neurons, synapses, and axons in their simulation were
is particularly relevant to learning why some program takes
                                                                  modelled as event-driven (asynchronous), digital, integrate-
a lot of memory to run (something that is memory-bound).
                                                                  leak-and-fire circuits. The leaky integrate-and-fire model is
    Lack of scalability in large-scale neural simulation can
                                                                  one of the simplest models of spiking neurons. Given its
indicate that the architecture of the simulating system
                                                                  lack of biophysical detail, the range of phenomena that this
cannot effectively solve problems of a certain size that
                                                                  model can address is limited. Nonetheless, the model is
biological brains can solve quickly. It can indicate that
                                                                  analytically solvable and relatively easy to implement in a
adding more simulated neurons and synapses to the
                                                                  computer simulation. For many integrate-and-fire neurons
simulating system is not an efficient strategy to execute a
                                                                  models, the model fits nicely with an event-driven
certain program more quickly, as the communication costs
                                                                  simulation, whereby all operations in the simulation are
would increase as a function of the number of processors
                                                                  driven by neural spike events, which is generally well suited
added to the system. It can also indicate that the power
                                                                  to decrease computational time and minimize memory load.
consumption required by a system that grows larger is too
                                                                  The inter-core pattern of connections embodied in Compass
costly. So, by taxing an artificial computing system by
                                                                  imitated the macaque’s neural wiring. The relationship
simulating millions of neurons and synapses, scientists can
                                                                  between the model-network and its neurobiological target
                                                              431

was not isomorphic; it was a similarity relation, which is         simulations are used to acquire new knowledge, it is
generally sufficient to allow scientists to learn from             knowledge about their represented targets that is ultimately
computer simulation, especially when, like in this case,           sought (Weisberg 2013, Ch. 5). Second, computer
some relevant aspects and degrees of similarity are specified      simulations imitate some features of their represented target
based on the question at hand, available background                just to serve as surrogates that are investigated to gain new
knowledge and the larger scientific context (Teller, 2001;         knowledge about it (Swoyer 1991). That is, the
Giere, 2004; Weisberg, 2013).                                      representational relation that holds between computer
    Implementing the macaque’s wiring diagram                      simulations and their represented targets allow scientists to
“challenges the communication and computational                    perform inferences just from the simulation to its
capabilities of Compass in a manner consistent with                represented target.
supporting brain-like networks” (11). The performance of               These two claims should be rectified in the light of
the simulating system could then be compared with that of          computer simulations like Preissl and colleagues’. For some
the real neurobiological system represented in the computer        large-scale neural simulations, computer simulations have
simulation. A quantitative characterization of the deviations      two kinds of targets about which one may want to gain new
between the real neural system and the simulating system           knowledge. One kind of target is a real neural system, which
allowed scientists to identify which features of architectural     is represented in the simulation; the other kind of target is
and communication-design contributed to computational              the computing system itself, which is not represented in the
efficiency.                                                        simulation, but studied either directly, or through
    Preissl and colleagues’ computer simulation could be           complicated inferences. Depending on the goal of the
used as a test-bed for learning about the performance of           scientists designing and running the computer simulation,
hardware and software components of a simulating system            these inferences may or may not be based on the assumption
put under serious computational stress. Simulating a neural        that the simulating system bears some representational
network at that scale poses major challenges for                   relation with its neural target.
computation, memory, and communication, even with                     Generally, computer simulations can instruct scientists
current supercomputers. If we consider N neurons, whose            about some aspect of reality even if it is not assumed that
average firing rate is H, and whose average number of              the mathematical model implemented in the simulation has
synapses is S, and we take account of all spike                    counterparts in the world about which scientists want or
transmissions, then a real-time simulation of 1 second of          hope to learn. In these cases, the aspects of reality about
biological time should process N x H x S spike                     which scientists hope to gain novel information are some of
transmissions. This minimal number of operations set a             the computational features of the simulating system, rather
benchmark to assess the computational performance of a             than some of the features of the real system represented in
neural simulation (Brette et al. 2007, 350-1).                     the computer simulation. Assuming that the simulating
    Preissl et al.’s (2012) simulation yielded two main            system bears some representational relation with a
results. First, as the average spiking rate of neurons was 8.1     neurobiological target is not necessary to gain this
Hz, the simulation was 388x slower than real time. Second,         information. In fact, benchmarking software exists that can
simulating the pattern of structural connectivity of the           be used to assess the relative performance of artificial
macaque’s brain, the simulating system displayed near-             computing systems’ hardware or programs.
perfect weak and strong scaling. While acquiring this type            However, assuming that a simulating system does bear
of information does not obviously yield novel insight about        some representational relation with its neural target allows
phenomena produced by biological brains, it is relevant to         scientists to study performance discrepancies between the
the development of more efficient artificial computing             simulating system and the neurobiological system, which
systems. As Preissl and colleagues put it: “Compass is a           can function as useful benchmark along some dimension of
harbinger of an emerging use of today’s modern                     interest. By characterising such discrepancies, constraints on
supercomputers for midwifing the next generation of                computational efficiency can then be identified, which is
application-specific processors that are increasingly              particularly useful when the goal is to acquire knowledge
proliferating to satisfy a world that is hungering for             useful for designing neuromorphic innovations.
increased performance and lower power while facing the                The claim that computer simulation can instruct scientists
projected end of CMOS scaling and increasing obstacles in          about kinds of target systems that are different from those
pushing clock rates ever higher” (11).                             represented in the simulation resonates with Humphreys’
                                                                   (2009), Parker’s (2009), and Winsberg’s (2010) emphasis
                                                                   on the specifically computational and material features of
  Representing and Learning with Large-scale                       computer simulations. Commenting on the philosophical
                     Neural Simulations                            novelty of computational science, writes Humphreys: a
Two claims are widely shared in the literature about the           “novel feature of computational science is that it forces us to
epistemology of computer simulation and scientific                 make a distinction between what is applicable in practice
modelling (Frigg & Hartmann 2012). First, in target-               and what is applicable only in principle… Ignoring
directed modelling, when scientific models and computer            implementation constraints can lead to inadvisable remarks
                                                               432

[e.g. about the epistemology of computer simulations]”               dynamics of a large-scale neural network, scientists need not
(2009, 623).                                                         draw any inference about the neural system represented in
   Learning about a simulating system’s computational                the simulation. Second, assuming that the simulating system
performance is one way to learn about “what is applicable in         does bear some representational relation with a set of
practice and what is applicable only in principle” with              computational properties instantiated by some biological
respect to the engineering of novel computing technologies.          neural network allows scientists to characterise the
If some computer simulations are intended to yield new               performance discrepancies between neurobiological
knowledge only about the computing system used in the                network and artificial simulating system.                  The
simulation, then scientific models and simulations need not          characterisation of this discrepancy can be valuable for
be vehicles to learn about their represented targets.                some scientific or engineering aim.
   Sometimes, scientists do not translate the results of a              The brain is a kind of computing machine. If the brain is a
computer simulation into knowledge about the represented             computing machine, then there is a set of properties
target. Since these simulating systems are computing                 possessed by both biological brains and artificial computing
systems, they instantiate a set of computational, measurable         systems such that specific instantiations of these properties
properties. Running a large-scale neural simulation can              determine the computational performance that the
yield measurements of these properties, which provide                computing machine—biological or otherwise—can reach.
information about the computational performance of the               From available information, biological brains instantiate
system, given some benchmark. Knowing about the                      determinate properties such that the computational
computational performance of the system along some                   performance they can reach is significantly higher than the
dimension of interest can ground the practical design of             performance of the best current artificial super-computers. If
neuromorphic computing devices.                                      these properties are known, and if some information is
   Examining the relationship between computer simulations           available about how they determine the performance of
and traditional experiments, Parker (2009) stresses “the             biological brains, then scientists may justifiably assume that
importance of… understanding computer experiments as,                in some large-scale neural simulation the simulating system
first and foremost, experiments on real material systems.            imitates some features of the brain relevant to instantiate
The experimental system in a computer experiment is the              those computational properties.
programmed digital computer—a physical system made of                   Unlike scale models such the scale model of a bridge or
wire, plastic, etc… In a computer simulation study,                  of a car, which are typically down-sized or enlarged copies
scientists learn first and foremost about the behavior of the        of their target systems, Preissl et al.’s (2012) large-scale
programmed computer” (488-9).                                        neural simulation imitated some features of the brain not in
   Learning about the behavior of a programmed computer is           order to serve as a surrogate that is investigated to draw
far from being trivial or unimportant, as Preissl and                conclusions on the represented neurobiological target.
colleagues’ (2012) work illustrates. Compass incorporated            Rather, the assumed representational relation between the
“several innovations in communication, computation, and              simulation and the biological brain justified scientists to
memory” based on available knowledge of some aspects of              draw inferences about how closely the function, power,
the function, power and volume of organic brains (10).               volume and real-time performance of the brain can be
Compass was found to have near-perfect weak and strong               approximated within the limits of current technology. The
scaling when a model was run of the neural dynamics of a             neural scale and pattern of connectivity embodied in
large circuit of the macaque’s brain. By themselves, these           Compass challenged its communication, memory and
types of results do not yield novel information about some           computational capabilities. In the face of these challenges,
set of computational properties instantiated by biological           the simulating system performance could be compared to
brains; and, given the aims of Preissl et al.’ simulation, they      that of a biological brain along some dimensions of interest
were not translated into knowledge about the represented             like neural spiking rates, latency and bandwidth. For
target system. Instead, the specific importance of these             example, running on the IBM Blue Gene/Q supercomputer,
results lies in their offering the basis for developing a novel,     Compass was found to be 388x slower than real-time
efficient, computational architecture that can support a host        performance of the brain, which is useful to characterise its
of neuromorphic applications (Modha et al. 2011).                    computational performance.
   Having stressed the importance of recognizing that “in a             So, in some cases, large-scale neural simulations imitate
computer simulation study scientists learn first and foremost        the brain not in order to serve as a surrogate investigated in
about the behavior of the programmed computer,” Parker               its stead. The brain is imitated because it offers a biological
(2009) claims that: “from that behavior, taking various              benchmark against which the simulating system’s design
features of it to represent features of some target system,          and performance can be assessed. Information about how
they hope to infer something of interest about the target            certain properties determine the computational performance
system” (489). This widely-held claim should be qualified            of biological brains can then be used not only to try and
in two ways, however.                                                instantiate those properties in the design of artificial
   First, Preissl and colleagues’ (2012) study shows that,           systems, but also to characterise the discrepancy between
from the behavior of a computing system that simulates the           the brain’s and the simulating system’s performance. This
                                                                 433

characterisation might provide insight into what types of            Goertzel, B., Ruiting, L., Itamar, A., de Garis, H., Chen S.
constraints and what determinate properties an artificial              (2010). A world survey of artificial brain projects, Part ii.
computing system need to instantiate for carrying out some             Biologically       inspired      cognitive     architectures.
task of interest more efficiently.                                     Neurocomputing, 74, 30-49.
                                                                     Grüne-Yanoff, T. & Weirich, P. (2010). Philosophy of
                        Conclusions                                    Simulation. Simulation and Gaming: An Interdisciplinary
For some large-scale neural simulation, what is learned                Journal, 41(1), 1-31.
concerns the computational performance of the simulating             Hill, M.D. (1990). What is scalability? ACM SIGARCH
system itself. Learning about the computational                        Computer Architecture News, 18(4):18-21.
performance of a computing machine is far from trivial, and          Humphreys, P. (2004). Extending ourselves: Computational
can afford knowledge useful for several engineering                    science, empiricism, and scientific method, New York:
purposes. Once this role is recognized of some large-scale             Oxford University Press.
neural simulations, some widely held beliefs about the               Indiveri G, Horiuchi TK (2011). Frontiers in neuromorphic
epistemology of computer simulations and modelling are in              engineering.      Frontiers      in      Neuroscience,      5,
need of qualification. First, computer simulation can involve          10.3389/fnins.2011.00118.
more than one kind of target system, about which one wants           Mainen, Z. F., & Pouget, A. (2014). European Commission:
or hopes to acquire new knowledge. Second, when scientific             Put brain project back on course. Nature, 511, 534.
models and computer simulations are employed to gain new             Markram, H. (2006). The blue brain project. Nature Reviews
knowledge, it is not always knowledge about their                      Neuroscience, 7, 153-160.
represented target systems that is sought. Third, assuming           Modha, D.S., R. Ananthanarayanan, S.K. Esser, A.
that some large-scale neural simulations imitate some                  Ndirango, A.J. Sherbondy, R. Singh (2011). Cognitive
features of their target neurobiological system allows                 computing. Communications of the ACM, 54 (8), 62-71.
scientists to characterize the performance discrepancies             Montague, P.R. (2007). Your Brain is Almost Perfect. New
between biological brains and artificial computers, which              York: Plume.
may help identify constraints on computational efficiency            Parker, W. (2009). Does Matter Really Matter? Computer
for the design of neuromorphic technologies.                           Simulations, Experiments and Materiality, Synthese,
                                                                       169(3), 483–96.
                                                                     Piccinini, G., & Bahar, S. (2013). Neural Computation and
                    Acknowledgments                                    the Computational Theory of Cognition. Cognitive
Thank you to Lasha Abzianidze, Luigi Acerbi, Julia Bursten             Science, 34, 453-488.
and Liz Irvine for their comments on previous drafts. The            Preissl R, et al. (2012). Compass: A scalable simulator for
work on this project was supported by the Deutsche                     an architecture for cognitive computing. Proceedings of
Forschungsgemeinschaft (DFG) as part of the priority                   the International Conference for High Performance
program “New Frameworks of Rationality” ([SPP 1516])                   Computing, Networking, Storage, and Analysis (SC
                                                                       2012), ed Hollingsworth JK (IEEE Computer Society
                        References                                     Press Los Alamitos, CA), 1–11.
Brette R., et al. (2007). Simulation of networks of spiking          Sporns, O. (2012). Discovering the Human Connectome.
  neurons: a review of tools and strategies. Journal of                MIT Press, Cambridge, MA.
  Computational Neuroscience, 23, 349‐398.                           Swoyer, C. (1991). Structural Representation and
Carandini, M. (2012). From circuits to behavior: a bridge              Surrogative Reasoning. Synthese 87: 449–508.
  too far? Nature Neuroscience, 15, 507-509.                         Teller, P. (2001). Twilight of the Perfect Model. Erkenntnis,
de Garis, H., Shuo, C., Goertzel, B., & Ruiting, L. (2010). A          55, 393–415.
  world survey of artificial brain projects, Part I: Large-          Weisberg, M. (2013). Simulation and similarity: using
  scale brain simulations. Neurocomputing, 74, 3-29.                   models to understand the world. Oxford University Press.
Modha, D.S., et al. (2011). Cognitive Computing.                     Winsberg, E. (2013). Computer Simulations in Science, The
  Communications of the ACM, 54(8), 62-71.                             Stanford Encyclopedia of Philosophy, EN Zalta (ed.),
Eliasmith, C. & Trujillo, O. (2014). The use and abuse of              URL=http://plato.stanford.edu/archives/sum2013/entries/simulat
                                                                       ions-science.
  large-scale brain models. Current Opinion in
                                                                     Winsberg, E. (2010). Science in the age of computer
  Neurobiology, 25, 1-6.
                                                                       simulations. Chicago: University of Chicago Press.
Eliasmith, C., et al. (2012). A Large-Scale Model of the
  Functioning Brain. Science, 338, 1202-1205.
Frigg, R. & Hartmann, S. (2012). Models in Science The
  Stanford Encyclopedia of Philosophy, EN Zalta (ed),
  URL=<http://plato.stanford.edu/archives/fall2012/entries/model
  s-science/>.
Giere, R. (2004). How Models Are Used to Represent
  Reality. Philosophy of Science, 71, S742-752.
                                                                 434

