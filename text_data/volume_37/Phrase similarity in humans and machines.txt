                           Phrase similarity in humans and machines
                 Samuel J. Gershman (sjgershm@mit.edu) & Joshua B. Tenenbaum (jbt@mit.edu)
                     Department of Brain and Cognitive Sciences, MIT, Cambridge, MA 02139, USA
                           Abstract                              focus is on challenging distributional approaches, where
   Computational models of semantics have emerged as             a key open problem is how to capture compositionality:
   powerful tools for natural language processing. Recent        expressing the meaning of a phrase or sentence in terms
   work has developed models to handle compositionality,         of the meanings of the words that compose it.
   but these models have typically been evaluated on large,
   uncontrolled corpora. In this paper, we constructed              Distributional semantic models are commonly imple-
   a controlled set of phrase pairs and collected phrase         mented by representing linguistic units as vectors in a
   similarity judgments, revealing novel insights into hu-       high-dimensional space, where spatial proximity encodes
   man semantic representation. None of the computa-
   tional models that we considered were able to capture         semantic relatedness (Turney et al., 2010). If the dimen-
   the pattern of human judgments. The results of a sec-         sions of the space are related to features of the linguistic
   ond experiment, using the same stimuli with a trans-          context, then distributional structure will be recapitu-
   formational judgment task, support a transformational
   account of similarity, according to which the similarity      lated in the spatial structure. A classic example is Latent
   between phrases is inversely related to the number of ed-     Semantic Analysis (Landauer & Dumais, 1997), which
   its required to transform one mental model into another.      derives a low-dimensional vector representation from a
   Taken together, our results indicate that popular mod-
   els of compositional semantics do not capture important       singular value decomposition of the word-document co-
   facets of human semantic representation.                      occurrence matrix. In recent years, vector space mod-
   Keywords: similarity, semantics, neural networks              els have achieved unprecedented success on a number of
                                                                 practical applications, through a combination of larger
                       Introduction                              datasets, more computing power, better learning algo-
A central concern of natural language processing is the          rithms, and sophisticated neural network architectures
compact representation of semantic content in words,             (e.g., Mnih & Hinton, 2009; Collobert et al., 2011).
phrases and documents. Researchers have pursued sev-                While most of this work has focused on lexical se-
eral different approaches to this problem. One approach          mantics, a number of researchers have extended vector
is grounded in formal (model-theoretic) semantics, which         space models to phrase and sentence meaning (Mitchell
maps words and sentences onto logical expressions (Mon-          & Lapata, 2010; Socher et al., 2011, 2012). Of particu-
tague, 1970). The meaning of a word or sentence, accord-         lar interest is recent work by Socher and colleagues on
ing to this view, is the set of possible worlds in which         recursive neural networks, which integrate vector repre-
the corresponding logical expression is true. While gen-         sentations with syntactic structure (Socher et al., 2011,
eral and powerful, this approach has been difficult to           2012). The key idea underlying recursive neural net-
apply on a large scale, since the process of mapping arbi-       works is that vector representations for sentences can be
trary linguistic fragments to logical expressions is highly      generically constructed by recursively applying a com-
non-trivial. A second approach is to build databases             position operator to vectors along a parse tree. Start-
of lexical knowledge, like WordNet (Fellbaum, 1998),             ing at the leaf nodes, the word vectors are composed to
which offer definitional representations of word meaning.        form constituent vectors (e.g., noun phrase, verb phrase,
However, this approach does not directly represent the           etc.), and these vectors are in turn composed until a sen-
meaning of more complex linguistic units like sentences.         tence vector is constructed, corresponding to the root
A third approach is to derive semantic representations           node of the parse tree. By parameterizing the compo-
from large corpora by analyzing the co-occurrence of             sition operator, and defining an objective function (e.g.,
words or phrases in the same context. This approach is           classification or reconstruction error), the model can be
grounded in the distributional hypothesis: words that oc-        fit to a text corpus using gradient descent. The choice
cur in similar linguistic contexts have similar meanings         of composition parameterization, word representation,
(Harris, 1954). This last approach has gained promi-             and objective function is still a matter of active research
nence recently, as the combination of massive text data          (Mitchell & Lapata, 2010); in practice, these choices may
sets and scalable machine learning methods have led to           vary from problem to problem. This is an important de-
useful applications in core natural language processing          velopment, because it attempts to directly address one
tasks, such as information retrieval, sentiment analysis         of the central criticisms of neural networks—that they
and paraphrase detection.                                        lack compositionality, and hence cannot capture the pro-
   Our goal in this paper is to present a basic challenge        ductivity of human thought (Fodor & Pylyshyn, 1988).
for any computational approach that seeks to capture             Although a number of valiant ripostes have been di-
linguistic meaning at the level of phrases or sentences,         rected against this criticism (Smolensky, 1988; Gelder,
or any semantic unit larger than a single word. Our              1990), the work of Socher and colleagues was the first
                                                             776

to make neural compositionality work on large text cor-         • Noun change (N): “A young man in front of an old
pora and thereby deliver state-of-the-art performance on           woman.”
paraphrase detection and sentiment analysis.
   Recursive neural networks, in common with most dis-          • Adjective change (A): “An old woman in front of
tributional semantic models, are typically trained and             a young man.”
evaluated on large, diverse text corpora using super-           • Preposition change (P): “A young woman behind
vised learning objectives (but see Socher et al., 2011,            an old man.”
for an unsupervised objective). While this approach is
useful for assessing the overall accuracy of a model, it        • Meaning preservation (M): “An old man behind a
may not detect subtle failure modes. From a cognitive              young woman.”
perspective, we were interested in devising a stimulus          The order of transformations was randomized across tri-
set that could be used to evaluate how well state-of-the-       als. Subjects were not shown the transformation labels
art computational models match human semantic rep-              of these sentences. Table 1 contains the complete set of
resentations. To this end, we created a set of simple           base sentences.
phrases and used them in a phrase similarity task. The
phrases were specially designed to highlight how small                A young woman in front of an old man.
changes (in some cases simply changing word order) pro-               A black cat in front of a white dog.
duce marked changes in similarity judgments. Our stim-                A short woman in front of a tall man.
uli were designed to highlight several of the most ba-                A small bug on a large flower.
sic aspects of compositionality in the structure of noun              A small book on a black table.
phrases: (i) the composition of an adjective and a noun,              A young girl in front of a happy soldier.
referring to a property and an object respectively, to re-            A black cow in front of a brown horse.
fer to an object with a specific property, and (ii) the               A sleeping boy in front of a smiling woman.
composition of a preposition and two noun phrases, re-                A white dog on a brown chair.
ferring to a spatial relation and two objects respectively,           A happy man in front of an old woman.
to refer to a spatial relation between those two objects.             A young doctor in front of a smiling patient.
   We found that none of the vector space models that                 A red apple on green paper.
we investigated could adequately capture these patterns               A white plate on a blue pillow.
of judgment. A further experiment provided support for                A blue pen on a red folder.
a transformational theory of similarity, whereby similar-             A green pear on a brown leaf.
ity is related to the number of edits required to trans-              A yellow banana on a green knife.
form one mental model into another (Hahn et al., 2003;                An orange pepper on a yellow folder.
Kemp et al., 2005). We conclude that more theoreti-                   A plastic bag in front of a brown bottle.
cal work is needed to design semantic representations                 A brown frog on green grass.
that combine the learnability and scalability of current              A black magazine in front of a white mug.
vector-space approaches with a plausible account of com-              A pink bowl in front of a blue cup.
positional meaning at the level of phrases and above, as              A tissue box on yellow paper.
in more traditional symbolic approaches.                              A purple shirt on a green knife.
       Experiment 1: phrase similarity                                A young man in front of an angry woman.
                                                                      A black phone on gray pants.
                       judgments                                      A rusty bicycle in front of an old fence.
Our first experiment collected phrase similarity judg-                A black marker on a red shirt.
ments using a ranking task. We compared several distri-               A white sock on black headphones.
butional semantic models to human performance on the                  An open book in front of a closed window.
task.                                                                 A full glass in front of an empty bottle.
Methods
                                                                                Table 1: Base sentences
Subjects. We recruited 25 human subjects using the
Amazon Mechanical Turk web service. All subjects were
given informed consent and paid for their participation.        Models We compared 6 computational models to the
The study was approved by the MIT Institutional Re-             human data. All models have in common the prop-
view Board.                                                     erty that phrase vectors are constructed by recursively
   Procedure. Subjects were shown 30 “base” phrases             applying a composition operator to word vectors. We
(e.g., “A young woman in front of an old man”) and              used the 100-dimensional word vectors from Collobert
then asked to rank order 4 transformations of each base         and colleagues (Collobert et al., 2011), because the re-
phrase in terms of similarity in meaning:                       cursive neural network models were trained using these
                                                            777

vectors. The word vectors were obtained by training a
neural language model to perform a variety of natural
                                                                         noun change
language processing tasks, such as part-of-speech tag-
ging and named entity recognition (see Collobert et al.,
2011, for details).                                                        adj change
   The structure of the recursion is determined either by
a syntactic parse tree or a simple chain. The models
                                                                         prep change
differed in terms of their choice of recursion structure
and the choice of elementary composition operator:
                                                                     meaning preserve
• Sum: Phrase vector is the sum of the word vectors.
                                                                                         1      2      3   4
• Syntactic sum: Phrase vector is the sum of the word                                         Average rank
   vectors along the parse tree. Specifically, starting at
   the leaves of the parse tree (corresponding to words        Figure 1: Experiment 1 results. 1 = most similar, 4
   in the phrase), the vectors for children of each con-       = least similar. Error bars represent standard error of
   stituent node are summed and passed through a hy-           the mean.
   perbolic tangent transformation, and then this sum
   is passed up the parse tree until the root node vec-
   tor (corresponding to the entire phrase) is computed.       vector and each of its transformations. We used Eu-
   The role of the hyperbolic tangent transformation is        clidean distance, but essentially indistinguishable results
   to make the phrase representation a nonlinear function      were obtained with cosine and correlation distance (dis-
   of the word representations.                                tance functions were obtained by taking the negative
                                                               of cosine similarity or correlation). The second method
• Product / Syntactic product: Same as the Sum                 used bilinear regression to learn a mapping from phrase
   / Syntactic sum models, but using elementwise mul-          vector pairs to dissimilarity. Mathematically, this model
   tiplication instead of addition as the elementary com-      has the following form:
   position operator.
                                                                                      D(i, j) = xi Λyj ,               (4)
• Recursive autoencoder (RAE; Socher et al., 2011):
   Similar to the syntactic sum model, except that the         where D(i, j) is the dissimilarity between phrases i and j,
   elementary composition operator is parameterized as         xi and yj are the corresponding vector representations,
   a linear combination:                                       and Λ is a matrix of regression coefficients. We used
                                                               leave-one-out cross-validation, fitting the least-squares
                  p = tanh(W1 x + W2 y + b),           (1)     coefficients to all the phrase pairs except one, and then
                                                               testing on the held-out pair. The regression model
   where x and y are the children vectors, p is the parent
                                                               was trained to predict human similarity rankings, which
   vector, “tanh” is the hyperbolic tangent function, and
                                                               range from 1 (most similar) to 4 (least similar); thus, the
   W1 , W2 and b are parameters.
                                                               model is predicting dissimilarity.
• Matrix-vector recursive neural network (MV-
   RNN; Socher et al., 2012): Similar to the RAE, but
                                                               Results and discussion
   now constituents are represented by both a vector and       Humans show a systematic pattern in their similarity
   a matrix, which allows the model to capture modula-         rankings (Figure 1): the meaning-preserving transfor-
   tory interactions between constituents:                     mation is judged most similar, followed by preposition
                                                               change, adjective change, and finally noun change. To
                  p = tanh(W1 Y x + W2 Xy)             (2)     quantify this pattern, we computed pairwise t-tests be-
                  P = W3 X + W4 Y,                     (3)     tween the rankings of the different transformations; all
                                                               tests were statistically significant (p < 0.05) except for
   where X and Y are the children matrices.                    the difference between the meaning preserving change
                                                               and the preposition change.
For the RAE and the MV-RNN models, we used the
                                                                  The model predictions, using the vector distance
parameters that were reported in the original papers
                                                               method, are shown in Figure 2. These results demon-
(Socher et al., 2011, 2012).
                                                               strate that none of the models described above can ade-
Prediction of similarity rankings We used two dif-             quately capture the behavioral results. None of the mod-
ferent methods to obtain similarity rankings from the          els correctly predicts the rank ordering exhibited by hu-
vector space models. The first method computed a rank          mans. A similar conclusion can be drawn from the model
ordering based on the distance between the base phrase         predictions using the bilinear method (Figure 3).
                                                           778

                                          sum                syntactic sum                 prod
                        noun change
                          adj change
                         prep change
                    meaning preserve
                                     1   2    3   4          1   2    3   4            1  2    3  4
                                     syntactic prod               RAE                    MV−RNN
                        noun change
                          adj change
                         prep change
                    meaning preserve
                                     1   2    3   4          1   2    3   4            1  2    3  4
   Figure 2: Model predictions, vector distance method. Human behavioral data are superimposed in red.
                                          sum                syntactic sum                 prod
                        noun change
                          adj change
                        prep change
                    meaning preserve
                                     1   2    3   4         1    2    3   4           1   2    3  4
                                     syntactic prod               RAE                    MV−RNN
                        noun change
                          adj change
                        prep change
                    meaning preserve
                                     1   2    3   4         1    2    3   4           1   2    3  4
 Figure 3: Model predictions, bilinear regression method. Human behavioral data are superimposed in red.
   To quantify the fit between models and human judg-           in the elementwise difference or ratio of the two vectors.
ment, we computed correlations for both the vector dis-         One could also explore using regularization or placing
tance and bilinear regression methods. The results are          constraints on the bilinear model (e.g., requiring that Λ
shown in Figure 4. In most cases, the correlations are not      be low rank).
significantly different from 0. In the case of the sum and
product models, the correlation is significantly greater             Experiment 2: event transformation
than 0 (p < 0.05). This is largely driven by their cor-                              judgments
rect prediction that preposition and meaning preserv-           One possible interpretation of the results from Exper-
ing changes are more similar than noun and adjective            iment 1 is that they reflect a process of mental model
changes. However, visual inspection of Figure 3 reveals         building (Johnson-Laird, 1983; Tenenbaum et al., 2011).
that they incorrectly order preposition vs. meaning pre-        In particular, semantic similarity may reflect the diffi-
serving change, as well as noun vs. adjective change.           culty of modifying the mental model of the base phrase
Thus, the significant correlation is not a particularly re-     to align with the transformed phrase. In the simplest
sounding endorsement of their descriptive adequacy.             terms, we can think of each of our base sentences as
   One direction for future research is to explore alterna-     establishing a mental model with three constituents ex-
tive regression models. For example, instead of learning        pressing the attributes of and relations among entities
a bilinear model, one could learn a model that is linear        in a scene. For instance, the mental model of “A young
                                                            779

                                                                ing.
                      Distance
                      Bilinear                                  Results and discussion
          sum
syntactic sum                                                   Figure 5 (left) shows the transformation rankings re-
                                                                ported by our subjects. These results closely resem-
          prod                                                  ble the phrase similarity rankings reported by subjects
                                                                in Experiment 1. To establish this correspondence at
syntactic prod                                                  an item level, we plotted the transformation rankings
          RAE
                                                                against the similarity rankings for each phrase (right
                                                                panel of Figure 5). The two rankings are strongly corre-
     MV−RNN                                                     lated (r = 0.87, p < 0.00001). These results support the
                                                                transformational view of similarity (Hahn et al., 2003;
            −0.4      −0.2         0       0.2       0.4        Kemp et al., 2005), which holds that similarity judg-
                               Correlation                      ments reflect the number (and probability) of transfor-
                                                                mations required to transform one object into another.
                                                                In our case the “objects” are descriptions of scenes,
Figure 4: Correlation between model predictions                 and the transformations are events that cause scenes to
and human similarity judgments. Error bars repre-               change.
sent bootstrapped 95% confidence intervals.
                                                                                General discussion
woman in front of an old man” would consist of these            Our experimental results are deflationary for several
constituents: (i ) A woman in front of a man, (ii ) a young     prominent models of compositional semantics. While
woman, and (iii ) an old man. The different transforma-         the models are effective at capturing some aspects of
tions of this base sentence can be ordered in terms of how      semantics in large corpora, they decisively fail in care-
many semantic constituents are changed: M (0 changes),          fully constructed test cases such as the ones presented
P (changes i ), A (changes ii and iii ), N (changes all         here (see also Pham et al., 2013). Some of this may be
three). This account of similarity judgment has much            attributable to the fact that the models were not trained
in common with the representational distortion account          to perform the task given to subjects, but the poor per-
of similarity (Hahn et al., 2003), according to which the       formance of the bilinear model (which is trained to per-
similarity between two entities is the complexity of the        form the task) suggests that other methods may be re-
operations required to distort one into the other (see          quired to match human performance. There may also be
Kemp et al., 2005, for a Bayesian treatment of this idea).      other, deeper problems with vector-based compositional
We believe that developing models of compositional se-          semantics, which are hard to diagnose because their se-
mantics along these lines is a promising avenue of future       mantic information content and algebraic composition
research. To explore this idea, we slightly modified the        operators are opaque to interpretation.
procedure from Experiment 1 to elicit judgments of tran-           We have suggested that a transformational account
sition probability rather than similarity.                      (Hahn et al., 2003; Kemp et al., 2005) may provide the
                                                                basis for a better theory of phrase similarity. Evidence
Methods                                                         in support of this account was provided by Experiment
Subjects. We recruited 20 human subjects using the              2, which showed that similarity judgments can be pre-
Amazon Mechanical Turk web service. All subjects were           dicted by transformation judgments. Models based on
given informed consent and paid for their participation.        recurrent neural networks (Sutskever et al., 2011) might
The study was approved by the MIT Institutional Re-             be able to capture certain kinds of transformations; how-
view Board.                                                     ever, we believe that doing this appropriately for natural
   Procedure. The procedure and stimuli were identical          language requires a theory of how syntactic structure re-
to the procedure for Experiment 1, except for a change of       lates to event knowledge. It is presently unclear whether
instructions. Instead of asking subjects to rank phrases        such a theory can be obtained via a purely distributional
according to their similarity in meaning, we asked them         approach.
to imagine the phrases as scenes, and rank the trans-              Our findings pose a general challenge to computational
formations according to the likelihood that each trans-         models of phrase similarity. We hope that the failures of
formed scene would occur following the base scene (i.e.,        the models explored in this paper will stimulate the de-
the transition probability). In other words, we asked           velopment of new approaches, perhaps based on some
subjects to rank phrases according to their transition          combination of probabilistic world models and vector
probability. We refer to this as the transformation rank-       space representations.
                                                            780

                                                                                                                r = 0.87
                                                                                                                p < 0.0001
                                                                                   Transformation ranking
            noun change                                                                                     4
              adj change                                                                                    3
             prep change                                                                                    2
        meaning preserve                                                                                    1
                                1       2        3      4                                                       1       2        3     4
                                       Average rank                                                                 Similarity ranking
Figure 5: Experiment 2 results. (Left) Transformation rankings. Error bars represent standard error of the mean.
(Right) Transformation rankings plotted against similarity rankings. Each point represents a single phrase.
Acknowledgments                                                        Montague, R. (1970). English as a Formal Language. Ed. di
We are grateful to Sam Ritter, Richard Socher, Tim                       Comunità.
O’Donnell and Anatole Gershman for helpful discussions.                Pham, N., Bernardi, R., Zhang, Y. Z., & Baroni, M. (2013).
This work was supported by the Office of the Director of Na-             Sentence paraphrase detection: When determiners and
tional Intelligence (ODNI), Intelligence Advanced Research               word order make the difference. In Proceedings of the
Projects Activity (IARPA), via Air Force Research Labora-                IWCS 2013 Workshop: Towards a Formal Distributional
tory (AFRL), under contract FA8650-14-C-7358. The views                  Semantics.
and conclusions contained herein are those of the authors              Smolensky, P. (1988). The constituent structure of connec-
and should not be interpreted as necessarily representing                tionist mental states: A reply to fodor and pylyshyn. The
the official policies or endorsements, either expressed or im-           Southern Journal of Philosophy, 26 , 137–161.
plied, of ODNI, IARPA, AFRL, or the U.S. Government.                   Socher, R., Huang, E. H., Pennin, J., Manning, C. D., &
The U.S. Government is authorized to reproduce and dis-                  Ng, A. Y. (2011). Dynamic pooling and unfolding recur-
tribute reprints for Governmental purposes notwithstanding               sive autoencoders for paraphrase detection. In Advances
any copyright annotation thereon.                                        in Neural Information Processing Systems, (pp. 801–809).
                                                                       Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012).
                      References                                         Semantic compositionality through recursive matrix-vector
Collobert, R., Weston, J., Bottou, L., Karlen, M.,                       spaces. In Proceedings of the 2012 Joint Conference on
  Kavukcuoglu, K., & Kuksa, P. (2011). Natural language                  Empirical Methods in Natural Language Processing and
  processing (almost) from scratch. The Journal of Machine               Computational Natural Language Learning, (pp. 1201–
  Learning Research, 12 , 2493–2537.                                     1211). Association for Computational Linguistics.
Fellbaum, C. (1998). WordNet. Wiley Online Library.                    Sutskever, I., Martens, J., & Hinton, G. E. (2011). Generat-
                                                                         ing text with recurrent neural networks. In Proceedings of
Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and                the 28th International Conference on Machine Learning,
  cognitive architecture: A critical analysis. Cognition, 28 ,           (pp. 1017–1024).
  3–71.
                                                                       Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman,
Gelder, T. (1990). Compositionality: A connectionist varia-              N. D. (2011). How to grow a mind: Statistics, structure,
  tion on a classical theme. Cognitive Science, 14 , 355–384.            and abstraction. Science, 331 , 1279–1285.
Hahn, U., Chater, N., & Richardson, L. B. (2003). Similarity           Turney, P. D., Pantel, P., et al. (2010). From frequency to
  as transformation. Cognition, 87 , 1–32.                               meaning: Vector space models of semantics. Journal of
Harris, Z. S. (1954). Distributional structure. Word , 10 ,              Artificial Intelligence Research, 37 , 141–188.
  146–162.
Johnson-Laird, P. N. (1983). Mental Models: Towards a Cog-
  nitive Science of Language, Inference, and Consciousness.
  Harvard University Press.
Kemp, C., Bernstein, A., & Tenenbaum, J. B. (2005). A
  generative theory of similarity. In Proceedings of the 27th
  Annual Conference of the Cognitive Science Society, (pp.
  1132–1137). Citeseer.
Landauer, T. K., & Dumais, S. T. (1997). A solution to
  plato’s problem: The latent semantic analysis theory of
  acquisition, induction, and representation of knowledge.
  Psychological Review , 104 , 211–240.
Mitchell, J., & Lapata, M. (2010). Composition in distribu-
  tional models of semantics. Cognitive Science, 34 , 1388–
  1429.
Mnih, A., & Hinton, G. E. (2009). A scalable hierarchical
  distributed language model. In Advances in Neural Infor-
  mation Processing Systems, (pp. 1081–1088).
                                                                 781

