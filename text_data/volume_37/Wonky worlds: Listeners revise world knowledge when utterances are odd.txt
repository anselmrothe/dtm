        Wonky worlds: Listeners revise world knowledge when utterances are odd
                                      Judith Degen, Michael Henry Tessler, Noah D. Goodman
                                                 {jdegen,mhtessler,ngoodman}@stanford.edu
                                                  Department of Psychology, 450 Serra Mall
                                                            Stanford, CA 94305 USA
                                Abstract                                      Generalizing our opening example, consider Some of the X
   World knowledge enters into pragmatic utterance interpreta-             sank, where X is a plural noun such as marbles, feathers, or
   tion in complex ways, and may be defeasible in light of speak-          balloons, and the X refers to a contextually established group
   ers’ utterances. Yet there is to date a surprising lack of sys-         of objects from category X. When the prior probability, θX ,
   tematic investigation into the role of world knowledge in prag-
   matic inference. In this paper, we show that a state-of-the-art         of an X1 sinking is not extreme (e.g., a feather sinking), RSA
   model of pragmatic interpretation greatly overestimates the in-         leads to the standard scalar implicature: the posterior prob-
   fluence of world knowledge on the interpretation of utterances          ability that all of the X sank, after hearing the utterance, is
   like Some of the marbles sank. We extend the model to cap-
   ture the idea that the listener is uncertain about the background       much lower than its prior probability (i.e., Some of the feath-
   knowledge the speaker is bringing to the conversation. This             ers sank yields that not all of them did). This is because a ra-
   extension greatly improves model predictions of listeners’ in-          tional speaker would have been expected to produce the more
   terpretation and also makes good qualitative predictions about
   listeners’ judgments of how ‘normal’ the world is in light of a         informative All of the X sank, had it been true. As we will
   speaker’s statement. Theoretical and methodological implica-            show below, RSA makes two strong predictions about the ef-
   tions are discussed.                                                    fect of the prior: (1) As θX approaches 1, the interpretation
   Keywords: scalar implicature; world knowledge; prior be-                probability that all X sank approaches 1, that is, the scalar
   liefs; experimental pragmatics; computational pragmatics                implicature disappears. This prediction follows because the
   How often do you think marbles would sink in water?                     extreme prior overwhelms the effect of the utterance. (2) For
Probably extremely often, if not always. Now imagine read-                 moderate to high prior probability (roughly 0.5<θX <1) and
ing Max threw fifteen marbles in the water. Some of the mar-               a large total number of objects (more than about 10), the pos-
bles sank. Have you begun to reconsider your assumptions?                  terior expectation of the number of X that sank should be ap-
Perhaps you now suspect that these marbles are in fact made                proximately the same as the prior expectation—that is, the ut-
of hollow plastic or the water is covered with thick algae?                terance shouldn’t affect the expected number of X that sank.
That is, maybe these are not just normal marbles in normal                 This prediction follows from the weak semantics of some and
water. Here we explore how prior world knowledge enters                    the isolated effect of the alternative all: Some of the X sank
into pragmatic utterance interpretation, and when this world               only restricts the interpretation (i.e., the number of marbles
knowledge is defeasible: some utterances lead listeners to                 that sank) to be greater than zero; competition with All of the
conclude that the world under discussion is abnormal and has               X sank results in the scalar implicature that can at most rule
appropriately different prior probabilities. We refer to such              out the state in which all of the X sank. Thus, a sufficiently
an abnormal world as a wonky world.                                        strong prior will dominate the inference about exactly how
   The Rational Speech Acts framework (RSA) (Frank &                       many X sank.
Goodman, 2012; Goodman & Stuhlmüller, 2013), and re-                         However, intuition is at odds with these predictions: as
lated models (Franke, 2011; Russell, 2012), treat communi-                 Geurts (2010) has observed, for events with very high prior
cation as a signaling game (Lewis, 1969) between a speaker                 probability of occurrence (e.g., marbles sinking), an utterance
and a listener. The listener reasons by Bayesian inference                 like Some of the marbles sank seems to yield strong implica-
about what the world is like given that a speaker who pro-                 tures; thus, contrary to RSA predictions, the subjective prob-
duced the utterance is trying to be informative (with respect              ability that all of the marbles sank is intuitively close to 0.
to a naı̈ve listener). Variants of these models have success-                 In Exp. 1 we collect prior probabilities for a variety of
fully captured listeners’ quantitative behavior on a number of             events (e.g., sinking) and categories (e.g., marbles). In
pragmatic inference tasks, including ad hoc Quantity impli-                Exps. 2a and 2b we collect corresponding posterior inter-
cature (Degen, Franke, & Jäger, 2013), markedness implica-                pretations after observing utterances containing quantifiers.
ture (Bergen, Goodman, & Levy, 2012), scalar implicature                   These experimental results confirm the intuition of rela-
(Goodman & Stuhlmüller, 2013), and non-literal language                   tively strong implicature—hence prediction (1) of RSA is
(Kao, Wu, Bergen, & Goodman, 2014). A defining feature of                  incorrect—and show that the prior has a muted effect on pos-
Bayesian reasoning is that prior beliefs affect inferences that            terior expectation—hence prediction (2) of RSA is incorrect.
will be drawn. Bayesian models of language interpretation,                 Given the previous success of RSA models, this constitutes a
accordingly, predict that prior beliefs about the world should             striking puzzle. To address this puzzle we pursue the intuition
affect the listener’s interpretation of an utterance. While this           raised at the very beginning of this paper: that sometimes, the
impact of prior knowledge has been noted, and included in                      1 We will use ‘X’ interchangeably to refer to both the category
models, it hasn’t been systematically studied.                             and the members of the category.
                                                                       548

speaker’s utterance will lead the listener to infer that the world
under discussion is wonky and she should therefore use less                                  8
extreme prior beliefs in the computation of speaker meaning.
                                                                               Number of cases
                                                                                             6
We introduce a variant of RSA, wonky RSA (wRSA), in which
the listener can revise her beliefs about the domain under dis-
                                                                                             4
cussion. We show that this extension resolves the puzzle of
the prior’s muted effects. In Exp. 3 we explore participants’                                2
intuitions about whether the world is normal or wonky in the
scenarios of Exps. 2a and 2b and find that wRSA predicts                                     0
                                                                                                  1   3         5       7       9      11        13   15
listeners’ wonkiness judgments.                                                                           Expected value of prior distribution
           Experiment 1: prior elicitation                                  Figure 1: Histogram of expected values of each empirically
Exp. 1 measured listeners’ prior beliefs about how many ob-                 elicited and smoothed prior distribution.
jects exhibit a certain effect (e.g., how many marbles sink).
Method                                                                                           Effect of the world prior in RSA
Participants, procedure and materials We recruited 60                       The basic Rational Speech Acts model defines a pragmatic
participants over Amazon’s Mechanical Turk.                                 listener PL1 (s|u), who reasons about a speaker PS1 (u|s), who
   On each trial,2 participants read a one-sentence description             in turn reasons about a literal listener PL0 (s|u). Each listener
of an event like John threw 15 marbles into a pool. They                    does Bayesian inference about the world state, given either
were then asked to provide a judgment of an effect, e.g. How                the literal truth of utterance u or the speaker’s choice of u; the
many of the marbles do you think sank?, on a sliding scale                  speaker is a softmax-optimal decision maker, with the goal of
from 0 to 15. Each item had a similar form: the first sentence              being informative about the state s. RSA is defined by:
introduced the objects at issue (e.g., marbles). The question
always had the form How many of the X Yed?, where X was                                                   PL0 (s|u) ∝ δ [[u]](s) · P(s)                    (1)
the head of the direct object noun phrase introduced in the
                                                                                                          PS1 (u|s) ∝ exp(λ ln PL0 (s|u))                  (2)
first sentence (e.g., marbles, cups, balloons) and Yed was
a verb phrase denoting an effect that the objects underwent                                               PL1 (s|u) ∝ PS1 (u|s) · P(s)                     (3)
(e.g., sank, broke, stuck to the wall). Each verb phrase oc-
curred with three different objects, e.g., sank occurred with               Here [[u]] : S → Boolean is a truth-function specifying the lit-
marbles, cups, and balloons. Items were constructed to intu-                eral meaning of each utterance.
itively cover the range of probabilities as much as possible,                  For concreteness, assume the set of states of the world S =
while also somewhat oversampling the upper range of proba-                  {s0 , s1 , s2 , . . . , s15 }, where the subscript indicates the number
bilities to have more fine-grained coverage of this region that             of objects (e.g., marbles) that exhibit an effect (e.g., sinking).
is of most interest for testing the RSA model. Judgments were               Further assume that the set of utterances All/None/Some of
obtained for 90 items, of which each participant saw a random               the marbles sank is denoted U = {uall , unone , usome } and each
selection of 30 items.                                                      has its usual literal meaning: [[unone ]] = {si |i = 0}, [[usome ]] =
                                                                            {si |i > 0}, [[uall ]] = {si |i = 15}.
Results                                                                        In Figure 2 we show the predictions of RSA (dark blue
Data from one participant, who gave only one response                       dots) for the items from Exp. 1 in two different ways: the
throughout the experiment, were excluded. Each item re-                     left panel shows the posterior expected number of affected
ceived between 12 and 29 ratings. Distributions of ratings                  objects as a function of the prior expectation; the right panel
for each item were smoothed using nonparametric density es-                 shows the posterior probability of the state in which all ob-
timation for ordinal categorical variables (Li & Racine, 2003)              jects are affected, as a function of the prior probability of that
with the np package in R (Hayfield & Racine, 2008). As in-                  state.3 We see that the prior has a strong effect, which can be
tended, items covered a wide range of probabilities. See Fig-               summarized by the two predictions described in the Introduc-
ure 1 for a histogram of expected values of each smoothed                   tion: (1) P(s15 |usome ) → 1 as P(s15 ) → 1. (2) E[P(s|usome )] '
prior distribution.                                                         E[P(s)] over the upper half of its range. We next turn to an
                                                                            empirical test of these predictions, or rather, of the intuition
   In the next section, we use these empirically obtained
                                                                            that they may be incorrect.
smoothed prior beliefs to derive RSA predictions for the inter-
pretation of utterances like Some of the marbles sank, before                  3 That the individual model predictions look somewhat noisy is
empirically measuring participants’ interpretations.                        due to the different shapes of the prior distributions, such that for
                                                                            the same expected value of the distribution, the distribution itself
   2 See this experiment at http://cocolab.stanford.edu/cogsci2015/         can take different shapes, which will be treated slightly differently
wonky/prior/sinking-marbles-prior.html                                      by the model.
                                                                      549

               Predicted posterior number of objects                                                                                                                 Predicted posterior all−state probability
                                                   15                                                                                                          ●
                                                                                                                                                                ●
                                                                                                                                                                ●
                                                                                                                                                                 ●
                                                                                                                                                                                                             1.00                                                                                                          ●
                                                                                                                                                              ●
                                                                                                                                                             ●●
                                                                                                                                                            ●
                                                                                                                                                           ●
                                                                                                                                                           ●
                                                                                                                                                           ●
                                                                                                                                                         ●●
                                                   13                                                                                                 ●●
                                                                                                                                                       ●●
                                                                                                                                                       ●
                                                                                                                                                       ●
                                                                                                                                                         ●
                                                                                                                                                         ●
                                                                                                                                                         ●
                                                                                                                                                   ●●●
                                                                                                                                                    ●
                                                                                                                                                                                                                                                                                                                      ●
                                                                                                                                             ●
                                                                                                                                             ●     ●●●                                                                                                                                                           ●
                                                   11                                                                                       ● ●
                                                                                                                                            ● ●
                                                                                                                                               ●
                                                                                                                                                                                                             0.75                                                                                                ●
                                                                                                                                     ●●      ●                                                                                                                                                               ●
                                                                                                                                   ●●                         ●●
                                                                                                                                 ●                         ●●●
                                                                                                                    ●           ●● ●                 ●●●●●
                                                                                                                                                         ●● ●●●
                                                                                                                                                  ●● ●
                                                                                                                                                  ●  ●●●●●                                                                                                                                               ●
                                                                                                                            ●               ●   ●● ●●●●                                                                                                                                                 ●
                                                                                                                            ●              ●●●● ●
                                                                                                                                           ● ● ●
                                                       9                                                    ●
                                                                                                                ●
                                                                                                                   ●●
                                                                                                                    ● ●●
                                                                                                                                    ●
                                                                                                                                      ●●
                                                                                                                                    ● ●
                                                                                                                                        ●●  ●
                                                                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                                                      ●
                                                                                                                 ●●    ●
                                                                                                                                ●
                                                               ●                                            ●●
                                                                                                            ●●
                                                                                                            ● ●
                                                                                                                  ●● ●
                                                                                                                ● ● ●
                                                                                                                  ● ●●●
                                                                                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                                                                                  ●                              Model
                                                               ●                             ●        ●
                                                                                                     ●●
                                                                                                              ●
                                                                                                                                                                                                             0.50
                                                       7       ●●   ●
                                                                        ●
                                                                        ●
                                                                                ●
                                                                                     ●
                                                                                           ●
                                                                                           ●
                                                                                                  ●
                                                                                                 ● ● ●                                                                                                                                                                                      ●                                     ●
                                                                                                                                                                                                                                                                                                                                      RSA
                                                           ●                             ● ●                                                                                                                                                                                               ●
                                                                        ● ●                      ●● ●                                                                                                                                                                                     ●
                                                                        ●        ●
                                                                                ●
                                                                                     ●
                                                                                         ●
                                                                                             ●
                                                                                                                                                                                                                                                                                     ●
                                                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                                      ●                                    ●      ●
                                                                                                                                                                                                                                                                                                                                      wRSA
                                                       5       ●    ●
                                                                                                                                                                                                                                                                                 ●
                                                                                                                                                                                                                                                                                 ●
                                                               ●            ●   ●                                                                                                                                                                                               ●                                ●●
                                                                                                                                                                                                             0.25                                                         ●
                                                                                                                                                                                                                                                                           ●
                                                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                                                                          ●           ●●
                                                                                                                                                                                                                                                                                                        ●
                                                       3   ●                                                                                                                                                                                                            ●●
                                                                                                                                                                                                                                                                        ●                         ●
                                                                                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                                                                                             ●
                                                                                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                    ●                     ●
                                                                                                                                                                                                                                                                ●                    ●●               ●
                                                                                                                                                                                                                                                          ●●         ●   ●●           ●
                                                                                                                                                                                                                                                                      ●●                      ●
                                                                                                                                                                                                                                                ●    ● ●●●●        ●●●    ●
                                                       1                                                                                                                                                                                    ●
                                                                                                                                                                                                                                                    ●●
                                                                                                                                                                                                                                                       ●
                                                                                                                                                                                                                                                     ● ●●
                                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                                         ●● ●
                                                                                                                                                                                                                                                                ●● ●
                                                                                                                                                                                                                                          ●● ●● ●
                                                                                                                                                                                                                                         ●●
                                                                                                                                                                                                                                         ● ●●● ●
                                                                                                                                                                                                                                            ●●      ●●●
                                                                                                                                                                                                             0.00       ●
                                                                                                                                                                                                                        ●● ●
                                                                                                                                                                                                                           ●
                                                                                                                                                                                                                         ●●●●● ●
                                                                                                                                                                                                                               ●   ●●
                                                                                                                                                                                                                                   ●●●   ●
                                                                                                                                                                                                                                         ●●●
                                                                                                                                                                                                                                          ●
                                                           1            3                5              7               9             11           13         15                                                       0.00               0.25                  0.50                          0.75                        1.00
                                                                   Prior mean number of objects                                                                                                                               Prior mean all−state probability
Figure 2: For each item, RSA and wRSA model predicted number of objects (E[P(s|usome )] as a function of E[P(s)], left) and
model predicted all-state probability (P(s15 |usome ) as a function of P(s15 ), right) after observing Some of the X Yed.
      Experiment 2a and 2b: comprehension                                                                                                                                                                              generic short fillers that were intended to communicate the
Exps. 2a and                         2b4
                     measured participants’ posterior beliefs                                                                                                                                                          prior, e.g., Typical. The rest were longer sentences that ad-
P(s|u) about how many objects exhibited a certain effect (e.g.,                                                                                                                                                        dressed a different aspect of the described scenario, e.g. What
marbles sinking), after observing an utterance. The only dif-                                                                                                                                                          a stupid thing to do. The utterances were randomly paired
ference between the experiments was the dependent mea-                                                                                                                                                                 with 30 random items for each participant.
sure. We used different dependent measures for two reasons.                                                                                                                                                            Results and discussion
First, this allowed for directly and independently estimating
the two values that the predictions above are concerned with:                                                                                                                                                          Data from eight participants in Exp. 2b were excluded from
E[P(s|usome )] and P(s15 |usome ). Second, this allows for eval-                                                                                                                                                       the analysis because these participants assigned less than .8
uating the generalizability of the empirical result, a growing                                                                                                                                                         probability to the interpretation corresponding to the correct
concern in experimental pragmatics.5                                                                                                                                                                                   literal interpretation on literal all and none trials.6
                                                                                                                                                                                                                          The main question of interest was whether participants’
Method                                                                                                                                                                                                                 judgments of how many objects exhibited the effect after
Participants, procedure and materials For each experi-                                                                                                                                                                 hearing an utterance with some followed the predictions of
ment we recruited 120 participants over Mechanical Turk.                                                                                                                                                               the basic RSA model laid out in the previous section. Mean
   Participants read the same descriptions as in Exp. 1. They                                                                                                                                                          empirical E[P(s|u)] and P(s15 |u) are shown in Figure 3 for
additionally saw an utterance produced by a knowledgeable                                                                                                                                                              each item. There was a small effect of the prior. For ut-
speaker about the event, e.g. John, who observed what hap-                                                                                                                                                             terances of Some of the X Yed, the mean number of ob-
pened, said: “Some of the marbles sank”. In Exp. 2a (just                                                                                                                                                              jects judged to exhibit the effect increased with increasing
as in Exp. 1), they then provided a judgment of an effect,                                                                                                                                                             expectation of the prior distribution (β=.18, SE=.02, t=7.4,
e.g. How many of the marbles do you think sank?, on a slid-                                                                                                                                                            p<.0001). Similarly, the probability of all 15 objects exhibit-
ing scale from 0 to 15. In Exp. 2b they instead rated on slid-                                                                                                                                                         ing the effect increased with increasing prior probability of
ing scales with endpoints labeled “definitely not” and “def-                                                                                                                                                           doing so (β=.06, SE=.01, t=5.0, p<.0001). However, the size
initely”, how likely they thought 0%, 1-50%, 51-99%, or                                                                                                                                                                of these effects is astronomically smaller than predicted by
100% of the objects exhibited the effect.                                                                                                                                                                              RSA (for comparison, see dark lines in Figure 2).
   Each participant saw 10 some trials and 20 filler trials, of                                                                                                                                                           One possible explanation for this highly attenuated effect
which 10 contained the quantifiers all or none, and the rest                                                                                                                                                           of the prior is that participants simply do not bring world
were utterances that did not address the number of objects                                                                                                                                                             knowledge to bear on the interpretation of utterances. How-
that displayed the effect. These 10 additional fillers were in-                                                                                                                                                        ever, this possibility is ruled out by examining participants’
tended to establish whether participants were using informa-                                                                                                                                                           performance in the filler conditions: in both Exps. 2a and 2b,
tion about the prior in the first place. Of these, half were                                                                                                                                                           the filler conditions closely tracked the prior (see Figure 3).
   4 See                                                                                                                                                                                                                   6 In general, this task yielded noisier results than the task in
           these                                       experiments
                                        http://cocolab.stanford.edu/                     at
cogsci2015/wonky/expectation/sinking-marbles.html    and     http://                                                                                                                                                   Exp. 2a (as can be seen in the average lower probability of the all-
cocolab.stanford.edu/cogsci2015/wonky/stateprobs/sinking-marbles                                                                                                                                                       state after observing all, in the right panel of Figure 3) because par-
-nullutterance.html                                                                                                                                                                                                    ticipants used the sliders in different ways. For example, for cases
   5 For a discussion of the role of dependent measures in experi-                                                                                                                                                     where intuitively, the all-state was true, some participants assigned
mental pragmatics, see e.g., Benz and Gotzner (2014); Degen and                                                                                                                                                        non-zero probability to only the all-state, while others were reluctant
Goodman (2014).                                                                                                                                                                                                        to do so and always assigned some probability to the 51-99% state.
                                                                                                                                                                                                                 550

                                             15                                                                                                                                          1.00
              Posterior mean number of objects                                                                                                          Posterior probability of all−state
                                             13
                                             11                                                                                                                                          0.75
                                                                                                                                              ● ●                                                                                                                                               Quantifier
                                                                                                                                    ●
                                                 9                                                      ●                      ●
                                                                                                                                   ●
                                                                                                                                   ● ● ●   ●
                                                                                                                                             ● ●●●
                                                                                                                                             ●  ●                                                                                                                                                ●
                                                                                                                                                                                                                                                                                                     Some
                                                                                                                            ●    ●         ● ● ●● ●
                                                                                                                         ●               ●
                                                                                                             ●         ● ● ●  ● ●        ●● ●      ●
                                                                                  ●                                        ●             ● ●
                                                            ●                 ●
                                                                                      ●       ●● ●
                                                                                                    ● ●●
                                                                                                            ●
                                                                                                                 ●
                                                                                                                     ●          ●● ● ●
                                                                                                                                         ●●
                                                                                                                                          ●●      ●●                                     0.50                                                                                                        All
                                                 7                    ●
                                                                                      ●   ●
                                                                                                  ● ● ●
                                                                                                        ●
                                                                                                                              ●
                                                                                                                               ●     ●
                                                                                                                                        ●     ●
                                                                          ●                                                        ●
                                                       ●        ● ●               ●
                                                                                  ●
                                                                                                        ●       ●
                                                                                                                 ●     ●              ●                                                                                                                                                              None
                                                     ● ●●
                                                 5              ●
                                                                      ●                                                                                                                                                                                                                              long_filler
                                                                                                                                                                                         0.25                                                                                                        short_filler
                                                 3                                                                                                                                                                                                            ● ●                ●
                                                                                                                                                                                                                                                                       ●             ●    ●
                                                                                                                                                                                                         ●     ●●                                    ●    ● ●
                                                                                                                                                                                                                                                             ● ●
                                                                                                                                                                                                                     ● ●               ● ●          ●             ●●     ●   ●
                                                                                                                                                                                                    ●● ●●      ●                              ●         ●    ● ●●        ●●
                                                                                                                                                                                                    ●● ●             ●●●               ● ●● ●
                                                 1                                                                                                                                                  ● ●●
                                                                                                                                                                                                    ●  ●
                                                                                                                                                                                                    ●● ● ●
                                                                                                                                                                                                    ●  ●
                                                                                                                                                                                                                ●
                                                                                                                                                                                                                ●
                                                                                                                                                                                                                 ●    ●●
                                                                                                                                                                                                                        ● ●●
                                                                                                                                                                                                                       ●●
                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                         ● ●
                                                                                                                                                                                                                                                  ●   ● ●
                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                        ●              ●   ●
                                                                                                                                                                                                                                                                                 ●        ●
                                                                                                                                                                                                    ●      ●                       ●
                                                                                                                                                                                                                               ●
                                                                                                                                                                                         0.00       ●
                                                     1           3            5               7             9            11          13          15                                                0.00               0.25                        0.50              0.75                 1.00
                                                            Prior mean number of objects                                                                                                                       Prior probability of all−state
Figure 3: For each item and quantifier, (left) empirical mean number of objects as a function of the prior mean number of
objects (i.e., E[P(s|usome )] vs. E[P(s)] from Exp. 2a and Exp. 1); and (right) empirical mean all-state probability as a function
of the prior mean all-state probability (i.e., P(s15 |usome ) vs. P(s15 ) from Exp. 2b and Exp. 1). Implicatures from some to not all
were generally strong, as evidenced in the low all-state probabilities after observing some.
   Exps. 2a and 2b demonstrate that there is an effect of lis-                                                                                                                                     We refer to this model as wRSA. Notice that the choice of w
teners’ prior beliefs on the interpretation of utterances with                                                                                                                                     that the listener makes will depend on PS1 (u|s, w): if a given
some. However, this effect is quantitatively much smaller                                                                                                                                          utterance can’t be explained by the usual prior because it is
than predicted by RSA, and qualitatively does not match the                                                                                                                                        unlikely under any plausible world state s, the pragmatic lis-
predictions identified above: the implicature is not canceled                                                                                                                                      tener infers that the world is wonky and backs off to the uni-
for extreme priors and the posterior expectation diverges from                                                                                                                                     form prior. That is, if the utterance is odd, the listener will
the prior expectation. In the next section, we extend the RSA                                                                                                                                      revise her opinion about appropriate world knowledge.
model to formalize a listener who may decide that her initial
beliefs about the domain are not shared by the speaker and                                                                                                                                            To make predictions for Exp. 2 from wRSA we use the
responds by revising her priors.                                                                                                                                                                   smoothed empirical priors from Exp. 1 as Pusual (s) for each
                                                                                                                                                                                                   item. The wonkiness prior P(w) and the speaker optimality
    Effect of the world prior in ‘wonky RSA’                                                                                                                                                       λ are fit to optimize mean squared error (MSE) with Exp. 2
To capture the idea that the pragmatic listener is unsure what                                                                                                                                     data. The optimal parameters (λ = 2, P(w) = 0.5) resulted in
background knowledge the speaker is bringing to the conver-                                                                                                                                        an MSE of 2.15 (compared to 14.53 for RSA) for the expected
sation, we extend the basic RSA model by using a “lifted                                                                                                                                           number of objects, and 0.01 (compared to 0.07 for RSA) for
variable” (Goodman & Lassiter, 2014; Lassiter & Goodman,                                                                                                                                           the all-state probability. The better fit of wRSA compared to
2013; Bergen et al., 2012; Kao et al., 2014) corresponding                                                                                                                                         RSA can be seen in the comparison of Figure 2 and Figure 3:
to the choice of state prior. That is, we posit that the prior,                                                                                                                                    in both cases, wRSA (light blue lines) predicts a much atten-
now P(s|w), depends on a “wonkiness” variable w, which de-                                                                                                                                         uated effect of the prior compared to regular RSA (dark blue
termines if it is the “usual” prior for this domain or a more                                                                                                                                      lines), in line with the empirical data. Furthermore, wRSA
generic back-off prior that we take to be uniform:                                                                                                                                                 does not make either of the problematic predictions identified
                          (                                                                                                                                                                        earlier for regular RSA.
                            1          if w
                 P(s|w) ∝                                                                                                                                                                             These results are encouraging: wRSA is able to account
                            Pusual (s) if not w
                                                                                                                                                                                                   for the qualitative and quantitative departures of participants’
   This inferred prior is used in both the literal and pragmatic                                                                                                                                   behavior from RSA, with respect to the effect of the prior. Is
listeners, indicating that it is taken to be common ground.                                                                                                                                        this because listeners are actually inferring that the world is
However, the w variable is reasoned about only by the prag-                                                                                                                                        unusual from an utterance like Some of the marbles sank? The
matic listener, which captures the idea that it is an inference                                                                                                                                    wRSA model makes predictions about the probability that a
the pragmatic listener makes about which assumptions are ap-                                                                                                                                       given world is wonky after observing an utterance; see Fig-
propriate to the conversation. Using the notation of the earlier                                                                                                                                   ure 4 for predicted wonkiness probabilities for uall , unone , and
modeling section:                                                                                                                                                                                  usome using the optimal P(w) and λ parameters from fitting
                                                                                                                                                                                                   wRSA to the Exp. 2 data. Note the U-shaped curve, in which
              PL0 (s|u, w) ∝ [[u]](s) · P(s|w)                                                                                                         (4)
                                                                                                                                                                                                   the world is judged wonky if usome is used in worlds with ex-
              PS1 (u|s, w) ∝ exp(λ ln PL0 (s|u, w))                                                                                                    (5)                                         treme priors. We can test these predictions directly by simply
              PL1 (s, w|u) ∝ PS1 (u|s, w) · P(s|w) · P(w)                                                                                              (6)                                         asking participants whether the situation is normal.
                                                                                                                                                                                             551

    Predicted posterior wonkiness probability                                                                                                                                        Mean empirical wonkiness probability
                                                                                                                                                                                                                        1.00
                                            1.00                                                                                                                                                                               ●    ●
                                                                                                                                                                                                                                   ●
                                                                                                                                                                                                                                        ●
                                                                                                                                                                                                                                   ●
                                                     ●
                                                         ●
                                                                                                                                                                                                                        0.75                ●
                                            0.75         ●
                                                          ●       ●
                                                                                                                                                                                                                                                                    ●
                                                                  ●                                                                                                                                                                         ●                                                                                                            ●
                                                              ●                           ●                                                                                                                                                                                                                                                         ●
                                                                                                               ●
                                                                                                      ●                 ●                    ●●●●
                                                                                                                                                ●
                                                                                                                                                 ●●●
                                                                                                                                                 ●
                                                                                                                                                ● ●
                                                                                                                                                   ● ●
                                                                                                                                                   ●   ●●
                                                                                                                                                   ●●●●●
                                                                                                                                                         ●●   Quantifier                                                                            ●
                                                                                                                                                                                                                                                                                                                                        ●       ●
                                                                                                                                                                                                                                                                                                                                                             Quantifier
                                                                                                          ●                              ●●● ● ●●
                                                                                                                                               ●
                                                                                                                                     ●●●                                                                                                                                                                                                        ●       ●●
                                                                      ●
                                                                          ●           ●                       ● ● ●
                                                                                                                            ●
                                                                                                                              ● ●
                                                                                                                                 ●
                                                                                                                                       ● ●
                                                                                                                                         ● ●●                  ●
                                                                                                                                                                 Some                                                                                                                                                                          ●
                                                                                                                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                                                                                                                Some
                                            0.50
                                                                              ●
                                                                                  ●           ●                   ●●        ● ●●
                                                                                                                                   ●
                                                                                                                                   ●●
                                                                                                                                                                                                                        0.50                    ●
                                                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                                                                        ●                                   ●               ●
                                                                                                                                                                                                                                                                                                                                             ● ●
                                                                                                                                                                                                                                                                                                                                            ● ●
                                                                          ●           ●
                                                                                      ●
                                                                                          ●       ●● ●
                                                                                                       ●       ●●
                                                                                                                ●
                                                                                                                   ●
                                                                                                                    ●
                                                                                                                                    ●
                                                                                                                                                                 All                                                                                                                                                                ●
                                                                                                                                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                                                                                                                                         ● ● ●
                                                                                                                                                                                                                                                                                                                                          ●● ●
                                                                                                                                                                                                                                                                                                                                                                All
                                                                                                                                                                                                                                                                                                                                          ●
                                                                                                                                                                 None                                                                                                                       ●
                                                                                                                                                                                                                                                                                                                  ●                 ●
                                                                                                                                                                                                                                                                                                                                      ●●        ●
                                                                                                                                                                                                                                                                                                                                                                None
                                                                                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                                                ●                            ●                ●             ●              ●● ●
                                                                                                                                                                                                                                                                                                                                                         ●
                                                                                                                                                                                                                                                        ●                   ●                ●                                  ●       ● ● ● ●
                                                                                                                                                                                                                                                                        ●           ●                             ●
                                                                                                                                                                                                                                                                                                              ●            ●●
                                            0.25                                                                                                                                                                        0.25                        ●
                                                                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                                                                                                 ● ●●
                                                                                                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                                                                                                            ●                       ●
                                                                                                                                                                                                                                                                    ●                                     ●                ● ●
                                                                                                                                                                                                                                                            ●                   ●                     ●               ●●
                                                                                                                                                                                                                                                                                                                      ●              ●
                                                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                                                            ●
                                                                                                                                                                                                                                                                                             ●
                                            0.00                                                                                                                                                                        0.00
                                                     1            3               5               7                9          11           13           15                                                                     1            3               5               7                    9                11                13                  15
                                                                  Prior mean number of objects                                                                                                                                              Prior mean number of objects
Figure 4: For each item, predicted wonkiness probability af-                                                                                                                     Figure 5: For each item, mean empirical wonkiness probabil-
ter observing an utterance (uall , unone , usome ), as a function of                                                                                                             ity after observing an utterance (uall , unone , usome ), as a func-
the prior expected number of affected objects.                                                                                                                                   tion of the prior expected number of affected objects.
                                                              Experiment 3: wonkiness                                                                                            be literally true, but it is not useful in the sense of providing
Exp.                                            37
         measured participants’ beliefs in world wonkiness                                                                                                                       the listener new information. For comparison to the compre-
after observing the scenarios and utterances from Exps. 2.                                                                                                                       hension data fit, the model’s MSE for empirical wonkiness
                                                                                                                                                                                 probability predictions, using the best parameters from fitting
Participants, procedure and materials We recruited 60                                                                                                                            the model to the comprehension data, was 0.07.
participants over Mechanical Turk.
   The procedure and materials were identical to those of                                                                                                                                                                               Discussion and conclusion
Exps. 2a and 2b, with the exception of the dependent mea-
sure. Rather than providing estimates of what they believed                                                                                                                      We have shown that listeners’ world knowledge, in the form
the world was like, participants were asked to indicate how                                                                                                                      of prior beliefs, enters into the computation of speaker mean-
likely it was that the objects (e.g., the marbles) involved in                                                                                                                   ing in a systematic but subtle way. The effect of the prior on
the scenario were normal objects, by adjusting a slider that                                                                                                                     interpretation was much smaller, and qualitatively different,
ranged from definitely not normal to definitely normal.                                                                                                                          than predicted by a standard Bayesian model of quantifier in-
                                                                                                                                                                                 terpretation (RSA). This suggests that in certain situations,
Results The extreme ends of the sliders were coded as 1                                                                                                                          listeners revise their assumptions about relevant priors as part
(definitely not normal, i.e., wonky) and 0 (definitely normal,                                                                                                                   of the computation of speaker meaning. Indeed, in the cases
i.e., not wonky). We interpret the slider values as probability                                                                                                                  where the largest deviations from RSA obtained, participants
of world wonkiness. Mean wonkiness probability ratings are                                                                                                                       also judged the world to be unusual. Extending RSA with a
shown in Figure 5 and closely mimic wRSA’s predictions (see                                                                                                                      lifted wonkiness variable that captures precisely whether lis-
Figure 4). For uall and unone , increasing prior expectation of                                                                                                                  teners think the world is unusual, and allows them to back off
objects exhibiting the effect resulted in a fairly linear decrease                                                                                                               to a uniform prior (i.e., ignore entirely their previously held
and increase in the probability of wonkiness, respectively. For                                                                                                                  beliefs about the world), provided a good fit to the empiri-
usome , the pattern is somewhat more intricate: probability of                                                                                                                   cal wonkiness judgments and dramatically improved the fit
wonkiness initially decreases sharply, but rises again in the                                                                                                                    to participants’ comprehension data. This model constitutes
upper range of the prior expected value.                                                                                                                                         the first attempt to explicitly model the quantitative effect of
   Qualitatively, the model captures both the linear increase                                                                                                                    world knowledge and its defeasibility on pragmatic utterance
and decrease in wonkiness probability for uall and unone , re-                                                                                                                   interpretation and raises many interesting questions.
spectively. Importantly, it also captures the asymmetric U-                                                                                                                         In one sense the revision of beliefs in the wRSA listener is
shaped wonkiness probability curve displayed by usome . In-                                                                                                                      standard Bayesian belief updating with respect to a complex
tuitively, this shape can be explained as follows: for very                                                                                                                      prior; however it is not the simple belief update of a flat or
low probability events, it is surprising to learn that such an                                                                                                                   hierarchical prior, because the different aspects of prior be-
event took place (which is what is communicated by usome ),                                                                                                                      lief (i.e. P(w) and P(s|w)) interact in complex ways with the
so wonkiness is high. For medium probability events, learn-                                                                                                                      listener’s assumptions about the speaker. As a result, an odd
ing that this event took place is not very surprising, so wonk-                                                                                                                  utterance can lead the listener to update their own view of w;
iness is relatively low. For high probability events, usome may                                                                                                                  this in turn impacts both their own prior over states and what
   7 See the experiment at http://cocolab.stanford.edu/cogsci2015/                                                                                                               prior they believe the speaker believes they are using—an odd
wonky/wonkiness/sinking-marbles-normal.html                                                                                                                                      utterance leads the listener to re-evaluate common ground.
                                                                                                                                                                           552

This is reminiscent of linguistic theories of presupposition                  fect language use. In Proceedings of the thirty-fourth an-
accommodation (Lewis, 1979; Stalnaker, 1973, 1998). It will                   nual conference of the cognitive science society.
be interesting to further explore the relation of the wRSA ap-              Degen, J., Franke, M., & Jäger, G. (2013). Cost-Based Prag-
proach to presupposition.                                                     matic Inference about Referential Expressions. In Proceed-
   Throughout this paper we discussed wonkiness as an at-                     ings of the 35th annual conference of the cognitive science
tribute of the world, yet empirically we elicited wonkiness                   society.
judgments about objects involved in events. This raises the                 Degen, J., & Goodman, N. D. (2014). Lost your marbles?
question of what exactly listeners are revising their prior be-               The puzzle of dependent measures in experimental prag-
liefs about: objects, events, the speaker’s beliefs, or the way               matics. In P. Bello, M. Guarini, M. McShane, & B. Scas-
the speaker uses language? Relatedly, we used a uniform                       sellati (Eds.), Proceedings of the 36th annual conference of
prior distribution as the alternative prior when the listener be-             the cognitive science society.
lieves the world is wonky. One could imagine various more                   Fine, A. B., Jaeger, T. F., Farmer, T. F., & Qian, T. (2013).
flexible alternatives. For instance, listeners may make min-                  Rapid expectation adaptation during syntactic comprehen-
imal adjustments to their prior knowledge, or alternatively,                  sion. PLoS ONE, 8(10).
may prefer extreme priors that rationalize the utterance once               Frank, M. C., & Goodman, N. D. (2012). Predicting prag-
they have discounted the usual priors. Future research should                 matic reasoning in language games. Science, 336, 998.
investigate the options listeners consider when their world                 Franke, M. (2011). Quantity implicatures, exhaustive inter-
knowledge must be revised to accommodate an utterance.                        pretation, and rational conversation. Semantics and Prag-
   This work also has methodological implications: re-                        matics, 4(1), 1–82.
searchers working in the field of experimental semantics and                Geurts, B. (2010). Quantity Implicatures. Cambridge: Cam-
pragmatics would be well served to take into account the ef-                  bridge Univ Press.
fect of ‘odd’ items, prior beliefs, and interactions between the            Goodman, N. D., & Lassiter, D. (2014). Probabilistic
two.8 In particular, if the attempt to design uniform stimuli                 Semantics and Pragmatics: Uncertainty in Language and
across conditions yields odd utterances in some conditions,                   Thought. Handbook of Contemporary Semantics.
we predict that participants will respond by revising their                 Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge
prior beliefs in ways that can be unpredictable. That is, we                  and implicature: modeling language understanding as so-
expect unpredictable interaction effects between stimuli and                  cial cognition. Topics in Cognitive Science, 5(1), 173–84.
conditions. This is likely to inflate or compress potential ef-             Hayfield, T., & Racine, J. S. (2008). Nonparametric econo-
fects of an experimental manipulation.                                        metrics: The np package. Journal of Statistical Software,
   Concluding, this work exemplifies the importance and util-                 27(5).
ity of exploring the detailed quantitative predictions of for-              Jaeger, T. F. (2010). Redundancy and reduction: speakers
mal models of language understanding. Exploring the prior                     manage syntactic information density. Cognitive Psychol-
knowledge effects predicted by RSA led us to understand                       ogy, 61(1), 23–62.
better the influence of world knowledge and its defeasibility               Kao, J., Wu, J., Bergen, L., & Goodman, N. D. (2014). Non-
on pragmatic interpretation. Listeners have many resources                    literal understanding of number words. Proceedings of the
open to them when confronted with an odd utterance, and re-                   National Academy of Sciences of the United States of Amer-
construing the situation appears to be a favorite.                            ica, 111(33), 12002–12007.
                                                                            Lassiter, D., & Goodman, N. D. (2013). Context, scale struc-
                      Acknowledgments                                         ture, and statistics in the interpretation of positive-form ad-
This work was supported by ONR grant N00014-13-1-0788                         jectives. In Proceedings of salt 23 (pp. 587–610).
and a James S. McDonnell Foundation Scholar Award to                        Lewis, D. (1969). Convention. A Philosophical Study. Har-
NDG, an SNF Early Postdoc.Mobility Award to JD, and an                        vard University Press.
NSF Graduate Fellowship to MHT.                                             Lewis, D. (1979). Scorekeeping in a Language Game. Jour-
                                                                              nal of Philosophical Logic, 8(1), 339–359.
                            References                                      Li, Q., & Racine, J. (2003). Nonparametric estimation of
                                                                              distributions with categorical and continuous data. Journal
Benz, A., & Gotzner, N. (2014). Embedded implicatures re-
                                                                              of Multivariate Analysis, 86, 266–292.
   visited: Issues with the Truth-Value Judgment Paradigm. In
                                                                            Russell, B. (2012). Probabilistic Reasoning and the Compu-
   J. Degen, M. Franke, & N. D. Goodman (Eds.), Proceed-
                                                                              tation of Scalar Implicatures. Unpublished doctoral disser-
   ings of the formal & experimental pragmatics workshop,
                                                                              tation, Brown University.
   european summer school for language, logic and informa-
                                                                            Stalnaker, R. (1973). Presuppositions. Journal of Philosoph-
   tion (esslli) (pp. 1–6). Tübingen.
                                                                              ical Logic, 2(4), 447–457.
Bergen, L., Goodman, N. D., & Levy, R. (2012). That’s
                                                                            Stalnaker, R. (1998). On the Representation of Context. Jour-
   what she (could have) said: How alternative utterances af-
                                                                              nal of Logic, Language and Information, 7(1), 3–19.
    8 For an in depth discussion of this issue in syntactic processing,
see, e.g., Jaeger (2010); Fine, Jaeger, Farmer, and Qian (2013).
                                                                        553

