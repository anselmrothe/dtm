                                  A model for full local image interpretation
                                     Guy Ben-Yosef1 (guy.ben-yosef@weizmann.ac.il))
                                          Liav Assif1 (liav.assif@weizmann.ac.il)
                                             Daniel Harari1,2 (hararid@mit.edu)
                                   Shimon Ullman1,2 (shimon.ullman@weizmann.ac.il)
                        1
                          Department of Computer Science, Weizmann Institute of Science, Rehovot, Israel
                 2
                   Center for Brains, Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA
                            Abstract                               corresponding to object parts in the scene, such as 'tail' or
                                                                   'tip of the ear', unlike 'curved contour' or 'dark region'
   We describe a computational model of humans' ability to
                                                                   describing image features. We approach the daunting
provide a detailed interpretation of a scene’s components.
                                                                   problem of full accurate object interpretation by
Humans can identify in an image meaningful components
                                                                   decomposing the full object or scene image into smaller,
almost everywhere, and identifying these components is an
                                                                   local, regions containing recognizable object components.
essential part of the visual process, and of understanding the
                                                                   As exemplified in Fig. 1B, in such local regions the task of
surrounding scene and its potential meaning to the viewer.
                                                                   full interpretation is still possible, but more tractable, since
Detailed interpretation is beyond the scope of current
                                                                   the number of semantic recognizable components is highly
models of visual recognition. Our model suggests that this is
                                                                   reduced. As will be shown, reducing the number of
a fundamental limitation, related to the fact that existing
                                                                   components plays a key factor in effective interpretation. At
models rely on feed-forward but limited top-down
                                                                   the same time, when the interpretation region becomes too
processing. In our model, a first recognition stage leads to
                                                                   limited, observers can no longer interpret or even identify its
the initial activation of class candidates, which is
                                                                   content, as illustrated in Fig. 1C. We therefore apply the
incomplete and with limited accuracy. This stage then
                                                                   interpretation process to local regions that are small, yet
triggers the application of class-specific interpretation and
                                                                   interpretable on their own by human observers.
validation processes, which recover richer and more
                                                                             The model proceeds by identifying within the local
accurate interpretation of the visible scene. We discuss
                                                                   region a familiar configuration of semantic features learned
implications of the model for visual interpretation by
                                                                   from examples. This configuration is found by identifying
humans and by computer vision models.
                                                                   the participating components, as well as their arrangement,
    Keywords: Image understanding; visual object
                                                                   which is defined by spatial relations among them. A central
interpretation; objects and parts recognition; top-down
                                                                   conclusion from the model is that full interpretation of even
processing;
                                                                   a local region at a human performance level depends on the
                                                                   use of relations that are currently not used by state-of-art,
                   Goal and introduction                           feed-forward image recognition models. These relations can
Computational models of object recognition and                     be relatively complex, relying for example on computing
categorization have made significant advances in recent            local continuity, grouping and containment. We conclude
years, demonstrating consistently improving results in             from the model that the interpretation process is likely to be
recognizing thousands of natural object categories in              local and involve top-down processing. We propose a
complex natural scenes. However, in a number of key areas,         general scheme in which the interpretation process is
existing models are far from approaching object recognition        applied initially to local and interpretable regions by
by the human visual system. A major limitation is the              combining bottom-up and top-down extraction of features
inability of current models to provide a detailed                  and relations, and can subsequently be integrated and
interpretation of a scene’s components, which is an integral       expanded to larger regions of interest.
part of human recognition. Models may label for instance                     The remaining of the paper proceeds as follows. In
an image region as containing a horse, while humans                the next section we briefly review previous work related to
looking at the image will naturally identify meaningful            image interpretation. Section 3 presents a model for full
components almost everywhere, e.g. the right eye, the left         interpretation of local regions, with the goal of achieving
ear, the mouth, mane, the right leg, the knee, the hoof, the       interpretation at the level of detail obtained by humans in
tail, harness etc.                                                 these regions. Section 4 describes experimental results to
          Identifying detailed components is an essential part     evaluate our model, and we conclude in Section 5 by
of the visual process, leading to the understanding of the         discussing possible implications to our understanding of
surrounding scene and its potential meaning to the viewer.         visual recognition and its mechanisms, and to the
However, interpretation is a difficult task since it requires      development of models and systems with visual capacities
the detection and localization of many semantic object parts,      that are closer to human perceptual capacities.
which can amount to dozens or even hundreds in a single
image (Fig. 1A,B). By 'semantic' we mean components
                                                               220

                                                                                                           Ear
                                                                                                                          Mane
                                                                                                                          contour
                                                                               Upper
                                                                               head
                                                                               contour
                                                                                                                         Eye
                                                                             Nostril                                       Lower-
                                                                                                                           neck
                                                                                                                           contour
                                                                               Mouth              lower head
                                                                                                  contour      Neck
                                                                                                        B                                             C
                               A
Figure 1. (A). A natural image in which humans can identify dozens of semantic features, arrows point to a subset of the identified features. (B). A local
region and a set of features identified consistently by human observers; the number of semantic recognizable components is highly reduced. (C). When the
local region becomes too limited, observers can no longer interpret or even identify its content.
                                                                                   dictionary of semantic contours in a supervised manner, via
        Image interpretation in previous work                                      human annotations of object images. The works above
Schemes related to object interpretation have been                                 suggest several techniques to represent contours, and
suggested under different names, including object parsing                          interpretation is obtained by matching contour in the image
(e.g., Kokkinos & Yuille, 2009; Si & Zhu, 2013), or object                         to contours in the dictionary, by modeling contour
grammar (e.g., Zhu et al., 2009), Fine-Grained recognition                         properties (e.g., curvature) and simple relations between
(e.g., Zhang et. al., 2014), and semantic segmentation (e.g.,                      contours, typically relative and global locations. Our
Chen. et al., 2015). Object parsing and object grammar                             approach extends such schemes by detecting points,
models often refer to probabilistic frameworks that                                contours, and regions of interest, that are semantic in the
gradually decompose an object into simpler image                                   sense that humans can consistently recognize them in an
structures. A recent representative work (Si & Zhu, 2013)                          image. As a result, we also use a significantly extended set
describes a dictionary of object parts, represented by                             of relations between the different types of feature primitives.
measures for shape, texture, and color. The parts primitives                          Fine-Grained recognition also aims to perform image
are learned in an unsupervised way and are not necessarily                         interpretation by finding attributes and sub-category
semantic in the sense described above. The relations                               discrimination of the object in scene and its semantic parts.
between them are modeled by basic geometric structure                              A recent example (Vedaldi et al. 2014) focuses on an
indicating global and relative locations of the parts.                             aircraft benchmark. The scheme modeled aircrafts by a few
   Some so called ‘part-based models’ provide another                              semantic parts, e.g., the airplane nose, tail, wing, etc. and
version of object interpretation at a coarser level (e.g.,                         attributes of the plane or its parts such as ‘does the plane
Deformable Part Model (DPM) (Felzenszwalb et al., 2010;                            have engine on tail?’, or ‘is it a propeller-plane?’, etc.
Zhang et al., 2014). Such algorithms represent the object                             Another form of image interpretation comes from work
image by a set of part region primitives (e.g., HoG                                on so-called semantic segmentation, which attempts to make
representation (Dalal & Triggs, 2005) or Convolutional                             precise localization of the object surfaces in the scene. For
Networks representation (Girshick et al.,2014)), which are                         example, a recent algorithm (Chen. et al 2015) based on
learned in an unsupervised manner, and model basic                                 features from the top layer of a 16-layers convolutional
geometric relations between them, such as relative location                        network, can identify the majority of pixels belonging to a
and distance. Such part-based models have proved highly                            horse surfaces in the PASCAL benchmark (Everingham et
useful for object recognition, however, the interpretation                         al., 2010), but it is far from predicting its precise boundary
they provide is coarse and less localized compared with the                        localization and detailed components. Our work differs from
current scheme (e.g. 'tail', 'wing' and 'body' for an airplane).                   the approaches above since it aims to provide ‘full’
   Several works have attempted to interpret a visual object                       interpretation of the input image, namely, to localize all
by its contour features. Such schemes suggest a dictionary                         object parts that humans can interpret, and to learn to
of informative contour fragments for the object, which is                          identify local configurations of these semantic features. We
often learned in an unsupervised manner and often do not                           next turn to describe our interpretation scheme, and
have a semantic meaning (Opelt et al., 2006; Arandjelovic                          experiments done for its evaluation.
& Zisserman, 2011; Ferrari et al., 2008). A more recent
work (Hariharan et al., 2011) also suggested building a
                                                                              221

              Interpretation of local regions                                A. Annotations for learning:
   Our goal is to describe the detailed semantic structure of a
local region. More specifically, given a recognizable and
interpretable local region in an object image, we aim to
output the full semantic structure humans can find in this
region. A natural choice for a formal description of semantic
structures includes a set of primitive features and a set of
relations defined over them. The primitive features are                      B. Input test images:
semantic components of the local region that are
recognizable by observers (as in Fig. 1B). In a correct
interpretation, the components are arranged in certain
configurations, which are naturally defined by relations                      C. Final output:
between components. The use of primitive components and
relations between them is a common approach for modeling
structured representations in areas of cognition and artificial
intelligence (Russell & Norvig, 2005(.
   The semantic features to be identified by the model, e.g.
'ear', 'eye', 'neck', were supplied to the model using features
                                                                         Figure 2. (A). Local recognizable images in which recognizable
which were consistently labeled in a prior experiment in                 components are annotated. These components are the model primitives,
Mechanical Turk (Crump et al., 2013). The ultimate task is               which appear in three types: points, contours, and regions. From a set of
then to identify these components in novel images by                     annotated images, we learn the set of model relations. (B). Given a novel
learning the image features and the relevant relations among             image, our target is to localize the set of primitives found in (A). (C).
                                                                         The interpretation scheme searches for combinations of primitives in the
them from examples. Our current model is not fully                       image, and output as interpretation the combination that matches best
automatic, but relies on a set of spatial relations identified in        the learned set of relations.
previous works, and on an analysis of so-called 'hard                 relations are taken from a set of possible relations to
negative' examples, described in the next section.                    compute (see below how this set is obtained). The relations
                                                                      identified in a given image are represented by a vector,
Scheme overview                                                       where each component represents a specific relation. These
Input: a local region to model Our interpretation scheme              vectors are then fed into a random forest classifier, which
begins by selecting a local recognizable object region, and           models the expected relations for correct primitive
getting from the Mechanical Turk a target set of semantic             configurations in positive examples. We repeat this learning
primitives to identify in it (e.g., Fig. 1B, Fig. 2A). The            process for several iterations; at each iteration we add
Mechanical Turk task required the naming of a certain                 negative interpretation examples that obtained high scores
highlighted object part. Consistent naming was examined               in the previous iteration.
and used to define the target interpretable components.
                                                                      Interpretation of novel image Given a raw novel image
Generating interpretation examples for learning Next,                 (e.g., Fig 2B), the scheme automatically searches for
we produced for learning a set of annotated images, in                multiple combinations of image parts to serve as primitives,
which the semantic features are marked manually (with                 computes a relation vector for each combination, and by the
automatic refinement). The final goal is to take a new image          learned classifier produces a score for the candidate
as an input, and mark in it all the detected semantic features        combination. This search is feasible due to the small number
(e.g., Figs. 2C,4,5). Having a set of positive interpretation         of primitives in the local region. It finally returns the highest
examples, we next search for negative interpretation                  scoring combination as the interpretation of the input image
examples. The negative examples are collected by finding              (e.g., Fig. 2C).
non-class images that are as similar as possible to true class
instances. For this purpose, we trained a detector based on           Primitives
Bag of visual Words (Csurka et al., 2004) using the recent               To capture the recognized internal components fully as
popular VLAD version (Jégou et al., 2010) with positive               perceived by humans, our primitives are divided into three
local region examples, and then applied these detectors to            types, 2-D (regions), 1-D (contours), and 0-D (points). For
random images from known image benchmarks (PASCAL).                   example, a point-type primitive describes the eye in the
The negative examples we use are non-class examples that              horse head model (Fig. 2A, left panel), and a contour-type
received high detection scoring, and are therefore more               primitive describes borders such as the sides of the tie in the
confusable or ‘hard’ negatives for the detectors.                     local region describing the man-in-suit model (Fig 2A, mid
                                                                      panel). Example sets of primitives for local regions are
Learning relations of correct interpretations                         shown in Fig. 2A.
For each positive and negative example we compute a set of
relations that exist between the annotated components. The
                                                                  222

          A. ‘Hard’ non-class example                            B. Class examples                                      C. Informative relation
                                     vs.
Figure 3. Inferring informative relations between internal components. (A). A ‘hard’ negative example, from Bag-of-Words classifier (B). Positive
examples. We search for a relation between primitives that exists in the positive but not in the negative instance. An informative relation in this and other
examples was contour ‘bridging’: the upper-head contour primitive (red), and the lower-head contour primitive (yellow) are linked by an edge through the
mouth region primitive.
   All three types of primitives have natural relations to both                  disconnected contours can be ‘bridged’ (i.e., linked by an
visual perception and computer vision models. Contour-type                       edge in an edge map used by the model) consistent with the
primitives have been linked to object perception from early                      way they are connected in the positive image (see
studies (e.g., Attneave, 1954) and to explicit representation                    illustration in Fig. 3C). Our final library of relations
in the visual cortex (e.g., Pasupathy & Connor, 1999).                           includes unary relations (properties), binary relations, and
Image contours are highly informative features that often                        relations among three or more primitives. Relations range
correspond to meaningful object components. It proved                            from simpler interactions such as relative location, to more
difficult to learn and extract meaningful contours in                            compound interactions such as continuity, containment, and
complex images, and consequently contours turned less                            bridging mentioned above.
popular in recent recognition schemes, but in the current
scheme they are more efficiently handled at the local region                                       Experimental evaluation
level. Point-primitive in our model include several types (in                    To evaluate our model and the library of derived relations,
particular, high curvature points and local intensity extrema                    we performed experiments to assess the interpretation of
points), based on their use in both visual perception and                        novel images, by matching assignment of primitives to
computer vision (e.g., Attneave, 1954; Lindeberg, 1998;                          human annotations over multiple examples. To get positive
Lowe, 2004). Region-type descriptors proved highly                               examples, we randomly collected full-object images from
efficient in computational models for identifying object                         known data sets (Flicker, Google images, ImageNet –
regions under different viewing conditions (e.g., Dalal and                      Russakovsky et al., 2014), and then manually extracted from
Triggs, 2005, Felzenszwalb et al., 2010) and proved useful                       them the local region showing a particular object part for
in the current model as well.                                                    interpretation. We used local regions containing a horse
                                                                                 head, a man in tie and suit, and a ship. These regions and the
Relations                                                                        primitives defined for them are shown in Fig. 2. A large-
   The set of relations used in our model was composed                           scale experiment was done to evaluate the horse-head
from two sources. One source consists of relations coming                        model, in which we collected 740 positive local image
from prior computer and human vision modeling, such as                           examples, from which we randomly selected 120 for
proximity of points, contours and regions, or continuity and                     training, and the rest used for testing. Negative set included
parallelism of contours. The second source includes                              25000 images. Our experiments for the man-in-suit and ship
relations inferred from our analysis of ‘hard’ non-class                         local regions contained 60 positive examples, and 6000
examples that were confused as positive by state-of-art                          negative examples.
detectors. More specifically, we have used the following                            To assess the extent of ‘full’ interpretation the model
iterative procedure:                                                             produces for novel images at a fine detail level, we
   1.     Identify a ‘hard’ negative example that received                       manually annotated the semantic components recognized by
high recognition score by region part detector based on Bag                      human via Mechanical Turk for each tested positive
of visual Words, e.g., as in Fig 3A.                                             example. We then automatically matched the ground truth
   2.     Identify a property or relation which exists in the                    annotated components to the interpretation output by
positive set but not in the negative example (e.g., Fig 3C).                     correspondence criteria based on normalized Euclidean
   It is worth noting that identifying missing property or                       distance: for point, location distance; for contour, distances
relations in step 2 becomes practical when analyzing small                       between ordered sample points; for regions, distance
local regions, since the amount and complexity of primitives                     between centers.
and relations is significantly reduced compared to standard                         Since our model is novel in terms of producing full
object image. This learning process is in part manual; in                        interpretation, it cannot be compared directly in terms of
human vision, it may come from a combination of                                  completeness and accuracy with existing models. However,
evolutionary and learned components, see discussion. The                         we made our set of annotations publically available and we
relations coming from the second source include cover of a                       provide baseline to match its results. Our results show an
point by contour, containment of a contour or point in                           average matching error of 0.2442 normalized Euclidean
region, a contour that ‘ends-in’ a region, and whether two                       distance over all eight primitive and 620 test images used
                                                                           223

for evaluating the horse head model. Example interpretation
results for three models of Fig. 2 are presented in Fig. 4.A
and in Fig. 5. Additional comparison measures can be used
to assess full interpretation, which are left for future
research.
  To assess the role of complex relations, we compared our
results to a version that uses the same interpretation scheme
described above, but with a library containing unary and
binary relations similar to those used in previous object
                                                                                                           A
interpretation schemes (as reviewed above), i.e., based on
unary descriptions for shape and texture, and binary for
relative location. In this reduced library we ‘turned off’
more complex relations from our analysis such as ‘ends in’
or ‘bridging’. We show in Fig. 4A,B ten example pairs of
the same image with two interpretations, full vs. reduced.
Images were chosen randomly from our test set such that
both schemes produced high interpretation score for them.                                                  B
Yet, the produced interpretations are perceptually different,
and interpretation by the full-set scheme is significantly
more precise. A comparison (not detailed here) shows that
the fraction of primitives correctly localized by the full set
scheme is increased by a factor of 1.45 than the reduced set
version. An illustration of ten randomly selected images is
shown in Fig. 4A,B.
                         Discussion                                                                         C
                                                                         Figure 4. (A). Interpretation results for horse-head region for ten
Local interpretation: Results of the current study show               class examples. Here the scheme uses the full set of relations. (B).
that a detailed and well-localized interpretation can be              Interpretation results of the same images in (A), by a scheme using a
                                                                      reduced set of relation (see text for details). There are 6 mis-localization
obtained already from a limited image region, which                   of primitives in (A) compared to 29 in (B). (C). Top-ranked
contains a small number of elements, by using an                      Interpretation results for five non-class examples.
appropriate set of relations. We suggest from the model and
our experiments that efficient interpretation can start at the
level of local regions, which can subsequently be integrated
to produce more global interpretation of larger regions of
interest.
Top-scoring non-class detections: of our model can be
used in the future for two purposes. First, for validation:
top-ranked false detections by bottom-up classification
models often have low interpretation score (as in Fig. 4C),           Figure 5. Examples of interpretation results for the man-in-suit and ship
and therefore will be rejected by the interpretation stage.           models described in Fig. 2.
Second, we expect negative examples of high interpretation         are insufficient for a reliable interpretation. More complex
score to be perceptually similar to positive ones. We              relations, such as the ’bridgeability’ of contours c i ,cj , or a
propose therefore to test psychophysically the agreement           contour ci ending-in region rj, contribute significantly to
between human errors and errors made by models, with and           successful interpretation. In learning to interpret a new
without interpretation.                                            configuration, the set of candidate relations will be
                                                                   examined, and the informative ones for the task will be
A universal library of relations: The set of relations             incorporated in the model.
needed for human-level interpretation is at present
unknown. In this work we proposed a set starting from a            Implications for difficult visual tasks: It will be
collection of relations used in previous modeling as first         interesting to examine in future studies the role of full
approximation, and continued by adding relation candidates         interpretation in challenging visual tasks, which are beyond
from analysis of hard non-class examples. This initial pool        the scope of current computational theories, because they
could be refined in the future by additional examples,             depend on fine localization of object parts and the relations
leading ultimately to a universal set of useful interpretation     between parts, as illustrated in Fig. 6. Full interpretation of
relations. One finding of the current study is that simple         components at the level produced by the current model is
spatial relations, such as displacements between primitives,       likely to prove useful for dealing with the interpretation of
                                                               224

complex configurations arising in areas such as actions or
social interactions between agents.
Top-down processing: Our model suggests that the
                                                                     Figure 6. Full interpretation is useful for difficult visual tasks that are
relations required for a detailed interpretation are in part         challenging for current computational theories. Such tasks include
considerably more complex than spatial relations used in             recognizing actions in local object-agent configurations, e.g., ‘fixing a
current recognition models. They are also often class-               tie’ (left pair), ‘feeding a horse’ (right pair) only one image in each pair
specific, in the sense that a relation such as 'connected by a       is correct.
smooth contour' is applied to a small selected set of              Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014,
components in some of the models. This suggests a scheme             June). Rich feature hierarchies for accurate object
in which complex relations are computed at selected and              detection and semantic segmentation. In Computer Vision
class-specific locations. The recognition and interpretation         and Pattern Recognition, 2014 (pp. 580-587). IEEE.
process is naturally divided on this view to two main stages.      Google images: http://www.google.com/imghp.
The first is a bottom-up recognition stage, which may be           Hariharan, B., Arbeláez, P., Bourdev, L., Maji, S., & Malik,
similar to current high-performing computer vision models.           J. (2011, November). Semantic contours from inverse
This leads to the activation of objects models, which lacks          detectors. In Computer Vision (ICCV), 2011 IEEE
detailed interpretation. Activated models will then trigger          International Conference on (pp. 991-998). IEEE.
the application of top-down extraction of additional features      Jégou, H., Douze, M., Schmid, C., & Pérez, P. (2010, June).
and the computation of relevant relations to selected                Aggregating local descriptors into a compact image
components, resulting in detailed interpretation as well as          representation. In Computer Vision and Pattern
validation of the initial recognition stage.                         Recognition (CVPR), 2010 IEEE Conference on.
                                                                    Kokkinos, I., & Yuille, A. (2009, June). Hop: Hierarchical
                          References                                 object parsing. In Computer Vision and Pattern
Arandjelovic, R., & Zisserman, A. (2011). Smooth object              Recognition, 2009. CVPR 2009. IEEE Conference on.
  retrieval using a bag of boundaries. In Computer Vision          Lindeberg, T. (1998). Feature detection with automatic scale
  (ICCV), 2011 IEEE International Conference on.                     selection. International journal of computer vision, 30(2),
 Attneave, F. (1954). Some informational aspects of visual           79-116.‫‏‬
  perception. Psychological review, 61(3), 183.‫‏‬                   Lowe, D. G. (2004). Distinctive image features from scale-
Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., &             invariant keypoints. International journal of computer
  Yuille, A. L. (2014). Semantic Image Segmentation with             vision, 60(2), 91-110.‫‏‬
  Deep Convolutional Nets and Fully Connected CRFs.                Opelt, A., Pinz, A., & Zisserman, A. (2006). A boundary-
  In International Conference on Learning Representations,           fragment-model for object detection. In Computer Vision–
  ICLR 2015                                                          ECCV 2006 (pp. 575-588). Springer Berlin Heidelberg.
Crump, M. J., McDonnell, J. V., & Gureckis, T. M. (2013).          Pasupathy, A., & Connor, C. E. (1999). Responses to
  Evaluating Amazon's Mechanical Turk as a tool for                  contour features in macaque area V4. Journal of
  experimental behavioral research. PloS one, 8(3), e57410.‫‏‬         Neurophysiology, 82(5), 2490-2502.‫‏‬
Csurka, G., Dance, C., Fan, L., Willamowski, J., & Bray, C.        Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
  (2004, May). Visual categorization with bags of                    Ma, S., & Fei-Fei, L. (2014). Imagenet large scale visual
  keypoints. In Workshop on statistical learning in                  recognition challenge. arXiv preprint arXiv:1409.0575.‫‏‬
  computer vision, ECCV.                                           Russell, S., & Norvig, P. (2005). AI a modern approach.
 Dalal, N., & Triggs, B. (2005). Histograms of oriented            Si, Z., & Zhu, S. C. (2013). Learning and-or templates for
  gradients for human detection. In Computer Vision and              object recognition and detection. Pattern Analysis and
  Pattern Recognition, 2005. CVPR 2005.                              Machine Intelligence, IEEE Transactions, 35(9), 2190-9.
Everingham, M., Van Gool, L., Williams, C. K., Winn, J., &         Vedaldi, A., Mahendran, S., Tsogkas, S., Maji, S., Girshick,
  Zisserman, A. (2010). The pascal visual object classes             R., Kannala,... & Mohamed, S. (2014, June).
  (voc) challenge. International journal of computer vision,         Understanding objects in detail with fine-grained
  88(2), 303-338.‫‏‬                                                   attributes. In Computer Vision and Pattern Recognition
 Felzenszwalb, P. F., Girshick, R. B., McAllester, D., &             (CVPR), 2014 IEEE Conference on.
  Ramanan, D. (2010). Object detection with                         Zhang, N., Donahue, J., Girshick, R., & Darrell, T. (2014).
  discriminatively trained part-based models. Pattern                Part-based R-CNNs for fine-grained category detection.
  Analysis and Machine Intelligence, IEEE Transactions               In Computer Vision–ECCV 2014 (pp. 834-849). Springer .
  on, 32(9), 1627-1645.‫‏‬                                           Zhu, L., Chen, Y., & Yuille, A. (2009). Unsupervised
Ferrari, V., Fevrier, L., Jurie, F., & Schmid, C. (2008).            learning of probabilistic grammar-markov models for
  Groups of adjacent contour segments for object detection.          object categories. Pattern Analysis and Machine
  Pattern Analysis and Machine Intelligence, IEEE                    Intelligence, IEEE Transactions on, 31(1), 114-128.
  Transactions on, 30(1), 36-51.
Flickr: http://www.flickr.com/
                                                               225

