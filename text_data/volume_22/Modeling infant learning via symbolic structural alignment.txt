UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling infant learning via symbolic structural alignment
Permalink
https://escholarship.org/uc/item/8cr4z049
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Kuehne, Sven E.
Gentner, Dedre
Forbus, Kenneth D.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                      Modeling infant learning via symbolic structural alignment
                                                Sven E. Kuehne (skuehne@ils.nwu.edu)
                                        Department of Computer Science, Northwestern University
                                             1890 Maple Avenue, Evanston, IL 60201 USA
                                                  Dedre Gentner (gentner@nwu.edu)
                                           Department of Psychology, Northwestern University
                                              2029 Sheridan Rd., Evanston, IL 60201 USA
                                              Kenneth D. Forbus (forbus@ils.nwu.edu)
                                       Department of Computer Science, Northwestern University
                                             1890 Maple Avenue, Evanston, IL 60201 USA
                              Abstract                               set of sentences that consisted of entirely new words. Half of
                                                                     the test stimuli followed the same grammar as in the habitua-
   Understanding the mechanisms of learning is one of the cen-       tion phase; the other half followed the non-trained grammar.
   tral questions of Cognitive Science. Recently Marcus et al.       Marcus et al. found that the infants dishabituated signifi-
   showed that seven-month-old infants can learn to recognize
   regularities in simple language-like stimuli. Marcus proposed
                                                                     cantly more often to sentences in the non-trained pattern
   that these results could not be modeled via existing connec-      than to sentences in the trained pattern.
   tionist systems, and that such learning requires infants to be        Based on these findings Marcus et al. proposed that in-
   constructing rules containing algebraic variables. This paper     fants had learned abstract algebraic rules. They noted that
   proposes a third possibility: that such learning can be ex-       these results cannot be accounted for solely by statistical
   plained via structural alignment processes operating over         mechanisms that track transitional probabilities. They fur-
   structured representations. We demonstrate the plausibility of    ther argue that their results challenge connectionist models
   this approach by describing a simulation, built out of previ-     of human learning that use similar information, on two
   ously tested models of symbolic similarity processing, that       grounds: (1) the infants learn in many fewer trials than are
   models the Marcus data. Unlike existing connectionist simu-
                                                                     typically needed by connectionist learning systems; (2) more
   lations, our model learns within the span of stimuli presented
   to the infants and does not require supervision. It can handle    importantly, the infants learn without feedback. In particular,
   input with and without noise. Contrary to Marcus’ proposal,       Marcus et al. demonstrated that a simple recurrent network
   our model does not require the introduction of variables. It      with the same input stimuli could not model this learning
   incrementally abstracts structural regularities, which do not     task.
   need to be fully abstract rules for the phenomenon to appear.        In response, several connectionist models have attempted
   Our model also proposes a processing explanation for why in-      to simulate these findings. Unfortunately, all of them to date
   fants attend longer to the novel stimuli. We describe our         include extra assumptions that make them a relatively poor
   model and the simulation results and discuss the role of struc-   fit for the Marcus et al experiment. For example, Elman
   tural alignment in the development of abstract patterns and
                                                                     (1999; Seidenberg & Elman, 1999) use massive pre-training
   rules.
                                                                     (50,000 trials) to teach the network the individual stimuli.
                                                                     More importantly, they turn the infants’ unsupervised learn-
                          Introduction
                                                                     ing task into a supervised learning task by providing the
Understanding the mechanisms of learning is one of the cen-          network with external training signals. Other models tailored
tral questions of cognitive science. Recent studies (Gomez &         to capture the data of the study seem unlikely to be applica-
Gerken, 1999; Marcus, Vijayan, Rao & Vishton, 1999) have             ble to other similar cognitive tasks (Altmann & Dienes,
shown that showed that infants as young as seven months              1999). Using a localist temporal binding scheme, Shastri
can process simple language-like stimuli and build generali-         and Chang (1999) model the infant results without pretrain-
zations sufficient to distinguish familiar from unfamiliar           ing and without supervision, but still require an order of
patterns in novel test stimuli. In Marcus et al’s study, the         magnitude more exposure to the stimuli than the infants re-
stimuli were simple ‘sentences,’ each consisting of three            ceived.
nonsense consonant-vowel ‘words’ (e.g., ‘ba’, ‘go’, ‘ka’).               We propose a third alternative. There is evidence that
All habituation stimuli had a shared grammar, either ABA or          structural alignment processes operating over symbolic
ABB. In ABA-type stimuli the first and the third word are            structured representations participate in a number of cogni-
the same: e.g, ‘pa-ti-pa.’ In ABB-type stimuli the second            tive processes, including analogy and similarity (Gentner,
and the third word are identical: e.g., ‘le-di-di’. The infants      1983), categorization (Markman & Gentner, 1993), detec-
were habituated on 16 such sentences, with three repetitions         tion of symmetry and regularity (Ferguson, 1994), and learn-
for each sentence. The infants were then tested on a different

ing and transfer (Gentner & Medina, 1998). Although these                             Sentence Encoding
representations and processes are symbolic, they do not need
to be rule-like, nor need they involve variables. Instead, we
view the notion of correspondence in structural alignment as                             Temporal           Regularity
an interesting cognitive precursor to the notion of variable         Stimuli                                 Encoding
                                                                                         Encoding
binding1. Correspondences between structured representa-                                                     (MAGI)
tions can support the projection of inferences, as the analogy
literature shows, and therefore a symbolic system can draw
inferences about novel situations even without having con-
structed rules. Moreover, as discussed below, comparison
can be used to construct conservative generalizations.                                                    Progressive
                                                                         Predictions
Across a series of items with common structure such a proc-                                             Generalization
                                                                           (SME)
ess of progressive abstraction can eventually lead to abstract                                             (SEQL)
rule-like knowledge. The attainment of rules, in those cases
where it occurs, is the result of a gradual process. As we
will show, symbolic descriptions can be used with structural
alignment to model learning that is initially conservative, but                 Figure 1: Simulation Architecture
which occurs fast enough to be psychologically realistic.
   We first describe our simulation model of the Marcus et al       Input stimuli: To make our simulation comparable with
task, which uses a simple combination of preexisting simula-     others, we use a representation similar to that of Elman
tion modules, i.e., SME, MAGI, and SEQL. All of these            (1999), namely, Plunkett & Marchman’s (1993) distinctive
modules have been independently tested against psychologi-       feature notation. Each word has twelve phonetic features,
cal data and independently motivated in prior modeling           which can be either present or absent. The presence or ab-
work. With the exception of domain-specific encoding pro-        sence of each feature for each word is encoded by symbolic
cedures, no new processing components were created for           assertions. If feature n is present for word w, the assertion
this task. We then describe the results of our simulation of     (Rn w) is included in the stimulus, and if absent, the asser-
the Marcus et al data, showing that our simulation can learn     tion (Sn w) is included. Thus the acoustic features of each
the concepts within the number of trials that the infants had,   word are encoded as twelve attribute statements.
without supervision and without pre-learning. We also show          We modeled the Marcus et al experiment both without
that the simulation can exhibit the same results with noisy      noise (Experiment 1) and with noise (Experiment 2). Marcus
input data. Finally, we discuss some of the implications of      et al. used a speech synthesizer to control the pronunciation
the symbolic similarity approach for models of cognitive         of the stimuli, but while this reduces variability, it cannot
processing.                                                      eliminate the possibility that the infant might encode some-
                                                                 thing incorrectly.
      Modeling infant learning via structural
                           alignment
                                                                    Temporal encoding: We assume that the infant encodes
   A psychological model of the infants’ learning must in-       the temporal sequence of the words in a sentence in two
clude the kind of input, the way the infants are assumed to      ways. First, each incoming word has an attribute associated
encode the individual sentences, and the processes by which      with it, corresponding to the order in which it appears (i.e.,
they generalize across the sentences. The architecture of our    FIRST, SECOND, or THIRD). We further assume that the
simulation is shown in Figure 1. We first describe our as-       infant encodes temporal relationships between the words in
sumptions concerning the infants’ processing capacities.         a sentence:; to code this, an AFTER relation is added be-
Then we describe each component in turn.                         tween pairs of words in the same sentence indicating their
   Processing Assumptions: We assume that infants can            relative temporal ordering. The particular labels used in this
represent the temporal order within the sentences (Saffran,      encoding step are irrelevant – there are no rules in the sys-
Aslin & Newport, 1996). We further assume that the infants       tem that operate on these specific predicates – the point is
notice and encode identities within the sentences: for exam-     simply that infants are encoding the temporal order of words
ple, the fact that the last two elements match in an ABB sen-    within sentences.
tence. This assumption is consistent with evidence that hu-         Regularity Encoding: We assume that the infants notice
man infants, as well as with studies of nonhuman primates        and encode identities within the sentences: for example, the
(Oden et al, in press), can detect identities. We also assume    fact that the last two elements match in an ABB sentence.
that infants can detect similarities between sequentially pre-   Thus the simulation must incorporate a process that detects
sented stimuli, consistent with studies of infant habituation,   when words are the same. We use the MAGI model of sym-
which demonstrate that infants respond to sequential same-       metry and regularity detection (Ferguson, 1994) to auto-
ness (e.g., Baillargeon, 1994).                                  matically compute these relationships. MAGI treats symme-
                                                                 try as a kind of self-similarity, using a modified version of
   1
                                                                 structure-mapping’s constraints to guide the self-alignment
     That structure-mapping algorithm neither subsumes, nor is   process. MAGI has been successfully used with inputs rang-
subsumed by, traditional pattern matching such as unification is ing from stories to mathematical equations to visual stimuli,
shown in Falkenhainer, Forbus, & Gentner (1988).

and it has done well at modeling certain aspects of visual         generalization consists of the overlap between the two input
symmetry, including making new predictions (Ferguson et al         descriptions: that is, the shared structure found by align-
1996). Here MAGI is used on the collection of words in a           ment. Thus generalizations are structured descriptions of the
sentence. For any pair of words w1 and w2 that MAGI finds          same type as the input descriptions, although containing
sufficiently similar, this module asserts (SIM w1 w2), and a       fewer specific features. If a new exemplar is sufficiently
DIFF statement for every other pair of words in the sen-           similar to a generalization (as determined comparing the
tence. (If MAGI does not find any pairs similar, DIFF              structural evaluation score to a set threshold), then (a) the
statements are asserted for every pair of words.) This mod-        generalization is updated by retaining only the overlapping
ule also asserts (GROUP w1 w2) for pairs of similar words,         description that forms the alignment between the generaliza-
to mark that they form a substructure in the stimulus, and         tion and the exemplar; and (b) candidate inferences are pro-
adds DIFF statements between groups and words not in the           jected from the generalization to the exemplar. Non-
group. This use of MAGI is an example of what Ferguson             overlapping aspects of a description (e.g., phonetic features
(1994, in preparation) calls analogical encoding.                  or relations that aren’t shared) are thus “worn away” with
                                                                   each new assimilated description. (The threshold that de-
SEQL                                                               termines when descriptions are sufficiently similar to be
   Once each sentence is encoded, we assume infants can de-        assimilated helps prevent descriptions from diminishing into
tect the similarities between sequential pairs of sentences.       vacuity.)
The detection of structurally parallel patterns across a se-          Returning now to the infant studies, we assume that babies
quence of examples is modeled by SEQL (Skorstad, Gentner           are carrying out an ongoing process of comparing and align-
& Medin, 1988; Kuehne, Forbus, Gentner & Quinn, 2000), a           ing the incoming exemplars with an evolving generalization.
model of the process of category learning from examples.           We further assume that the relational candidate inferences
SEQL constructs category descriptions via incremental ab-          from the general pattern to a new exemplar represent expec-
straction. That is, the representation of a category is a struc-   tations on part of the infant.3 When these expectations are
tured description that has been generated by successive            violated by an incoming stimulus that does not fit the gener-
comparison with incoming exemplars. If the new exemplar            alized pattern (e.g., an ABB test sentence after the ABA
and the category are sufficiently similar, the category de-        generalization has been formed), we assume the infant re-
scription is modified to be their intersection -- i.e., the com-   quires extra time to process the inconsistent stimulus.
monalities computed via structural alignment by a generali-
zation algorithm. If the new exemplar is not sufficiently                          Simulation Experiments
similar, it is stored separately and may later be used as the         In both experiments, we followed the procedure of Mar-
seed of a new category.                                            cus et al. Each stimulus was a simple three-word sentence,
   The structural alignment process is implemented via             encoded as described earlier. There were two sets of train-
SME, (Falkenhainer et al 1988; Forbus et al 1994) a cogni-         ing stimuli, one following the ABA pattern and one follow-
tive simulation of analogical matching. Here the base de-          ing the ABB pattern. The training stimuli were (ABA) de-
scription is a category description, and the target description    di-de, de-je-de, de-li-de, de-we-de, ji-di-ji, ji-je-ji, ji-li-ji, ji-
is the new exemplar. The structural alignments that SME            we-ji, le-di-le, le-je-le, le-li-le, le-we-le, wi-di-wi, wi-je-wi,
computes are used in three ways by SEQL. First, the nu-            wi-li-wi, wi-we-wi and (ABB) de-di-di, de-je-je, de-li-li, de-
merical structural evaluation score it computes2 is used as a      we-we, ji-di-di, ji-je-je, ji-li-li, ji-we-we, le-di-di, le-je-je, le-
similarity metric, a numerical measure for deciding whether        li-li, le-we-we, wi-di-di, wi-je-je, wi-li-li, wi-we-we. The
or not two descriptions are sufficiently similar. Second, the      test stimuli in both experiments were four descriptions rep-
candidate inferences it computes serve as a model for cate-        resenting two novel ABA-type (ba-po-ba, ko-ga-ko) and two
gory-based induction (c.f. Blok & Gentner, 2000; Forbus,           novel ABB-type sentences (ba-po-po, ko-ga-ga). The
Gentner, Everett, & Wu, 1997). Third, the correspondences          threshold value for SEQL was set to 0.85 in both experi-
in the best mapping SME produces serves as the basis for           ments.
SEQL’s generalization algorithm.
   SEQL maintains a set of generalizations and a set of sin-       Experiment 1
gular exemplars. When a new exemplar comes in, it is com-             This experiment is most comparable to previous simula-
pared against existing generalizations to see if it can be as-     tion models of the phenomena, in that we assume noise-free
similated into one of them. Otherwise, it is compared with         encoding of the stimuli. A simulation run consists of expos-
the stored exemplars to see if a new generalization can be         ing SEQL to all of the stimuli from a particular training set
formed. If it is insufficiently similar to both the generaliza-    (either ABA or ABB) once and then seeing the response
tions and the stored exemplars, it is stored as an exemplar        given the four test sentences. To avoid possible biasing due
itself.                                                            to sequence effects (See Kuehne et al., 2000), 20 simulation
   SEQL begins with no generalizations; it simply stores its       runs were made for each training set using different random
first exemplar. If the next exemplar is sufficiently close to
the first, their overlap is stored as the first generalization. A
                                                                      3
                                                                        SME can also produce attribute-level candidate inferences, and
   2
     Although SME can compute multiple mappings, we use the        does so on these stimuli. We assume that, since these inferences
structural evaluation score of the best mapping, normalized by the concern directly perceivable features, testing them takes very little
size of the base description.                                      time.

orders. Identical match score and relational candidate infer-   stimuli produce relational candidate inferences while the in-
ences were produced for all sequences with a given stimulus     grammar stimuli do not.
set. In each case, SEQL produced a single generalization
during the learning phase. For the test phase we used encod-                   Table 2a: ABA training stimuli
ings of the corresponding stimuli used with infants, as noted   Test          Average Match Candidate
above. Tables 1a and 1b show the results of this series for     Stimulus      Score                  Inferences
two generalizations paired against the four test sentences.                                          Min, Average, Max
                                                                ba-po-ba      0.647                  0, 0, 0
               Table 1a: ABA training stimuli                   ko-ga-ko      0.682                  0, 0, 0
Test             Match           Candidate                      ba-po-po      0.435                  2, 2.45, 3
Stimulus         Score           Inferences                     ko-ga-ga      0.395                  2 , 2.55, 3
Ba-po-ba         0.658           None
Ko-ga-ko         0.689           None                                          Table 2b: ABB training stimuli
Ba-po-po         0.486           (DIFF po1 ba1)                 Test           Match                 Candidate
                                 (DIFF po1 po2)                 Stimulus       Score                 Inferences
                                 (SIM ba1 po2)                                                       Min, Average, Max
Ko-ga-ga         0.455           (DIFF ga1 ko1)
                                 (DIFF ga1 ga2)                 ba-po-ba       0.339                 2, 2, 2
                                 (SIM ko1 ga1)                  ko-ga-ko       0.352                 2, 2.05, 3
               Table 1b: ABB training stimuli                   ba-po-po       0.805                 0, 0, 0
Test             Match           Candidate
Stimulus         Score           Inferences                     ko-ga-ga       0.783                 0, 0, 0
Ba-po-ba         0.328           (SIM po1 ba2)
                                 (DIFF ba1 (GROUP po1
                                 ba2))                                     Comparison with other models
Ko-ga-ko         0.350           (SIM ga1 ko2)                     The results of Marcus et al. (1999) have sparked an active
                                 (DIFF ko1 (GROUP ga1           debate focused on two issues: (1) Can current connectionist
                                 ko2))                          models (e.g., simple recurrent networks) model these re-
Ba-po-po         0.776           None                           sults? (2) Do infants generate abstract rules that include
Ko-ga-ga         0.753           None                           variables?
                                                                   Regarding the adequacy of simple recurrent networks,
The in-grammar (bold) and out-of-grammar (plain text)           Marcus et al. state “Such networks can simulate knowledge
matches show clear differences in their match scores. In-       of grammatical rules only by being consequently trained on
grammar matches are above 0.64 and do not generate rela-        all items to which they apply; consequently, such mecha-
tional candidate inferences. Out-of-grammar matches have        nisms cannot account for how humans generalize rules to
match scores below 0.5, and lead to relational candidate        new items that do not overlap with the items that appeared in
inferences. Thus out-of-grammar test sentences lead to          the training.” Elman’s (1999) response describes his use of
longer looking behavior, as predicted.                          a simple recurrent network to model this task. Elman’s
                                                                model requires tens of thousands of training trials on the
Experiment 2                                                    individual syllables, and treats the problem as a supervised
   As noted earlier, we believe that noise-free stimulus en-    learning task, unlike the task facing the infants. By contrast,
codings are unrealistic. Consequently, we used the same         our simulation handles the learning task unsupervised, and
procedure as Experiment 1, but this time introducing noise      produces human-like results with only exposure to stimuli
into the representations for the training and test stimuli. For equivalent to that given to the infants. Moreover, our model
each sentence, one of the words was randomly picked, and        also continues to work with noisy data, something not true of
one of its attributes (also chosen at random) was dropped or    any other published model of this phenomenon that we know
flipped, with the rest of its description being unchanged.      of.
Such changes can be significant: for example, flipping a           The learning in our model is due to the “wearing away” of
single phonetic feature turns the word ‘de’ into the word       non-identical phonetic attributes through subsequent com-
‘di’. Again, 20 simulation runs were made for each training     parisons. Although SEQL’s learning proceeds faster than
set using different random orders. Naturally the match          connectionist models, it is still slower than systems that gen-
scores and, to a lesser degree, the generated candidate infer-  erate abstractions immediately (e.g., explanation-based
ences, did vary across the individual runs. Tables 2a and 2b    learning (DeJong & Mooney, 1986)). In SEQL’s progres-
show the results. The scores were averaged over all 20 runs.    sive alignment algorithm, the entities in the generalizations
   Although the noise affected the details of the computa-      lose their concrete attributes across multiple comparisons,
tions, the overall pattern of results remains the same. The     leaving the relational pattern of each grammar as the domi-
in-grammar (bold) match scores are far higher than the out-     nant force in the generalization only after a reasonable num-
of-grammar (plain text) scores; and the out-of-grammar

ber of varied examples are seen.4 There is considerable evi-          we suspect that to model large-scale learning, it will need to
dence for this kind of conservative learning (Forbus &                keep track of more statistical information than it does cur-
Gentner, 1986; Medin & Ross, 1989).                                   rently, so that properties wear away more slowly.
   Turning to the second issue, whether infants have vari-                We note that it is common to conflate symbolic process-
ables and generate abstract rules, Marcus et al (1999) claims         ing with rule-based behavior, and parallel processing with
“[I]nfants extract abstract algebra-like rules that represents        connectionist models. The model described here is sym-
relationships between placeholders (variables), such as ‘the          bolic, but it need not involve variables or rules. Further, it
first item X is the same as the third item Y,’ or more gener-         involves extensive parallel processing (most of SME and
ally that ‘item I is the same as item J.’” But our simulation         MAGI’s computations are parallel). Given the complexity
does not introduce variables, in the sense commonly used in           of the phenomena, such conflations seem unwise.
mathematics or logic. The generalizations constructed by                 The debates stirred by the Marcus et al. results bear on a
SEQL do indeed include relational patterns that survive re-           critical issue in human learning and development: namely,
peated comparisons because they are shared across the in-             what knowledge or mechanisms must be assumed to account
grammar exemplars. Furthermore, the entities (words) in the           for the rapid and powerful achievements demonstrated by
generalizations have many fewer features than the original            infants in both cognition and language. Our results suggest
words, as a result of the wearing away of features in succes-         that the general learning mechanism of structure-mapping
sive comparisons. One could consider these patterns as a              theory may go a long way in accounting for these
form of psychological rule, as proposed by Gentner and                accomplishments.
Medina (1998), with the proviso that the elements in the rule
are not fully abstract variables, although they might asymp-                              Acknowledgments
totically approach pure variables.                                       We thank Ron Ferguson, Ken Kurtz and Tom Mostek for
                                                                      valuable help and discussions. This research was supported
                           Discussion                                 by the Cognitive Science Division of the Office of Naval
   This paper proposes a third kind of explanation for the in-        Research.
fant learning phenomena of Marcus et al (1999): incremental
abstraction of symbolic descriptions via structural align-                                     References
ment. We believe our explanation is currently the best one            Altmann, G.T.M. and Dienes, Z. (1999). Rule learning by
for three reasons. First, it models the infant data with fewer          seven-month-old infants and neural networks, Science
extra concessions than previously published models (i.e., no            284, 875.
pre-training, no supervision, and noisy data). Second, the            Aslin, R. N., Saffran, J. R., & Newport, E. L. (1998). Com-
processes we postulate are cognitively general; they apply to           putation of conditional probability statistics by 8-month-
a large set of phenomena. Third, the abstraction processes              old infants. Psychological Science, 9(4), 321-324.
we propose are consistent with research demonstrating that            Baillargeon, R. (1994). How do infants learn about the
human learning is initially conservative (Brooks, 1987; For-            physical world? Current Directions in Psychological Sci-
bus & Gentner, 1986; Medin & Ross, 1989). Interestingly,                ence, 3(5), 133-140.
there is ongoing research in developing symbolic connec-              Blok, S. V., & Gentner, D. (2000). Reasoning from shared
tionist models consistent with these processes (e.g., Holyoak           structure. Proceedings of the 22nd Meeting of the Cogni-
& Hummel, 1997).                                                        tive Science Society.
   Many issues remain to be explored. For example, al-                Brooks, L. R. (1987). Decentralized control of categoriza-
though our system does not introduce variables in its gener-            tion: The role of prior processing episodes. In U. Neisser
alization process, there is a sense in which the entities in the        (Ed.), Concepts and conceptual development: The eco-
generalization are on their way to becoming variables. Gent-            logical and intellectual factors in categorization (pp. 141-
ner and Medina (1998) have proposed that the process of                 174). Cambridge: Cambridge University Press.
progressive alignment can lead to rules. They further sug-            Christiansen, M.H. and Curtin, S.L. (1999). The power of
gested that the application of rules to instances can be ac-            statistical learning: No need for algebraic rules, in Pro-
complished using the same general processes of structural               ceedings of the Twenty-first Annual Conference of the
alignment and projection that are used in analogy. The dif-             Cognitive Science Society, Erlbaum, Mahway, NJ.
ference is that the base domain is an abstraction, the entities       Christiansen, M.H. and Curtin, S.L. (1999). Transfer of
are ‘dummies’ with no features to either help or impede the             learning: rule acquisition or statistical learning? Trends in
match with the specific entities in the exemplar. Another               Cognitive Science 3, 289-290
issue concerns the incorporation of statistical notions in            DeJong, G.F. and Mooney, R.J. (1986). Explanation-based
SEQL. Although SEQL is to a certain degree noise-resistant,             learning: An alternative view. Machine Learning 1(2), pp.
                                                                        145-176
   4
                                                                      Elman, J. (1999). Generalization, rules, and neural net-
     SEQL learns with only one exposure to the 16 learning sen-         works: A simulation of Marcus et. al, (1999). Ms., Uni-
tences, whereas Marcus’s infants received three exposures for each      versity of California, San Diego.
sentence. It is possible that the infants would have learned with     Falkenhainer, B., Forbus, K., and Gentner, D. (1986). The
only one pass; however it is also possible that the infants were less   Structure-Mapping Engine. In: Proceedings of AAAI 86,
consistent in detecting the similarities than our simulation with its   Philadelphia, PA, August.
current parameters.

Falkenhainer, B., Forbus, K.D. and Gentner, D. (1989). The     Medin, D. L., & Ross, B. H. (1989). The specific character
  Structure Mapping Engine: an algorithm and examples.           of abstract thought: Categorization, problem-solving, and
  Artificial Intelligence, 41: 1-63                              induction. In R. J. Sternberg (Ed.), Advances in the psy-
Ferguson, R.W. (1994). MAGI: A model of analogical               chology of human intelligence (Vol. 5, pp. 189-223).
  encoding using symmetry and regularity. In Proceedings         Hillsdale, NJ: Erlbaum.
  of the Sixteenth Annual Conference of the Cognitive Sci-     Oden, D. L.. Thompson, R. K. R., and Premack, D. (in
  ence Society. Hillsdale, NJ: Erlbaum.                          press). Can an ape reason analogically? Comprehension
Ferguson, R.W., Aminoff, A. and Gentner, D. (1996). Mod-         and production of analogical problems by Sarah, a chim-
  eling qualitative differences in symmetry judgments. In        panzee (Pan troglodytes). In D. Gentner, K. J. Holyoak, &
  Proceedings of the Eighteenth Annual Conference of the         B. Kokinov (Eds.), The analogical mind: Perspecitives
  Cognitive Science Society. Hillsdale, NJ: Erlbaum.             from cognitive science. Cambridge, MA: MIT.
Forbus, K. D., & Gentner, D. (1986). Learning physi-           Plunkett, K. and Marchman, V. (1993). From rote learning
  cal domains: Toward a theoretical framework. In R.             to system building: Acquiring verb morphology in chil-
  S. Michalski, J. G. Carbonell, & T. M. Mitchell                dren and connectionist nets. Cognition, 48, 21-69.
  (Eds.), Machine learning: An artificial intelligence         Saffran, J., Aslin, R. and Newport, E. (1996). Statistical
  approach (Vol. 2, pp. 311-348). Los Altos, CA:                 learning by 8-month-old infants, Science, 274, 1926-1928
  Kaufmann.                                                    Seidenberg, M.S. and Elman, J. (1999), Do infants learn
Forbus, K. D., Ferguson, R. W., and Gentner, D. (1994).          grammar with algebra or statistics?, Letter, Science 284,
  Incremental Structure-mapping. In: Proceedings of the          434-436
  Sixteenth Annual Conference of the Cognitive Science So-     Seidenberg, M.S. and Elman, J. (1999). Networks are not
  ciety. Hillsdale, NJ: Erlbaum.                                 hidden rules, Trends in Cognitive Science 3, 288-289
Gentner, D. (1983). Structure-mapping: a theoretical frame-    Skorstad, J., Gentner, D. and Medin, D. (1988). Abstraction
  work for analogy. Cognitive Science, 7: 155-170.               processes during concept learning: a structural view. In:
Gentner, D. and Markman, A.B. (1997). Structure-mapping          Proceedings of the Tenth Annual Conference of the Cog-
  in analogy and similarity. American Psychologist, 52, 45-      nitive Science Society. Montreal: Lawrence Erlbaum As-
  56.                                                            sociates.
Gentner, D., & Medina, J. (1998). Similarity and the devel-
  opment of rules. Cognition, 65, 263-297.
Goldstone, R.L. (1994). The role of similarity in categoriza-
  tion: Providing a groundwork. Cognition 52(2), 125-157.
Goldstone, R.L., Medin, D.L., and Gentner, D. (1991). Re-
  lational similarity and the non-independence of features in
  similarity judgements. Cognitive Psychology, 23, 22-264.
Gomez, R. L., & Gerken, L. (1999). Artificial grammar
  learning by 1-year-olds leads to specific and abstract
  knowledge. Cognition 70,109-135.
Hummel, J. E., & Holyoak, K. J. (1997). Distributed repre-
  sentations of structure: A theory of analogical access and
  mapping. Psychological Review, 104(3), 427-466.
Kuehne, S.E., Forbus, K.D., Gentner, D. and Quinn, B.
  (2000). SEQL- Category learning as incremental abstrac-
  tion using structure mapping, Proceedings of the Twenty-
  second meeting of the Cognitive Science Society.
Marcus, G.F., Vijayan, S., Bandi Rao, S. and Vishton, P.M.
  (1999). Rule-learning in seven-month-old infants. Sci-
  ence, Vol. 283, 77-80
Marcus, G.F. (1999). Do infants learn grammar with algebra
  or statistics?, Response to Seidenberg & Elman, Negishi,
  and Eimas. Science 284, 436-437
Marcus, G.F. (1999). Simple recurrent networks and rule-
  learning: http://psych.nyu.edu/~gary/science/es.html.
Markman, A.B. and Gentner, D. (1993). Structural align-
  ment during similarity comparisons. Cognitive Psychol-
  ogy, 25, 431-467.
McClelland, J.L. and Plaut, D.C. (1999). Does generaliza-
  tion in infant learning implicate abstract algebraic rules?,
  Trends in Cognitive Science 3, 166-168
Medin, D.L., Goldstone, R., and Gentner, D. (1993). Re-
  spects for similarity. Psychological Review, 100(2), 254-
  278.

