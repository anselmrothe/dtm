UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Decoding Syntactic Parameters: The Superparsers as Oracle
Permalink
https://escholarship.org/uc/item/1hb315vc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Fodor, Janet Dean
Teller, Virigina
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Decoding Syntactic Param eters: The Superparser a s Oracle
       Janet Dean Fodor (jfodor@gc.cuny.edu)                               Virginia Teller (teller@cs.hunter.cuny.edu)
   Ph.D. Program in Linguistics; CUNY Graduate Center                   Department of Computer Science ; Hunter College CUNY
        365 Fifth Avenue, Ne w York, NY 10016 USA                             695 Park Avenue, Ne w York, NY 10021 USA
                              Abstract                                not sufficiently uniform and superficially identifiable (Clark,
                                                                      1994; Gibson & Wexler, 1994). To find the abstract proper-
  Syntactic parameter setting has proven extremely difficult to       ties that identify v, the derivation of the sentence must be
  model. The original 'switch-setting' m etaphor failed because       computed, i.e., the sentence must be parsed. But parsing is
  parametrically relevant properties of a natural language sentence   work, and the computational workload of a learner must be
  cannot be recognize d withou t considerable structural analy sis.   kept within psychologically plausible limits. The problem is
  The result has been a move to trial-and-error learners which        that when the parser lacks the correct grammar, as it does by
  attempt to guess a grammar that can analyze (parse) the current     definition in a learning event, it must apparently try out
  input sentence. But standard variants of grammar gu essing are
                                                                      multiple grammars unti l it finds a successful one. Even
  wasteful of the parametric information in input sentences because
  they use it only as feedback after a candidate grammar has been     though the number of parameters is limited, there are too
  chosen. We show here that performance is significantly              many possible grammars for it to be feasible for a learner to
  improved by a 'superparsing' ro utine which constructs a candidate  try them all on a single sentence, either serially or in parallel.
  grammar on-line in response to the properties of the input             This is where current learning models diverge . Some test
  sentence. No sentences then need to be discarded for lack of a      one gramma r per sent ence (e .g. Gibson & Wexler, 1994,
  grammar to parse them. The gain in learning speed can be            henceforth GW94) and are very slow to converge on the
  quantified in terms of the average number of sentences required     correct grammar. Othe rs test batches of grammars at a time
  for convergence. Superparsing can b e achieved by the normal        (e.g. Clark, 1992; Nyberg, 1992) and thereby go beyond
  sentence parsin g routines, applying a grammar that incorp orates   what it is plausible to suppose a child is capable of. In this
  all possible parameter values. The superp arsing learner is ro bust paper we discuss a novel way to use the parser to decode the
  and imposes no special demands o n the input.                       parameter values that license an input sentence, without
                                                                      undue use of resources (Fodor, 1998a). We provide here a
              Natural Language Acquisition                            quantitative assessment of the substantial increase in
Children exposed to a sample of sentences from a natural              learning speed that this model permits compared with
language acquire its grammar in a few years. There is as yet          traditional grammar gue ssing models.
no computational model of the acquisition process that is both
effective and psychologically realistic. The conception of the               A Simple Model of Grammar Selection
learner's task was greatly simplified with the advent of
parameter theory (C homsky, 1981, 1995) under which a                 A learning event begins with the learner receiving a novel
natural language grammar consists of an innate component              input sentence s, a sentence of the target language not
(Universal Grammar, UG) and a selection from among a finite           licensed by the learner's current grammar Gc. Gc may have
set of properties by which languages can differ (the                  some parame ters set to corre ct values, but it also has one or
parameters). Depending on the particular linguistic theory            more set to incorrec t values or not set at all. The le arner first
assumed, a parameter might be a choice between grammar                attempts to parse s with Gc. If the parse were successful, no
rules, or between the presence or absence of a rule in the            change would be made to G c; this is error-driven learning
grammar. But as developed in the Chomskyan framework a                (Gold, 1967). Since by hypothe sis Gc does not license s, this
parameter specifies a more abstract property of grammatical           parse attempt fails, and the learning device seeks an
derivations, such as the direction of case assignment by a            alternative grammar. To preserve psychological plausibility
verb, or the derivational level at which a universal constraint       we will make the strong assumption here that only one more
applies, or, in more recent versions, the 'strength' of a             parsing attempt may be made on this same input. Hence, the
syntactic feature on a tree node. The learner's task is to select     task of finding a new grammar that does license s must be
the correct setting for each relevant parameter, i.e., each           achieved by means of just one parse. If it is not, s must be
parameter whose value con tributes to the derivation of at least      discarded and learning must await further i nput. Each such
one sentence of the target language. Though the concept is            discard increases the total number of inputs needed before
simple, the task is more challenging than was originally              the target grammar is attained, and hence decreases the
appreciated.                                                          speed of learning and increases the effort expended. It is
  The earliest idea was that some property of an input                 important, therefore, for the learner to make good use of the
sentence would trigger the correct setting of each parametric          one parse test of a new gramma r that it is able to conduct .
switch. But the needed property detectors could not be                  However, standard grammar guessing procedures extract
devised, because the characteristic properties of sentences            only minimal information from their interrogation of the
derived by means of some parameter value v (inter alia) are            input. Because of the single-parse restriction, just one

                                                                                                                                                 2
grammar must be selected to undergo the parse test, and this            license the current i nput meets the Licensing condition. For
selection must of necessity be made prior to parsing. The               a learner that satisfies Licensing, the chance of
success or failure of this parse attempt with the hypothesized          hypothesizing Gt would be 1/A, where A is the degree of
grammar, Gh, provides feedback on the basis of which the                ambiguity of s, measured as the number of grammars in the
                                                                                                                    i
                                                     1
learner decides whether or not to adopt Gh. If the parse is             domain that license s divided by 2 (the irrelevance factor).
successful the learner will adopt Gh; if the parse fails the            Clearly this is responsive to how informative the language
learner is assumed to retain G c. This policy of shifting to a          sample is. For extremely ambiguous input (A approaching
                                                                           n-i
new grammar onl y if it license s the current input is the              2 ), the success rate is hardly better than without the oracle.
Greediness constraint of GW 94 and others. If Gh happens to             But if s is unambiguous with respect to even one relevant
be the target grammar, it will be retained permanently and              parameter, the probability of a successful guess is increased.
learning is comple te. A grammar is correct for the target                   This shows up in the speed of learning, estimated in terms
language if it has the target value for each paramet er relevant        of the number of inputs required, on average, to arrive at Gt.
to the language. If some parameters are irrelevant to the               This is the reciprocal of the probability of a successful
                                                                                   3
target language there will be an equivalence class of correct           guess. For the oracle learner this is A; for the random guess
                                                                                           n-i
paramete rs. For convenience in what follows we will refer to           learner, it is 2 . Table 1 shows the differences in average
these gra mmars collectively as 'the target gra mmar', G t.             number of inputs consumed for various values of A and
                                                                                                                           n-i
                                                                        numbers of relevant parameters (= 2 -A). On average,
                                                                                                                                   n-i       n-i
      Assessing Grammar Selection Efficiency                            performance is improved by a factor of (2 -A)/2 . The
                                                                        oracle learner, unlike the random choice learner, benefits to
A simple random choice learning model such as this will                 the extent that the input constrains the set of candidate
demonstrably converge on the target grammar (Berwic k &                 grammars.
Niyogi, 1996). However, learning is slow largely because the
learning component bases its actions on the mere fact that                                              Table 1
some randomly chosen G h does, or does not, license s. We                 Reduction due to oracle in average inputs to convergence
will show that learning could be substantially faster if instead
the parser could reliably identify for the learner a grammar
that licenses s. For the present let us suppose that this                                        Number of parameters relevant to Gt (=n-i)
information is provided to the learner by an oracle. Later we
will show how this oracle can be implemente d.                                Avg. A    10        15        20            25                30
    Given Gc Gt, what is the probability that the learner will             1           1023     32,767   1,048,575    33,554,431       1,073,741,823
shift to Gt as a result of an encounter with an arbitrary input
sentence s? At the point where Gc is rejected, the random                  10          1014     32,758   1,048,566    33,554,332       1,073,741,822
choice learner (without oracle) picks an alternative from
                                                                      n    100          924     32,668   1,048,476    33,554,422       1,073,741,724
among the set of all possible grammars, of which there are 2
for n binary parameters. (For simplicity we treat Gc as a                  1000           24    31,768   1,047,576    33,553,432       1,073,740,824
candidate grammar e ven though it has just failed.) Of these,
  i                                                                        1 million     -        -         48,576    32,554,432       1,072,741,824
2 are correct (are in the equivalence class Gt) where i = the
number of parameters irrelevant to the target languag e. Thus              1 billion     -        -          -             -              73,741,824
                                                                   i n
the probability that the learner's selected Gh is Gt = 2 /2 .
Observe that this is not sensitive if s uniquely determines                                                           a
                                                                             The values of A range from 1 to 2 , where a is the number
every parameter value in the target grammar, the learner has             of parameters relevant to G t whose values s does not deter-
no more chance of guessing correctly than if s is fully                  mine. For a simple example: Assume that 30 parameters are
ambiguous. This is because, as noted, this learner must make             relevant to Gt and a = 25. Such a sentence might be licensed
its selection before testing out the selected grammar on s, and          by exactly 2 grammars, with opposite values for each of
so it cannot restrict its guesses to grammars which license s.           those 25 parameters. Or it might be licensed by grammars
    Imagine now that this learning device is equipped with an            with all possible combinations of values for those
oracle which offers the learner a grammar that licenses s (any                                                  25
                                                                         paramete rs, of which there are 2 = 33,554,432. The former
one of the grammars in the domain that do so). Then the                  situation we will term sparse ambiguity, and the latter
learner could take this grammar to be Gh, and avoid wasting              dense ambiguity; clearly, all situations in between are
attention on any grammar that does not license s. Let us say             possible also.
that a learning device which considers only grammars that
                                                                             It seems likely that the parametric ambiguity of natural
                                                                        2
                                                                          Licensing is related to Greediness but the difference between them
1
  Licensing is related to Greediness but the difference between them is is important. Licensing applies in the selection of G h, while
important. Licensing applies in the selection of Gh, while Greedin ess  Greediness governs only the grammar adoption stage at the end of
governs only the grammar adoption stage at the end of each learning     each learning event. A learner that resp ects Licensing can also
event. A learner that respects Licensing can also respect Greediness.   respect Greediness. The simple learning model discussed above
The simple learning model discussed above shows that it is possible to  shows that it is possible to obey Greediness but not Licensing.
                                                                        3
obey Greediness but not Licensing.                                        Homogeneity is assumed in these calculations; no grammar is
                                                                        antecedently more likely than any other to license s or to be G t.

                                                    a
languages is quite sparse (A much less than 2 ). In the minia-        parameter (Clark, 1989). Linguists have mostly been
ture natural language domain defined by 3 parameters pre-             cautious about embracing markedness theory, but many
sented in GW94, ambiguity is less than fully dense in every           markedness-type rankings are nevertheless implicitly as-
one of the 11 sentence types in which two or more parameters          sumed in linguistic descript ions (Wachol der, 1995). Also
are ambiguously expressed. It remains to be seen how this             under type (a): grammars could be prioritized by linguistic
scales up in a domain of more realistic size. But the principle       maturation if, as has been proposed, some aspects of UG
is clear. Wit h maximall y sparse ambigui ty, a sentence could        develop later than others (Wexler, 1999).
be ambiguous with respect to every paramete r and                       Criteria of type (a) may be helpful in resolving parametric
neverthele ss offer the oracle learner a 50% chance of guessing       ambiguitie s. To the extent that linguistic markedness has an
the target grammar. In general, for a constant degree of              impact on the frequency of grammar adoption by language
parametric ambiguity in terms of a, sparse ambiguity is more          communities (though this is a fraught topic), type (a) criteria
informative for a learning system capable of making use of it,        can reflect the antecedent likelihood that any given grammar
i.e., a learning system that has knowledge of which grammars          is the target. They may also reduce effort by holding
do and do not license the current input.                              learners to simpler or linguistically more natural grammars
   In a simple random choice system this information is               as long as the evi dence perm its.
unobtainable. It could be established only by testing every
possible grammar on s, which clearly violates the limit of one        (b) Rankings Relating to Gc
parse per sentence (plus the original parse with G c). Of             One Gc-related criterion is the Single Value Constraint
course, this one-parse limit is just one instantiation of a           (SVC) of GW94, which requires the learner to select
practical ban on excessive processing, and the limit might be         grammars that differ from Gc in the value of just one
raised to two or three parses per sentence. But this will make        parameter. Another is the assumption of 'indelible' or
little difference. In order to significantly re duce the amount       'deterministic' learning, which requires that Gh include Gc.
of input needed for convergence, it would be necessary to             For parameter theory thi is taken to mean that once a
permit testing of eac h sentence with as many grammars as             parameter has been set, it may never be switched to its other
                                         4
required to find one that licenses it.                                value (Clahsen, 1990/91).
   However, there are other ways of improving the quality of             Type (b) criteria reflect what has already been gained by
grammar selection which do not presuppose an ability to sort          experience of the target language, insofar as this is com pres-
grammars into those that do and do not license s. We review           sed into the grammar Gc that the learne r has been led t o so
these in the next section. Their effects are less easy to             far. (A learner is standardly assumed to have no memory of
quantify, but it is highly doubtful that either singly or jointly     past inputs or past grammar hypotheses, other than their
they could substitute for the usefulness of a Licensing oracle.       legacy in determining the current gramm ar.) Because of
                                                                      Greediness, a grammar that has been adopted by the learner
              Criteria for Grammar Selection                          may be assumed to be more likely to have some parameters
Given a particular input sentence s from the target language,         correctly set than an arbitrary grammar in the domain; and
which grammar in the domain is it optimal for a learner to            a grammar similar to such a grammar may be presumed to
hypothesize? The grounds for selecting a grammar ma y be of           share its virtues. The worth of these considerations has been
several kinds, differing with respect to how much information         disputed, but we need not enter the debate here; see Berwick
they draw from the current learning situation. The preference         & Niyogi (1996) and Sakas & Fodor (in press) for
for one grammar over another may be (a) independent of the            discussion. Clearly it is desirable for a learning device to
 current situation, or it may (b) reference the current grammar,      have some way to hold onto past gains. To adopt a
 and/or it may (c) reference prope rties of the current input.        completely fresh hypothesis at each step, as permitted in an
 Some criteria of this latter kind may (d) require parsing of         unconstrained guessing model, does nothing to improve the
 s with more than one new grammar, and thus exceed the                probability of success as lear ning proceed s.
 limit on feasible processing for a learner without an oracle,
 which must select candidate grammars before knowing how              (c) Rankings Based on Properties of s Identifiable
 they relate to the input.                                                 by Parsing s with at Most One New Grammar
                                                                      Type (c) criteria are sensitive to the current input but com-
 (a) Orderings on the Class of Grammars                               patible with the ban on excessive processing even for a
 Grammar orderings may be imposed by linguistic principles            learner that first selects a grammar and then tests it. Input-
 of markedness. One value of a parameter may be less marked           sensitive criteria can deliver hard information. They consti-
 (more favored) than its other value; e.g., local binding of          tute the learner's contact with the facts of the language and
 anaphors may be less marked than long-distance anaphors              so should be a particularly helpful guide to the correct
 (Manzini & Wexler, 1987). Or parameters may be ordered               grammar.
 with respect to each other: the marked value of one parameter           Greediness and error-driven learning fall under type (c) as
 may be less marked than the marked value of another                  well as (b), since they refer to s as well as G c. Greediness
                                                                      ranks all grammars that do not license s lower than Gc. The
                                                                      requirement of error-drive n learning ra nks Gc above all other
4
  Parse-testing two grammars on s would double the chanceof guessing  grammars if Gc licenses s. These two input-sensitive criteria
Gt. This would as helpful as if s were unambiguou s with respect to   can be incorporated into a simple grammar guessing
one parameter. However, testing m grammars on each sentence would     procedure without a Licensing oracle, because each can be
increase the chance of success only linearly in m, not exponentially. checked with limited r esources: a parsing attempt with G c

for error-driven learning, and then with one new grammar for     (d) criteria which improve the quality of guesses in case more
Greediness.                                                      than one grammar meets the Licensing condition, i.e., in case
   By contrast, some input-sensitive selection criteria do not   of parametric ambiguity.
qualify as type (c) conditions because they require (or may do
so) the checking of two or more new candidate gramm ars.
This threatens to violate the ban on excessive processing for           Superparsing: A Constructive Process
a learner without oracle. Licensing (as opposed to Greedi-                               of Gh Selection
ness) is one casualty already noted: it imposes the tough        Inefficiency results from formulating a grammar hypothesis
requirement that a grammar must be known to be capable of        in advance of parsing the input string. This was assumed to
parsing s in order to be selected for parsing s. Also not        be unavoidable, given the patent unfeasibility of first analyz-
possible under type (c) is comparison of the derivations         ing the string with all grammars as a basis for selecting one
assigned to s by different candidate gramm ars, as would be      from among them. But if an optimal grammar choice cannot
necessary for application of a structural simplicity metric.     be made before parsing s, or after parsing s, perhaps it can be
   Even the grammar-similarity constraints of type (b) are       made in the course of parsing s. The solution we will outline
affected by the limitation on processing. The SVC has been       is to let the ongoing parse shape the formulation of Gh. Total
demonstrated by GW94 to be too stringent in that, in con-        parametric decoding cannot be achieve d by this means, for
junction with Greediness, it can trap the learner at a local     reasons we will explain, but most of the desirable learning
maximum where there is no grammar that both licenses an          characteristics we have been seeking do follow. By the end
input sentence and differs from Gc in only one parameter         of the parse, the learner will know of one grammar that
value. If a range of alternative grammars coul d be evaluated,   licenses s. Hence there will be no wastage of input due to
a more general Closeness criterion could be applied instead.     lack of a grammar to parse it. The grammar the parser finds
The learner would adopt the grammar most similar to Gc           will always be drawn from among the A grammars that
                                                                                                               n                n-i
among those that license s (with dead heats resolved by          license s, rather than from the total set of 2 grammars (or 2
random choice or other criteria). The adopted grammar            relevant grammars), so the learner will be taking full advan-
would differ from Gc by only one parameter value in ma ny        tage of parametric disambiguation provided by the input.
cases, but could differ by two or more if necessary. This        Where disambiguati on is not total, Close ness and a structural
would maintain the fruits of past learning while eliminating     simplicity metric can be applied to choose a good candidate,
all local maxima. (Note that Closeness can be seen as a          as indicated below.
generalization of error-driven learning: Gc is to be changed        Selecting a grammar that licenses s, during the course of
only to the extent that is necessary in order to lice nse the    parsing s, is feasibl e. Fodor (1998a) suggested the following
input.) But for this we must move up to type (d) criteria,       procedure. The parsing routines set about parsing s with the
which are not feasible for a standard gramm ar guessing          current grammar Gc. If s is not licensed by Gc this parse
learner.                                                         attempt will break down at some place in the sentence. When
                                                                 it does, the parser should not stop and merely report back its
(d) Rankings Based on Properties of s Identifiable               failure, as we assumed earlier. Instead, it should supplement
     Only by Evaluation of Multiple New Grammars                 Gc with all possible parameter values and continue processing
Closeness is an ideal similarity metric but (unlike the less     s with this 'supergrammar' SG. SG must afford at least one
flexible SVC) it is a comparative crite rion which demands       parse for s (as long as the sentence contains no unknown
knowledge of a ll the gramm ars that lice nse s, so that the one lexical items, and does not cause a severe 'garden path' be-
most similar to Gc can be selec ted. This puts it beyond the     yond the capacity of the parser to recover from; see Fodor,
scope of any resource-limited pre-parse grammar selection        1998b). Where there is a choice of analysis for s, priority is
process. Also falling under type (d) would be a simplicity       given to the parameter values in Gc; this incorporates the
measure which favors grammars that assign the small est          error-driven learning condition. But new parameter values
syntactic tree, or the shortest transformational derivation,     can be made use of as ne eded. Any new parameter value that
compatible with the word string. This selection criterion        is found to be necessary for parsing s is adopted by the
seems very plausible both linguistically and psychologically,    learner. Thus, the superparser shuttles through the sentence
but is not easy to impose. How could a resource-limited          flipping parameter settings as it goes, in response to the
learner set about discovering which of a million or a billion    demands of the input sentence. Its output consists of (i) a
grammars assigns the simplest structure to s?                    complete parse tree, an d (ii) a gramm ar that satisfi es Licens-
    In general: Adding suitable grammar selection principles     ing.
 to a random choice learner can improve performance, com-           If s is fully unambiguous with respect to all the parameter
 pensating in part for inefficiency due to inability to discrim- values it expresses, the superparser has no choices to make
 inate between grammars that do and do not license s before      (above the usual within-grammar ambiguity resolution
 committing resources to those that do not. However, the         choices of normal sentence processing). If s is ambiguous
                                                                                                                         a
 present analysis of grammar selection strategies makes clear    with respect to a parame ters, SG assigns it up to 2 distinct
 that the most potent selection principles are also beyond the   parse trees. In principle the parser might identify them all.
 reach of such a system, and for much the same reason. We        In practice it could not, since this would require massive
 next show that both weaknesses can be remedied by the same      parallel parsing which would violate the general ban against
 means. With one change in how the parse test is conducted,      excessive processing (even though it doesn't strictly violate
 the guessing learner can gain both a Licensing oracle which     the one-parse-per-sentence constraint imposed above). More
 eliminates useless grammar guesses, and also the powerful type  reasonable is to suppose that the parser employed by the

learner for superparsing is the same parser that will be used             it applies the supergrammar, augmented with all possible
throughout life for sentence comprehension. A standard                    parameter values. This could exact a heavy cost in on-line
assumption is that this is a serial device which, when it hits a          processing due to the massive ambiguity of sentences in
point of ambiguity, selects one structural analysis to pursue for         relation to the supergrammar, far greater in ma ny cases than
                  55
the sentence. (Parallel parsing models have been proposed,                ambiguity levels relative to a settled adult grammar. How-
but to conserve resources their parallelism is strictl y limited,         ever, the added cost of ambiguity is negligible as long as no
and their consequences for superparsing do not differ signifi-            attempt is made to compute all analyses of a sentence. For
cantly from those of serial parsing.) Thus the superparser may            a serial parser, alternative parses are eva luated only momen-
be faced with choices to make between al ternative ways of                tarily as each new word is encountered and attached into the
resetting parameters to assign an analysis to s. It can output            parse tree. They are not pursued through the sentence, and
only one of the A grammars that would satisfy Licensing. The              are not multiplicative. As soon as one of the alternatives has
choice between them might be random, or other selecti on                  been chosen, the others can be forgotten. And arguably, even
criteria must be invoked.                                                 the selection process is cost-free in a least-effort system, since
    Markedness and conservatism criteria (types (a) and (b))              it consists of adopting the first (simplest) attachment option
could be employed, as well as input-sensitive type (c) criteria.          that is computed (Frazier & Fodor, 1978; Lewis, 1999). The
The more powerful type (d) criteria such as Closeness and a               only difficult analyses will be (i) those the human sentence
minimal structure constraint are also available in this system.           parser has trouble with even when the grammar is settled, e.g.,
In fact, both of the latter are more or less automatic conse-             center-embedded constructions; and (ii) analyses which are
quences of superparsing given that the human parser is a least-           not complex in themselves but are systematically masked by
effort device (Inoue & Fodor, 1995). For instance, the Minimal            more attractive analyses allowed by the supergrammar.
Attachment parsing strategy entails that superparsing will                Grammar guessing models without superparsing would suffer
prefer simple, compact trees over more com plex ones; the                 from (i), but not from (ii). The incidence of such cases is not
learning device inherits this and so favor grammars that assign           known. They could lead to false negative reports from the
simpler structures. A conservative policy of staying close to             parser to the learner, indicating wrongly that s is not licensed
the previous grammar will result if, as is natural, the parser            by the grammar being tested. For examples and discussion
makes the effort of changing paramete r settings in Gc only               see Fodor (1998b).
when it is forced to do so to avoid parse failure. Again, parser             A requirement for smooth functioning of the superparsing
preference translates into learner preference. In much the same           routine is that the parameter values defined by UG are such
way, frequency sensitivity in parsing could lead to frequency-            that they can be added into a natural language grammar
sensitive learning (e.g. Cha rniak, 199 3).                               without altering its basic character. The competing values
    The exact mix of these various criteria remains to be estab-          of one parame ter must be able to co -exist in the same gram-
lished (e.g., Minimal Attachment versus minimal resetting of              mar without internal contradiction. And the parameter values
 parameters). The supergrammar model allows various policies              temporarily added into Gc to create the supergrammar should
 for resolving conflicts; which of these is adopted by human              be no harder to access and use on-line than other elements of
 learners is an empirical question. To the extent that these              natural language grammars. This may preclude any kind of
 criteria help the learner select an optimal grammar from among           precompiling process by which the combination of Gc and the
 those that license s, the fact that they can be applie d by a            added parameter values is reformulated for convenience in
superparsing learner means that its efficiency gain compared              parsing, since the computational costs of repeated compiling
with pre-parse selection criteria is even greater than was                would be added into the workload of the superparser. For
calculated above (Table 1). However, exact benefits are not               some kinds of parameters (e.g., Subjacency applies at Surface
easily quantified. The effects of Closeness and other such                Structure or at Logical Form; Huang, 1981/82) these condi-
rankings are complex, and are best assessed by simulation                 tions are hard to meet. But a variety of current linguistic
 studies. This awaits future research.                                    theories conceive of parameter values as fragments of tree
                                                                          structure (see Fodor, 1998c), and these 'treelets' do meet the
                                                                          needs of superparsing. They can be directly added into a
                  Limitations of Superparsing                             normal grammar to create another perfectly normal grammar,
                                                                          only slightly more complex than the original, and yet inc orpo-
 Does superparsing as a means of parameter setting carry                  rating all th e structural o ptions that UG per mits.
 significant costs to offset these advantages, so that no net gain           The one limitation of superparsing that is unavoidable is
 in efficiency results? This appears not to be so.                        that it delivers only one structural analysis for each sentence.
    As noted, the parsing routines need not be unusually power-           Because of the ban on excessive processing, it is impossible
 ful. The mechanism can be the normal huma n sentence pa rs-              for the parser to present the learning component with all
 ing device, which clearly must be present in chi ldren for               analyses for s, to compare and evaluate in order to make the
 comprehension of sentences already licensed by the current               best possible guess. The process of selecting one of the
 grammar. Thus, all that is special about the superparser is that         licensing grammars is piecemeal and order-dependent as each
                                                                          ambiguity must be resolved as it arises on-line. Interestingly,
                                                                          this appears to do relatively little damage, because there
5
  In a serial parse, the selected resolution of an ambiguity may prove to seems to be an excellent fit betwee n the choices made on-line
be incorrect, by failing on subseq uent words of the sentence. In a       by the human parser and the choices that a well-designed
garden path situation such as this the superparser would en gage in       learning device would be expected to favor: the minimization
reanalysis procedures just as the human parser normally does in the
case of a garden path.                                                    of derivational complexity, and the minimization of grammar

revision. Whether this is merely a coincidence is not clear, but                        Acknowledgements
at any rate it is fortunate for superparsing as a method for        This research was supported in part by PSC-CUNY Grants61595-00-
parametric decoding.                                                30 to Fodor an d 61403-0 0-30 to Teller.
                Summary and Conclusions
A start has been made here on quantifying the efficiency
advantage of a learning device which has the abi lity to read off
a set of parameter values for licensing a sen tence, in the course
of parsing that sentence in the normal way for com prehension.
                                                                                              References
The superparsing approach was developed originally for a            Berwick, R. C. and Niyogi, P. (1996) Learning fro m triggers.
different purpose. It was designed to provide a feasible ambi-        Linguistic Inquiry 27.4, 605-622.
guity detection syste m, so that all parametrica lly ambiguous      Charniak, E. (1993) Statistical Language Learning, MIT
input could be discarded. This permitted development of a             Press, Cambridge, MA.
                                                                    Chomsky, N. (1981) Lectures on Government and Binding,
non-guessing learning routine, capable of error-free parameter        Foris Publications, Dordrecht.
setting based exclusively on unambiguous items in the input         Chomsky, N. (1995) The Minimalist Program, MIT Press,
sample (Fodor, 1998a). Whether this is the best research goal,        Cambridge, MA.
either for modelling human learning or for engineering appli-       Clahsen, H. (1990/91) Constraints on parameter setting: A
cations, remains to be seen. The error-free lea rner has the          grammatical analysis of some acquisition stages. Language
advantage that it never has to re- set a par ameter. Also, once       Acquisition 1.4, 361-391.
it has set a parameter it can ignore the alternative value of that  Clark, R. (1989) On the relationship between the input data
parameter thereafter, so the size of the domain to be searched        and para meter se tting. NELS 19, 48-62. GLSA, UMass.
for Gt shrinks as learning proceeds. It is an empirical issue       Clark, R. (1992) The selection of syntactic knowle dge. Lan-
whether these benefits balance the nee d to discard all ambigu-       guage Acquisition 2.2, 83-149.
ous sentences in the input sam ple. It is therefore of interest,    Clark, R. (1994) Finitude, boundedness and complexity. In
as we have shown here, that superparsing can also make a              B. Lust, G. Hermon and J. Kornfilt (eds.) Syntactic The ory
                                                                      and First Language Acquisition: Cross-linguistic Perspec-
useful contribution to a grammar guessing routine.                    tives. Lawrence Erlbaum, Hillsda le, NJ.
   To the extent that the input sample does carry parametric        Fodor, J.D. (1998a) Unambiguous triggers. Linguistic Inquiry
information, superparsing allows the learner to exploit it.           29.1, 1-36.
Despite its modest consumption of resources, and despite its        Fodor, J.D. (1998b) Parsing to Learn. Journal of Psycho-
practical inability to list all parametric analyses of a sentence,    linguistic Research 27.3, 339-374.
the superparser nevertheless extracts from a sentence all the       Fodor, J.D. (1998c) What is a paramete r? Presidential Ad-
definitive parametric information it contains. If a sentence is       dress to the Lingui stic Society of America.
compatible with only one grammar in the whole domain, the           Frazier, L. and Fodor, J.D. (1978) The sausage machine: A
superparser will identify that grammar and the lea rning compo-       new two-st age parsing mode l. Cognition 6, 291-325.
nent will adopt it. If a sentence is less informative, e.g., is     Gibson, E. and Wexler, K. (1994 ) Triggers. Linguistic Inquiry
compatible with a thousand grammars, the superparser will             25.3, 407-454.
identify one of the thousand. (Which one of the thousand it         Gold, E.M. (1967) Language identification in the lim it. Infor-
                                                                       mation and Control 10, 447-474.
identifies depends on which ambiguity resolution criteria it        Huang, C.-T. J. (1981/82) Move Wh in a language without Wh
applies on-line.) All parameter values expressed unambigu-             moveme nt. The Linguistic Review 1, 369-416.
ously by a sentence will be set correctly by the time it has been   Inoue, A. and Fodor, J.D. (1995) Information-paced parsing
parsed. The values of the other paramete rs can only be                of Japanese. In R. Mazuka and N. Nagai (eds.) Japanese
guessed. They will be left unchanged from previous learning            Sentence Processing, Lawrence E rlbaum, Hillsdal e, NJ.
where possible; otherwise they will be changed to some combi-       Lewis, R. (1999) Attachment without competition: A compu-
nation of valu es which lice nses s.                                   tational model of race-based parsing. 12th Annual CUNY
   The superparsing learner is quite undemanding about the             Conference on Human Sentence Proce ssing.
nature of its input. For example, it does not require a language    Manzini, R. and Wexler, K. (1987) Parameters, binding
to contain unambiguous trigg ers for all of its pa rameter va lues.    theory and learnabili ty. Linguistic Inquiry 18.3, 413-444.
A simple random grammar guessing learner also needs no              Nyberg, E. (1992) A Non-deterministic Success-driven Model
                                                                       of Parameter Setting in Language Acquisition. Unpublished
unambiguous triggers, but that is because, as noted above, it          Ph.D. Dissertation, Carnegie Mellon University.
gains hardly more from unambiguous than from ambiguous              Sakas, W. and Fodor, J.D. (in press) The structural triggers
input. By contrast, a non-guessing error-free learner is very          learner. To appear in S. Bertolo (ed.) Parametric Lingui s-
choosy; it needs an unambiguous trigger for each parameter,            tics and Learnability: A Self-contained Tutorial for Lin-
and moreover it needs some of these to be fully unambiguous            guists, Cambridge University Press, Cambridge, UK.
(i.e., unambiguous with respect to all parameters they e xpress).   Wacholder, N. (1995) Acquiring Syntactic Generalizations
The superparsing learner has the dual virtues that it ca n use the     from Positive Evidence: An HPSG Model. Unpublished
information in fully unambiguous triggers when they are                Ph.D. Dissertation, City University of New York.
present, but it can also make progress when input sentences are     Wexler, K. (1999) Maturation and growth of grammar. In
ambiguous with respect to many (or even all) of the parameters         W.C. Ritchie & T.K. Bhatia (eds.) Handbook of Language
they express. Thus it is robust as well as efficient.                  Aquisition, Academic Press, San Diego.

