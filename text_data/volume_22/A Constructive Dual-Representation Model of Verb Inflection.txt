UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Constructive Dual-Representation Model of Verb Inflection
Permalink
https://escholarship.org/uc/item/5229b3wr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Author
Westermann, Gert
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        A Constructivist Dual-Representation Model of Verb In ection
                                       Gert Westermann (gert@cogsci.ed.ac.uk)
                         Institute for Adaptive and Neural Computation, Division of Informatics
                                          University of Edinburgh, 2 Buccleuch Place
                                              Edinburgh EH8 9LW, Scotland UK
                           Abstract                              two mechanisms interact to produce the in ected form.
                                                                 Marcus et al. (1995) proposed the Blocking Principle
   A constructivist neural network is presented that models      which states that a lexical entry (indicating an irregular
   impaired in ectional processing in German agrammatic          verb) blocks the application of the rule, but an imple-
   aphasia. The model is based on a single mechanism and
   develops two types of representation through a construc-      mentation of this principle (Nakisa et al., 1997) showed
   tivist learning process. The model accounts for data          that in practice it involves parameters for which a useful
   that has been taken as evidence for a dual mechanism          setting cannot be found. Therefore, the DMT remains
   theory of in ection, and it suggests an in ectional pro-      highly underspeci ed and thus hard to falsify. However,
   cessing system that is based not on a distinction between
   regular and irregular cases, but between in ections that      even in its underspeci ed form, the DMT is contradicted
   are easy and hard to learn. The model represents a suc-       by some empirical data, e.g., frequency e ects for regular
   cesful single-mechanism neural network account of verb        English past tense (Stemberger and MacWhinney, 1986)
   in ections.                                                   and regular Dutch plural (Baayen et al., 1997) forms,
                                                                 and similarity e ects for regular German participles in
                      Introduction                               agrammatic aphasia (Penke et al., 1999).
                                                                    In this paper I present a neural network model of
The debate between rule-based and association-based              in ectional processing in German agrammatic aphasia
theories of in ection has been continuing for many years         that accounts for dissociations between regular and irreg-
and has moved from the initial focus on the English past         ular forms without postulating two qualitatively distinct
tense to other languages such as the German participle           mechanisms. Instead, the model develops two types of
(e.g. Clahsen, 1999; Marcus et al., 1995). The reason            representations in a constructivist process, driven by the
for this shift is that in English, the issues of \regulari-      structure of the training data, and it displays emerging
ty" and \high frequency" are confounded which makes              areas of functional specialization that correspond largely,
it di√Ücult to distinguish between the di erent theories.         but not completely, to the distinction between regular
By contrast, in the German participle the regular case           and irregular forms. The trained model is lesioned in
does not apply to the majority of all verbs, making it a         di erent ways and it accounts for empirical data better
so-called \minority default" (Marcus et al., 1995).              than the DMT. Based on these results I propose a new
   A popular recent theory of how in ections are formed          theory of in ectional processing that is based on a dis-
is the Dual Mechanism Theory (DMT) that postulates               tinction not between regular and irregular, but between
two qualitatively distinct mechanisms for the production         \easy to learn" and \hard to learn" forms.
of regular and irregular cases (e.g. Clahsen, 1999; Pinker,         The rest of this paper is organized as follows: rst, the
1991, 1997; Marcus et al., 1995). According to the DMT,          structure of the German participle and the impairment
regular in ections are produced by a mental symbolic             pro les observed in agrammatic aphaisa are reviewed.
rule, whereas irregulars are stored in an associative lex-       Then, the network model, the data, and the training
icon. Based on these mechanisms, the DMT claims to               regime are described, followed by a detailed analysis of
account for di erences in the processing of regular and          the performance of the model in comparison with agram-
irregular in ections: whereas regular forms are applied          matic aphasics. Finally, the resulting new theory of
productively to novel forms independently of their simi-         in ectional processing is presented and related to the
larity to existing forms (e.g., faxed ), irregular in ections    DMT.
show similarity e ects both in existing \families" (read
!   read, lead  !  led, breed !   bred ) and in the extension
                                                                              The German Participle
to novel forms (cleed    !  cled ).
   However, while considerable empirical research has es-        German participles are comparable in usage to the En-
tablished processing di erences between regular and ir-          glish past tense in describing an event in the past. There
regular forms on many di erent levels from acquisition           are three groups of participles: Weak participles are
over psycholinguistic and ERP studies to impaired adult          formed by a (prosodically determined) pre x ge-, the
processing (see Clahsen, 1999, for an overview), little                                                            !
                                                                 verb stem, and the ending -t, e.g., sagen (say) gesagt
progress has been made in the speci cation of the DMT.           (said). Strong participles take the ending -en, e.g., geben
Particularly problematic is the question in which way the               !
                                                                 (give) gegeben (given) and they may also change the

verb stem, e.g., gehen (go)     ! gegangen (gone). A few    and irregular participles (the remaining two made more
strong verbs have idiosyncratic participle forms, e.g.,     irregular errors but their total number of errors was too
           !
sein (be) gewesen (been). The third group are mixed         small to establish a signi cant di erence between regu-
verbs that take the weak ending -t but change their         lars and irregulars). Penke et al. (1999) concluded that
stems like strong verbs, e.g., wissen (know)    !  gewusst  irregular in ection can be selectively imapired in agram-
(known). It is generally claimed that the weak verbs        matic aphasia.
form the regular class, while strong verbs are irregular,
and the terms regular and irregular will here be used in                         The Network Model
this sense.
   In contrast to English, German does not have a ma-       For the simulations described in this paper, a construc-
jority of regular tokens (each verb counted according to    tivist neural network (CNN) model was developed that
how often it occurs in a corpus), and the majority of       builds the hidden layer of a radial basis function (RBF)
types (each verb counted just once) is less pronounced      network. Each hidden unit has a Gaussian activation
than in English.                                            function and thus acts as a receptive eld for an area
   The CELEX database (Baayen et al., 1993) lists 3015      of the input space. The problem in building RBF net-
German participles. After cleaning out some obvious er-     works is to decide on the number and positions of these
rors and homophones and choosing the more frequent of       receptive elds. The CNN algorithm solves this problem
di erent participle forms of one stem, 2992 participles     by constructing the hidden layer during learning, adding
remain. However, German verbs are often formed by           units when and where they are needed. The network
modifying other existing verbs with a pre x or separa-      starts with just two units in the hidden layer, each cov-
ble particle, e.g., the simplex verb fahren (drive) occurs  ering roughly half of the input space (see gure 1). The
in CELEX in 28 composite forms such as hinausfahren,        network tries to learn the task with this architecture
losfahren, fortfahren etc. (drive out, drive o , continue). (by adjusting the weights with quickprop), and when
Since a pre x or particle do not alter the way in which     learning no longer improves the performance, a new unit
the participle of a simplex verb is formed, all composite   is inserted. The place where the new unit is inserted
forms were combined into one simplex form.                  is determined by the classi cation error resulting from
   For the simulation experiments described below,          treating inputs within one receptive eld as similar: the
20,000 verb tokens were randomly extracted from this        receptive eld that previously caused the highest error is
corpus according to their frequency. To ensure that each    shrunk and the new unit is inserted next to it. The idea
verb occurred at least once, all verb types which had not   here is that a unit which produces a high output error is
been randomly selected were added onto the resulting        inadequate, and therefore more structural resources are
corpus with a token frequency of one (this applied to 18    needed in that area. A similar network has already been
verbs).                                                     successfully used to model the acquisition of the English
   The structure of the resulting training corpus is shown  past tense (Westermann, 1998).
in table 1.
                                                                            Initial                       Final
                       type               token
     Regular     518     (78.01%)    9306    (46.49%)
     Irregular   134     (20.18%)    9717    (48.54%)                                  x lachen      11
                                                                                                     00                   x 00
                                                                                                                            11
                                                                                                                             lachen
                                                                                                                            00
                                                                                                                            11
                                                                                                              11
                                                                                                              00 000
                                                                                                                 111
     Mixed         12     (1.81%)     995     (4.97%)                               x machen
                                                                                                         00
                                                                                                         11 1111
                                                                                                              00 111
                                                                                                                 000   x machen
                                                                 x schw√∂ren
                                                                         11
                                                                         00         00
                                                                                    11
                                                                                                 x schw√∂ren
                                                                                                            00
     Sum         664    (100.00%)   20018   (100.00%)                    00
                                                                         11         11
                                                                                    00                      00
                                                                                                        111 11
                                                                                                  h√∂ren 000
                                                                   h√∂ren
                                                                                                                         00
                                                                                                                         11
                                                                x                               x
                                                                                                 00
                                                                                                 11              000
                                                                                                                 111     11
                                                                                                                         00
      Table 1: The structure of the training corpus.                                                     000
                                                                                                         111
                                                                                                                     11
                                                                                                                     00
                                                                                                                     11
                                                                                                                     00
               Agrammatic Aphasia                           Figure 1: Receptive elds covering the input space at
Agrammatic (Broca's) aphasia is a language disorder         the beginning (left) and the end (right) of learning.
that is generally caused by a stroke predominantly a ect-
ing anterior parts of the left hemisphere. One of the char-    Figure 1 shows a hypothetical start and end state in
acteristic symptoms of Broca's aphasia is the tendency      a two-dimensional input space. While initially only two
to omit or confuse in ections. Investigating the precise    units cover the whole of the space, later hidden units
nature of these de cits can therefore lead to insights into have been inserted with di erent densities across the
the internal representation of in ectional morphology.      space to account for the speci c learning task.
Penke et al. (1999) analyzed data from eleven aphasic          Figure 2 shows the network architecture. The input
subjects who each produced 39 regular and 39 irregular      layer takes a phonological representation of the verb in-
participles in a sentence completion task with respect        nitive, and the output layer has one unit for each pos-
to regular and irregular errors, overregularizations and    sible output class (see below). The hidden layer initially
irregularizations, frequency e ects, and e ects of ablaut-  consists of only two units but is grown during learning.
patterns on error rates. They found irregular in ections    There are direct connections from the input to the out-
to be selectively imapaired in six of the subjects, and     put layer, and each hidden unit is fully connected to the
three showed no signi cant di erence between regular        output layer.

                              gezogen
                                                                                     Output Layer       Localized Lesioning
                                                                                   (Inflection class)
                                                                                                        The output in the CNN model is produced through two
                                                                                                        sets of connections: the direct connections between the
                                                 Hidden-Output
                                                 Connections
                  Input-Output
                  Connections
                                                                                   Hidden Layer with
                                                                                   Gaussian Units       input and the output layer that the network started out
                                                                                   (receptive fields)   with, and the connections from the growing hidden to
                                                            constructed                                 the output layer. A localized lesioning of these path-
00000
1111111111
00000
11111
          0000011111
     0000011111
          0000000000
                1111111111
                     1111111111
                     000000000000000
                          1111111111
                               1111111111
                                    000001111111111
                                    1111111111
                                         000001111111111
                                              00000000001111111111
                                                   1111111111
                                                        000000000000000
                                                             1111111111
                                                                  1111111111
                                                                       00000       Input Layer          ways in the CNN resulted in a double dissociation be-
     00000
111111111111111
          0000000000
                0000000000
                     00000000000000000000
                          00000000000000000000
                                         0000000000
                                              0000000000
                                                   0000000000
                                                        0000000000
                                                             0000000000
                                                                  0000000000
                                                                       11111
000000000011111
     11111      111111111111111111111111111111111111111111111111111111111111
                                                                       00000       (Template)
                                                                                                        tween regular and irregular verbs for four out of the ve
                                                                                                        runs. The further analyses were conducted with these
                        ‚Äôt    s        i                       @ n
                              ziehen
                                                                                                        four networks.
     Figure 2: The initial architecture of the network.
                                                                                                                                100
                                               Data
                                                                                                                                                                                        regulars
                                                                                                                                 90                                                     irregulars
                                                                                                                                                                                        mixed
                                                                                                                                 80
The 664 German verbs were classi ed according to the
way in which their participles are formed, resulting in a
                                                                                                                                 70
total of 22 classes, one of which was the \stem+-t " (reg-                                                                       60
                                                                                                              % still correct
ular) class, 6 were for mixed verbs, and 15 for irregular                                                                        50
verbs.                                                                                                                           40
   The verbs were represented phonologically, and each                                                                           30
phoneme was encoded by a 7-bit feature vector with fea-
tures such as fricative, plosive, voiced etc. for consonants,
                                                                                                                                 20
and front, high, open etc. for vowels. Presence of a fea-                                                                        10
ture was encoded with 1 and absence with -1.                                                                                      0
                                                                                                                                      hidden‚àíoutput                      input‚àíoutput
   For the training of the network, the phonological                                                                                                  Connections lesioned
representation of the in nitive of each verb was then                                                   Figure 3: Double dissociation between regular and irreg-
inserted into a template consisting of three syllables:                                                 ular (and mixed) verbs after lesioning the two pathways
XCCCVVCC-XCCCVVCC-XCCCVVCC; C stands for                                                                in the networks.
consonant, V for vowel, and X for whether the sylla-
ble is stressed or not. Since the endings of verbs are                                                     Figure 3 shows the results of lesioning the hidden-
signi cant for the determination of the participle class,                                               output (HO) and the direct input-output (IO) con-
the verbs were right-aligned in this template so that the                                               nections. Lesioning the HO connections resulted in a
endings occurred in the same slots.                                                                     marked decrease of the performance of irregular and
                                                                                                        mixed verbs, with regular in ections remaining nearly
   The resulting network had 150 input units (three syl-                                                fully intact. By contrast, lesioning the IO connections
lables with seven phonemes each represented by seven                                                    resulted in the opposite pro le: performance of regulars
features, plus one stress-bit per syllable), and 22 output                                              was signi cantly more impaired than that of irregular
units for the 22 in ection classes.                                                                     and mixed verbs. It is important to note that this dou-
                                                                                                        ble dissociation emerged as a result of the structure of
                                           Training                                                     the training data together with the constructivist devel-
                                                                                                        opment of the model and was in no way prespeci ed.
The task to be learned by the network was the mapping                                                      Removing the HO connections in the network thus
from the phonological representation of the verb in ni-                                                 modeled the basic de cit in the in ection of agrammatic
tive to the class of its participle. Viewing the learning                                               aphasics, namely, the breakdown of irregular and selec-
of the participle as a classi cation task avoids confound-                                              tive sparing of regular participles. Based on this result,
ing it with phonological details such as di erent pro-                                                  the performance of the HO-lesioned CNN models was
nunciation of regular forms depending on the last stem                                                  investigated with respect to the more detailed results re-
phoneme (e.g., holen                       !
                           geholt vs. landen    gelandet ).                    !                        ported by Penke et al. (1999).
   Five CNN models were trained on this corpus with                                                        Penke et al. (1999) found that all subjects who made
di erent random initial weight settings. The networks                                                   more errors on irregulars than on regulars overgeneral-
were tested before the insertion of a new hidden unit.                                                  ized the regular ending -t to irregular verbs, but they
An output class was counted as correct when the corre-                                                  only rarely irregularized regular verbs (i.e., their regu-
sponding unit, but no other unit, had an activation value                                               lar errors consisted mainly in using a wrong su√Üx or
over 0.7.                                                                                               none at all). Testing the four corresponding CNN mod-
                                                                                                        els for this behavior showed a good match of the aphasic
                                           Results                                                      pro les: the networks over-applied the regular class to
                                                                                                        73.7% of all wrong irregulars (aphasics: 63.3%), but only
In order to model agrammatic aphasia, the CNN was le-                                                   6.5% of all regular errors were irregularizations (apha-
sioned in di erent ways. It was assumed that the removal                                                sics: 14.3%). The other errors that can be made by the
of weights in the model corresponds to the destruction                                                  CNN models are no output, or ambiguous output when
of neural tissue in the brain by a stroke.                                                              two (or more) output units are simultaneously activated.

   Based on the assumption of two qualitatively distinct      Global Lesioning
processing mechanisms for regular and irregular in ec-        As shown in the previous section, the lesioning of the HO
tions, Penke et al. (1999) predicted and found a fre-         pathway in the CNN model can account for a selective
quency e ect in the aphasic production of irregulars, but     impairment in the in ection of irregular verbs and thus
not of regulars: there were signi cantly more errors for      model the performance of agrammatic aphasic subjects.
infrequent irregulars than for frequent ones, but no such     This selective and total lesioning of one pathway might
e ect occurred for regulars. When tested on the same          suggest that the processing of regular and irregular verbs
verbs as the aphasic subjects, the CNN models equally         is subserved by locally di erent brain structures that can
showed a small frequency e ect for irregulars but not         be selectively a ected by a stroke. To establish whether
for regulars: the error rate for low frequency irregulars     the observed pro le could be modeled without this as-
(93.3%) was signi cantly higher than for high frequency       sumption, the e ects of globally lesioning the network to
irregulars (89.0%) (Wilcoxon, p = 0:068), but error rates     di erent degrees was investigated, without making a dis-
for regulars did not di er statistically (1.7% for low fre-   tinction between the IO and the HO connections. Over
quency and 2.4% for high frequency regulars, p = 0:273).      200 trials, the network was lesioned in 5%-steps by ran-
   Alternatively to a qualitative distinction, regulars and   domly removing weights from both sets of connections.
irregulars might represent two ends of a continuum: a
regular verb can be said to be \very regular" if it is
similar to other regulars and dissimilar to irregulars. It
                                                                                                         Distribution of 4000 regular/irregular error pairs
                                                                                     100
is \less regular" if it is dissimilar to other regulars but
similar to irregulars. The reverse is true for irregulars                             90
(see also Daugherty and Seidenberg, 1992).                                            80
   This assumption is attractive because it integrates
mixed verbs which fall between regulars and irregulars
                                                                                      70
in that they combine an irregular stem with the regular
                                                              % irregulars correct
                                                                                      60
ending. Mixed verbs are generally ignored in the DMT
because they are hard to consolidate with the proposed                                50
qualitative distinction between regulars and irregulars.                              40
   A regularity continuum would predicted that \less reg-
ular" regulars, being more similar to irregulars, should                              30
be more error prone than \very regular" regulars in                                   20
agrammatic aphasics. Penke et al. (1999) analyzed the
distribution of verbs with respect to stem vowels and                                 10
found that for the stem vowel <e>, irregulars outnum-                                  0
ber regulars, making regulars with this stem vowel less                                    0   10   20    30         40       50        60
                                                                                                                       % regulars correct
                                                                                                                                                     70       80   90   100
regular. Therefore, regular verbs with <e> should have
a higher error rate because they are similar to irregulars.   Figure 4: Performance on regulars vs. irregulars for 200
   This prediction was con rmed in the analysis of the        lesioning trials at 20 lesioning steps each (in 5%-steps).
aphasic data: all regular su√Üxation errors occurred with      Greyscale indicates degree of lesioning (from dark to
<e>-stems. While Penke et al. (1999) interpreted their
                                                              light). Data for the aphasic subjects are marked by cir-
results within the framework of a qualitative distinction     cles.
between regulars and irregulars (allowing grading e ects
for both mechanisms with the qualitatively distinct verb         The result of this global lesioning is shown in gure 4.
groups in uencing each other), a more plausible interpre-     The 4,000 lesioned networks showed some variety of reg-
tation is that of a regularity continuum where a single       ular vs. irregular errors, but, like with the aphasic sub-
mechanism underlies the production of both forms.             jects, there was never a selective sparing of irregulars
   Testing the CNN model, which is based on such a            with a breakdown on regular participles (top left of the
single mechanism, for this e ect yielded the same pattern     plot). Instead, in most cases impairment of irregulars
of results as in the aphasic subjects: when tested on the     was stronger than of regulars (below the diagonal).
same verbs, 4 out of 5 of the regular errors were for the        The data for the eleven aphasic subjects from (Penke
stem vowel <e>, indicating that these verbs are treated       et al., 1999) are also displayed in gure 4. All aphasic
more like irregulars.                                         data are within the range of performance predicted by
   In summary, by lesioning the HO connections in the         the simulations, showing that although there is variabil-
CNN model, detailed aspects of the performance of             ity in the performance of agrammatic aphasics, di er-
agrammatic aphasics on German participle in ections           ently lesioned CNNs can model the performance of each
could be modeled. These results comprise both those           of them. The model is not over-general, however: like in
that have been claimed to be evidence for the dual            aphasic subjects, a selective sparing of irregulars with a
mechanism theory (double dissociations; frequency ef-         breakdown of regular in ections did not occur in any of
fects only for irregulars) and those that contradict the      the lesioning trials.
predictions of the dual mechanism theory (regularity             Why does global lesioning in the CNN lead to a pro-
continuum e ect).                                               le in which irregular participles are more impaired than

regulars? An answer to this question can be found by         its structure. The CNN is therefore a single mechanism,
analyzing the connections in the model. Many of the IO       but dual representation model. This dual representation
connections are inhibitory, suppressing the activation of    view sheds a di erent light on the dissociations between
the wrong in ection class by other IO connections. This      regular and irregular forms. The DMT does not assume
pro le is due to the distributed representation of the in-   that any regular verbs are produced by the irregular
put: overlapping representations between classes make        mechanism, or vice versa. The common aphasic pro-
the inhibition of wrongly activated classes necessary, and     le where both regular and irregular cases are partially
with increased lesioning this inhibition is lost, resulting  impaired (albeit to di erent degrees) is therefore often
in the activation of wrong output classes for regular and    attributed to performance errors or the unpredictability
irregular verbs equally. By contrast, the HO connections     of aphasic impairment.
from one receptive eld usually contain only one strongly        A more compelling explanation is o ered by the CNN:
excitatory weight to the correct output class. Therefore,    here, the dissociations that become visible in the lesion-
the HO weights do not tend to activate a wrong output        ing trials do not run clearly along the lines of regulars
class. This di erent weight structure can be explained by    vs. irregulars. Instead, all verbs for which the in ection
the localist nature of the receptive elds: due to the con-   class cannot be learned in the direct IO pathway are
structivist growth process, receptive elds tend to cover     shifted to the developing hidden layer and the HO path-
only verbs from one class. Therefore, representations for    way. This shift concerns regular, irregular, and mixed
di erent classes do not overlap and inhibition is not re-    verbs, to di erent degrees. The dissociation between
quired. An analysis of the distribution of the receptive     verbs is thus better described as easy to learn vs. dif-
  elds over the verbs showed that they had been preferen-      cult to learn, with the di√Ücult forms relying on the
tially allocated for the di√Ücult-to-learn irregular verbs.   hidden layer, whereas easy forms are produced in the IO
Therefore, a partial lesioning of the HO connections af-     pathway alone. This distinction can account better for
fected predominantly irregulars. Taken together, irregu-     the data such as mixed verbs, a regularity continuum, or
lars were impaired by the removal of weights in both the     the di erent aphasic pro les.
IO and the HO connections, while regulars were a ected          But what factors determine whether a form is easy or
only by lesioned IO connections. Together, a global le-      di√Ücult to learn? The degree of di√Üculty is determined
sioning therefore led to a more pronounced breakdown         by several interacting distributional factors that can be
for irregulars than for regulars.                            derived from the principles of associative learning:
   A global lesioning pro le in which regular in ections
are selectively impaired could only arise from a total le-  1. Frequency: a frequent transformation is easier to learn
sioning of the IO connections together with no or weak          than an infrequent one. Therefore, in ection classes
lesioning of the HO connections. Based on the CNN               with a high summed token frequency will be easier to
model therefore the prediction is made that a selective         learn than those that only apply to rare verbs.
impairment of regular in ections in aphasics would be
evidence for a locally separate processing of regular and   2. Class size: a transformation that applies to many dif-
irregular in ections in the brain, whereas the selective        ferent verbs is easier to learn than one that just applies
impairment of irregulars cannot be taken as evidence for        to one verb. Therefore, in ection classes with many
such a separation.                                              members (counted in types) are easier to learn than
                                                                those con ned to only a small group of verbs.
  A Dual-Representation Theory of Verb                      3. Similarity of class members to members of other
                        In ection                               classes: the in ection class of a verb is easier to learn
The results described in this paper show that the CNN           if other similar verbs share the same class.
can account for detailed empirical results from agram-      4. Ambiguity of in ectional morpheme: an in ection is
matic aphasic in ectional processing. At the same time,         easier to learn if it applies uniquely to members of
the CNN avoids the problems of the DMT, namely,                 its class, i.e., if it does not exist in other context as
underspeci cation and contradiction to some empirical           well. For example, the -ed su√Üx in English is highly
data.                                                           indicative of the past tense: an analysis of the CELEX
   Whereas the DMT proposes two mechanisms operat-              corpus showed that 99.6% of all word types in English
ing on a single representation of a verb stem, the CNN          that end in -ed are past tense forms. By contrast, the
develops so that a single mechanism operates on two             German irregular participle ending -en is much more
representations of the verb. Initially, the direct phono-       ambiguous: it also occurs in verb in nitives (gehen, to
logical input is used in the IO pathway to produce the          go), noun plurals (Wiesen, meadows), and as part of
output class. For verbs for which the output cannot             noun singulars (Drachen, kite).
be learned based on this structural representation alone,
the CNN develops through a constructivist process ad-           These factors in uence each other, and further re-
ditional representations in the hidden layer. In contrast    search will be needed to establish in detail how they
to the structure-based input representations, these new      interact. Nevertheless they show that the regular |
representations are identity-based and localist: the acti-   irregular distinction is a good rst approximation of
vation of a hidden unit receptive eld only indicates the     the easy |di√Ücult distinction: the regular in ection, al-
presence of a certain input, without information about       though it does not apply to the most frequent individual

verbs, is the single most frequent in ection in both En-   to empirically verify the dual-representation model of
glish and German: 57.2% of English past tense tokens       verb in ection.
and 46.89% of German participle tokens are regular. At        While connectionist, single-mechanism models of in-
the same time, these classes are also the biggest in type   ections have been rejected by proponents of the DMT
size (88.4% and 64.7%, respectively). However, the third   (e.g. Clahsen, 1999; Pinker, 1997; Marcus et al., 1995),
point, similarity of class members to members of other     the CNN model presents evidence that such models can
classes, does not separate along the lines of regular and  account for in ectional processing more successfully than
irregular verbs: many regular verbs are similar to irreg-  theories that rely on qualitatively distinct processing
ulars which should make them harder to learn in this       mechanisms.
view. And in fact the regularity continuum that has
been shown for aphasics indicates that regulars that are   Acknowledgements The author is now at Sony Com-
similar to irregulars are more prone to impairment than    puter Science Lab, 6 rue Amyot, 75005 Paris, France.
others, that is, they rely more on storage in the lexicon. (gert@csl.sony.fr).
   A similar analysis of factors in uencing errors in past
tense formation has been conducted with school children                          References
(Marchman, 1997), where their errors on an elicited past   Baayen, H., Piepenbrock, R., and van Rijn, H. (1993).
tense production task were determined by frequency, the       The CELEX Lexical Database. CD-ROM. Linguistic
number of similar sounding stems in the same and in           Data Consortium. University of Pennsylvania, PA.
di erent in ection classes, and the phonological charac-
teristics of the stem and past tense forms.                Baayen, R. H., Dijkstra, T., and Schreuder, R. (1997).
   Taken together, although the dissociations of verbs        Singulars and plurals in Dutch: Evidence for a parallel
into easy and di√Ücult corresponds largely to the regular-     dual-route model. Journal of Memory and Language ,
irregular dissociation, it nevertheless suggests that the     37(1), 94{117.
regular case is a post-hoc extraction and idealization of  Clahsen, H. (1999). Lexical entries and rules of language:
the developed structure of the in ectional processing sys-    A multidisciplinary study of German in ection. Be-
tem.                                                          havioral and Brain Sciences , 22(6), 991{1013.
                                                           Daugherty, K. and Seidenberg, M. S. (1992). Rules or
                      Discussion                              connections? The past tense revisited. In Proceedings
                                                              of the 14th Annual Conference of the Cognitive Sci-
The results presented in this paper suggest a novel ac-       ence Society , pages 259{264, Hillsdale, NJ. Erlbaum.
count of in ection learning and processing: it is a single Marchman, V. A. (1997). Children's productivity in the
mechanism system in which dual representations emerge         English past tense: The role of frequency, phonology,
from a constructivist learning process together with the      and neighborhood structure. Cognitive Science , 21(3),
structure of the environment. The system separates            283{304.
verbs along the lines of easy vs. hard to learn and can
thus better explain empirical results that have so far     Marcus, G., Brinkmann, U., Clahsen, H., Wiese, R., and
been taken as evidence for the Dual Mechanism Theory.         Pinker, S. (1995). German in ection: The exception
The qualitative distinction between regular and irregular     that proves the rule. Cognitive Psychology , 29, 189{
in ections that lies at the core of the DMT, is a projec-     256.
tion of formal linguistic analysis onto the human data.    Nakisa, R. C., Plunkett, K., and Hahn, U. (1997). A
Because according to formal linguistics, human language       cross-linguistic comparison of single and dual-route
data does not correspond to the abstract \competence"         models of in ectional morphology. In P. Broeder and
but is instead corrupted as \performance", any data that      J. Murre, editors, Cognitive Models of Language Ac-
does not correspond to the predictions of the formal the-     quisiton . MIT Press, Cambridge, MA.
ory (i.e., regulars that behave like irregulars and vice   Penke, M., Janssen, U., and Krause, M. (1999). The
versa) can therefore be attributed to performance. This       representation of in ectional morphology: Evidence
method makes the DMT hard to falsify based on such            from Broca's aphasia. Brain and Language , 68, 225{
data. By contrast, the CNN model is fully speci ed,           232.
and it shows how the actual human data can be mod-         Pinker, S. (1991). Rules of language. Science , 253, 530{
eled without recourse to a competence-performance dis-        535.
tinction. Whereas the abstract category of \regularity"    Pinker, S. (1997). Words and rules in the human brain.
remains a good formal description of language structure,      Nature , 387, 547{548.
the fallacy is in drafting it into service as a processing
category, as done in the DMT.                              Stemberger, J. P. and MacWhinney, B. (1986). Fre-
                                                              quency and the lexical storage of regularly in ected
   A way to test the validity of the CNN model empir-         forms. Memory & Cognition , 14, 17{26.
ically is to abandon the regular/irregular distinction in
favour of an easy/hard distinction, by identifying \hard"  Westermann, G. (1998). Emergent modularity and U-
regulars and \easy" irregulars. Such a distinction should     shaped learning in a constructivist neural network
then better predict impairment pro les in agrammatic          learning the English past tense. In Proceedings of the
aphasics and other aspects of dissociations in in ectional    20th Annual Conference of the Cognitive Science So-
systems. More research along these lines will be needed       ciety , pages 1130{1135, Hillsdale, NJ. Erlbaum.

