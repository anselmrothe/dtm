UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Fast and Frugal Use of Cue Direction in States of Limited Knowledge
Permalink
https://escholarship.org/uc/item/8103t135
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Persson, Magnus
Juslin, Peter
Publication Date
2000-01-01
Peer reviewed
  eScholarship.org                              Powered by the California Digital Library
                                                                 University of California

              Fast and Frugal Use of Cue Direction in States of Limited Knowledge
                                     Magnus Persson (Magnus.Persson@psyk.uu.se)
                                             Department of Psychology, Uppsala University
                                                 Box 1225, SE-751 42, Uppsala, Sweden
                                            Peter Juslin (Peter.Juslin@psy.umu.se)
                                              Department of Psychology, Umeå University
                                                        SE-901 87, Umeå, Sweden
                             Abstract                                on-the-spot detection of cue directionality. Is this possible
                                                                     without violating psychological plausibility? In this paper,
   An exemplar-based algorithm, PROBEX, (Juslin & Persson,           we show that PROBEX (PROBabilities from EXemplars:
   1999) is shown to make robust decisions in multiple-cue in-       Juslin & Persson, 1999) is such an algorithm. It relies on
   ference tasks when very few exemplars are known. We dem-          similarity-guided retrieval of exemplars to capture nonlinear
   onstrate the crucial role of knowledge of cue directions for      relationships and to estimate cue directions. An added bonus
   performance and confront PROBEX with an artificial envi-          is that because PROBEX belongs to the class of lazy algo-
   ronment specifically construed to favor a non-compensatory
                                                                     rithms (Aha, 1997), which do not require pre-computed
   algorithm like Take The Best (Gigerenzer & Goldstein, 1996).
   PROBEX is demonstrated to perform well even in these unfa-
                                                                     knowledge, this is achieved in a fast and frugal fashion.
   vorable conditions. The explanation for the robust perform-          An important development in the judgment literature is the
   ance is that PROBEX approximates Dawes Rule by using in-          concern with evaluating cognitive algorithms within real
   formation of the cue directions for all cues, while yet making    environments (Gigerenzer & Goldstein, 1996; Gigerenzer,
   few a priori assumptions about the structure of the environ-      Todd, & the ABC-group, 1999). Gigerenzer and Goldstein
   ment.                                                             (1996) demonstrate that, when applied to the structure of a
                                                                     real environment, simple heuristics that only rely on a single
                         Introduction                                cue perform on par with complex algorithms that integrate
                                                                     multiple cues. Take-The-Best (TTB) relying on the single
In this paper we will discuss the importance of knowledge of
                                                                     most valid cue that is applicable, performed as well as linear
cue directions for probabilistic inference. The cue direction
                                                                     multiple regression that integrates 9 cues. It was concluded
refers to the valence of the relationship between two vari-
                                                                     that although TTB falls short of classical norms of rational-
ables, for example, as represented by the sign of a correla-
                                                                     ity, it provides the same accuracy at a minimum of compu-
tion, or a coefficient in a linear equation (e.g., a beta weight
                                                                     tation: It is ”fast and frugal”. One shortcoming of the simu-
in linear regression). We will concentrate on a simple binary
                                                                     lations in Gigerenzer and Goldstein was that all algorithms
choice task format of the following sort, “Which German
                                                                     were provided with a priori knowledge of cue directions.
city has the higher population: a) Bonn, b) Hamburg?”. In
                                                                        Connectionist, exemplar-based and decision-tree archi-
this case, knowledge of a cue direction corresponds to
                                                                     tectures have been shown to compete evenly with TTB in
knowing if a binary probability cue, say, that Hamburg, but
                                                                     regard to accuracy (Chater et al, 1999). Specifically, in Jus-
not Bonn, has a soccer team in the Bundesliga, increases or
                                                                     lin and Persson (1999) it was shown that PROBEX outper-
decreases the probability that alternative (a) (Bonn) is the
                                                                     formed TTB and linear multiple regression in regard to
correct answer to the question.
                                                                     accuracy while relying on no pre-computed knowledge.
   The constraints on a plausible cognitive mechanism for
                                                                     PROBEX further provided a good quantitative fit to the
learning cue-directions are complex and multifaceted. In
                                                                     quantitative point-estimates, binary decisions, and probabil-
some tasks, learning cue-directions is not important because
                                                                     ity judgments made by human participants (see Dougherty,
they are known a priori. In those cases, it is the relative
                                                                     Gettys, & Ogden, 1999, for a similar approach).
weight of the cues that is important. On the other hand, it is
                                                                        In this paper, we complement Juslin and Persson (1999) in
sometimes proposed that knowledge of cue directions is the
                                                                     three respects: First, we illustrate the crucial role of esti-
crucial aspect of learning to make probabilistic inferences
                                                                     mating cue directions for the performance of any algorithm.
(Dawes & Corrigan, 1974). Moreover, inferences have to be
                                                                     Second, we explore boundary conditions for the robust per-
made for new and unexpected tasks for which little previous
                                                                     formance of PROBEX by exposing it to an environment
experience is available. This means that the system cannot
                                                                     deliberately construed to favor a non-compensatory algo-
rely extensively on pre-computed knowledge—which cues
                                                                     rithm like TTB and a linear, additive algorithm like linear
are predictive, and the direction of predictive validity, has to
                                                                     multiple regression. Finally we present a simple demonstra-
be detected on the spot. Finally, a flexible algorithm should
                                                                     tion to elucidate why evolution should favor decisions algo-
map both linear and nonlinear aspects of an environment.
                                                                     rithms that are robust in states of limited knowledge.
   These constraints boil down to whether we can find an al-
gorithm integrating the ability to represent non-linearity with

                   PROBEX—The Algorithm                          ∀y ( y ∈ Kn ) , P (x = y ) = S (t , y )                         (2)
Many theories in cognitive science stress the storage of                                 n  n
                                                                                                          ∑ S (t , z )
                                                                                                         z ∈Kn
exemplars (traces, instances) (e.g.; Kruschke, 1992; Logan,
1988; Medin & Schaffer, 1978; Nosofsky, 1984, Nosofsky              The summation in the denominator is performed across
& Palmeri, 1997). One property of exemplar-based models          exemplars not yet sampled. A response is generated at itera-
is that they describe algorithms that respond to both fre-       tion N , where the decision rule specified below terminates
quency and similarity. In this respect, they map onto well-      the sampling process. N is a random variable, the distribu-
known properties of human probability judgment (Juslin &         tion of which can be used to predict response times.
Persson, 1999). PROBEX was developed from one of the                To estimate the target value v '(t ) of the new exemplar t ,
well-known and successful exemplar-based model, the con-         the target values v( xi ) of the retrieved exemplars xi are
text model (Medin & Schaffer, 1978; Nosofsky, 1984).             considered. The estimate of the target value at iteration n is,
PROBEX amends the context model in the following re-                                            n
spects (see Juslin & Persson, 1999): (a) With a sequential                                    ∑ S ( t , x )v( x ) ,
                                                                                                           i     i
                                                                                                                                 (3)
sampling mechanism that allows prediction of response                         v ′ ( t , n) =  i =1
                                                                                                     n
times (as such PROBEX provides a humble cousin of the
EBRW model presented by Nosofsky & Palmeri, 1997); (b)
                                                                                                   ∑ S (t , x )
                                                                                                   i =1
                                                                                                               i
A dampening in order to predict pre-asymptotic performance          a weighted average of the retrieved target values, where
(see also Nosofsky et al., 1992); and (c) response rules that    the similarities are weights. The final estimate is
allows prediction of point-estimates and subjective prob-        v '(t ) = v '(t , N ) where N is the first iteration where the con-
ability judgments. The simple stopping rule and the mecha-       ditions for the stopping rule are satisfied. Eq. 3 can also be
nisms for point-estimation and probability judgment are the      produce probability assessments (Juslin & Persson, 1999).
main differences from previous exemplar-based models.               The sampling of exemplars is terminated at the first itera-
   Knowledge of the environment is modeled by an R × C           tion N where the following condition has been satisfied. The
matrix, with R exemplars, C cue dimensions and one vector        stopping rule is:
with R target values. The exemplars in the knowledge matrix
represent distinct psychological entities, either traces of                    v ′ (t , n) − v ′ (t , n − 1) < k ⋅ v ′ (t , n) . (4)
dated events from episodic memory or semantic knowledge.            The free parameter k decides the sensitivity of the stop-
Exemplars are described in terms of binary feature values,       ping rule. One can interpret this rule as a way of judging
except for the continuous target dimension t. Each exemplar      when the change in the point estimate from v′ (t , n − 1) to
                                                        
is represented by D binary features xi = [ xi 1 , xi 2 , xiD ] , v ′ (t , n) is too small to merit further sampling.
where 1 denotes presence of the feature and 0 its absence.
The participant is presented with a new exemplar t and is        Is PROBEX Frugal?
required to make a judgment or a decision. The similarity        Gigerenzer and Goldstein (1996) proposed the notion of
between t and stored exemplar y is computed by the mul-          ‘fast and frugal’ as a conceptual threshold that a model of
tiplicative similarity rule of the context model,                human decision making must reach in order to be plausible.
               D
                            1 if t⋅ j = y⋅ j                    At first sight, the mathematics of PROBEX that model re-
 S (t , y ) = ∏ d j , d j =                  , (1)              trieval of exemplars seems too complex to be ”fast and fru-
              j =1          s if t⋅ j ≠ y⋅ j                    gal”. But there are two major issues that resolve this di-
where dj is 1 if the values on a feature match and s if they     lemma. First, the complex part of PROBEX models quick
mismatch. Similarity s is a parameter in the interval [0, 1]     and effortless memory processes that operate in parallel.
for the impact of mismatching features. For low values of s,     Secondly, it is not enough to prove that an algorithm is effi-
the similarity is close to one only for an exemplar that is      cient at the moment the decision is made, without concern
almost identical to the new exemplar, but for high values of     for the requirements on pre-computation. PROBEX belongs
s all of the stored exemplars are deemed very similar.           to the class of lazy algorithms (Aha, 1997) in artificial intel-
   We examine PROBEX with s=0.5 for all cue dimensions           ligence, that avoid the processing of data before the task is
which is a compromise between these extremes. The idea is        given. On the other hand, all algorithms discussed in this
that this compromise, referred to as similarity-graded prob-     paper, except PROBEX, rely on pre-computed representa-
ability, is a particularly robust and efficient way to exploit   tions. TTB (Gigerenzer & Goldstein, 1996) for example
states of limited knowledge (Juslin & Persson, 1999).            need to compute a sorted list of cue validities for the specific
   The stored exemplars race to determine the response           task, which in it self is computationally demanding. Argua-
(Logan, 1988; Nosofsky & Palmeri, 1997). The stored ex-          bly, if PROBEX makes good decisions by retrieving few
emplars are retrieved one-by-one from an initial set K to        exemplars and without using pre-computed representation, it
                              
yield a sequence x1 , x2 , x N . The probability that exemplar   is fast and frugal in a more general and important sense.
 y is the sampled exemplar xn at iteration n is:

The Ecological Rationality of Five Algorithms                     negative cue value2. To estimate a population the algorithm
The German City-population Task The task in the initial           starts by checking if a city has the cue value that is first in
study on ecological rationality was the German city-              this rank-order, then the next, and so on until a match is
population task (Gigerenzer & Goldstein, 1996; see Giger-         encountered. Then the mean population for the cities which
enzer, et al., 1999, for applications to other environments).     have this cue is the estimate. Given the skew of the city-
The task is to answer questions such as ”Which city has the       population distribution with mostly small cities, this algo-
larger population: Heidelberg or Erlangen?” The decision          rithm is frugal in the sense of minimizing the number of cues
process is modeled by an algorithm that relies on some strat-     that have to be accessed (i.e., for most cities the algorithm
egy to make intelligent guesses about the populations of          will stop for the first negative cue values in the rank-order).
German cities. The simulation also requires an environment-       In pair-comparisons, QUICKEST decides on the city with
model containing facts about German cities which—once             the larger estimate.
known to the algorithm—can be used to infer city-                    Gigerenzer and Goldstein (1996) tested the algorithms by
populations. The environment is represented by nine binary        feeding them with all the pair-wise comparisons between
cues that characterize each city, for example, whether a city     German cities with more than 100 000 inhabitants. One
is a state capital or not, whether it has a university or not,    weakness of this procedure was that the knowledge of the
where the nine cues vary in predictive validity.                  algorithms was assumed to consist of all the cities. A better
                                                                  test (Gigerenzer, et al., 1999) is to split the set of German
The Algorithms PROBEX was compared to four algorithms             cities into a training set and a test set. The training set is
for guessing which of two objects have the largest value on       used to train the algorithm and the test set is the pool from
the target dimension: (1) A linear multiple regression model      which the test questions are constructed. This cross-
with cues as independent variables and population as de-          validation is a true test of the robustness and detects any
pendent variable. In a pair-wise comparison task, the algo-       over-fitting to the training set. Moreover, it highlights the
rithm decides on the city with the higher estimate. The di-       issue of learning the cue directions.
rection of the cues is the sign of the regression weights.
Linear multiple regression is included because it is routinely    Learning Cue Directions In the simulations, all algorithms
claimed to provide robust and accurate predictions. How-          except A Priori Dawes’ Rule have no a priori knowledge of
ever, it cannot handle situations with few observations un-       the cues, but have to learn them from the known exemplars.
less one amends it with a method such as Ridge Regression         In some tasks it, may be possible to infer cue direction by
to compensate for cue dimensions with no information1.            reasoning, but this topic is not addressed in this paper.
   (2) Dawes’ Rule (Gigerenzer, et al., 1999): A heuristic           To illustrate the importance of knowing the cue directions,
version of the linear model that counts how many of the cues      two versions of Dawes’ Rule were implemented. The first
support each of the two cities and decides on the city im-        version is the A Priori Dawes’ Rule and it assumes that cue
plied by more cues. Cue direction is represented as 1 or -1 if    directions are known a priori. Its purpose is to show the
there is positive or negative correlation in the training data    theoretical upper limit of Dawes’ Rule with a priori knowl-
respectively and zero if it was not computable. As detailed       edge of cue directions. The second version, Dawes’ Rule,
below, two versions of Dawes’ Rule were implemented in            relies on observed training exemplars to estimate cue direc-
order to investigate the importance of a priori knowledge of      tions by calculating whether each cue is positively or nega-
the cue directions. (3) TTB (Gigerenzer & Goldstein, 1996):       tively correlated with the target dimension among the train-
In pair-comparisons, TTB decides on the task implied by the       ing exemplars. The difference between the variants indicates
first most valid cue that differentiates between the pair.        the importance of a priori knowledge of cue directions.
   (4) QUICKEST (Gigerenzer, et al., 1999) is an algorithm           It is important to make special solutions for several of the
appropriate for skewed distributions like the German city-        algorithms below. With Dawes’ Rule, insufficient data can
populations, where most cities have small populations. For        make the correlation between a cue and the target variable
each cue, the mean population for cities with negative cue-       undefined and then this cue is never used in the test phase.
values is computed (negative cue values are those that go         TTB must also be treated with care, because it use a sorted
with small populations, e.g., not being a state capital). Cues    list of cue validities. When there are few exemplars it is
are rank-ordered from the cue with lowest mean given a            often the case that several cue validities get the same nu-
negative cue value to the cue with the highest mean given a       merical value and then these have to be listed in random
                                                                  order within the list. If this is not done the computer imple-
                                                                  mentation can lead to a biased cue order which may either
1                                                                 increase or decrease the accuracy of the algorithm.
  Ridge regression has the drawback of biasing the predictions
  towards the mean, and thus lowers the predictive accuracy when
  the weights are calculated from many observations without
                                                                  2
  problems with correlated variables. We hand-picked an interme-    We did not implement QUICKEST exactly as Gigerenzer et al.
  diate ridge constant, 0.1, that increased accuracy with limited   (1999) did as we did not use approximations to natural numbers
  information (small training set) but did not lower performance    which probably implies that our implementation gives slightly
  with much information (large training set).                       better predictions.

   Study 1: Pair-Comparisons in the German                                                                             set is negative. With less information there is a greater
             City-Population Task                                                                                      chance to get a training set with only positive correlations. If
                                                                                                                       cue validities had been calculated instead of correlations,
  From an evolutionary perspective, it is important that an                                                            Dawes’ Rule would have done better.
algorithm is good also when information is limited, because                                                               In sum: The algorithms that use all information, such as
a decision maker has to survive as a ”beginner” (see Study 3                                                           PROBEX, perform robustly in states of limited knowledge.
below). Performance with small training sets is therefore a                                                            PROBEX, however, also performs best when more informa-
most important aspect of the robustness of an algorithm.                                                               tion is available. The superior performance of A Priori
Also, each algorithm soon reaches an asymptote that de-                                                                Dawes’ Rule shows that a crucial aspect to be acquired by
pends on both the cue structure and the algorithm itself. In                                                           any algorithm is knowledge of the cue directions.
the first simulation we thus compared the algorithms in the
standard binary choice task with a particular eye to their
performance in states of severely limited knowledge.
                                                                                                                                     PROBEX and Dawes Rule
                                                                                                                       In order to understand why PROBEX performs better than
                         0.75                                                                                          TTB with few training exemplars, it is instructive to con-
                                    PROBEX
                                    A PRIORI DAWES                                                                     sider only two optimal training exemplars: Big-city with all
                         0.70       DAWES
                                                                                                                       cues set to 1 and Small-city with all cues set to 0. For sim-
    Proportion Correct
                                    TAKE THE BEST
                                    RIDGE REGRESSION
                         0.65       MUL. REGRESSION                                                                    plicity, all cues are assumed to be positively correlated with
                                    QUICKEST
                                                                                                                       population. For PROBEX the similarity of the probe to Big-
                         0.60
                                                                                                                       city decreases monotonically as a function of the number of
                         0.55                                                                                          cues in the probe that are not 1. The opposite holds for the
                                                                                                                       similarity to Small-city which increases for each cue not set
                         0.50
                                                                                                                       to 1. Because it is the number of cues set to 1 that differenti-
                         0.45
                                                                                                                       ate the probes, PROBEX has the same high accuracy as
                                1               3          5       7       9        20        40        60        80
                                         2             4       6       8       10        30        50        70        Dawes’ Rule.
                                                      Training Set Size                                                   TTB computes the cue direction for all cues but cannot
                                                                                                                       apply this information, because the order of the cues is se-
Figure 1. Accuracy and robustness of the algorithms. For each                                                          lected at random when the cue validity is the same for all
training set size, 1000 sets were randomized.                                                                          cues. The data point with two training exemplars in Figure 1
                                                                                                                       suffers from this problem because the search order will be
Method The dependent variable was proportion of correct                                                                picked at random from those cues that have a well defined
inferences among the pair-comparisons of the test set. For                                                             direction. PROBEX, Ridge Regression and Dawes’ rule, on
each training set-size (2-80), as many as 1000 participants                                                            the other hand, integrate cue direction information from all
were simulated in order to make the errors of the means                                                                cues in every decision in a similar way, which explains why
negligible. For each simulated participant, the German cities                                                          they have identical proportions correct for the case of two
were randomly partitioned into training and test sets. Each                                                            training exemplars in Figure 1.
algorithm was given the training set and the remaining cities
were combined into all possible unique pairs and used as the                                                                Study 2: A Non-Compensatory Data Set
test set. The data for the 83 German cities were collected
                                                                                                                       When is PROBEX outperformed by other algorithms, like
from Gigerenzer and Goldstein (1996).
                                                                                                                       linear multiple regression and TTB? A good guess is in an
                                                                                                                       environment with additive and non-compensatory cues. The
Results and Discussion Aside from A Priori Dawes’ Rule                                                                 cues are non-compensatory if the optimal regression weights
that gets a head-start by virtue of its initial knowledge,                                                             are such that the largest weight is bigger than the sum of the
PROBEX seems to be the winner in Figure 1. At minimum                                                                  smaller weights. The second largest cue should likewise be
knowledge, PROBEX, Ridge Regression, and Dawes’ Rule                                                                   larger than the sum all the remaining cues, and so on (Giger-
utilize the information equally well. TTB seems to have                                                                enzer et al., 1999). An environment with linear, additive
problems sorting the cues in a good order with little infor-                                                           relations favors the regression model and a non-
mation, but does very well with 6 or more training exem-                                                               compensatory cue-structure favors TTB. A simple environ-
plars. QUICKEST trade off accuracy for speed which is                                                                  ment for which this is true can be defined by ordinary binary
seen clearly in the cross validation paradigm. Note that A                                                             numbers. Each binary number can be used as a cue. For
Priori Dawes’ Rule does not define the asymptote that the                                                              example, 5 is written as 00101 in binary numbers, and gives
other algorithms converge on, because it does not depend on                                                            the cues c1= 0, c2= 0, c3=1, c4=0 and c5= 1. The optimal
the training set and does not suffer from over-fitting. This                                                           weights here are {16, 8, 4, 2, 1}, respectively (i.e.,
effect of cross-validation explains why it is constant at the
                                                                                                                       16 ⋅ 0 + 8 ⋅ 0 + 4 ⋅ 1 + 2 ⋅ 0 + 1 ⋅ 1 = 5 ).
high proportion correct of .74. It is surprising that Dawes’
Rule performs worse with more information, but this is be-
cause the correlation between two cues in the full training

  How much better than PROBEX will linear multiple re-                                   sions strategies, Early Learner (EL), Late Slow Learner
gression and TTB perform in this environment specifically                                (LSL) and Late Fast Learner (LSF), as linear functions
construed to fit the latter two algorithms?                                              roughly similar to the functions in Figure 1 and 2. EL is
                                                                                         PROBEX-like in the sense that it has a slight advantage
Method The same procedure was used as in Study 1, except                                 early in the learning process. LSL and LSF are more TTB-
that the data were binary numbers between 0 and 32. Analo-                               like in that they start at a lower level but increases in accu-
gously with the German city-population task, the task was to                             racy, where LSF increases at a higher speed.
guess which of two binary vectors has the highest number.
                                                                                                                               pEL ( correct ) = 0.55 + 0.010 ⋅ i           (5)
                         1.00
                                                                                                                               pLSL (correct ) = 0.50 + 0.015 ⋅ i           (6)
                         0.95
                         0.90                                                                                                  pLFL (correct ) = 0.50 + 0.020 ⋅ i           (7)
    Proportion Correct
                         0.85
                         0.80
                                                                                           Equation 5, 6 and 7 define the probability of a correct de-
                         0.75
                                                                                         cision if the decision maker has i training exemplars as guid-
                         0.70
                         0.65
                                                                  PROBEX
                                                                                         ance. Two simulations were made where EL was pitted once
                                                                  A PRORI DAWES
                         0.60                                     DAWES                  against LSL and once against LFL. Ten generations were
                                                                  TAKE THE BEST
                         0.55                                     RIDGE REGRESSION
                                                                                         simulated, where the relative proportion of surviving deci-
                         0.50                                     MUL. REGRESSION        sion makers decided the relative proportion in the next gen-
                                                                  QUICKEST
                         0.45
                                2   3   4    5   6   7   8 9 10       20            30
                                                                                         eration. Each generation made 11 decisions, where i was
                                            Training Set Size
                                                                                         varied from 0 to 10. For example, a member of the EL-
                                                                                         species had a 0.55 chance to survive the first decision and
                                                                                         0.65 chance to survive the last. LSL and LFL both had to
Figure 2. PROBEX in an artificial environment composed of the                            start off at 0.5, but ended with 0.65 and 0.7, respectively.
binary numbers.
                                                                                           LSL is an example of a decision strategy that starts off
                                                                                         poorly and barely catches up with EL. LFL, on the other
Results and Discussion Ridge regression is clearly superior
                                                                                         hand, is better than EL for the last 5 trials. Note that, if the
in Figure 2, but suffers slightly from biased estimates with 8-
                                                                                         decisions were not fatal, LFL and EL would both make the
15 exemplars in the training set. TTB does not reach the
                                                                                         same overall amount of correct decisions.
asymptote unless it is trained with almost every exemplar,
and is easily beaten by PROBEX when there are few train-
                                                                                                                     1.0
ing exemplars. The two variants of Dawes Rule cannot use
the non-compensatory nature of the cues and are stuck at                                                                                                                    Early Learner
                                                                                          Proportion of population
                                                                                                                     0.8                                                    Late Slow Learne
about the same level of performance as in Study 1.                                                                                                                          Early Learner
  As expected, multiple regression is perfectly suited for this                                                                                                             Late Fast Learner
                                                                                                                     0.6
environment, but both TTB and PROBEX converge on the
same asymptote. More surprisingly, PROBEX is better than
                                                                                                                     0.4
TTB with few exemplars. TTB only evens the score when
more training exemplars are available, despite the fact that
this cue structure is optimal for TTB. Thus, linear multiple                                                         0.2
regression converges more rapidly on the asymptote, but
TTB enjoys no clear advantage over PROBEX.                                                                           0.0
                                                                                                                           0     1    2    3     4    5     6       7   8         9   10        11
                                                                                                                                                 Generation
               Study 3: An Unforgiving Environment
Does it matter if a decision algorithm is a few percentages                              Figure 3: Two simulations of decision making in a unfor-
better for decisions made in states of limited knowledge?                                giving environment. Open symbols present Early Learners
The answer to this question, of course, depends on the con-                              vs. Late Slow Learners, filled symbols present Early Learn-
sequences of these decisions. In this final section, we pro-                             ers vs. Late Fast Learners.
vide an simple demonstration that in an environment where
poor decisions are fatal, and experience is only gained con-                             Results and Discussion Figure 3 presents the relative
ditional on the survival of previous decisions, a small differ-                          population proportions in a competition between EL and
ence in decision quality may add up quickly. Arguably, these                             LSL, on the one hand, and EL and LFL, on the other. LSL
are the living conditions of many animals, including those of                            would vanish very quickly in such a harsh environment. LFL
humans for a large portion of the evolutionary history.                                  are better but the losses in the beginning of every generation
                                                                                         cannot be repaired despite the superior performance at the
Method In order to cover a wider range of possibilities and                              end of each generation (experienced decision makers).
simplify the demonstration, we modeled three ideal deci-

   This example is artificial and simplified, but shows that it   Chater, N., Oaksford, M., Nakisa, R., Redington, M. (1999). Fast,
is important to be a few percent better with little information     frugal and rational: How rational norms explain behavior.
than a few percent better with a lot of information, if learn-      Submitted for publication.
                                                                  Dawes, R. M., & Corrigan, B. (1974). Linear models in decision
ing is potentially dangerous. Indeed, the differences need not
                                                                    making, Psychological Bulletin, 81, 95-106.
be large if they sum up over thousands of generations.            Dougherty, M. R. P., Gettys, C, F., Ogden, E. E. (1999).
                                                                    MINERVA-DM: A memory processes model for judgments of
                    General Discussion                              likelihood. Psychological Review, 106, 180-209.
   We propose that a plausible model of the cognitive proc-       Gigerenzer, G. (1993). The bounded rationality of probabilistic
                                                                    mental models. In K. I. Manktelow, & D. E. Over (Eds.), Ra-
esses that underlie memory-based judgment and decision
                                                                    tionality: Psychological and philosophical perspectives (pp.
making should have at least three properties: First, the            129-161). London: Routledge.
model should be consistent with—and preferably extend             Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning the fast and
on—previous models with independent support in the cog-             frugal way: Models of bounded rationality. Psychological Re-
nitive science literature. PROBEX is a moderately modified          view, 103, 650-669.
version of one of the most successful models from the cate-       Gigerenzer, G., Hoffrage, U., & Kleinbölting, H. (1991). Prob-
gorization literature—the context model (Medin & Schaffer,          abilistic mental models: A Brunswikian theory of confidence.
1978; Nosofsky, 1984). Second, algorithm-details that per-          Psychological Review, 98, 506-528.
tain to implementation in judgment and decision making            Gigerenzer, G., Todd, P., & the ABC Research Group (1999).
                                                                    Simple heuristics that make us smart. New York: Oxford Uni-
needs to be tested. First steps along these lines have been         versity Press.
taken by fitting the predictions by PROBEX to empirical           Juslin, P., & Persson, M. (1999). PROBabilities from EXemplars:
judgment data (Juslin & Persson, 1999).                             On the role of similarity and frequency in probability judgment.
   Third, as implied by the research on ecological rationality      Submitted for publication.
(Gigerenzer et al., 1999), a cognitive algorithm should make      Kruschke, J. K. (1992). ALCOVE: An exemplar-based
sense also from an evolutionary perspective. An algorithm           connectionist model of category learning. Psychological Review,
favored by natural selection should produce accurate judg-          99, 22-44.
ments when applied to the constraints of a real environment,      Logan, D. G. (1988). Towards an instance theory of automatiza-
                                                                    tion. Psychological Review, 95, 492-527.
require a minimum of mental effort, and be robust in states
                                                                  Medin, D. L., & Schaffer, M. M. (1978). Context model of classi-
of limited knowledge. In this paper we have scrutinized the         fication learning. Psychological Review, 85, 207-238.
ability of PROBEX to infer and use cue direc tions, in com-       Nosofsky, R. M. (1984). Choice, similarity, and the context theory
parison with a number of fast-and-frugal algorithms dis-            of classification. Journal of Experimental Psychology: Learn-
cussed by Gigerenzer et al. (1999).                                 ing, Memory, and Cognition, 10, 104-114.
   PROBEX provides a flexible and efficient way to compute        Nosofsky, R. M., Kruschke, J., & McKinley, S. C. (1992). Com-
and use directions of many cues on the spot, that is, without       bining     exemplar-based     category    representations    and
requiring any pre-computed knowledge. Moreover, TTB                 connectionist learning rules. Journal of Experimental Psychol-
enjoys no systematic advantage over PROBEX in an envi-              ogy: Learning, Memory, and Cognition, 18, 211-233.
                                                                  Nosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-based
ronment specifically designed to favor a non-compensatory
                                                                    random walk model of speeded classification. Psychological Re-
strategy. Importantly, in contrast to the other algorithms,         view, 104, 266-300.
PROBEX brings no strong commitment in regard to the
presumed structure of the environment (e.g., linear, compen-
satory), but applies equally well to nonlinear environments
(such as the classic X-OR-problem). Arguably, this is the
kind of flexibility favored by evolution in adaptation to a
complex and uncertain environment.
                     Acknowledgments
   The research reported in this paper was supported by the
Swedish Council for Research in the Humanities and Social
Sciences.
                          References
Aha, D. W. (1997). Lazy Learning. Kluwer, Norwell, MA.
Aha, D. W., & Goldstone, R. L. (1992). Concept learning and
   flexible weighting. Proceedings of the Fourteenth Annual Con-
   ference of the Cognitive Science Society (pp.534-539). Mahwah,
   New Jersey: Lawrence Erlbaum.

