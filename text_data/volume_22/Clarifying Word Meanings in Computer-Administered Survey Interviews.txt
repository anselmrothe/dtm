UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Clarifying Word Meanings in Computer-Administered Survey Interviews
Permalink
https://escholarship.org/uc/item/0846695d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Schober, Michael F.
Conrad, Frederick G.
Bloom, Jonathan E.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Clarifying Word Meanings in Computer-Administered Survey Interviews
                                            Michael F. Schober (schober@newschool.edu)
                      New School for Social Research; Department of Psychology, AL-340; 65 Fifth Avenue
                                                       New York, NY 10003 USA
                                              Frederick G. Conrad (conrad_f@bls.gov)
                                  Room 4915; Bureau of Labor Statistics; 2 Massachusetts Ave. NE
                                                      Washington, DC 20212 USA
                                       Jonathan E. Bloom (jonathan_bloom@dragonsys.com)
                                   Dragon Systems, Inc.; 320 Nevada St.; Newton, MA 02160 USA
                             Abstract                                contained within the words themselves, and that the proc-
                                                                     ess of comprehension involves looking up those meanings
   We investigated the extent to which a collaborative view          in the mental dictionary and combining them appropri-
   of human conversation transfers to interaction with non-          ately; a collaborative view argues that accurate compre-
   human agents. In two experiments we contrasted user-              hension also requires dialog so that people can clarify
   initiated and mixed-initiative clarification in computer-
   administered surveys. In the first study, users who could         what is meant (see Clark, 1996).
   clarify the interpretations of questions by clicking on              In the studies reported here we investigate the extent to
   highlighted text comprehended questions more accurately           which this collaborative view of human conversation
   (in ways that more closely fit the survey designers’ inten-       transfers to interaction with non-human agents, and we
   tions) than users who couldn’t, and thus they provided            examine whether a collaborative view can improve user
   more accurate responses. They were far more likely to             interface design. Examining collaboration in human-
   obtain help when they had been instructed that clarification      computer interaction forces us to specify details of the
   would be essential than when they were merely told it was         collaborative view that can test its limits and refine our
   available. In the second study, users interacting with a          theories of human communication.
   simulated speech interface responded more accurately, and            We contrast two approaches to designing collaborative
   asked more questions, when they received unsolicited
   clarification about question meaning from the system in
                                                                     systems that support the clarification of word meanings.
   response to their linguistic cues of uncertainty. The results     Under one approach, clarification is user-initiated—that
   suggest that clarification in collaborative systems will only     is, if the user explicitly requests clarification, the system
   be successful if users recognize that their own conceptions       provides it. This requires users to recognize that they need
   may differ from the system’s, and if they are willing to          clarification and to be willing to ask for it. Under the
   take extra turns to improve their understanding.                  other approach, clarification is mixed-initiative—that is
                                                                     the system also provides (or offers to provide) clarifica-
                                                                     tion when it diagnoses misunderstanding, based on user
                         Introduction                                behavior. For example, in a desktop or speech interface a
                                                                     system could provide clarification when the user takes too
Saying something doesn’t guarantee it will be understood.            long to act; in a speech interface a system could provide
People engage in dialog to make sure that what the                   clarification when the user’s speech is hesitant or disflu-
speaker intended has been understood—to ground their                 ent (containing ums and uhs, restarts, etc.).
understanding (e.g., Clark & Brennan, 1991; Clark &                     We examine these issues in the context of survey inter-
Schaefer, 1989; Clark & Wilkes-Gibbs, 1986; Schober &                viewing systems, where systems present questions and
Clark, 1989). People ground their understanding to a cri-            users answer them. To our knowledge, current dialog
terion sufficient for their current purposes; in casual con-         systems for surveys (see Couper et al., 1998 on “comput-
versations (e.g., at a cocktail party), people may not need          erized self-administered questionnaires”) do not allow
to understand precise details to satisfy their conversational        either user-initiated or mixed-initiative clarification of
goals, but in other settings (e.g., air traffic control tower        meaning. Rather, they embody strict principles of stan-
conversations, calls to a technical help desk when your              dardization developed for human-human interviews,
computer crashes, or conversations with your ex-spouse               where the interpretation of questions should be left en-
about child visitation) the stakes are higher.                       tirely up to respondents (e.g., Fowler & Mangione, 1990).
   This collaborative view of human conversation differs             The argument for standardization is that if interviewers
from traditional accounts of language use (what Akmajian             help respondents to interpret questions, they might influ-
et al., 1990 called the “message model” of communica-                ence responses, but if interviewers read scripted questions
tion), where listeners interpret utterances directly. The            and provide only “neutral” feedback, responses are less
traditional view is that the meaning of an utterance is

likely to be biased. We have demonstrated that in human-      were about housing, from the Consumer Price Index
human interviews even supposedly nonbiasing feedback          Housing survey (e.g., “How many people live in this
by interviewers can affect responses (Schober & Conrad,       house?”); four questions were about purchases, from the
1997, in press). More importantly, strict standardization     Current Point of Purchase Survey (e.g., “During the past
can actually harm data quality because it prevents respon-    year, have you purchased or had expenses for household
dents from grounding their understanding of the ques-         furniture?”). For each question, the survey designers had
tions. This is a problem because people's interpretations     developed official definitions for the key concepts, which
of seemingly straightforward questions like “How many         clarified whether, for example, a floor lamp should be
bedrooms are there in your house?” can vary enormously;       considered a piece of household furniture, or whether a
without grounding their understanding of questions, re-       student away at college should be considered to be living
spondents may conceive of questions in unintended ways,       at home.
and the resulting data may not fulfill the survey designers’     Users answered these questions on the basis of fictional
purposes (Clark & Schober, 1991). We have shown that          scenarios, so that we could measure response accuracy—
responses in strictly standardized interviews can be less     that is, the fit between users’ answers and the survey de-
accurate than responses in more interactive interviews        signers’ official definitions. For each question there were
where respondents can ground their understanding of           two alternate scenarios, one typical and one atypical.
questions with the interviewers (Conrad & Schober, 2000;      With the typical scenario, the survey question was de-
Schober & Conrad, 1997).                                      signed to be easy for users to interpret—to map onto the
   The task of responding to a computerized survey differs    user’s (fictional) circumstances in a straightforward way.
from many human-computer interaction situations. First,       For example, for the question “Has Kelley purchased or
in survey systems users provide information to the system     had expenses for household furniture?”, the typical sce-
rather than retrieving information from the system, as        nario was a receipt for an end table, which is clearly a
with a database query system or a web search interface.       piece of furniture. With the atypical scenario, it was less
Second, survey system users’ need for precise under-          clear how the survey question should be answered. For
standing may be lower than when they interact with other      example, for the household furniture question the atypical
systems. Users may care less about precisely understand-      scenario was a receipt for a floor lamp, which is harder to
ing the words in survey questions when providing opin-        classify without knowing the official definition of
ions to market researchers (misunderstanding has few          “household furniture.”
consequences for the user) than understanding the words          For each user, half the scenarios described typical
in an on-line job application or an on-line health claims     situations and half atypical situations.
form (where misunderstandings can be costly).
                                                                              Study 1: Desktop interface
                 Experimental Methods
                                                              In this study, we varied the way the survey system pro-
In our studies we assess whether systems that enable users    vided clarification. When clarification was user-initiated,
to clarify the survey concepts do actually lead to im-        users could request the official definition for a survey
proved comprehension of questions (and thus improved          concept by clicking the mouse on highlighted text in the
response accuracy), as a collaborative theory would pre-      question. When clarification was mixed-initiative, the
dict. We examine the effects of clarification on task dura-   system would also offer a definition when users were
tion—clarification probably takes more time, and this         “slow” to respond. This was defined as taking longer than
may offset any benefits. We also examine the effects of       the median response time for atypical scenarios when no
clarification on user satisfaction; even if clarification im- clarification was available. This offer was presented as a
proves comprehension, it could be annoying.                   Windows dialog box; users could reject the offer by
   Our first study (Conrad & Schober, 1999) uses a desk-      clicking “no” if they didn’t think clarification was needed.
top interface, in which the computer displays questions on       We also varied instructions to the users about how pre-
a screen. The user enters responses and asks for clarifica-   cisely they would need to understand the system’s ques-
tion with the keyboard and mouse. Our second study            tions—that is, we varied the grounding criterion. Some
(Bloom, 1999) uses an interface, in which questions are       users were told that clarification was essential; they were
presented in a synthesized voice through a headset. The       encouraged to obtain definitions from the computer be-
user answers questions and asks for clarification by          cause their everyday definitions might differ from the
speaking into the headset microphone.                         survey’s. Other users were told merely that clarification
   In both studies, all users were asked the same survey      was available, that definitions would be available if users
questions, which had been used in earlier studies of hu-      wanted them. The five experimental conditions are dis-
man-human survey interviews (e.g. Schober & Conrad,           played in Table 1.
1997). We adapted 12 questions from three ongoing U.S.           54 users, recruited from an advertisement in the Wash-
government surveys. Four questions were about employ-         ington Post, were paid to participate. Most (44) reported
ment, from the Current Population Survey (e.g., “Last         using a computer every day.
week, did you do any work for pay?”); four questions

                                                             and not annoying (1.0) by “clarification essential” users,
         Table 1: Experimental conditions, Study 1.          and less useful (3.9) and more annoying (4.25) by “clari-
                                                             fication available” users. Presumably users who had been
     Type of clarification          User instructed that…    told that clarification was available found it jarring for the
                                                             system to offer unsolicited help for seemingly straight-
   1 no clarification
                                                             forward questions.
   2 at user’s request              Clarification essential     Overall, these results suggest that the success of human-
   3 at user’s request              Clarification available  machine collaboration may depend both on users’
                                                             grounding criteria—how important they believe it is to
   4 when user is slow or at        Clarification essential  understand accurately—and also on whether users recog-
     user’s request                                          nize that system concepts may differ from theirs.
   5 when user is slow or at        Clarification available
     user’s request
                                                                             Study 2: Speech interface
Results
Users’ responses were almost perfectly accurate (their       This study used a Wizard-of-Oz technique to simulate a
responses fit the official definitions) when they answered   speech interface. Users believed they were interacting
about typical scenarios. For atypical scenarios, users were  with a computer, when actually a hidden experimenter
more accurate when they could get clarification than         presented the questions and scripted clarification. To en-
when they couldn’t (see Figure 1). Response accuracy         hance believability, we used an artificial-sounding com-
mainly depended on the instructions to the user about the    puter voice (Apple’s “Agnes” voice).
grounding criterion. When users had been told that defi-        This study used exactly the same questions and scenar-
nitions were merely available, their accuracy was as poor    ios as Study 1. Users participated in one of four experi-
as when they couldn’t get clarification. When they had       mental conditions. In the first condition, the system never
been told that definitions were essential, response accu-    provided clarification. In the second condition, clarifica-
racy was much better, whether users had to request clari-    tion was user-initiated—the system would provide clarifi-
fication, F(1,49) = 9.82, p < .01, or whether the system     cation if users asked for it explicitly. In the third condi-
also offered it, F(1,49) = 14.38, p < .01.                   tion, the initiative was mixed—the system would “auto-
   As Figure 2 shows, response accuracy was strongly re-     matically” provide full definitions when users displayed
lated to how often users received clarification. When us-    specific uncertainty markers that had been shown to be
ers had been told that definitions were essential, they re-  more prevalent in atypical situations in human-human
quested clarification most of the time; in fact, they fre-   interviews collected with these materials (Bloom & Scho-
quently requested it for typical scenarios, when presuma-    ber, 1999). These included ums, uhs, pauses, repairs, and
bly it wasn’t necessary. They also requested clarification   talk other than an answer. In the fourth condition, the
quickly, which meant that when the system could also         system always provided clarification; no matter what the
provide clarification (conditions 4 and 5) it rarely did. In user did, the system would present the full official defini-
contrast, users who had been told that clarification was     tion for every question.
merely available rarely asked for it, and they responded to     40 users recruited from an advertisement in the Village
the questions so quickly that system-initiated clarification Voice were paid to participate.
was rarely triggered. Apparently, it didn’t occur to these
users that their interpretation of ordinary terms like “bed-
room” and “job” might differ from the system’s, and so       Results
they answered confidently, quickly, and inaccurately.        As in Study 1, users’ responses were almost perfectly ac-
   As Figure 3 shows, clarification took time. Response      curate when they answered about typical scenarios. For
times were much longer in cases where users received         atypical scenarios, users were substantially more accurate
clarification. As we anticipated, improved accuracy from     when they were always given clarification (80%) than
clarification can be costly.                                 when they were never given clarification (33%), F(1,36) =
   Users’ ratings of their satisfaction with the system sug- 10, p < .005. When users initiated clarification, their re-
gested two things. First, users who could not get clarifica- sponse accuracy was no better (29%) than when they were
tion reported that they would have asked for clarification   never given clarification, because they almost never asked
if they could. This suggests that interacting with dialog    for it. As in Study 1, it seems likely that it didn’t occur to
survey systems that don’t allow clarification may be rela-   users that clarification was necessary. Response accuracy
tively unsatisfying. Second, users’ grounding criteria af-   was better when the initiative for clarification was mixed
fected their perceptions of the system. System-initiated     (59%), F(1,36) = 10.11, p < .005, although it was not as
clarification was rated on a 7 point scale as useful (6.0)   good as when clarification was given always.

                                                          100
                                                          80
                                        Percent correct
                                                          60                                                     Clarification
                                                                                                                 Essential
                                                          40
                                                                                                                 Clarification
                                                          20                                                     Available
                                                           0
                                                                     No           User-         Mixed-
                                                                clarification    initiated     initiative
                                                                Figure 1: Response accuracy for atypical scenarios, Study 1
                                  100
           Percent of questions
                                   80
                                   60                                                                                      Typical scenarios
                                   40                                                                                      Atypical scenarios
                                   20
                                    0
                                        Essential                      Available       Essential       Available
                                                           User-initiated               Mixed-initiative
                                                                 Figure 2: How often users received clarification, Study 1
                                                          60
                                                          50
                                                          40
                                        Seconds
                                                                                                                        Clarification Essential
                                                          30
                                                                                                                        Clarification Available
                                                          20
                                                          10
                                                           0
                                                                      No           User-             Mixed-
                                                                 clarification     initiated        initiative
                                                                      Figure 3: Response time per question, Study 1
  System-initiated clarification increased the amount of                                       tion might be useful. These users also spoke less fluently,
user-initiated clarification: users were more likely to ask                                    producing more ums and uhs. We speculate that this was
questions in the mixed-initiated condition, presumably                                         because these users at some level recognized that the sys-
because they were more likely to recognize that clarifica-                                     tem was sensitive to their cues of uncertainty.

   Overall, the users in this study requested clarification far implementations, which are based on the most generic of
less often than the users in Study 1. This might result from    user models (see Kay, 1995). A stronger test of collabora-
any or all of the differences between our desktop and           tive approaches requires more customized interfaces, in
speech interfaces. In the speech interface, clarification was   which, for example, the system would reason about which
harder to request; requests had to be formulated into ex-       parts of definitions would be appropriate to present at any
plicit questions rather than being accomplished by simple       given moment, what particular users are likely to misun-
mouse clicks. Also, in the speech interface the definition      derstand, etc. (see Moore, 1995).
unfolded over time (sometimes a substantial amount of              Our findings demonstrate that computer implementations
time, up to 108 seconds), rather than appearing all at once,    of surveys seem to run into exactly the same problems as
and in our application it was impossible to shut off; in the    human-human survey and instructional situations, where
desktop interface, the definition appeared all at once and      people don’t always recognize they need help or aren’t
could be closed with a simple mouse click. Also, unlike in      willing or able to ask for help (e.g., Graesser & McMahen,
the desktop study, users couldn’t reject system-initiated       1993; Schober & Conrad, 1997).
offers of clarification; here the system immediately pro-          But our findings also show that in some situations (our
vided clarification when triggered, without giving the op-      desktop interface, when users were told that clarification
tion of rejecting the help.                                     was essential), users are indeed willing to ask for clarifica-
   As in Study 1, clarification took time. The more clarifi-    tion more often than they are with human interviewers
cation a user received, the more time the interviews took.      (Schober & Conrad, 1997). This is consistent with findings
Sessions where clarification was always provided took           in other domains that interaction with a computer can lead
more than twice as long as sessions with no clarification or    to better task outcomes than interaction with a person. For
when it was (rarely) user-initiated (12.8 versus 5.2 and 4.9    example, people may be more willing to accept correction
seconds per question, respectively); mixed-initiative clari-    from an intelligent computer tutor than from a human tutor
fication took an intermediate amount of time (9.6 seconds       (Schofield, 1995), and people are more willing to admit to
per question).                                                  undesirable behaviors when asked about them on self-
   Also as in Study 1, users rated the system more posi-        administered computer surveys than in human-
tively when it was responsive (user- or mixed-initiative        administered surveys (Tourangeau & Smith, 1996).
conditions). When the system was not responsive (no clari-         We propose that some of these improvements from in-
fication or clarification always), users wanted more control    teracting with computers don’t arise simply from the fact
and felt that interacting with the system was unnatural.        that the computer isn’t a person. They arise in part from
Users didn’t report finding system-initiated clarification      the fact that the costs and constraints of grounding vary in
particularly more annoying than user-initiated clarifica-       different media, as Clark and Brennan (1991) argued. Most
tion—which they almost never used.                              tutoring and survey systems to date have been direct ma-
   Overall, these results suggests that enhancing the col-      nipulation or simple (textual) character entry systems like
laborative repertoire of a speech system can improve com-       our desktop interface; in such interfaces the user’s costs of
prehension accuracy without harming user satisfaction, as       requesting information from the system can be low. The
long as the system provides help only when it is necessary.     human interactions to which such systems are often com-
But these improvements come at the cost of increased task       pared are speech interactions, where people have to for-
duration, which could make such systems impractical in          mulate clarification requests explicitly and clarification
real-world survey situations.                                   takes significant amounts of time. Any differences in task
                                                                performance may just as likely result from the differences
                                                                between direct manipulation and speech as from the differ-
                         Conclusions                            ences between computers and humans.
                                                                   We believe our findings also require us to refine a theory
Our findings demonstrate that a collaborative view can          of human-human collaboration by explicitly introducing
indeed transfer to interaction with non-human agents. In-       the notion of initiative. Our findings that comprehension
creased system clarification abilities can improve users’       success can vary depending on whether the user or system
comprehension (and thus their response accuracy), while         takes the initiative should be extended to the human realm;
increasing (or not reducing) user satisfaction. But this        a collaborative theory should include who takes the respon-
comes at the cost of increased task duration, which could       sibility for clarifying meaning. In many cases speakers are
lower survey completion rates in the real world.                responsible for what they mean, and listeners assume that
   Our findings also demonstrate that extended clarification    what speakers say is readily interpretable to them in the
sequences are likely to be rare or unnecessary when users’      current context (the “interpretability presumption,” in
conceptions are likely to be the same as the system’s, as in    Clark and Schober’s [1991] terms). But in situations where
our typical scenarios. The need for building survey systems     the speaker is less competent than the addressee, the ad-
with enhanced collaborative abilities may depend on the         dressee may take responsibility for the meaning, and may
likelihood of potential misunderstandings; if this likelihood   initiate clarification (Schober, 1998). Who should be re-
is high or unknown, enhanced collaborative abilities may        sponsible under what circumstances, and what determines
be worth implementing.                                          how speakers decide whose effort should be minimized,
   The benefits we have shown for collaboratively en-           are important questions for a theory of collaboration.
hanced survey systems come even with our rudimentary

   Altogether, our results suggest that user-initiated clarifi- Clark, H.H., & Wilkes-Gibbs, D. (1986). Referring as a
cation will work only if users recognize that clarification       collaborative process. Cognition, 22, 1-39.
will help, recognize that the system’s concepts may differ
from theirs, are motivated to understand precisely, and are     Conrad, F.G., & Schober, M.F. (1999). A conversational
willing to take the extra turns to ground understanding.          approach to text-based computer-administered question-
Explicit instructions to users can help make this happen—         naires. Proc. of the 3rd ASC Int’l Conference (pp. 91-
help set a high grounding criterion—but it’s unclear              102). Chesham, UK: Association for Survey Computing.
whether such instruction is feasible in real-world situations.
Our results suggest that system-initiated clarification will    Conrad, F.G., & Schober, M.F. (2000). Clarifying question
work only if users give reliable evidence of                      meaning in a household telephone survey. Public Opin-
misunderstanding and if they are willing to accept offers of      ion Quarterly, 64, 1-28.
clarification. It won’t work if users are confident in their
misinterpretations.                                             Couper, M.P., et al. (Eds.) (1998). Computer assisted sur-
   In general, the opportunity for clarification dialog won’t     vey information collection. New York: Wiley.
help if users don’t recognize it’s needed.
                                                                Fowler, F.J., & Mangione, T.W. (1990). Standardized sur-
                                                                  vey interviewing: Minimizing interviewer-related error.
                    Acknowledgments                               Newbury Park, CA: SAGE Publications, Inc.
   This material is based upon work supported by the Na-        Graesser, A.C., & McMahen, C.L. (1993). Anomalous in-
tional Science Foundation under Grant No SBR9730140               formation triggers questions when adults solve quantita-
and by the Bureau of Labor Statistics. The opinions ex-           tive problems and comprehend stories. Journal of Edu-
pressed here are those of the authors and not of the Bureau       cational Psychology, 85, 136-151.
of Labor Statistics. We thank Susan Brennan and Albert
Corbett for help with an earlier version of this paper.         Kay, J. (1995). Vive la difference! Individualized interac-
                                                                  tion with users. In C.S. Mellish, (Ed.), Proc. of the 14th
                                                                  International Joint Conference on Artificial Intelligence,
                         References                               978-984. San Mateo, CA: Morgan Kaufmann.
Akmajian, A., Demers, R.A., Farmer, A.K., & Harnish,
   R.M. (1990). Linguistics: An introduction to language        Moore, J.D. (1995). Participating in explanatory dia-
   and communication, 3rd ed. Cambridge, MA: MIT.                 logues: Interpreting and responding to questions in con-
                                                                  text. Cambridge, MA: MIT.
Bloom, J.E. (1999). Linguistic markers of respondent un-
   certainty during computer-administered survey inter-         Schober, M.F. (1998). Different kinds of conversational
   views. Doctoral dissertation, Department of Psychology,        perspective-taking. In S.R Fussell & R.J. Kreuz (Eds.),
   New School for Social Research, New York.                      Social and cognitive psychological approaches to inter-
                                                                  personal communication. Mahwah, NJ: Erlbaum.
Bloom, J.E., & Schober, M.F. (1999). Respondent cues
   that survey questions are in danger of being misunder-       Schober, M.F., & Clark, H.H. (1989). Understanding by
   stood. In Proc. of the ASA, Section on Survey Research         addressees and overhearers. Cognitive Psychology, 21,
   Methods (pp. 992-997). Alexandria, VA: American Sta-           211-232.
   tistical Association.
                                                                Schober, M.F., & Conrad, F.G. (1997). Does conversa-
Clark, H.H. (1996). Using language. Cambridge, UK:                tional interviewing reduce survey measurement error?
   Cambridge University Press.                                    Public Opinion Quarterly, 61, 576-602.
Clark, H.H., & Brennan, S.E. (1991). Grounding in com-          Schober, M. F., and. Conrad, F. G. (In press). A collabora-
   munication. In L.B. Resnick, J.M. Levine, & S.D Tea-           tive view of standardized survey interviews. In D.
   sley, (Eds.), Perspectives on socially shared cognition.       Maynard, H. Houtkoop, N. C. Schaeffer, & J van der
   Washington, DC: APA.                                           Zouwen (Eds.), Standardization and tacit knowledge:
                                                                  Interaction and practice in the survey interview, New
Clark, H.H., & Schaefer, E.F. (1989). Contributing to dis-        York: Wiley.
   course. Cognitive Science, 13, 259-294.
                                                                Schofield, J.W. (1995). Computer and Classroom Culture.
Clark, H.H., & Schober, M.F. (1991). Asking questions             Cambridge, UK: Cambridge University Press.
   and influencing answers. In J.M. Tanur (Ed.), Questions
   about questions: Inquiries into the cognitive bases of       Tourangeau, R., & Smith, T. (1996). Asking sensitive
   surveys. New York: Russell Sage Foundation.                    questions: The impact of data collection mode, question
                                                                  format, and question context. Public Opinion Quarterly,
                                                                  60, 275-304.

