UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Infinite RAAM: A Principled Connectionist Basis for Grammatical Competence
Permalink
https://escholarship.org/uc/item/49x7t2s4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Levy, Simon
Melnik, Ofer
Pollack, Jordan
Publication Date
2000-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

  Infinite RAAM: A Principled Connectionist Basis for Grammatical Competence
                                               Simon Levy, Ofer Melnik and Jordan Pollack
                                          levy, melnik, pollack@cs.brandeis.edu
                                            Dynamical and Evolutionary Machine Organization
                                                    Volen Center for Complex Systems,
                                              Brandeis University, Waltham, MA 02454, USA
                                                              February 6, 2000
                               Abstract                                 extraordinary burden on the human parsing mechanism when
                                                                        they did occur (Bach, Brown, and Marslen-Wilson 1986).
   This paper presents Infinite RAAM (IRAAM), a new fusion of
   recurrent neural networks with fractal geometry, allowing us to
   understand the behavior of these networks as dynamical sys-                  Connectionism and Natural Language
   tems. Our recent work with IRAAMs has shown that they are            While debates about the complexity of NL were raging,
   capable of generating the context-free (non-regular) language
    for arbitrary values of  . This paper expands upon that      connectionism was beginning to awaken from a fifteen-year
   work, showing that IRAAMs are capable of generating syntac-          sleep. In connectionist models many researchers found a
   tically ambiguous languages but seem less capable of gener-          way of embodying flexibility, graceful degradation, and other
   ating certain context-free constructions that are absent or dis-     non-rigid properties that seem to characterize real cognitive
   favored in natural languages. Together, these demonstrations
   support our belief that IRAAMs can provide an explanatorily          systems like NL. This research culminated the publication
   adequate connectionist model of grammatical competence in            of a highly controversial paper by Rumelhart and McClel-
   natural language.                                                    land (1986) which provided a connectionist account of part
                                                                        of the grammar of English using a feed-forward neural net-
                                                                        work. The paper was soon criticized by more traditional cog-
                  Natural Language Issues                               nitive scientists (Fodor and Pylyshyn 1988; Pinker and Prince
In an early and extremely influential paper, Noam Chomsky               1988), who cited the non-generative nature of such connec-
(1956) showed that natural languages (NL‚Äôs) cannot be mod-              tionist models as a fundamental shortcoming of the entire
eled by a finite-state automaton, because of the existence of           field.
center-embedded constructions. A second and equally im-                    Partly in response to these criticisms, many connection-
portant observation from this work was that a minimally ade-            ists have spent the past decade investigating network models
quate NL grammar must be ambiguous, assigning more than                 which support generativity through recurrent (feedback) con-
one structure (interpretation) to some sentences, for example,          nections (Lawrence, Giles, and Fong 1998; Rodriguez, Wiles,
They are flying planes.                                                 and Elman 1999; Williams and Zipser 1989). The research
   The first observation led to the development of Chomsky‚Äôs            we present here is an attempt to contribute to this effort while
formal hierarchy of languages, based on the computational               focusing as strongly as possible on the natural language is-
resources of the machines needed to recognize them. In this             sues described above. Such an attempt faces a number of
hierarchy, Chomsky‚Äôs observation about center-embedding is              challenges.
expressed by saying that NL‚Äôs are non-regular; i.e., they can-             First, despite analysis of how a network‚Äôs dynamics con-
not
  be generated by   a grammar having only rules of the form          tribute to its generativity, it is often uncertain whether the
           , where and are non-terminal symbols and is                  dynamics can support generation of well-formed strings be-
a terminal symbol.                                                      yond a certain length. That is, it is unknown whether the net-
   Whether NL‚Äôs are merely non-regular, belonging in the                work has a true ‚Äúcompetence‚Äù for the language of which it has
next, context-free (CF) level of the Chomsky hierarchy, or are          learned a few exemplars, or is merely capable of generating
more powerful, belonging further up in the hierarchy, became            a finite, and hence regular, subset of the language. 1 Second,
the subject of heated debate (Higginbotham 1984; Postal and             it is often easier to model weak, rather than strong genera-
Langendoen 1984; Shieber 1985). Non-CF phenomena such                   tive capacity, by building networks that generate or recognize
as reduplication/copying (Culy 1985) and crossed serial de-             strings having certain properties, without assigning any syn-
pendencies (Bresnan, Kaplan, Peters, and Zaenen 1982) sug-              tactic structure to the strings. Third, this lack of syntactic
gested that a more powerful approach, using syntactic trans-            structure inhibits the formulation of an account of syntactic
formations (Chomsky 1957) was called for, but some re-                  ambiguity in such networks, making them less plausible as
searchers criticized transformations as having arbitrary power          models of NL.
and thus failing to constrain the types of languages that could
                                                                            1
be expressed (Gazdar 1982). Further criticism of the entire                   To be fair, not all connectionists, or cognitive scientists, take
formal approach came from observing that even CF gram-                  seriously the notion that human language has infinite generative ca-
                                                                        pacity. Though we obviously do not have the resources to argue
mars (CFGs) had the power to generate structures, such as               the issue here, we are certain that a model with a provably infinite
a sequence followed by its mirror image, that did not seem              competence would be more persuasive to the cognitive science com-
to occur in NL (Manaster-Ramer 1986), or which placed an                munity as a whole than would a model without one.

   In sum, we are concerned with formulating a recurrent net-    rors instead of a greater set of reliable representations. 3) The
work model that rigorously addresses the set of criteria that    ‚ÄúTerminating Non-Terminal‚Äù problem arises when there is a
emerged from the long debate over the complexity of NL.          ‚Äúfusion‚Äù between a non-terminal and a terminal, such that the
As an candidate,the remainder of this paper presents a new       decoding of an encoded tree terminates abruptly.
formulation of RAAM (Pollack 1990), a recurrent network             In the following section of this paper we present a new for-
model that addresses the NL issues in a principled way.          mulation of RAAM networks based on an analysis of the it-
                                                                 erated dynamics of decoding, that resolves all these problems
                    Traditional RAAM                             completely. This formulation leads to a new ‚Äúnatural terminal
                                                                 test‚Äù, a natural labeling of terminals, and an inherently higher
Recursive Auto-Associative Memory or RAAM (Pollack               storage capacity.
1990) is a method for storing tree structures in fixed-width
vectors by repeated compression. Its architecture consists of                      New RAAM Formulation
two separate networks ‚Äì an encoder network, which can con-
struct a fixed-dimensional code by compressively combining                   XL               YL           XR                   YR
the nodes of a symbolic tree from the bottom up, and a de-
coder network, which decompresses a fixed-width code into
its two or more components. The decoder is applied recur-
sively until it terminates in symbols, reconstructing the tree.
These two networks are simultaneously trained as an autoas-
sociator with time-varying inputs. If the training is success-
ful, the result of bottom up encoding will coincide with top
down decoding.
   Following the publication of (Pollack 1990), RAAM                                          X             Y                     Bias
gained widespread popularity as a model of NL syntax. Some
                                                                                                                        
researchers (Blank, Meeden, and Marshall 1991) found it an
attractive way of ‚Äúclosing the gap‚Äù between the symbolic                                          !#"%$&          !'( $! *)
and sub-symbolic paradigms in cognitive science. Others                           +                                   
(Van Gelder 1990) saw in RAAM a direct and simple refu-                                             ', "%$&         '-' ( $&  ' )
tation of the traditional cognitive scientists‚Äô backlash against               .                                           
connectionism, and went as far as to show how traditional                                          /  "0$!/          ' ( $&/  )
syntactic operations like transformations could be performed                     + .                                     
directly on RAAM representations (Chalmers 1990). As the
power of the RAAM model became apparent, variants be-                                                 / ',*"%$& /        ','( $& / '1)
gan to emerge. These included the Sequential RAAMs of            Figure 1: An example RAAM decoder that is a 4 neuron net-
(Kwasny and Kalman 1995), which showed how a RAAM                work, parameterized by
could behave like a linked list, and the Labeling RAAMs                                        43512 +76 weights. Each application of the
                                                                 decoder converts an 2                    coordinate into two new coordi-
of (Sperduti 1993), which encoded labeled graphs containing
cycles.                                                          nates.
   In short, RAAM seemed to hold a great deal of promise
as a general connectionist solution to encoding not just NL         Consider the RAAM decoder shown in figure                           435+81.6 It consists
syntax, but all sorts of structured representations.             of four neurons that each receive the same 2                                     input. The
   Still, RAAM was plagued by an apparently diverse set of       output portion of the network is divided into a right and a left
problems, most notably a failure to scale up to realistically    pair of neurons. In the operation of the decoder the output
large structures. We believe that these problems can be traced   from each pair of neurons is recursively reapplied to the net-
to the original formulation of the RAAM decoder, which           work. Using the RAAM interpretation, each such recursion
works in conjunction with a logical ‚Äúterminal test‚Äù, answer-     implies a branching of a node of the binary tree represented
ing whether or not a given representation requires further de-   by the decoder and initial starting point. However, this same
coding. The default terminal test merely asks if all elements    network recurrence can also be evaluated in the context of dy-
in a given code are boolean, e.g. above 0.8 or below 0.2.        namical systems. This network is a form of iterated function
This analog-to-binary conversion was a standard interface in     system or IFS (Barnsley 1993), consisting of two pseudo-
back-propagation research of the late 1980‚Äôs to calculate bi-    contractive transforms which are iteratively applied to points
nary functions from real-valued neurons. However, although       in a two-dimensional space.
it enabled the initial discovery of RAAM training, it led to        In the past we have examined the applicability of the IFS
several basic logical problems which prevented the scaling       analogy to other interpretations of neural dynamics (Blair and
up of RAAM: 1) The ‚ÄúInfinite Loop‚Äù problem is that there         Pollack 1997; Kolen 1994; Melnik and Pollack 1998; Stucki
are representations which ‚Äúbreak‚Äù the decoder by never ter-      and Pollack 1992). But in the context of RAAMs the main
minating. In other words, some trees appear ‚Äúinfinitely large‚Äù   interesting property of contractive IFSes lies in the trajecto-
simply because their components never pass the terminal test.    ries of points in the space. For contractive IFSes the space
This behavior breaks computer program implementations or         is divided into two sets of points. The first set consists of
requires depth checking. 2) The ‚ÄúPrecision vs. Capacity‚Äù         points located on the underlying attractor (fractal attractor) of
problem is that tighter tolerances lead to more decoding er-     the IFS. The second set is the complement of the first, points

that are not on the attractor. The trajectories of points in this        set of points any further at that resolution. Hence, we can
second set are characterized by a gravitation towards the at-            visualize the behavior of the decoder in the unit square by
tractor. Finite, multiple iterations of the transforms have the          examining the set of points obtained through iterated applica-
effect of bringing the points in this second set arbitrarily close       tions of the two transforms.
to the attractor.                                                           In figure 2, we have applied the transforms once to all
    As noted before, the Infinite Loop and Terminating Nonter-           points in the unit square, obtaining two large, overlapping re-
minal problems arise from an insufficient terminal test. Since           gions, corresponding to the left and right transforms of all the
some trajectories never leave the attractor and all others even-         original points. Note that some points are part of both the left
tually hit the attractor. The only terminal test that guarantees         and right regions.
the termination of all trajectories of the RAAM (IFS) is a test
that includes all the points of the attractor itself.                                          1
    By taking the terminal test of the decoder network to be
‚Äúon the attractor‚Äù, not only are problems of infinite loops
and early termination corrected, but it is now possible to                              <
have extremely large sets of trees represented in small fixed-                            ;<
dimensional neural codes. The attractor, being a fractal, can                                ;
be generated at arbitrary resolution. In this interpretation,                                  Y
each possible tree, instead of being described by a single
point, is now an equivalence class of initial points sharing
the same tree-shaped trajectories to the fractal attractor. For
this formulation, the set of trees generated and represented
by a specific RAAM is a function of the weights, but is also
governed by how the initial condition space is sampled, and                                    0         X                 1
by the resolution of the attractor construction. Note that
the lower-resolution attractors contain all the points of their          Figure 2: The unit square after one application of the trans-
higher-dimensional counterparts (they cover them); therefore,            forms. The attractor is shown in gray: dark gray = points
as a coarser terminal set, they terminate trajectories earlier           reachable from attractor on left transform, light gray = points
and so act to ‚Äúprefix‚Äù the trees of the higher-dimensional at-           reachable on right. The small white wedge where the gray ar-
tractors.
                                                                         eas overlap contains ‚Äúambiguous‚Äù attractor points reachable
    Two last pieces complete the new formulation. First, the
                                                                         on both transforms.
encoder network, rather than being trained, is constructed di-
rectly as the mathematical inverse of the decoder. The termi-
nal set of each leaf of a tree is run through the inverse left
or right transforms, and then the resultant sets are intersected
and any terminals subtracted. This process is continued from
the bottom up until there is an empty set, or we find the set of
initial conditions which encode the desired tree.
    Second, using the attractor as a terminal test also allows a
natural formulation of assigning labels to terminals. Barns-
ley (1993) noted that each point on the attractor is associated
with an address which is simply the sequence of indices of the
transforms used to arrive on that point from other points on
the attractor. The address is essentially an infinite sequence
of digits. Therefore to achieve a labeling for a specific alpha-         Figure 3: The unit square after two and five applications of
bet we need only consider a sufficient number of significant             the transforms.
digits from this address.
                                                                            Figure 3 shows the unit square after another iteration of
           Example of New RAAM Formulation                               the transforms, and after five such iterations. Figure 4 shows
In this section, we describe how we obtain the attractor and             the final ‚ÄúGalaxy‚Äù attractor obtained when further iterations
the trees for a RAAM decoder of the sort shown in figure 1.              fail to produce any more contraction. Like any fractal, this
The decoder weights in the present example were obtained by              attractor exhibits self-similarity, with the two longest arms of
a hill-climbing search for an aesthetically appealing attractor,         the galaxy ending in shapes like that of the whole attractor.
but the demonstration is valid for any set of decoder weights.              Figure 4 also shows how we derive the tree (1 (1 2)) from
    Recall that we are       treating
                        435+7  6      the decoder as an IFS that maps   a point not on the attractor. Starting at a point not on the
each     +9&6 point 2 . 3:+ . 6 in the range [0,1] to two other points
  #35input                                                            attractor (the small circle at the top of the figure), the left
2              and 2               in the same range. To generate the    transform (dashed line) takes us immediately to the attractor;
attractor of the IFS, we first apply the two mappings (trans-            specifically, to an attractor region labeled 1, indicating that
forms) to the entire unit square at some fixed resolution. We            this region is reachable from the other attractor points on the
then re-apply the transforms to the resulting set of points. We          left (first) transform only. Hence our tree so far is (1. . . ). The
repeat this operation until the transforms do not change the             right transform of the point at the top takes us to another point

not on the attractor, indicated by the circle in the lower left
part of the figure. Like the first point, this point goes to the                                                                                     2)
                                                                                                                                                  2)
attractor region labeled 1 on its left transform; however, it also                                                                            ((1               2)
                                                                                                                                                            2))
                                                                                                                                                         2)
goes to the attractor on its right transform; specifically, to the                                                         2                         ((1
                                                                                                                       ((1                       ((1
region labeled 2, which indicates that this region is reachable                                                    ((1
from the other attractor points on the right (second) transform                                                ((1
                                                                                                           ((1
only. So this second point decodes the tree (1 2), and its par-                                        ((1
                                                                                                    (1
ent tree is (1 (1 2)), completing the derivation.                                            (1                          )2
                                                                                                                              2
                                                                                              ((1                          ))
                                                                                                                               )) 2
                                                                                                  ((1
                                                                                                                                  )) 2
                                                                                                       2))
                                                                                                    2)                                )) 2
                               (1 (1 2))                                            (1
                                                                                                         2))                             ))
                                                                                     ((1
                                                                                            2))
                                                                                         2)
                                                                     Figure 5: Tree equivalence classes for the =?> > system. At-
                           2                                         tractor points cluster at extreme left (colored black, labeled 1
                                                                     or = ) and right (colored white, labeled 2 or ).
                                           1
               (1 2)                                                    Briefly, the dynamics of the network are such that for any
                                                                     point in the unit square, one of the two transforms of the
                                                                     point is guaranteed to be on the attractor. This behavior cor-
                                                                     responds to the terminal component of a recursive grammar
Figure 4: The final attractor, showing derivation of the tree        in Chomsky Normal Form for the language. In addition, the
(1 (1 2)) and its daughter tree (1 2). The left transform is         left transform
                                                                                  LK of any point ends up on the left side of the unit
shown as a dashed line, and the right transform as a straight        square ( J       ) and the right transform ends up on the right
line.                                                                side (J      ). Hence, successive application of left/right/left...
                                                                     transforms leads to a zigzag dynamics that balances = ‚Äôs on the
   By repeating this process for every point not on the attrac-      left with ‚Äôs on the right, until a zig or zag lands on the attrac-
                                                                     tor and terminates the oscillation. This behavior corresponds
tor, we can map out the set of all trees decoded by the RAAM
                                                                     to the recursive component of the grammar. In (?), we pro-
at a given resolution. As described earlier, each tree in this
set corresponds to an equivalence class of points that all de-       vide a constructive proof for obtaining these behaviors at any
                                                                     resolution.
code to that tree. Points in the same class tend to cluster to-
gether, giving us an interesting way of laying out the RAAM‚Äôs           The proof gives us an exact IRAAM ‚Äúcompetence‚Äù model
language spatially. Figure 5 shows this phenomenon for a             for this non-regular CF language. Specifically, we show that
RAAM that we hill-climbed to decode the language =?> >               there exists a set of weights for which a RAAM with an at-
(described in the next section), with grayscale denoting tree        tractor generated at a predetermined resolution contains all
equivalence classes rather than attractor points. The dramatic       and only the trees in the =?> > language. Performance limita-
striping pattern of the equivalence classes in this figure is not    tions on the sizes of the trees actually produced derive from
inherent in the fractal RAAM model, but derives from the             the resolution at which the non-attractor unit space is sam-
comparatively elegant solution that hill-climbing produced           pled, and not from an arbitrary stipulation or a breakdown of
for this language.                                                   the model.
                                                                        This infinite competence is not the only thing that IRAAM
                                                                     brings to connectionist NL modeling, however. Because
      Linguistic Advantages of New RAAM                              IRAAM is a method of encoding and decoding trees, not just
As we described earlier, the new RAAM formulation thor-              strings, its strong generative capacity is known. We can there-
oughly addresses the three shortcomings of the traditional           fore use IRAAM as a direct model of hierarchical linguistic
RAAM model. Infinite loops and terminating non-terminals             structure. An immediate implication of this result is that an
are both eliminated by making the terminal test be a test of         IRAAM can be used as a parser and not just a recognizer. To
whether or not a point is on the fractal attractor of the RAAM       the extent that real NL processing involves the assignment of
decoder.                                                             meaning to strings based on structure, and not merely gram-
   Furthermore, the new formulation provides a principled ac-        maticality judgments, this ability represents a significant ad-
count of generativity (grammatical competence). By treating          vance in the application of connectionism to NL.
the RAAM as a fractal that can be generated at any arbi-                Finally, and perhaps most interesting, is the way in which
trary resolution, we can increase the generative capacity of         IRAAM handles syntactic ambiguity. Consider the fractal
the RAAM without bound, giving us a model that scales per-           addressing scheme that we described earlier. Each terminal
fectly: hence the name Infinite RAAM (IRAAM). As we have             point (word) on the attractor is associated with an address
recently shown (?), it is a straightforward matter to hill-climb     which is simply the sequence of indices of the transforms
the weights for an IRAAM that generates   3:EGFIall
                                                H and only the       taken to arrive on the attractor point from other points on the
strings in the language =?> >A@B=?> > $DC         .                  attractor. Given M transforms, we would therefore assume

                                                              3N-30OPOQOP3
each digit in the sequence would fall in the range                          M . in increasing order of length)3 , with the fractal address 1 rep-
For example, a binary-branching IRAAM, with two N trans-                         resenting = and 2 representing . Hill-climbing was used to
forms, would have terminals with address digits  and . Us-                      learn the weights. Both the initial weights and the noise added
ing a one-digit address, this effectively puts each word into                    to each weight came from a Gaussian distribution with zero
one of M ‚Äúpart of speech‚Äù equivalence classes.                                   mean and a standard deviation of 5.0, with the added noise‚Äôs
      This is not the whole story, though. Because there can be                  standard deviation being scaled by the fraction of the training
more than one path to a given terminal from some other ter-                      set missed. The resulting weights were             N used to generate trees
minal on the attractor, some terminals will haveOQO ‚Äúambiguous‚Äù                  on an IRAAM with a resolution of ^ . The attractor was
addresses, containing digits out of the range  M , to express                   generated at that resolution and the initial starting point space
the fact that more than one transform was taken to arrive at                     was also sampled at that resolution.
that point in the sequence. Continuing the linguistic analogy,                         Hill-climbing did not produce good results on either of
this ambiguity corresponds to a given word‚Äôs belonging to                        these languages; the average success was six out of 14 strings
more than one part of speech, as in Chomsky‚Äôs ‚Äúflying planes‚Äù                    covered for both languages. It is, however, instructive to look
example, where flying can be either a verb or an adjective. For                  out how those successes were achieved. Comparing the best
the binary-branching IRAAM example, if a given point had                         hill-climbed networks from each language (10 strings cov-
both a left and right inverse on the attractor, a one-digit ad-                  ered), we found that most of the strings generated by the =?> >
                                                                                                                                                               . 74% of
dress
N         for that point would have to be a symbolNRT    S than  or
                                                       other                     network fit the general pattern of the training set:
    . In general, for a M -ary IRAAM, there are               possible          the strings fit the pattern =?> > . For the          . best WXW                    network,
one-digit
  NRUS       S addresses, consisting of M  unambiguous     values          and  however, only 14% fit the pattern WXW . In other words, the
           M       ambiguous values.                                             =?> > network was actually       . producing mostly ‚Äúgrammatical‚Äù
      This fact has great linguistic importance for IRAAM, for                   strings, whereas the WXW             network was essentially guessing.
the following reason: typically (but not exclusively), an                              We attribute these results to IRAAM‚Äôs           6 aforementioned ten-
IRAAM decoder will favor putting the V th non-ambiguous                          dency to put symbols of one class 2_= on                    6 the left side of a
terminal class in the V th position in a string of terminals,                    branch and symbols of another class 6 2                          on the    6a6 right side.
                                                                                                                                                                          6 6
because the same set of weights is used to generate the at-                      In other words,
                                                                                               6a6a6      trees of
                                                                                                               6 6 6the  form 2 ` =     ,  _
                                                                                                                                           2   = 4 2 ` =        , 2a2_=       ,
tractor and the transients to the attractor. The likeliest non-                    2_=b2`=42_=       , 2a2a2`=         , are much more    6 d 6          ‚Äúnatural‚Äù   6  for an
terminal structure of a binary-branching IRAAM will there-                       IRAAM than are trees of the form 2_=c= , 2                                , 2 = . But it
fore be (1 2), with structures (1 1), (2 1) and (2 2) being possi-               is precisely the latter types of trees that are              . used as building
ble but less likely to occur. If, however, this IRAAM contains                   blocks for the mirror-image language WXW . This bias makes
ambiguous terminals, it will very likely decode the structures                   the mirror-image language much harder for an IRAAM to
(1 3), (3 2) and (3 3) as well.                                                  learn than the counting language =e> > , despite the fact that
      Returning to the ‚Äúflying planes‚Äù example, let us assign un-                both are expressible by a simple CFG.
ambiguous verbs like are the category 1, unambiguous nouns                             Although this result is by no means a proof of any sort,
like planes2 the category 2, and the ambiguous flying the cat-                   we consider it interesting for two reasons. First, it suggests
egory 3. With this assignment, the natural ability of a binary                   that the languages generable by an IRAAM share an im-
IRAAM to decode the structures (1 (3 2)) and ((1 3) 2) gives                     portant formal property with NL, namely, the avoidance of
us both parses of the expression are flying planes. Hence,                       mirror-image constructions. Second, the result illustrates how
we have an existence proof of a RAAM that can deal with                          IRAAM imposes a constraint between the terminal symbol
syntactic ambiguity and non-deterministic grammars.                              ‚Äúsemantics‚Äù and the nonterminal ‚Äúsyntax.‚Äù This constraint
      In short, we believe that IRAAM not only solves the prob-                  is absent from the definition of CFG‚Äôs (or of any grammar
lems of the earlier RAAM model, but also addresses the lin-                      in the Chomsky hierarchy), where any terminal symbol can
guistic inadequacies of recurrent neural net models that we                      appear anywhere. To the extent that individual natural lan-
discussed earlier.                                                               guages favor putting a given part of speech in fixed locations
                                                                                 in a sentence or phrase (e.g., English generally has subject-
                     What IRAAM Can‚Äôt Do                                         verb-object, Japanese subject-object-verb), IRAAM appears
                                                                                 to have an advantage over traditional grammars as a model of
In the first section of this paper we outlined two linguis-                      NL.
tic criteria for a plausible NL model: the model should be
able to handle ‚Äúslightly‚Äù non-CF phenomena like copying and                                     Conclusion and Interpretations
crossed serial dependencies and should also be incapable of
handling CF phenomena absent from or deprecated in NL‚Äôs,                         We have demonstrated a new formulation of RAAM, which,
like mirror-image constructions, or should incur a relatively                    by using a fractal attractor as a terminal test, enables the
high cost in producing or parsing those structures.                              model to show competence and ambiguity, to represent a
      To investigate the latter point, we tested the ability of the              variety of tree structures, and not to represent deprecated
IRAAM model shown in. figure 1 to3 %]‚Äúlearn‚Äù the context-free                    mirror-image structures. We plan to relate this new formula-
languages = > > and WXW , WZY\[%=              . The training set con-                 3
                                                                                         The number 14 was chosen because it allowed us to include
sisted of the first 14 exemplars of each language (enumerated                    all the members of fgfih for j fkjmlon . This language has more
                                                                                 strings of a given length than the language 5 , which meant that
       2                                                                         the exemplars of the latter had to be longer in order to enumerate the
         Readers troubled by the possibility of planes being a singular
verb (The carpenter planes the wood) can substitute cars or some                 first 14 of them. In effect, this makes the    task harder than the
other unambiguous noun here.                                                     fgfih task .

tion to linguistic formalisms like Tree-Adjoining Grammars       Kolen, J. (1994). Exploring the Computational Capabilities
(Joshi and Schabes 1997) and Categorial Grammars (Steed-           of Recurrent Neural Networks. Ph. D. thesis, Ohio State.
man 1999) having similar properties. We hypothesize that
this relation may be achieved through the use of multiplica-     Kwasny, S. and B. Kalman (1995). Tail-recursive distributed
tive connections to gate lexical varieties into naturally recur-   representations and simple recurrent neural networks. Con-
sive dynamics.                                                     nection Science 7(1), 61‚Äì80.
   Our work is by no means complete; nor do we mean to im-       Lawrence, S., C. Giles, and S. Fong (1998). Natural lan-
ply that NL grammar can be represented in four neurons with        guage grammatical inference with recurrent neural net-
12 weights! On the other hand, the principle of contractive        works. IEEE Transactions on Knowledge and Data En-
maps and the emergence of fractal attractors in the limit be-      gineering, to appear.
havior of nonlinear systems are mathematical facts, and have
been used successfully in image-compression systems. Re-         Manaster-Ramer, A. (1986). Copying in natural languages,
cent work by Tabor (1998) provides further evidence for the        context-freeness, and queue grammars. In Proceedings of
relevance of such principles to connectionist modeling of nat-     the 24th meeting of the Association for Computational Lin-
ural language. We now have reason to believe that these prin-      guistics, pp. 85‚Äì89.
ciples, under the right interpretation and scale, can support a
                                                                 Melnik, O. and J. Pollack (1998). A gradient descent method
neurally plausible universal grammar.
                                                                   for a neural fractal memory. In WCCI 98. International
                         References                                Joint Conference on Neural Networks: IEEE.
Bach, E., C. Brown, and W. Marslen-Wilson (1986).                Pinker, S. and A. Prince (1988). On language and connection-
   Crossed and nested dependencies in german and dutch:            ism: Analysis of a parallel distributed processing model of
   A psycholinguistic study. Language and Cognitive Pro-           language acquisition. Cognition 28, 73‚Äì193.
   cesses 1(4), 249‚Äì262.
                                                                 Pollack, J. (1990). Recursive distributed representations. Ar-
Barnsley, M. (1993). Fractals everywhere. New York: Aca-           tifical Intelligence 36, 77‚Äì105.
   demic Press.
                                                                 Postal, P. and D. Langendoen (1984). English and the class of
Blair, A. and J. Pollack (1997). Analysis of dynamical recog-      context-free languages. Computational Linguistics 10(3‚Äì
   nizers. Neural Computation 9(5), 1127‚Äì1142.                     4), 177‚Äì181.
Blank, D., L. Meeden, and J. Marshall (1991). Exploring the      Rodriguez, P., J. Wiles, and J. Elman (1999). A recurrent
   symbolic/subsymbolic continuum: A case study of raam.           neural network that learns to count. Connection Science 11,
   Technical Report TR332, Computer Science Department,            5‚Äì40.
   University of Indiana.
                                                                 Rumelhart, D. and J. McClelland (1986). On learning the past
Bresnan, J., R. Kaplan, S. Peters, and A. Zaenen (1982).           tenses of english verbs. In D. Rumelhart and J. McClel-
   Cross-serial dependencies in dutch.           Linguistic In-    land (Eds.), Parallel Distributed Processing: Explorations
   quiry 13(4), 613‚Äì634.                                           in the Microstructure of Cognition, Volume 2. MIT.
Chalmers, D. (1990). Syntactic transformations on dis-           Shieber, S. (1985). Evidence against the context-freeness of
   tributed represenations. Connection Science 2, 53‚Äì62.           natural language. Linguistics and Philosophy 8, 333‚Äì343.
Chomsky, N. (1956). Three models for the description of          Sperduti, A. (1993). Labeling raam. Technical Report TR-
   language. IRE Transactions on information theory 2, 113‚Äì        93-029, International Computer Science Institute.
   124.
                                                                 Steedman, M. (1999). Categorial grammar. In R. Wilson
Chomsky, N. (1957). Syntactic Structures. Mouton.                  and F. Keil (Eds.), The MIT Encyclopedia of Cognitive Sci-
                                                                   ences. MIT.
Culy, C. (1985). The complexity of the vocabulary of bam-
   bara. Linguistics and Philosophy 8, 345‚Äì351.                  Stucki, D. and J. Pollack (1992). Fractal (reconstructive ana-
                                                                   logue) memory. In 14th Annual Cognitive Science Confer-
Fodor, J. and Z. Pylyshyn (1988). Connectionism and cogni-         ence, pp. 118‚Äì123.
   tive architecture: A critical analysis. Cognition 28, 3‚Äì71.
                                                                 Tabor, W. (1998). Dynamical automata. Technical Report
Gazdar, G. (1982). Phrase structure grammar. In P. Jacobson        TR98-1694, Computer Science Department, Cornell Uni-
   and G. Pullum (Eds.), The Nature of Syntactic Representa-       versity.
   tion. Reidel.
                                                                 Van Gelder, T. (1990). Compositionality: a connectionist
Higginbotham, J. (1984). English is not a context-free lan-        variation on a classical theme. Cognitive Science 14, 355‚Äì
   guage. Linguistic Inquiry 15(2), 225‚Äì234.                       384.
Joshi, A. and Y. Schabes (1997). Tree-adjoining grammars.        Williams, R. and D. Zipser (1989). A learning algorithm for
   In G. Rozenberg and A. Salomaa (Eds.), Handbook of For-         continually running fully recurrent neural networks. Neu-
   mal Languages and Automata, Chapter 3. Berlin: Springer         ral Computation 1, 270‚Äì280.
   Verlag.

