UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Measuring Verb Similarity
Permalink
https://escholarship.org/uc/item/9bw0t5sb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Resnik, Philip
Diab, Mona
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                         Measuring Verb Similarity
                                                 Philip Resnik and Mona Diab
                                              fresnik,mdiabg@umiacs.umd.edu
                                                 Department of Linguistics and
                                           Institute for Advanced Computer Studies
                                                     University of Maryland
                                                     College Park, MD USA
                           Abstract                                care must be taken in selecting items, as discussed below,
   The way we model semantic similarity is closely tied            and it also means that the same computational measures
   to our understanding of linguistic representations. We          may be capturing di erent properties for verbs than for
   present several models of semantic similarity, based on         nouns. For example, the is-a relationship in WordNet's
   di ering representational assumptions, and investigate          verb taxonomy (Fellbaum, 1998), central in the compu-
   their properties via comparison with human ratings of           tation of some measures, signi es generalization accord-
   verb similarity. The results o er insight into the bases        ing to manner, as in devour is-a eat; concomitantly, the
   for human similarity judgments and provide a testbed            verb taxonomy is considerably wider and shallower than
   for further investigation of the interactions among syn-
   tactic properties, semantic structure, and semantic con-        WordNet's noun taxonomy. Similarly, measures based
   tent.                                                           on syntactic dependencies may be sensitive to syntactic
                                                                   adjuncts, such as locative and temporal modi ers, that
                       Introduction                                occur predominantly with verbs rather than with nouns.
                                                                      In what follows, we rst discuss several di erent mea-
The way we model semantic similarity is closely tied to            sures of word similarity and their properties. We then
our understanding of how linguistic representations are            describe an experiment designed to obtain human sim-
acquired and used. Some models of similarity, such as              ilarity ratings for pairs of verbs, discuss the t of the
Tversky's (1977), assume an explicit set of features over          alternative measures to the human ratings, and suggest
which a similarity measure can be computed, and re-                some implications of these results for future work.
cent computational methods for measuring word similar-
ity can be thought of as an update of this idea on a large
scale, representing words in terms of distributional fea-
                                                                               Models of Verb Similarity
tures acquired via analysis of text corpora (e.g., Brown,          We consider three classes of similarity measure, corre-
Della Pietra, deSouza, Lai, & Mercer, 1992; Schutze,              sponding to three kinds of lexical representation. In the
1993). Other methods, following in the semantic net-                 rst, verbs are associated with nodes in a semantic net-
works tradition of Quillian (1968), focus less on explicit         work. In the second, verbs are represented by distri-
features and more on relationships among lexical items             butional syntactic co-occurrence features obtained via
within a conceptual taxonomy, sometimes going beyond               analysis of a corpus. In the third, verbs are associated
taxonomic relationships to also take advantage of fre-             with lexical entries represented according to a theory of
quency information derived from corpora (e.g., Rada,               lexical conceptual structure. These classes of represen-
Mili, Bicknell, & Blettner, 1989; Resnik, 1999).                   tation can be viewed as occupying three di erent points
   Although some of these approaches are not explicitly            on the spectrum from non-syntactic to syntactically rel-
designed as cognitive models, we have proposed that pre-           evant facets of verb meaning.
diction of human similarity can provide a useful point             Taxonomic Models
of comparison for computational measures of similarity,
noting that one must be aware that such comparisons                Taxonomic models of lexical and conceptual knowledge
can be quite sensitive to the speci c choice of test items         have a long history. In this work we use WordNet version
(Resnik, 1999). To date, we are only aware of compar-              1.5, a large scale taxonomic representation of concepts
isons having been done using noun similarity.                      lexicalized in English. As a model of the lexicon, Word-
   In this paper, we consider the problem of measuring             Net's verb hierarchy is limited by design to paradigmatic
the semantic similarity of verbs. Verb similarity is in            relations, in explicit contrast to attempts to organize se-
many respects a di erent problem from noun similar-                mantically coherent verb classes through shared syntac-
ity, because verb representations are generally viewed as          tic behavior.
possessing properties that nouns do not, such as syn-                 The simplest and most traditional measure of semantic
tactic subcategorization restrictions, selectional prefer-         similarity in a taxonomy counts the number of edges in-
ences, and event structure, and there are dependencies             be part-of-speech per se; one could argue that some nouns
among these properties.1 This means that particular                carry similar kinds of participant information, observing, for
                                                                   example, that x's gift of y to z parallels x gave y to z. We are
    1
      Admittedly, the relevant contrast may turn out not to        not attempting to address that issue here.

tervening between nodes (\edge counting"). A distance                closely resembles the measure just described. It di ers
in edges is converted to similarity by subtracting from              in normalizing the shared information content using the
the maximum possible distance in the taxonomy, giving                sum of the unshared information content of each item
the following measure of distance between verbs 1 and       w        being compared:
   2:
w
                                                             
                                                                              siminfo2( 1 2) = log2 p(   log p(Ti i) C
                                                                                                                                (4)
  wsimedge ( 1 2) = (2  max) , min len( 1 2) (1)                                                           1 ) + log p( 2 )
                                                                                        c ;c
               w ;w                                    c ;c                                               c              c
                                           c1 ; c2
where 1 ranges over ( 1 ), 2 ranges over ( 2 ), max is               where the i are the \maximally speci c superclasses"
                                                                                  C
          c             s w     c                   s w
                                                                     of both 1 and 2 . As a result of this normalization, the
the maximum depth of the taxonomy, and len( 1 2) is
                                                                               c      c
                                                           c ;c
                                                                     measure possesses some desirable properties, such as a
the length of the shortest path from 1 to 2, with ( )
                                            c        c         s w
                                                                       xed range from 0 to 1. Word similarity wsiminfo2 is
denoting the set of concepts in the taxonomy that rep-               de ned analogously to De nition (3).
resent senses of word . If all senses of 1 and 2 are in
                         w                       w        w
separate sub-taxonomies of the WordNet verb hierarchy
their edge-count similarity is de ned to be zero.
                                                                     Distributional Co-Occurrence Model
    The simple edge-counting approach has well known                 Information-based measures of similarity can be applied
problems, and arguments have been made for the follow-               to representations other than taxonomic structures. In-
ing measure of semantic similarity between concepts in a             deed, Lin demonstrates the generality of the idea by
taxonomy based on shared information content (Resnik,                showing how such a measure can be used to measure
1999):                                                               not only taxonomic distance but also string similarity
                                                                     and the distance between feature sets a la Tversky. The
        siminfo1( 1 2) = 2 max
                  c ;c
                                     ( 1 2)
                                                [, logp( )] (2)
                                                          c  ;       latter approach is illustrated by representing words as
                              c    S c ;c
                                                                     collections of syntactic co-occurrence features obtained
where ( 1 2) is the set of concepts that subsume both
          S c ;c                                                     by parsing a corpus. For example, both the noun duty
c 1 and 2, and , log p( ) quanti es the \information con-
          c              c                                           and the noun sanction would have feature sets contain-
tent" of node . This yields a measure of verb similarity
                 c                                                   ing the feature subj-of(include), but only sanction would
       wsiminfo1( 1 2) = max
                                       
                                         sim         (  1 2)
                                                             
                                                                 (3) have the feature adj-mod(economic), since \economic
                  w ;w
                                  1 2
                                 c ;c
                                               info1   c ;c    ;
                                                                     sanctions" appears in the corpus but \economic duties"
where 1 ranges over ( 1 ) and 2 ranges over ( 2 ), and               does not. Because these features include both labeled
          c             s w         c
p( ) is estimated by observing frequencies in a corpus.2
                                                         s w
                                                                     syntactic relationships and the lexical items lling argu-
    c
Intuitively, the quantity de ned in (3) measures the max-            ment roles, the underlying representational model can
imum overlap in information between the words being                  be thought of as capturing both syntactic and semantic
compared. When two words are not very similar, the in-               components of verb meaning.
formation content of their most informative subsumer                    Lin computes the quantity of shared information as
(the node maximizing , logp( )) is low: that sub-                    the information in the intersection of the distributional
               c                       c
sumer resides high in the taxonomy and thus has high                 feature sets for the two items being compared. This
probability, implying low information content. In the                yields the following measure:
most extreme case, the most informative subsumer is
just the top node of the taxonomy, in which case the                     wsimdistrib ( 1 2) = 2 ( (( ()) +1 ) \( (( 2)))) (5)
                                                                                        w ;w
                                                                                                         I F w          F w
probability is 1 and the shared information content (and                                              I F w   1     I F w    2
hence similarity) is 0. When two words are similar, that             where ( i) is the feature set associated with word i,
                                                                             F w                                                w
means there is a node lower in the taxonomy that sub-                and where (S ), the quantity of information
                                                                                  I
                                                                                                        P              in a feature
sumes them both; being lower in the taxonomy its prob-               set S , is computed as (S ) = , f 2S logp( ).4 In the
                                                                                             I                         f
ability is lower and therefore its information content is            experiments described here, we use similarity values ob-
higher. Crucially, structural notions such as \lower"                tained for verb pairs using Lin's implementation of his
and \higher", and the number of intervening arcs be-                 model, with his feature sets and probabilities obtained
tween nodes, play no actual role in this model of sim-               via analysis of a 22-million-word corpus of newswire text.
ilarity. As a result, unlike edge counting, this measure
does not fall prey to the rampant variation in density               Semantic Structure Model
within any realistic conceptual taxonomy, where a single             Our third method for assessing the semantic similarity
is-a link could represent a tiny semantic distance (e.g.             of verbs relies on elaborated representations of verb se-
ballpoint pen is-a pen) or a very large semantic distance            mantics according to the theory of lexical conceptual
(e.g. toy is-a artifact).3                                           structure, or LCS (Dorr, 1993; Jackendo , 1983). LCS
    Lin (1998) argues for an alternative information-based           representations make an explicit distinction between se-
measure of similarity that, when applied to a taxonomy,              mantic structure, which characterizes the grammatically
     2 For taxonomic measures described in this section, prob-       relevant facets of verb meaning, from semantic content,
abilities of nodes in WordNet 1.5 were estimated on the basis        which characterizes idiosyncratic information associated
of word frequencies in the Brown Corpus (Francis & Kucera,          with the verb but not re ected in its syntactic behavior.
1982).
     3
       Examples are from WordNet 1.5, where artifact signi es           4
                                                                          Note the assumption that features are independent, per-
a man-made object.                                                   mitting the summation of log probabilities.

This di erence between semantic structure and seman-        using ( ) as in (5), and we compute wsimlcs ( 1 2)
                                                                    I S                                             w ;w
tic content plays an important role in current research     as the maximum value of simlcs taken over the cross
on lexical representation (e.g. Grimshaw, 1993; Pinker,     product of all the words' lexical entries.6
1989; Rappaport, Laughren, & Levin, 1993). We take             It is worth emphasizing that this similarity mea-
advantage of this distinction here to derive a measure      sure considers only semantic structure, not seman-
that focuses exclusively on similarity of semantic struc-   tic content, and therefore only syntactically relevant
ture as disentangled from semantic content.                 components of meaning enter into the computation.
   To illustrate with a simple example, within an LCS       For example, in the comparison of LCS entries for
representational system roll and slide might both have      slide and roll , ( 1 ) \ ( 2 ) will never contain either
                                                                               F e      F e
semantic structure indicating a change of location, e.g.,   [manner hrollingi] or [manner hslidingi], and there-
   (goloc x                                                 fore any potential similarities or di erences between the
                                                            content elements | the physical aspects of sliding mo-
      (toloc x (atloc x y))                                 tion versus rolling motion based on real-world knowledge
      (fromloc x (atloc x z))                               | are excluded from the model.
      (manner hMi)),                                                                Experiment
and di er only in the value hMi | an element of seman-      In order to assess alternative computational models of
tic content within the semantic structure | indicating      similarity, we collected human ratings of similarity for
the manner of motion (either hslidingi or hrollingi).       pairs of verbs, following a design after that of Miller and
Such regularities in semantic structure are argued to       Charles (1991). Considering the additional complexities
provide an explanation for systematic relationships be-     in the verb lexicon, however, the selection of materials
tween meaning and syntactic realization (Levin & Rap-       required considerable care: we were careful to pay close
paport Hovav, 1998).                                        attention to syntactic subcategorization, thematic grids,
   If those regularities are a part of verb lexical repre-  and aspectual class information, as described below, in
sentations, then they also plausibly in uence ratings of    order to limit the possible dimensions across which the
verb similarity, and the question is how to assess similar- two verbs in a pair could di er and to focus on semantic
ity between two such structured representations. Lin's      similarity. We also designed two versions of the task,
work provides one plausible answer: decomposing com-        with and without presentation of verbs in context, in
plex representations into (pseudo-)independent feature      order to investigate the extent to which contextual nar-
sets and then comparing feature sets.5 Our method of        rowing of verbs' senses a ects ratings of similarity.
decomposition was particularly simple, recursively cre-
ating an independent feature from each primitive com-       Participants. Participants were 10 volunteers, all na-
ponent of the representation and the \head" of its subor-   tive speakers of English, ranging in age from 24 to 53,
dinates. So, for example, the feature set representation    without signi cant background in psychology or linguis-
of roll would contain six features:                         tics. All participated by e-mail.
   [goloc toloc fromloc manner]
   [toloc x atloc ]                                         Materials. In constructing the set of verb pairs for
   [atloc x y]                                              similarity ratings, we began with the set of verbs in a
                                                            large lexicon of LCS entries, containing entries for 4900
   [fromloc x atloc ]                                       verbs. Verb entries in the lexicon contain information
   [atloc x z]                                              about both aspectual features (dynamicity, durativity,
   [manner hrollingi].                                      telicity; Olsen, 1997) and thematic grid (identifying
                                                            whether or not a verb takes an agent, theme, goal, etc.)
The features of slide would be identical but for the last   | for example, the verb broil requires both an agent and
feature, which would instead be [manner hslidingi], and     a theme, and is marked as both durative and telic but
the nearly complete overlap between the feature sets for    not dynamic. For subcategorization information, we re-
the two verbs captures the fact that the semantic distinc-  ferred to the Collins Cobuild dictionary (Sinclair, 1995),
tion between this particular pair of verbs rests entirely   using the subcategorization frame for the rst listed verb
on semantic content and not semantic structure.             sense.
   Since we had available to us a large lexicon of LCS rep-    To construct verb pairs, we began by eliminating all
resentations for verbs in English (Dorr & Olsen, 1996,      verbs whose thematic grid did not require a theme, in
1997), containing thousands of lexical entries, we esti-    order to limit the range of variation in thematic grids.7
mated the probability of each feature by counting feature
occurrences within the lexicon. We de ne the similarity         6
                                                                  Although our probability estimate counts features within
of two LCS lexicon entries 1 and 2 using the shared         a set of types (entries in a large lexicon) rather than tokens
                              e       e
                                                            (verb instances in a large corpus), inspection of the estimated
information content of their feature sets:                  probabilities suggests that frequent features are suitably dis-
                                                            counted, having low information content, and rare features
           simlcs ( 1 2 ) = ( ( 1 ) \ ( 2))
                    e ;e        I F e      F e          (6) are highly informative. Corpus-based estimates are a matter
                                                            for future work.
    5
      We are grateful to Dekang Lin for suggesting this ap-     7
                                                                  All verbs require an agent, so the remaining variation is
proach to us.                                               in the presence or absence of oblique roles such as goal.

We then grouped the full set of verbs into eight lists
corresponding to the eight possible combinations of the                  Table 1: Comparing sets of ratings
three aspectual features, and restricted   our attention to
the four most numerous lists.8 Within each of those                         wsim        Context No Context
four lists, we created 12 pairs of verbs subject to the                     edge          .720          .675
constraint that the verbs' associated subcategorization                     info1         .779          .658
frames had to match, so as to avoid e ects of purely syn-                   info2         .768          .668
tactic similarity. Items were selected to span the range                   distrib        .453          .433
from low- to high-similarity verb pairs.                                     lcs          .313          .385
   In summary, a set of 48 verb pairs was constructed                    Combined .872                  .785
so that (i) both verbs in every pair require a theme,                    Inter-rater .793               .764
(ii) both verbs have the same subcategorization frame,
and (iii) both verbs come from the same aspectual class.
Verbs on the list were all given in the past tense. In
order to avoid ordering e ects, half the subjects in each   Results and Discussion. In order to judge the de-
condition saw items in a random order, and the other        gree to which sets of similarity ratings are predictive of
half saw the items in the reverse order.                    each other, we use a similarity coecient computed as
   To assess the e ects that contextual narrowing of verb   Pearson's . Table 1 provides a summary showing for
                                                                         r                                             r
senses might have on similarity ratings, the materials as   each computational model as compared to the mean of
just described were duplicated in order to create No Con-   the human subject ratings in the Context and No Con-
text and Context conditions. The conditions were iden-      text conditions.9
tical except that in the Context condition, each item was      The Combined row of the table shows the value of
accompanied by an example sentence for each verb illus-     multiple when the ve computational measures are
                                                                       R
trating the verb's intended sense. Each example sentence    compared with human ratings using a multiple regres-
came from the corresponding verb entry in the Collins       sion (see below), and the Inter-rater row of the ta-
Cobuild dictionary. For example, the example sentence       ble shows human average inter-rater agreement, mea-
for loosen was \He loosened his seat belt."                 sured by , using leave-one-out resampling (Weiss & Ku-
                                                                       r
                                                            likowski, 1991).
                                                               Examining these gures, we rst consider each com-
Procedure. The 10 subjects were split evenly into           putational model separately. It is unsurprising that the
Context and No Context groups. Subjects in the No           similarity measure based on LCS representations fares
Context group were given the set of 48 verb pairs,          worst, given the design of the experiment: the verb pairs
without example sentences, and asked to compare their       were selected so as to eliminate di erences of subcat-
meanings on a scale of 0{5, where 0 means that the verbs    egorization frame, aspectual class, and thematic grid,
are not similar at all and 5 indicates maximum similar-     ruling out a priori pairs that di er interestingly with
ity. Subjects were explicitly asked to ignore similarities  respect to semantic structure. The distributional mea-
in the sound of the verb and similarities in the num-       sure based on syntactic co-occurrence features may be
ber and type of letters that make up the verb. Subjects     a victim of its dependence on a particular corpus, and
were also asked explicitly to rate similarity rather than   of data sparseness | for example, glaring divergences
relatedness, with the instructions giving an example of     with human ratings include some verb pairs containing
the distinction. (For example, pay and eat are related      some lower-frequency words, such as embellish/decorate
in that they are things we do in restaurants, but they      and dissolve/dissipate. Turning to the taxonomic meth-
are not particularly similar.) Since some verbs in the set  ods, the information-based approaches appear superior
have low frequency, a \don't know" box was included for     to edge counting in the Context condition, consistent
subjects to mark if they were unsure of the meaning of      with previous work on noun similarity, though in the No
either verb. There was no time limit on the task, which     Context condition there are no clear di erences. We sus-
tended to take approximately 20 minutes.                    pect a di erence will emerge with a larger set of items,
   Subjects in the Context group were given exactly the     but this remains to be seen. Our inspection of by-item
same task, but using the Context materials, i.e. with           9
                                                                  From the full set of items, 10 verb pairs were excluded
each verb accompanied by an example sentence illustrat-     because some participant did not know the meaning of one or
ing the intended sense. As in the previous condition, two   the other verb. Moreover, in preparation of the nal version
orders of presentation were used within this condition to   of this paper, we discovered that 11 verb pairs inadvertently
avoid ordering e ects.                                      had been included despite failing to strictly match the crite-
   Each computational similarity measure took the set       ria described in the Materials section or having other minor
                                                            errors of presentation, and these are now excluded, as well.
of verb pairs as input, without context, and computed a     Although this is a large number of excluded items, we con-
similarity score for each.                                  sider them quite unlikely to have a ected participants' judg-
                                                            ments since the excluded pairs were distributed almost per-
                                                            fectly evenly over the four verb lists and varied across degrees
    8
      These were fdurativeg,            fdurative,dynamicg, of similarity, and since the pattern of results was una ected.
fdynamic,telicg, fdurative,dynamic,telicg. Verbs could and  We report all quantitative results in the paper based on only
did appear on multiple lists.                               the 27 non-excluded verb pairs.

ratings of the information measures suggests strongly       models being sensitive to at least some di erent informa-
that the di erences between the unnormalized and nor-       tion.
malized information-based measures are small in com-
parison to the role played by the structure of the Word-
Net verb taxonomy.
                                                                             General Discussion
                                                            The experimental results re ect the fact that similar-
   Comparison of human raters yields several interest-      ity measures model di erent aspects of verb represen-
ing observations. First, a comparison of the Context        tation and use. Taxonomic similarity measures place
and No Context mean ratings by human participants           little emphasis on verbs' argument structure, empha-
yields = 89, which provides some reassurance that
        r      :                                            sizing relationships of semantic content; for example,
subjects in the No Context condition are generally inter-   drag and tug appear quite close in the taxonomy (un-
preting the verbs in the same sense as are subjects in the  der displace) although they di er signi cantly in seman-
Context condition | where, recall, the context sentence     tic structure (e.g. in \the tailpipe dragged" and \the
encouraged interpretation according to the rst listed       donkey tugged" the syntactic subjects have di erent the-
verb sense in the Collins Cobuild dictionary. Second,       matic roles). Conversely, semantic structure is empha-
however, average inter-rater agreement in the two con-      sized in the measure based on LCS representations to the
ditions (.79 and .76) is much lower than that obtained      exclusion of real-world knowledge, such as the similarity
in a noun ratings experiment using the same method,         of the physical motions of dragging and tugging. Distri-
where leave-one-out resampling yielded an estimate of       butional similarity based on syntactic co-occurrence fea-
r = 90 (Resnik, 1999). This may re ect the small sam-
     :                                                      tures is a combination, capturing elements of semantic
ple size in each group ( = 5), but we suspect that in
                          N                                 structure by means of the syntactic relationships (one-
actuality it is evidence that word similarity is harder for versus two-participant relationships), and also indirectly
subjects to quantify for verbs than for nouns. Third,       capturing elements of semantic content by means of the
we nd that subjects in the No Context condition have        lexical items co-occurring in those syntactic positions
a very strong tendency to assign higher similarity rat-     (tug being weighted more heavily against inanimate sub-
ings to the same pair as compared to subjects in the        jects than drag, for example). Based on the performance
Context condition, as determined using a paired -testt      of the models, and improved predictive power of the mul-
( = 27 (26) = 4 49
 N          ;t         :  ;p <   : 0002).                   tiple regression, we interpret our results as evidence that
   This last observation is consistent with the idea that   human ratings of similarity are sensitive to both paradig-
subjects in the No Context condition are accommodat-        matic and syntagmatic facets of verb representation, and
ing verb comparisons | allowing for more exible in-         we believe the computational models are capturing rel-
terpretations of verb meaning | in a way not available      evant aspects of verb representation in order to make
to subjects in the Context condition because their inter-   predictions about similarity judgments.
pretations are constrained by the context sentence. For        On a somewhat speculative note, it is interesting to
example, the verb pair compose/manufacture has a mean       brie y examine cases where the computational mod-
rating of 2.8 in the Context condition, and the context     els fail to capture similarities identi ed by the human
sentences are He sees the whole, not the various lines      raters. Consider, for example, items unfold/divorce,
that compose it and Many factories were manufacturing       chill/toughen, initiate/enter. Based on the WordNet
desk calculators. In the No Context condition, the mean     taxonomy, the verbs in these pairs have no common sub-
rating for this pair is 4.0, likely indicating that in the  sumer, so the shared information content is zero; nor do
process of comparison, subjects focused on available se-    the distributional or LCS measures predict that they are
mantic elements of compose's meaning that are closest to    at all similar. The human mean ratings are low (aver-
manufacture (e.g., the notion of composing as creating,     aging 1.6, 1.4, and 3.2, respectively, in the No Context
She composed satirical poems for the New Statesman).        condition), but why are they not zero | and why are
                                                            they in fact higher than the ratings for some other pairs,
   As a preliminary step toward combining models, we        such as open/in ate (0.6), where one could also iden-
performed a multiple regression predicting human rat-       tify reasons for believing the meanings have something
ings using the ratings of the ve computational models       in common? It would appear that in these cases subjects
as independent variables, with the results shown in Ta-     are nding similarities of meaning according to dimen-
ble 1 as Combined. Although we have not5 extensively        sions that we have not yet formalized. The apparent
analyzed these data, regressions using all 2 , 1 = 31       sense extensions verge on the metaphorical: one can de-
combinations of models show that the highest multiple    R  scribe divorce as the unfolding of a marriage, observe a
is obtained when all ve models are combined, that the       person chill and toughen in response to an insult, en-
two di erent information-based measures are making es-      ter a group by being initiated into it. Capturing those
sentially the same contribution to the combined model       dimensions of similarity in our models will require a bet-
(consistent with our observation that WordNet structure     ter understanding than we have at present of how word
plays the dominant role, rather than details of the mea-    meanings are represented and organized.
sure), and that the LCS measure contributes little for         Even for the time being, however, the work described
this set of items. Taking these observations into account,  in this paper o ers a method and a testbed for investi-
the improvement in predictive power when combining          gating lexical issues that can go well beyond the present
models comes from distributional and information-based      experiments. We chose here to tightly control aspect and

syntactic subcategorization while allowing our test items          manuscript, Center for Cognitive Science, Rutgers
to di er on thematic grids and vary widely with respect            University, New Brunswick, New Jersey.
to semantic content. Having validated the approach |         Jackendo , R. (1983). Semantics and Cognition. The
performance being consistent with what one would pre-              MIT Press, Cambridge, MA.
dict of the alternative models given the design of the       Levin, B., & Rappaport Hovav, M. (1998). Building
task | the initial work opens the door to other con g-             Verb Meanings. In Butt, M., & Geuder, W. (Eds.),
urations, controlling variation among subcategorization            The Projection of Arguments: Lexical and Com-
frames, aspectual features, thematic grids, and semantic           positional Factors, pp. 97{134. CSLI Publications,
content in other combinations. What is crucial is that             Stanford, CA.
implemented models of similarity, drawing on such theo-      Lin, D. (1998). An information-theoretic de nition of
retical constructs, yield testable predictions that can be         similarity. In Proceedings of the Fifteenth Inter-
veri ed through careful experimentation.                           national Conference on Machine Learning (ICML-
                    Acknowledgments                                98) Madison, Wisconsin.
We are grateful to Dekang Lin and Amy Weinberg for           Miller, G. A., & Charles, W. G. (1991). Contextual cor-
valuable discussions, to Dekang Lin for his kindly com-            relates of semantic similarity. Language and Cog-
puting values of distributional similarity (De nition 4)           nitive Processes, 6 (1), 1{28.
for the verb pairs in our experiment, and to three           Olsen, M. B. (1997). A Semantic and Pragmatic Model
anonymous reviewers for their helpful comments. This               of Lexical and Grammatical Aspect. Garland, New
work was supported in part by DARPA/ITO Contract                   York.
N66001-97-C-8540.                                            Pinker, S. (1989). Learnability and Cognition. MIT
                                                                   Press, Cambridge, MA.
                Appendix: Verb Pairs                         Quillian, M. R. (1968). Semantic memory. In Minsky,
                                                                   M. (Ed.), Semantic Information Processing. MIT
  bathe         kneel                 loosen     open              Press, Cambridge, MA.
  chill         toughen               neutralize energize
  compose       manufacture           obsess     disillusion Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989).
  compress      unionize              open       in ate            Development and application of a metric on se-
  crinkle       boggle                percolate  unionize          mantic nets. IEEE Transaction on Systems, Man,
  displease     disillusion           plunge     bathe             and Cybernetics, 19 (1), 17{30.
  dissolve      dissipate             prick      compose
  embellish     decorate              swagger    waddle      Rappaport, M., Laughren, M., & Levin, B. (1993). Lev-
  festoon       decorate              unfold     divorce           els of lexical representation. In Pustejovsky, J.
    ll          inject                wash       sap               (Ed.), Semantics and the Lexicon. Kluwer.
  hack          unfold                weave      enrich
  initiate      enter                 whisk      de ate      Resnik, P. (1999). Semantic similarity in a taxonomy:
  lean          kneel                 wiggle     rotate            An information-based measure and its applica-
  loosen        in ate                                             tion to problems of ambiguity in natural language.
                          References                               Journal of Arti cial Intelligence Research (JAIR),
                                                                   11, 95{130. http://www.cs.washington.edu/
Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai,            research/jair/abstracts/resnik99a.html
        J. C., & Mercer, R. L. (1992). Class-based n-gram    Schutze, H. (1993). Word space. In Hanson, S. J.,
        models of natural language. Computational Lin-             Cowan, J. D., & Giles, C. L. (Eds.), Advances
        guistics, 18 (4), 467{480.                                 in Neural Information Processing Systems 5, pp.
Dorr, B. J. (1993). Machine Translation: A View from               895{902. Morgan Kaufmann Publishers, San Ma-
        the Lexicon. The MIT Press, Cambridge, MA.                 teo CA.
Dorr, B. J., & Olsen, M. B. (1996). Multilingual Gener-      Sinclair, J. (Ed.). (1995). Collins Cobuild English Dic-
        ation: The Role of Telicity in Lexical Choice and          tionary. Collins. Patrick Hanks, managing editor.
        Syntactic Realization. Machine Translation, 11 (1{   Tversky, A. (1977). Features of similarity. Psychological
        3), 37{74.                                                 Review, 84, 327{352.
Dorr, B. J., & Olsen, M. B. (1997). Deriving Verbal          Weiss, S. M., & Kulikowski, C. A. (1991). Computer sys-
        and Compositional Lexical Aspect for NLP Appli-            tems that learn: classi cation and prediction meth-
        cations. In Proceedings of the 35th Annual Meeting         ods from statistics, neural nets, machine learning,
        of the Association for Computational Linguistics           and expert systems. Morgan Kaufmann, San Ma-
        (ACL-97), pp. 151{158 Madrid, Spain.                       teo, CA.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic
        Lexical Database. MIT Press.
Francis, W. N., & Kucera, H. (1982). Frequency Anal-
        ysis of English Usage: Lexicon and Grammar.
        Houghton Miin, Boston.
Grimshaw, J. (1993). Semantic structure and seman-
        tic content in lexical representation. unpublished

