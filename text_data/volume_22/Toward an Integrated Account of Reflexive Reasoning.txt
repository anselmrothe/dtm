UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Toward an Integrated Account of Reflexive Reasoning
Permalink
https://escholarship.org/uc/item/1kz6g4nc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Hummel, John E.
Choplin, Jesse M.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                                  reflective inferences become more reflexive with experience?
                                                                  To a young child, it may not be immediately obvious that
      Toward an Integrated Account of                             Bill's selling Mary his car implies that she now owns the
     Reflexive and Reflective Reasoning                           car; but after a sufficient number of examples, the child will
                                                                  eventually induce a schema that makes the relationship
        John E. Hummel (jhummel@lifesci.ucla.edu)                 between buying and owning reflexive (if evidenced only by
                      Department of Psychology                    the fact that the inference is reflexive for an adult).
                University of California Los Angeles                   In the literature on human cognition, the study of
                           405 Hilgard Ave.                       reflexive and reflective reasoning have been largely separate,
                    Los Angeles, CA 90095-1563                    with the former more common in the study of (for instance)
                                                                  story comprehension (e.g., Kintsch & van Dijk, 1978;
         Jesse M. Choplin (choplin@lifesci.ucla.edu)              Shastri & Ajjanagadde, 1993; St. John, 1992; St. John &
                      Department of Psychology                    McClelland, 1990), and the latter predominating in the
                University of California Los Angeles              study of problem solving (e.g., De Soto, London &
                           405 Hilgard Ave.                       Handel, 1965; Byrne & Johnson-Laird, 1989; Newell &
                    Los Angeles, CA 90095-1563                    Simon, 1976) and reasoning by analogy (e.g., Forbus et al.,
                                                                  1995; Gentner, 1983; Holyoak & Thagard, 1989; Hummel
                                                                  & Holyoak, 1997). Similarly, computational accounts of
                                Abstract                          reflexive inference (e.g., Shastri & Ajjanagadde, 1993; St.
                                                                  John, 1992) have typically had little to say about more
   Some inferences are seemingly automatic (reflexive;            reflective forms of reasoning, and models of reflective (e.g.,
   Shastri & Ajjanagadde, 1993), whereas others require more      analogical) reasoning have had little to say about the nature
   effort (i.e., are reflective). We present the beginnings of an of reflexive reasoning.
   integrated account of reflexive and reflective reasoning,           The most reflexive form of inference is encoding—
   based on the LISA model of analogical reasoning (Hummel        inferring, for example, that "Mary" in "Bill loves Mary" is
   & Holyoak, 1997). The account holds that reflexive             an adult human female. It is this most reflexive form of
   inferences are those that can be generated automatically       inference that has been most neglected in models of
   based on existing knowledge in long-term memory,
                                                                  reflective reasoning. One consequence is that these models
   whereas reflective inferences require explicit structure-
   mapping and therefore demand greater attention and             must be given, in full detail, the representations they are to
   working memory. According to this account, reflexive           use for reasoning. For example, the models of Forbus et al.
   inferences manifest themselves in the semantic encoding        (1995), Holyoak and Thagard (1989) and Hummel and
   of objects and predicates, whereas reflective inferences       Holyoak (1997) draw analogies between situations whose
   manifest themselves as explicit propositions. In contrast      representations are fully specified for them. In contrast to
   to reflexive inferences, which are equally reflexive,          human reasoners, who can read a sentence such as "Bill
   reflective inferences may require more or less effort. We      loves Mary, but Mary loves Tom" and infer the details for
   present preliminary simulation results demonstrating that      themselves (e.g., that Bill and Tom are adult human males,
   both kinds of inference can be modeled in a single             etc.), these models must be handed all this information for
   architecture for representing propositional knowledge.
                                                                  each analogy they are asked to solve.1 One reason for this
                                                                  division between models of reflective and reflexive
          Reflexive vs. Reflective Reasoning                      reasoning may be that the two kinds of reasoning obey
                                                                  different computational constraints, and therefore demand
      Some inferences are so effortless that we are barely        different kinds of algorithms. At the same time, however,
aware of making them. Told that Bill sold Mary his car,           both kinds of inference take place within the same cognitive
you will infer that Mary now owns the car so automatically        architecture, and must operate on the same mental
that Shastri and Ajjanagadde (1993) describe the inference as     representations.
reflexive. Even more reflexive is the inference that Bill is           This paper presents the beginnings of an algorithmic
probably an adult human male, and Mary an adult human             account of the relationship between reflexive and reflective
female. Other inferences require more effort. Told that Bill      reasoning. In broad strokes the account holds that both
loves Mary and Mary loves Tom, it is natural to infer that        kinds of reasoning require the capacity to dynamically bind
Bill is likely to be jealous of Tom, but this inference
arguably requires a bit more reflection (and is less certain)
                                                                  1 One notable exception to this generalization is Hofstadter &
than the inference that Mary is a woman. More effortful
still are many kinds of inferences made in the context of         Mitchel's (1994) CopyCat model, which solves analogy
scientific and mathematical reasoning, planning, and so           problems of the form X:Y::Z:?, and uses routines to change its
                                                                  representation of X, Y and Z in order to find the best possible
forth. What is the relationship between reflexive inferences,     analogy. In contrast to other models of analogy, CopyCat is
such as Bill is male or Mary owns the car, and more               not "stuck" with fixed representations of the elements of its
reflective inferences, such as Bill may be jealous of Tom, or     analogies.    At the same time, however, this model cannot
matter and energy must be special cases of a common               simulate the kind of encoding discussed here, or the type of
physical principle? And what is the process by which              reflexive inferences discussed by Shastri & Ajjanagadde (1993).

variables to values (or equivalently, roles to fillers) in order    represents the buyer and object roles, respectively, and are
to permit flexible (rule-like) generalization (cf. Shastri &        connected to the corresponding semantics. Semantically-
Ajjanagadde, 1993, on the role of variable binding in               related predicates share units in corresponding roles (e.g.,
reflexive reasoning; Holyoak & Hummel, 2000, and                    seller and giver share many units), making the semantic
Hummel & Holyoak, 1997, on the role of variable binding             similarity of different predicates explicit. Object units are
in reflective reasoning). That is, both reflexive and               like predicate units except that they are connected to
reflective inferences are operations on symbolic                    semantic units describing things rather than roles. For
representations. In addition, we propose that what makes            example, Mary might be connected to units for human,
reflective reasoning more effortful than reflexive reasoning        adult, female, etc., whereas car might be connected to
is, at least in part, that the most reflexive inferences result     object, vehicle, etc.
from a kind of structured memory retrieval (i.e., retrieval
that exploits and maintains variable-value bindings),                                   sell-to (Bill, Mary, car).
whereas more reflective inferences require explicit structure
mapping. That is, as illustrated in the simulations below,
we propose that an inference will be fully reflexive when the
to-be-inferred information is already available in long-term
memory (LTM), and that it becomes progressively more                                Bill+          Mary+       car+
reflective as the to-be-inferred information must be                                seller         buyer       buy-obj.
constructed on the basis of mapping large, multi-
proposition structures.
     The starting point for this effort is Hummel &                                Bill      Mary               buy-    car
                                                                          seller                       buyer    obj.
Holyoak's (1997) LISA model of analogical reasoning, so
we will briefly sketch that model's approach to knowledge
representation and reflective inference (including memory
retrieval, structure mapping, and schema induction).
Mapping and retrieval are described in detail in Hummel
and Holyoak (1997), and inference an schema induction are              Figure 1. The LISA LTM representation of the
described in detail in Holyoak and Hummel (2000).                      proposition sell-to (Bill, Mary, car).
                      The LISA Model                                     Sub-proposition units (SPs; rectangles in Figure 1)
                                                                    bind roles to objects in LTM. Sell-to (Bill, Mary, car)
     The core of LISA's architecture is a system for                would be represented by three SPs, one binding Bill to
representing propositions in working memory (WM) by                 seller, one binding Mary to buyer, and one binding car to
dynamically binding roles to their fillers, and encoding            sell-object. SPs have bi-directional excitatory connections
those bindings in LTM. LISA uses synchrony of firing for            with the object and predicate units they bind together.
dynamic binding in WM (Hummel & Holyoak, 1992;                      Proposition (P) units (oval in Figure 1) reside at the top of
Shastri & Ajjanagadde, 1993). Case roles and objects are            the hierarchy and have bi-directional excitatory connections
represented in WM as distributed patterns of activation on a        with the corresponding SPs. Complete, multi-proposition
collection of semantic units (small circles in Figure 1); case      analogs (i.e., situations, events or schemas) are represented
roles and objects fire in synchrony when they are bound             by collections of structure units (see Figure 2).
together and out of synchrony when they are not. For                     The final component of LISA's architecture is a set of
example, to represent the proposition sell-to (Bill, Mary,          mapping connections between structure units of the same
car) in WM, semantic units representing the seller role of          type in different analogs. Every P unit in one analog may
the sell-to relation (e.g., transaction, exchange, etc.) fire in    share a mapping connection with every P unit in every other
synchrony with units representing Bill while units                  analog; likewise, SPs share connections across analogs, as
representing the buyer role fire in synchrony with units            do objects and predicates. For the purposes of mapping and
representing Mary, and units for the object role fire in            retrieval, analogs are divided into two mutually exclusive
synchrony with units representing car. The three sets of            sets: a driver and one or more recipients. Retrieval and
units (Bill+seller, Mary+buyer and car+object) must be              mapping are controlled by the driver. LISA performs
mutually de-synchronized with one another.                          retrieval and mapping as a form of guided pattern matching.
   A proposition is encoded in LTM by a hierarchy of                As P units in the driver become active, they generate (via
structure units (Figures 1 and 2). At the bottom of the             their SP, predicate and object units) synchronized patterns
hierarchy are predicate and object units (triangles and large       of activation on the semantic units (one pattern for each
circles, respectively, in Figure 1). Each predicate unit            role-filler binding). The semantic units are shared by all
locally codes one case role of one predicate. For example,          propositions, so the patterns generated by one proposition
seller represents the first (seller) role of the predicate sell-to, tend to activate one or more similar propositions in LTM
and has bi-directional excitatory connections to all the            (retrieval) or in working memory (analogical mapping).
semantic units representing that role; buyer and sell-object        Mapping differs from retrieval solely by the addition of the

modifiable mapping connections. During mapping, the            (such as Bill and Beth) will fire in synchrony with one
weights on the mapping connections grow larger when the        another, and non-corresponding elements (e.g., Bill and
units they link are active simultaneously, permitting LISA     futon) will fire out of synchrony. As a result, LISA learns
to learn the correspondences generated during retrieval.       mapping connections from Bill to Beth, Mary to Peter, and
These connection weights also serve to constrain subsequent    car to futon. Likewise, the roles of sell-to in situation 1
memory access.           By the end of a simulation run,       map to the corresponding roles of sell-to in situation 2.
corresponding structure units will have large positive         However, nothing in situation 2 maps to the roles of owns
weights on their mapping connections, and non-                 in situation 1. Therefore, when owns (Mary car) fires in
corresponding units will have strongly negative weights.       situation 1, LISA will build units in situation 2 to
Hummel & Holyoak (1997) showed that these operations           correspond to the structures in situation 1 representing that
account for a large body of findings in the literature on      proposition: It will build units corresponding to owner and
human analogical reasoning.                                    owned, and connect them to the semantic units representing
                                                               those roles; it will build SPs corresponding to owner+Mary
                                                               and owned+car, and connect them to owner and Peter and
   Situation 1
                                           own (Mary, car).    owned and futon, respectively; finally, it will also build a P
     sell-to (Bill, Mary, car).                                unit corresponding to the whole proposition, connecting it
                                                  P2           to the newly created SPs. (LISA "knows" what to connect
                    P1                                         to what simply by virtue of which units are firing in
                                                               synchrony with one another; see Holyoak & Hummel,
                                            Mary+    car+
                                                               2000.) That is, it will infer that Peter now owns the futon.
        Bill+       Mary+     car+          owner    own-obj.
                                                                    The same operations permit LISA to perform schema
        seller      buyer     buy-obj.                         induction in a third "schema" analog. Although we have
                                                               described the activation of semantic units only from the
                                                   owner own-  perspective of the driver, recipient analogs also feed
                      Mary            buy-   car
               Bill                                       obj. activation to the semantic units. The activation of a
      seller                 buyer    obj.
                                                               semantic unit is a linear function of its input, so any
                                                               semantic unit that is common to both the driver and
                                                               recipient will receive input from both and become roughly
                                                               twice as active as any semantic unit receiving input from
                                                               only one analog. Shared semantic elements are thus tagged
                       Peter          buy- futon               as such by their activations. These shared elements are
               Beth
       seller                buyer    obj.                     encoded into the schema by the same unsupervised learning
                                                               algorithm that performs analogical inference: Units in the
                                                               schema connect themselves to semantic units and to one
          Beth+       Peter+    futon+
                                                               another based on their co-activity. Because the learning
          seller      buyer     buy-obj.
                                                               algorithm is sensitive to the activations of the semantic
                                                               units, object and predicate units in the schema preferentially
                     P1                                        learn connections to the semantic that are common to—i.e.,
                                                               the intersection of—the corresponding units in the known
        sell-to (Bext, Peter, futon).                          situations. In the case of the current example, the induced
                                                               schema would be roughly sell-to (person1, person2, object),
   Situation 2                                                 and own (person2, object).
   Figure 2. LISA LTM representation of sell-to (Bill,                  Extension to Reflexive Reasoning
   Mary, car) and own (Mary, car) (Situation 1; top) and
   sell-to (Beth, Peter, futon) (Situation 2; bottom).              As described above, LISA is a model of reflective
                                                               reasoning that makes inferences about novel situations based
     Augmented with unsupervised learning and intersection     on explicit analogies (i.e., structure-mappings) to familiar
discovery, LISA's approach to mapping supports inference       situations. However, the operations it uses for analogy,
and schema induction as a natural consequence (Holyoak &       inference and schema induction—most notably, the feedback
Hummel, 2000).            Consider an analogy between two      from the recipient analog to the semantic units (henceforth
situations (Figure 2): In situation 1, Bill sells his car to   recipient feedback)—suggest themselves as the beginnings
Mary (proposition P1), so Mary now owns the car (P2); in       of an account of reflexive inference. The basic idea is to use
situation 2, Beth sells her futon to Peter (P1), but there is  the recipient feedback from structures in LTM (including
no explicit statement that Peter now owns the futon.           both general schemas and specific situations) to encode the
During mapping, corresponding elements in the two analogs      semantic representation of predicates and objects in the
will become active simultaneously. For instance, sell-to       driver. That is, encoding is seen as a collection of reflexive
(Bill, Mary, car) in the driver, will activate sell-to (Beth,  inferences about the properties of the predicates and objects.
Peter, futon) in the recipient, so corresponding elements

Using the recipient feedback in this way solves only one of        and these attributes will have to replace the corresponding
several problems that must be solved in order to provide a         default values in the schema.
general integrated account of reflexive and reflective                  We simulated the reflexive form of this inference as
reasoning; however, the simulations reported here suggest          follows. We generated five situations (analogs), one
that it is a useful first step.                                    corresponding to a "new" situation (thinking about a
                                                                   wooden spoon; analog 1), and the other four corresponding
   (a)                     (b) large(wooden-spoon)                 to various schemas in LTM. Analog 1 consists of the
                                                                   single proposition exist (wooden-spoon), where the object
      wooden-spoon                                                 wooden-spoon is connected to a single semantic unit,
      (the object unit)                                            wooden-spoon, which serves to represent the type wooden
                                                                   spoons; the predicate exist is not connected to any semantic
                                       wooden-spoon+               units. (Exist is a vehicle for instantiating wooden-spoon in
                                       large                       a proposition so that it can be activated—it allows LISA to
                                                                   "think about" wooden spoons devoid of any particular
                                large            wooden-spoon      context.) Analog 2 is a schema for wooden spoons
                           (the predicate)       (the object unit) consisting of two propositions: wooden (wooden-spoon)
                                                                   and big (wooden-spoon). The object unit wooden-spoon
                                                                   has positive connections to the type semantic wooden-spoon
                                                                   and to semantics for utensil, spoon, wooden, and big. It
                                                                   has negative (inhibitory) connections to semantics for metal.
            large                                                  The predicate wooden has positive connections to semantics
     (the semantic unit)                                           for material and wood, and an inhibitory connection to
                                                                   metal; big excites semantics for size and big, and inhibits
                                                                   small. Analog 3 is a schema for metal spoons, and consists
   Figure 3. Ways to represent the fact that wooden                of metal (metal-spoon) and small (metal-spoon). The
   spoons are large in LISA: (a) large as a semantic feature       semantic representations of metal-spoon, metal and small
   connected to the object unit wooden-spoon; (b) large as         are analogous to those of wooden-spoon, wooden and big,
   a predicate in the proposition large (wooden-spoon).            respectively, except that the appropriate semantics are
                                                                   reversed (e.g., the predicate small is connected to the
      Consider the concept of a wooden spoon.                  The semantic small rather than big, etc.). Analog 4 is a schema
statement "wooden spoon" makes exactly two properties of           for spoons in general, and consists of the propositions
the object wooden spoon explicit: it is wooden and it is a         utensil (spoon) and concave (spoon). Analog 5 is a schema
spoon. But upon encountering these properties, it is               for horseback riding, consisting of the single proposition
irresistible to infer additional properties, such as that it is    ride (horse). Analog 5 serves as a foil to ensure that the
large (as spoons go), it is more likely to be used for             model will not simply activate all knowledge in LTM in
cooking than for eating, etc. (cf. Medin & Shoben, 1988).          the course of drawing reflexive inferences about the
These properties may be represented in two qualitatively           (semantically empty) wooden spoon in analog 1.
different ways: as semantic "features" of wooden spoons, or             The goal of the simulation is to activate exist (wooden-
as explicit propositions. In terms of the LISA architecture,       spoon) in analog 1 and observe which schemas it activates
these ways of representing the properties of wooden spoons         in LTM, and whether the recipient feedback from the objects
are, respectively, as connections from the wooden-spoon            and predicates in those schemas allow wooden-spoon in
object unit to semantic units for large, cooking, etc.,            analog 1 to learn an appropriate semantic encoding. We
(Figure 3a) and as full propositions, complete with                therefore set analog 1 to be the driver, and left analogs 2 - 4
predicates, SPs and P units (Figure 3b). We hypothesize            "dormant" in LTM (see Hummel & Holyoak, 1997). The
that the former type of (semantic feature) representation is       proposition exist (wooden-spoon) was then fired, and
established reflexively, as an automatic part of encoding the      propositions in LTM were allowed to respond, feeding
representation of wooden spoons, whereas the latter                activation back to the semantic units. When exist (wooden-
(propositional) form is established more reflectively, by          spoon) first fired, both wooden (wooden-spoon) and big
thinking explicitly about the properties of wooden spoons.         (wooden-spoon) became active in analog 2 (the wooden
It is important to note that inferring the properties of           spoon schema). Exist is semantically empty, so the only
wooden spoons (either reflexively or reflectively) it is not a     semantic feature of analog 1 that activated anything in
simple matter of replacing the default value of the material       analog 2 is the type semantic wooden-spoon (which is
slot in the spoon schema (i.e., metal) with the value              shared by the object wooden-spoon in analog 2). As a
wooden (e.g., as suggested by Smith & Osherson, 1984;              result, wooden (wooden-spoon) and big (wooden-spoon)
Smith et al., 1988), because the attributes of spoons (i.e,        became equally active in analog 2. The feedback from these
the values bound to the slots of the schema) are correlated:       propositions to the semantic units began to activate other
Other attributes such as size=large will have to be infered,       schemas in LTM: the semantics utensil and spoon activated
                                                                   units in analogs 3 (the metal spoon schema) and 4 (the

generic spoon schema). At the same time, the object            abstraction, whereas reflective inferences—which cause the
wooden-spoon (in analog 2) inhibited the semantics for         construction of explicit propositional structures—will only
metal. This inhibition propagated into analog 3 (the metal     take place when the reasoner explicitly reflects on the fact
spoon schema), preventing that schema from becoming            that the object belongs to the category (i.e., explicitly maps
active and in turn, preventing it from activating its own      the category schema onto the object). To our knowledge,
semantics. When the pattern of activation settled, analogs 2   no one has yet tested this prediction of the model.
and 4 were fully active (i.e., both propositions were active
in both analogs), along with all the semantic units to which                            Discussion
they are connected. As a result, the object wooden-spoon in
analog 1 learned connections to the semantics for utensil,           Using simple operations already in place to simulate
spoon, wooden and big (due to the feedback from the            reflective analogy-based inference—namely, recipient
wooden spoon schema), and to concave and utensil (based        feedback and unsupervised learning—LISA was able to
on the generic spoon schema): The model reflexively            reflexively infer the meaning of "wooden spoon" and "metal
inferred the semantic properties of the wooden spoon.          spoon" based on examples in LTM. These inferences were
     In a second simulation, analog 1 consisted of the         reflexive in the sense that they did not require the model to
proposition exist (metal-spoon)—this time having LISA          explicitly map the structures in the new example (analog 1)
"think about" metal spoons—and we ran the same                 to the structures in LTM. Instead, they were drawn in the
operations described above. This time, analog 3 (the metal     course of what is analog retrieval in LISA (i.e., the process
spoon schema) and analog 4 (the generic spoon schema)          of retrieving a source analog or schema from LTM given a
became active, and the model inferred the properties of the    novel target as a cue; see Hummel & Holyoak, 1997). By
metal-spoon in analog 1: metal-spoon learned connections       the end of the first two simulations, the objects wooden-
to the semantic units for utensil, spoon, metal and small      spoon (in the first simulation) and metal spoon (second
(due to the feedback from the metal spoon schema), and to      simulation) had semantic encodings that were richer than
concave and utensil (based on the generic spoon schema).       what was provided at the beginning of the simulation. In
In both these simulations, it is interesting to note that LISA each case, the object unit started with a single semantic
assigned each object (the metal spoon or the wooden spoon)     feature (wooden-spoon or metal-spoon) and ended with a
to the most general category appropriate (by activating the    semantic encoding specifying its size (big or small),
generic spoon schema), but it did not categorize metal         material (wooden or metal), shape (concave) and use
spoons as wooden spoons, or vice versa. As a result, it        (utensil). However, in neither of the first two simulations
made appropriate inferences about the objects at multiple      did analog 1 end up with any new propositions.               By
levels of abstraction (e.g., that the wooden spoon would be    contrast, in the third and fourth simulations, when the
big [which is specific to wooden spoons] and that it would     schemas were called into WM and allowed to map to analog
be concave [which is general to all spoons]).                  1, the model inferred propositions that explicitly stated the
     In the previous simulations, the inferences were purely   properties of the wooden spoon. According to the present
reflective, in the sense that we did not allow LISA to         account, inferring a new proposition (e.g., one stating
retrieve the schemas from memory and map them back onto        explicitly that the wooden spoon is big) is a reflective
analog 1. When we allowed the model to reflect on the          process that requires retrieval of a schema (or specific
properties of wooden spoons—by making the wooden               situation), and an explicit mapping of the structures in that
spoon schema the driver, the wooden spoon version of           schema to the structures in the new example.
analog 1 the recipient, and allowing it to explicitly map the        In this respect, our use of the term "reflexive" is
schema onto analog 1—it inferred the explicit propositions     somewhat more restrictive than Shastri & Ajjanagadde's
wooden (wooden-spoon) and big (wooden-spoon) in analog         (1993). On our account, an inference such as "Mary now
1. (It did do by exactly the same operations described         owns the car" is not strictly reflexive unless it is represented
previously in the discussion of LISA's operation.)             strictly as features in the semantic representation of Mary
Similarly, when we allowed it to reflect on the fact that      (i.e., as connections from the unit Mary to units
wooden spoons are spoons—by mapping the spoon schema           representing ownership). If instead (or in addition) the
into analog 1—it inferred the propositions utensil (wooden-    inference is represented as an explicit proposition (own
spoon) and big (wooden-spoon). But importantly, it did         (Mary car)), we would classify it as "reflective but easy" (as
not infer any of these propositions until it explicitly        noted previously, some reflective inferences are easier than
brought the corresponding schema into WM and mapped it         others). This distinction between our account and that of
onto analog 1. This property is interesting in combination     Shastri & Ajjanagadde stems primarily from the fact that
with the model's ability to reason reflexively to the most     LISA represents objects and predicates as distributed
generic category applicable (e.g., to assign wooden spoons     patterns of activation in WM, which precludes binding an
semantic features that are true of all spoons based on the     object to more than one predicate role at a time (see
generic spoon schema): Together, they predict that reflexive   Hummel & Holyoak, 1997). As a result, LISA makes a
inferences—which manifest themselves in the (implicit)         strong distinction between properties qua semantic features
semantic encoding of an object or predicate—will               and properties qua explicit propositions. By contrast,
automatically take place across multiple levels of category    Shastri & Ajjanagadde's model represents each object or

predicate as a localist unit, making it possible to "stack"   Gentner, D. (1983). Structure-mapping: A theoretical
predicates on objects, effectively representing multiple           framework for analogy. Cognitive Science, 7, 155-170.
predicate-object bindings (i.e., multiple propositions) in    Hofstadter, D. R., & Mitchell, M. (1994). An overview of
parallel (cf. Hummel & Holyoak, 1997). Whether the                 the Copycat project. In K. J. Holyoak & J. A. Barnden
human mind makes a strong distinction between features             (Eds.), Advances in connectionist and neural
and propositions (like LISA), or permits "stacking" of             computation theory, Vol. 2: Analogical connections.
predicates (like Shastri & Ajjanagadde's model) is an              Norwood, NJ: Erlbaum.
empirical question.                                           Holyoak, K. J., & Hummel, J. E. (2000). The proper
                                                                   treatment of symbols in a connectionist architecture. In
The Origins of Object Features                                     E. Dietrich and A. Markman (Eds.).             Cognitive
                                                                   Dynamics: Conceptual Change in Humans and
     To this point our discussion of reflexive inference—          Machines. Hillsdale, NJ: Erlbaum.
based on learning connections to features activated by        Holyoak, K. J., & Thagard, P. (1989). Analogical mapping
feedback from structures in LTM—has begged a major                 by constraint satisfaction. Cognitive Science, 13, 295-
question: If objects learn the features that describe them by      355.
"comparing themselves to" other objects in LTM, then how      Hummel, J.E., & Holyoak, K. J. (1992).                Indirect
did the objects in LTM learn the features that describe            analogical mapping. Proceedings of the 14th Annual
themselves in the first place? Answering this question "by         Conference of the Cognitive Science Society, pp 516 -
comparing themselves to objects in LTM when they were              521.
encoded" is unsatisfying because it brings to mind the        Hummel, J. E., & Holyoak, K. J. (1997). Distributed
infinite regress, "and where did those objects learn their         representations of structure: A theory of analogical
features?" etc. Although we are far from being able to             access and mapping. Psychological Review, 104, 427-
provide a complete answer to this very difficult question,         466.
one aspect of the model that we have not yet discussed may    Kintsch, W. & van Dijk, T. A. (1978). Toward a model of
provide a partial answer. Specifically, we allow some              text comprehension and production. Psychological
semantic features that belong to predicate units to attach         Review, 85, 363-394.
themselves to object units during the course of reflexive     Medin, D. L., & Shoben, E. J. (1988). Context and
inference. (In the original LISA, predicate semantics were         structure in conceptual combination.           Cognitive
attached strictly to predicates and object semantics strictly      Psychology, 20, 158-190.
to objects [see Hummel & Holyoak, 1997]; this approach is     Newell, A., & Simon, H. A. (1976). Computer science as
a departure from that convention.) As a result, roles to           empirical      inquiry:      Symbols      and     search.
which an object is attached in many situations or schemas          Communications of the ACM, 19, 113-126.
in LTM (e.g., the role big, in the case of big (wooden-       Shastri, L., & Ajjanagadde, V. (1993). From simple
spoon)) can become attached directly to new instances of           associations to systematic reasoning: A connectionist
those objects as semantic features. In this way, an inference      representation of rules, variables and dynamic bindings.
like own (Mary, car) can become truly reflexive in the sense       Behavioral and Brain Sciences, 16, 417-494.
of the definition suggested here: If, in several examples in  Smith, E. E. & Osherson, D. N. (1984). Conceptual
LTM, the buyer of a product is also represented as the             Combination with prototype concepts, Cognitive
owner of that product (i.e., in a separate own (person,            Science, 8, 337-361.
object) proposition), then the semantics of the predicate own Smith, E. E., Osherson, D. N., Rips, L. J. & Keane, M.
will tend to become attached as semantic features of               (1988).     Combining       prototypes:    A    selective
subsequent objects bound to the buyer role of a sell-to            modification model, Cognitive Science, 12, 485-527.
relation. We have yet to work out fully the details of this   St. John, M. F. (1992). The Story Gestalt: A model of
proposal, but preliminary simulations have so far been very        knowledge-intensive processes in text comprehension.
promising.                                                         Cognitive Science, 16, 271-302.
                                                              St. John, M. F., & McClelland, J. L. (1990). Learning and
                        References                                 applying     contextual     constraints    in   sentence
                                                                   comprehension. Artificial Intelligence, 46, 217-257.
Byrne, R. M. J., & Johnson-Laird, P. N. (1989). Spatial
     reasoning. Journal of Memory and Language, 28, 564-      Acknowledgments
     575.                                                     This research was supported by NSF Grant SBR-9729023,
DeSoto, C., London, M., & Handel, S. (1965). Social           and by grants from the UCLA Academic Senate and HRL
     reasoning and spatial paralogic. Journal of Personality  Laboratories.
     and Social Psychology, 2, 513-521.
Forbus, K. D., Gentner, D., & Law, K. (1995).
     MAC/FAC: A model of similarity-based retrieval.
     Cognitive Science, 19, 141-205.

