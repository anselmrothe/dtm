UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Harmonia loosely praestabilitia: discovering adequate inductive strategies
Permalink
https://escholarship.org/uc/item/6nb9d3wq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Bensusan, Hilan
Giraud-Carrier, Chrsitophe
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                     University of California

        Harmonia             loosely       praestabilita         : discovering adequate inductive
                                                          strategies
                                                          Hilan Bensusan
                                                     Christophe Giraud-Carrier
                                    Department of Computer Science, University of Bristol,
                                                       Bristol BS8 1UB, UK
                                                      hilanb,cgc@cs.bris.ac.uk
                           Abstract                                    This paper explores some of the general consequences
   Landmarking is a novel approach to inductive model               of a new way of doing meta-learning, called landmarking.
   selection in Machine Learning. It uses simple, bare-             The technique has been introduced recently (Bensusan
   bone inductive strategies to describe tasks and induce           & Giraud-Carrier 2000; Pfahringer, Bensusan & Giraud-
   correlations between tasks and strategies. The paper             Carrier 2000) and some new results are reported here.
   presents the technique and reports experiments showing           Landmarking searches for correlations between tasks and
   that landmarking performs well in a number of di erent           inductive strategies by exploring the similarities between
   scenarios. It also discusses the implications of landmark-
   ing to our understanding of inductive re nement.                 di erent strategies in order to locate the task in a map
                                                                    of areas of expertise. The discovery of similarities be-
                       Introduction                                 tween strategies can prove to be a tool to re ne inductive
                                                                    strategies and, ultimately, a way to sketch an explana-
One of the central goals of cognitive science is to un-             tion of human inductive success.
cover mechanisms that allow agents to produce and man-                 This paper is organised as follows. Next section intro-
age knowledge. Although informed by existing theories               duces landmarking. The following section presents ex-
of living organisms, the chief contribution of arti cial            periments that assess its performance. Then we consider
intelligence, is to investigate knowledge mechanisms in             some of its implication for the general study of induction
abstract, that is, independently of their psychological or          and cognition. A last section concludes the paper.
neurological plausibility. Machine learning endeavours
to study induction, one of the basis of knowledge pro-                  Meta-learning through landmarking
duction. It considers di erent inductive strategies, their
performance in di erent scenarios.                                  Meta-learning is the endeavour to automatically discover
   Not surprisingly, di erent inductive strategies are ad-          correlations between tasks and inductive strategies. To
equate for di erent inductive tasks. Theoretical results            simplify without loss of generality, let's concentrate on
show that there is no inductive strategy that can per-              supervised learning tasks.1 These tasks are composed
form well in every conceivable task (Scha er, 1994).                by a set of examples described by attribute values and
Some practitioners of machine learning reacted to this              classi ed according to a target function. The induction
predicament by insisting that not every conceivable in-             of the di erence in extension of the predicates \lemon"
ductive tasks is equally deserving of attention. If we              and \watermelon", for example, may include attributes
concentrate on a subset of all conceivable tasks, some              such as colour, shape, size. Something yellow,
people claim that we should restrict ourselves to \real             egg-shaped, small quali es as lemon whereas some-
world problems", we can nd a small number of strate-                thing green, round, big is a watermelon. If the at-
gies that can handle induction (Rao, Gordon & Spears                tributes that describe the example are not well-chosen,
1995). The problem arises when we try to give a pre-                learning could be very dicult. Consider, as an exam-
cise de nition for the set of \real world problems". In             ple, the following worse set of attributes for the \lemon-
any case, we face correlations between sets of tasks, or            watermelon" problem above: is it a vegetable?, is
problems, and induction strategies. Strategies perform              it a fruit?, does it fly?. The two examples are now
well only in a subset of the set of all tasks, this sub-            described as no, no, yes. The importance of the ex-
set is often called the area of expertise of a strategy.            ample description derives from the fact that inductive
Machine learning is then left to discover, by induction,            strategies rely on representations to generalise. Success-
correlations between inductive strategies and their area            ful inductive hypotheses are the ones that can represent
of expertise. One way of doing this is by automating                accurately the similarities and the di erences relevant to
this search for correlations between tasks and strategies.          the task.2
This process is often called meta-learning and a number
of di erent approaches has been proposed (see Bensusan                 1
                                                                         Although there are di erent uses of the terms \induc-
(1998,1999), Giraud-Carrier & Hilario (1998), Giraud-               tion" and \learning", in this paper we shall use the terms as
Carrier & Pfahringer (1999), Lindner & Studer (1999)).              interchangeable.
                                                                       2
                                                                         Data representation is important because every learning
Meta-learning has a number of general consequences for              strategy has what machine learning calls a representational
the study of cognition.                                             bias, a preference for hypotheses with a speci c representa-

   Meta-learning tasks are inductive tasks. Here, the           the expertise map.
examples, instead of being lemons or watermelons, are
inductive tasks classi ed according to the best induc-
tive strategy to tackle them. Thus, we have: task1 ->
Naive Bayes, task2 -> Backpropagation, task3
-> Nearest Neighbor etc where each of the inductive
strategy mentioned after the arrow is the best strategy                               i5
for the task before the arrow.3 The meta-learning task
is to use these examples to learn how to classify new
tasks in terms of the most suitable inductive strategy.                     i1
The crucial question for meta-learning is therefore how                                                  i2      i7
to describe tasks.
   Di erent approaches to task description have been                                             i6
proposed. These include the use of statistical features                        i4
of the dataset in the task (Michie et al. 1994) and the
use of features of a decision tree representation of the
task (Bensusan 1998; Bensusan 1999). In the latter, an                                       i3
inductive hypothesis, namely the one produced by a deci-
sion tree induction method, is used to describe the task.
Landmarking also makes use of speci c inductive meth-
ods to describe the task, but makes use of the method's
performance rather than the method's induced hypoth-
esis.                                                                 Figure 1: Example of map of areas of expertise
   The basic idea of the landmarking approach is that the          Landmarking relies on some simple and ecient in-
performance of an inductive strategy on a task uncovers         ductive strategies to signpost the location of a task in a
information about the nature of the task. Tasks are de-         map of expertise areas. Landmarking discovers experi-
scribed by a set of attributes corresponding to the per-        mentally which inductive strategies are similar enough
formance of simple, ecient strategies on them. These           to have neighbouring areas of expertise. It therefore
strategies are expected to indicate which other, more re-         nds neighbourhoods of inductive strategies and, ulti-
  ned strategy is the best to tackle the task. They act,        mately, draws a map of areas of expertise. While other
therefore, as landmarkers, indicating where, in the space       approaches represent tasks in a way only indirectly re-
of all areas of expertise, the task belongs. It explores        lated to their location in an expertise map, landmarking
empirically the relationships between areas of expertise        faces them as points in the map to be described in terms
of di erent learners.                                           of their distance to some known milestones.
   The kind of inference on which landmarking relies can           Landmarking is a tool to discover the areas of expertise
be illustrated with the help of Figure 1. The rectan-           of a learning device. In fact, this is the very goal of
gle represents a set of inductive tasks and the ellipses        machine learning research: to establish the strength and
represent subsets of the set of tasks where a given induc-      the scope of di erent inductive strategies. In addition,
tive strategy performs well, that is, areas of expertise.       it highlights which tasks fail to belong to the area of
Assume that i1, i2, and i3 are taken as landmarkers.            expertise of any of the existing inductive strategies. It
In this case, landmarking concludes that problems on            can therefore direct the search of new strategies towards
which both i1 and i3 perform well, but on which i2 per-         areas of the expertise map not currently covered by any
forms poorly, are likely to be in i4's area of expertise        learning method. If successful, it can guide the crafting
etc. Of course, the proximity of the areas of expertise         of new inductive methods and work as the basis for a
of two strategies indicates some similarity between the         model of inductive re nement. Let's turn now to some
inductive mechanisms behind them. For landmarking               experiments designed to measure its success.
purposes, however, it is sucient to concentrate on so-to-
speak cartographic considerations. Tasks are described
by how some landmarkers fare on them. Exploring the                     Experiments with landmarking
meta-learning potential of landmarking amounts to in-           A number of experiments to test landmarking are re-
vestigating how well a landmark learner's performance           ported in (Bensusan & Giraud-Carrier 2000). The re-
hints at the location of the respective learning tasks in       sults show that landmarking successfully meta-learns in
                                                                a number of di erent scenarios. Successful results mean
tion (Haussler, 1989; Russell & Grossof 1990). Thus, most       that selection of inductive strategies can be done through
Decision Tree induction algorithms prefer simpler decision      the information contained in the performance of some
trees, most rule induction algorithms prefer simpler rules.     landmark inductive strategies. In this section, we sum-
There is a trade-o between the need for good input represen-    marise some of these results and present new ones.
tation and the strength of the strategy's preference (Craven       Experiments on landmarking can be done only
& Shavlik, 1995).                                               through selecting a set of landmarkers. The landmark-
   3
     For a survey of the most used inductive strategies includ-
ing all those to be mentioned in this paper consult Mitchell    ers change according to what we call the learners pool,
(1997).                                                         i.e., the set of target learners from which one must be

 selected. It remains to be investigated how close can we          The performance of the di erent landmarkers are
 get from a universal set of landmarkers that can be used       given by the average performance on all the existing ex-
 in any learners pool. In the experiments reported here,        amples of the induction problem, the so-called instance
 we used the following set of landmark learners. For each,      space of the induction made from 10 di erent subset of
 we include the motivation for its inclusion in the set.        examples (training sets) of equal size. This approach,
                                                                although di erent from the standard practice of cross-
1. Decision node: A single decision node is chosen              validation where the sets of examples are drawn with-
    according to C5.0's information gain-ratio (Quinlan         out replacement, is an ecient way to estimate how the
    1993, Mitchell 1997). The node is then used to classify     landmark learners perform in each task. Therefore, in-
    test examples. This landmark learner aims to estab-         ductive tasks are described by landmarker's performance
    lish closeness to linear separability.                      values. The task is then labelled by the learner with
                                                                greater average accuracy on the task, using a 10-fold
2. Randomly chosen node: A randomly chosen attribute            cross-validation approach. Each task can be labelled by
    is used to split the training set and classify the test     a learner's name or as \tie" when the di erence in per-
    examples. This landmark learner informs about irrel-        formance between the best and the worst learner is less
    evant attributes.                                           than 10%. A (meta-)dataset with 5 examples described
                                                                by 4 landmarkers looks as follows:
3. Worst node: Here the gain-ratio information criterion        0.42187,0.46875,0.46250,0.30781,Ripper
    is used to pick up the least informative attribute to       0.45312,0.42187,0.45000,0.26250,IB
    make the single split. This landmarker, together with       0.54687,0.56250,0.45937,0.29844,C5.0tree
    the rst one, is supposed to tell us something else          0.51562,0.59375,0.43750,0.28750,MLP
    about linear separability: if neither the best nor the      0.43750,0.51562,0.43125,0.27812,tie
    worst attribute produce a single well performing sep-
    aration, it is likely that linear separation is not an      Given the (meta-)dataset, the meta-learner aims at nd-
    adequate learning strategy.                                 ing correlations between the performances of the learners
                                                                in the pool and that of the landmarkers.
4. Naive Bayes: The training set is used to estimate the           In the rst experiment, we compared landmarking
    probabilities required to use the Bayes theorem to clas-    with an existing approach to task description for meta-
    sify test cases (Mitchell, 1997). This landmark learner     learning. This approach uses a number of information-
    intends to measure how conditionally independent the        theoretical properties of the data to describe the task
    attributes are, given the class.                            (Michie et al. 1994). We implemented this information-
                                                                theoretical approach by considered the following 6 fea-
5. 1-Nearest Neighbor: The test set is classi ed based          tures de ned on literature as meta-attributes: Entropy
    on the classi cation of the closest training example        of the class, Average entropy of the attributes, Mu-
    (Mitchell 1997). This landmark learner measures how         tual information, Joint entropy, Equivalent number of
    close instances that belong to the same class are.          attributes, Signal-to-noise ratio. The task was to se-
                                                                lect among the following 10 learning algorithms: C5.0,
6. Elite 1-Nearest Neighbor: This computes 1-Nearest            C5.0 with boosting, C5.0 rules, Multi-layer percep-
    Neighbor on a subset of all attributes. This elite sub-     tron trained with backpropagation (MLP), Radial-based
    set is composed by the most informative attributes if       function networks (RBF), Linear discriminant, Ltree
    the gain-ratio di erence between them is smaller than       (see Gama, 1999), Naive Bayes (NB), Instance-Based in-
    0.14. Otherwise, the elite subset is a singleton and the    ducer (IB) and Ripper. Landmarkers 1,2,3,4,5,6,8 were
    learner acts like a decision node learner. This land-       used. 320 Boolean tasks were considered. The 10 learn-
    mark learner intends to establish whether the task is       ing algorithms in the learner pool were also used for
    a relational one, that is, if it involves parity-like rela- meta-learning in all experiments. Error rates were based
    tionships between the attributes (Clark & Thornton,         on strati ed 10-fold cross-validation. Results are given
    1997). In relational tasks, no single attribute is con-     in Table 1. The rst line reports the error rate of the
    siderably more informative than others.                     default class that, in this case, was \tie".
                                                                   The table shows that landmarking outperforms the
7. Majority-class guesser: The test set is classi ed ac-        information-based task description and therefore it is a
    cording to the most common class in the training set.       suitable competitor. Notice that landmarking outper-
    This landmark learner intends to inform about the fre-      forms the information-based approach with all of the
    quency of the majority class.                               10 meta-learners. Moreover, the di erence in error is
                                                                around 10% with the three C5.0 meta-learners. The ta-
8. Linear Discriminant: A linear approximation of the           ble also shows that adding the information-based fea-
    target function is sought (Gama, 1999). This land-          tures to describe the task impairs landmarking perfor-
    mark learner intends to measure closeness to simple         mance.
    linear separation.                                             Next, we considered a number of learners pools with
                                                                two inductive strategies. Learners pools were composed
     4
       This threshold is based on the results reported in Ben-  by pairs of the following inductive strategies: C5.0(with
 susan (1999).                                                  boosting), C5.0(rules), Naive Bayes (NB), Instance-

                                                             gies (Ripper and C5.0rules) and decision tree strategies
Table 1: Comparison between di erent ways to describe        (C5.0, C5.0boost, Ltree). The error rates given in table
tasks: performances of the landmarking approach (L),         3 are the average 10-fold cross-validation error of the 10
the information-based approach (Info) and the combined       inductive strategies used for meta-learning.
approach (Combined) using 10 di erent meta-learners.
       Meta-learner Land Info Combined
       Default Class 0.460 0.460            0.460            Table 3: Suitability of inductive approaches. Error rates
       C5.0boost        0.248 0.360         0.295            for the default class prediction and for meta-learning
       C5.0rules        0.239 0.333         0.301            with landmarking are given.
       C5.0tree         0.242 0.342         0.314                    Approach Default class Landmarking
       MLP              0.301 0.317         0.320                    IB            0.420          0.297
       RBFN             0.289 0.323         0.304                    NB            0.380          0.298
       LD               0.335 0.311         0.301                    C5.0boost 0.510              0.449
       Ltree            0.270 0.317         0.286                    NN            0.440          0.386
       IB               0.329 0.366         0.342                    Rules         0.370          0.281
       NB               0.429 0.407         0.363                    Trees         0.470          0.390
       Ripper           0.292 0.314         0.295
       Average          0.298 0.339         0.312               These results show that most meta-learners produce
                                                             error levels smaller than the default error class and often
                                                             the di erence is substantial. Notice that error rate g-
Based induction (IB), Ripper and Multi-layer percep-         ures don't re ect the overall performance, that is the ac-
tron (MLP). Landmarkers 1,2,4,5,6,7,8 were used. Tasks       curacy of the selected learning model. In another experi-
were classi ed as a tie between the two strategies when      ment, we tried to estimate this by using the 222 Boolean
the average error di erence between the learners in the      problems as tasks of a meta-learning training set and 18
pool was less than 0.1. We used 927 arti cially gen-         other tasks to test the hypotheses and compare the se-
erated Boolean and monk-like datasets (Thrun et al,          lected approach with the best performing one. The 18
1991). Boolean instance spaces had between 5 and 12          tasks of the test set were from the standard repository of
attributes. The error rates given in table 2 are the aver-   benchmark induction problems maintained by the Uni-
age 10-fold cross-validation error of 5 inductive strategies versity of California at Irvine (UCI); these are commonly
used for meta-learning: IB, MLP, C5.0boost, Ripper and       considered to be \real world problems". We chose the
Radial Basis Function Network Induction (RBF).               following problems: mushrooms, abalone, crx, sat, acety-
                                                             lation, titanic, waveform, yeast, car, chess(king-rook-vs-
                                                             king), led7, led24, tic-tac-toe, monk1, monk2, monk3,
Table 2: Landmarking to choose between pairs of learn-       satimage, quisclas.
ers                                                             The results reported for this experiment are the av-
                Learner pool         Error                   erage error di erence between the best choice and the
                NB-IB                0.383                   selected choice in the 18 UCI problems. If the average
                NB-MLP               0.179                   is in fact better than the chosen model, we consider the
                NB-Ripper            0.181                   error di erence between the chosen model and the av-
                C5.0boost-MLP 0.246                          erage. Similarly if the meta-learner had chosen against
                C5.0boost-NB         0.359                   the model that in fact is better than the average of the
                                                             10 learners. Here we used only C4.5 (Quinlan, 1993) as
                C5.0rules-Ripper 0.204                       meta-learner. Average error di erence appear in table 4.
   In a di erent experiment, we looked at the suitabil-      Table 4: Average error di erence between best and cho-
ity of inductive strategies and groups of similar induc-     sen option in the 18 UCI datasets
tive strategies. We considered that a task is suitable
for a learner if it performs better than the average of                     Approach Error di erence
10 standard learners: C5.0, C5.0rules, C5.0boost, MLP,                      IB            0.0356
RBF, Linear Discriminant, Ltree, NB, IB and Ripper.                         NB            0.0165
For this experiment we used only landmarkers 1,2,3 and                      C5.0boost 0.0443
6 as they are all decision node based and are arguably                      NN            0.0314
enough to diagnose at least whether decision tree induc-                    Rules         0.0360
tion is a good way to approach the task. We used 222                        Trees         0.0211
tasks from the set used in the previous experiment and
the 10 standard learners mentioned above to perform the
meta-learning induction. We looked at the suitability of        The small average error di erence shows that the cho-
IB, NB, C5.0boost, neural network inductive strategies       sen strategy, even when is not the best, performs well. It
(MLP and RBF) in general (NN), rule induction strate-        shows that landmarking seldom make choices that per-

form considerably worse than the best alternative. This    theory building, to perform successful inductions from
is con rmed further by an experiment in the same sce-      one or few examples.
nario. Now we used only the 14 UCI tasks listed above         Human inductive trajectory from innate instincts to
as training set and tested the C4.5 hypothesis in the re-  re ned theories about the world is Quine's view of the
maining four UCI tasks (monk2, monk3, satimage, quis-      problem of induction: a problem about the world. A
clas). The results obtained have a greater variation than  plausible partial complement to Darwin's natural selec-
the previous one but shows that in some cases landmark-    tion is to nd a model of exploiting previous induc-
ing perform completely accurately. Table 5 summarises      tion experience to boost performance. Such model, of
the new results.                                           course, has to accommodate the partial explanation role
                                                           that natural selection plays. The inductive trajectory
                                                           towards greater eciency in familiar environments had
Table 5: Average error di erence between best and cho-     its origins in evolutionary selection of relevant induc-
sen option in 4 UCI tasks after training on 14 UCI tasks   tive mechanisms. Recent there have been attempts to
only                                                       characterise human innate inductive tendencies in terms
                Approach Error di erence                   of learning biases (Elman et al., 1996; Dessalles, 1998).
                IB           0.0675                        Leaving aside the question of how our inductive prac-
                NB           0.0605                        tices are guided by our innate instincts, we can sketch
                C5.0boost 0.0000                           a model of the human inductive trajectory according to
                NN           0.0000                        which our similarity standards by means of which we
                Rules        0.0443                        generalise are partly product of evolution, partly a con-
                Trees        0.0172                        sequence of a gradual process of re nement. We claim
                                                           that landmarking can be part of an account of inductive
                                                           re nement.
   These results, although still preliminary, show that       Landmarking is a technique to select the most ade-
landmarking is capable to select inductive approaches.     quate inductive strategy for a task, but it can also be
They suggest that it pays o to run bare-bone, landmark     seen as an instrument for inductive re nement. It sug-
inductive strategies on a number of tasks and learn how    gests ways in which better, increasingly appropriate in-
their performance relates to that of other, more eshed-    ductive strategies, can be constructed from rudimentary
out strategies. This far, we have indicated how the per-   ones. Landmarkers are simple inductive strategies that
formance of simple inducers in a task can be used for      can characterise tasks. Thus, they can outline new in-
meta-learning. We move now to the signi cance of land-     ductive strategies to adequately cover areas of the exper-
marking for a general theory of induction.                 tise map; describing the area in terms of how di erent
                                                           learning biases fare there is a step towards construct-
         Discovering inductive strategies                  ing more re ned biases that can tackle it. As a way
                                                           to describe tasks, landmarking has far-reaching conse-
   For me [...] the problem of induction is a problem      quences beyond strategy selection: to landmark a group
   about the world: a problem of how we, as we are now     of tasks could be the rst step towards the development
   [...], in a world we never made, should stand better    of an inductive strategy to tackle it. This is arguably
   than random or coin-tossing chances of coming out       what happens when a scientist applies various simple
   right when we predict by inductions that are based      methods to a problem in order to get information about
   on our innate [...] similarity standard. Darwin's       what more sophisticated method to develop. This could
   natural selection is a plausible partial explanation.   also be what happens when new problems had to be
   W. V. O. Quine                                          addressed by humans with only few, unre ned inductive
                                                           tools. Landmarking is a way to discover relationships be-
   One of the problems of explaining human (and ani-       tween di erent strategies and, as such, to establish what
mal) cognitive practices in general and inductive prac-    is needed to ease learning. In this sense, it not only bears
tices in particular is to account for success. Humans are  similarities with other methods that exploit the nature
remarkably good at inducing in familiar environments       of the task to decide which way to go (Clark & Thorn-
and seem to make heavy use of their background knowl-      ton, 1997) but also can be seen as a general framework
edge accumulated through inductions made in their life-    for those methods as it describes tasks only in terms of
time history or received as cultural material. Studies     a portfolio of learning performances. The emerging pic-
on human induction on tasks similar to the monk prob-      ture is one where the records of failure and success of
lems have established that prior knowledge in uences the   the current induction tools are used to inform how these
rate of concept learning, and the logical form of concepts tools need re nement. Successful learning, landmarking
formed during learning is a function of the logical form   suggests, might require learning with previous mistakes
of the concepts previously acquired (Pazzani, 1991). In    and accomplishments.
general, humans rely on previous acquisition of concepts
and common-sense knowledge about the area to learn
new concepts (Wisniewski & Medin, 1994; Heit, 1994).
                                                                                 Conclusions
                                                                  War nicht das Auge sonnenhaft, die Sonne koennt'es
Background knowledge and the ability to meta-learn en-        nie erblicken. Goethe, Zahme Xenien, Werke, Weimar
able humans, when for instance engaged with scienti c         1887-1918, bk 3, 1805.

   Landmarking is a strategy to describe tasks so that no    Technische Universitat Chemnitz, Chemnitz, Ger-
more than a small class of ecient learning algorithms       many.
is required. Tasks are described by their position in the  Giraud-Carrier, C. & Pfahringer, B., ed. (1999). Pro-
expertise map. It can also be used to locate and explore     ceedings of the ICML'99 Workshop on Recent Ad-
expertise terra incognita. It can be seen as part of a       vances in Meta-Learning and Future Work, J. Stefan
model of inductive re nement whereby the description         Institute, Ljubljana, Slovenia.
of a task in terms of landmarkers o ers the raw mate-      Haussler, D. (1989). Quantifying Inductive bias: AI
rial for the development of new induction tools. The         learning algorithms and Valiant's learning framework.
picture o ered by this model is one in which human in-       Arti cial Intelligence, 32, 2, (pp. 177-222).
ductive abilities are roughly tuned to their environment;
no survival and no re nement could start from a com-       Heit, E. (1994). Models of the E ects of Prior Knowl-
pletely alien inductive toolkit. Evolution gives part of     edge on Category Learning. Journal of Experimental
the explanation. But the gradual re nement that sharp-       Psychology: Learning, Memory and Cognition. 20, 6,
ens the kit and assembles new instruments is what turns      (pp. 1264-1282).
the original harmonia loosely praestabilita into an induc- Lindner, G. and Studer, R. (1999). AST: Support for
tively adapted species.                                      Algorithm Selection with a CBR Approach. Proceed-
                                                             ings of the 3rd European Conference on Principles and
                 Acknowledgements                            Practice of Knowledge Discovery in Databases. (pp.
This work is part of the METAL project supported by          418-423). Berlin: Springer.
an ESPRIT Framework IV LTR Grant (Nr 26.257). We           Michie, D., Spiegelhalter, J. & Taylor, C. C. (1994). Ma-
wish to thank the members of the Consortium for useful       chine Learning, Neural and Statistical Classi cation.
comments and discussion.                                     London: Ellis Horwood.
Bensusan, H. (1988). God doesn't always shave with         Mitchell, T. (1997). Machine Learning. New York, NY,
   Occam's Razor { learning when and how to prune.           USA: McGraw Hill.
   Proceedings of the 10th European Conference on Ma-      Pazzani, M. (1991). In uence of prior knowledge on con-
   chine Learning (pp. 119-124). Berlin: Springer.           cept acquisition: Experimental and computational re-
Bensusan, H. (1999). Automatic bias learning: an in-         sults. Journal of Experimental Psychology: Learning,
   quiry into the inductive basis of induction. PhD The-     Memory and Cognition. 17, 3, (pp. 416-432).
   sis School of Cognitive and Computing Sciences, Uni-    Phahringer, B., Bensusan, H. & Giraud-Carrier, C.
   versity of Sussex, UK.                                    (2000). Meta-learning by landmarking various learn-
Bensusan, H. & Giraud-Carrier. (2000) Discovering            ing algorithms. To appear in: Proceedings of the Sev-
   task neighbourhoods through landmark learning per-        enteenth International Conference on Machine Learn-
   formances. Submitted                                      ing, ICML'2000.
Clark, A., & Thornton, C. (1997). Trading spaces: com-     Quinlan, J. R. (1993) C4.5: Programs for Machine
   putation, representation and the limits of uninformed     Learning San Mateo, CA, USA: Morgan Kaufmann.
   learning. Behaviour and Brain Sciences, 20, 1, (pp.     Rao, R.B. and Gordon, D. and Spears, W. (1995).
   57-90).                                                   For every generalization action, is there really an
Craven, M. & Shavlik, J. (1995). Investigating the Value     equal and opposite reaction? Proceedings of the
   of a Good Input Representation. in: Computational         Twelfth International Conference on Machine Learn-
   Learning Theory and Natural Learning Systems, ed:         ing, ICML'95 San Mateo, CA, USA: Morgan Kauf-
   Petsche, T. and Judd, S. and Hanson, S., Cambridge,       mann.
   MA, USA: MIT Press.                                     Russell, S. J. & Grossof, B. (1990). Declarative bias: an
Dessales, J-L. (1998). Characterising Innateness in arti-    overview. in: Benjamim, P. (ed.) Change of represen-
     cial and natural learning. Proceedings of the Work-     tation and inductive bias. (pp. 267-308). Dordrecht,
   shop on Human and Machine Learning, 10th European         Netherlands: Kluwer.
   Conference on Machine Learning, Technische Univer-      Scha er, C. (1994). A Conservation Law for Generaliza-
   sitat Chemnitz, Chemnitz, Germany.                       tion Performance. Proceedings of the Eleventh Con-
Elman, J. L., Bates, E. A., Johnson, M. H., Karmilo -        ference on Machine Learning, ICML'94. San Mateo,
   Smith, A., Parisi, D. & Plunkett, K. (1996) Rethinking    CA, USA: Morgan Kaufmann.
   Innateness. Cambridge, MA, USA: MIT Press.              Thrun, S. et alii (1991). The MONK's problems -
                                                             a performance comparison of di erent learning algo-
Gama, J. (1999). Discriminant trees. Proceeding of the       rithms. Technical report CMU-CS-91-197. School of
   Sixteeth International Conference on Machine Learn-       Computer Science, Carnegie-Mellon University, Pitts-
   ing, (pp. 134-142), San Mateo, CA: Morgan Kauf-           burgh, PA, USA.
   mann.                                                   Wisniewski, E, J. & Medin, D. L. (1994). On the interac-
Giraud-Carrier, C. & Hilario, M., ed. (1998). ECML'98        tion of theory and data in concept learning. Cognitive
   Workshop Notes - Upgrading Learning to the Meta-          Science, 18, (pp. 221-281).
   Level: Model Selection and Data Transformation,

