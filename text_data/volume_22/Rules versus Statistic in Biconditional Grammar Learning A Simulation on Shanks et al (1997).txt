UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Rules versus Statistic in Biconditional Grammar Learning: A Simulation on Shanks et al.
(1997)
Permalink
https://escholarship.org/uc/item/8cj217zb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Timmermans, Bert
Cleeremans, Axel
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                      Rules versus Statistics in Biconditional Grammar Learning:
                                      A Simulation based on Shanks et al. (1997)
           Bert Timmermans (Bert.Timmermans@vub.ac.be)                 Axel Cleeremans (Axel.Cleeremans@ulb.ac.be)
             Dienst Persoonlijkheids- en Sociale Psychologie           Séminaire de Recherche en Sciences Cognitives
                           Vrije Universiteit Brussel                            Université Libre de Bruxelles,
                                   Pleinlaan 2                              Avenue F.-D. Roosevelt, 50 – CP 122
                           1050 Brussels – Belgium                                 1050 Brussels – Belgium
                               Abstract                             its "function within a causal system"). Therefore, an entity
                                                                    that is an object of representation has to exist independently
  A significant part of everyday learning occurs incidentally —     from the "hardware" of the system by which it is
  a process typically described as implicit learning. A central     represented, making it available for information-processing
  issue in this and germane domains such as language
                                                                    operations in a variety of contexts (Cleeremans, 1997) —
  acquisition is the extent to which performance depends on the
  acquisition and deployment of abstract rules. In an attempt to    such as a rule that is applicable to different instances of a
  address this question, we show that the apparent use of such      certain problem.
  rules in a simple categorisation task of artificial grammar          Inherent to this issue is the question of whether the
  strings, as reported by Shanks, Johnstone, and Staggs (1997),     mechanisms through which implicit and explicit knowledge
  can be simulated by means of a simple recurrent network, and      are acquired are best viewed as being subtended by separate
  may thus turn out not be incompatible with the acquisition of     processing systems. This is exactly what has been suggested
  statistical regularities rooted in the processing of exemplars of by Shanks and colleagues (Shanks and St John, 1994;
  the presented material.                                           Shanks, Johnstone, and Staggs, 1997; St John and Shanks,
                                                                    1997), who proposed to abandon the distinction between
                           Introduction                             Implicit and Explicit Learning in terms of conscious
Over development and learning, we acquire a considerable            awareness being present or not, and instead suggested that
amount of information incidentally. Natural language offers         the distinction is one of rule-based versus memory-based
perhaps the most striking example of such incidental                learning processes. Before going any deeper into this matter,
learning: Infants do not need to be explained grammar rules         let us consider two different ways of looking at learning in
in order to be able to communicate effectively and are              general, to illustrate how they can possibly account for
presumably unaware of the fact that they are learning               Implicit Learning.
something at all. Adult speakers likewise “know” whether
expressions of their native language are grammatically              Computational Modelling of IL
correct but can seldom explain why.                                 Two views come forth when considering the mind in
                                                                    general, and implicit learning in particular: the symbolic and
Implicit Learning                                                   the connectionist approach. Each has its own view on how
The notion of "implicit learning" (IL) usually designates           knowledge is represented and how it might be manipulated.
cases in which a person learns about the structure of a fairly      The symbolic metaphor is usually associated with rule-
complex stimulus environment, without necessarily                   based learning, while the connectionist metaphor is
intending to do so, and in such a way that the resulting            associated with memory-based learning based on the
knowledge is difficult to express (Berry and Dienes, 1993).         statistical characteristics of the stimuli.
In short, IL is the ability to learn without awareness
(Cleeremans, Destrebecqz, and Boyer, 1998), as opposed to           The Symbolic Metaphor. Cleeremans & Jiménez
explicit learning, which is strategy- and/or hypothesis-            (submitted) point out that a symbol system leaves no room
driven, and of which one tends to be consciously aware.             for a concept like IL. In a symbol system, expressions that
  IL can produce implicit knowledge. According to                   are formed are static representations of (real-world) entities
Cleeremans (1997), "at a given time, knowledge is implicit          or relations, stored in the system's memory. These symbols,
when it can influence processing without possessing in and          be it of objects or of rules, have to be interpreted by
of itself the properties that would enable it to be an object of    something — a processor — when they are to be used by
representation, and implicit learning is the process by which       the system to augment its knowledge base (memory), that is,
we acquire such knowledge." (p.199) As for the notion of            to learn. From this perspective, IL can only exist if one
"representation", we agree with Perruchet and Vinter                assumes the existence of a cognitive unconscious, i.e. a
(submitted), who state that a representation has to represent       subset of the mind that can basically process all the
an entity in the real world and has to be in and of itself          information that the conscious system can process, only
manipulable as that entity (Perruchet and Vinter talk about         minus consciousness. Consequently, consciousness is purely

epiphenomenal in this framework. It is exactly the fact that    Experimental Research on IL
all symbols have in and of themselves the property of being     Recently, some of the processes involved in word
an accessible representation, independent of the processor,     segmentation have been described as rooted in the same
which makes them unsuitable as a metaphor for implicit          mechanisms as implicit learning and frequency estimation.
knowledge. For it is impossible to conceive of any              For instance, Saffran et al. (1997) conducted an experiment
knowledge that could influence processing while remaining       on word segmentation in artificial speech. They exposed
unavailable to outside inspection. Importantly, this            children (6-7 years old) and adult subjects to a continuous
perspective also makes it possible to assume the existence of   speech flow such as bupadapatubitutibudutabapidabu.
abstract knowledge that remains inaccessible to conscious       Subjects were told that the experiment was about the
inspection.                                                     influence of auditory stimuli on creativity (to make sure
                                                                learning was incidental and not intentional). The only cues
The Connectionist Metaphor. By constrast, in a                  to word boundaries were the transitional probabilities
connectionist network, there is no external processor           between pairs of syllables (e.g., bu-pa), which were higher
engaged in learning, that is, learning does not consist of      within words than between words. Afterwards, subjects
augmenting a distinct knowledge base. Instead, learning in a    heard two sets of sounds, each consisting of three syllable
connectionist network is the result of changes that occur in    pairs, and were told to decide which one sounded more like
the network (weight-change between units). These changes        the tape they had heard. Both adult and child subjects
are themselves caused by information processing, i.e. the       managed to perform well above chance, suggesting that
coupling of a certain input with a desired output Thus, this    learning might proceed in the absence of attention and the
processing also changes the process of learning (through for    intention to do so, even despite the brevity of the exposure
example back-propagation of the error between the actual        (one ore two times a 21' tape). The fact that children did as
and the desired output). Furthermore, as transient              well as adults suggests a robust phenomenon that might play
knowledge in a connectionist network consists of activation     a role in natural language acquisition.
patterns, instead of symbols, a piece of knowledge does not        In another interesting artificial language experiment,
have to be "interpreted" by the central processor before it     Marcus et al. (1999) claim to have showed that 7-month-old
can influence processing. These properties make it possible     infants can "represent, extract, and generalise abstract
for a connectionist network to possess knowledge that can       algebraic rules." In short, the infants were exposed to
influence behaviour despite failing to be represented as        artificial "sentences" during a training phase, and
such. It makes it possible to consider implicitness as          subsequently were presented with a few test items, some of
something more than simply a property of the database or a      them belonging to the same language, while others
property of the processor.                                      introduced some structural novelty. For example, when an
   From the connectionist point of view, subjects are said to   infant had been habituated to gatiti or linana (both having
base their judgements on the basis of exemplar information,     an ABB structure), it was subsequently presented with test
without explicitly extracting abstract generalities, or rules – sentences such as wofefe or wofewo (the last one being of
the abstract processing is performed online during the test,    ABA structure). The basic set-up is similar to the Saffran et
when necessary. The episodic account provides a refined         al. (1997) experiment, with the important difference that
version of mere instance-based processing (e.g. Neal &          there where the Saffran et al. test items were composed of
Hesketh, 1997). One of the most popular instances of            the same material as the training items, Marcus et al.
traiditional connectionist networks is the Simple Recurrent     introduced a change in the sensory content of the material.
Network (SRN), as proposed by Elman (1990). Here,               That is, prior to hearing the above illustrated test items, the
judgements are no longer based on instances, but on             infants had never heard /wo/, or /fe/. Still, infants tended to
instances within their context. Learning is nothing more than   listen more to the sentences containing a structural novelty.
a byproduct of the processing itself (weight-change), while     As a result, since this task could not be performed on the
retrieval results from the overlap between processes            basis of mere transitional probabilities, Marcus et al.
operating during study- and test-phases. Several variations     concluded that infants had the capacity to represent
on this basic principle have been proposed, but the main        algebraic rules. However, Marcus et al.'s claim that an SRN
point remains as stated: no abstract rules in implicit          could not model the observed effect was disputed by Elman
learning. Instead, more fragmentary knowledge is used to        (Seidenberg & Elman, 1999; Elman, 1999) and McClelland
gradually and dynamically build up representations op the       and Plaut (1999), basically on the account that an overlap
stimulus environment. This leaves room for implicitness,        need not be present in the "raw input" itself. Instead "the
not in the way of equating the existence of representations     relevant overlap of representations required for
with accesibility to consciousness (as do for example           generalisation […] can arise over internal representations
Perruchet and Vinter, submitted; O'Brien and Opie, 1999),       that are subject to learning." (McClelland & Plaut, 1999,
but in virtue of the dynamical aspects of representation        p.2) Transfer and generalization remain precarious issues,
building. For example, it might be possible to conceive of      however, when it comes to computational modelling in a
conscious representations as being structured differently       connectionist network. An experiment by Shanks et al.
than unconscious ones, or as being of lower quality.            (1997) clearly illustrates this point.

Biconditional AGL: Shanks et al. (1997)                             grammatical structure, and that, subsequently, the
                                                                    distinction made between grammatical and nongrammatical
As mentioned before, Shanks and St John (1994) proposed             strings cannot be simulated by a connectionist network
to abandon the idea of the conscious/unconscious dichotomy          making use of simple frequency statistics. The goal of this
in favour of a rule-based/instance-based dichotomy. The             paper is to demonstrate that in fact no such abstract rules are
basic idea is that humans possess two learning systems              necessary and that, at least under some conditions,
capable of creating distinct forms of mental representation,        biconditional grammar learning can be accomplished by a
one system consisting of symbolic rule-abstraction                  network developing representations based on frequency
mechanisms and the other involving subsymbolic, memory-             statistics.
based, connectionist mechanisms (see Shanks, 1998, for a
discussion). In this context, Shanks et al. considered transfer                  A Simulation of Shanks et al.
in AGL tasks to be at least to some extent mediated by
abstract (rule-) knowledge and claimed that people                  Simulation Parameters and Procedure
systematically become aware of the relevant regularities in
AGL tasks where only rule learning is possible. To                  The Simple Recurrent Network (SRN) is a connectionist
demonstrate, Shanks et al. exposed subjects to artificial           network especially designed to predict the next step in a
grammar strings generated by a biconditional grammar (see           sequence. Its design allows it to "keep in memory" the
also Mathews et al., 1989). Biconditional grammars involve          earlier steps in that sequence, by using what preceded as a
cross-dependency recursion (see Christiansen & Chater,              context. This context is a copy of the learning-state at time t-
1999) such that letters that appear at each position before         1, which is fed back into the network at time t, together with
and after a central dot depend on each other. An example is         the new input. In this way, the network is able to integrate
given in Figure 1, where letter D is paired with F, G with L,       the new input with what it has already learned in earlier
and so on.                                                          stages, and will predict on this basis the sequence step at
                                                                    t+1. A typical example of an SRN is given in Figure 2.
                   DFGK.FDLX
                                                                                                             next
                                                                                          copy           hidden units
Figure 1. A biconditional grammar string as used by Shanks et al.
(1997). Possible letters in each position before the dot are linked
biconditionally with the letters that may appear after the dot.                    context units            current
Shanks et al. constructed biconditional grammar training
strings as well as a set of grammatical and nongrammatical          Figure 2. The Simple Recurrent Network as conceptualised by
and test strings, in such a way that grammatical and                Elman (1990).
nongrammatical test items could not be distinguished on the
basis of their overlap with the training strings in terms of        Importantly, on each time step the context units contain a
bigrams or trigrams (or any other n-gram). During training,         copy of the patterns of activation that existed over the
two groups of subjects were shown strings one a time on a           hidden units at t-1. As described in Servan-Schreiber,
computer screen and had to perform one of two tasks on              Cleeremans and McClelland (1988, 1989; see Cleeremans,
each trial. One group (the match group) had been told that          1993), learning progresses in a continuous fashion through
the task was about memory, and had to select the correct            three stages, during which more and more temporal
string among five strings presented on screen. The other            contingency information is incorporated in the context, and
group (the edit group) was told that the strings had been           hence in the hidden unit representations. The statistical
constructed according to rules and that their task was to find      regularities the SRN uses to predict the next letter are thus
them. On each trial, edit subjects had to indicate which            gradually "stored" in the hidden unit representations of the
letters they thought violated or confirmed to the rules, and        network. As a consequence, the network becomes able to
were subsequently given feedback. All subjects then                 behave in a rule-like manner and to predict the next element
performed a classification test in which they were asked to         in the sequence as if it knew the grammar rules.
decide which strings were grammatical or not. Shanks et al.
showed a dissociation between the two groups: While the             Network Architecture and Parameters. The SRN had 9
edit group performed well and most subjects extracted the           input and output units, necessary for representing the
rules, the match group performed at a chance level, thus            information that was available to the subjects in the Shanks
suggesting that "instance-memorisation and hypothesis-              et al. experiment. (The six letters of which the strings were
testing instructions recruit partially separate learning            composed, D, F, G, L, K and X, as well as the beginning
processes." (Shanks et al., 1997, p.243)                            and end of each string, together with the dot in between the
   The basic claim is thus that, in order to perform the            first and the last four letters of a string.) The number of
biconditional grammar task, it is necessary to conceive of          hidden units (and hence context units) was 100, which made
some abstract (symbolic) rule-like knowledge of the                 use of logistic adjustment. The learning algorithm was error
                                                                    backpropagation, with a learning rate of .15 and the context

being reset to zero after each complete string presentation.   p<.005. The figure also makes it clear that the main effect is
Weight adjustment was not applied on the connections from      not due to some initial biasing since initial performance is
context to hidden units (1 on 1 relation). Momentum was set    identical for the three types of strings (prior to training,
at .9.                                                         F(1,161)=1.13, ns; at 10 epochs, F(1,161)=.048, ns).
Training Material. The basic training material consisted of
                                                                     1,0
a set of 18 strings as designed by Shanks et al. (List 1).
Based on these strings, they created 18 grammatical and 18                          training
nongrammatical strings.                                              0,9
   The items were to meet four objectives: (1) Grammatical                          new/G
strings had to conform to the biconditional grammar: Letter          0,8            new/NG
position 1 is linked to 5, 2 to 6 and so on, with the linked
letters being D–F, G–L, and K–X. (2) The use of the 6
                                                                     0,7
letters was balanced, so that each letter appeared 3 times in
each of the 8 letter locations. (3) Each training string
differed from all other training strings by at least 4 letter        0,6
locations. (4) Each training item had a grammatical similar
item and an ungrammatical similar item that each differed            0,5
from the training item by only 2 letter positions. Each
training item was different from all other test items by at
                                                                     0,4
least 3 letter locations. The basic simulation was carried out
on exactly these strings. A training epoch consisted of all 18
strings being presented once to the network, in a random             0,3
fashion.
                                                                     0,2
Measurement of Accuracy. Different measurements of
accuracy exist, of which we used the Luce ratio (Luce,
1969) — a simple measure of relative strength in which the           0,1
activation of the target output unit is divided by the sum of
the activations of all output units. To assess network               0,0
performance, we considered the average Luce ratios for all                 zero      10        50        100     300 1000   3000
strings. In addition, we also considered the Luce ratio on a                                 # of training epochs
letter-by-letter basis for more detailed analyses.
                                                               Figure 3. Network learning, measured with the Luce ratio. Error
Simulation Results                                             bars are shown for novel G and nonG strings.
Learning. In order to assess learning, the network was         Based on these findings we can therefore conclude that
tested before and during training on seven occasions. On       contrary to what Shanks et al. claimed, the SRN can in fact
each test, the network was tested on the 18 grammatical        distinguish between grammatical and nongrammatical
training strings, the 18 new grammaticals, and 18              strings generated by a biconditional grammar without
nongrammaticals. Results were obtained over 9 simulations      making use of explicit rules. In order to rule out the
and averaged. As described before, the Luce ratio of the       possibility of the SRN merely having learned to predict the
output was computed for each element of each string.           dot and/or the end of a string, we computed the mean Luce
Subsequently, the ratios were compared over the two            ratios on a letter-by-letter basis, as presented below in Table
conditions of interest (grammatical test/nongrammatical        1. Shown are the important ratios, belonging to the letters
test) by means of an ANOVA, for each learning step.            after the dot (ratios for training strings had value 1). When
   As can be seen in Figure 3, the SRN was indeed able to      the difference exceeds .05, the highest ratio is in bold.
discriminate between grammatical and nongrammatical               Table 1 clearly shows the mean Luce ratios on a letter-by-
strings. Original training strings were learned almost         letter basis to be higher in grammatical than in
perfectly from 100 epochs onwards. Further, the network        nongrammatical strings. This indicates that the network has
clearly discriminates between novel grammatical and            learned something other than merely the dot or the end of
nongrammatical strings (i.e., better predictions for           the string.
grammatical strings), even before it is completely successful
in mastering the training strings. ANOVA measures are, at
50 epochs, F(1,161)=24.1, p<.001; at 100 epochs,
F(1,161)=36.3, p<.001; at 300 epochs, F(1,161)=33.5,           Table 1. Mean Luce ratios on a letter-by-letter basis, in each
p<.001. From 1000 epochs onwards, the network gets a little    position, after 3000 epochs, for grammatical and nongrammatical
'overtrained' on the original strings, causing it to do        test strings (included is the frequency of occurrence of the letter in
somewhat less well on the unseen strings; at 1000 epochs       each position).
F(1,161)=13.3, p<.001; at 3000 epochs F(1,161)=8.34,

GRAM                                                                                                                                                        accomplish the categorisation task. This paper clearly
                                  5th         #           6th            #                                         7th      #         8th               #   demonstrates that this is not the case, and that judgements of
               D                  .25       (2)           .89          (2)                                         .01    (4)         .69             (4)   grammaticality using biconditional grammars can be made
               F                  .70       (4)           .50          (4)                                         .55    (1)         .18             (3)   by extracting statistical features out of the material.
               G                  .71       (2)           .49          (2)                                         .26    (5)         .38             (3)      One of the major challenges in working with
               K                  .41       (2)           .72          (5)                                         .09    (3)         .99             (2)
                                                                                                                                                            connectionist networks is how to probe the hidden units in
               L                  .58       (4)           .99          (3)                                         .11    (3)         .68             (3)
               X                  .43       (4)           .56          (2)                                         .33    (2)         .01             (3)   order to "unfold" the complex representation of the stimulus
                                                                                                                                                            material. Cluster analysis or principal component analysis
NGRAM                                                                                                                                                       performed on the hidden unit activations are standard ways
                                  5th         #           6th            #                                         7th      #         8th               #   of doing so, but may not always provide insight into how
               D                  .19       (2)           .70          (2)                                         .72    (4)         .13             (4)   the representations enable the network to solve the task. The
               F                  .39       (4)           .87          (4)                                         .20    (1)         .75             (3)   fact that cluster analysis does not reveal a clear structure
               G                  .37       (2)           .37          (2)                                         .17    (5)         .01             (3)   does not necessarily imply that there is no structure. It may
               K                  .46       (2)           .32          (5)                                         .19    (3)         .33             (2)   simply mean that the representational aspect needed to
               L                  .74       (4)          1.00          (3)                                         .00    (3)         .50             (3)   accomplish the most important aspect of the task, is not the
               X                  .50       (4)           .38          (2)                                         .50    (2)         .00             (3)   most important aspect. Thus, clustering will not be carried
                                                                                                                                                            out on that aspect — which, importantly, does not
                                                                                                                                                            necessarily entail that the network is unable to use the
When the network fails to learn. In order to illustrate                                                                                                     relevant information successfully (see Cleeremans, 1993).
exactly when a network can learn, we include a simulation                                                                                                      Biconditional grammars are difficult to master because
of a situation in which it fails to learn. We created two                                                                                                   they require maintaining information accross intervening
(grammatical) strings with a high degree of similarity,                                                                                                     irrelevant items. Servan-Schreiber et al. (1991) explored the
FGGG.DLLL and KGGG.XLLL, and presented them to the                                                                                                          conditions under which the network can carry information
network in the same way as was done in the main                                                                                                             about distant sequential contingencies (e.g. 1–5) across
simulation. Figure 4 shows the activation values of the 9                                                                                                   intervening elements, to distant, to-be-predicted elements. It
output nodes for one string (the other showed the same                                                                                                      appeared that this information is retained as long as it is in
evolution in activation values), as well as the evolution of                                                                                                some way relevant to predicting each intervening item (the
the Luce ratios.                                                                                                                                            prediction-relevance criterion). When it is not, the relevant
                                                                                                                                                            information tends to be lost as training progresses, as a
                   1,0
                                                                 s
                                                                                                                   1,0
                                                                                                                                                            consequence of the way of the way in which representations
                                                                 D
                                                                             Luce ratio of D and X after the dot
                   0,9                                                                                             0,9
                   0,8
                                                                 F
                                                                 G                                                 0,8
                                                                                                                                                            of the temporal context are only gradually built up. Indeed,
                                                                 .
                   0,7                                           K                                                 0,7                                      for different predictions to be achieved at any point in a
activation value
                                                                 L
                   0,6
                                                                 X
                                                                 e
                                                                                                                   0,6                                      sequence, the network needs to have developed different
                   0,5                                                                                             0,5
                                                                                                                                                            internal representations of the sequence so far. When two
                                                                                                                   0,4
                   0,4
                                                                                                                   0,3
                                                                                                                                                            sequences are identical for a number of time steps so that
                   0,3
                                                                                                                   0,2                                      the relevant information for making different predictions has
                   0,2                                                                                                                    fggg.Dlll
                   0,1
                                                                                                                   0,1                    kggg.Xlll
                                                                                                                                                            to be retained over these intervening elements, each training
                   0,0
                                                                                                                   0,0
                                                                                                                           10      10 0
                                                                                                                                                            trial actually induces the development of increasingly
                                                                                                                     ro
                                                                                                                                                            similar internal representations of the two sequences
                                                                                                                                  30
                         0   10    50      100    300     1000       3000                                           ze    50     10
                                                                                                                                30
                                                                                                                                     0
                                                                                                                                    00
                                                                                                                                   00
                                  # of training epochs
                                                                                                                            after # of trials
                                                                                                                                                            (because they require similar predictions)— exactly the
                                                                                                                                                            opposite of what would be required for the network to
                                                                                                                                                            master the material. Hence, the network fails to predict the
Figure 4. Left Panel: Evolution of output unit activations after                                                                                            fifth letter in the example above because the first letter of
presentation of the dot in the FGGG.DLLL string. Right Panel:
                                                                                                                                                            each string fails to be prediction-relevant when processing
Evolution of the Luce ratio after presentation of the dot for the two
                                                                                                                                                            the intermediate Gs and ends up, as a result, with internal
strings FGGG.DLLL / KGGG.XLLL.
                                                                                                                                                            representations that fail to be sufficiently distinctive of each
                                                                                                                                                            string to enable it to make different predictions about the
Here , the network fails to reach a decision: it gets stuck at a                                                                                            fifth letter when presented with the dot.
"post-dot" activation value of 0.5 for both D and X (exactly                                                                                                   Shanks et al. however, could not present the extremely
the same plot is produced for the other string). The reason                                                                                                 simple (and for the network, extremely difficult) material to
why learning fails in this case is addressed in the discussion.                                                                                             their subjects, for everyone would have discovered the rule
                                                                                                                                                            in that case. Importantly however, the way in which their
                                                    Discussion                                                                                              material is constructed results, for instance, in all the
What had to be shown was shown, namely that a                                                                                                               training strings to be determined by their first two elements
connectionist network, more precisely a Simple Recurrent                                                                                                    — something that enables the network to learn the
Network, is able to make a distinction between grammatical                                                                                                  construction paths of each training string very quickly. In
and nongrammatical letter strings, generated from a                                                                                                         addition, in most cases, sequential information was in fact
biconditional grammar as used by Shanks et al. (1997).                                                                                                      prediction-relevant on each step, which makes it easy for the
These strings were designed so that, according to them,                                                                                                     network to distinguish between grammatical and
subjects had to make use of abstract rules in order to                                                                                                      nongrammatical strings. These findings suggest that the

Shanks et al. material was in fact inadequate to test for the   Cleeremans, A., Destrebecqz, A., and Boyer, M. (1998).
rule based versus memory based distinction. As mentioned          Implicit learning: News from the front. Trends in
before however, it is clearly impossible to conceive of easy      Cognitive Sciences, 2, 406–416.
strings like KGGG.XLLL for which the rules are not              Cleeremans, A. and Jiménez, L. (submitted). Implicit
discovered by subjects.                                           cognition with the symbolic metaphor of mind: Theories
   Insofar as simulations are concerned, while the SRN fails      and methodological issues.
on such degenerate cases (unlike human subjects), the issue     Elman, J.L. (1990). Finding structure in time. Cognitive
of whether this failure reflects a principled limitation of       Science, 14, 179–211.
connectionist networks in general remains an open issue.        Elman, J.L. (1999). Generalization, rules and neural
Servan–Schreiber et al. showed that even very slight              networks: A simulation of Marcus et al. (1999).
adjustments to the statistical structure of otherwise identical Luce, R. D. (1963). Detection and recognition. In R. D.
sequences could greatly enhance the prediction accuracy of        Luce, R. R. Bush, and E. Galanter (Eds.), Handbook of
the SRN. Thus, embedded information, as in recursive              mathematical psychology (Vol.1). New York: Wiley.
structures, need only be prediction-relevant in terms of the    Marcus, G.F., Vijayan, S., Bandi Rao, S., and Vishton, P.M.
statistical distribution of the embedded elements for such        (1999). Rule learning by seven-month-old infants.
structures to be successfully mastered by an SRN. There is        Science, 283, 77–80.
also accumulating evidence that the pattern of failures         Mathews, R.C., Buss, R.R., Stanley, W.B., Blanchard-
observed with models like the SRN closely mimic that              Fields, F., Cho, J.R., & Druhan, B. (1989). Role of
observed with human subjects (e.g., Christiansen & Chater,        implicit and explicit procesess in learning from examples.
1999) in the domain of natural language learning.                 Journal of Experimental Psychology: Learning, Memory,
   Empirically, we would like to suggest that experiments be      and Cognition, , 15, 1083-1100.
carried out on a slightly different basis than used in Shanks   McClelland, J.L. and Plaut, D. (1999). Does generalization
et al., since their 'match' group showed no sign at all of        in infant learning implicate abstract algebra-like rules?
having learned the material. One possibility would consist        Trends in Cognitive Sciences, 3, 166-168.
of changing the instructions of the match group so that         Neal, A. and Hesketh, B. (1997). Episodic knowledge and
attention is not drawn away from certain properties that          implicit learning. Psychonomic Bulletin and Review, 4,
might allow subjects to become sensitive to the structural        24–37.
properties of the material.                                     O'Brien, G. and Opie, J. (1999). A connectionist theory of
   To conclude, we have demonstrated that a simple                phenomenal experience. Behavioral and Brain Sciences,
connectionist network can in fact master material previously      22, 175-196.
considered to require the acquisition of rule-based             Perruchet, P. and Vinter, A. (submitted). The self-
knowledge for mastery of novel instances to occur. This           organizing consciousness.
outcome does not entail that rule-based learning never          Saffran, J.R., Newport, E.L., Aslin, R.N., Tunick, R.A., and
occurs (as it obviously does for some subjects in Shanks et       Barrueco, S. (1997). Incidental language learning:
al.'s experiments), but simply that biconditional grammars        Listening (and learning) out of the corner of your ear.
might not address all the issues involved in efforts to           Psychological Science, 8, 101–105.
dissociate rule-based vs. memory-based learning processes       Seidenberg, M.S. & Elman, J.L. (1999). Do infants learn
in the implicit learning literature. Further simulation work      grammar with algebra or statistics? Letter in Science, 284.
will attempt to explore these issues in greather depth.         Servan-Schreiber, D., Cleeremans, A., & McClelland, J.L.
                                                                  (1991). Graded State Machines: The representation of
                     Acknowledgments                              temporal contingencies in simple recurrent networks.
Axel Cleeremans is a Research Associate of the National           Machine Learning, 7, 161–193.
Fund for Scientific Research (Belgium). This work was           Shanks, D.R. (1998). Distributed representations and
supported by a grant from the Université Libre de Bruxelles       implicit knowledge. In K. Lamberts and D.R. Shanks
in support of IUAP program #P/4-19.                               (Eds.), Knowledge, concepts & categories, pp.197–214.
                                                                  Psychology Press, Hove.
                         References                             Shanks, D.R., Johnstone, T., and Staggs, L. (1997).
                                                                  Abstraction processes in artificial grammar learning. The
Berry, D.C. and Dienes, Z. (1993). Implicit learning:             quarterly Journal of Experimantal Psychology, 50A,
   Theoretical and empirical issues. Lawrence Erlbaum             216–252.
   Associates, Hove.                                            Shanks, D.R. and St John, M.F. (1994). Characteristics of
Christiansen, M., & Chater, N. (1999). Toward a                   dissociable human learning systems. Behavioral and
   connectionist model of recursion in human linguistic           Brain Sciences, 17, 367–447.
   performance, Cognitive Science, 23, 157-205.                 St John, M.F. and Shanks, D.R. (1997). Implicit learning
Cleeremans, A. (1993). Mechanisms of implicit learning.           from an information processing standpoint. In D.C. Berry
   MIT Press, Cambridge, MA.                                      (Ed.), How implicit is implicit learning?, pp. 124–161.
Cleeremans, A. (1997). Principles for implicit learning. In       Oxford University Press.
   D.C. Berry (Ed.), How implicit is implicit learning?, pp.
   195–234. Oxford University Press.

