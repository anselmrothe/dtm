UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Use of a High-dimensional, "Environmental" Context Space to Model retrieval in
Analogy and Similarity-Based Transfer
Permalink
https://escholarship.org/uc/item/2t3210zv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Ramscar, Michael
Yarlett, Daniel
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

  The use of a high-dimensional, “environmental” context space to model
                     retrieval in analogy and similarity-based transfer
                                           Michael Ramscar and Daniel Yarlett
                                              {michael,dany}@cogsci.ed.ac.uk
                                                    Division of Informatics
                                                   University of Edinburgh
                                                  Edinburgh, Scotland EH8 9LW
                                                                      bicycle should call to mind memories of other
                          Abstract
                                                                      bicycles).
                                                                   2.Surface superiority: Retrievals based on surface
                                                                      similarity alone (without structural similarity) should
   Current models of the retrieval of analogies from a long-
   term memory store assume mental representations that               also be frequent (e.g. a fairy story about a frog might
   are generally either underspecified or implausible. In             call to mind other stories about frogs, although the
   this paper we conduct two experiments which                        structure of the stories might differ greatly).
   demonstrate that an ‘environmental’ approach t o                3.Rare insights: Memories that are structurally similar to
   retrieval can produce appropriate retrieval patterns o n           the target context should be retrieved only occasionally
   cognitively plausible styles of representation, utilising          (e.g. the orbits of the solar system reminding one of
   information that can be easily learned from a linguistic           electrons orbiting an atom).
   environment.                                                    4.Scalability: The model must plausibly extend to
                                                                      realistically sized memory pools because people
  Introduction: Similarity-Based Transfer                             typically have vast numbers of memories, and are able
Analogy (and similarity-based transfer) is a central                  to access them in a matter of seconds.
cognitive process that represents a versatile problem-                Gentner, Ratterman and Forbus’ (1993) investigation
solving and reasoning strategy, allowing agents to bring           demonstrated that retrieval is sensitive to surface (or
previous experience to bear on novel problems. Its                 ‘semantic’, Hummel and Holyoak, 1997) similarities
operation embodies two distinct processes: (i) reminding,          between a target representation and a base analogy that
or retrieval, of appropriate analogs from a long-term              needs to be to be retrieved. (As opposed to the shared
memory store; after which (ii) candidate analogs are               relational structure that determines an analogical match.)
mapped onto the representation of the current problem (the         The retrieval process, being relatively computationally
target) to determine deeper relational matches, and to allow       cheap, acts as an efficient prefilter to the more expensive
inferences to be made (Gentner, Ratterman & Forbus,                process of structural alignment (albeit at the expense of
1993; Forbus, Gentner & Law, 1995; Holyoak &                       potentially passing over useful analogies that share
Thagard, 1995).                                                    structural commonalities with the target domain).
   The latter mapping process has been shown to rely
largely on structural commonalities (Gentner, 1983;                Meeting the Constraints
Holyoak & Thagard, 1995; Hummel & Holyoak, 1997),                  MAC/FAC (Forbus, Gentner & Law, 1994) and LISA
and computational models of the mapping processes that             (Hummel & Holyoak, 1997) are the two foremost models
determine structural commonalities have been subject to            of similarity-based transfer. Below we review the approach
much critical scrutiny (Falkenhainer, Forbus & Gentner,            taken by both models with regards to retrieval, and
1989; Holyoak & Thagard, 1989; Keane, Ledgeway &                   examine the theoretical basis for each.
Duff, 1994, Hummel & Holyoak, 1997). In contrast
retrieval has been subject to less investigation. Here, we         MAC/FAC: Content Vectors
subject the relatively more neglected issue of modelling
analog retrieval to a more focussed theoretical                    MAC/FAC models retrieval by generating a content
examination.                                                       vector for each representation that is stored in its memory-
                                                                   pool. A content vector summarises the surface features of
                                                                   a representation by recording the frequency with which
          Four Constraints on Retrieval                            each lexically distinct predicate occurs in it. Thus, the
Empirical studies by Gentner, Ratterman and Forbus                 following proposition:
(1993) established four primary constraints on the patterns              (CAUSE (STRIKES-WITH JOHN CUE CUE-BALL)
that an appropriate theory of retrieval should produce
given a specific context or probe:                                               (AND (POTS CUE-BALL) (POTS BLACK)))
                                                                   would be assigned the following content vector:
1.Primacy of the mundane: The majority of retrievals                           ((CAUSE . 1) (STRIKES-WITH . 1)
   evoked should be literally similar to the context,
   sharing both surface and structural characteristics (e.g. a                       (AND . 1) (POTS . 2))

   A measure of the degree that two representations share
the same surface features can then be derived by                        LISA: Semantic Features
calculating the dot-product of their content vectors (if a              The other leading model of analogy in the literature, LISA
particular predicate does not appear in a representation then           (Hummel & Holyoak, 1997) also relies upon the notion
it is implicit, adopting a sparse-encoding approach, that it            of semantic units (or links) – and re-representations into
has a frequency of zero). It is important to note that only             ‘semantic primitives’ – in its structured representations to
predicates that are identical from one another can                      model retrieval. These semantic elements are largely
contribute to the magnitude of a dot-product between two                constrained by the representation strategy adopted in LISA
content vectors: there is no potential for multiplying the              (e.g. \verb+likes1+ or \verb+likes2+). Hummel and
frequencies of distinct predicates in the dot-product                   Holyoak’s claim is that these allow appropriate patterns of
calculation.                                                            retrieval to be produced by their model. However, they
   Forbus, Gentner and Law (1994) argue that the dot-                   offer no empirical support for the selection of their
product between two content vectors provides an                         particular set of primitive semantic features. At present,
empirically adequate measure of the retrievability of one               the semantic information in LISA’s representations is
representation, given another as a context, because it                  hand-coded, and ultimately reliant upon humanistic
satisfies the four constraints on retrieval performance.                intuitions about similarities of meaning.
A Critique of Content Vectors                                           Summary of Current Approaches
In order to model the way that lexically distinct items in              Both MAC/FAC and LISA present models of retrieval
stimuli prime one another for retrieval, the content vector             that are theoretically under-specified. Both accounts rely
theory makes a commitment to a theory of mental                         on the problematic (i.e. currently undefined) notion of re-
representation we shall call canonical representation (CR)              representation, either into ‘canonical conceptual
theory. This presupposes a translation procedure that                   representations’ (MAC/FAC) or ‘semantic primitives’
allows tokens that are lexically distinct but share similar             (LISA). Ultimately, this means that both models rely on
semantic “meanings” to be re-encoded using identical                    hand-coded information to drive their retrievals. Neither
tokens. This translation procedure accounts for cross-                  LISA nor MAC/FAC actually models the representation
lexeme priming effects by identically encoding distinct                 of lexical information. They rely instead on imported
lexemes that should prime for one another, thus ensuring                information (primarily intuition) to underpin their
that they can contribute to the dot-product score between               behaviour, thus neither can be said – at present – to offer
the two representations in which they feature. CR theory                any real explanation of the role of lexico-conceptual
assumes that during the process of comprehension                        knowledge in retrieval.
(representation building):                                                 None of this means, of course, that the shortcomings
   “Two concepts that are similar but not identical (such as ‘bestow’   that we describe in each of the two theories could not
   and ‘bequeath’) are decomposed into a canonical representation       ultimately be addressed. We do, however, feel that in the
   language so that their similarity is expressed as a partial identity
   (here, roughly, give’).” Forbus, Gentner and Law, 1994, pp. 153      light of these shortcomings there is room for an
                                                                        investigation of whether another approach to the
‘Canonical Form’?                                                       representation of lexico-conceptual knowledge might be
                                                                        used to ground an alternative theory of retrieval.
According to CR theory, complex semantic elements can
be recursively decomposed -- or re-represented -- until a                    Co-occurrence Models of Semantics
canonical measure of their semantic significance is
reached. Hence CR theory assumes that the mental                        One approach to lexico-conceptual knowledge that seems
encoding of semantically complex concepts can ultimately                promising in this respect is the high dimensional
be analysed in terms of a stock of canonical forms.                     modelling of context spaces. This is a data-intensive
Clearly the correctness or otherwise of this assumption is              technique that analyses a set of corpora, and from this
an empirical matter. However, it does seem worth noting                 derives a summary of the variety of different contexts that
that research into the mental representation of concepts                different words can be used in. There is a growing body of
suggests that human conceptual representations are                      evidence that the frequency with which different lexemes
anything but canonical. The proposals for generalised                   co-occur with one another (that is, are used together
theories of representation that exist in the concepts                   within a particular context, such as a paragraph or
literature fall well short of providing the kind of “neat”              moving-window) can provide useful information about the
account of concepts that canonical conceptual                           semantic properties of those lexemes.
representation assumes (see Komatsu, 1992; Ramscar &                       In co-occurrence analyses, a contextual distribution is
Hahn, 1998 for reviews). Lacking as it does an account of               calculated for each lexeme encountered in a corpus analysis
what a canonical conceptual form is, in its current form                by counting the frequency with which it co-occurs with
CR theory is under-specified, and thus fails to                         every other lexeme in the corpora being analysed. The
operationalise the notion of semantic similarity in a                   contextual distribution of a lexeme can then be
sufficiently tight manner. This prevents specific                       summarised by a vector showing the frequency with which
predictions being made from the theory (e.g. how strongly               it is associated with the other lexemes in a common
do ‘cat’ and ‘dog’ prime for one another based on an                    linguistic environment. One can think of this information
analysis of the overlap in their shared semantic features?).            as defining a model containing a network of links between
                                                                        the lexemes in a language, each with varying strengths,

and representing the varying contextual co-occurrences of                  dimensional reduction techniques on the vectors
lexemes in that language. Two such co-occurrence models                    associated with each lexeme (Landauer & Dumais,
are the Latent Semantic Analysis (LSA) model (Landauer                     1997) offers evidence that, in fact, there may not be a
and Dumais, 1997; Landauer, Foltz & Laham, 1998), and                      unique set of semantic features used in the encoding of
the Hyperspace Analog to Language (HAL) model                              semantic relations, but rather that multiple encodings
(Burgess & Lund, 1997).                                                    can provide sufficient information to meet empirical
   There is good evidence that co-occurrence analysis                      constraints
extracts information from corpora that can be used to                    5.Because co-occurrence techniques do not rely on a
model certain linguistic behaviour. For example, Landauer                  predefined set of semantic features (such as gender,
and Dumais (1997) report that the LSA model can pass a                     plurality, animacy and so on), this eliminates
multiple-choice TOEFL synonym test. Lund, Burgess and                      subjectivity from the decisions that are made during the
Atchley (1995) present evidence that co-occurrence data                    process of hand-coding representations during the
can act as a good predictor of various priming effects.                    modelling process.
Burgess and Lund (1997) demonstrate that the HAL model                     The success of co-occurrence techniques in accounting
can produce clustering in its high-dimensional space of                  for priming effects (c.f. Lund, Burgess and Atchley,
lexemes from differing grammatical categories.                           1995), has shown them to be useful models of lexical
   Whilst the exact parameters of LSA and HAL are                        retrieval. Here, we seek to establish whether these models
different, they both adopt the general approach outlined                 can be used to account for the retrieval of structured
above to generate co-occurrence vectors. We feel that there              composite representations, and not just individual
are a number of attractive benefits to be gained from                    lexemes, from a memory-pool.
modelling the semantic information used in analogical and
similarity based retrieval in this way:                                  The ‘Karla the Hawk’ Stories
1.The proposed semantic metric is clearly specified. By                  The experiments detailed below use the ‘Karla the Hawk’
   proposing that the semantic information used in                       materials as originally used by Gentner, Ratterman and
   retrieval is learned from observing the varying                       Forbus (1993). The Karla materials consist of twenty sets
   contextual co-occurrences of lexemes in a language, we                of stories written in natural language. Each set consists of
   avoid having to postulate entities – such as ‘semantic                a base story, and four systematic variations of that story.
   primitives’ whose theoretical and psychological nature                Two factors are crossed over the four variant stories, as
   is massively under-specified.                                         shown in Table 1.
2.The semantic information used could be easily learned
   from the environment,1 thus avoiding the problems                                            +ST                        -ST
   inherent in positing entities whose learnability is                     +SF           Literal Similarity         Surface similarity
   somewhat controversial, and whose innateness might
                                                                            -SF               Analogy              1st Order Relations
   otherwise have to be treated as axiomatic (as canonical
                                                                           Table 1: The Karla materials
   concepts seem to be; see Laurence & Margiolis, 1999;
   Fodor, 1981).                                                           The four story categories systematically vary the
3.An         environmental          context      model        contains   commonalities that are shared with the base-story from
   representationally cheap, summarised information, the                 which they are derived. Each variant can either share or not
   usage of which makes only limited processing demands.                 share surface (±SF) and structural (±ST) commonalities
   Thus it allows one to avoid the theoretical problems                  with the corresponding base-story. This 2 x 2 materials
   inherent in theories of re-representation which explain               design allows for the controlled examination of the
   cheap surface matches in terms of semantic                            sensitivity of various putative measures of retrieval.
   decomposition and expensive structural alignment (c.f.                Gentner, Ratterman and Forbus (1993) found that the
   Holyoak & Hummel, 1997; Forbus et al, 1997).                          prime determinant of retrievability was shared surface
4.Environmental context models are relatively objective:                 commonalities, whilst shared structural commonalities
   they do not require that a particular set of ‘semantic                had a nonsignificant effect. This is the pattern of results
   features’ are defined before textual analysis begins.                 that we will look for in our experiments. The empirical
   Instead the co-occurrence technique takes the lexemes                 results reported in Gentner, Ratterman and Forbus (1993)
   themselves as features, and uses frequency relations                  are summarised in Table 2.
   between them to define their associativity. This is an
   advantage given the difficulty we have already                                                     LS      SS        AN       FOR
   highlighted of empirically grounding claims as to the                 Retrieval Scores             1.92    1.64      0.44     0.27
   identity of semantic features. Furthermore, the use of                Inferential Soundness        4.41    2.70      4.16     2.58
   1
                                                                           Table 2: The results of the experiments conducted by Gentner,
      Indeed, despite some of the stronger claims made for co-
                                                                         Ratterman and Forbus (1993).
occurrence models of language (c.f. Landauer & Dumais, 1997) we
feel that they are best characterised as being essentially models of the   Below, we report two experiments that compare the
associativity of lexemes in a common linguistic environment, such that   performance of the content vector (CV) theory of retrieval,
we prefer to call them “environmental context models”. It is also
worth noting that co-occurrence techniques are also compatible with a    as implemented in MAC/FAC, against the measure
neural implementation. Lowe (1997) demonstrates that a co-               provided by the LSA model.
occurrence model can easily be implemented as a self-organising
Kohonen map, and this offers some support for the idea that some form
of co-occurrence counting could occur in the brain.

                                                                    3. As noted above, each variant story either exhibits ±SF
Experiment 1: Stripped Natural Language.                            and ±ST, depending on whether it shares or does not share
Experiment 1 was designed to determine whether there is             object-attributes and higher-order relations (structure) with
sufficient informational content in a reduced representation        the base story it is derived from. The ANOVA analysis
of the Karla the Hawk stories to produce retrieval patterns         revealed that the CV metric was sensitive to both ±SF
conformable to the empirical data.                                  (F(1,19) = 11.965, p<0.01) and ±ST (F(1,19) = 10.027,
   It is clear from experimental studies that in addition to        p<0.01), with no significant interaction effect (F(1,19) =
the accretion of structural information during                      3.717, p>0.05). For the LSA metric there was a main
comprehension, there is a concomitant loss of superficial           effect of ±SF (F(1,19) = 68.985, p<0.01); no effect of
verbatim information as propositional representations are           ±ST (F(1,19) = 2.611, p>0.05), and no significant
built up (Sachs, 1967; Gernsbacher, 1985). Since we                 interaction between the factors (F(1,19) = 2.428, p>0.05).
wanted to simulate retrieval of what subjects in Gentner et
al’s studies actually stored (and there is good evidence that                                       LS        SS         AN        FOR
people do not store texts verbatim), we decided to initially        CV Metric                       0.116     0.084      0.057     0.053
test retrieval on versions of Gentner et al’s stimuli that          LSA Metric                      0.442     0.412      0.151     0.152
had all of the closed-class2 lexemes removed from them.                Table 3: Experiment 1 -- The category means for the CV and LSA
   Applying this principle resulted in a set of words for           scores derived from comparing each base-story with its four variants
each story which constituted the words which are, in some           on the stripped (‘bag-of-words’) representations. All twenty story-sets
sense, maximally informative about the context that the             had closed-class lexemes removed from them, and were used in the
representation defines. For example, some words                     comparison.
(generally the closed-class words) may occur in almost any
(and every) possible context (e.g. ‘the’ can co-occur               Discussion
plausibly with an extremely diverse set of lexemes). Thus           The clustering in the mean LSA scores for each category
encountering such a word in a probe representation has              of variant (LS-SS and AN-FOR) mirrors the subject data
little informational utility with respect to retrieval because      in Gentner, Ratterman and Forbus’s (1993) study closely.
it fails to narrow the set of candidate retrievals at all. Such     The same pattern is not observable in the CV metric.
lexemes are unlikely to influence the kind of retrieval             Furthermore, the only significant factor in Gentner’s
studied by Gentner, Ratterman and Forbus (1993).                    original retrieval experiments was ±SF and only the LSA
   The original Karla the Hawk base-story after it had been         scores conform to this pattern. The CV metric was also
pruned of all closed-class lexemes is given below, as an            sensitive to the ±ST factor, which indicates that it is
example of the characteristic ‘bag of words’ that remained          sensitive to a factor which has been shown to have little
once the natural language representations had been                  significant impact on retrieval performance. It appears that
stripped:                                                           there is sufficient information remaining in the reduced
                                                                    representation to allow different contexts for retrieval to be
Karla old hawk lived top tall oak tree
afternoon saw hunter ground bow crude arrows                        discriminated from one another in a way that simulates the
feathers hunter aim shot hawk missed Karla                          empirical findings discussed.3 Moreover, it seems clear
knew hunter wanted feathers glided down hunter                      from these results that LSA models the original empirical
offered give hunter grateful pledged shoot                          data more accurately than CV.
hawk shot deer
                                                                    Experiment 2: Faithful Dgroups
Method
                                                                    Experiment 2 investigated the performance of the CV and
The base story for each story-set of the reduced
                                                                    LSA measures on a style of representation that explicitly
representations was compared with each of its four
                                                                    encodes the structural features implicit in the original
variants in turn, using the LSA and CV (MAC/FAC
                                                                    stories. This structural information is required to be able
content vector) models. This was done in order to
                                                                    to complete the mapping phase of similarity-based
reproduce the experimental format embodied in Gentner’s
                                                                    transfer, and so these experiments were conducted to
original retrieval experiments. The LSA model was set to
                                                                    determine whether a single style of representation would
compare items in document-to-document mode, using the
                                                                    be sufficient to underpin both the retrieval and mapping
300 most significant factors extracted by the model from a
                                                                    processes of similarity-based transfer. The style of
corpus that approximates the general reading a first year
                                                                    representation that we chose shares the substantial core of
college student is exposed to (which seemed appropriate
                                                                    its form with that used in SME and MAC/FAC, but we
given the participants in Gentner et al’s studies). Because
                                                                    developed a series of constraints for translating text into
of the 2 x 2 design of the experiment, a repeated-measure
                                                                    these structured representations whilst avoiding any
ANOVA analysis is the appropriate test to determine
                                                                    commitment to the CR theory (we call these
which of the factors, ±SF or ±ST, the two metrics are
                                                                    representations Faithful Dgroups, ‘Dgroup’ being the
sensitive to.
                                                                    usual term used to describe individual – “chunked” –
Results                                                             structured representations in the SME literature.).
The results of the inter-story comparisons conducted with
the LSA and CV models of retrieval are recorded in Table               3
                                                                         It should be noted here that the LSA retrieval scores remain more
                                                                    or less unchanged from pilot testing on the full NL versions. The CV
                                                                    scores, however, are significantly reduced from the original NL
   2
     Closed-class words belong to the set of words which are closed materials. This seems to indicate that the LSA model is more robust
under the grammatical rules of a language.                          across representations.

Producing The Faithful Dgroups                                    relations should be the minimum set required to
Humans are capable of extracting more meaning from                articulate the narrative structure of the story.4
language than the basic information that is encoded in the     Thus we sought to minimise unwarranted inferences, and
surface structure of texts and dialogues might suggest. To     the addition of features not warranted by their inclusion in
take the following as an example:                              the original materials. In contrast to the original Dgroups,
                                                               the Faithful Dgroups incorporate much of the lexical
     John hit Mary; Mary cried. The Headmaster                 information that is present in the original natural language
                       expelled John.                          representations.
In interpreting this passage, a reader has to infer firstly    Method
that John’s hitting Mary caused her to cry, and secondly       Faithful Dgroups representing nine of the original story-
that the relationship between John’s hitting Mary, and her     sets were created.5 The faithful Dgroup representing the
crying, caused the Headmaster to expel John. We might          base story for each story-set was then compared with each
express this information in terms of the following nested      of its four variants in turn, again using both the CV and
propositional structure:                                       LSA models. The LSA model was again set to compare
    cause( cause( hit(john,mary), cry(mary) ),                 items in document-to-document mode, using the 300 most
                 expel(headmaster,john))                       significant factors extracted by the model from the “first
                                                               year college student, general reading’’ corpus.
   None of this causal information appears explicitly in
the original utterance, so it is clear that it must in some    Results
way be inferred from a prior source. (The need for             The result of the CV and LSA comparisons on the
inference here is uncontroversial: all theories of             Faithful Dgroups are presented in Table 4 below.
comprehension agree that language comprehension                   For the CV method there was no significant effect of
requires a great deal of active involvement on the part of     ±SF (F(1,8) = 3.647, p>0.05), no significant effect of
the comprehender when it comes to inferring information        ±ST (F(1,8) = 3.383, p>0.05), and no interaction effect
that is not explicitly encoded in language (e.g. McKoon &      (F(1,8) < 1). For the LSA method there was an effect of
Ratcliff, 1992); where they disagree is on what, and how       ±SF (F(1,8) = 66.091, p<0.01); no significant effect of
much, inference actually happens.)                             ±ST (F(1,8) = 2.190, p>0.05); and no significant
   Whilst we haven’t attempted to make a commitment to         interaction between the factors (F(1,8) = 1.094, p>0.05).
a particular theory of comprehension in specifying the
procedure for translating texts into Faithful Dgroups, what                                   LS         SS          AN        FOR
we have tried to do is to provide the beginnings of a          CV Metric                      0.751      0.718       0.735     0.688
method that requires a minimal amount of inference, and        LSA Metric                     0.670      0.633       0.466     0.456
is broadly compatible with the bulk of the available data         Table 4: Experiment 2 -- The category means for the CV and LSA
in this area (again, see McKoon & Ratcliff, 1992).             scores derived from comparing each base-story with its four variants in
   The basic outline of a procedure for forming the            the Faithful Dgroups. Nine of the NL story-sets were encoded in this
Faithful Dgroups from natural language samples is              format
described below.
Algorithm for Construction of Faithful Dgroups                 Discussion
Seeking to maximally preserve closed-class lexical             As expected, on representations make no commitment to
information:                                                   CR theory – using instead the lexico-semantic
1.Identify the objects that are referred to in the text, and   information derived from the external representations to
   list them using (sme:defEntity ...) commands.               drive retrievals – these results demonstrate that the CV
2.Identify all the lexeme structures used to express           method is insensitive to the surface-features of the stories,
   attributes of the objects in the text, and express these as and thus fails to produce empirically adequate retrieval
   unary expressions.                                          patterns. This is because the CV method only permits
3.Identify the lexeme structures used to express relations     priming between lexically identical items. The LSA
   between the identified objects, and express these in the    method, however, performs much better: its retrievals are
   Dgroup form as expressions with two or more                 only sensitive to the ±SF factor, which is what is required
   arguments, taking only objects as arguments.                to model the empirical evidence.
4.Now deal with higher-order information (i.e. temporal           It is particularly noteworthy that the LSA method
   and causal information that is frequently implicit in NL    assigned high retrieval scores to the LS and SS categories
   representations). Express this            information as    in this experiment, when their representations need not
   expressions taking other expressions as arguments.          share any identical lexemes with their corresponding base
   Note that because this information is often implicit in     representation. It follows that the LSA model is not
   the NL forms of the stories, a standard (or canonical)      simply relying on identical lexemes in distinct
   lexical identity for each expression must be adopted
                                                                  4
   (this has the effect of minimising the influence of              Thus, as with other models of similarity-based transfer, some hand
                                                               coding of representations does occur (though the freedom to make
   inferred structures on retrieval, which is in accordance    unprincipled coding decisions is greatly reduced in comparison with
   with Gentner’s empirical findings). The set of inferred     other models). This procedure was designed to minimise the influence
                                                               of such hand coding, although our ultimate goal is the automation of this
                                                               process.
                                                                  5
                                                                    For comparison purposes, we encoded the same set of stories that
                                                               Forbus, Gentner and Law (1994) coded for MAC/FAC.

representations to facilitate retrievals, but is modelling     helpful discussion of these issues. This work was
instead a more complex kind of relationship between the        supported in part by EPSRC Grant GR/M59846
ways that individual lexemes are used in differing
linguistic contexts.                                                                 References
                                                               Burgess, C., and Lund, K. (1997). Modelling Parsing
                        Conclusion                               Constraints with High-Dimensional Context Space.
The performance of the LSA measure on both styles of             Language and Cognitive Processes, 12, 177-210.
representation offers concrete evidence that it can act as a   Falkenhainer, B., Forbus, K.D., and Gentner, D. (1989).
good predictor of retrieval. That it can do so even when         The Structure-Mapping Engine: Algorithm and
operating on a style of representation that remains faithful     Examples. Artificial Intelligence, 41, 1-63.
to the natural language source of information, and relies      Fodor, JA (1981) The present status of the innateness
on only a psychologically plausible range of inferences for      controversy. In J Fodor (ed) RePresentations, MIT Press
its structure (i.e. a structured, propositional representation Forbus, K., Gentner, D., and Law, K. (1994).
that handles lexeme-encoding realistically) is encouraging.      MAC/FAC: A Model of Similarity-based Retrieval.
As is the fact that we were able to model the empirical          Cognitive Science, 19, 141-205.
data without hand tailoring a model of semantics, instead      Gentner, D. (1983). Structure-Mapping: A Theoretical
using an objectively, and independently, derived model of        Framework for Analogy. Cognitive Science, 7, 155-70.
lexico-semantic information.                                   Gentner, D., Ratterman, M., and Forbus, K. (1993). The
   We alluded above to a potential problem in employing          Roles of Similarity in Transfer: Separating
the idea of re-representation in retrieval: that studies have    Retrievability from Inferential Soundness. Cognitive
shown retrieval to act as a cheap pre-filter for the more        Psychology, 25, 524-575.
computationally expensive – and conceptually rich –            Gernsbacher, M. A. (1985). Surface Information Loss in
process of analogical mapping. Yet the use of re-                Comprehension. Cognitive Psychology, 17:324-363.
representation in this process will result in multiple         Holyoak, KJ & Thagard, P (1995) Mental Leaps. MIT
structural mappings being carried out at the conceptual          Press, Cambridge, Ma.
decomposition stage (as many as there are lexically            Hummel, J.E., and Holyoak, K.J. (1997). Distributed
distinct but        "semantically" similar        items     in   Representations of Structure: A Theory of Analogical
representations to be mapped). It doesn’t take much              Access and Mapping. Psychological Review, 104, 427-
reflection to realise that will lead to a situation where        66.
more structural mapping is required in reconciling             Keane, M., Ledgeway, T., and Duff, S. (1994).
semantic differences than in mapping an analogy itself.          Constraints on Analogical Mapping: A Comparison of
   At some point mappings between richly represented             Three Models. Cognitive Science, 18, 387-438.
structure will have to stop, if only because cognitive         Komatsu, L K (1992) Recent views of conceptual
processing capacity is limited. Our contention is that re-       structure. Psychological Bulletin, 112(3), 500-526
representation – in retrieval at least – is expensive and      Landauer, T.K., and Dumais, S.T. (1997). A Solution to
unnecessary. Structure mappings can be retrieved – and           Plato’s Problem: The Latent Semantic Analysis Theory
conceptualised – using a far cheaper source of information.      of Acquisition, Induction, and Representation of
Not only does the use of high-dimensional,                       Knowledge. Psychological Review, 104, 211-40.
“environmental” context space to model retrieval in            Landauer, T.K., Foltz, P.W., and Laham, D. (1998). An
analogy and similarity-based transfer appear to be a             Introduction to Latent Semantic Analysis. Discourse
plausible approach, it also seems to satisfy Gentner,            Processes, 25, 259-84.
Ratterman and Forbus’ scalability constraint better than       Laurence, S. & Margiolis, E. (1999) Concepts and
other models as well.                                            cognitive science. In Laurence & Margiolis (eds.)
   Given the role structure appears to play in concepts,         Concepts. MIT Press, Cambridge, Ma.
any conceptual solution to matching semantics may suffer       Lowe, W. (1997). Semantic Representation and Priming
from to re-representation problem as well. It may be that        in a Self-Organizing Lexicon. Proceedings of the 4th
all conceptualisation – analogical and literal – is about        Neural Computation and Psychology Workshop pp.
retrieving and mapping the right information in context.         227-39. Springer-Verlag.
Gentner, Ratterman and Forbus (1993) showed that an            Lund, K., Burgess, C., and Atchley, R.A. (1995).
inexpensive source of information was all that was needed        Semantic and Associative Priming in High-
to contextualise retrieval: our results indicate that a of       Dimensional Semantic Space. In Proc. 17th Annual
high-dimensional, “environmental” model can provide that         Conference of the Cognitive Science Society LEA pp.
context in analogy and similarity-based transfer. Our            660-65. LEA.
suspicion is that it might also serve to contextualise         McKoon, G., and Ratcliff, R. (1992). Minimal Inference:
broader conceptual processing as well.                           A Framework for Discourse Processes. Psychological
                                                                 Review, 99:440-66.
                 Acknowledgements                              Ramscar, M.J.A. and Hahn, U. (1998) What family
We are grateful to Andrew Wishart for his help in coding         resemblances are not In Proc., 20th Annual Conference
the Faithful Dgroups, and also for insightful comments           of the Cognitive Science Society, LEA, pp 865-870
on an initial draft of this paper. We would like to thank      Sachs, R.S. (1967) Recognition Memory for Syntactic
Lera Boroditsky, Ken Forbus and Dedre Gentner for                and Semantic Aspects of Connected Discourse.
                                                                 Perception and Psychophysics, 2, 422-437.

