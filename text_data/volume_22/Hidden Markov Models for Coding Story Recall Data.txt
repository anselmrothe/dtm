UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Hidden Markov Models for Coding Story Recall Data
Permalink
https://escholarship.org/uc/item/39b2n5vc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Durbin, Michael A.
Earwood, Jason
Golden, Richard M.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

CogSci2000: Durbin, Earwood, and Golden                                                                                         1
                           Hidden Markov Models for Coding Story Recall Data
                                           Michael A. Durbin (golden@utdallas.edu)
                                        Cognitive Science Program (Attention: Professor Golden)
                                             University of Texas at Dallas, GR4.1, Box 830688
                                                         Richardson, TX 75083-0688
                                               Jason Earwood (golden@utdallas.edu)
                                            Psychology Program (Attention: Professor Golden)
                                             University of Texas at Dallas, GR4.1, Box 830688
                                                         Richardson, TX 75083-0688
                                            Richard M. Golden (golden@utdallas.edu)1
                                   Psychology and Cognitive Science Programs, GR4.1, Box 830688
                                                         University of Texas at Dallas
                                                         Richardson, TX 75083-0688
                              Abstract                                     are typically not well documented. Second, the reliability of
                                                                           such procedures is often highly dependent upon ''human
   Current methods of coding recall, summarization, talk-aloud,            coders'', who despite their best intentions, are prone to in-
   and question-answering data are inherently unreliable and not           consistent coding behaviors (especially over very large cod-
   effectively documented. If the process of coding protocol               ing tasks). Third, such coding procedures are typically not
   data could even be partially automated, this would be an im-            readily accessible to other researchers. And fourth, coding
   portant scientific advance in the field of text comprehension.
   Twenty-four human subjects read and recalled each of four
                                                                           procedures across research labs located in different parts of
   short texts. Half of the human recall data (the ''training data'')      the world are not standardized in any particular manner.
   was coded by a human coder and then used to estimate the                   An ideal solution to these problems would be to develop
   parameters of a set of Hidden Markov Models (HMMs)                      an automated approach to coding human protocol data (as
   where each HMM was associated with a particular complex                 advocated by Ericsson and Simon, 1993). Although impor-
   proposition in the text. The Viterbi algorithm was then used            tant progress in this area has been made (see especially
   to assign the ''most probable'' complex proposition to human-           Kintsch, 1998, Chapter 3), additional work is required. It
   coder specified text segments in the remaining half of the              should also be emphasized that the task of coding human
   human recall data (the ''test data''). The HMM algorithm                protocol data is not nearly as complex as the full-fledged
   made coding decisions which agreed well with a human
                                                                           natural language understanding problem. Consider a typical
   coder's decision on the test data indicating that the HMM is
   indeed capable of formally representing a human coder's                 experiment where a group of human subjects are asked to
   "theory'' of how text segments should be mapped into com-               recall the same story from memory. Although the resulting
   plex propositions for simple texts.                                     protocol data will be extremely rich and varied, typically the
                                                                           text comprehension researcher is only interested in detecting
                          Introduction                                     a relatively small number of complex propositions. This
                                                                           dramatically simplifies the pattern recognition problem.
   Theories and experiments in the field of text comprehen-                   The main goal of this research is to develop and empiri-
sion often require mapping recall (e.g., Golden, 1997),                    cally evaluate a new theoretical framework for reliably
summarization (e.g., van den Broek & Trabasso, 1986),                      mapping protocol data into a textbase microstructure. Spe-
talk-aloud (e.g., Trabasso & Magliano, 1996), and question-                cifically, a Hidden Markov Model (HMM) (see Allen, 1995;
answering (e.g., Graesser & Franklin, 1990) protocol data                  Charniak, 1993; Jelinek, 1997; for relevant reviews) is con-
into a semantic model of the implicit and explicit informa-                structed for each complex proposition in each of four short
tion in text clauses. This semantic model of the information               stories. The stories, based upon classic fables, each con-
in the text clauses has been referred to by Kintsch (1998) as              sisted of approximately 10-15 short sentences with each
the textbase microstructure. Typically this initial coding                 sentence corresponding roughly to a complex proposition
procedure of mapping the protocol data into a textbase mi-                 (Golden, 1997). Twenty-four human subjects read and re-
crostructure is done using human coders. Inter-coder reli-                 called each of the four short texts (see Golden, 1997, for
ability measures are then used to establish the reliability of             additional details). Half of the human recall data (the ''train-
the coding procedure.                                                      ing data'') was coded by a human coder and then used to
   This widely used coding procedure methodology, how-                     estimate the parameters of the HMM associated with each
ever, has several problems. First, such coding procedures
   1
     The order of the authors is arbitrary. Please address all correspondence to Richard M. Golden.

CogSci2000: Durbin, Earwood, and Golden                                                                             2
complex proposition. The prior probability that a particular  in this paper. Here is an example recall protocol extracted
complex proposition was used by the human coder was also      from the training data set.
recorded. Next, the Viterbi algorithm (Viterbi, 1967; see
Allen, 1995; Charniak, 1993; Jelinek, 1997) was used to                      Subject 1 recall of "Miser Text"
assign the ''most probable'' complex proposition to human-                           (training data set)
coder specified text segments in the remaining half of the
human recall data (the ''test data''). Measures of agreement  someone that a servant that knew that
between the human coder and AUTOCODER were then               discovered the money# and took it# and
computed using only the test data. A high measure of          then the miser saw that the money was
agreement indicates that the HMM is indeed capable of         gone# and he was upset# and complained
formally representing a human coder's "theory'' of how text   to a neighbor# and the neighbor said
segments should be mapped into complex propositions.          well just get a stone and bury your
                                                              money# dig a hole and bury the money#
                            Method                            because it'll do you just as much good
                                                              as your real money your gold is doing
                                                              you#
Human Protocol Data
   Texts. The human protocol data used consisted of recall       The symbol # in the above recall protocol associated with
data associated with four texts collected by Golden (1997).   subject 1 refers to the marking of text segments by an ex-
The four texts ("Cuckoo", "Miser", "Eagle", and "Doctor")     perienced human coder. Text segments corresponding to
were especially written to have approximately similar levels  complex propositions were marked by experienced human
of syntactic and semantic complexity. Each sentence in the    coders for both the training data and test data sets. Here is a
text was written to conform approximately to: (1) a standard  representative recall protocol from subject 12 who was as-
subject-verb-object form, and (2) such that each sentence     signed to the test data set. The complexity of the recall data
corresponded roughly to one complex proposition. For ex-      (even when a human coder has already identified text seg-
ample, the "Miser" text read by the human subjects is shown   ments) is readily apparent (compare recall data of Subject 1,
below.                                                        Subject 12 with one another and the original "Miser" text).
               The "Miser" Text (Golden, 1997)                              Subject 12 recall of "Miser Text"
                                                                                         (test data set)
   A miser bought a lump of gold using
all of his money. The miser buried the                        and he buried it in the ground # and he
gold in the ground. The miser looked at                       went over every day to look at where the
the buried gold each day. One of the mi-                      money was where the lump of gold was
ser's servants discovered the buried                          buried# and one day when the miser was-
gold . The servant stole the gold . The                       n't there a thief came and dug up the
miser , on his next visit , found the                         lump of gold# and so the miser goes and
hole empty . The miser was very upset .                       he sees the hole in the ground# and he's
The miser pulled his hair . A neighbor                        very upset by that# and a bystander
                                                              tells the miser to take a rock and bury
told the miser not to be upset . The
                                                              it in the ground# and the miser says
neighbor said , " Go and take a stone ,                       why# and the bystander says well all you
and bury it in the hole . "The neighbor                       ever did was look at the ground anyway#
said , " And imagine that the gold is                         you never did use the gold# so there
still lying there ." The neighbor said ,                      might as well be a rock there#
" The stone will be as useful to you as
the gold . " The neighbor said, " When
you had the gold , you never used it . "                      Parameter Estimation (Learning Algorithm)
   Recall Protocol Data. Twenty-four college students read       The learning process involves a specially designed
and verbally recalled each of four texts ("Miser", "Cuckoo",  graphical user-interface which is referred to as
"Doctor", and "Eagle") from memory as described in            AUTOCODER. Figure 1 shows a typical AUTOCODER
Golden (1997). The recall data was then transcribed. Text     display. A subject's recall data (in this case, the recall data
segments in all of the recall protocol data corresponding to  for Subject 12) is displayed. The human coder first seg-
complex propositions were then identified by human coders.    ments the text so that each word sequence in each text seg-
The recall data from twelve of the college students was des-  ment corresponds to a complex proposition. Beneath each
ignated as training data, while the recall data from the re-  word is a pull-down menu consisting of a series of concepts.
maining twelve college students was designated as test data.  The human coder decides which words (or word sequences)
   To provide some insights into the richness and complex-    should be assigned concepts, and then uses the pull-down
ity of the statistical pattern recognition problem considered menu to assign a concept to each selected word within a
                                                              given text segment. Another pull-down menu is then used to

CogSci2000: Durbin, Earwood, and Golden                                                                                  3
assign a complex proposition to a given sequence of con-            complex proposition for a particular text segment. The "in-
cepts within a text segment.                                        formation content" in bits (i.e., a normalized log-likelihood
   Probabilistic Modeling Assumptions. Let W1, ..., WM be           measure) I of a complex proposition F consisting of M
the ordered sequence of words (or more generally word               concepts C1, C2, ..., CM and represented by M word phrases
phrases) within a particular text segment which an experi-          W1, ..., WM is computed using the formula:
enced human coder has decided should be assigned con-
cepts. Let Ci denote the concept assigned to the ith word, Wi.      where log[x] denotes the logarithm base 2.
                                                                                                                    (        )
Let F be the complex proposition assigned to the concept                                      M                             
sequence C1, ..., CM.                                                I = −(1 / M ) log p(F ) ∏ p(C | C       , F )p W | C 
                                                                                                      i   i −1        i    i 
   After the human coder has completed the coding task,                                    i =1                            
AUTOCODER has stored the following items for the hu-                    Next, the complex proposition which was "most prob-
man coder. First, a concept dictionary consisting of the con-       able" (i.e., had the smallest information content score I) was
cepts created by the human coder. Second, a complex                 selected. Complex propositions whose information content
proposition dictionary consisting of the complex proposi-           exceeded some maximum critical value were discarded and
tions created by the human coder. Third, the percentage of          those text segments were defined as "incomprehensible" to
times that a particular complex proposition F has been used         AUTOCODER. This threshold was set sufficiently high,
(denoted by p(F) ). Fourth, the percentage of times that a          however, so that the occurrence of "incomprehensible"
word (or word phrase) Wi is used to express the concept Ci          complex propositions was very rare. Notice that unlike the
(denoted by p(Wi |Ci ) ) is computed (this is referred to as        usual HMM approach to syntactic and semantic parsing, a
the "emission probability" in the HMM literature). And              unique HMM is constructed for each complex proposition
fifth, the percentage of times that one concept follows an-         rather than trying to construct a general HMM applicable to
other concept given a particular complex proposition F (de-         all possible complex propositions which could occur in the
noted by p(Ci+1 |Ci , F) ) (this is referred to as the ''transition text.
probability" in the HMM literature). Given the usual condi-
tional independence assumptions of an HMM, these statis-            Procedure
tics in conjunction with the concept and complex proposi-
tion dictionaries correspond to a particular type of probabil-         Three human coders jointly coded the recall data from the
istic theory of how the human coder codes the recall data.          training data set using AUTOCODER. The human coders
   For example, consider the text segment "He buried his            were careful not to examine the test data, so the dictionaries
life savings deeply in the ground". The human coder might           created as a result of coding the training data were likely to
choose to model this text segment as an ordered sequence of         not contain all concepts, complex propositions, and statistics
word phrases: (W1="He", W2 = "buried", *, W3 = "life sav-           necessary to code the test data set. Text segments in the test
ings", *, *, *, *) might be associated with the ordered se-         data were then identified by the three human coders as well.
quence of concepts: (C1="MISER", C2="BURY", *, C3 =                 AUTOCODER then assigned the "most probable" complex
"GOLD", *,*,*,*) where the notation * is used to refer to a         proposition to each text segment using the information con-
word (or word phrase) which is not assigned a concept for           tent score described in the previous section. The three hu-
the purposes of coding the protocol data. The complex               man coders then coded the test data without the use of
proposition F="BURY(MISER, GOLD)" would be as-                      AUTOCODER and measures of agreement between
signed to the concept sequence (C1="MISER",                         AUTOCODER's performance and the human coder per-
C2="BURY", *, C3 = "GOLD", *,*,*,*).                                formance on the test data set were recorded.
   Once the assignments have been made, statistics are com-
puted. Specifically, the probability that one concept follows                         Results and Discussion
another given a particular complex proposition (e.g.,                  In order to compare performance of AUTOCODER and
P(BURY|MISER, BURY(MISER,GOLD)) is estimated                        the human coder on the test data set, three different meas-
from the observed relative frequencies. In addition, the            ures of agreement were used. All measures were computed
probability of a word given a concept is estimated (e.g.,           individually for each text across all relevant subject data. It
P("life savings"| GOLD)). The probability that a given              is important to emphasize that AUTOCODER always codes
complex proposition is used is also estimated from the              the same set of protocol data in exactly the same manner
coder's behavior (e.g., P(BURY(MISER,GOLD)). Instead                with 100% reliability. Thus, the agreement measures actu-
of assigning a zero probability to transition and emission          ally are measures of the validity as opposed to the reliability
probabilities whose corresponding observed relative fre-            of AUTOCODER's coding performance.
quencies were equal to zero, a small "smoothing" probabil-
ity was used to facilitate processing of novel word se-             Agreement Measures
quences. Figure 2 shows a possible HMM representation for              The first measure was percent agreement which is de-
the complex proposition BURY(MISER,GOLD).                           fined as the percentage of times the two coders agree that a
                                                                    proposition was mentioned in the recall protocol plus the
Protocol Data Coding Algorithm                                      percentage of times the two coders agree that a proposition
   The Viterbi algorithm (Viterbi, 1967) as described in Al-        was not mentioned. One difficulty with the percent agree-
len (1995, p. 202) was then used to construct the "most             ment measure is that percent agreement can be artificially
probable" concept sequence associated with each possible            increased by simply increasing the number of complex

CogSci2000: Durbin, Earwood, and Golden                                                                                4
propositions in the proposition dictionary! Accordingly,             Table 2: Performance of Autocoder on Training Data
other agreement measures were considered.                                       (Sequential Agreement Measures)
   The second measure of agreement was Cohen's Kappa
score (Cohen, 1960) which essentially corrects for agree-                   Text         N       Percent
ment by chance. The formula for Cohen's Kappa is given                                           Agreement
by: κ=(p-pc)/(1-pc) where p is the percent agreement de-                    "Miser"      111     90%
scribed in the previous paragraph and pc is the expected                    "Cuckoo"     111     86%
agreement between the two coders if the coding strategy of                  "Doctor"     105     98%
one coder provided no information (i.e., was statistically                  "Eagle"      150     92%
independent of the coding strategy of the other coder). The
performance of the model for the percent agreement and
kappa agreement measures on the training data set is pro-        Analysis of Test Data
vided in Table 1. The quantity N denotes the number of              Tables 3 and 4 show the performance of AUTOCODER
opportunities for agreement. Typically, in the text compre-      on the test data set using the standard agreement measures
hension literature. Percent agreement scores for coding data     and the sequential agreement measure. As can be seen from
which are above 90% and kappa scores which are above             Tables 3 and 4, AUTOCODER's performance is almost
70% are deemed acceptable for publication. The data was          comparable to experienced human coders keeping in mind
also analyzed using a third more stringent agreement meas-       the limitation that the test data set was parsed into text seg-
ure we call sequential agreement. Sequential agreement is        ments corresponding to complex propositions by a human
typically not computed. But since the same coder has iden-       coder. On the other hand, the AUTOCODER methodology
tified the text segments in both the training and test data, the has the important advantage that it is entirely well-
percentage of times both the human coder and                     documented and can be reliably implemented by computer
AUTOCODER agreed upon the coding of a particular text            software (unlike coding schemes implemented by human
segment across recall protocols could be computed. This          coders).
coding criterion thus takes into account the sequential struc-
ture of the recall data unlike the previously described                 Table 3: Performance of Autocoder on Test Data
agreement measures which are typically reported in the lit-                      (Standard Agreement Measures)
erature.
                                                                            Text         N      Percent        Cohen
Analysis of Training Data                                                                       Agreement      Kappa
   Table 1 shows the performance of AUTOCODER on the                        "Miser"      192    83%            65%
training data set using standard agreement measures, while                  "Cuckoo"     336    88%            71%
Table 2 shows the performance of AUTOCODER using the                        "Doctor"     228    88%            75%
sequential agreement measure. As can be seen from Tables                    "Eagle"      384    84%            66%
1 and 2, AUTOCODER's performance clearly demonstrates
that it is picking up on a sufficient number of statistical
regularities from the skilled human coder's data to almost              Table 4: Performance of Autocoder on Test Data
completely reconstruct the skilled human coder's decisions.                     (Sequential Agreement Measures)
    Table 1: Performance of Autocoder on Training Data                      Text         N       Percent
                (Standard Agreement Measures)                                                    Agreement
                                                                            "Miser"      111     69%
          Text          N     Percent         Cohen                         "Cuckoo"     111     67%
                              Agreement       Kappa                         "Doctor"     105     76%
          "Miser"       192   95%             91%                           "Eagle"      150     68%
          "Cuckoo"      336   93%             84%
          "Doctor"      228   99%             97%                   To provide a qualitative feeling regarding AUTO-
          "Eagle"       384   97%             93%                CODER's performance. Table 5 shows AUTOCODER's
                                                                 "coding" of the protocol data of Subject 12 who was as-
                                                                 signed to the test data set.
                                                                    It is extremely encouraging (despite the simple texts con-
                                                                 sidered in this initial study) that the performance of the
                                                                 AUTOCODER algorithm was so effective on the test data.
                                                                 In almost all cases, AUTOCODER automatically and relia-
                                                                 bly coded the data at an almost publishable agreement level
                                                                 using completely documented and accessible algorithms.
                                                                 We are excited and pleased with these preliminary results
                                                                 even though the text segments in the test data had to be pre-

CogSci2000: Durbin, Earwood, and Golden                                                                     5
parsed by a human coder. Future work in this area is cur-
rently being pursued.                                                             References
                                                          Allen, J. (1995). Natural language understanding. Redwood
   Table 5: AUTOCODER's "coding" of novel recall data       City, CA: Benjamin/Cummings.
                                                          Charniak, E. (1993). Statistical language learning. Cam-
Human Recall Data                AUTOCODER                  bridge, MA: MIT.
                                 Interpretation           Cohen, J. (1960). A coefficient of agreement for nominal
"and he buried it in             BURY                       scales. Educational and Psychological Measurement, 20,
the ground"                      AGENT: MISER               37-46.
                                 OBJECT: GOLD             Earwood, J. (2000). AUTOCODER: An intelligent assistant
"and he went over                ATTEND                     for coding protocol data. Psychology Program Senior
every day to look at             AGENT: MISER               Honors Thesis. School of Human Development. Univer-
where the money was              OBJECT: GOLD               sity of Texas at Dallas. Richardson, TX.
where the lump of gold                                    Ericsson, K. & Simon, H. (1993). Protocol analysis: Verbal
was buried"                                                 reports as data. Cambridge, MA: MIT.
"and one day when the            ATTEND                   Golden, R. M. (1997). Causal network analysis validation
miser wasn't there a             AGENT: MISER               using synthetic recall protocols. Behavior Research
thief came and dug up            OBJECT: GOLD               Methods, Instruments, and Computers, 29, 15-24.
the gold"                        [Disagrees with          Graesser, A. & Franklin, S. (1990). Quest: A cognitive
                                 Human Coder!]              model of question-answering. Discourse Processes, 13,
"and so the miser goes           BURY                       270-304.
and he sees the hole             AGENT: MISER             Jelinek, F. (1997). Statistical methods for speech recogni-
in the ground"                   OBJECT: GOLD               tion. Cambridge, MA: MIT.
                                 [Disagrees with          Kintsch, W. (1998). Comprehension: A paradigm for cogni-
                                 Human Coder!]              tion. New York: Cambridge.
"and he's very upset             MISER                    Trabasso, T. & Magliano, J. (1996). Conscious understand-
by that"                         STATE: PLEASED             ing during comprehension. Discourse Processes, 21, 255-
                                 [Disagrees with            287.
                                 Human Coder!]            van den Broek, P. & Trabasso, T. (1986). Causal networks
"and a bystander tells           TELLS-INFO                 versus goal hierarchies in summarizing texts. Discourse
the miser to take a              FROM: NEIGHBOR             Processes, 9, 1-15.
rock and bury it in              TO: MISER                Viterbi, A. J. (1967). Error bounds for convolutional codes
the ground"                      INFO:BURY(STONE)           and an asymptotically optimal decoding algorithm. IEEE
"and the miser says              ATTEND                     Transactions on Information Theory, 13, 260-269.
why"                             AGENT: MISER
                                 OBJECT: GOLD
                                 [Disagrees with
                                 Human Coder!]
"and the bystander               TELLS-INFO
says well all you ever           FROM: NEIGHBOR
did was look at the              TO: MISER
ground anyway"                   INFO:ATTEND
                                 (MISER,GROUND)
"you never did use the           TELLS-INFO
gold"                            FROM: NEIGHBOR
                                 TO: MISER
                                 INFO:NOTUSE
                                 (MISER,GOLD)
"so there might as               TELLS-INFO
well be a rock there"            FROM: NEIGHBOR
                                 TO: MISER
                                 INFO:ASGOOD
                                 (STONE,GOLD)

CogSci2000: Durbin, Earwood, and Golden                                                                                6
                                                           :RUG%R[                                :RUG&RQQHFWRU
       3XOO'RZQ&RQFHSW0HQX                           3XOO'RZQ3URSRVLWLRQ0HQX                           7H[W6HJPHQW
                                                                                                             'HOLPLWHU
Figure 1. A portion of the AUTOCODER user-interface associated with the coding of the phrase "buried his life savings".
Each word in the text appears in a particular window called the word box. Word boxs can be connected to form word phrases
using the connector button. Beneath each word phrase is a pull-down concept menu. Another pull-down proposition menu
which lists the set of available complex propositions which can be assigned to the phrase is also displayed to the user. Both
concept and proposition menus provide facilities for the addition of new concepts and propositions by the skilled human
coder. Menu choices are made by a skilled human coder for the purposes of providing training data for the Hidden Markov
Models (HMMs). The HMMs are then used to automatically make "most probable" menu selections without the aid of a
skilled human coder through the use of the Viterbi algorithm for HMMs as described in the text.
                                       ³0LVHU´                   ³JROG´                ³WUHDVXUH´
                    ³KH´
                           0,6(5                       %85<                          *2/'
                                    ³EXULHG´                        ³SXWLQJURXQG´
Figure 2. Each complex proposition is represented by its own HMM (Hidden Markov Model). In this figure, the HMM for
the proposition BURY(MISER, GOLD) is graphically displayed. Transition probabilities are represented by solid arrows
while emission probabilities are represented by dashed arrows. Line thickness indicates the relative magnitude of the corre-
sponding transition or emission probability. Thus, the line thicknesses for the emission probability P(Word = "gold" | Con-
cept = GOLD) and transition probability P(Concept=GOLD | Concept = BURY, Proposition = BURY(MISER, GOLD))
are both much thicker than the line thicknesses for the emission probability P(Word = "Miser" | Concept = GOLD) and
transition probability P(Concept=BURY | Concept = GOLD, Proposition = BURY(MISER, GOLD)) .

