UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning-Based Constraints on Schemata
Permalink
https://escholarship.org/uc/item/4981z7ck
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Lane, Peter C.
Gobert, Fernand
Cheng, Peter C-H.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                                Learning-Based Constraints on Schemata
                                     Peter C.R. Lane (pcl@psychology.nottingham.ac.uk)
                                     Fernand Gobet (frg@psychology.nottingham.ac.uk)
                                    Peter C-H. Cheng (pcc@psychology.nottingham.ac.uk)
                               ESRC Centre for Research in Development, Instruction and Training,
                                         School of Psychology, University of Nottingham,
                                         University Park, NOTTINGHAM NG7 2RD, UK
                                                                  cific information, these slots will be filled with default val-
                           Abstract                               ues. So a room will, by default, be considered to have four
   Schemata are frequently used in cognitive science as a de-     walls, a ceiling, a door, lighting, probably a window, and so
   scriptive framework for explaining the units of knowledge.     forth.
   However, the specific properties which comprise a schema          Less committal is the definition by Rumelhart (1980;
   are not consistent across authors. In this paper we attempt    italics in original): “A schema theory is basically a theory
   to ground the concept of a schema based on constraints         about knowledge. It is a theory about how knowledge is
   arising from issues of learning. To do this, we consider the   represented and about how that representation facilitates the
   different forms of schemata used in computational models       use of the knowledge in particular ways.” Rumelhart there-
   of learning. We propose a framework for comparing forms        fore focuses on the form of the schema theory (representa-
   of schemata which is based on the underlying representa-
   tion used by each model, and the mechanisms used for
                                                                  tion and reuse), whereas Brewer (1999) defines the form of
   learning and retrieving information from its memory.           the schema (a molar form of knowledge). Rumelhart’s defi-
   Based on these three characteristics, we compare examples      nition is also echoed in that of Sweller (1988), whose con-
   from three classes of model, identified by their underlying    cern is with modelling problem-solving behaviour. Accord-
   representations, specifically: neural network, production-     ing to Sweller (1988), a schema is simply a “structure
   rule and symbolic network models.                              which allows problem solvers to recognize a problem state
                                                                  as belonging to a particular category of problem states that
                        Introduction                              normally require particular moves. ... certain problem states
                                                                  can be grouped, at least in part, by their similarity and the
One of the unifying themes in cognitive science is the use        similarity of the moves that can be made from those
of schemata for explaining the units of knowledge within          states.” Each of these definitions stresses the functionality of
humans. However, the specific properties which comprise a         the knowledge in the schema. Also worth noting is that the
schema usually vary between authors. Early work in the AI         schema is a form of retrieval structure, identifying elements
and cognitive traditions (e.g. Rumelhart, 1980) set the scene     from earlier experience which can be reused in the current
for the use of schemata in computational models of learning.      situation.
It is now appropriate, with a number of successful models in         Our interest in this paper is in describing computational
the literature, to see what forms of schemata arise within a      models of learning, and for this purpose, as will become
learning-based system. This question is especially interest-      evident later, a fairly loose definition of schemata is required
ing because computational models do not simply implement          to provide the basis of comparison between different models.
basic concepts such as schemata with an added learning            Hence, we will use the following definition:
mechanism. Instead, each computational model is based on
some core representational structure and primitive learning          A schema is a cognitive structure for representing and re-
mechanisms, from which structures such as schemata may            trieving classes of typical situations for which a similar
be inferred.                                                      response is required of the learner.
   The aim of this paper is to consider examples from a
number of computational models and simply extract those              Our comparison looks at the variation in schema-form
elements which most relate to schemata. The difficulty here       based on the different assumptions underlying each model.
is that the models have not been tested on identical tasks,       The greatest assumption made is the basic representation
and so the comparison must be at a more qualitative level.        used by the model for storing learnt information in its
Hence, we begin with some informal definitions of schemata        memory. This representation may be highly structured, lo-
to define our analytical framework.                               calised or distributed. The type of the representation affects
                                                                  the processes which the model can use to learn, where learn-
           Learning and Using Schemata                            ing is the process of converting what has been experienced
Brewer (1999) defines schemata as “the psychological con-         into an internal representation. In this context, some repre-
structs that are postulated to account for the molar forms of     sentations provide better support for incremental real-time
human generic knowledge.” The idea is that knowledge of           learning, whereas others are better for complex rule induc-
visual scenes or discourse structure may be considered in         tion. The type of representation also affects the retrieval of
terms of basic units. For instance, house-scenes typically        information from the model’s memory for use in novel
consist of rooms, each room containing certain basic proper-      situations. Some systems assume that every item of mem-
ties, such as walls or furniture. The schema for a room will      ory is compared to determine the closest match to the current
contain slots for the properties, and, in the absence of spe-     situation, whereas others maintain a hierarchy for indexing

their memory and consequently only search a subset of the        Representing a schema
total memory.                                                    All knowledge contained within a neural network is held
   These three characteristics, for representing, learning and   implicitly across the weights within the network. Once acti-
retrieving a schema, provide a framework for analysing how       vation is presented on the input, every weight and node
different computational models address the questions of          within the network interact to generate an output. In this
learning and using schemata. We use this framework in the        situation, specific schemata are not really represented within
next three sections, where we compare examples from three        the network, in the sense of identifiable units, but instead
classes of model. The classes are distinguished by their un-     emerge as a consequence of the specific set of inputs. Hence,
derlying representations: neural network, production-rule and    the schemata used by the model cannot be extracted for use
symbolic network models. The examples are selected to be         as explicit rules, but instead must be inferred from their ef-
representative (without attempting to be comprehensive).         fects on the network’s output.
                Neural network models                            Learning a schema
The ability of a PDP (Parallel Distributed Processing) model     Given the nature of distributed representations, it is not pos-
(otherwise known as a neural network) to learn schemata          sible to learn about just one schema, because of the unpre-
was addressed at an early stage by Rumelhart, Smolensky,         dictable effect on other information held in the weights. In-
McClelland and Hinton (1986), who described how such             deed, the process by which the SG model (and most similar
properties can arise within a class of PDP models. However,      neural network models) is trained involves continuous
they did not address the question of learning. A better dem-     passes of the entire training dataset whilst the weights in the
onstration of these ideas within the context of a learning       network are gradually altered to approximate the mapping
system is the Sentence Gestalt (SG) model of St. John and        between the input data and its target output. This process
McClelland (1990). We also consider the CLARION system           means that the network captures generalisations true of the
of Sun, Merrill and Peterson (in press), which is a hybrid       entire dataset, making it robust in novel situations.
model of skill learning.
                                                                 Retrieving a schema
Sentence comprehension
                                                                 Again, the nature of the distributed representation within the
The aim of the SG model (St. John & McClelland, 1990) is         model implies that the whole network is activated when
to capture the process by which people fill out semantic         obtaining a response to a novel input. Hence every piece of
information whilst reading a sentence. For example, given        acquired information (every weight value) is used in generat-
the sentence ‘Bobby pounded the board together with nails’,      ing a response. This process additionally ensures a robust
the inference “with a hammer” is made automatically. We          response in novel but similar situations, because the re-
can explain such behaviour by hypothesising that people          trieval process is based on the similarity between the novel
recall (subconsciously) some schema for the sentence from        input and the model’s previous experience. For instance, if a
which default information (the hammer) can be inferred. The      large number of examples are presented to the network, and
SG model attempts to account for such phenomena. It con-         the responses analysed, it will be seen that those examples
sists of a two-stage recurrent neural network. The first stage   which are most similar tend to generate similar responses.
learns a distributed representation for the sentence, called the Conversely, if a novel input is partly similar to one type of
sentence gestalt, from a temporal sequence of constituents.      example in the training data, and partly similar to another
Each constituent is either a simple noun phrase, a preposi-      type, the computed response will fall somewhere between
tional phrase or a verb. The second stage acts as a probe for    that for the two items of training data. Note that the similar-
information contained in the sentence gestalt. Each probe is     ity in input to the network is heavily dependent on the form
a role/filler pair, and the sentence gestalt is probed by pre-   of encoding used for representing each item of data to the
senting either a role or a filler, from which the network is to  network on numeric input units.
supply the complete pair. Requested information need not
refer directly to words in the sentence. For example, after      Bottom-up skill learning
seeing ‘Mary ate the spaghetti’, the model should return the
filler “fork” for the role “instrument”.                         CLARION (Sun, Merrill & Peterson, in press) is a hybrid
   The experiments performed by St. John and McClelland          model for bottom-up skill learning. It is designed to model
demonstrate that the SG model successfully assigns con-          the process by which low-level perceptual-motor skills are
stituents to thematic roles based on syntactic and semantic      converted into explicit rules, and also capture the interaction
constraints. Further, the model can disambiguate meanings        between these two levels of knowledge whilst carrying out a
and instantiate vague terms as appropriate to their context      complex task. CLARION assumes that declarative knowl-
and the training data previously seen by the model. This         edge is represented explicity within a rule-based system,
behaviour fulfills the requirements for schemata as discussed    whereas procedural knowledge is represented implicity
previously: the model classifies sentences into various          within a neural network. CLARION has been tested in a
groups, and these groups can have variable or default infor-     perceptual-motor task involving navigation through a mine-
mation associated with them.                                     field, in which the model must learn to react to particular
   We can now consider the schemata used in SG against the       visual patterns of mines with appropriate navigation instruc-
three basic characteristics of our framework:                    tions to avoid the mines and reach a target. The dual use of
                                                                 knowledge is reflected in subjects’ responses: mostly they

react instinctively, but after some experience in the domain     based metric to generalise to novel situations. This is natu-
some explicit planning is reported. CLARION’s use of two         ral in the chosen domain, where all inputs are visual scenes;
knowledge levels is intended to capture this shift towards       the rules basically contain a localist representation of in-
more explicit knowledge.                                         formation similar to that in the neural network.
   The novelty in CLARION is that the rules can either be
pre-programmed (i.e. taught in the standard top-down man-        Summary
ner) or learnt based on the low-level knowledge in the neural    The form of schemata possible in these neural network mod-
network. Specifically, if the neural network suggests an         els is determined partly by their learning mechanisms and
action which satisfies its criterion for success, then the cur-  partly by their retrieval mechanisms. The basic neural net-
rent sensory state is turned into the condition part of a new    work is capable of learning complex mappings from the
production in the rule set, with its action part being the cur-  input to output data, and inherent mechanisms within the
rently suggested action. Further learning processes on the       neural network are used to retrieve information most similar
rules update statistics and may refine and alter rules for effi- to the current situation. In CLARION, situations may be
ciency. CLARION therefore contains two independent learn-        learnt explicitly with specific rules consisting of core and
ing mechanisms, but the two can also work together with an       variable information.
interesting transfer of bottom-up (procedural) knowledge into
the explicit rule-set. As with SG, schemata are evident in                      Production-rule models
the similarity-based generalisations made by the model.
                                                                 Production rules have been a popular representation for a
                                                                 number of computational models, two notable examples
Representing a schema
                                                                 being Soar (Laird, Newell & Rosenbloom, 1987) and ACT-
CLARION uses a two-level representational structure: a           R (Anderson & Lebiere, 1998). However, such models are
rule-based system and a feed-forward neural network. As with     also difficult to discuss in our framework, as their inherent
SG, schemata are seen to emerge through the interaction of       power makes them suitable for application in a wide range
many elements in the model. Hence, the network and the           of domains and settings, as well as for testing various theo-
rules can generalise robustly to novel situations based on       ries of learning: there are few architectural constraints which
partial similarity. The purpose of the rule-based system is to   have a significant bearing on the forms of knowledge learnt.
‘fix’ the generalisations learnt by the neural network and       Here, we describe the generic learning and retrieval mecha-
prevent later experience ‘blurring’ them. These rules may in     nisms in Soar.
themselves represent broader classes of situation, because
some of the attributes can have variables instead of specific    Soar: chunking of productions
values, rather akin to slots on a more generic template.
                                                                 The Soar system integrates perceptual-motor behaviour with
                                                                 basic capabilities for learning and problem solving. All
Learning a schema                                                knowledge within Soar is held in the form of productions,
The procedural knowledge in CLARION is learnt in a simi-         with a working memory holding specific attributes and their
lar manner to the SG model described above, using a modi-        values. Soar operates in a cycle, attempting to satisfy some
fied form of backpropagation: an additional reinforcement        goal within its working memory. This cycle takes the con-
term is included in the training error because the correctness   tents of working memory and matches it to productions in
of a specific action is only known at the completion of the      its knowledge-base. These matching productions place new
task. The rule-based declarative knowledge includes mecha-       goals or other elements into working memory (this is
nisms for constructing new rules, or expanding or shrinking      known as the elaboration phase, which proceeds until all
the conditions of existing rules. The mechanism for con-         eligible productions have fired, quiescence), and then a deci-
structing a new rule is merely to include, for a successful      sion is made as to which of the new goals to pursue next.
action, the situation and action as the condition and action
parts of a new rule. Expanding or shrinking a rule’s condi-      Representing a schema
tions amounts to increasing or decreasing the likelihood of
                                                                 All behaviour within Soar is goal oriented, in the sense that
the rule matching future inputs by altering the range of pos-
                                                                 the system is always trying to satisfy some goal or another.
sible values in one of its attributes. Before making any such
                                                                 Each goal contains three slots: the current problem space,
changes to a rule’s conditions, an information gain for each
                                                                 state and operator. The specific representations for informa-
rule is computed to determine whether a modified version
                                                                 tion in these slots can vary across applications. A particular
would do better than the current rule.
                                                                 schema may not be represented specifically in a production,
                                                                 but instead, in a specific context, a number of similar rules
Retrieving a schema                                              will be matched, suggesting interrelated subgoals, and so
As with SG, the whole of CLARION’s memory is probed              yield the effect of a schema.
simultaneously to determine all information relating to the
current situation. The possible actions suggested by the         Learning a schema
separate procedural and declarative levels are then chosen
                                                                 Learning within Soar is based on a chunking process that
through a weighted competition, reflecting the degree of
                                                                 creates new rules. Each rule recreates the results of subgoals
emphasis CLARION is placing on each type of knowledge.
                                                                 in relevantly similar future situations (Laird, Rosenbloom &
Note that in both levels CLARION relies on a similarity-

Newell, 1986). Chunking relies on an analysis of the de-            A template is created in the following manner. During
pendencies within the solution to a given subgoal to create     training, CHREST (just like EPAM) builds a discrimination
new rules. A new rule is created for each independent result,   network of chunks of information. Specific to CHREST is
with a condition relating to the dependency analysis of the     the ability to create lateral links (Gobet, 1996): in this case,
subgoal, and an action relating to the specified subgoal. This  similarity links. These similarity links can be used whilst
chunking mechanism is a universal learning mechanism,           searching the network to suggest chunks not directly linked
similar to explanation-based learning (see Rosenbloom &         by the tests in the network. However, the novel aspect of
Laird, 1986). The interesting facet of learning within Soar is  this is that a node can reorganise information in similar
its ability to focus on those aspects of the situation used for chunks (satisfying an overlap criterion) into a template. This
problem solving, and to use only these relevant aspects in      template contains a core pattern, based on the original
chunking. This focus ensures that the chunks learnt by Soar     chunk, and a set of slots, for the information which varied
will generalise to novel situations. In addition, Soar has a    across the associated chunks.
process of variabilisation, in which information is made as
general as possible before it is stored as a chunk in a produc- Representing a schema
tion.                                                           CHREST represents all information as chunks within nodes
                                                                in a discrimination network: a chunk is a familiarised pat-
Retrieving a schema                                             tern. Nodes are linked by test links, which require some fea-
The retrieval mechanisms within Soar operate only in its        tures to be matched on traversal. Some of the nodes in the
elaboration phase, in which “all directly available knowledge   network contain templates, where a template contains a core
relevant to the current situation is brought to bear” (Laird,   chunk and a number of slots. However, CHUMP (Gobet &
Rosenbloom & Newell, 1987). In this phase, every produc-        Jansen, 1994) and CHREST+ (Lane, Cheng & Gobet, 2000)
tion in its memory whose condition directly matches some-       additionally allow nodes in the network to be associated with
thing in the working memory is activated, and its suggested     information about possible moves or problem solutions,
subgoals and other information are added to memory. Match-      allowing CHREST to learn to solve problems.
ing productions against working memory is based on the
similarity of the attributes and their values.                  Learning a schema
                                                                The discrimination network within CHREST is learnt
Summary                                                         through four learning mechanisms. Beginning from the root
Just as with neural networks, no specific structure corre-      node, CHREST sorts a novel pattern through the network
sponding to a schema exists in Soar. However, the basic         until no further test links can be applied. At the node
learning mechanism within Soar, chunking, does limit the        reached, two things can occur. First, the pattern may match
form and content of learnt productions. Firstly, productions    the chunk, in which case more information can be added to
are retrieved based on their similarity to items in working     the chunk from the pattern (familiarisation). Second, the
memory. The features placed within a production are taken       pattern may mismatch the chunk, in which case a further
from the set of dependent relations in the attainment of a      test link and node are created based on the mismatching fea-
goal. In addition, some variabilisation can occur on the fea-   tures (discrimination). The third learning mechanism con-
tures.                                                          structs similarity links between two nodes when their
                                                                chunks have at least 3 identical items. Finally, for a node
              Symbolic network models                           with at least 5 similarity links satisfying an overlap crite-
This section considers a pair of models which construct         rion, the chunk may be replaced by a template. This tem-
symbolic networks of symbol-level information within a          plate uses the existing chunk as its core, and the varying
hierarchy. Each of these lays some claims to universality of    information across the other nodes as its slots.
application, but have currently only demonstrated good re-
sults in one or two areas. The first is the CHREST model,       Retrieving a schema
which learns about chess patterns, and the second is            Retrieving knowledge within CHREST is achieved simply
EUREKA, which learns about physics problems.                    by following the test links from the root node, applying the
                                                                tests to the target pattern until no further test applies. The
CHREST: storing chunks into templates                           chunk at the node reached is the retrieved schema.
The CHREST (Chunk Hierarchy and REtrieval STructure)
model of expertise (Gobet & Simon, in press) is a recent        EUREKA: restructuring knowledge
development of EPAM (Elementary Perceiver and Memo-             EUREKA (Elio & Sharf, 1990) demonstrates how an effec-
riser) (Feigenbaum & Simon, 1984). The learning processes       tive organisation for large amounts of domain-specific
in EPAM include mechanisms for constructing a discrimina-       knowledge can support efficient recognition and application
tion network and incorporating information into it; the         of relevant knowledge to the problem at hand. Secondly, the
learnt information is known as chunks. CHREST includes          model demonstrates how the qualitative shift from novice to
extra mechanisms for learning templates (Gobet & Simon,         expert levels of knowledge and organisation can arise within
in press); it is this template which is of interest to us here, a learning framework. EUREKA uses a discrimination net-
as it possesses schema-like properties.                         work, rather like the CHREST model described above, but
                                                                instead of simple chunks, the nodes in EUREKA’s network

hold Memory Organization Packets (MOPs) (Schank, 1980).          from the P-MOP and included in the new organisation. This
Each MOP represents a complex knowledge structure hold-          process has the side-effect that partial solution methods may
ing generalised knowledge extracted from a group of individ-     reside on P-MOPs. A further reorganisation can occur in
ual experiences. Differences between experiences are encoded     cases where a descendant P-MOP covers most of the prob-
in the tests between the links in the discrimination network,    lem-solving experiences of its parent P-MOP; in such situa-
and so similar previous experiences are retrieved based on the   tions the organisation of the network is not efficient, and
features in the network which match the current experience.      one of the discriminating features might be better seen as a
   EUREKA has been applied to physics problems, and is           commonality.
initialised with a set of MOPs containing basic knowledge           These two learning mechanisms can lead the network to
about physics concepts, equations and inference rules. How-      focus on abstract features in the following way. A property
ever, this knowledge does not contain any information about      such as a force may not be represented within the problem
their usefulness or relevance in any particular type of prob-    statement, however, it will be referred to in the problem
lem. When EUREKA is given its first physics problems, it         solution. As problem-solving experiences are gathered, a
must use its basic knowledge in conjunction with a means-        number will be seen to include force within their solution,
ends problem-solving strategy to construct a solution. Hav-      and so this feature will become a norm within the P-MOP.
ing done this, EUREKA then places the entire problem and         From there, the feature may be used to discriminate between
its solution (features, inferences and solution steps) into a P- different P-MOPs, because it has been derived as a feature of
MOP (Problem MOP). This P-MOP is then stored in the P-           a number of problem-solving experiences.
MOP network, where some reorganisation of the network
may occur. When solving later problems, EUREKA can use           Retrieving a schema
information in a P-MOP in preference to its means-ends           Each P-MOP in EUREKA's memory is a separate schema,
analysis, which can lead to a shift in EUREKA’s problem-         and each is indexed through the P-MOP network. Any of the
solving strategy towards a greater use of important abstract     features in the initial problem representation can serve as
physics concepts, such as force or energy, usually not pre-      indices into the P-MOP network. Whenever the feature ap-
sent in the problem statement. Also, the use of a P-MOP          pears as a difference in the P-MOP, the corresponding index
instead of means-ends analysis means the model begins to         is traversed. If a number of indices may be traversed, then
solve problems working forwards from the given informa-          EUREKA prefers the index leading to the P-MOP that or-
tion instead of backwards from the target, in accordance with    ganises the most problem-solving experiences. Hence,
observed differences between novice and expert problem           EUREKA is directed preferentially to patterns that recur
solvers (cf. Koedinger & Anderson, 1990; Larkin, McDer-          most often. During the traversal, EUREKA will apply the
mott, Simon & Simon, 1980).                                      inference rules of any P-MOPs that match the current situa-
                                                                 tion; this process will alter the current situation (the set of
Representing a schema                                            equations and unknowns) and so affect the further traversal of
EUREKA stores information in a network of P-MOPs. At             the P-MOP network. Note that EUREKA's bias towards P-
the root of the network is a P-MOP representing a “generic       MOPs which organise larger numbers of problem-solving
physics problem”. Each P-MOP contains several elements:          experiences means that P-MOPs arising from reorganisation
firstly, a set of norms represent the features which a problem   of the network will be preferred during problem solving. It
must satisfy for this P-MOP to apply; secondly, a set of         is this bias which ensures EUREKA will preferentially use
indices (links) to other P-MOPs, with the index specifying       P-MOPs emphasising the presence of forces or abstract enti-
the feature(s) which distinguish between them; thirdly, the      ties: as discussed above, such P-MOPs are formed from the
P-MOP includes a general inference rule; fourthly, the P-        aggregate of several more concrete P-MOPs, and so organise
MOP includes a specific solution method for carrying out         a larger number of problem-solving experiences.
the inference rule; and fifthly, the P-MOP includes a count
for the number of problem-solving experiences which it           Summary
organises (i.e. has matched in the past). The P-MOP repre-       The symbolic network models are closer to the spirit of tra-
sentation is a clear example of an explicit schema, with the     ditional schemata theories. In particular, there is a close cor-
norms indicating the class of similar problems to which its      respondence between the information in a P-MOP or the
inference rules will apply.                                      pairing of problem and solution nodes within CHREST, and
                                                                 the schemata discussed in Koedinger and Anderson (1990).
Learning a schema                                                Both models can use information to partially match a current
EUREKA's learning mechanisms operate through a process           situation. However, different learning mechanisms encode
of reorganisation. Once a problem has been solved, every-        different kinds of information in their nodes; CHREST re-
thing about the problem and its solution is collected into a     stricting itself to perceptual similarity, with EUREKA infer-
problem-solving experience. This experience is then com-         ring more abstract quantities for use in discrimination.
pared with the existing P-MOP retrieved from the network.
If any of the norms differ between the P-MOP and the expe-                                Conclusion
rience, these are removed from the P-MOP and used as indi-       This paper has taken an inductive approach to the question
ces to new organisation beneath this P-MOP; any inference        of how to learn schemata by applying an analytical frame-
rules referring to these differing norms are also removed        work to a number of computational models, and describing

the ways in which these models represent, learn and retrieve   Koedinger, K. R., & Anderson, J. R. (1990). Abstract plan-
schemata. Our aim has been to uncover, from existing mod-        ning and perceptual chunks: Elements of expertise in ge-
els, the origins of constraints on the possible forms of         ometry. Cognitive Science, 14, 511-550.
schemata. From our analysis we can see some similarities       Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987).
across all the models. Firstly, all use a distributed form of    Soar: An architecture for general intelligence. Artificial In-
representation, in the sense that schemata for novel situa-      telligence, 33, 1-64.
tions will usually arise from a number of partial matches,     Laird, J. E., Rosenbloom, P. S., & Newell, A. (1986).
although the symbolic network models possess more ex-            Chunking in Soar: The anatomy of a general learning
plicit schema-like structures. Secondly, all use a similarity-   mechanism. Machine Learning, 1, 11-46.
based form of retrieval, differing in the features which may   Lane, P. C. R., Cheng, P. C-H., & Gobet, F. (2000).
be used for discrimination. In particular, EUREKA allows         CHREST+: Investigating how humans learn to solve
abstract features (not perceptually obvious) to become sig-      problems using diagrams. AISB Quarterly, 103, 24-30.
nificant.                                                      Larkin, J. H., McDermott, J., Simon, D. P., & Simon, H.
   However, the differences in behaviour of the various mod-     A. (1980). Models of competence in solving physics
els are largely down to their specific learning mechanisms.      problems. Cognitive Science, 4, 317-345.
As stated in the introduction, the motivation for these mod-   Rosenbloom, P. S., & Laird, J. E. (1986). Mapping expla-
els has not been to learn schemata, as such, but instead to      nation-based generalization onto Soar. Proceedings of the
learn effectively in general situations. We therefore conclude   Fifth National Conference on Artificial Intelligence (pp.
that, for the purposes of developing a more meaningful defi-     561-567). Philadelphia, PA: MIT Press.
nition of schemata, we should begin by analysing the avail-    Rumelhart, D. E. (1980). Schemata: The building blocks of
able range of learning mechanisms in models such as those          cognition. In R.J. Spiro, B.C. Bruce and W.F. Brewer
referred to here. These learning mechanisms should be ex-          (Eds.) Theoretical Issues in Reading Comprehension
plored in their cognitive implications. For instance, the use      (Lawrence, Erlbaum), pp. 33-58.
of seriality or resource bounds, the malleability of learnt    Rumelhart, D. E., Smolensky, P., McClelland, J. L. &
features and how wide-ranging any changes to previous              Hinton, G. E. (1984). Schemata and sequential thought
knowledge may be. Most of these properties will come di-           processes in PDP models. In D. E. Rumelhart & J. L.
rectly from the learning mechanisms, whereas others will be        McClelland (Eds.) Parallel Distributed Processing, Vol,
imposed by the interaction of the learning mechanisms with         II. MIT Press, Cambridge, MA.
the other properties of the system, such as its use of percep- St. John, M. F., & McClelland, J. L. (1990). Learning and
tual-motor stimuli. Once these properties have been under-         applying contextual constraints in sentence comprehen-
stood, the use of schemata for describing the units of knowl-      sion. Artificial Intelligence, 46, 217-257.
edge within humans will become grounded in the processes       Schank, R. C. (1980). Language and memory. Cognitive
by which that knowledge has been learnt.                           Science, 4, 243-284.
                                                               Sun, R., Merrill, E. & Peterson, T. (in press). From im-
                        References                                 plicit skills to explicit knowledge: A bottom-up model
Anderson, J. R., & Lebiere, C. (1998). The atomic compo-           of skill learning. Cognitive Science.
   nents of thought (Lawrence Erlbaum).                        Sweller, J. (1988). Cognitive load during problem solving:
Brewer, W. F. (1999). Schemata. In R. A. Wilson & F. C.          Effects on learning, Cognitive Science, 12, 257-285.
   Keil (Eds.) MIT Encyclopedia of the Cognitive Sciences,
   pp. 729-730.
Elio, R. & Scharf, P. B. (1990). Modeling novice-to-expert
   shifts in problem-solving strategy and knowledge organi-
   zation. Cognitive Science, 14, 579-639.
Feigenbaum, E. A., & Simon, H. A. (1984). EPAM-like
   models of recognition and learning. Cognitive Science, 8,
   305-336.
Gobet, F. (1996). Discrimination nets, production systems
   and semantic networks: Elements of a unified framework.
   Proceedings of the Second International Conference of the
   Learning Sciences (pp. 398-403). Evanston, III: North-
   western University.
Gobet, F. & Jansen, P. (1994). Towards a chess program
   based on a model of human memory. In H. J. van den
   Herik, I. S. Herschberg, & J. W. Uiterwijk (Eds.), Ad-
   vances in Computer Chess 7. Maastricht: University of
   Limburg Press.
Gobet, F. & Simon, H. A. (in press). Five seconds or
   sixty? Presentation time in expert memory. Cognitive
   Science.

