UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Subjacency Constraints without Universal Grammar: Evidence from Artificial Language
Learning and Connectionist Modeling
Permalink
https://escholarship.org/uc/item/4zx1k0pp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Ellefson, Michelle R.
Christiansen, Morten H.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                               Subjacency Constraints without Universal Grammar:
              Evidence from Artificial Language Learning and Connectionist Modeling
                                            Michelle R. Ellefson (ellefson@siu.edu )
                                          Morten H. Christiansen (morten@siu.edu)
                                                      Department of Psychology
                                               Southern Illinois University - Carbondale
                                                   Carbondale, IL 62901-6502 USA
                              Abstract                               These mechanisms presumably also underwent changes after
   The acquisition and processing of language is governed b y        the emergence of language, but the selective pressures are
   a number of universal constraints, many of which undoubt-         likely to have come not only from language but also from
   edly derive from innate properties of the human brain.            other kinds of complex hierarchical processing, such as the
   However, language researchers disagree about whether
   these constraints are linguistic or cognitive in nature. In
                                                                     need for increasingly complex manual combinations follow-
   this paper, we suggest that the constraints on complex            ing tool sophistication. Thus, many language universals
   question formation, traditionally explained in terms of the       may reflect non-linguistic, cognitive constraints on learning
   linguistic principle of subjacency, may instead derive from       and processing of sequential structure rather than innate UG.
   limitations on sequential learning. We present results from          This perspective on language evolution also has important
   an artificial language learning experiment in which sub-          implications for current theories of language acquisition and
   jects were trained either on a “natural” language involving       processing. It suggests that many of the cognitive con-
   no subjacency violations, or an “unnatural” language that         straints that have shaped the evolution of language are still
   incorporated a limited number of subjacency violations.           at play in our current language ability. If this is correct, it
   Although two-thirds of the sentence types were the same           should be possible to uncover the source of some linguistic
   across both languages, the natural language was acquired
   significantly better than its unnatural counterpart. The
                                                                     universal in human performance on sequential learning
   presence of the unnatural subjacency items negatively af-         tasks. Christiansen (2000; Christiansen & Devlin, 1997)
   fected the learning of the unnatural language as a whole.         has previously explored this possibility in terms of a se-
   Connectionist simulations using simple recurrent net-             quential learning explanation of basic word order universals.
   works, trained on the same stimuli, replicated these results.     He presented converging evidence from theoretical considera-
   This suggests that sequential constraints on learning can         tions regarding rule interactions, connectionist simulations,
   explain why subjacency violations are avoided: they make          typological language analyses, and artificial language learn-
   language more difficult to learn. Thus, the constraints o n       ing in normal adults and aphasic patients, corroborating the
   complex question formation may be better explained i n            idea of cognitive constraints on basic word order universals.
   terms of innate cognitive constraints, rather than linguis-          In this paper, we take a similar evolutionary approach to
   tic constraints deriving from an innate Universal Grammar.
                                                                     one of the classic linguistic universals: subjacency. We first
                                                                     briefly discuss some of the linguistic data that have given
                          Introduction                               rise to the subjacency principle. Next, we present an artifi-
One aspect of language that any comprehensive theory of              cial language learning experiment that investigates our hy-
language must explain is the existence of linguistic univer-         pothesis that limitations on sequential learning rather than
sals. The notion of language universals refers to the observa-       an innate subjacency principle provide the appropriate con-
tion that although the space of logically possible linguistic        straints on complex question formation. Finally, we report
subpatterns is vast; the languages of the world only take up         on a set of connectionist simulations in which networks are
a small part of it. That is, there are certain universal tenden-     trained on the same material as the humans, and with very
cies in how languages are structured and used. Theories of           similar results. Taken together, the results from the artifi-
language evolution seek to explain how these constraints             cial language learning experiment and the connectionist
may have evolved in the hominid lineage. Some theories               simulations support our idea that subjacency violations are
suggest that the evolution of a Chomskyan Universal                  avoided, not because of an innate subjacency principle, but
Grammar (UG) underlies these universal constraints (e.g.,            because of cognitive constraints on sequential learning.
Pinker & Bloom, 1990). More recently, an alternative per-
spective is gaining ground. This approach advocates a refo-                              Why Subjacency?
cus in evolutionary thinking; stressing the adaptation of lin-
                                                                     According to Pinker and Bloom (1990), subjacency is one of
guistic structures to the human brain rather than vice versa
(e.g., Christiansen, 1994; Kirby, 1998). Accordingly, lan-           the classic examples of an arbitrary linguistic constraint that
guage has evolved to fit sequential learning and processing          makes sense only from a linguistic perspective. Informally,
mechanisms existing prior to the appearance of language.             the subjacency principle involves the assumption of certain

        S’                                                      principles governing the grammaticality of sentences. "Sub-
                                                                jacency, in effect, keeps rules from relating elements that are
                                                                ‘too far apart from each other’, where the distance apart is
 Comp         S
                                                                defined in terms of the number of designated nodes that are
                                                                between them" (Newmeyer, 1991, p. 12). Consider the fol-
                                                                lowing sentences:
       NP            VP
Sara                                                               1. Sara heard (the) news that everybody likes cats.
                                                                        N V Wh            N        V N
              V                 S’                                 2. What (did) Sara hear that everybody likes?
       heard                                                            Wh         N V Comp N                V
                                                                   3. *What (did) Sara hear (the) news that everybody likes?
                       Comp               S                             Wh          N V            N Comp N              V
                  that
                                                                   According to the subjacency principle, sentence 3 is un-
                                  NP              VP            grammatical because too many boundary nodes are placed
                        everybody                               between the noun phrase complement (NP-Comp) and its
                                                                respective 'gaps'.
                                           V              NP       The subjacency principle, in effect, places certain restric-
                                      likes        cats(what)
                                                                tions on the ordering of words in complex questions. The
          2a. Sara heard that everybody likes cats.             movement of wh-items (what in Figure 1) is limited as far
          2. What (did) Sara hear that everybody                as the number of so-called bounding nodes that it may cross
              likes?                                            during its upward movement. In Figure 1, these bounding
                                                                nodes are the S and NP’s that are circled. Put informally, as
                                                                a wh-item moves up the tree it can use comps as temporary
                                                                “landing sites” from which to launch the next move. The
         S’                                                     subjacency principle states that during any move only a sin-
                                                                gle bounding node may be crossed. Sentence 2 is therefore
                                                                grammatical because only one bounding node is crossed for
 Comp         S                                                 each of the two moves to the top comp node. Sentence 3 is
                                                                ungrammatical, however, because the wh-item has to cross
         NP         VP                                          two bounding nodes—NP and S—between the temporary
     Sara                                                       comp landing site and the topmost comp.
                                                                   Not only do subjacency violations occur in NP-
              V              NP                                 complements, but they can also occur in Wh-phrase com-
       heard                                                    plements (Wh-Comp). Consider the following examples:
                     NP                S’
              (the) news                                           4. Sara asked why everyone likes cats.
                                                                        N V              N Comp N             V N
                                Comp           S                   5. Who (did) Sara ask why everyone likes cats?
                          that                                          Wh          N V Wh N                V N
                                                                   6. *What (did) Sara ask why everyone likes?
                                       NP            VP                 Wh           N V Wh           N         V
                               everybody
                                               V           NP   According to the subjacency principle, sentence 6 is un-
                                            likes    cats(what) grammatical because the interrogative pronoun has moved
                                                                across too many bounding nodes (as was the case in 3).
          1.  Sara heard (the) news that everybody                 In the remainder of this paper, we explore an alternative
              likes cats.                                       explanation of the restrictions on complex question forma-
          3.  * What (did) Sara hear (the) news that            tion. This alternative explanation suggests that subjacency
              everybody likes?                                  violations are avoided, not because of a biological adaptation
                                                                incorporating the subjacency principle, but because language
                                                                itself has undergone adaptations to root out such violations
                                                                in response to non-linguistic constraints on sequential learn-
 Figure 1. Syntactic trees showing grammatical (2) and          ing
 ungrammatical (3) Wh-movement.

   Table 1. The Structure of the Natural and Unnatural Languages (with Examples)
                            NAT                                                           UNNAT
   Sentence                      Letter String Example         Sentence                             Letter String Example
   1. N V N                        ZVX                          1. N V N                             ZVX
   2. Wh N V                       QZM                          2. Wh N V                            QZM
   3. N V N comp N V N             QXMSXV                       3. N V N comp N V N                  QXMSXV
   4. N V Wh N V N                 XMQXMX                       4. N V Wh N V N                      XMQXMX
   5. Wh N V comp N V              QXVSZM                       5*. Wh N V N comp N V                QXVXSZM
   6. Wh N V Wh N V N              QZVQZVZ                      6*. Wh N V Wh N V                    QZVQZV
Note: Nouns (N) = {Z, X}; Verbs (V) = {V, M}; comp = S; Wh = Q.
                                                                    SUB and 20 GEN training strings were created for each lan-
          Artificial Language Experiment                            guage.
Artificial language learning has been shown to be an effec-
tive tool in the understanding of the acquisition of language       Test Stimuli An additional set of novel letter strings was
(e.g., Gomez & Gerken, 1999; Saffran, Aslin, & Newport,             created for the test session. For each language there were 30
1996). More recently, artificial language learning has been         grammatical items and 30 ungrammatical items. Twenty-
used to explore how languages themselves may have                   eight novel SUBs were constructed. For these unique SUB
evolved in the human species (Christiansen, 2000).                  letter strings there were 14 each, of grammatical and un-
                                                                    grammatical complement structures. Grammaticality in both
Subjects                                                            languages was based on what the grammar for that condition
Sixty undergraduates were recruited from an introductory            specified as legal sentences (Table 1)—not by what may be a
psychology class at Southern Illinois University, and earned        grammatical/ungrammatical sentence in English. Thus, for
course credit for their participation.                              the UNNAT language, the ungrammatical SUBs (from the
                                                                    viewpoint of English) were scored as grammatical and the
Materials                                                           grammatical SUBs (from the viewpoint of English) were
We created two artificial languages, natural (NAT) and un-          scored as ungrammatical. Grammaticality in the NAT lan-
natural (UNNAT). Each artificial language consisted of a set        guage corresponded to English, with grammatical SUBs
of letter strings. The letters in the strings each represented a    scored as grammatical and ungrammatical SUBs scored as
specific grammatical class (see Table 1). The letters Z and         ungrammatical. Testing in both groups also included 16
X represented nouns. V and M stood for verbs. The letter S          novel grammatical GEN items and 16 novel ungrammatical
designated a complementizer. Interrogative pronouns were            GEN items in which one of the letters, except those in the
denoted by the letter Q. These strings were constructed             first and last position, were changed.
based on the sentence structure of the six examples discussed          Previous artificial language learning research has estab-
above. Unique letter strings were created for training and          lished that distributional “surface” information, computed
testing sessions.                                                   over fragments consisting of two or three consecutive letters
                                                                    (bigrams/trigrams), may affect how well a language is
                                                                    learned. In order to ensure that the NAT language was not
Training Stimuli Twenty letter strings, 10 of each for
                                                                    more “regular” than the UNNAT language, in terms of dis-
NAT and UNNAT, were created to represent grammatical and            tributional information, and therefore potentially easier to
ungrammatical complex question formation structures                 learn, we controlled our stimuli for five different kinds of
(SUB). The grammatical SUB items used for the NAT train-            fragment information.
ing, while the ungrammatical SUB items were used for                   1) Associative chunk strength is measured as the sum of
UNNAT training. Examples of SUB letter strings for both             the frequency of occurrence in the training items of each of
conditions can be seen in Table 1 as sentences 5 and 6.             the fragments in a test item, weighted by the number of
   An additional 20 general training items were constructed         fragments in that item (Knowlton & Squire, 1994). E.g.,
to represent general grammatical structures (GEN) that do           the associative chunk strength of the item ZVX would be
not involve subjacency. These items were the same for both          calculated as the sum of the frequencies of the fragments ZV,
languages. Examples of GEN letter strings for both condi-           VX and ZVX divided by 3. Two-tailed t-tests indicated that
tions are sentences 1 through 4 in Table 1. In summary, 10          there were no differences between the languages in associa-
                                                                    tive chunk strength for the grammatical (t<1) and the un-
                                                                    grammatical (t<1) items.

   2) Anchor strength is measured as the relative frequency of
initial and final fragments in similar anchor positions in the                             75
training items (Knowlton & Squire, 1994). E.g., the anchor                                 70
                                                                    Percent Correct
strength of the item QXMSXV is calculated as the sum of                                    65
the frequencies of the fragments QX and QXM in initial
positions in the training items and of the fragments XV and                                60
SXV in final positions in the training items. Again, there                                 55
were no differences between the two languages in the anchor                                50
strength of the grammatical (t(58)=1.75, p>.085) or the un-
grammatical items (t<1).                                                                   45
   3) Novelty is measured as the number of fragments that                                  40
did not appear in any training item (Redington & Chater,                                                       NAT UNNAT
1996). E.g., if the fragments XVS and VS from the item
QXVSZM never occurred in a training item, then the test                                    Figure 2. Overall correct classification for
item would receive a novelty score of 2. Here there is a sig-                                    NAT and UNNAT languages.
nificant difference between the novelty scores for the gram-
matical test items in the NAT language (.43) and the                                       75
UNNAT language (0) (t(58)=3.50, p<.001). However, given
that items with novel fragments will seem less familiar they                               70
                                                                    Percent Correct
are more likely to not to be accepted as grammatical, mak-                                 65
ing it more difficult to correctly classify the test items from                            60
the NAT language. Thus this difference provides a bias
against our hypothesis that the NAT language should be                                     55
easier to learn. There were no differences between the un-                                 50
grammatical items (t<1).                                                                   45
   4) Novel fragment position is measured as the number of
fragments that occur in novel absolute positions where they                                40
did not occur in any training item (Johnstone & Shanks,                                                        NAT UNNAT
1999). E.g., if the fragment VQZ from the item QZVQZV
never occurred in this absolute position in any of the train-                               Figure 3. Correct classification of GEN
ing items then this item would be assigned a novel fragment                                 items for NAT and UNNAT langauges.
position score of 1. There were no differences between the
novel fragment scores for the grammatical (t(58)=1.54,                                     75
p>.13) or ungrammatical items (t<1) across the two lan-
                                                                                           70
guages.
                                                                        Percent Correct
   5) Global similarity is measured as the number of letters                               65
that a test item is different from the nearest training item                               60
(Vokey & Brooks, 1992). E.g., if the test item QZM has
                                                                                           55
QZV as its closest training item then it would be assigned a
global similarity score of 1. There were no differences be-                                50
tween the two languages for the grammatical (t=0) and un-                                  45
grammatical (t<1) items.
                                                                                           40
                                                                                                               NAT UNNAT
Procedures                                                                                Figure 4. Correct classification of SUB items
Subjects were randomly assigned to one of three conditions                                      for NAT and UNNAT languages.
(NAT, UNNAT, and CONTROL). NAT and UNNAT were
trained using the natural and unnatural languages, respec-
tively. The CONTROL group completed only the test ses-
sion. During training, individual letter strings were pre-        Results and Discussion
sented briefly on a computer. After each presentation, par-       Control Group Since the test items were the same for all
ticipants were prompted to enter the letter string using the      groups, but scored differently depending on training condi-
keyboard. Training consisted of 2 blocks of the 30 items,         tion, the control data was scored from the viewpoint of both
presented randomly. During the test session, participants         the natural and unnatural languages. Differences between
decided if the test items were created by the same (grammati-     correct and incorrect classification from both language per-
cal) or different (ungrammatical) rules as the training items.    spectives were non-significant with all t-values <1 (range of
Testing consisted of 2 blocks of 60 items, again presented        correct classification: 49.5%–50.5%). Thus, there was no
randomly.                                                         inherent bias in the test stimuli toward either language.

Experimental Group An overall t-test indicated that
                                                                                      0.1
NAT (59%) learned the language significantly better than
UNNAT (54%) (Figure 2; t(38)=3.27, p<.01). This result
                                                                   MSE Difference
                                                                                    0.08
indicates that the UNNAT was more difficult to learn than
the NAT. Both groups were able to differentiate the gram-                           0.06
matical and ungrammatical items (NAT: t(38)=4.67,
p<.001; UNNAT: t(38)=2.07, p<.05). NAT correctly clas-                              0.04
sified 70% of the grammatical and 51% of the ungrammati-                            0.02
cal items. UNNAT correctly classified 61% of the gram-
matical and 47% of the ungrammatical items. NAT (66%)                                   0
exceeded UNNAT (59%) at classifying the common GEN                                                      NAT UNNAT
items (Figure 3; t(38)=2.80, p<.01). Although marginal,                             Figure 5. MSE differences for grammatical
NAT (52%) was also better than UNNAT (50%) at classify-                             (low error) and ungrammatical (high error)
ing SUB items (Figure 4; t(38)=1.86, p=.071). Note that                                items for NAT and UNNAT networks.
the presence of the SUB items affected the learning of the
GEN items. Even though both groups were tested on exactly        the hidden units can influence the processing of subsequent
the same GEN items, the UNNAT performed significantly            inputs, providing an ability to deal with integrated sequences
worse on these items. Thus, the presence of the subjacency       of input presented sequentially.
violations in the UNNAT language affected the learning of
the language as a whole, not just the SUB items. From the        Materials
viewpoint of language evolution, languages such as               For the simulations we used the same training and test items
UNNAT would loose out in competition with other lan-             as in the artificial language learning experiment.
guages such as NAT because the latter is easier to learn.
                                                                 Procedures
                                                                 Forty networks, with different initial weight randomizations
             Connectionist Simulations                           (within ± .5), were trained to predict the next consonant in a
In principle, one could object that the reason why we found      sequence. The networks were randomly assigned to the NAT
differences between the NAT and the UNNAT groups is be-          and UNNAT training conditions, and given 20 passes
cause the NAT group is in some way tapping into an in-           through a random ordering of the 30 training items appropri-
nately specified subjacency principle when learning the lan-     ate for a given condition. The learning rate was set to .1 and
guage. Another possible objection is that the NAT language       the momentum to .95. After training, the networks were
follows the general pattern of English whereas the UNNAT         tested separately on the 30 grammatical and 30 ungrammati-
language does not, and that our human results could poten-       cal test items (again, according to their respective grammar).
tially reflect an “English effect”. To counter these possible       Following successful training, an SRN will tend to out-
objections and to support our suggestion that the difference     put a probability distribution of possible next items given
in learnability between the two languages is brought about       the previous sentential context. Performance was measured
by constraints arising from sequential learning, we present a    in terms of how well the networks were able to approximate
set of connectionist simulations of our human data.              the correct probability distribution given previous context.
                                                                 The results are reported in terms of the Mean Squared Error
Networks                                                         (MSE) between network predictions for a test set and the
For the simulations, we used simple recurrent networks           empirically derived, full conditional probabilities given the
(SRNs; Elman, 1991) because they have been successfully          training set (Elman, 1991). This error measure provides an
applied in the modeling of both non-linguistic sequential        indication of how well the network has acquired the gram-
learning (e.g., Christiansen & Devlin, 1997; Cleeremans,         matical regularities underlying a particular language, and
1993) and language processing (e.g., Christiansen, 1994;         thus allows for a direct comparison with our human data.
Elman, 1991). SRNs are standard feed-forward neural net-
works equipped with an extra layer of so-called context          Results and Discussion
units. The SRNs used in our simulations had 7 input/output       The results show that the NAT networks had a significantly
units (corresponding to each of the 6 letters plus an end of     lower MSE (.185; SD: .021) than the UNNAT networks
sentence marker) as well as 8 hidden units and 8 context         (.206; SD: .023) on the grammatical items (t(38)=2.85,
units. At a particular time step t, an input pattern is propa-   p<.01). On the ungrammatical items, the NAT nets had a
gated through the hidden unit layer to the output layer. At      slightly higher error (.258; SD: .036) compared with the
the next time step, t+1, the activation of the hidden unit       UNNAT nets (.246; SD: .034), but this difference was not
layer at time t is copied back to the context layer and paired   significant (t<1). This pattern resembles the performance of
with the current input. This means that the current state of     the human subjects where the NAT group was 11% better

than the UNNAT group at classifying the grammatical              may have been eliminated altogether through an evolution-
items, though this difference only approached significance       ary process of linguistic adaptation constrained by prior cog-
(t(38)=1.10, p=.279). The difference was only <3% in favor       nitive limitations on sequential learning and processing.
of the NAT group for the ungrammatical items (t=1). Also
similarly to the human subjects, there was a significant dif-                        Acknowledgments
ference between the MSE on the grammatical and the un-           We would like to thank Takashi Furuhata, Lori Smorynski,
grammatical items for both the NAT nets (t(38)=7.69,             and Brad Appelhans for their help with data collection.
p<.001) and the UNNAT nets (t(38)=4.33, p<.001). It is
plausible to assume that the greater the difference between                               References
the MSE on the grammatical (low error) and the ungram-           Christiansen, M. H. (1994). Infinite languages, finite
matical (higher error) items, the easier it should be to distin-   minds: Connectionism, learning and linguistic struc-
guish between the two types of items. As illustrated in            ture. Unpublished doctoral dissertation, Centre for
Figure 5, the NAT networks have a significantly better basis       Cognitive Science, University of Edinburgh, U. K.
for making such distinctions than the UNNAT networks             Christiansen, M. H. (2000). Using artificial language
(.072 vs. .040; t(38)=4.31, p<.001). Thus, the simulation          learning to study language evolution: Exploring the
                                                                   emergence of word order universals. In J. L. Dessalles &
results closely mimic the behavioral results, even though
                                                                   L. Ghadakpour (Eds.), The Evolution of Language: 3rd
the SRNs clearly do not have a built-in subjacency principle       International Conference (pp. 45-48). Paris, France:
nor do they have prior knowledge of English. The results           Ecole Nationale Superieure des Telecommunications.
therefore corroborate our suggestion that constraints on the     Christiansen, M.H. & Devlin, J.T. (1997). Recursive In-
learning and processing of sequential structure can explain        consistencies Are Hard to Learn: A Connectionist Per-
why subjacency violations tend to be avoided: they were            spective on Universal Word Order Correlations. In Pro-
weeded out because they made the sequential structure of           ceedings of the 19th Annual Cognitive Science Society
language too difficult to learn.                                   Conference (pp. 113-118). Mahwah, NJ: Lawrence Erl-
                                                                   baum Associates.
                        Conclusion                               Cleeremans, A. (1993). Mechanisms of implicit learning:
                                                                   Connectionist models of sequence processing. Cam-
In this paper, we have provided evidence in favor of an alter-
                                                                   bridge, MA: MIT Press.
native account of the universal constraints on complex ques-     Elman, J.L. (1991). Distributed representation, simple
tion formation. The artificial language learning results show      recurrent networks, and grammatical structure. Machine
that not only are constructions involving subjacency viola-        Learning, 7, 195–225.
tions hard to learn in and by themselves, but their presence     Gomez, R. L., & Gerken, L. (1999). Artificial grammar
also makes the language as a whole harder to learn. The            learning by 1-year-olds leads to specific and abstract
connectionist simulations further corroborated these results,      knowledge. Cognition, 70, 109-135.
emphasizing that the observed learning difficulties in rela-     Johnstone, T., & Shanks, D. R. (1999). Two mechanisms
tion to the unnatural language arise from non-linguistic con-      in implicit artificial grammar learning? Comment on
straints on sequential learning. These results, together with      Meulemans and Van der Linden (1997). Journal of Ex-
the results on word order universals (Christiansen, 2000;          perimental Psychology: Learning, Memory, and Cogni-
                                                                   tion. 25(2), 524–531.
Christiansen & Devlin, 1997), suggest that constraints aris-
                                                                 Kirby, S. (1998). Language evolution without natural
ing from general cognitive processes, such as sequential           selection: From vocabulary to syntax in a population of
learning and processing, are likely to play a larger role in       learners. Edinburgh Occasional Paper in Linguistics,
sentence processing than has traditionally been assumed.           EOPL-98-1.
This means that what we observe today as linguistic univer-      Knowlton, B. J, & Squire, L. R. (1994). The information
sals may be stable states that have emerged through an ex-         acquired during artificial grammar learning. Journal of
tended process of linguistic evolution. When language itself       Experimental Psychology: Learning, Memory, and Cog-
is viewed as a dynamic system sensitive to adaptive pres-          nition. 20(1), 79-91.
sures, natural selection will favor combinations of linguistic   Newmeyer, F. (1991). Functional explanation in linguis-
constructions that can be acquired relatively easily given         tics and the origins of language. Language and Com-
existing learning and processing mechanisms. Consequently,         munication, 11(1/2), 3–28.
difficult to learn language fragments, such as our unnatural     Pinker, S., & Bloom, P. (1990). Natural language and
                                                                   natural selection. Brain and Behavioral Sciences, 13(4),
language, will tend to disappear. Furthermore, if we assume
                                                                   707–727.
that the production system is based conservatively on a          Redington, M., & Chater, N. (1996). Transfer in artificial
processing system acquired in the service of comprehension,        grammar learning: A reevaluation. Journal of Experi-
then this system would be unlikely to produce subjacency           mental Psychology: General. 125(2), 123-138.
violations because they would not be represented there in the    Saffran, J. R, Aslin, R. N., Newport, E. L. (1996). Sta-
first place. In conclusion, rather than having an innate UG        tistical learning by 8-month-old infants. Science, 274,
principle to rule out subjacency violations, we suggest they       1926–1928.

