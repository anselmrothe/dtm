UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Addressing the Learnability of Verb Subcategorization with Bayesian Inference
Permalink
https://escholarship.org/uc/item/1bq6n8jt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Author
Dowman, Mike
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                 University of California

   Addressing the Learnability of Verb Subcategorizations with Bayesian Inference
                                                 Mike Dowman (Mike@cs.usyd.edu.au)
                                              Basser Department of Computer Science, F09,
                                                University of Sydney, NSW2006, Australia
                               Abstract                                word occurred in each position. Only the 150 most frequent
                                                                       words were used as context, and so this resulted in 600 di-
   Elman (1993) has shown that simple syntactic systems can be         mensional vectors for each word (there being one entry for
   learned solely on the basis of distributions of words in text       each of the 150 context words in each of four positions).
   presentation. However Pinker (1989) has proposed that chil-
   dren must make use of verbs’ semantic representations in or-
                                                                       Clustering those words whose vectors were most similar in
   der to infer their syntactic subcategorizations (semantic boot-     terms of Spearman’s rank correlation resulted in clusters
   strapping). Results reported here demonstrate how Bayesian          which corresponded to appropriate word classes for most of
   statistical inference can provide an alternative, and much          the 1,000 target words. While this system was good in that it
   simpler, account of how subcategorizations are learned. The         could be applied to naturally occurring speech, it was neces-
   acquisition mechanism described here suggests that syntactic        sary to decide at what level of dissimilarity to form separate
   acquisition may involve a much larger component of learn-           classes, and so it doesn’t completely solve the problem of
   ing, and less innate knowledge, than is presumed within             recovering the syntactic classes used by the original speak-
   mainstream generative theory.                                       ers.
                                                                          Elman (1993) demonstrated that not only word classes,
                          Introduction                                 but also syntactic patterns in which words belonging to
This paper investigates how children learn their first lan-            those classes appeared, could be learned without much in-
guage, and in particular the syntactic system of that lan-             nate syntactic knowledge, at least for simple languages. He
guage. It conceives of the problem in the following way:               trained a recurrent neural network to predict the following
when exposed to utterances in that language, how is it pos-            word in artificially generated sentences conforming to a
sible to infer the grammatical system which produced those             simple syntactic system containing 23 words, and syntactic
utterances. Further, the learner is assumed not to know the            features such as number agreement and recursion in relative
meanings of the words, have access to prosodic cues to                 clauses. Once trained on 50,000 sentences in this simple
structure, or to receive feedback about which sentences are            language, the network performed at near optimum accuracy
not grammatical.                                                       at predicting the subsequent word at any stage in a sentence,
   Currently the major paradigm within which language ac-              showing that the network had internalized the structural
quisition is explained is the parameter setting framework              constraints implicit in the data.
(Chomsky, 1995). Within this framework it is proposed that                While both Redington et al (1998) and Elman (1993)
knowledge of language is largely specified innately, and               demonstrate that much of syntactic structure can be learned
learning consists of identifying word tokens and setting a             by making statistical inferences based on the distributions of
limited number of parameters according to the syntactic                words, Pinker (1989) suggests that some aspects of syntax
structures to which the child is exposed. Chomsky argues               cannot be learned in this way. He proposes that, in order to
that this position is necessary because ‘even the most super-          determine verbs’ subcategorizations in the absence of nega-
ficial look reveals the chasm that separates the knowledge of          tive evidence, children must rely on complex innate rules
the language user from the data of experience.’ (p. 5).                combined with knowledge of the verbs’ semantic represen-
   Gold (1967) investigated this problem more formally, and            tations.
proved that without negative evidence (explicit information               Verbs such as give can appear in both the prepositional
about which sentences are ungrammatical) languages are not             dative construction (1a), and the double object dative con-
‘learnable in the limit’ unless the class of languages which           struction (1b), but there is a class of verbs such as donate
the learner may consider is restricted a p riori, for example          which can only appear in the prepositional construction, (1c
by innate knowledge. Below I will discuss an alternative               and 1d). However Gropen et al (1989) observe that, based
result by Feldman, Gips, Horning and Reder (1969) which                on the alternation between (la) and (lb), children generalize
suggests that Gold’s result is not relevant to the circum-             this alternation to verbs such as donate, and so produce un-
stances under which children learn languages.                          grammatical sentences such as (1d). They also demonstrated
   Redington, Chater and Finch (1998) investigated to what             that when presented with novel, nonce, verbs in the preposi-
extent syntactic categories could be inferred based on distri-         tional construction, children will productively use them in
butions alone, without knowing a p riori what syntactic                the double object construction in appropriate contexts.
categories existed in the language. They formed vectors by             However, ultimately children do learn which verbs cannot
taking the two preceding and two following context words               occur in the double object construction, and so we need a
for each occurrence of each target word in a large corpus of           theory which can explain why children first make such gen-
transcribed speech, and recorded how often each context

eralizations, and then subsequently learn the correct sub-          Minimum coding length provides an efficient implemen-
categorizations.                                                 tation of Bayesian inference, using information theory
                                                                 (Shannon, 1948), which allows us to quantify the amount of
 (1)    a.   John gave a painting to the museum.                 information in a formal description of a grammar. The
        b.   John gave the museum a painting.                    amount of information conveyed by an event (or symbol in
        c.   John donated a painting to the museum.              a grammar) is equal to the negative logarithm of its prob-
        d.   *John donated the museum a painting.                ability. It is conventional to take logarithms to base two,
                                                                 resulting in the units of quantity of information being bits.
   While the main point of Pinker (1989) is that syntax can-     Within this framework the best grammar is that which, to-
not be learned from distributions alone, he acknowledges         gether with a description of a corpus of data in terms of the
that the fact that certain syntactic structures do not occur     grammar, can be specified using the least amount of infor-
could be used as indirect negative evidence that these           mation.
structures were ungrammatical. However, he notes that chil-         While Feldman et al (1969) showed that, given two or
dren can neither consider that all sentences which they have     more grammars, it is possible to decide which is the best
not heard are not grammatical, and nor do they rule out all      given a corpus of data, they did not show how these gram-
verb argument structure combinations which they haven’t          mars could be created. For any reasonably complex gram-
heard. He notes that it is necessary to identify ‘under exactly  mar, the number of possible, but incorrect, grammars of
what circumstances does a child conclude that a nonwit-          equal or simpler complexity is so large that it is not plausi-
nessed sentence is ungrammatical?’ (p.14). The computa-          ble that a child could consider each in turn. However, in the
tional model presented in this paper is able to do just this,    next section, I describe computational models which are
and so predict that a verb such as donate cannot occur in the    able to learn grammars by starting with a simple grammar,
double object construction, while at the same time predict-      and then making small iterative changes which gradually
ing that a novel verb encountered only in the prepositional      lead towards the correct grammar. This avoids the need to
construction will follow the regular pattern and also appear     consider every single possible grammar, and so allows
in the double object construction.                               grammars to be learned within a reasonable amount of time.
           Bayesian Grammatical Inference                        Computational Models of Syntactic Acquisition
Most work in syntactic theory assumes that grammars are          Langley (1995) and Stolcke (1994) used simplicity metrics
not statistical, that is that they specify allowable structures, to learn simple syntactic systems, while Goldsmith (sub-
but do not contain information about how frequently par-         mitted) has applied this approach to the acquisition of mor-
ticular words and constructions occur. However, if gram-         phology. Both Langley and Stolcke’s systems produced
mars were statistical, it appears that it would be much easier   similar results to those found by Dowman (1998) using the
to account for how they were learned. Feldman et al (1969)       model described in the next section, although Langley’s
proved that as long as grammars were statistical, and so ut-     (1995) system did not incorporate considerations of how
terances were produced with frequencies corresponding to         well the grammar fitted the data. It is shown below how
the grammar, then languages are learnable. They note that        Dowman’s (1998) model was used to obtain new results
proofs that language isn’t learnable rely on the possibility of  concerning the acquisition of verb subcategorizations.
an unrepresentative distribution of examples being pre-
sented to the learner. While under Feldman et al’s learning      Description of Model
scheme it is not possible to be certain when a correct gram-     Dowman’s (1998) model learned grammars for simple sub-
mar has been learned, as more data is observed it becomes        sets of several languages, including the English data given
more and more likely that the correct grammar will be iden-      in Table 1, which corresponds to the grammar given in Ta-
tified.                                                          ble 2. The only a priori knowledge of the structure of the
   Feldman et al’s proof uses Bayes’ theorem, which relates      corpus which was available to the model was implicit in the
the probability of a hypothesis given observed data to the a     grammatical formalism with which grammars were speci-
priori probability of the hypothesis and the probability of      fied. This formalism restricted the model to using binary
the data given the hypothesis. For a fixed set of data the best  branching or non-branching phrase structure rules, intro-
hypothesis is that for which the product of the a priori prob-   ducing each word with a non-branching rule, and using no
ability of the hypothesis and the probability of the data        more than eight non-terminal symbols. The non-terminal
given the hypothesis is greatest. Feldman et al relate the       symbols were all equivalent arbitrary symbols, except that
probability of a grammar (seen as a hypothesis about lan-        each grammar would contain one special symbol, S, with
guage) to its complexity – more complex grammars are less        which each top down derivation would begin.
probable a priori. As grammars are statistical, it is also pos-     The frequency, and hence probability, with which each
sible to calculate the probability of the data given a gram-     symbol (including words) appeared in the grammar was
mar. This leads to an evaluation criterion for grammars          specified, and so the amount of information required to
where the complexity of a grammar is weighed off against         specify each symbol in a grammar could be calculated (us-
how much data it has to account for, and how well it fits that   ing Shannon’s (1948) information theory). A specification
data. A more complex grammar can be justified if it ac-          of a grammar would consist of a list of groups of three sym-
counts for regularities in the data, but otherwise a simpler     bols, one for a rule’s left hand side, and two for its right
grammar will be preferred.

hand side (a special null symbol being incorporated for use    tion itself, but it does not describe any regularities in the
in non-branching rules). As the grammar was statistical, it    data, and so has a very bad evaluation in that respect, re-
was also necessary to record how often each rule was used      sulting in a poor overall evaluation.
in parsing the corpus. It was assumed that a fixed amount of
information could be used to specify these probabilities, and                 Table 3: Form of Initial Grammars
so 5 bits of information was added to the evaluation of the
grammar per rule. (The assumption of 5 bits of information         S→XS                       S→X
is fairly arbitrary, but sufficient for the purposes described     X → John                   X → thinks
here.) The total cost of the grammar was the amount of in-         X → screamed               X → Ethel
formation needed to specify each symbol in the grammar,
and each rule’s frequency.                                        The model would begin learning by making one of four
                                                               random changes to the grammar, either adding a new rule
                   Table 1: Data for English                   (which would be the same as an old rule, but with one of the
                                                               symbols changed at random), deleting a randomly chosen
    John hit Mary           Ethel thinks John ran              rule, changing one of the symbols in one of the rules, or the
    Mary hit Ethel          John thinks Ethel ran              order of the rules, or adding a pair of rules in which one
    Ethel ran               Mary ran                           non-terminal symbol occurring on the left hand side of one
    John ran                Ethel hit Mary                     and the right hand side of another was changed to a different
    Mary ran                Mary thinks John hit Ethel         non-terminal symbol. These changes are slightly simpler
    Ethel hit John          John screamed                      than those described in Dowman (1998), but further investi-
    Noam hit John           Noam hopes John screamed           gations have revealed that this learning system works well,
    Ethel screamed          Mary hopes Ethel hit John          and it was able to reproduce the results obtained with the
    Mary kicked Ethel       Noam kicked Mary                   more complex system, so it was used for deriving the new
    John hopes Ethel thinks Mary hit Ethel                     results presented in this paper.
                                                                  After each change the evaluation of the new grammar
          Table 2: Grammar Describing English Data             with respect to the data would be calculated. If the change
                                                               improved the evaluation of the grammar then it would be
    S → NP VP                   Vs → thinks                    kept, but if the new grammar was unable to parse the data, it
    VP → ran                    Vs → hopes                     would be rejected. If the change made the evaluation of the
    VP → screamed               NP → John                      grammar worse, then the probability that it would be kept
    VP → Vt NP                  NP → Ethel                     would be inversely proportional to the amount by which it
    VP → Vs S                   NP → Mary                      made the evaluation worse, and also throughout learning the
    Vt → hit                    NP → Noam                      probability that changes resulting in worse evaluations
    Vt → kicked                                                would be accepted was gradually reduced. This is an im-
                                                               plementation of annealing search, which enables the system
   Given such grammars, the data was then parsed left to       to learn despite finding locally optimal grammars in the
right, bottom up, with only the first parse found for each     search space. The program learned in two stages, in the first
sentence being considered, and an ordered list of rules        only taking account of the evaluation of the data in terms of
needed to derive the sentence obtained. This list allows us to the grammar (making it easier to find the grammatical con-
make a probabilistic encoding of the data in terms of the      structions which best fitted the data), and in the second tak-
grammar. Given the probabilities of the rules, and always      ing account of the overall evaluation (and so removing any
knowing the current non-terminal symbol being expanded         parts of the grammar which could not be justified given the
(starting with S, and always expanding the left most unex-     data). After a fixed number of changes had been considered
panded non-terminal), it is only necessary to specify which    (less than 18,000 in the case of the above data) learning
of the possible expansions of that symbol to make at each      would finish with the current grammar, no improvements
stage. Hence, if a grammar accounts well for regularities in   usually having been found for a long time. For efficiency
the data, little information will be required to specify the   reasons, there were also limits placed on how deeply the
data. If a symbol can only be expanded by a single rule        parser could search for correct parses, and on the maximum
(such as S in the grammar above), then no information is       number of rules which the grammar could contain at any
necessary to specify that that rule is used.                   stage of the search. Because the search strategy is stochastic,
   By summing the amount of information needed to specify      it is not guaranteed to always find the optimal grammar
the grammar rules, the frequencies of those rules, and the     every time, so the learning mechanism would run the search
data given that grammar, we obtain an evaluation for each      several times, and select the grammar with the best overall
grammar, with lower evaluations corresponding to better        evaluation.
grammars. However, in order to complete the model of ac-
quisition, it is necessary to describe the search mechanism    Results
that was used for generating and testing grammars.             When used to learn from the English data in Table 1, the
   The model started learning with a simple grammar of the     system learned a grammar which corresponded exactly to
form given in Table 3, with a rule introducing each word.      that in Table 2 in structure. (As linguistic categories are not
This grammar is very simple, hence having a good evalua-       known a priori, the system simply used a different arbitrary

symbol to represent each learned category.) Table 4 shows      (2)     a.   John gave a painting to Sam.
that this grammar was preferred because, while the grammar             b.   Sam donated John to the museum.
itself is more complex than the initial one, and so receives a         c.   The museum lent Sam a painting.
worse evaluation, it captures regularities in the data, and so         d.   The museum sent a painting to Sam.
improves the evaluation of the data with respect to the
grammar by a greater amount. Dowman (1998) used this
same learning system (without any modifications except to      Results
the maximum number of non-terminal symbols) to learn           The initial and final evaluations of the grammars are given
aspects of French, Japanese, Finnish and Tigak.                in Table 5. Again a more complex grammar has been
                                                               learned which accounts better for regularities in the data
           Table 4: Evaluations for English Grammar            than the original grammar. Examination of the learned
                                                               grammar showed that the verbs had been divided into two
                          Initial state   Learned              classes (they have different symbols on the left hand sides
                          of learning     Grammar              of the rules producing them). gave, passed, lent and sent had
    Overall Evaluation 406.5 bits         329.5 bits           all been placed in one class, while donated appeared in a
    Grammar                160.3 bits     199.3 bits           class of its own. The grammar is able to generate only
    Data                   246.2 bits     130.3 bits           grammatical sentences, so gave, passed, lent and sent may
                                                               appear in both double object and prepositional construc-
          Learning Verb Subcategorizations                     tions, while donated may occur only in the prepositional
Given Dowman’s (1998) success in learning simple syntac-       dative construction. This has been learned even though there
tic systems, it was decided to investigate whether the same    was no data explicitly indicating that donated did not follow
model could be used to learn some of the kinds of phenom-      the regular pattern, and even though sent only occurred
ena which it has been argued are especially problematic for    once, and in the prepositional structure.
theories of learning. In particular it was investigated
whether the distinction between sub-classes of ditransitive           Table 5: Evaluations for Ditransitive Verbs Data
verbs such as gave and donated could be learned.
   There were three key results which the model aimed to                                   Initial state   Learned
replicate. Firstly, children eventually learn a distinction                                 of learning   Grammar
between verbs which can appear in both the double object           Overall Evaluation 3445.6 bits         1703.4 bits
and prepositional dative constructions, and those which do         Grammar                  190.3 bits     321.0 bits
not show this alternation. Secondly, when children encoun-         Data                    3255.3 bits    1382.3 bits
ter a previously unseen verb they use it productively in both
constructions. Finally, during learning, before children have     The results above account both for eventual learning of
seen many examples of an irregular verb which only occurs      the distinction between syntactically distinct verbs such as
in a subset of the possible constructions of other verbs, they gave and donated, and the productive use of novel verbs in
use that verb productively in constructions in which it is not regular constructions. The final phenomenon which we
grammatical.                                                   aimed to demonstrate was that, at earlier stages of learning,
                                                               children overgeneralize and use verbs such as donated pro-
Data Used for Learning                                         ductively in constructions in which they are ungrammatical.
The same model was used as in Dowman (1998), but this          In order to investigate this phenomenon, the total amount of
time the data consisted of two types of sentences, preposi-    data was reduced, to simulate a stage of acquisition where
tional datives such as (2a) and (2b), containing one of the    children had not been exposed to so many examples of each
verbs gave, passed, lent, or donated, and double object da-    kind of verb. When the model learned from this data it
tives such as (2c), containing gave, passed or lent, but not   failed to maintain a distinction between sub-classes of verbs,
donated. Each of these four verbs occurred with roughly        allowing all verbs to occur in both constructions. This was
equal frequency, and the alternating verbs were just as likely because there were not enough examples of donated to jus-
to appear in either construction. In addition the sentence     tify making the grammar more complex by creating a sepa-
(2d) was added, containing the only example of the verb        rate syntactic class, and so it was simply placed in the regu-
sent. Noun phrases consisted of either one of two proper       lar class.
nouns, or one of the two determiners a or the, followed by
either painting or museum. There were no biases as to which                               Discussion
noun phrase was most likely to occur in which position, and    These results on the acquisition of regular and irregular verb
overall the data consisted of 150 sentences.                   subcategorizations show that an aspect of syntax is learnable
   No modifications were made to the model of Dowman           which many other theories would have difficulty accounting
(1998), except that in order to cope with the more complex     for. In particular it is interesting to compare the performance
data set the maximum number of non-terminals was in-           of the model described here to that of connectionist models
creased to 14, and the number of iterations in the search was  of syntactic acquisition such as Elman (1993).
also increased.                                                   Elman’s network learned a language containing only 23
                                                               words, and yet 50,000 sentences were used to train the net-

work. This means that every word could have been observed       vance to the problem of determining or characterizing the
in every syntactic position may times over, greatly reducing    set of grammatical utterances….[P]robabilistic models give
the need to form generalizations. Christiansen and Chater       no particular insight into some of the basic problems of
(1994) investigated to what extent this kind of model was       syntactic structure.’ (Chomsky, 1957, p17). It seems hard to
able to generalize to predict that a word observed in one       explain how any system which didn’t monitor the frequen-
syntactic position would also be grammatical in another         cies with which verbs such as donated and gave are used
position. In order to do this, they trained a similar           would be able to account for how the different subcategori-
connectionist network on a more complex language con-           zations of these verbs could be acquired.
taining 34 words, again using 50,000 sentences. In the             Probably an even more important difference between the
training data they did not include girl and girls, in any geni- kind of simplicity measure proposed in Chomsky (1965)
tive contexts, and, boy and boys in any noun phrase con-        and the kind used here, is that Chomsky did not incorporate
junctions. After training they found that the network was       a measure of goodness of fit to data into his simplicity met-
able to generalize so that it would allow boy and boys to       ric. Chomsky’s metric simply looked for the grammar which
appear in noun phrase conjunctions, but it didn’t generalize    was shortest, in terms of the number of symbols which it
to allow girl and girls to occur in genitive contexts. Chris-   contained. The theory relied on innate constraints on what
tiansen and Chater considered the learning to have been         forms grammar could take in order that ‘significant consid-
successful in the case of boy and boys, but not in the case of  erations of complexity and generality are converted into
girl and girls.                                                 considerations of length, so that real generalizations shorten
   However, the account of the acquisition of verb subcate-     the grammar and spurious ones do not.’ (p42). Ultimately
gorizations presented in this paper relies on statistical prop- any notion of a simplicity metric was dropped from syntac-
erties of the data, and in particular the non-occurrence of     tic theory, because little progress seemed to be being made
certain forms. So, given 50,000 sentences of a language         in understanding grammar selection in this way.
with only 34 words, in which two words did not appear in a         Interestingly however, Chomsky’s (1965) theory shows
given construction, it would seem that a learner would pre-     that simplicity metrics are not necessarily incompatible with
dict that this could not simply be due to chance. Given this    theories which postulate very strong innate constraints on
perspective, it seems that Christiansen and Chater’s network    grammar. It seems that even within a parameter setting
has learned correctly in the case of girl and girls, but not in model of language acquisition, statistical inferences would
the case of boy and boys.                                       make the task of learning much easier, especially given the
   In order to account for distinctions between gave and do-    presence of noise in the data from which people learn (due
nated, it seems that neural networks must be more sensitive     primarily to grammatical errors, and exposure to data from
to quantitative information in language. The degree to          children who have not mastered certain aspects of gram-
which recurrent neural networks generalize is partly de-        mar). Showing that Bayesian inference can be useful in ex-
pendent on the fixed architecture of the network, and in        plaining language acquisition does not necessarily mean that
particular on the number of hidden nodes. Bayesian learning     it is actually used. Essentially it allows us to return the de-
methods for neural networks (MacKay, 1995) should be            gree to which language is determined by innate principles of
able to solve this problem, by placing a prior probability      grammar to an empirical question, allowing the possibility
distribution on network structures and parameter values,        of a much greater degree of learning in the process of syn-
although I am not aware of any applications of such net-        tactic acquisition.
works to models of language acquisition.                           However, postulating that a Bayesian mechanism is used
   Redington et al’s (1998) system for learning word classes    in acquiring syntax results in very different predictions
is capable of making very fine distinctions between sub-        about what form syntactic knowledge will take than if we
classes of verbs, but unlike the system described here it is    presume that language is largely determined by universal
not able to decide when the distributions of two words are      principles. Chomsky (1995) has argued that the language
dissimilar enough that they should be placed into separate      faculty of the mind should satisfy ‘general conditions of
classes, and when the difference in distributions is simply     conceptual naturalness that have some independent plausi-
due to chance variation within a class. However Boulton         bility, namely, simplicity, economy, symmetry, nonredun-
(1975) describes a program which does incorporate a Baye-       dancy, and the like’ (p. 1). While Chomsky notes this is ‘a
sian based metric into this kind of clustering system, and so   surprising property of a biological system’ (p. 5) he argues
demonstrates that it is possible to learn discrete classes      that this view is justified because throughout the history of
automatically.                                                  syntactic research systems conforming to this kind of prin-
   Certainly evaluation procedures based on simplicity met-     ciple have turned out to be the right ones. However, if lan-
rics are not new to linguistic theory. Chomsky’s (1965) the-    guage is learned with a Bayesian system we would not ex-
ory of syntactic acquisition relied on such a measure to        pect it to conform to such principles. Grammars could con-
choose between alternative grammars. However, it is possi-      tain a lot of irregular rules if these accounted well for regu-
ble to identify some key differences which make Chomsky’s       larities in observed language. Even the principle of lexical
theory very different to the Bayesian approach suggested        minimization is not so clear cut within a Bayesian based
here. Firstly Chomsky considered syntax to be fundamen-         account of learning, as Bayesian metrics will favor gram-
tally non-statistical. He had earlier argued that ‘Despite the  mars which associate a lot of information with individual
undeniable interest and importance of semantic and statisti-    words if this allows them to account better for regularities in
cal studies of language, they appear to have no direct rele-    the data. Hence, one prediction of Bayesian theory is that

the most commonly occurring words may be very idiosyn-                             Acknowledgments
cratic and irregular in their behavior, while very rare ones    I would like to thank Jeff Elman, David Powers, Brett
must conform to regular patterns.                               Baker, Hong Liang Qiao, Adam Blaxter Paliwala, Cassily
   It is interesting to compare the Bayesian account of acqui-  Charles, and two anonymous reviewers, for helpful com-
sition of subcategorizations presented here to Pinker’s         ments on this paper. This research was supported by ARC
(1989) theory. Pinker’s theory predicts that universal innate   and IPRS scholarships.
principles relate the meaning of a word to its syntactic sub-
categorization. Instead of the syntactic subcategorization of                           References
a verb being determined empirically by a learner based on
observations of patterns of occurrence, it is determined by     Boulton, D. M. (1975). The Information Measure for Intrin-
the meaning of that verb. Certainly Gropen et al (1989) have      sic Classification. Ph.D. Thesis, Monash University, Mel-
shown that children are sensitive to correlations between         bourne.
semantic and phonological characteristics of verbs, and         Chomsky, N. (1957). Syntactic Structures. The Hague:
which subcategorization frames they are most likely to oc-        Mouton & Co.
cur in. However, it is quite possible that these patterns were  Chomsky, N. (1965). Aspects of the Theory of Syntax. Cam-
learned by the child in much the same way as we have pro-         bridge, MA: MIT Press.
posed that syntactic subcategorizations may be learned. It      Chomsky, N. (1995). The Minimalist Program. Cambridge,
would be interesting to investigate empirically whether chil-     MA: MIT Press.
dren or adults could be influenced to prefer verbs in one       Christiansen, M. H. & Chater, N. (1994). Generalization and
construction or another by controlling the exemplars of           Connectionist Language Learning. Mind and Language,
these verbs to which they were exposed, perhaps by using          9, 273-287.
artificial language experiments or nonce verbs integrated       Dowman, M. (1998). A Cross-linguistic Computational In-
into natural languages. This kind of experiment should be         vestigation of the Learnability of Syntactic, Morpho-
able to resolve to what extent children make use of innate        syntactic, and Phonological Structure (Research Paper
principles versus learning in determining verbs’ subcatego-       EUCCS-RP-1998-6). Edinburgh, UK: Edinburgh Univer-
rizations.                                                        sity, Centre for Cognitive Science.
   The main limitation of the computational model described     Elman, J. L. (1993). Learning and Development in Neural
here is that it can only learn from small artificial data sets.   Networks: The Importance of Starting Small. Cognition,
There is no reason in principle why it cannot operate on          48, 71-99.
naturally occurring language, it is simply that it would take   Feldman, J. A., Gips, J., Horning, J. J., & Reder, S. (1969).
an extremely long time to run on this kind of corpus. This is     Grammatical Complexity and Inference (Tech. Rep. CS
clearly a limitation which is shared with connectionist ap-       125). Stanford, CA: Stanford University: Computer Sci-
proaches, though Redington et al (1998) demonstrate im-           ence Department.
pressive results learning from real language corpora. Cur-      Gold, E. M. (1967). Language Identification in the Limit.
rent research is investigating ways in which the search pro-      Information and Control, 16, 447-474.
cedure could be made more efficient, so that learning from      Goldsmith, J. (submitted). Unsupervised Learning of the
more realistic corpora is possible, though it seems worth         Morphology of a Natural Language.
acknowledging that we are modeling a process which takes        Gropen, J., Pinker, S., Hollander, M., Goldberg, R. & Wil-
place over many years, and that the human brain is much           son, R. (1989). The Learnability and Acquisition of the
more powerful than any computer.                                  Dative Alternation in English. Language, 65, 203-257.
                                                                Langley, P. (1995), Simplicity and Representation Change
                                                                  in Grammar Induction (Unpublished Manuscript). Palo
                          Conclusion
                                                                  Alto, CA: Institute for the Study of Learning and Exper-
This paper has shown that Bayesian inference is able to pro-      tise.
vide a simple and plausible account of how a number of          MacKay, D. J. C. (1995). Bayesian Methods for Supervised
aspects of syntax could be learned. In particular the compu-      Neural Networks. In Arbib, M. A. (Ed.) The Handbook of
tational model described here can learn verb subcategoriza-       Brain Theory and Neural Networks. Cambridge, MA:
tions where one verb is grammatical in only a subset of the       MIT Press.
structures in which another can appear, and yet predicts that   Pinker, S. (1989), Learnability and Cognition The Acquisi-
newly encountered verbs are used productively in regular          tion of Argument Structure. Cambridge, MA: MIT Press.
patterns. The model also accounts for overgeneralization        Redington, M., Chater, N. & Finch, S. (1998). Distributional
and hence the use of irregular items in regular constructions     Information: A Powerful Cue for Acquiring Syntactic
during early stages of acquisition. While it is not logically     Categories. Cognitive Science, 22, 425-469.
necessary that children must make use of Bayesian inference     Shannon, C. E. (1948). A Mathematical Theory of Commu-
in learning language, it has the potential to be incorporated     nication. Bell System Technical Journal, 27, 379-423 &
into theories as diverse as recurrent neural networks and         623-656.
universal grammar.                                              Stolcke, A. (1994). Bayesian Learning of Probabilistic Lan-
                                                                  guage Models. Doctoral dissertation, Department of
                                                                  Electrical Engineering and Computer Science, University
                                                                  of California at Berkeley.

