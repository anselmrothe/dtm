UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Reaction Times and Predictions in Sequence Learning: A Comparison
Permalink
https://escholarship.org/uc/item/31w8h46p
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Visser, Ingmar
Raijkmakers, Maartje E.J.
Molenaar, Peter C.M.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

         Reaction Times and Predictions in Sequence Learning: A Comparison
                             Ingmar Visser and Maartje E.J. Raijmakers and Peter C.M. Molenaar1
                          op visser,op raijmakers,op molenaar @macmail.psy.uva.nl
                                              Developmental Processes Research Group
                                        Department of Psychology, University of Amsterdam
                                                Roetersstraat 15, 1018 WB Amsterdam
                                                             The Netherlands
                             Abstract                                                      Sequence learning
   In the simple recurrent network (SRN) model, proposed by           One of the more recent paradigms to study implicit learn-
   Cleeremans and McClelland (1991) to describe implicit se-          ing is so-called sequence learning. Subjects are typically
   quence learning, the distinction between reaction time and         offered sequences of stimuli that are formed according to
   prediction of the next trial is somewhat blurred. That is, the     some (formal) rule(s). The only thing subjects have to do
   reaction time of the network is taken to be inversely propor-
   tional to the activation value of the corresponding node. In       is press some key that corresponds to the current stimulus.
   a prediction task the prediction would also be directly de-        For example when the stimuli are just zeros and ones, the
   rived from the activities of the output nodes. In order to         current stimulus could be formed by taking the xor of the
   investigate the difference between ability to predict follow-      preceding two stimuli. It is now interesting to see if sub-
   ing stimuli and reaction times, we study implicit sequence         jects implicitly learn this rule. This is measured by compar-
   learning in a similar vein as done by Cleeremans and Mc-
   Clelland (1991), using a slightly less complex grammar than        ing RTs on correct trials, that is trials on which the current
   they did. In addition we ask subjects to guess where the next      stimulus is in fact the xor of the two preceding trials, with
   stimulus will be at randomly chosen trials during the learn-       RTs on incorrect trials, where the current stimulus is not the
   ing process. Results show a direct correspondence between          xor of the two preceding trials.
   fast reaction times and correct predictions.
                                                                         Cleeremans and McClelland (1991), using this para-
                                                                      digm, had their subjects learn an endless sequence of stim-
                          Introduction                                uli genrated by a finite state grammar. To determine the ef-
                                                                      fects of implicit learning, they assessed reaction times, and
Implicit learning has been studied for over thirty years start-
ing with Reber (1967). Only recently attention has been               found these to be decreasing as subjects got more training.
                                                                      Similar studies have been done where, instead of measuring
given to modeling this kind of learning behavior in de-
                                                                      reaction times, performance was assessed by asking sub-
tail, mainly using neural networks. Specifically simple re-
current networks have been used successfully by Cleere-               jects to predict the next stimulus after having seen an initial
                                                                      segment of a string. However, few studies have investigated
mans and McClelland (1991) to model subjectsâ€™ behavior
                                                                      the exact relation between RTs and prediction performance
on learning sequences that are generated by a finite state
automaton, in fact the very same automaton that was used              in implicit learning. The present study aims to gain insight
                                                                      into this relation by analyzing RTs and prediction perfor-
by Reber (1967).
                                                                      mance simultaneously.
   Many different paradigms have been developed for stu-
dying implicit learning behavior. One characteristic that                In this context the work of Cleeremans and McClelland
divides those paradigms is the way in which they assess the           (1991) on the SRN model for implicit learning, is of inter-
possession of implicit knowledge. In this paper two such              est. They use the SRN model to predict RT performance
measures, reaction times and predictions, are studied. In             of subjects by taking the reaction time of the network to be
implicit learning research the sequential implicit learning           inversely proportional to the activity of the output unit cor-
paradigm has become increasingly popular and with that                responding with the correct response2 . The activity of the
the use of reaction time as the primary measure of perfor-            â€˜correctâ€™ output unit can thus be interpreted as a measure
mance (see for example Nissen & Bullemer, 1987; Cleere-               of anticipation of the position of the next stimulus. This
mans & McClelland, 1991; Seger, 1997). We used an aug-                anticipation in turn can be used to make predictions of the
mented sequence learning paradigm in which a direct com-              next stimulus as well; in this case the position correspond-
parison between reaction times and predictions was possi-             ing with the output unit with the highest activity has the
ble.                                                                  highest probability of being predicted. This means that the
    1                                                                     2
      The authors wish to thank their students Sander van Duyn,             Note that this doesnâ€™t leave the possibility for incorrect re-
Wanda Toxopeus, Stijn Gooskens, Thijs de Jongh & Edibe Tali           sponses. This is not a big problem, however, since typically in-
for valuable help in setting up this experiment and collecting and    correct responses are very seldom because of the simplicity of the
analyzing the data.                                                   task.

SRN model predicts a negative relation between prediction       second is an interaction effect of condition and time on pre-
performance and RTs, with the RTs decreasing as predic-         diction performance: over time, prediction should improve
tion performance gets better. The aim of the present study      for the grammatical trials, but not for the random trials. Fi-
is to test this hypothesis empirically.                         nally, on trials leading to correct predictions, RTs
                         Experiment                             Method
To assess the relation between RTs and prediction of stim-      Subjects Twenty-four subjects, undergraduates at the De-
uli directly we did a sequence learning experiment in which     partment of Psychology of the University of Amsterdam,
the standard series of RT trials was interspersed with pre-     participated in this experiment. They received both course
diction trials at which subjects had to guess where the next    credits and money for participation. On top of that they
stimulus would come. A similar procedure is proposed            could earn bonusses for fast and accurate responses.
by Jimenez, Mendez, and Cleeremans (1996) which they
named the continuous generation task. The main differ-          Procedure At the start of the experiment subjects were
ence between this procedure and other generation tasks is       told that in this task both accuracy and speed were impor-
that no feedback is given on the correctness of the predic-     tant. The experiment started with two small blocks of trials
tion; rather, after subjects have made their prediction the     that were not recorded to familiarize the subjects with the
next stimulus of the sequence is presented with the same        task. Each block consisted of four subblocks: 20 random
response-stimulus interval as between consecutive RT tri-       RT trials, 100 grammatical RT trials, 100 grammatical pre-
als.                                                            diction trials and 20 random prediction trials. In the RT
   Subjects were given a four-choice RT task, consisting of     subblocks only reproduction of the stimuli was asked of
a total of 4800 trials divided in twenty blocks of 240 trials   the subjects; in the prediction subblocks RT trials were in-
each. The blocks were split into two sessions that were pre-    terspersed with prediction trials. At the end an extra block
sented on two consecutive days. Unknown to subjects the         was added in which the order of the random and grammat-
sequence of stimuli followed a pattern that was generated       ical trials was reversed to test whether the order of the sub-
using the finite state grammar which is described below.        blocks could influence the results.
Because of the rather complex structure of the sequences           To enable a more direct comparison between prediction
generated with such a grammar subjects were presented           and reaction times, an extra block was added in which the
with 4800 trials. There were two types of stimuli: RT trials    series of trials for both the RT subblock and the prediction
and prediction trials. At the RT trials subjects were asked     block was identical. In this way it is possible to directly
merely to reproduce the current stimulus by pressing the ap-    compare the RT on a given trial with the prediction made
propriate key. At the prediction trials subjects were asked     on the very same trial.
to predict the next stimulus by pressing the appropriate key.   Stimulus material The sequence of stimuli in the gram-
Each block of 240 trials was divided into subblocks of four     matical subblocks was generated from the finite state gram-
types: grammatical RT, random RT, grammatical predic-           mar in figure 1. Sequences are produced by this grammar
tion and random prediction. The switch from one subblock        in the following manner:
to the next was not marked so subjects were unaware of the
existence of these subblocks. The sequence of stimuli in        1. Start in state #1 and randomly choose one of the arcs
the random subblocks was unrestricted but for the fact that        leaving that state while noting the letter corresponding
no two consecutive stimuli could be the same, which would          to the followed arc.
lead to undesired speed-up of responses due to priming.
   The random trials are used as a control condition, ac-       2. In the next state repeat this process of choosing an arc
comodating for possible effects of motor training, as well         and noting the corresponding letter
as for additional effects of subjects gaining implicit knowl-
edge of the grammar. This design provides the possibil-         3. The process ends when state #7 is reached and the pro-
ity to assess the effects of implicit learning, by comparing       cess starts over again to create strings of unbounded
RTs and prediction performance in the grammatical trials           length.
to those obtained in the random trials. Note that this is
a within subjects design, so that each subject is his own       Display As can be seen in figure 1 the alphabet of the
control group (i.e., the performance of each subject on the     grammar consists of four letters. The letters were translated
grammatical trials is compared with that same subjectâ€™s per-    into screen positions as shown in figure 2. In the grammat-
formance on random trials). The prediction of an inversely      ical RT subblock subjects were exposed to 100 trials; in
proportional relation between RTs and prediction perfor-                        
                                                                each trial the -symbol appeared in one of the quadrants of
mance, as derived from the SRN model (Cleeremans & Mc-          the computer display and the subjects had to press the cor-
Clelland, 1991), translates into three statistical hypotheses.  responding key on the numerical keypad on the keyboard.
The first is an interaction effect of condition and time on the The keys 1,2,4 and 5 on the numerical keypad were used
RTs: If implicit learning occurs, RTs should decrease more      to ensure that the spatial configuration of the response keys
for the grammatical trials than for the random trials. The      matched the spatial configuration of the stimulus positions

                              D                         B                       A                              B
                   #3                     #4                     #6
                                                        A
         A                                                                                                                    X
  #1                                    A                      C
         B                                                                      D                              C
                   #2                      #5                    #7
                              C                         D
Figure 1: Finite state automaton used to generate strings for
sequence learning experiments. A string is formed by start-
ing in state #1 and then randomly choosing one of the arcs
leaving that state meanwhile noting the letter correspond-                      A                              B
ing to that arc. Continue stepping from state to state until
the end state #7 is reached; from there the process starts                                 ?                                  ?
over again from state # 1.
on the display. Subjects were instructed to hold their in-
dex finger over the middle of the four keys and press the
                                                                                D                              C
appropriate key only with the index finger.
Exit interviews All subjects were asked a series of ques-                                  ?                                  ?
tions after the experiment was completed to assess whether
subjects had acquired any explicit knowledge of the gram-
matical sequence.
Results                                                                    Figure 2: The top panel shows the computer display for the
                                                                           RT trials. Subjects have to press the key corresponding to
The data of one of the subjects was was not included in
the analyses, because the subject had too many errors in
                                                                           the quadrant of the screen where the              is shown. In the
three consecutive blocks due to misplacing the index fin-                  bottom panel the screen lay-out for a prediction trial: all
ger over the numerical keypad. Comparison of the last two                  quadrants have a question mark and subjects have to choose
blocks revealed that the order of the subblocks, random be-                whatever letter they think will occur next. The letters in the
fore grammatical or vice versa, did not significantly influ-               top-left corner of the quadrants were not part of the actual
ence reaction times.                                                       display.
RT trials Grammatical RTs decreased from 404.7 ms at
the beginning of the experiment to 342.6 ms at the end;
random RTs decreased from 414.2 ms to 370.3 ms. The                        and random trials,     +,"%% -. with
mean RTs are displayed in Figure 3.                                        Greenhouse-Geisser correction for non-homogeneous vari-
                                                                           ances.
   The first hypothesis predicts that RTs decrease more for
the grammatical trials than for the random trials. In or-
der to test this hypothesis, RTs were averaged over sub-                   Prediction trials The percentage of correct predictions
jects and over two consecutive blocks. A repeated mea-                     in grammatical subblocks increased from 33.6 % at the be-
sures ANOVA with two within factors, block (10 levels)                    ginning to 52.2 % at the end of the experiment. The corre-
grammaticality (2 levels), indicates a significant interaction             sponding percentages for the random predictions are 30 and
between grammaticality and blocks: as predicted, gram-                     34 % respectively. The proportions of correct responses on
matical trial RTs decreased more over time than did ran-                   predictions for both randam and grammatical subblocks are
dom trial RTs,          . The analysis      displayed in Figure 4.
also yielded significant main effects for grammaticality and                  The second hypothesis states that prediction perfor-
training: grammatical trial RTs were significantly smaller                 mance should improve over time for the grammatical tri-
than the random trial RTs,       !#""$&% '( )*    , als, but not for the random trials. In line with this pre-
and RTs became faster over blocks for both grammatical                     diction, a significant interaction between blocks (time) and

     420                                                                                                  0.6
                                                                                     Proportion correct
                                                          Random
     410
                                                          Grammatical                                     0.5
     400
RT
     390                                                                                                  0.4
     380
                                                                                                          0.3
     370
     360                                                                                                  0.2
     350
                                                                                                                                        Grammatical
     340                                                                                                  0.1                           Random
     330
                                                                                                          0.0
           0     1     2     3     4       5   6      7     8    9 10 11
                                                                                                                0   1   2   3   4   5     6   7       8   9 10 11
                                       Block                                                                                        Block
Figure 3: Mean reaction times for grammatical and random
                                                                                                                                    / "
                                                                                  Figure 4: Proportion correct predictions of grammatical
/ "
trials. Means are averaged over two consecutive blocks,
        .
                                                                                  and random prediction trials,       .
                                                                                  Discussion
grammaticality was found,               01""%2  "
                                                            ,
                                                                                  The results show that implicit learning occurs: subjects give
                                                                                  faster responses on grammatical trials than on random tri-
showing that the grammatical predictions did show more                            als and this effect becomes larger towards the end of the
improvement over time than did the random predictions.                            experiment. Secondly, subjects gradually get better at pre-
More specifically, there was no improvement over time
                                                                                  dicting following stimuli due to training as well. Thirdly,
for the random trial predictions when analyzed seperately,
3#""415'6%2 %
                                , as was to be expected.
                                                                                  as expected, smaller RTs correspond with a better ability to
                                                                                  predict the following stimulus.
Prediction and RT trials: comparison To compare per-
formance on prediction and RT trials directly we added a                                                        Models of sequence learning
block of trials in which the strings used for the RT trials
and for the prediction trials were identical. Table 1 shows                       Cleeremans and McClelland (1991) applied the SRN to im-
the mean RTs for correctly and incorrectly predicted items                        plicit sequence learning. The SRN successfully describes
in this added block of trials. An anova with one within fac-                      subjectsâ€™ growing sensitivity to dependencies between suc-
tor (correct vs. incorrect) confirms that correct predictions                     cessive stimuli. The success of the SRN model is due to
correspond to fast RTs,          3#""78  ''921 
                                                      .                           its ability to capture the â€˜statistical constraintsâ€™ inherent in
                                                                                  the sequence of stimuli. The SRN model also correctly
                                                                                  predicts, at least in a qualitative manner, the inverse rela-
                                                                                  tion between RTs and the proportion of correct predictions
Table 1: Mean reaction times for correctly and incorrectly                        as we have shown above. A drawback of the SRN model
predicted trials.                                                                 is that it is not very well suited for describing individual
                                                                                  differences. The SRN model construes implicit sequence
                     Prediction         mean         sd                           learning in subjects as statistical learning. Subjects first
                     correct           360.96       49.64                         grow sensitive to first order frequencies of symbols, then to
                     incorrect         389.97       30.30                         second order freqeuncies, that is bigram frequencies, then
                                                                                  third order frequencies et cetera. Individual differences in
                                                                                  both the learning process and the resulting implicit knowl-
Exit interviews Subjects were asked whether they no-                              edge base, that is knowledge of frequency constraints, are
ticed anything particular in the sequence of stimuli. Al-                         not brought out by the model. Below we will describe how
though some subjects felt there was some â€˜regularityâ€™ in                          hidden Markov models can be used to model individual be-
the seqeunce, none of the subjects could specify this, ex-                        havior of subjects.
cept for three subjects that said that the subsequence                     :<;    The hidden Markov model
occured rather frequently. This is the subsequence in the
grammar which corresponds with the loop between the two                           Hidden Markov models, henceforth HMMs, are also called
top right nodes in Figure 1.                                                      stochastic finite automata since they are equivalent to finite

automata where the arcs between states have probabilities
corresponding to them. The only restriction is that the prob-                          A                         D                          B
abilities on the arcs leaving a particular state should sum
to one. This resemblance to finite automata is the reason                                                                       0.5
for exploring the possibility of applying HMMs to implicit
learning. Before presenting results of fitting HMMs to sub-                               0.5
jectsâ€™ data we give a short introduction to HMMs.
      Hidden Markov models have mainly been used in speech                                          D                   C         0.5
recognition applications such as Schmidbauer, Casacu-
berta, Castro, and Hegerl (1993), Chien and Wang (1997)                                   0.5                0.5
although recently more psychologically oriented applica-
tions have come up as well such as in action learning (Yang,
Xu, & Chen, 1997). The main reason that HMMs are used                                                                         0.5
in speech recognition is that they are espially well suited for                        B                         C                          A
capturing temporal dependencies in a series of utterances
which then helps in identifying phonemes. This feature can
be used to model the temporal dependencies that are inher-                         Figure 5: Representation of a hidden Markov model. This
ent in the series of stimuli that are typically used in implicit                   model produces exactly the same sequences as the gram-
learning.                                                                          mar we used in the experiment with equal probabilities. Se-
      More formally a HMM consists of a the following el-                          quences are generated in the same manner as in FSAs: start
ements (notations adapted from Rabiner (1989)), also see                           in one of the states on the left with letter A or B, then follow
figure 5 for clarification:
                       =?>@A4B    /
                                                                                   the arcs leading from those states. A sequence ends when
1. a set of states                                                                 one of the accepting states is reached, that is the two states
                                                                                   with the double circle around them. From there the pro-
2. a set     C  of observation symbols           C(D?E.   GF            cess continues by going to one of the starting states again.
3. a matrix : of transition probabilities H> I for moving from
                                                                                   For the accepting state with the letter D the arcs are drawn
      state =?> to state =6I
                                                                                   to the start states. For reasons of clarity the arcs from the
                                                                                   accepting state with the C are left out. The arcs leading
4. a matrix ; of observation probabilities JKIE of observ-                    from one state to the next have probabilities corresponding
      ing symbol C D while being in state =6I                                      to them which are given in the figure for some of the arcs.
5. a vector L of initial state probabilities L > corresponding
      to the probability of starting in state =?> at MNB
                                                                                   finding the best automaton to describe a given sequence of
The equations describing the dynamics of the model are as                          observations. This procedure can be applied to any kind of
follows:                                                                           sequence of categorical observations and hence also to a se-
                                                                                   quence of responses in a sequence learning experiment. In
                            =?OP7Q+R:S=?O0TVUWOP7Q
                           X O P7Q 1;V= O T-Y OP7Q 
                                                                                   simulation studies we have shown that in fitting a HMM the
                                                                                   right automaton can be induced from the data (Visser, Rai-
                                                    X
where =?O is the hidden process and O is the observed pro-
                                                                                   jmakers, & Molenaar, accepted for publication). That is,
                                                                                   having generated a sequence from the grammar used in the
cess; U@OP7Q and Y OP7Q are zero mean martingale increment                       experiment we found the HMM in Figure 5 exploratively.
processes, cf. Elliott, Aggoun, and Moore (1995, p. 20)                            Sequence learning data In the prediction subbblocks
for further details. A hidden Markov process then is a                             of the experiment subjects were presented with question
Markov process with multiple indicators for each (hidden)                          marks on the screen at random points in the sequence of
                                 =O X
state. By substituting by its definition in terms of                    = OKZ7Q in stimuli. In between the prediction trials normal RT trials
 X OP7Q
the defining equation for              OP7Q it is easily seen that in fact        were presented. For each subject this resulted in a sequence
  X Q     is dependent on all foregoing observations back to
       . Hence, at any given point observations can depend on
                                                                                   of responses consisting of the trials that were presented on
                                                                                   the screen interspersed with their own predictions about the
all foregoing observations. This is in contrast with a normal                      position of the next stimulus.
Markov model where the next observation only depends on                               In order to characterize sequence learning we fitted
the current observation.                                                           HMMs on these sequences of responses. To bring out the
                                                                                   learning we fitted separate HMMs on the initial and final
Characterizing sequence learning behavior                                          segments of the sequence of responses. Both segments con-
Fitting a hidden Markov model is in fact the inverse of pro-                       sisted of 500 trials. We expected to see a rise in number of
ducing a sequence of stimuli from a finite state automaton:                        hidden states of the model from beginning to end; that is,

we expected subjects to gradually build a more complex                                 References
model of the grammar underlying the sequence of stimuli.       Chien, J. T., & Wang, H. C. (1997). Telephone speech
A rise in number of states would reflect subjectsâ€™ growing            recognition based on bayesian adaptation of hid-
sensitivity to the structure of the sequence. For two sub-            den Markov models. Speech Communication, 22(4),
jects we indeed found such a rise in the number of states             369â€“384.
from two states at the start of learning to four states at the
end of learning. Overall however, results were inconclu-       Cleeremans, A., & Jimenez, L. (1998). Implicit sequence
sive. This is, we think, mainly due to the fact that only a           learning: The truth is in the details. In M. Stadler &
small proportion of the series of responses that were ana-            P. Freuch (Eds.), Handbook of Implicit Learning (pp.
lyzed were actually produced by the subject. Of the series            323â€“364). Thousand Oaks (Ca): Sage Publications.
of 500 trials that the HMMs were fitted on, only 125 were
produced by the subjects, the others were generated by the     Cleeremans, A., & McClelland, J. L. (1991). Learning
finite state automaton and only reproduced by the subjects.           the structure of event sequences. JEP: General, 120,
As a consequence, of all the responses only a quarter could           235â€“253.
were useful in discriminating between beginning and end
                                                               Elliott, R. J., Aggoun, L., & Moore, J. B. (1995). Hidden
of the learning phase. Hence the low power of the test. In
                                                                      Markov models: Estimation and control. New York:
future research it would be useful to have longer sequences
                                                                      Springer Verlag.
of freely generated responses to which HMMs can be fitted
more reliably.                                                 Jimenez, L., Mendez, C., & Cleeremans, A. (1996).
                                                                      Comparing direct and indirect measures of sequence
                        Conclusion                                    learning. JEP: Learning, Memory and Cognition,
                                                                      22â€“4, 948â€“969.
In sequence learning both RTs and prediction have been
used as a measure of performance. The results of this ex-      Nissen, M. J., & Bullemer, P. (1987). Attentional require-
periment show that when measured simultaneously it is                 ments of learning: Evidence from performance mea-
possible to relate directly improvement in prediction per-            sures. Cognitive Psychology, 19, 1â€“32.
formance and improvement in RT performance on gram-
matical trials. The direct comparison shows what is to be      Rabiner, L. R. (1989). A tutorial on hidden Markov mod-
expected: fast RTs are indicative of the subjectsâ€™ level of           els and selected applications in speech recognition.
anticipation of the next trial and on the same count result           Proceedings of IEEE, 77(2), 267â€“295.
in correct predictions. With this study it is also shown that  Reber, A. S. (1967). Implicit learning of artificial gram-
prediction is possible even in a fairly complex rule system,          mars. Journal of Verbal Learning and Verbal Behav-
that can not be verbalized by subjects.                               ior, 6, 317-327.
   The SRN model has proved to be a valuable model for
describing the learning processes inherent in implicit se-     Schmidbauer, O., Casacuberta, F., Castro, M. J., & Hegerl,
quence learning. However the model does not seem es-                  G. (1993). Articulatory representation and speech
pecially suitable to describe individual subjectsâ€™ behavior.          technology. Language and Speech, 36(2), 331â€“351.
Therefor we introduced the hidden Markov model as a
stochastic counterpart of the FSA to characterize individual   Seger, C. A. (1997). Two forms of sequential implicit learn-
learning behavior. Since hidden Markov models are an ex-              ing. Consciousness and Cognition: An International
cellent means of describing temporal denpencies between               Journal, 6(1), 108-131.
responses they are in principle well suited for describing     Visser, I., Raijmakers, M. E., & Molenaar, P. C. (accepted
implicit learning behavior. Our results with fitting HMMs             for publication). Confidence intervals for hidden
are promising in that we can reliably estimate them on the            Markov model parameters. British journal of mathe-
kind of sequences that are generally used in implicit se-             matical and statistical psychology.
quence learning. It would be interesting to do experiments
where subjects generate longer sequences of responses in-      Yang, J., Xu, Y., & Chen, C. S. (1997). Human action learn-
stead of the single predictions they made in the experiment           ing via hidden Markov model. IEEE Transactions on
described in this paper.                                              Systems, Man and Cybernetics, 27(1), 34â€“44.

