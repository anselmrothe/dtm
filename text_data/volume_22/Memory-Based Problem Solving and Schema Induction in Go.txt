UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Memory-Based Problem Solving and Schema Induction in Go
Permalink
https://escholarship.org/uc/item/2xd197mw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Heneveld, Alex
Bundy, Alan
Ramscar, Michael
et al.
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Memory-Based Problem Solving and Schema Induction in Go
                 Alex Heneveld, Alan Bundy, Michael Ramscar                             Julian Richardson
                   fheneveld,bundy,michaelg@cogsci.ed.ac.uk                           julianr@cee.hw.ac.uk
                    Institute for Representation and Reasoning                    Deptartment of Computing and
                                Division of Informatics                                Electrical Engineering
                               University of Edinburgh                                 Heriot-Watt University
                           Edinburgh, Scotland EH8 9LW                              Edinburgh, Scotland EH14
                                                            Abstract
  This project presents a memory-based, analogical model of complex problem solving with a technique of schema formation.
  Cases in the game of Go are described in a predicate logic representation of spatial stone arrangements near recent moves
  on the board, and then structure-mapping (Gentner 1983) is used to suggest candidate moves in novel situations based
  on exemplar cases from expert games. The analogy process is also used to generalise across previous cases to form new
  schema cases. Problem solving using these prototype schemas is compared with the exemplar-only model. The exemplar
  run e ectively found solutions to about 50% of the problems; schemas performed very similarly, taking half as long and
  identifying a few useful Go principles. This suggests to us that pure-exemplar models of memory-based processing can be
  made faster and more compact by introducing schemas. Analysing the model's weaknesses highlights the need for richer board
  representation and for a reminding stage to select relevant cases. Future work will also focus on using a move evaluation
  stage to lter spurious generalisations, and using both the evaluation and the generated schemas to enrich the representation.
                      Introduction                                    A schema is a description of general experiences,
This paper explores a model of memory-based problem                often formed from a family of episodes with elements
solving to see whether analogical reasoning can be                 in common. They can supplement or organise a
e ective at suggesting solutions to complex problems and           simple exemplar model by o ering a concise source of
to see whether schema induction via analogy can serve as           the essential factors in many experiences without the
a basis for learning useful generalisations. We describe           incidental details present in episodic memories. Frames,
two machine learning experiments designed to test how              scripts, and model-based reasoning are examples of
well exemplars and abstracted schemas perform when                 their use in AI. The theory of pragmatic reasoning
used to suggest candidate moves in the game of Go, and             schemas (Cheng & Holyoak, 1984) is a clear account
review some computational and cognitive implications of            of how schemas can combine the best attributes of
this work.                                                         competing memory-based and rule-based views to model
  In the memory-based psychological paradigm, experi-              logical reasoning. In the categorisation literature, both
ence cases stored in memory are the starting point for             prototype and theory (see Komatsu, 1992, for review)
solving a target problem, roughly in a three step process.         models can be considered as relying solely on a schema-
                                                                   de nition of categories. Unfortunately in all these
  (1) Reminding: surface features prime cases                      theories how to form these schemas is a diÆcult problem:
  (2) Matching: cases analysed to suggest solutions                the second set of experiments below tries a rudimentary
                                                                   method of inducing schemas from analogical matches
  (3) Evaluation: solutions considered in context                  performed during the memory-based process.
Available features of the problem cue the retrieval of                The strategy game Go was selected as a domain for
experiences containing similar features, in the rst,               our experiments for three main reasons: it is a plentiful
reminding step. These potentially relevant experiences             source of diÆcult problems, many of which computers
are then analysed | such as by looking for analogous               cannot currently solve; a vast amount of data is available
propositional structure | and if it is a good match,               on the Internet Go Server (IGS, 2000); and it does not
a portion of the remembered experience may be                      involve much outside knowledge. The game is played
transferred as a potential solution. (In iterative models,         by two players, one with black stones and the other
the matching step may uncover new surface features                 with white, who take turns placing their stones in any
which cause new cases to be retrieved.) Potential                  unoccupied space on a 19x19 board, with the goal of
solutions are evaluated relative to the context and the            amassing the greatest amount of territory. Players
goals of the target problem, in the third step, and used           can capture their opponent's stones, individually or in
to form the eventual solution. This approach is closely            orthogonally connected groups, by surrounding them in
related to case-based reasoning, and is also apparent              such a way that the captured group is not adjacent
in exemplar models of categorisation (Nosofsky, 1984)              to any empty squares of the board. Captured groups
and some recent work on natural language processing                are removed from the board, and the newly-unoccupied
(Daelemans, van den Bosch, & Weijters, 1997).                      region typically becomes the capturing player's territory.
                                                                1

Many books can give more information on the rules,              This subset of information was selected because it
the strategy, and the history of the game (Bozulich,          corresponds generally to the initial observations that
1992, is good for this). Archives for research in Go-         a player makes, guided by attentional cues to recent
playing computer programs are on-line (Reiss, 2000), as       moves and nearby stones, and because as limited and
are archives for psychological studies in Go (Burmeister,     easily-compiled as it is, it already contains enough
2000).                                                        information to begin drawing conclusions about where
  One of these psychological studies (Saito & Yoshikawa,      to play in certain circumstances. A more complete
1996) indicates that expert Go players quickly focus on       model of the expert's initial representation might note a
their eventual move, (order of 200 ms in TsumeGo); and        great many more features, symbolising more complicated
that they usually consider the outcome of only one or         concepts, but we hoped that this simpli cation would
two possible moves. For these candidate moves, there is       yet give promising results. The routines we developed
a lengthy lookahead evaluation which may go as far as
11 moves deep. Traditional Go-playing programs, even          to build the description from on-line game records and
those which have been proposed as cognitive models,           the routines to evaluate the descriptions as LISP code
perform a broader search on a much greater number of          for visual output are designed to work with a family of
candidate moves, necessarily to a lesser depth and with       vocabularies.
the result that no Go programs play any better than an
amateur. It is fascinating that skilled players are able      Analogical Processing
to focus rapidly | intuitively | on the best moves. In          The target problems were compared with cases
this research, we explore a memory-based model of how         in memory using the Structure-Mapping Theory of
this might be done and also whether the abstraction of        Analogy (Gentner, 1983). We chose this theory because
schemas can bene t performance.                               it has been widely examined in the literature on
                                                              the psychology and computer modelling of analogy,
          Experimental Design and Setup
                                                              and because the Structure-Mapping Engine program
In our view, the expert Go player relies on a large           (Falkenhainer, Forbus, & Gentner, 1989) is ideally suited
number of case experiences in memory, eÆciently               to our representation. Routines in SME can easily read
represented, and when a new problem is presented,             our descriptions, perform the analogy in keeping with
retrieves a small number of possibly-relevant exemplars       a demonstrated psychological theory, score potential
for fuller evaluation. Schemas may also be involved in        matches, and return inferences which correspond to
representing common, recurring segments of exemplars          candidate move suggestions.
as easily-accessible, general cases. The major issues           In the implementation of our model, individual
involved in developing a computational cognitive model
of this view are how the experiences are internally           analogies are taken between a target problem and each
represented, how relevant experiences are selected from       of 1500 cases in memory. In practice, this is a slow,
memory, and how schemas are formed.                           sequential process that takes between 10 and 30 minutes
                                                              per problem on a Sun Ultra-10 workstation. In theory,
Representation                                                however, each analogy is independent, and this might
  As in many machine models of complex problem solv-          correspond to a very quick, parallel neural computation
ing, the representational format used here was a proposi-     done by the brain. It could also be made signi cantly
tional description language recording a small number of       quicker by incorporating a more diverse description
perceptually basic, salient features. Speci cally, for each   language with a \reminding" approach. A smaller
problem, this encodes the colours of neighbouring stones      number of cases which seem relevant can be primed
around the two previous moves (X and O); the relative         by surface features in the target problem, with only
position between these two moves (in the format (rel X        those cases containing similar predicates used for the
(rel-1 w 1)), meaning one position to the west of the         analogical matching (Forbus, Gentner, & Law, 1995).
last move); and, for cases including the expert's solution,   Whether these improvements could get the time per
the relative position between the actual response (Q) and     problem down to the order of 200 ms is unclear; up to 30
the two previous moves.                                       minutes per problem, though, is not prohibitive at this
                                                              stage of the experimentation.
  E1    (my-last-move O)
  E2    (is-colour white O)
  E3    (is-colour black (rel O (rel-1 w 1)))                 Schema Induction
  E4    (is-colour black                                        Once an analogical match has been made between
           (rel O (rel-1 n 1)))                               a target problem and a source (base) case, the result
  E5    (is-colour black                                      can also be used to abstract the common substructure.
           (rel O (rel-1 s 1)))                               Instead of looking only at the inferences, the process
  E6    (is-colour white
           (rel O (rel-1 s 2)))
                                                              copies all the matching expressions into a new, schema
  E7    (make-move Q)
                                                              case. This encodes the essential description elements
  E8    (is-colour white Q)                                   from pairs of exemplar cases into more compact,
  E9    (equal-position Q (rel O (rel-1 e 1)))                abstracted descriptions which can then be used as base
                                                              cases for solving future problems. Additionally, by
 Figure 1: A fragment describing an atari opening east        repeating the induction on schema cases, the process can
                                                            2

also capture patterns recurring across many experiences          Materials
in memory. This cognitive model of generalisation                  We simulated the human's experience as consisting
learning has been argued for on the basis of order               of 1500 exemplar cases drawn from ten tournament
e ects by Kuehne, Forbus, & Gentner (1999). A similar            games (freely available on the Internet Go Server, IGS,
algorithm, the Least General Generalisation (Plotkin             2000). These source cases, ten-games, are each a
1971), has been widely used in AI to induce descriptions         LISP-style description in the vocabulary outlined above,
in logic, and a related approach has been applied to             recording the neighbourhood of the last two moves with
pattern learning in Go (Stoutamire, 1991). In our model,         an indication of the move the expert made at that
applying this type of schema induction after analogy             point in the game. One-hundred target problems, query-
has been used to generate candidate solutions has the            random-100, were compiled from random turns (before
advantage that the generalisation is returned with very
little additional computational e ort.                           the end-game) in other IGS tournament games in the
   To perform the induction, we developed a LISP                 same manner but without any indication of the expert's
module within the SME package that transforms the                response.
result of a completed analogy into an abstracted schema.
The schema uses the same description language as the             Results
parent cases, copying identical predicates exactly and             Figure 2 illustrates one of the better responses to a
inventing new tokens where the corresponding labels in           target problem. The program's top suggestion (1) is
the parents di er. The resulting schema can be used              the expert's \right" answer (Q), and is quite close to
directly as a base case for analogical reasoning or output       the opponent's previous move (X). Many of the other
to a le. The second set of experiments reviewed below            candidates were in other sections of the board, re ecting
investigates the utility of this induction algorithm in          problems locally similar to this one where the expert did
solving complex problems and learning patterns.                  play in other areas. (In this situation, playing elsewhere
                                                                 might be better, but as this model attends to stones near
        Problem Solving from Examples                            X and O, it cannot draw very good conclusions about
The rst experiment tried to solve 100 random Go                  distal play.)
problems by analogy to a library of exemplar cases.                The program solved 51 of the 100 problems after
We take solve to mean that the move chosen by the                running for 13 hours (taking the top 50 suggestions). If
expert player in the actual game is ranked in the top            all suggestions are considered, solutions to 93 problems
50 in the program's list of suggestions (sometimes also          were found; however, the program made an average
top 3 or top 10). In a full- edged Go-playing model              of 6791 suggestions per problem. (There are only 361
this would be followed by an elaborate evaluation stage          positions on the board). There were a large number of
which would consider the e ects of the candidate moves,          repetitions | on average, the right answer is suggested
typically using some form of lookahead search. Go                104 times. Looking at di erent suggestion depths gives
players perform this lookahead on 1.5 moves on average           a better picture of the program's performance: among
(Saito & Yoshikawa, 1996), whereas most computer Go              the top 3 suggestions, the right answer was found for 7
programs will evaluate between 20 and 70 moves.                  problems; among the top 10, for 20; and among the top
                                                                 50, for 51. Figure 3 shows the performance as up to 200
                                                                 suggestions are considered.
                                                                                          100
                                                                                           90
                                                                                           80
                                                                                           70
                                                                                           60
                                                                        Percent Correct
                                                                                           50
                                                                                           40
                                                                                           30
                                                                                           20
                                                                                                                                                 ten−games
                                                                                           10
                                                                                                                                                 random
                                                                                            0
                                                                                                0   20   40   60   80      100      120   140   160    180   200
                                                                                                                     Suggestion Depth
                                                                   Figure 3: Experiment 1 results. This graph shows
                                                                   the percentage of problems solved by considering the
                                                                   x highest-scoring suggestions. The heavy line is our
                                                                   program, and the light line a chance player based
  Figure 2: Sample results to a target problem.                    on the 1970 Zobrist program. The dotted line is the
  Numbers indicate ranked suggestions; X and O are                 asymptotic percent solved when all inferences were
  the last two moves; and Q is the expert's move.                  considered.
                                                             3

   To put these numbers in perspective, we developed           the induction process to investigate how well the
an informed random heuristic on the basis of the 1970          generalisation technique captures the essential, common
Zobrist program (Burmeister, 2000), also shown in              aspects in families of cases.
Figure 3. This program selected positions at random,
weighted heavily nearby the two prior moves. It took           Comparison with Exemplars
6 minutes to query the same set, nding solutions to 3
of the 100 problems within its top 3 suggestions; 10 in           The most signi cant result with the schema-based run
its top 10; and 35 in its top 50. These numbers are not        was that the computations took about half as long,
very sensitive to the precise weights used, so we take this    achieving approximately the same success rate. On the
as a baseline for how a rudimentary statistical learning       same problem set (query-random-100), this run took 7
algorithm would perform given only X, O, and Q.                hours; in the top three suggestions, the schemas-1500
   There were a large number of repeated suggestions in        source set found answers to 7 of the 100 problems; in the
the lists of candidate moves, as well as quite a few invalid   top 10, 23; and in the top 50, 51. In the limit, schemas
moves, either o the board or in an occupied square. A          suggested the solution to 88 problems.
human player would immediately lter these out, and a              Two main factors explain the speed di erence. Firstly,
machine evaluation routine would also quickly eliminate        the schema cases are much smaller, about 1/3 the size.
them from the lookahead search; it is interesting to           Secondly, a much smaller number of suggestions were
review the e ect this has. For our system, after removing      made per problem, 3204 on average. Nonetheless, the
these candidates, 13 of the correct solutions were in the      performance at low suggestion depths was about the
top 3 suggestions, 32 in the top 10, and 64 in the top         same. In fact, as a percentage of the total number of
50. For the informed random player, 5 were in the top 3        suggestions, the correct move was suggested 60 more
suggestions, 14 in the top 10, and 51 in the top 50.
                                                               often by the schemas (77 of 3204) than by the exemplars
Analysis                                                       (104 of 6791). This implies two things:
   A comparison of our analogical problem solver with              When the schema set contained a case which
the weighted random player shows that, for very small             solved the problem, it contained a lot of cases which
numbers of suggestions, our program performs much                 solved the problem.
better, solving twice as many problems, though taking              Schemas were more likely to make suggestions to
orders of magnitudes longer. At greater suggestion                appropriate problems than were exemplars; i.e. they
depths, the chance player improves relative to our                were less likely to give wrong answers.
program, for the trivial reason that will eventually guess
every space on the board; considering more than 50 or          The rst point tells us that there was even more
100 suggestions is not very practical either for input to a    repetition in the schema set than in the exemplar set,
machine evaluation routine or as even a rough model of         which might have been expected, considering that the
human candidate move generation. If we further focus           schemas encode common patterns among the exemplar
on the source and target problems where the expert             set. The second point was quite surprising, though: one
played within three stones of one of the last two moves        might expect the schemas to be more general, and hence
(between 30 and 50 possibilities), the correct answer          more applicable than the exemplars. What happened,
is in the top 10 di erent valid suggestions for 67 of          however, was that the exemplars, because they contained
the 100 problems. (This compares with 20 for the               so much background information, could match more
chance player). This shows that our analogical Go solver       situations. With many exemplar cases, analogies were
performs best on localized problems, which are in fact         based on irrelevant criteria but were still strong enough
those instances where our solver has the largest amound        to form inferences | inappropriately | about Q.
of relevant information. This points to the fact that the      Asymptotically, however, exemplars solved 93 of the 100
representation was the biggest weakness when reasoning         problems; in some cases, exemplars appropriately made
from exemplars.
                                                               inferences about Q based on information that had been
               Schema-Based Processing
                                                               lost in the schema formation process.
                                                                  After ltering out repetitions and invalid moves, the
The second set of experiments was designed to explore          schema-based run was slightly better than the exemplar-
the use of schemas in memory-based problem solving.            based run. The top three suggestions gave solutions
Hundreds of thousands of pairs of cases from the               to 14 problems (compared to 13 for exemplars); the
ten-games set were passed to our Structure-Mapping             top 10 solved 39 (versus 32); and the top 50 solved
Engine schema induction module, and the highest-
scoring schemas (after normalisation) were kept as the         70 (versus 64). This strengthens our conclusion that
set schemas-1500. This set was then used to solve              in this experiment, the schema induction process was
the same selection of 100 random problems as in the            somewhat successful in discarding irrelevant, distracting
previous experiments, using the same procedure as              information from cases and achieving better performance
described above. Next, we examined the schemas which           much more quickly.
were most e ective in solving problems and repeated
                                                             4

The E ectiveness of Schemas                                  can form: for reliable, iteratable induction, a better tech-
  If this conclusion is correct, then many of the            nique is needed.
schemas should correspond to common Go situations or                                 Conclusion
aphorisms. Figure 4 below shows one schema that was
                                                             This memory-based model of the candidate move gen-
                                                             eration phase of Go has promising and interesting re-
                                                             sults. One of the three highest-scoring valid suggestions
                                                             matched the expert's move in one out of eight problems,
                                                             and the best 50 suggestions solved more than half the
                                                             problems despite a simple and locally-con ned represen-
                                                             tation. Still, a large number of problems could not be
                                                             solved by this model, and in considering future research
                                                             directions it is useful to analyse these failings.
                                                             Reminding and Richer Representations
  Figure 4: A particularly good schema. Playing Q to           Most of the problems that were not solved were ones
  the left of X in a situation like this solved many of      where the subsequent move was in a di erent region of
  the target problems.                                       the board than either of the two previous moves. In
                                                             these instances, the source exemplars simply did not
particularly useful: it gave correct suggestions to 20%      store the information to enable our approach to nd the
of the posed problems, usually in the top 50 and             answer. It seems likely that human players attend to
several times in the top 10. Other e ective schemas          a much wider range of features; if more of these could
also helped to solve a large number of problems, to          be recognised and encoded by the routines that generate
a much greater extent than individual exemplars. On          the descriptions, we might expect better performance.
the other hand, there were some problems which no              On the other hand, it is expensive to keep all this
schemas matched but which were matched closely and           information: the cases would become too large to
                                                             perform analogies on the entire set. One possible
solved by some exemplars. In summary, it appears             resolution would be to encode initially only the most
that some good schemas can e ectively replace a large        salient features of the target; a reminding stage, such
number of common exemplars, but that in outlying             as in MAC/FAC (Forbus et al., 1995), could select a
cases, exemplars are important to keep around.               small number of cases on which to perform the analogical
  An approach we are currently investigating is to           matching. Some inferences would posit the presence
lump exemplars and schemas together, developing new          of certain features in the target problem, which could
schemas at random (weighted by the SME match score)          be added to the initial representation if they hold, and
and adding them to the pool. Some of the high-               the analogy could continue iteratively, re-representing,
generation schemas, i.e. those formed after multiple         re-reminding, and re-matching, until it ounders or
generalisations, match patterns in standard Go reference     suggests something about where to play.
books. One of these is the principle to \hane at the head      There is also the question of where these features will
of two stones", to jump out in front of an opponent's line   come from. Most Go programs have an extensive feature
of stones (shown in the left of gure 5 below). Another       recognition routine, and it would be possible at rst
interesting con guration left out the colour of X and Q,     just to duplicate some of these. Ideally, these features
suggesting that whether black or white played X relative     could then evolve, with better ones developing as the
                                                             program collects more experience. Schemas might be
to the other two stones, then the other player should set    useful here, to replace common, structured phrases in the
Q down to the lower right of X.                              representation by a single reference to a schema, similar
                                                             in a way to chunking. This model would be interesting to
                                                             test, in Go, or in any domain where expertise might take
                                                             the form of good feature recognition for case retrieval.
                                                             Evaluation and Improving Abstraction
                                                               A major criticism of this particular approach to
                                                             schema formation is that it only identi es patterns in
                                                             static input descriptions. It does this without any
  Figure 5: Some very generalised schemas. Interest-         regard for the signi cance of stones, and so encodes a
  ing schemas found included the \hane", on the left         lot of useless, concidental substructures. An interesting
  (play just below X), and the loose rhombi, both on         AI perspective would be to grade cases and individual
  the right (play to the lower right of X).                  description lines according to their performance. A
                                                             similar, more cognitive approach comes from Riesbeck
  However, not all the high-generational schemas corre-      & Schank (1989) who stress the importance of building
spond to nicely stated principles or even to reasonable      logical explanations for generalisations to eliminate this
Go play. Anytime there is a large intersection between       sort of spurious abstraction. This is precisely what
cases, even if it is meaningless and accidental, a schema    is done in the evaluation phase. While we have so
                                                           5

 far ignored this phase as distinct and unrelated to                                      References1
 representation and matching, it could conceivable be
 used to build explanations for good schemas; by storing            Bozulich, R., ed. (1992) The Go Player's Almanac.
 this information as part of exemplar cases, it could also            San Jose: Ishi.
 serve to enrich the representation.                                Burmeister, J. (2000) Research Page, http://www.psy
                                                                      .uq.edu.au/~jay/
 General Discussion                                                 Cheng, P. W., & K. J. Holyoak. (1985) Pragmatic
    This model gives an example of how experiences                    reasoning schemas, Cognitive Psychology 17:391-416.
 and generalisations might be used, by people and by
 machines, to solve diÆcult problems. It essentially                Daelemans, W., A. van den Bosch, & T. Weijters.
 performs pattern matching using analogy, with good                   (1997) Empirical Learning of Natural Language
 initial results in a very complex domain. Additionally,              Processing Task, workshop position paper, The 9th
 it embeds some powerful logic and machine learning                   European Conference on Machine Learning.
 techniques in a cognitive framework of schema induction.           Falkenhainer, B., K. D. Forbus, & D. Gentner.
 The major weaknesses of our model seem to be in                      (1989) The structure-mapping engine: algorithm and
 the simplicity of the representation and the absence                 examples, Arti cial Intelligence 41:1-63.
 of the reminding and evaluation stages. These issues,              Forbus, K. D., D. Gentner, & K. Law. (1995) MAC/
 sometimes considered separate from the matching stage,               FAC: a model of similarity based retrieval, Cognitive
 must on the contrary be addressed simultaneously and in              Science 19:141-205.
 depth when developing a memory-based problem solver.
    Our approach can also be viewed as nding solution               Gentner, D. (1983) Structure-mapping: a theoretical
 categories for a target problem, as analogical problem               framework for analogy, Cognitive Science 7:155-170.
 solving and categorisation are closely related. In this            Holyoak, K. J., & P. Thagard. (1989) Mental Leaps.
 light, our results suggest that it is more eÆcient (in               Cambridge, MA: MIT Press.
 terms of time, memory, and to a lesser extent success              IGS (2000) The Internet Go Server. http://igs.
 rate) to de ne categories on the basis of schemas when               joyjoy.net/
 there are many similar cases. This o ers circumstantial            Komatsu, L. K. (1992) Recent views of conceptual
 evidence in favour of multiple-prototype and theory-                 structure, Psychological Bulletin 112:500-526.
 based views of categorisation with the added bene t of
 describing how schema de nitions might be formed from              Kuehne, S. E., K. D. Forbus, & D. Gentner. (1999) Cat-
 structural and surface features of exemplars. Instead                egory Learning as Incremental Abstraction using
 of relying on a complete set of episodic exemplars, a                Structure-Mapping, poster presentation, 21st Annual
 memory-based approach can bene t from the clustering                 Meeting of the Cognitive Science Society. Vancouver.
 and compression given by analogical induction and the              Nosofsky, R. M. (1984) Choice, similarity, and the con-
 formation of schematic (semantic?) memories.                         text theory of classi cation, Journal of Experimental
    On the other hand, our schemas did not even suggest               Psychology: Learning, Memory, and Cognition 10:104-
 solutions to some of the problems that were easily solved            114.
 by exemplars; forming good schemas, if it is possible              Plotkin, G. D. (1971) Automatic Methods of Inductive
 in most cases, is more diÆcult than our technique                    Inference, PhD Thesis, University of Edinburgh.
 recognises. No matter what, exemplars will always be
 needed for those areas where experience is minimal and             Reiss, M. (2000) Mick's Computer Go Page, http://
 where categories are not neatly de ned. For Go, the data             www.reiss.demon.co.uk/webgo/compgo.htm
 suggest that the best categorisation and problem solving           Riesbeck, C. K., & R. C. Schank. (1989) Inside Case-
 would be achieved by a mixed source set containing a                 Based Reasoning. Hillsdale, NJ: Erlbaum.
 few very general schemas, more speci c schemas, and                Saito, Y., & A. Yoshikawa (1996) A Protocol Study of
 exemplars in areas not well represented by the schemas.              Problem Solving in Go, Poster presentation, abstract
    In conclusion, we have implemented and analysed                   in Proceedings of the 18th Annual Conference of the
 a model of memory-based cognition | in a symbolic                    Cognitive Science Society 833.
 architecture | and applied it to complex problem
 solving in Go, achieving better-than-chance performance            Stoutamire, D. (1991) Machine Learning, Game
 with a very limited representation. At this stage, it                Playing, and Go, Technical Report TR 91-128.
 seems that schemas can assist but not supplant pure                  Cleveland, OH: Case Western Reserve University.
 exemplars in this type of problem solving. It seems also
 that the central matching stage may be more intricately
 dependent on the reminding and the evaluation stage
 than is typically acknowledged, particularly regarding
 the representation. This indicates compelling research
 directions both for Computer Go and for the psychology
 of problem solving.
1   In addition to the referees, we are indebted to Jon Oberlander, Ian Frank (now at ETL Japan), and others in the Edinburgh
    Informatics community for helpful discussions; and to Ken Forbus's group at Northwestern University for their insights and the
    SME code. The work was funded principally by a British Marshall Scholarship.
                                                                6

