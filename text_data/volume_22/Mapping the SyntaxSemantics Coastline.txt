UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Mapping the Syntax/Semantics Coastline
Permalink
https://escholarship.org/uc/item/6nq7r6fh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 22(22)
Authors
Tabor, Whitney
Hutchins, Sean
Publication Date
2000-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            Mapping the Syntax/Semantics Coastline
                                                         Whitney Tabor
                                                  tabor@uconnvm.uconn.edu
                                                          Sean Hutchins
                                                      sonicmoose@aol.com
                                                   Department of Psychology
                                                   University of Connecticut
                                                     Storrs, CT 06269 USA
                           Abstract                               around grammatical heads which select the semantic at-
                                                                  tributes of their complements, then (1a) can be diag-
   A number of language processing studies indicate that          nosed as an amalgam of selection violations. Subcate-
   violations of syntactic constraints are processed dif-         gory errors involve incorrect selection of an argument-
   ferently from violations of semantic constraints (Brain        structure constellation, typically of a verb, (e.g., in *Er-
   imaging: e.g., Ainsworth-Darnell et al., 1998; Ni et al.,      min put the book). Agreement errors involve incon-
   in press; Speeded grammaticality judgment: McElree
   & Grith, 1995; Eye-tracking: Ni et al., 1998). Al-            sistencies between elements that are required to share
   though these results are often taken as support for the        a common feature like number or gender (e.g., *They
   view that the processor employs two separate modules           eats.) We refer to other mistakes in the sequencing of
   for enforcing the two classes of constraints, we nd (in        categories (e.g. *See dog dog) as category errors. The
   keeping with Rohde & Plaut, 1999, and Tabor & Tanen-           last three types are standardly considered syntactic er-
   haus, 1999) that a nonmodular connectionist network
   can learn a quantitative distinction between the two           rors.
   types of constraints. But prior connectionist studies
   have been inexplicit about why the distinction arises.
   We argue that it stems from the distinct distributional        Evidence for the distinction
   correlates of the di erent types of information: syn-          Drawing a fundamental distinction between syntax and
   tax involves gross distinctions; semantics involves subtle     semantics has several advantages.
   ones. We also describe the Bramble Net, an attractor
   network which derives grammatical categories and mod-             First, it is only by factoring out the variation in sen-
   els an approximation of the syntax/semantics distinc-          tence quality due to semantic contrast that it is possible
   tion in qualitative terms. These results support Elman's       to discern the simple approximation of the range of a
   (1990) suggestion that grammatical structures may arise        language that its phrase structure rules provide (Chom-
   by self-organization, rather than by hardwiring. They          sky, 1957). These rules receive independent justi cation
   also help clarify what the grammatical structures are in
   a self-organizing connectionist network, and emphasize         from the observation that they permit a compositional
   the usefulness of dynamical systems theory in grammat-         treatment of meaning that largely accords with human
   ical explanation.                                              judgment (Frege, 1892).
                                                                     Second, several recent language processing studies in-
                                                                  dicate distinct processing responses to syntactic and
                       Introduction                               semantic anomaly. McElree and Grith (1995) used
De nition of syntax vs. semantics                                 a speeded grammaticality judgment task to nd out
                                                                  how quickly people could detect syntactic and seman-
By the distinction between syntax and semantics we                tic anomalies. They found that detection of syntactic
mean the fundamental one that Chomsky (1957) identi-              anomaly (both subcategorization violation and category
  ed when he contrasted (1a) with (1b).                           violation) rose above the level of chance about 100 ms.
                                                                  sooner than detection of semantic anomaly (selection vi-
(1) a. Colorless green ideas sleep furiously.                     olation). Ni et al. (1998) and Braze et al. (submit-
   b. Furiously sleep ideas green colorless.                      ted) used an eye-tracker to monitor participants as they
                                                                  read sentences that were semantically (selection viola-
   The modi cational relationships between the words in           tion) and syntactically (agreement violation) anomalous.
(1b) are not evident to a native English speaker, and             They found that readers slowed down at both kinds of
one cannot identify any coherent phrasal hierarchy. We            anomalies, but for syntactic anomalies the distribution
thus label (1b) as syntactically anomalous. By contrast,          of their regressive eye movements spiked abruptly on
native speakers have no trouble deciding on a parse tree          the anomaly itself or shortly after, while for semantic
for (1a), but the meanings of the complex phrases are             anomalies it was strongly skewed toward the end of the
odd and seemingly contradictory. We thus call (1a) se-            sentence. Ainsworth-Darnell et al (1998), tied together
mantically anomalous.                                             many previous EEG studies by demonstrating indepen-
   By employing some of the basic apparatus of Genera-            dent responses to the two types of anomalies in individ-
tive Grammar, we can make a ner characterization of               ual participants. Ni et al. (in press) showed distinct
the two types. If we assume that phrases are organized            regions of brain response to the two types using fMRI.

0 SIMULATION 1                                                                                                      2
Models                                                    Table 1: The grammar for simulation 1. All productions
The distinction between syntactic and semantic anomaly    have equal likelihood of being used. The lexical classes
seems to be well supported both theoretically and em-     expand to between 1 and 4 individual lexical items.
pirically. It is therefore desirable to have a good un-     S ! N[human] V[eat] N[food] p
derstanding of how it is instantiated in mental repre-      S ! N[human] V[perceive] N[inanimate] p
sentations. The standard view, coming from Gener-
ative Linguistic Theory, assigns separate modules the       S ! N[human] V[destroy] N[breakable] p
jobs of checking the two types of anomaly. But this         S ! N[human] V[cogitate] p
model leaves open the question of how a learner decides     S ! N[human] V[perceive] N[human] p
whether to attribute an observed distributional system-     S ! N[human] V[pursue] N[human] p
aticity to a syntactic or semantic module. For example,     S ! N[human] V[move] N[inanimate] p
why is \Dogs moo" classi ed as a semantic anomaly,          S ! N[human] V[move] p
while \Dogs barks" is a syntactic one?                      S ! N[animate] V[eat] N[food] p
   Connectionist models have exhibited an ability to        S ! N[animate] V[perceive] N[animate] p
glean both syntactic and semantic information from text     S ! N[animate] V[pursue] N[animate] p
data. Elman (1990, 1991) trained a Simple Recurrent         S ! N[animate] V[act-on] N[animate] p
Network or \SRN" on the task of predicting each next        S ! N[animate] V[move] N[inanimate] p
word in a simple, English-like corpus. He found that        S ! N[animate] V[move] p
a hierarchical cluster analysis of the trained-network's
hidden unit space contained clusters corresponding to       S ! N[inanimate] V[move] p
both syntactic classes (Noun, Verb, and various transi-     S ! N[aggressive] V[destroy] N[fragile] p
tivity classes of verbs) and semantic classes (Animate,     S ! N[aggressive] V[eat] N[human] p
Large, Edible, etc.). Rohde and Plaut (1999) stud-          S ! N[aggressive] V[eat] N[animate] p
ied a similar simulation and found that the inclusion       S ! N[aggressive] V[eat] N[food] p
of semantic-like lexical cooccurrence biases signi cantly
enhanced the ability of the network to learn complex
phrase structures. Moreover, the average lowest transi-   in 3, 10 in 4) had xed sigmoid activation functions.
tion likelihoods in natural grammatical sentences were    The target at each point in time was an activation of 1
higher than the average lowest in grammatical but se-     on the output unit corresponding to the next word in
mantically odd (selection violation) sentences, which in  the training sequence. We wanted the outputs to con-
turn were higher than the average lowest in ungrammat-    verge on probability distributions over next words, so
ical sentences (including verb subcategorization, agree-  the output units as a group had the softmax (normal-
ment, and other category sequencing violations). Allen    ized exponential) activation function. We thus employed
& Seidenberg (in press) used a continuously settling re-  the multinomial cost function (Rumelhart et al, 1995)
current network and included a bidirectional mapping      and the delta rule was used to adjust the hidden-to-
from form to meaning. The resulting xed point dy-         output weights. The remaining feedforward units were
namics provided good generalization behavior.             trained using additional backpropagation (Rumelhart,
   These results indicate that connectionist networks     Hinton, & Williams, 1986), and the recurrent connec-
can derive a distinction between syntactic and seman-     tions were trained on the approximation to backprop-
tic structure, while encoding both in a common metric     agation through time (BPTT) in which the gradient is
space. But the results raise many questions about what    estimated on the basis of only a single previous time step
syntactic and semantic structure consist of in such self- of the hidden units (see Pearlmutter, 1995).
organizing models. While, the resemblance of network         We used probabilistic context free rewrite rules to con-
cluster structures to linguistic categories is suggestive struct a simple grammar similar to the one used by
and the alignment of graded network properties with       Elman 1990 for training a syntax network (Table 1).
category levels (well-formed, semantically odd, ungram-   The grammar generated only nouns, verbs, and end-of-
matical) are encouraging, the ndings do not provide       sentence markers (\periods"). The verbs were either
much insight into why the resemblances hold or what       transitive or intransitive. Both the nouns and verbs
general properties of the networks produce these results. fell into a number of semantic classes (See Table 1).
We performed several additional simulations to better     We de ned a selectional violation to be a sentence in
understand how connectionist networks represent syn-      which a verb had the right transitivity, but the noun
tactic and semantic structure.                            features were not consistent with the grammar (e.g.,
                                                          N[inanimate] V[eat] N[food]). We de ned a subcatego-
                     Simulation 1                         rization violation to be a sequence in which a strictly
Following Elman (1991) and Rohde and Plaut (1999),        intransitive verb took an object, or a strictly transitive
we employed a SRN with three hidden layers, and re-       verb did not.
current connections only in the middle hidden layer.         The grammar was used to generate strings of words at
The 30 input units were clamped on or o , one at a        random. These were strung together end to end and pre-
time, with each unit uniquely coding the appearance of    sented to the network one word at a time. The network
a particular word. The hidden units (10 in layer 2, 20    was trained with a learning rate of 0.01. Momentum was

0 SIMULATION 1                                                                                                         3
Table 2: Means of the grammaticality measure. All          Table 3: Distances to closest grammatical state. All
within-language comparisons are signi cant (p < .001).     within-language comparisons are signi cant (p < .001).
  Language Class               N      Mean SD                Language Class                N      Dist SD
  SVO          Well-formed 662 -1.56 0.35                    SVO           Well-formed 662 0.040 0.029
  SVO          Sel Viol        2002 -4.18 1.08               SVO           Sel Viol        2002 0.176 0.206
  SVO          Subcat Viol 1098 -5.21 1.27                   SVO           Subcat Viol 1098 0.360 0.266
  SOV          Well-formed 662 -1.60 0.34                    SOV           Well-formed 159 0.020 0.025
  SOV          Sel Viol        2002 -5.37 1.66               SOV           Sel Viol        1000 0.288 0.329
  SOV          Subcat Viol 1098 -6.81 0.82                   SOV           Subcat Viol 1000 0.625 0.394
not used.                                                  probability transition). We studied the response of the
   The grammar was used to compute exact target distri-    network to anomalies by examining the hidden unit rep-
butions for every juncture between words in the training   resentations. To do this, we presented a long sequence
corpus (see Rohde & Plaut, 1999). The Kullback-Leibler     (2000 words) of grammar-generated words to the net-
divergence (E ) between the network's output and the       work and recorded the hidden unit states associated with
correct distribution wasPcomputed at each word in the      each word. Tabor et al. (1997) called this kind of sam-
training corpus (E = t ln t =o where t is the tar-
                    w      i i    i  i        i            ple a Visitation Set. We then tested the network on
get for unit i and o is its output on word w). Training
                     i                                     ill-formed sentences by nding the hidden unit location
was stopped when the cumulative divergence error over      visited following the transition with the lowest output ac-
a large sample of patterns was consistently small enough   tivation over the course of the sentence (the low-point).
that we could conclude that the network was not con-       Table 3 shows the mean distance in hidden unit space
  ating any of the target distributions with one another   between the low-point and the nearest point in the Visi-
(approximately 1 million word presentations).              tation set for samples of selection violation sentences and
   Rohde & Plaut (1999) studied a measure of sentence      subcategorization violation sentences. For comparison,
goodness based on the network's output predictions.        a new random sample of grammatical sentences was also
They found that the mean goodness (log of the prod-        tested against the visitation set.
uct of the two lowest output activation transitions) of       The minimum distance measure parallels Rohde and
normal grammatical sentences was higher than that of       Plaut's grammaticality measure, and points to a useful
selection violation sentences, and the selection violation way of characterizing the e ect of anomaly on the net-
sentences, in turn, had a higher mean than syntactic       work: there is a subset of the hidden unit space that the
violation sentences. Because our sentences were much       network sticks to during grammatical processing. This
shorter than theirs, we used a simpli ed version of their  subset is approximated by the Visitation Set. Selection
goodness measure (log of the single worst transition) and  violations throw the network o the track somewhat.
tested it on well-formed sentences, selection violations,  Syntactic violations throw it o more substantially.
and subcategorization violations. We also found a clear        This geometrical contrast between the anomaly types
strati cation (See the \SVO" rows in Table 2).             has a simple explanation in terms of the distribu-
   One of the consequences of de ning syntactic category   tional distinction between selection and subcategoriza-
descriptions independently of semantic classi cations is   tion. Subcategorization refers to more abstract classes
that category order is expected to be able to vary inde-   than selection. Thus more instances of training are in-
pendently of the contrast between semantic and syntac-     volved in the development of subcategorization contrasts
tic violation. Generative theory thus predicts that the    than in the development of selection contrasts, and sub-
distinction between selection and subcategorization will   categorization distinctions produce larger separations in
persevere across languages with di erent fundamental       hidden unit space. Violations are cases where the in-
word orders. To see if the network made a similar sepa-    formation provided by the current word clashes with the
ration, we tested it on the output of a grammar exactly    information provided by the preceding context. The net-
like Grammar 1 except that the order of constituents       work responds to such clashes by averaging the con ict-
was systematically Subject (Object) Verb (SOV) rather      ing signals. In the case of selection violation, this averag-
than Subject Verb (Object) (SVO). Indeed a similar re-     ing interpolates between nearby structures. In the case
lationship between goodness values obtained in the SOV     of syntactic violation, the averaging interpolates between
case (Table 2).                                            widely separated, major clusters. As a result, syntactic
   A disadvantage of Rohde and Plaut's goodness mea-       violations tend to result in greater displacement from
sure is that it does not explicitly characterize the ef-   familiar territory. We hypothesize that the empirical
fects on processing of making a low-probability transi-    results of McElree & Grith (1995), Ni et al. (1998),
tion. The experiments of Ni et al. (1998) and Braze et al. and Braze et al. (1999), which found syntactic viola-
(submitted) indicate that people react to the anomaly of   tions more readily detected than semantic, stem from
a sentence at or after the anomalous word or words (in     this contrast: wildly divergent states are easier to distin-
Rohde and Plaut's terms, after they have made a low-       guish from normal states than slightly divergent ones.

0 SIMULATION 2                                                                                                                                                4
                      Simulation 2                              Figure 1: Principal component projection of the visita-
Samples of geometric relationships in the SRN's hidden          tion set for the Simulation 2 network.
unit space do not make it clear what the network's to-                                         0
tal generalization behavior is, nor whether its coverage
of a language can match that of symbolic phrase struc-
ture rules. Nor do relative distance measures alone ex-
                                                                                             −0.2
plain the eye-tracking and brain-imaging results indicat-
                                                                     Principle Component 2
ing qualitatively distinct responses to semantic and syn-
                                                                                             −0.4
                                                                                                                         Noun
                                                                                                     Noun
tactic anomaly. Our previous work on sentence process-                                                                 (Object)
ing (Tabor et al, 1997; Tabor & Tanenhaus, 1999) sug-                                                                                          Period
                                                                                             −0.6
                                                                                                     (Subject)
gests that the study of dynamical settling networks can
clarify the structural principles underlying connectionist                                   −0.8
sequence-learning. We designed the Bramble Network
(BRN) to explore this hypothesis. The BRN is similar                                          −1                                        Verb
to the simple version of the SRN that has one input layer,
one recurrently connected hidden layer, and one output                                       −1.2
layer. But the BRN has two sets of recurrent connections
in the hidden layer. One set, the discrete weights, works                                    −1.4
                                                                                                −1         −0.5        0          0.5          1        1.5
like the recurrent connections in the SRN, changing the                                                           Principle Component 1
hidden activations discretely every time a new word is
read. The other set, the continuous weights, undergoes
continuous settling according to Equation (1).                     As in Simulation 1, the network was trained on output
                                                                from Grammar 1. In this case, we trained it directly on
                     dvi                                        the output of the grammar for 200,000 words of discrete
                         = neti ; vi                   (1)
                                                                training (learning rate = 0.002, momentum = 0.9) and
                     dt
                                       P
where vi = unit state, neti = bi + j wij (vj ), bi = unit      then 120,000 words of continuous training (learning rate
bias, wij =weight from j to i, and (x) = tanh(x).              = 0.05, no momentum). At this point, both discrete
   In the BRN, the input and context units are updated          and continuous training had successfully distinguished
  rst. Then the input-to-hidden weights and the discrete        the states of the grammar.
hidden-to-hidden weights are used to compute an initial            To gain insight into the organization of the trained
state of the hidden units. Continuous settling is carried       BRN's processing, we saved the trajectories associated
out via the continuous weights among the hidden units.          with a random sample of 200 words in sequence from
Finally, the hidden-to-output weights map the nal state         Grammar 1. We performed Principal Component Anal-
of the hidden units to the output.                              ysis (Jolli e, 1986) on this set of points in order to make
   The discrete weights in the BRN are updated just             the structure visible. The trajectories are graphed in
as in the SRN. We also assume that settling only oc-            Figure 1. (The two principal components shown account
curs for brief periods of time (1 cycle) before the dis-        for 87% of the variance). Note that there are regions cor-
crete weights are updated. This makes it easier for the         responding the major lexical classes (Noun, Verb, and
network to discover dependencies across words. The              Period). There are also discernible subclusters within
continuous weights are updated according to a princi-           the lexical classes. These correspond to both syntactic
ple of stability maximization. That is, for continuous          (e.g. Subject versus Object, Transitive vs. Intransitive)
weights, we de ne the error on unit i as E = (dv =dt)2          and semantic (e.g. Big vs. Small, Edible vs. Inedible)
so that dE =dw = 2(v )(net ; v ). This equation
                                               i      i
                                                                classes as well as some clusters whose determinants we
            i    ij         j      i       i
says: change the weights in the direction that minimizes        have not yet ascertained.
the magnitude of recent activation change. Continuous              We tested the network on the same sets of good and
weight learning is applied only when the network has al-        anomalous sentences that were used in Simulation 1. We
most converged to a stable state. It thus moves the stable      de ned convergence times for the network by using Euler
state in the direction of the initial state, causing bifurca-   integration to compute trajectories with t = 0.05, and
tions when widely separated initial states are associated       stopping a trajectory when the distance between succes-
with a single attractor. The overall e ect is that the at-      sive points on the trajectory passed below a threshold
tractors of the continuous weights tend to track the cen-       (0.005) or when a maximum of 200 steps was reached.
ters of masses of clusters de ned by the discrete weights       The number of steps in the trajectory was taken as a
(cf. Tabor, Juliano, & Tanenhaus, 1997). We found it            model of reading diculty. Table 4 shows mean conver-
most e ective to train the network with a mixture of fast       gence times for several string classes of interest.
(1 cycle) discrete weight training and slow (approximat-           When we designed this model, we expected conver-
ing convergence) continuous weight training. A similar          gence times to provide a good model of human reading
result was produced more quickly when we did all the            times. This prediction is partially sustained in the con-
discrete training rst and then followed it with the con-        trast between normal sentences in their most familiar se-
tinuous training. The simulation we report below used           quence (71.43) and selection violations (84.52), for much
this batch technique.                                           processing evidence supports the claim that readers slow

0 SIMULATION 2                                                                                                                                                       5
Table 4: Mean convergence times (MCT) for Simulation                          Figure 3: The trajectories the network follows upon pro-
2. All comparisons signi cant with p < .001 except be-                        cessing subcategorization violations (solid lines).
tween selection violations and the sample from all well-                                                     0
formed sentences.
  Class                  N MCT                                                                             −0.2
  Well-formed (Ran-
                                                                                   Principle Component 2
  domly generated by                                                                                       −0.4
  grammar)               265 71.43
  Well-formed                                                                                              −0.6
  (Randomly sampled
  from list of all well-                                                                                   −0.8
  formed strings)        220 83.69
  Sel Viol               250 84.52
                                                                                                            −1
  Subcat Viol            274 122.85
  Syntactic Viol         251 155.13
                                                                                                           −1.2
                                                                                                           −1.4
                                                                                                              −1     −0.5          0       0.5         1       1.5
                                                                                                                            Principle Component 1
Figure 2: The trajectories the network follows upon pro-
cessing selection violations (solid lines) against a back-
ground of normal processing (dotted lines).                                   Figure 4: Figure 7. The trajectories the network follows
                               0                                              upon processing category violations (solid lines).
                                                                                                             0
                             −0.2
                                                                                                           −0.2
     Principle Component 2
                             −0.4
                                                                                   Principle Component 2
                                                                                                           −0.4
                             −0.6
                                                                                                           −0.6
                             −0.8
                                                                                                           −0.8
                              −1
                                                                                                            −1
                             −1.2
                                                                                                           −1.2
                             −1.4
                                −1   −0.5        0        0.5       1   1.5
                                            Principle Component 1                                          −1.4
                                                                                                             −1.5   −1      −0.5       0         0.5       1   1.5
                                                                                                                            Principle Component 1
down when they encounter less familiar sequences (see
Jurafsky, 1996). In a loose sense, the model's very high                      tinuous structure that attracts nearby trajectories) run-
reading times for syntactic anomalies are also consistent                     ning from the upper left of the gure to near the lower
with empirical evidence, for Ni et al. (1998) and Braze                       right.
et al. (submitted) found readers making substantial re-                          There also appear to be pieces of connected manifolds
gressive eye movements at syntactic anomalies, which                          extending to the various other regions where normal pro-
implies that they take quite a long time to read past the                     cessing trajectories end. Perhaps the combination of
anomalies. However, it is not clear whether the BRN can                       these manifolds is the locus of grammatical processing.
predict the McElree and Grith results showing fast de-                       Even semantically anomalous transitions and subcate-
tection of syntactic anomalies. It needs to be able to tell                   gorization anomalies land by and large on this manifold,
quickly when it's not in a familiar attractor basin. We                       though the anomalous cases tend to land on di erent
leave this as a question for future work.                                     parts from the normal cases. By contrast, the category
   Figures 2{4 show a sample of selection violations, sub-                    violations generally lead to attractors that are separate
categorization violations, and category violations (tra-                      from the manifold. This suggests that the highly rela-
jectories end on the x's) against the background of nor-                      tivistic network model does make a qualitative distinc-
mal processing (end on the o's). The sample of anoma-                         tion between types of sentences, and its distinction lines
lous events was generated by picking the longest trajec-                      up approximately with current notions of syntactic vs.
tory in each sentence. These graphs reveal an interesting                     semantic structure. It is true that the dividing line seems
structure around which the computation is organized.                          to be di erent from that of standard linguistic theory,
There appears to be a stable connected manifold (con-                         for it is between subcategorization and category error,

0 ACKNOWLEDGMENTS                                                                                                      6
rather than between selection and subcategorization er-      Elman, J. L. (1991). Distributed representations, simple
ror. This di erence may stem from a di erence between          recurrent networks, and grammatical structure. Ma-
our training grammar and natural language: in natural          chine Learning, 7, 195-225.
language, subcategorization constraints are generaliza-      Frege, G. (1892). ber Sinn und Bedeutung. Zeitschrift
tions over more populous classes of items than they are        fr Philosophie und philosophische Kritik 100, 25-50.
in Grammar 1.
                                                             Jurafsky, D. (1996). A probabilistic model of lexical and
                     Conclusions                               syntactic access and disambiguation. Cognitive Sci-
                                                               ence, 20, 137-194.
These graphical results suggest an interesting possibility:  McElree, B. & Grith, T. (1995). Syntactic and the-
the skeleton of a language may be a connected manifold         matic processing in sentence comprehension: Evidence
in a dynamical system. Such a nding would be appeal-           for a temporal dissociation. Journal of Experimen-
ing because a connected manifold contains an in nity of        tal Psychology: Language, Memory, and Cognition,
points, more than we could ever observe. Thus, identi-         21(1), 134-157.
fying such a skeleton could be a way of characterizing
one aspect of the unbounded nature of linguistic gener-      Ni, W., Constable, R.T., Mencl, W.E., Pugh, K.R., Ful-
alization. Such an insight would be similar to the sort of     bright, R.K., Shaywitz, S.E., Shaywitz, B.A., & Gore,
insight that Generative Theory strives for when it posits      J.C. (in press) An Event-related Neuroimaging Study
a phrase structure or transformational architecture. The       Distinguishing From and Content in Sentence Process-
trouble with current Generative models, however, is that       ing. Journal of Cognitive Neuroscience.
the steps leading to their creation are very controver-      Ni, W., Fodor, J.D., Crain, S., & Shankweiler, D. (1998).
sial (witness the plethora of current syntactic theories),     Anomaly detection: eye movement patterns. Journal
the data themselves are controversial (note the disagree-      of Psycholinguistic Research, 27(5), 515-539.
ment about grammaticality judgments), and much of
the decision-making that goes into building models of        Pearlmutter, B.A. (1995). Gradient calculations for dy-
speci c parses is not made explicit (note the paucity of       namic recurrent networks: a survey. IEEE Transac-
implemented parsers that employ modern syntactic the-          tions on Neural Networks, 6(5), 1212{1228.
ory). The dynamical connectionist approach may be an         Rohde, D.L.T. & Plaut, D.C. (1999). Language acquisi-
e ective alternative, for it is based on a relatively uncon-   tion in the absence of explicit negative evidence: How
troversial mathematical theory, it uses performance data       important is starting small? Cognition, 72, 67-109.
rather than competence data and thus does not depend         Rumelhart, D E., Durbin, R., Golden, R., & Chauvin,
on grammaticality judgments, and the process of choos-         Y. (1995). Backpropagation: The basic theory. In
ing a parse is explicit. Moreover, unlike the natural lan-     D.E. Rumelhart & Y. Chauvin (Eds.), Backpropaga-
guage parsers that have been implemented for practical         tion: Theory, Architectures, and Applications. Hills-
application, the connectionist theory makes contact with       dale, NJ: Lawrence Erlbaum Associates.
fundamental questions about the principles that underlie
linguistic representation.                                   Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986).
                                                               Learning Internal Representations by Error Propaga-
                 Acknowledgments                               tion. In Parallel Distributed Processing, Volume I (pp.
                                                               318-362). Cambridge, Massachusetts: MIT Press.
This work was supported by University of Connecticut
Research Foundation Grant # 477138.                          Tabor, W. & Tanenhaus, M. K. (1999). Dynamical Mod-
                                                               els of Sentence Processing. Cognitive Science, 23(4),
Ainsworth-Darnell, K., Shulman, H. and Boland, J.E.            491-515.
   (1998). Dissociating brain responses to syntactic and     Tabor, W., Juliano, C., and Tanenhaus, M. (1997).
   semantic anomalies: Evidence from event-related po-         Parsing in a dynamical system: An attractor-based
   tentials. Journal of Memory and Language, 38, 112-          account of the interaction of lexical and structural con-
   130.                                                        straints in sentence processing. Language and Cogni-
Allen, J. & Seidenberg, M.S. (in press). The emergence         tive Processes, 12(2/3), 211-271.
   of grammaticality in connectionist networks. In B.
   Macwhinney (Ed.), Emergentist Approaches to Lan-
   guage. Hillsdale, NJ: Lawrence Erlbaum Associates.
Braze, D., Shankweiler, D., Ni, W., & Palumbo, L.C.
   (1999). Readers' eye movements distinguish anoma-
   lies of form and content. Manuscript, Department of
   Psychology, University of Connecticut.
Chomsky, N. (1957). Syntactic Structures. The Hague:
   Mouton and Co.,
Elman, J. L. (1990). Finding structure in time. Cogni-
   tive Science, 14, 179-211.

