UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Statistical Brain: Reply to Marcus’ The Algebraic Mind
Permalink
https://escholarship.org/uc/item/9vc1h3sv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Calvo, Francisco
Colunga, Eliana
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

             The Statistical Brain: Reply to Marcus’ The Algebraic Mind
                                       Francisco Calvo (fjcalvo@um.es)
                                   Cognitive Science Programme, East Third Street
                                          Bloomington, IN 47405 USA, and
                                  Departamento de Filosofía, Campus de Espinardo
                                                Murcia, 30100 SPAIN
                                Eliana Colunga (colunga@psych.colorado.edu)
                                        Department of Psychology, 345 UCB
                                           Boulder, CO 80309-0345 USA
                        Abstract                              conform to the pattern they’ve been exposed to
                                                              during habituation (e.g., “ba po po” for infants
   Marcus (2001) argues that only those connectionist         habituated to ABA sequences, and “ba po ba” for
   models that incorporate (classical) rules can              those habituated on ABB ones). The data has been
   account for the phenomenon of transfer of learning
                                                              interpreted as showing that infants exploit (rule-
   in infants. Seidenberg and Elman (1999) have tried
   to counter to Marcus by means of a simple                  governed) abstract knowledge in order to induce
   recurrent network (SRN) trained on a                       the implicit grammar common to different
   categorization task. In this paper we show how a           sequences of syllables: “infants extract abstract-
   prediction-SRN, trained on a simple but structured         like rules that represent relationships between
   pre-training set, can preserve its computational           placeholders (variables) such as ‘the first item X
   equivalency with respect to classical counterparts         is the same as the third item Y’ or more generally
   while eschewing the need to posit rule-governed            that ‘item I is the same as item J’.” (Marcus et al.,
   underlying mechanisms; a criticism that has been           1999).
   raised     against   Seidenberg    and    Elman’s
   categorization-based reply.
                                                              Seidenberg and Elman’s simulation
                     Introduction                             Seidenberg and Elman (1999a) tried to account for
                                                              Marcus et al.’s data in purely statistical terms,
Marcus (2001) distinguishes two separate                      while avoiding implementing a classical
ontologies       in    the     connectionist     realm:       architecture in doing so. Their strategy was
implementational connectionism and eliminative                twofold: They first pre-trained an SRN (Elman,
connectionism. The former accounts for cognitive              1990; fig. 1) on a categorization task. The network
phenomena by positing sets of explicit rules that             had to output 1 or 0, depending on whether the
serve the purpose of symbolic manipulation. The               syllable being fed at a given point was a token of
latter, in terms of computational abilities which             the same type as the syllable fed at the previous
are the result of an associative memory. Marcus               time step. Once the weights were frozen, they
argues that the connectionist models which                    encoded information about the same/different
preserve their computational equivalency with                 relationship that holds for a large set of syllables
respect to classical ones are those that implement            that infants surely have already encountered prior
classical rules.                                              to the experiment. The pre-trained SRN can then
                                                              exploit that knowledge in a subsequent
Transfer of learning in infants                               habituation phase where it is exposed to strings of
                                                              syllables similar to those infants had become
Marcus (2001) assesses the relationship between
                                                              familiar with in Marcus et al.’s experiment. The
connectionist theory and rule-governed behaviour              network’s task in this second phase is to
by challenging the connectionist to account for               categorize strings of syllables in the ABA or ABB
data collected in a number of experiments with                grammatical subsets by outputting a 0 or a 1,
infants. In one well-known experiment, Marcus et              respectively.1
al. (1999) habituated 7-month-old infants to
strings of artificial syllables that belonged to an
                                                              1
ABA or an ABB abstract grammare.g., “le di                     No training took place after presentation of the first
le” or “wi je je”. As Marcus et al. report, infants           two syllables during habituation, nor on the pre-training
listen longer to novel sequences that do not                  output unit (see Seidenberg and Elman, 1999a, for the
                                                              details).
                                                        210

                                                                  Stimuli
              OUTPUT UNITS
                         (habituation/testing)
                                                                  Pretraining corpus
     (pretraining)
                                                                  Following Seidenberg and Elman (1999a), we
                                                                  simulated infants’ linguistic environment, prior to
                                                                  taking part in the experiment, by exposing a SRN
  HIDDEN UNITS                                                    to a pool of syllables (table 1). The corpus
                                                                  consisted of CV syllables formed by
                                                                  concatenating all possible combinations of
                                                                  consonants and vowels in the corpus (resulting in
                                                                  85 syllable types). CV syllables were encoded in a
                                               CONTEXT UNITS      semi-localist way by concatenating consonants
                                                                  and vowels represented locally (table 3).2
                INPUT UNITS
                                                                          Table 1: Stimuli (pre-training phase).
                                                                    va          ve          vi        vo     vu
  Figure 1: Seidenberg and Elman’s SRN (output
                                                                    pa          pe          pi        po     pu
 units for pre-training, and habituation and testing,
                                                                    da          de          di        do     du
                        are different.
                                                                    ya          ye          yi        yo     yu
   The network was then tested on the same ABA                      ga          ge          gi        go     gu
and ABB strings that infants had been tested on in                  ka          ke          ki        ko     ku
the Marcus et al. experiment. Seidenberg and                        ba          be          bi        bo     bu
Elman’s results show that the network can                           wa          we          wi        wo     wu
generalize the knowledge acquired in the                            ma          me          mi        mo     mu
habituation phase to novel stimulialthough see                     na          ne          ni        no     nu
Vilcu & Hadley (2001) for a skeptical appraisal of                  za          ze          zi        zo     zu
their data. According to Seidenberg and Elman,                      sa          se          si        so     su
this supports the view that the infants’ behaviour                  fa          fe          fi        fo     fu
can be accounted for without the positing of an                     la          le          li        lo     lu
abstract grammar that is somehow being tacitly                      ra          re          ri        ro     ru
followed.                                                           ta          te          ti        to     tu
   Marcus, however, is not moved by Seidenberg                      ha          he          hi        ho     hu
and Elman’s simulation. Although their model
neither implements a classical architecture, nor
makes use of symbolic structures directly, the
external teaching signal that it requires to                                   Table 2: Habituation stimuli.
backpropagate error so as to adjust the weight                      le                  di                di
matrix in the categorization task of the pre-                       je                  le                le
training phase implements a rule: “It incorporates                  li                  le                le
a universally open-ended rule of the sort, for all                  we                  le                le
syllables x, y, if x=y then output 1 else output 0.”                wi                  di                di
(Marcus, 1999)                                                      je                  wi                wi
                                                                    li                  we                we
    Generalization in prediction-based                              we                  wi                wi
                           SRNs                                     di                  ji                ji
It is not clear that Marcus’ comments are a real                    je                  ji                ji
threat to connectionism (see Seidenberg and                         ji                  li                li
Elman, 1999b, for a rebuttal), but granting, for                    we                  ji                ji
simplicity’s         sake,       Marcus’          charge   of       de                  di                di
implementation, in what follows we report the                       je                  de                de
results of a simulation that is empirically adequate                li                  de                de
in the face of the infants’ data and does not                       de                  we                we
implement classicism.
                                                                  2
                                                                    Network performance using a distributed version of
                                                                  Plunkett & Marchman (1993) phonetic coding didn’t
                                                                  yield significantly different outcomes.
                                                                2
                                                              211

Habituation corpus                                             During pre-training, on the one hand, and
The habituation corpus consisted of the same 12             during habituation and testing, on the other,
ABB or ABA strings of syllables used by Marcus              different banks of output units were deployed. (No
et al. (table 2). The same semi-localist input              training took place on the habituation bank during
encoding was used as in the pre-training corpus.            pre-training or vice versa).
     Table 3: Habituation stimuli coding.                   Task
  de            0010000000000000                            Pre-training
                000000000010000                             In the pre-training phase we fed the network with
  yi            0000000000000000                            50,000 syllable tokens from the set of 85 syllable
                010100000000000                             types. The task was to predict the next item in the
  li            0000000000000001                            sequence.      There     were     no     (complex)
                000100000000000                             “grammatical” constraints such as those in
  we            0000000000000010                            Elman’s (1990) classical prediction task. To make
                000000000010000                             the task learnable, we varied the amount of
  di            0010000000000000                            syllables that were duplicated. 5 corpora were
                000100000000000                             created where syllable duplication ranged from
  ye            0000000000000000                            0% to 100%. In this way, a network exposed to a
                010000000010000                             0%-duplication corpus would hardly reduce its
  le            0000000000000001                            overall error since random dependencies in such a
                000000000010000                             (noisy) data set would cancel each other out. At
  wi            0000000000000010                            the other end of the spectrum, a network pre-
                000100000000000                             trained on a 100%-duplication corpus would
                                                            decrease its prediction error significantly since it
                                                            would learn that every syllable in the corpus is
Network architecture                                        followed by an identical token. Intermediate
Based on the architecture of Seidenberg and                 corpora (25%, 50%, and 75% repetition) yielded
Elman’s model, we trained a SRN to test if it               error scores in between (figure 3).
could generalize to novel strings of syllables in
the line of Marcus et al.’s experiment. The
network had 31 input and output (pre-training and
habituation) units, and 41 units in both the hidden
and context layers (figure 2).
                                                            pre-training sum_rms error
                OUTPUT UNITS
       (pretraining)     (habituation/testing)
                                                                                         0%   25%            50%            75%   100%
                                                                                                    % syllable repetition
  HIDDEN UNITS
                                                                          Figure 3: Error scores in trained networks
                                                                       decreased as the structure in the pre-training set
                                                                            (percentage of duplication) increased.
                                      CONTEXT UNITS         Habituation
                               INPUT UNITS                  With the weights from the pre-training phase
                                                            frozen, networks were trained on either the ABB
                                                            or the ABA habituation corpus of table 2. Training
  Figure 2: A SRN trained on a prediction task              was stopped at 4 different points in
    (output units for (i) pre-training, and (ii)            learningwhenever the sum of the root-mean-
 habituation and testing, are in different banks).
                                                        3
                                                      212

square error (sum_rms) for the habituation output                                with no duplication (0%), 25% of syllables
bank of units fell below 1,2; 1,1; 1,0; and 0,9.3                                dupplicated, 50%, 75% and 100%. Half of these
                                                                                 networks were trained on ABB patterns during the
Generalization                                                                   habituation phase and the other half were trained
With the weights frozen from the habituation                                     on ABA patterns during the habituation phase.
phase, we tested performance on novel data. To                                   Figure 4 shows the relative facilitation for
confirm the robustness of our results, we used an                                predicting congruent over incongruent patterns.
extended corpus of 1,000 syllables forming a                                     To calculate this, we used the average of the
sequence of ABB and ABA novel strings. The                                       incongruent minus the congruent sum_rms for
same syllables were used for both grammatical                                    networks pre-trained on 0% to 100%-duplication
patterns.                                                                        corpora, and networks that were not pre-trained
   20 tests were done for the 20 different weight                                previously.
matrices obtained by habituating the SRN on the                                    If the networks have abstracted the general
0,9 to 1,2 sum_rms measure stops for all pre-                                    patterns of the grammars present during
training corpora (0% to 100% duplication). For                                   habituation, the difference of error in incongruent
each trial in the generalization corpus, the                                     minus congruent patterns should be positive.
prediction error was recorded for next item                                      Additionally, if the kind of pre-training we are
presented after the presentation of the first two                                using is in fact necessary for modeling this
syllables. Differences in prediction error between                               phenomenon, we expect to see an advantage of
ABA’s and ABB’s third syllable were computed.                                    congruent patterns in networks trained with some
The prediction is that networks habituated to ABB                                repetition, but not in those trained without
patterns will have an advantage when predicting                                  repetition or those with no pre-training.
ABB patterns over when predicting ABA patterns,                                    These data were submitted to a 6(Pre-training) x
but networks habituated to ABA patterns will                                     2(Habituation) ANOVA. The results show a
show an advantage predicting ABA patterns over                                   significant main effect of Pre-training (p<.01).
ABB patterns.                                                                    The congruency effect is bigger for networks that
                                                                                 have more duplication in their pre-training. Post-
1,2                                                                              hoc tests revealed no difference between networks
                                                                                 that received no pre-training and networks that
      1
                                                                                 were pre-trained in an unstructured corpus that
0,8                                                                              had no consistent (p=.28). There was also a
                                                                                 significant main effect of Habituation (p<.01).
0,6
                                                                                 Post-hoc analysis revealed that the congruency
0,4                                                                              effect was bigger for networks habituated to ABB
                                                                                 patterns than for networks habituated to ABA
0,2
                                                                                 patterns (p<.01).
                                                                                   The analysis also revealed a significant
    difference
      0
                     no          0%       25%      50%      75%     100%         interaction between Habituation and Pre-training
                 pretraining
                                                                                 (p<.01) – the effect of pre-training was greater for
                               percentage of syllables duplicated                networks habituated to ABB patterns than for
                                                                                 networks habituated for ABA patterns.
         Figure 4: difference between congruent vs.
       incongruent patterns during generalization after
                         habituation.
                                                                                                    Discussion
                                                                                 The results of our simulations show that simple
                                          Results                                SRN networks, when trained on a simple but
                                                                                 structured corpus, will generalize the abstract
To see the role pre-training played in the overall                               patterns embodied in their training set and gain an
task, we ran 10 networks in each of five Pre-                                    advantage in processing subsequent patterns of the
training conditions: no pre-training, pre-training                               same type. That is, like the infants in Marcus et
3
                                                                                 al’s study, the networks that were pre-trained in a
   The simulations were run with PDP++ (O’Reilly,                                corpus in which some syllables were consistently
Dawson, and McClelland), and trained with a learning                             duplicated, learned to distinguish ABB patterns
rate of 0,05 during pre-training, and of 0,01 during
                                                                                 from ABA patterns after a brief period of training
habituation. Although calculating error measures of
probability-based predictions against likelihood vectors
                                                                                 akin to infant’s habituation.
would have been more informative, for current purposes                              There is crucial difference between Seidenberg
(i.e., assessment of the role of pre-trainingsee                                and Elman’s model and ours. Their network was
discussion, below) sum_rms values suffice.                                       trained both in the pre-training and the habituation
                                                                             4
                                                                           213

phases on a categorization task. Although, that is           habituation task? One possibility is that networks
perfectly legitimate, there is a striking difference         need to develop consistent and coherent
between the habituation phase in their simulation            representations for the syllables in the corpus (the
and the way infants were familiarized with their             words in Marcus’ habituation) in order to more
linguistic environment in the Marcus et al.’s                effectively encode the patterns seen during the
original experiment. Whereas infants were                    short habituation phase. This possibility is
exposed to strings of syllables that belonged to a           weakened, however, by the fact that networks pre-
single grammar (e.g., “le di di” and “wi je                  trained in a corpus made out of syllables but
je”ABB), Seidenberg and Elman’s network was                 containing no additional structure in terms of
habituated to two sets of strings that conformed to          duplication, do not perform better than the
two different grammars (e.g., “le di                         networks that receive no pre-training at all. A
di”ABB“wi je wi”ABA). The task then                       second possibility is that during pre-training the
consisted in the correct categorization of                   networks learn to represent something general
exemplars into the ABB and ABA categories. Our               about duplication, in other words, sameness. This
network, on the contrary, is trained in both phases          abstraction could be crucial in encoding the
(pre-training and habituation) on a prediction task.         patterns during the habituation phase. This could
Importantly, the model, like the infants of the              explain the apparent advantage of networks in
Marcus et al. experiment, is exclusively exposed             dealing with ABB patterns compared to ABA
to positive examples of ABB strings of syllables             patterns – duplication is a part of ABB and could
(i.e., all generated by a single grammar).                   be used to encode it more efficiently. This
   The fundamental component of our simulation               advantage of ABB patterns, then, may disappear if
is the nature of the pre-training environment. The           the networks are trained on a corpus containing
pre-training environment does not merely consist             longer distance dependencies.
of a large set of syllables where the input signal is           Interestingly, the networks predict differences
generated by the random concatenation of                     between ABA and ABB patterns that may fall
exemplars in the data set, but, crucially, some              from the nature of short-term vs. long-term
syllable tokens are normally encountered followed            memory. Networks learn ABB patterns faster than
by other tokens that belong to the same                      ABA patterns and habituate faster to ABB than
representational type. So, for example, in a                 ABA patterns. We are currently working on an
(reduced) ecological context, such as the one                extended model that differentiates short-term
infants encounter in their first months of life,             (habituation) and long-term (pre-training) memory
“ma” and “pa” are very frequently followed by                and concurrently testing these predictions in 7-
another “ma” and “pa” exemplar, respectively,                month-old infants. By combining these
whereas some other syllables (e.g., “the”) may be            simulations with empirical testing of their
followed by almost any syllable in the corpus. In            predictions we hope to shed light on the
these simulations we abstract this distinction to its        mechanisms involved in this phenomenon.
two extremes and present the network during pre-
training with syllables that are either always                                   Conclusion
duplicated or never duplicated. These first-order            The research reported here shows how Marcus’
correlations in the environment amount to sub-               challenge can be met while avoiding the positing
regularities that can be exploited by the network            of rule-fitting patterns of behavior (allegedly
in a semi-deterministic prediction task.4 This is            required to constrain novel data). In our view,
how a SRN can be pre-trained on a prediction                 Marcus correctly points out that “Seidenberg and
task, bypassing some of the problems faced by                Elman do not give an account of how the
Seidenberg and Elman’s categorization-based                  supervisor’s rule could itself be implemented in
solution.                                                    the neural substrate” (2001, p. 65). The teaching
   So what do the networks learn from pre-training           signal of a fully supervised categorization task is
that helps them abstract the regularities in the             not ecologically grounded. In our self-supervised
                                                             prediction task, that’s not an issue. Activation-
4                                                            based signals in a prediction task are not to be
  The assumption that syllables are not concatenated in
a purely random fashion is empirically justified. Notice
                                                             interpreted in terms of rule-implementation. The
that infants start to babble at the age of 6-months. The     teaching signal exploited is an activation state of
linguistic stream they encounter as input is thus made       experience. As McClelland emphasizes with
up of the combination of their natural environment           regard to the ecological plausibility of this type of
(with the existing first-order correlations among            signals: “the brain is constantly generating
syllable sets) plus the babbling repetitions uttered by      expectations and the discrepancies between these
themselves.                                                  expectations and subsequent outcomes can be
                                                           5
                                                         214

used for error-driven learning” (from O’Reilly and                            References
Munakata, 2000). The network encodes the
                                                         Elman, J.L. (1990). Finding structure in time.
existing statistical regularities with no need to
                                                           Cognition, 14, 179-211.
process algebra-like information. Our working
                                                         Marcus, G. (1999). Reply to Seidenberg and
hypothesis is that infants make use of
                                                           Elman. Trends in Cognitive Sciences, 3, 289.
discrepancies based on expectations to make
                                                         Marcus, G.F. (2001). The Algebraic Mind.
successful predictions.
                                                           Cambridge, Mass.: MIT Press.
   Marcus (2001) claimed that connectionist
                                                         Marcus, G.F., Vijayan, S., Rao, S.B. & Vishton,
networks would lose their empirical adequacy               P.M. (1999). Rule learning in seven-month-old
unless they implemented a classical architecture.          infants. Science, 283, 77-80.
Marcus’ infant data is accounted for in statistical      O’Reilly, R. & Munakata, Y. (2000).
terms without the positing of devices that store          Computational Explorations in Cognitive
particular values of variables to perform variable         Neuroscience: Understanding the Mind by
bindings, such as register sets, or other classical        Simulating the Brain. Cambridge, Mass.: MIT
resources. The charge of implementation is                 Press.
therefore not applicable to our results, since the       Plunkett, K., & Marchman, V. (1993). From rote
ecologically grounded prediction task of the               learning to system building: Acquiring verb
networks does not incorporate universally open-            morphology in children and connectionist nets.
ended rules.                                               Cognition, 48, 21-69.
                                                         Seidenberg, M.S. and Elman, J.L. (1999a).
              Acknowledgements                             Generalization, rules, and neural networks: A
                                                           simulation         of      Marcus      et      al.
This work was supported in part by a Ramón y               http://crl.ucsd.edu/~elman/Papers/MVRVsimulation.
Cajal research contract to the first author (Spanish       html.
Ministry of Science and Technology). This                Seidenberg, M.S. and Elman, J.L. (1999b).
material draws out of preliminary work presented           Networks are not ‘hidden rules.’ Trends in
at the 6th Conference of the Association for the           Cognitive Science, 3, 288-289.
Scientific Study of Consciousness (ASSC’6-2002)          Vilcu, M. and Hadley, R.F. (2001). Generalization
in Barcelona, Spain.                                       In Simple Recurrent Networks. Proceedings to
                                                           the 23rd Annual Conference of the Cognitive
                                                           Science Society (pp. 1100-1105). Hillsdale, NJ:
                                                           Lawrence Erlbaum Associates.
                                                       6
                                                     215

