UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Theory-Based Induction
Permalink
https://escholarship.org/uc/item/18h7c6p0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Kemp, Charles
Tenenbaum, Joshua B.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                        Theory-Based Induction
                                       Charles Kemp (ckemp@mit.edu)
                                   Joshua B. Tenenbaum (jbt@mit.edu)
                                     Department of Brain and Cognitive Sciences
                                         Massachusetts Institute of Technology
                                           Cambridge, MA 02139-4307 USA
                         Abstract                             possible unless a learner begins with some sort of in-
                                                              ductive bias (Mitchell, 1997). The challenge that
   We show how an abstract domain theory can be in-           inspires our work is to develop a model with an in-
   corporated into a rational statistical model of induc-     ductive bias that is well motivated by a theory of
   tion. In particular, we describe a Bayesian model          the domain under consideration.
   of category-based induction, and generate the prior
   distribution for our model using a formal theory              Many previous models have taken similarity
   of the distribution of biological properties across        judgments as their representation of prior knowl-
   classes of biological kinds. Our theory-based model        edge (Nosofsky, 1986; Osherson et al., 1990). This
   is both more principled than previous approaches           approach has been dominant within the tradition
   and better able to account for human ratings of ar-        of category-based induction, and Osherson et al.’s
   gument strength.
                                                              (1990) similarity-coverage model will be one stan-
                                                              dard against which our new model will be compared.
   Philosophers since Hume have struggled with the            Using similarity data to represent prior knowledge
logical problem of induction, but children solve an           is a reasonable flrst attempt, but similarity judg-
even more difficult task — the practical problem of           ments are less than ideal as a starting point for a
induction. Children somehow manage to learn con-              model of inductive inference. As Goodman (1972)
cepts, categories, and word meanings, and all on the          has pointed out, similarity is a vague and elusive
basis of a set of examples that seems hopelessly in-          notion. It is meaningless to say that two objects
adequate. The practical problem of induction does             are similar unless a respect for similarity has been
not disappear with adolescence: adults face it ev-            specifled. Any model based on similarity alone is
ery day whenever they make any attempt to predict             therefore a model without a secure foundation.
an uncertain outcome. Inductive inference is a fun-              Instead of relying on similarity, the model devel-
damental part of everyday life, and a fundamental             oped in this paper is founded on a simple theory
phenomenon in need of a psychological explanation.            of a particular domain of reasoning: kinds of ani-
   Two important questions can be asked about in-             mals and their properties. The theory consists of
ductive generalization: what resources does a per-            two components: the ‘taxonomic principle,’ which
son bring to an inductive task, and how are these             holds that the set of animals naturally forms a tree-
resources combined to generate a response to the de-          structured taxonomy, and the ‘distribution princi-
mands of the task? In other words, what is the pro-           ple,’ which specifles how properties are probabilis-
cess of induction, and what is the prior knowledge            tically distributed over the taxonomy. These two
required by this process? Psychologists have consid-          principles are used to generate a prior distribution
ered both of these questions in depth, but previous           for a Bayesian model of category-based induction.
computational models of induction have tended to                 Our approach is located at the most abstract of
emphasize process to the exclusion of prior knowl-            Marr’s three levels: the level of computational the-
edge. This paper attempts to redress this imbalance           ory (Marr, 1982). Our goal is not to describe the
by showing how prior knowledge can be included in a           process by which people make inductive inferences,
computational model founded on rational statistical           but rather to explain why people reason the way that
inference.                                                    they do and how they can reliably come to true be-
   The importance of prior knowledge has been at-             liefs about the world from very limited data. Intrigu-
tested by psychologists and machine learning the-             ingly, both the taxonomic principle and the distri-
orists alike. Murphy and Medin (1985) have sug-               butional principle resemble analogous principles in
gested that the acquisition of new concepts is guided         evolutionary biology and genetics. People’s remark-
by “theories” — networks of explanatory connec-               able ability to make successful inductive leaps may
tions between existing concepts. Machine learning             thus be explained as the result of rational inference
theorists have built formal models of learning, and           mechanisms operating under the guidance of a do-
argued that generalization within these models is not         main theory that reflects the true structure of the
                                                          658

environment.                                               Both are deflned in terms of sim(·), the standard
   We begin by introducing previous approaches             pairwise similarity metric, assumed as a primitive.
to the problem of category-based induction. We                The similarity-based approach can ofier no princi-
then set out a ‘theory of biological properties’ that      pled reason for preferring one of these metrics. Osh-
can generate the prior distribution for a Bayesian         erson et al. suggest that maxsim(·) conforms better
model of induction. Next we turn to experimental           to their intuitions, yet sumsim(·) is more standard
data, and show that our new model performs bet-            in models of inductive learning, categorization, and
ter than previous approaches across a collection of        memory. Later we show that maxsim(·) flts the ex-
four data sets. We conclude by discussing ways in          perimental data much better than sumsim(·) and of-
which Bayesian and traditional similarity-based ap-        fer a possible explanation for its success.
proaches might be complementary, and directions for
future work.                                               Bayesian Models. Heit (1998) and Sanjana and
                                                           Tenenbaum (2003) considered Bayesian approaches
         Category-Based Induction                          to category-based induction. Assume that we are
                                                           working within a flnite domain. For the tasks mod-
The tasks to be considered were introduced by Osh-         eled here, the domain will be a set of ten mammal
erson et al. (1990). In the flrst task (the speciflc in-   kinds. We are interested in a concept, C, that picks
ference task), subjects are asked to rate the strength     out some subset of these objects. In the examples
of arguments of the following form:                        above, C is the concept “mammals that can get dis-
             Horses can get disease X                      ease X.” Let H be our hypothesis space: the set of
             Cows can get disease X                        all possible concepts over our domain. With 10 ani-
             Dolphins can get disease X                    mal types, there are 210 distinct subsets of animals,
The premises state that one or more speciflc mam-          or logically possible concepts. To each hypothesis h
mals can catch a certain disease, and the conclusion       in H we assign a prior probability p(h), where p(h)
(to be evaluated) states that another speciflc species     is the probability that h is the concept of interest.
(here dolphins) can also catch the disease.                   Osherson’s flrst task may now be formalized as
   In the second task (the general inference task),        follows. We observe X, a set of n examples of the
subjects are asked to consider a generalization from       concept C, and want to compute p(y ∈ C|X), the
speciflc premises to a property of all mammals. For        probability that another object, y, is also a member
instance:                                                  of C. Summing over all hypotheses in H, we have:
           Seals can get disease X                                                  X
           Dolphins can get disease X                           p(y ∈ C|X) =            p(y ∈ C, h|X)         (1)
           All mammals can get disease X                                           h∈H
                                                                                    X
                                                                               =        p(y ∈ C|h, X)p(h|X). (2)
                Previous Models                                                    h∈H
Similarity-based models. Osherson’s similarity-
coverage model expresses the strength of an ar-            Now p(y ∈ C|h, X) equals one if y ∈ h and zero
gument as a linear combination of two compo-               otherwise (independent of X). Thus:
nents: a term representing the similarity between                                        X
the premises and the conclusion, and a term rep-                   p(y ∈ C|X)     =            p(h|X)         (3)
resenting the extent to which the premises cover                                       h∈H:y∈h
the lowest level taxonomic category including both                                       X     p(X|h)p(h)
premises and conclusion.                                                          =                           (4)
                                                                                                  p(X)
   Formalizing these ideas, the strength of the ar-                                    h∈H:y∈h
gument from a set of premises X to a conclusion
category Y is:                                             where the last step follows from Bayes’ rule.
                                                              The numerator in Equation 4 depends on p(X|h),
    α setsim(X, Y ) + (1 − α) setsim(X, [X; Y ])
                                                           the likelihood of X given h, as well as on the prior
where α is a free parameter, setsim(·) is a setwise        p(h). Assuming the n examples in X are sampled
similarity metric, and [X; Y ] is the lowest level tax-    independently at random from h yields:
onomic category including X and Y .
   Several setwise similarity metrics might be tried.                 (
                                                                          1
                                                                            n, if all n examples in X belong to h
Osherson et al. propose maxsim(·) but also consider        p(X|h) = |h|
sumsim(·):                                                              0,     otherwise
                           X
      maxsim(X, Y ) =          maxi sim(Xi , Yj )          where |h| denotes the size of h (the number of in-
                             j
                           XX                              stances in its extension). The denominator in Equa-
      sumsim(X, Y )    =            sim(Xi , Yj ).         tion 4 can be computed
                                                                               P        by summing over all hy-
                             j   i                         potheses: p(X) = h∈H p(X|h)p(h).
                                                       659

   Osherson’s general inference task is formulated
similarly. The probability that all of the members
of category Y belong to C is:
                      X
    p(Y ⊂ C|X) =          p(Y ⊂ C|h, X)p(h|X) (5)
                       h∈H
                         X
                  =               p(h|X).         (6)
                       h∈H:Y ⊂h
   The only piece missing from the Bayesian frame-
work is a speciflcation of how the prior probabilities
p(h) are calculated. Heit (1998) does not address
                                                                     gorilla
                                                                                             elephant                   squirrel
                                                                                                        rhino
                                                             chimp                                                                 dolphin
                                                                               horse                            mouse
this question, and Sanjana and Tenenbaum (2003)                                        cow                                                   seal
use a prior distribution that is not deeply motivated
by a theory of the domain. We now describe a prin-
cipled method for generating the prior distribution.       Figure 1: A taxonomy built from similarity data
          A Theory-Based Model                               A simple prior distribution can be generated us-
The prior distribution for our Bayesian model is mo-      ing the taxonomy alone. There are 19 nodes in the
tivated by two principles: the ‘taxonomic principle’      tree (one for each animal type, together with nine
and the ‘distribution principle.’ Together these prin-    internal nodes), and each specifles a concept that
ciples form a theory of the distribution of biological    includes the animals falling beneath it on the tree.
properties.                                               A straightforward way to set the prior is to assign a
   The taxonomic principle holds that animals nat-        uniform probability to each of these 19 concepts, and
urally fall into a tree-structured taxonomy – a col-      zero probability to all other possible concepts. We
lection of hierarchical groups. This belief may well      call the model that uses this prior the ‘Taxonomic
be universal. A substantial body of work has doc-         Bayesian model.’
umented that cultures all over the world organize            The Taxonomic model is appealingly simple and
living kinds into ‘folk taxonomies’ (Atran, 1995). It     corresponds roughly to some informal accounts of
is also scientiflcally sound, as the theory of evolu-     taxonomically driven induction (e.g., Atran, 1995).
tion implies that living kinds should conform to a        But to see that it is not adequate, compare the argu-
tree structure.                                           ment “seals and squirrels catch disease X, so horses
   Our flrst step towards generating a prior distribu-    are also susceptible” with “seals and cows catch dis-
tion is therefore to build a folk taxonomy for the ten    ease X, so horses are also susceptible.” The second
mammals in our domain. Osherson collected simi-           is stronger than the flrst, yet the Taxonomic model
larity ratings between all pairs of animals in the do-    assigns them both the same rating, since each set of
main, and we use these ratings to deflne a distance       premises is compatible with only one hypothesis, the
measure d, where d(x, y) = 1 − sim(x, y). We then         set of all mammals.
perform average-link clustering, which flrst assigns         The distribution principle, the second part of our
each animal to its own cluster, then proceeds by re-      theory, states that concept membership (or feature
peatedly merging the two closest clusters. The tree       possession) depends on a process of random muta-
produced is shown in Figure 1.                            tion operating over the taxonomy. This principle
   Although our distance measure is deflned in terms      acknowledges that convergent evolution can occur:
of similarity, our approach does not depend crit-         that two animals may share a property even if it
ically on similarity as a primitive. We could use         occurs in none of their common ancestors. Some
other measures of distance: for example, one could        additional notation is needed to make this principle
represent each animal using a set of behavioral and       precise.
morphological features (e.g., ‘lives in water,’ ‘has a       Imagine that membership in C, the concept to be
tail’), and set the distance between two animals to       learned, depends on a single feature F that could
be the distance between their feature vectors. We         have evolved at any point in the tree and may have
could also use more structured domain knowledge           evolved independently along difierent branches of
that might obviate the need for any bottom-up clus-       the tree. Once F has arisen along a branch, all
tering. Building the taxonomy without reference to        nodes falling below that branch are members of C.
similarity is our preferred approach, but using Os-       For example, if F appears at the points marked with
herson’s similarity data yields one important payofi      crosses in Figure 1, then C will include chimps, go-
for the present study: it allows the performance of       rillas, dolphins and seals.
our model to be directly compared with the perfor-           We model the evolution of F using a Poisson ar-
mance of the similarity-based models.                     rival process with a free parameter, λ that will be
                                                    660

called the mutation rate. The Poisson process as-         strength. Figure 2 shows how well these ranks match
sumes that mutations arrive randomly, potentially         the ranks assigned by humans.
occurring at any point along any tree branch, but            The flrst three columns show the performance of
are constrained to respect an overall rate of λ. The      the models on three data sets published in previous
probability that the feature develops along a branch      studies. The Osherson general set contains 45 three-
b of length |b| is one minus the probability that the     premise general arguments. The Osherson speciflc
mutation arrives zero times along that branch:            set contains 36 two-premise arguments, and the San-
                                                          jana set contains 28 speciflc arguments with a vari-
        p(F develops along b)    =   1 − e−λ|b| .   (7)   able number of premises.
                                                             All of the models (except Taxonomic Bayes) in-
A branch is ‘marked’ if F develops along that             clude a single free parameter, and each correlation
branch.                                                   in Figure 2 is shown for the setting of the param-
   To obtain a single estimate of the extension of        eter that best flts the human data. As expected,
C, we consider all branches in the tree, label each       Taxonomic Bayes performs poorly, but the other
as marked or unmarked according to Equation 7,            two Bayesian models both outperform the similarity-
then collect all external nodes that fall beneath a       based models over the flrst three datasets. Both
marked branch. Repeating this many times gener-           of these Bayesian models show robust performance
ates a prior distribution over all possible concepts,     across a range of parameter settings, and both admit
where the probability assigned to a concept is the        a single setting that achieves correlations exceeding
proportion of times it was chosen as our estimate of      0.9 on the flrst three data sets.
C.
   This prior distribution may also be computed an-       A New Experiment. A limitation of the Dis-
alytically. First consider all single branches in the     junctive Bayes model is that it does not capture at
tree. For each branch, calculate the probability that     least one phenomenon documented by Osherson et
F arises along that branch and nowhere else in the        al. (1990). General arguments tend to increase in
tree, and add this probability to the prior for the       strength as the premises become more typical of the
corresponding concept. Continue by considering all        conclusion category. For example, since horses are
pairs of branches, all triples, and so on.                more typical mammals than seals,
   Our model of the evolution of F captures two im-                   Horses can get disease X
portant intuitions. First, F is more likely to develop                All mammals can get disease X
along the longer branches of the tree. Second, since      is a stronger argument than
F develops independently along difierent branches
and the probability of arising on any branch is small,                Seals can get disease X
the model favors simpler hypotheses — hypotheses                      All mammals can get disease X
consisting of fewer rather than more clusters.               Although the Evolutionary model was not built
   An alternative prior over all possible concepts was    with premise typicality in mind, we collected new
considered by Sanjana and Tenenbaum (2003). They          data which show that it captures this efiect more
also compute p(h) by taking disjunctions of the 19        successfully than the Disjunctive Bayes model. Ten
hypotheses represented by the folk taxonomy. The          single-premise general arguments (one for each
19 original hypotheses are assigned the highest prior     species in our domain) were printed on a set of cards,
probability, disjunctions of two of these are assigned    and 25 undergraduates sorted these cards in order of
a somewhat smaller probability, and disjunctions of       increasing argument strength. The average rank of
three hypotheses are assigned a still smaller prob-       each argument was calculated and compared with
ability. This approach represents a general strat-        the ranks assigned by the models. Owing to the
egy for expanding any hypothesis space, and can           limited number of arguments, correlations are much
be applied to hypothesis spaces that have nothing         lower than for the previous three data sets. Figure 2
at all to do with taxonomies. Generality, however,        nonetheless shows that the Evolutionary Bayes and
is bought at a price: unlike our new ‘Evolutionary’       similarity models partially capture the premise typ-
model, the ‘Disjunctive Bayes’ model of Sanjana and       icality efiect, but the other Bayesian models do not.
Tenenbaum is not deeply motivated by the structure
of the domain. A symptom of this lack of principled            Evolutionary Bayes and Maxsim
motivation is that Disjunctive Bayes does not take        Sumsim performs dramatically worse than Maxsim,
the branch lengths of the taxonomic tree into ac-         to the point of being anticorrelated with people’s
count, and thus fails to predict important efiects of     judgments on the Osherson general stimuli. This
typicality that we describe below.                        conflrms the intuition that maxsim(·) is the better
                                                          metric for category-based induction, but the superi-
         Performance of the Models                        ority of Maxsim still awaits a principled explanation.
Each model presented in the previous section can be          Heit (1998) and Sanjana and Tenenbaum (2003)
used to rank a set of arguments in order of increasing    have suggested that Bayesian analyses might explain
                                                      661

                                       r=0.91             r=0.98                       r=0.97                    r=0.54
       Disjunctive Evolutionary
                                       r=0.94             r=0.97                       r=0.97                    r=−0.43
                                       r=0.51             r=0.41                        r=0.9                    r=−0.43
       Taxonomic
                                       r=0.87              r=0.9                       r=0.93                    r=0.62
       Maxsim
                                      r=−0.33             r=0.87                       r=0.39                    r=0.62
       Sumsim
                                  Osherson General   Osherson Specific             Sanjana Specific         Premise Typicality
Figure 2: Model predictions (x axis) vs human judgments (y axis). Each row shows the performance of a
model, and each column shows the performance over a data set. Every model (except Taxonomic Bayes)
includes a single free parameter, and the plots shown are for the best setting of that parameter.
the success of other approaches to category-based                              ary Bayes, even though Evolutionary Bayes is super-
induction. Noticing that Maxsim and Evolution-                                 flcially much more similar to Disjunctive Bayes than
ary Bayes perform similarly on all four data sets,                             Maxsim. This result supports the idea that Maxsim
we conjectured that Maxsim might efiectively be an                             and Evolutionary Bayes may specify a very similar
approximation to the more principled but more com-                             mapping between their input (the similarity matrix)
plex Evolutionary Bayes model. Such a correspon-                               and their output (ratings of argument strength).
dence would support a rational justiflcation for why                              Marr (1982) proposed three broad levels at which
human inference might follow the Maxsim rule in                                a psychological account may be situated. The
this domain.                                                                   Bayesian approach is best suited for the formula-
  To further explore the relationship between these                            tion of models at the most abstract level of “com-
two models, we ran a simulation using a set of 100                             putational theory.” In contrast, Maxsim falls most
randomly generated taxonomies. Each taxonomy                                   naturally into Marr’s second level as an algorithm
was generated by starting with a set of 10 nodes,                              that might implement the computational theory in
and merging pairs of nodes at random until only                                a psychologically plausible way. The similar perfor-
one remained. The branch lengths were generated                                mance of these two models supports the idea that
by choosing 9 random numbers between 0 and 1,                                  they are complementary. Evolutionary Bayes helps
and setting the height of the node created by the                              to show why Maxsim may be a reasonable model
kth merge to the kth smallest of these numbers. For                            of inductive generalization, and Maxsim provides an
each taxonomy, we calculated the correlations be-                              existence proof that the computations required by
tween the predictions of each pair of models on an                             the Bayesian model can be approximated by simple
analog of the Osherson general task. To calculate the                          heuristics.
predictions of the similarity models, the similarity of
two objects was deflned to be one minus the length of                                                 Discussion
the path joining the objects in the tree. This makes                           The prior distribution used by the Evolutionary
sense under the assumption that the tree approxi-                              model follows directly from the theory consisting
mates the structure used to generate the similarity                            of the taxonomic and distribution principles. It is
judgments.                                                                     striking that a model inspired by ideas about ran-
  Figure 3 shows the outcome of this simulation.                               dom mutation and convergent evolution can predict
The pair of models that matched most closely on                                people’s intuitive judgments so well, but this should
these general arguments was Maxsim and Evolution-                              not be surprising if we believe that the success of
                                                                         662

           mean=0.94                  mean=−0.22            tree. Artifacts are one example of a non-biological
  50                                                        domain that may meet this condition. Consider, say,
                                                            the set of all electronic devices. Any two devices that
    0                                                       share a QWERTY keyboard are similar partly be-
     Evolutionary and Maxsim    Evolutionary and Sumsim
                                                            cause both grew out of a single previous technology.
                                                               Our behavioral experiment explored typicality ef-
           mean=0.84                  mean=0.57
  50                                                        fects, which have often been interpreted as evi-
                                                            dence against taxonomically structured represen-
    0                                                       tations with all-or-none concepts and in favor of
   Evolutionary and Disjunctive  Disjunctive and Maxsim     more graded, similarity-based representations. We
                                                            showed that typicality efiects in inductive reason-
           mean=−0.53                 mean=−0.014           ing are in fact compatible with a taxonomy of all-
  50
                                                            or-none concepts, under the appropriate inference
    0                                                       engine and prior probability distribution. Typical-
    −1            0        1    −1          0         1     ity may arise not from the intrinsic format of the
      Disjunctive and Sumsim       Maxsim and Sumsim        knowledge representation, but from the inferential
                                                            processes operating over that representation.
Figure 3: Frequencies (y-axis) against correlations            More generally, by allowing a natural combina-
(x-axis). Each histogram shows the distribution of          tion of structured domain knowledge with proba-
the correlations achieved by a pair of models across        bilistic inference, our Bayesian framework ofiers an
100 random trees.                                           alternative to the traditional debates of “structure
                                                            versus statistics” that have polarized much of cogni-
human cognition rests on our ability to abstract the        tive science. From a rational functional standpoint,
deep enduring structure of our environment. It is           “structure” and “statistics” are not competitors. In-
an open question whether the biological principles          ductive inference should be most successful when it
guiding our model are explicitly represented in peo-        brings the most powerful inferential machinery to-
ple’s minds, or only implicitly present in the infer-       gether with the most accurate domain knowledge.
ence procedures they use. Studies with experts in           Here we have worked out one version of this ratio-
biological taxonomy, creationists, or other groups of       nal approach to integrating a domain theory with
individuals who at a conscious level hold difierent         statistical inference, and shown that it provides a
theories of biology may yield some insight into this        satisfying account of people’s inductive judgments
question.                                                   about animal properties.
   Many aspects of intuitive (and scientiflc) theories
                                                            Acknowledgments Thanks to Neville Sanjana for con-
of biology have not been included in our model so           tributing code and unpublished results, Liz Baraff for
far, but will need to be incorporated as we enlarge         collecting data, and Tom Griffiths, Tevye Krynski and
its explanatory scope. We have assumed that all fea-        an anonymous reviewer for valuable suggestions.
tures are independent and are equally likely to arise
anywhere in the taxonomy. In contrast, both scien-                                References
tiflc and intuitive biological theories posit that fea-     Atran, S. (1995). Classifying nature across cultures. In
tures are not independent but causally related, and               Smith, E. E. and Osherson, D. N., editors, An Invi-
that causally deeper features tend to arise higher                tation to Cognitive Science, volume 3. MIT Press.
                                                            Goodman, N. (1972). Seven strictures on similarity. In
up in the taxonomy (Atran, 1995). Also, many                      Problems and Projects. Bobbs-Merril Co., Indiana.
features are not taxonomically organized, but de-           Heit, E. (1998). A Bayesian analysis of some forms of in-
pend on an animal’s ecological niche — its behav-                 ductive reasoning. In Oaksford, M. and Chater, N.,
iors and its interactions with other species. We are              editors, Rational Models of Cognition, pages 248–
currently studying how to incorporate these prin-                 274. Oxford University Press.
ciples into our approach by adopting more sophisti-         Marr, D. (1982). Vision. W. H. Freeman.
cated mutation-and-selection models and alternative         Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
structures for hypotheses organized around causal           Murphy, G. L. and Medin, D. L. (1985). The role of theo-
networks or spaces of behavioral traits.                          ries in conceptual coherence. Psychological Review,
                                                                  92:289–316.
   A theory-based approach should not be criticized         Nosofsky, R. M. (1986). Attention, similarity, and the
if it fails to generalize beyond the domain for which             identification-categorization relationship. Journal
it was designed. The main reason for wanting to                   of Experimental Psychology: General, 115:39–57.
model a theory is that domain-speciflc knowledge is         Osherson, D., Smith, E. E., Wilkie, O., López, A., and
likely to be important. Still, we are optimistic that             Shafir, E. (1990). Category-based induction. Psy-
                                                                  chological Review, 97(2):185–200.
the theory used to build our model will be useful
                                                            Sanjana, N. E. and Tenenbaum, J. B. (2003). Bayesian
beyond the domain of animals. It should apply to                  models of inductive generalization. In Becker, S.,
all living kinds, and more generally to any set of                Thrun, S., and Obermayer, K., editors, Advances
objects that can be represented by a developmental                in Neural Processing Systems 15. MIT Press.
                                                        663

