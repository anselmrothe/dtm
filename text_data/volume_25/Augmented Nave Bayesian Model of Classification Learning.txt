UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Augmented Naïve Bayesian Model of Classification Learning
Permalink
https://escholarship.org/uc/item/0sh6v35r
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Frey, Lewis
Fisher, Douglas
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Augmented Naïve Bayesian Model of Classification Learning
                                          Lewis Frey (frey@Vuse.Vanderbilt.Edu)
                           Computer Science Department, Box 1679 Station B, Vanderbilt University
                                                     Nashville, TN 37232 USA
                                      Douglas Fisher (dfisher@Vuse.Vanderbilt.Edu)
                           Computer Science Department, Box 1679 Station B, Vanderbilt University
                                                     Nashville, TN 37232 USA
                          Abstract                                   examined. Optimal classifiers, dependence and
                                                                     independence are discussed formally below in
   The Naïve Bayesian Classifier and an Augmented Naïve              relationship to the optimal Bayesian Classifier and the
   Bayesian Classifier are applied to human classification           Naïve Bayesian Classifier. The human categorization
   tasks. The Naïve Bayesian Classifier is augmented with            phenomena to be modeled will be reviewed along with
   feature construction using a Galois lattice. The best
   features, measured on their within- and between-category
                                                                     model fits. This is used as motivation for the
   overlap, are added to the category’s concept description.         augmentation of the Naive Bayesian Classifier. The paper
   The results show that space efficient concept descriptions        ends with a discussion of the findings.
   can predict much of the variance in the classification
   phenomena.                                                                                  Optimal Classifier
                                                                     The Bayesian model is a probabilistic classifier, which
                       Introduction                                  assigns a probability to an object’s membership in each of
The optimal Bayesian classifier chooses the most likely              a set of contrast categories. Assuming the categories
category given the evidence. Determining the most likely             partition the instance space, Bayes’ theorem (Eq. 1) is
category in "real" environments is complicated due to                used to assign the probability that an instance, represented
inadequate evidence and dependencies between events. A               as a feature vector, F1..n, is a member of class Ci.
way to simplify estimating the most likely category is to            Probability of Ci, P(Ci), is the base rate of class Ci.
                                                                                          P ( Ci )P ( F1..n Ci )
assume independence between events. This assumption is                                                                          (1)
an oversimplification and can result in a sub-optimal                 P ( Ci F1..n ) =
classifier when dependencies exist.                                                    ∑ P( Ck )P ( F1..n Ck )
   The Naive Bayesian Classifier (NB), which assumes                                    k
independence between events (e.g., features used to                      An optimal classifier is a classifier that minimizes the
categorize), has a wide range of optimal behavior even               misclassification rate or zero-one loss. If P(Ci|F1..n) is the
when dependencies exist. Because it only uses the                    probability that an instance represented by the feature
independent events to estimate the ranking of categories,            vector F1..n is in class Ci, zero-one loss is minimized if,
it also has low memory requirements.                                 and only if, F1..n is assigned to the class Ck for which
   Assuming that humans approximate an optimal                       P(Ck|F1..n) is maximum (Duda & Hart, 1973; Domingos &
classifier but with severe memory constraints, this paper            Pazzani, 1997). The Bayesian classifier, assigns F1..n to
addresses the natural question of whether human                      the class with maximal probability given the evidence.
categorization phenomena can be modeled with a NB.                   The Bayesian classifier does not assume independence
   When the Naive Bayesian Classifier supplies a non-                between features, and consequently, the amount of
optimal ranking of categories it needs to be augmented               information to determine the probabilities of classes
with a mechanism that gives an optimal ranking. An                   becomes impractical as the number of features and
Augmented Naive Bayesian Classifier (ANB) that uses                  categories grow.
feature construction to find relevant conjunctions of                    In contrast, the Naive Bayesian classifier assumes
features as well as singleton features is proposed as a way          independence between the features given a class: knowing
of better approximating the optimal classifier with                  the probability of one feature gives no information about
memory constraints (i.e., not all feature conjunctions are           another feature conditioned on class. That is,
used). This is similar in motivation to Anderson's (1991)
rational model, except with a different method of finding
                                                                                             (
                                                                      P ( F1..n Ck ) = ∏ P F j Ck     )                         (2)
                                                                                        j
relevant features and conjunctions.
   The Augmented Naive Bayesian Classifier is compared                                 Naive Bayesian Classifier
to the Naive Bayesian Classifier on the task of modeling
human performance on categorization tasks. The models'               Substituting Equation 2 into Equation 1 yields the Naïve
ability to provide an account of human categorization is             Bayesian Classifier (Equation 3).
                                                               414

                                  (
                     P( C i )∏ P F j C i )                        due to dependencies and are not optimally classified by
 P (C i F1..n ) =
                              j                                   the Naive Bayesian Classifier. The proportion predicted
                  ∑ P ( C k )∏ P ( F j C k )
                                                         (3)      for each of these stimuli falls directly on fifty-percent
                   k            j                                 giving no preference for either category. This is
   By definition, the Naive Bayesian classifier is optimal        inconsistent with human performance and suggests that
when the independence assumption holds. Additionally it           human subjects find and exploit correlations between
remains optimal when the independence assumption is               features. The NB choice proportion for transfer or test
violated, but the maximal class’s relative rank remains the       stimuli T1-T8 are also inconsistent with human
highest. Domingos and Pazzani (1997) show that it is              performance.
optimal under a greater range of independence violations
than previously assumed. Further support for the utility of       Table 1: Predicted proportions for the Naive Bayesian
NB is found in Langley, Iba, and Thompson (1992) and              Classifier (NB) and the Augmented Naive Bayesian
elsewhere; when compared to other machine learning                Classifier (ANB) across stimuli (Stim) and categories
techniques, NB worked well on natural domains.                    (Cat). The overall observed proportions (Obs Prop) were
                                                                  obtained from Pavel et al. (1988).
Bayesian approaches have also been used to model human
categorization (Anderson, 1991; Frey & Fisher, 1998;
                                                                      Cat   Stim Obs Prop NB Predict        ANB Predict
Tenenbaum, 1999).
                                                                      A1    1111      0.990       0.800          0.99
Data Sets & Model Fits                                                A2    2111      0.980       0.500          0.99
                                                                      A3    1122      0.990       0.800          0.99
The phenomena of correlated features and learning trends              A4    1222      0.950       0.500          0.99
are fit with the Naive Bayesian Classifier. Examining                 B1    1212      0.010       0.500          0.01
correlated features, the first data set comes from Pavel et           B2    2212      0.010       0.200          0.01
al. (1988). This experiment examines human                            B3    2121      0.010       0.500          0.01
classification when there is competition between single               B4    2221      0.000       0.200          0.01
and conjunctive features. The second data set from                    T1    2222      0.580       0.200          0.50
Nosofsky et al. (1994) replicates Shepard et al. (1961)               T2    2211      0.560       0.200          0.50
examination of category complexity. Nosofsky et al.                   T3    2122      0.710       0.500          0.50
extends Shepard et al. by examining how these categories              T4    1211      0.700       0.500          0.50
are learned over multiple trials. This is referred to as              T5    1112      0.460       0.800          0.50
learning trend phenomena. This experiment explores                    T6    1121      0.450       0.800          0.50
different levels of category complexity and how quickly               T7    2112      0.400       0.500          0.50
humans learn them.                                                    T8    1221      0.260       0.500          0.50
Data Set 1: Comparison of singleton vs. conjunction
features (Pavel et al. 1988)                                      Data Set 2: Learning Trends (Nosofsky et al. 1994)
Pavel et al. (1988) replicates Medin, Altom, Edelson and          Shepard, Hovland, and Jenkins (1961) studied the
Freko (1982) who explored how singleton and conjoined             complexity of categorization tasks. The six tasks,
features influence the classification of stimuli. The stimuli     presented in Table 2, consist of studying categories that
varied on four binary dimensions and are represented by a         could be distinguished by examining one dimension (task
four digit number. Each digit corresponds to one of the           type I), two dimensions (type II), a dimension with
dimensions' logical values of 1 or 2 (e.g., 2111). In             exceptions (types III-V) and all dimensions (type VI). The
Pavel’s experiment the training stimuli are presented with        point of such tasks is to determine what category types are
accuracy feedback and then a transfer phase is given with         easier to learn.
both trained and novel instances. In this experiment, the            Each task consists of learning two categories given
category structure compares single dimension, dim 1 and           eight stimuli each with three binary dimensions. The
dim 2, which have values that are strongly associated with        stimuli are split into two four-stimulus categories. There
each category, against conjoined dimensions, dim 3 and            are six different category structures, which are
dim 4, which have values pairs that are perfectly                 represented by the I-VI columns in Table 2.
correlated with each category. The measure used is the               Task-type I can be distinguished by one dimension
probability of choosing category A.                               (e.g., A: 1**, for category A the where the first dimension
Results: Data Set 1                                               has a value of one and wildcards, asterisks, representing
The NB does not exploit dependencies/correlations in              any value assignment). Type II is non-linearly separable
data; whereas apparently human subjects were able to              and needs two dimensions to distinguish the categories
exploit the correlations found in this data (see Table 1,         (e.g., B: 11*). One dimension and a single exception can
compare observed to NB predicted proportion). The NB              distinguish Task-types III-V. Type VI requires all three
accounts for only 18% of variance (SSD=1.7252,                    dimensions to determine the category. The participants
RMSD=0.328). There are four training stimuli (i.e., A2,           are presented with a stimulus and their task is to put it in
A4, B1 & B3) that the NB does not categorize correctly            one of the two categories. After each trial, the participant
                                                              415

receives feedback. Performance is measured by the                                                                Nosofsky et al.'s (1994) results are consistent with
number of errors made for each category across all trials.                                                       Shepard, Hovland, and Jenkins (1961). There is a main
                                                                                                                 effect for type of task. Task-type I has the least errors.
Table 2. The six category (A or B) assignments for task                                                          Type II has fewer errors than Types III-V. Type VI has
types I, II, III, IV, V & VI. There are eight stimuli with                                                       the most errors.
three binary dimensions. The dimensional values are                                                              Results: Data Set 2
logical 1 or 2.                                                                                                  The ordering of task difficulty predicted by the Naive
                                                                                                                 Bayesian Classifier is inconsistent with human
                                               Task Type                                                         performance, and it accounts for only 7% of variance (R2
                                                                                                                 = 0.07, SSD=13.139 RMSD=0.2959). Tasks I and IV the
                       #       Stim            I    II III                IV       V       VI
                                                                                                                 Naive Bayesian Classifier can distinguish the class of the
                       1       111             A B       B                B        B       A                     stimuli and are easiest to learn. For tasks III and V are
                       2       112             A B       A                A        A       B                     harder for it to learn and it predicts 50% for stimuli 1,2, 7
                       3       121             A A A                      A        A       B                     and 8. Tasks II and VI are the hardest for the Naive
                       4       122             A A A                      A        A       A                     Bayesian Classifier with all stimuli predictions split at
                       5       211             B    A B                   B        B       B                     50% between classes.
                       6       212             B    A B                   B        B       A                     Discussion
                       7       221             B    B    A                B        A       A                     When there are stimuli in the data set that are ranked sub-
                       8       222             B    B    B                A        B       B                     optimally by the Naive Bayesian Classifier then the fit to
                                                                                                                 human performance is poor (18% of variance for data set
   The one-dimensional Task-type I is the easiest to learn,                                                      1). The fit degrades more when there are a larger number
followed by the non-linearly separable Task-type II,                                                             of non-optimal stimuli (7% of the variance in data set 2).
followed by tasks III-V. Type VI, which required all                                                             While humans are memory-constrained, they appear to
dimensions to determine membership, is the most difficult                                                        nonetheless find and exploit (at least simple)
to learn.                                                                                                        correlations/dependencies in data.
   Nosofsky, Gluck, Palmeri, McKinley and Glauthier
(1994) use a large population of subjects and recorded                                                                Augmented Naïve Bayesian Classifier
error rates per block of training instead of only the total                                                      Since the Naïve Bayesian Classifier is space efficient and
number of errors. This provides a representation of                                                              an optimal classifier under a wide range of conditions, it
category learning over time. The participants were trained                                                       is used as a base classifier to model human categorization.
until they achieved 32 trials without error. If this level                                                       But to account for categorization phenomena where NB
was not reached they maximally went through twenty-five                                                          does not find optimal classification but human subjects
blocks of sixteen trials (two repetitions of each stimulus),                                                     do, the NB model is augmented with feature construction.
except in the first and second blocks which consisted of                                                         This results in a model with some similarities to Gluck,
eight trials. Nosofsky et al.’s results are presented in                                                         Bower and Hee's (1989) configural-cue network model
Figure 1.                                                                                                        except it is not restricted to pair-wise conjunctions.
             0.5
                                                                                                    I
                                                                                                                 Instead a Galois Lattice is used to organize the space of
                                                                                                    II
                                                                                                    III
                                                                                                                 conjunctive features, which are scored and selected for a
                                                                                                    IV
                                                                                                    V
                                                                                                                 space efficient concept representation. This augmentation
             0.4
                                                                                                    VI
                                                                                                                 gives the model the ability to find conjunctive features
                                                                                                                 that allow stimuli that would be sub-optimally categorized
             0.3                                                                                                 by NB to be optimally categorized by the Augmented
  P(error)
                                                                                                                 Naive Bayesian Classifier.
             0.2
                                                                                                                 Feature Construction
                                                                                                                 The Galois lattice consists of nodes that are each
             0.1
                                                                                                                 represented by a feature description. With feature
                                                                                                                 construction there can be conjunctive features which
             0.0
                                                                                                                 consist of more than one feature value assignment plus
                   0   1   2   3   4   5   6   7   8   9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
                                                               Block
                                                                                                                 wildcards: asterisks that represent any feature value
                                                                                                                 assignment. Figure 2 is a Galois lattice initialized with
                                                                                                                 singleton features and the first two stimuli from Table 2 in
Figure 1. Observed proportion of errors across training                                                          the learning trend data.
blocks and Task-types I - VI (Nosofsky et al.’s, 1994).                                                            Each node is linked as a parent to lower boundary
                                                                                                                 nodes that are maximally general in their conjunctive
                                                                                                                 feature description from the set of nodes that are more
                                                                                                           416

specific than the given node (i.e., fewer wildcards). For                          (e.g., Cardinality(111*) = 3). With a high specificity
example given node [*1*], then the set of more specific                            setting, larger conjunctions are scored higher. With a
nodes is {11*, 111, 112}. The lower boundary is [11*]                              large negative value, only singleton valued conjunctions
because it is the most general (i.e., most wildcards) from                         (e.g., 1***) are scored high. If the parameter is set to zero,
the specific set. Each node is also linked to as a child to                        the cardinality of the conjunctions has no effect. The
upper boundary nodes that are maximally specific from                              maximum value of the product of within-category,
the set of nodes that are more general than the given node.                        between-category and specificity is kept for each feature
Given node [111], then the upper boundary is {11*, **1}.                           as FScore.
The Galois lattice is built incrementally using the                                   Equation 5 is the heuristic scoring mechanism for the
algorithm in Godin and Missaoui (1994). A stimulus is                              conjunctive features in the lattice. The features are scored
intersected with existing nodes in the lattice, and any                            as a ratio relative to the highest scored feature. Features
novel intersection is linked to its parents and children.                          with a RScore above the threshold parameter are included
This is similar to the algorithm used by Carpineto and                             in the concept. FScore is calculated via Equation 4.
Romano (1996). The Galois lattice supplies a partial order                                              FScore( F | Cmax )                         (5)
                                                                                    RScore( F ) =                                 > threshold
of the space of conjunctions.                                                                      max ( FScore( F j | Cmax ) )
                                                                                                     ∀j
                                                                                      The threshold parameter is used to constrain the
                               Inf                                                 number of features of the lattice that are placed into the
                                                                                   concept and participate in the choice proportion
                                                                                   calculation. A RScore is assigned to the conjunctive
                                                                                   features and all conjunctions above the threshold are
                 *1*                     2**                       **2             added to the concept. If the threshold parameter is high
   1**                                                                             then the concept has fewer conjunctions and if it is low
                             **1                  *2*                              then more conjunctions score above threshold and are a
                                                                                   part of the concept.
                                                                                      In the Augmented Naïve Bayesian Classifier, the
               11*
                                                                                   classes are scored by calculating the score for the features
                                                                                   in the concept representation. The probability of F given
                                                                                   C (i.e., P(Fj|Ck), from Equation 3) has been replaced with
                  111              112
                                                                                   Score(Fj|Ck), Equation 6.
                                                                                                            2 strength * F j ∩ Ck + 1 
                                                  Sup                               Score( F j | Ck ) =                                          (6)
                                                                                                                       Ck + 1           
Figure 2: The Galois Lattice initialized with singleton                               When the Strength parameter is zero, Score
features and stimuli 111 & 112 from table 2.                                       approximates P(F|C) in the sample limit and Equation 7 is
                                                                                   the Naïve Bayesian Classifier (Equation 3). Strength is
  FScore is used to measure the utility of the features in                         used to scale the model's output with that of the human
the Galois lattice. It is a combination of three measures:                         behavior so that a comparison can be made. It can be
within-category overlap, between-category overlap and                              thought of as an index of how strongly the evidence is
specificity.                                                                       weighted. When strength is negative, the evidence is not
 FScore( F | Cmax ) =                                                              strongly weighted. That is it will take more evidence to
                                                                                   increase the distinction between categories for stimuli.
   max  P ( F | C ) ∗
     ∀i
         
                   i
                        Confirm( F | Ci )
                         Infirm( F | Ci )
                                                                         
                                          ∗ Cardinality( F ) specificity  (4)
                                                                         
                                                                                                           P ( Ci ) ∏                (
                                                                                                                               Score F j Ci  )   (7)
                                                                                    Score( Ci F1..n ) =
                                                                                                                    j∈ concept
  The within-category overlap is the probability of the
feature occurring within the category P(F|C). The                                                       ∑ P ( Ck ) ∏                   (
                                                                                                                                 Score F j Ck  )
                                                                                                         k            j∈ concept
between-category overlap is the ratio of the number of
times the feature occurs in the category (Confirming                                  The category scores for the stimuli’s feature vectors are
evidence) over the number of times the feature occurs in                           compared against the human categorization probabilities
other categories (Infirming evidence). Confirming and                              for the categories given the stimuli. Note that this
infirming evidence is similar to a measure used in                                 collapses to its base Naïve Bayesian Classifier when
Schlimmer’s (1987) Stagger system. The bias chosen for                             specificity biases towards singleton features and the
the scoring is towards features that have high within-                             strength parameter is set to zero. When dependencies
category overlap and low between-category overlap.                                 between features make the Naïve Bayesian Classifier sub-
  The specificity parameter allows an exploration of                               optimal conjunctive features are used to improve the
specific versus general features. Specific conjunctions                            model’s fit. The parameters are chosen by the downhill
have a larger cardinality due to fewer wildcard values                             simplex method in multi-dimensions (Press et al, 1994)
                                                                               417

that minimizes one minus R2. This results in parameters              positive. Thus, the conjunctions are given more weight as
that account for the greatest amount of variance.                    in the fit for Data Set 1 (1.02). Rate (0.08) is used to
Results: Data Set 1                                                  compare the model with learning trend. The parameter is
The ANB accounts for 92% of the variance (SSD=0.1676,                set to quickly learn features in first trials and then learn
RMSD=0.1023) in Table 1. The ANB provides a better                   new features more slowly in later trials. This in addition
prediction of human behavior than the NB, F(3, 13) =                 to feature construction and compression accounts for the
40.27, p<.05.                                                        phenomena.
   Augmented Naive Bayesian Parameters. Specificity is
                                                                               0.5
1.0. This allows a more specific search of the space. The                                                                                                           I
                                                                                                                                                                    II
threshold (0.75) compresses the representations using                                                                                                               III
                                                                                                                                                                    IV
features with low inter-category overlap. For this data set,                   0.4                                                                                  V
                                                                                                                                                                    VI
all the features only occur within one category. This is a
case where inter-category overlap is very important for
                                                                               0.3
scoring the features. Strength (1.02) is positive and thus,
the conjunctions are given strong weight.                               P(error)
   The result shows that the competition is not strictly                       0.2
single dimensions 1 & 2 versus conjoined dimensions 3 &
4. Large conjunctions using combinations of three
                                                                               0.1
dimensions can be used to account for 92% of the
variance. The features used by Augmented Naive
Bayesian model for classifying the stimuli are 1*22 and                        0.0
                                                                                     0   1   2   3   4   5   6   7   8   9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
*111 for category A and. *212 and 2*21 for category B.                                                                            Block
The representation used is also compact, using a smaller
amount of space resources. The ANB uses four
conjunctive features that correctly classify the training              Figure 3. Proportion of errors across training blocks for
stimuli (in categories A & B, see Table 1) but do not                the Augmented Naïve Bayesian.
occur in any of the transfer (T1-T8) stimuli. This results
in all the transfer stimuli having a fifty-percent chance of           The feature sets for the six task types learn the
being classified in category A.                                      categories accurately. The model is consistent with the
Results: Data Set 2                                                  complexity of the tasks as measured by human error rates
In order to compare the ANB to the learning trends                   in the learning trend data. The ANB selects conjunctive
exhibited in Nosofsky et al. (1994) a mechanism for                  features that occur exclusively in one category. For this
controlling the rate with which features are added to the            human categorization behavior, the Augmented Naive
concept is needed. The below equation is used to                     Bayesian Classifier provides a better prediction than the
incrementally add features to the concept being learned.             Naive Bayesian Classifier, F(4, 120) = 1967.6, p<.05.
                  (
TotalFConcept = 1 − ( trial )    )
                              − rate
                                     ∗ LatticeSize       (8)         Discussion
  As the number of trials increase a larger number of                The ANB gives two features for task I (A: 1**: B: 2**),
features are added to the concept. Features are no longer            four features for task II (A: 12*, 21*; B: 11*, 22), six
added when there are no more features above threshold.               features for task III (A: 12*, 1*2, *21; B: 21*, 2*2, *11),
This allows a gradual addition of features to the concept            IV (A: 12*, *22, 1*2; B: 21*, *11, 2*1) and V (A: 12*,
and makes the model comparable for the Nosofsky et al.               1*2, *21; B: 21*, 2*2, *11) and eight features for task VI
(1994) learning trend data.                                          (all eight stimuli).
  The augmented Bayesian (Figure 3) has similar                        The number of features in each task is consistent with
performance to human subjects (Figure 1) on the six task             the ordering of difficulty for the Shepard et al. results.
types. The R2 is 0.92 and rmsd is 0.0401. The same                   The ordering of difficulty in the experiment is task I
parameter values are used across all six tasks. The fits are         followed by task II. Tasks III-V are the next most difficult
made based on the average probability of misclassifying              and task VI is the most difficult. The number of features
any of the stimuli for a given trial block.                          in the ANB concept representation also corresponds to the
  Augmented Naive Bayesian Parameters. Specificity is                Boolean complexity demonstrated by Feldman (2000).
0.94 and biases towards larger conjuncts, but still allows
for singleton features as in task I. The threshold (0.76) is                                             General Discussion
similar to that of Data Set 1 (0.75) and compresses the              Because of independence violations, the NB is non-
representations using features with low inter-category               optimal in classifying stimuli across a number of tasks.
overlap (i.e., all features only occur within one category).         This is inconsistent with human performance in the
This is a case where inter-category overlap is very                  domains studied here. The ANB finds conjunctions that
important for scoring the features. Strength (1.22) is               can classify these stimuli in the tasks. Thus, ANB as
                                                               418

compared to the NB provides a better account of human                                  References
performance.                                                     Anderson, J. R. (1991). The adaptive nature of human
   The Augmented Naive Bayesian parameters are                     categorization. Psychological Review, 98, 409-429.
effective    at     exploring     possible    classification     Carpineto, C. & Romano, G. (1996). A Lattice
representations. The ANB converges on representations              Conceptual Clustering System and Its Application to
that account for the phenomena. For data set 1                     Browsing Retrieval. Machine Learning 24, 95-122.
conjunctive features win over singleton features. The            Domingos, P. & Pazzani, M. (1997). On the Optimality
conjunctive features enabled certain stimuli, which are            of the Simple Bayesian Classifier under Zero-One Loss.
non-optimal for NB, to be learned in a consistent way to           Machine Learning, 29, 103-130, 1997.
human performance. The compressed representation for             Duda, R. & Hart, P. (1973). Pattern classification and
data set 1 accounts for much of the variance. For data set         scene analysis. New York: John Wiley & Sons.
2 conjunctive features along with compressed                     Feldman, J. (2000). Minimization of Boolean complexity
representations for the tasks allow ANB to follow                  in human concept learning. Nature, 407, 630-633.
learning trend data patterns. The different tasks in this        Frey, L. & Fisher, D. (1998). Naive Bayesian Accounts of
experiment have varying degrees of representation                  Base Rate Effects in Human Categorization. In the
complexity (number of features) which maps onto                    Proceedings of the Twentieth Annual Conference of the
complexity of the task as measured by human                        Cognitive Science Society, 380-385.
performance.                                                     Gluck, M. A. & Bower, G. H. (1988). From conditioning
                                                                   to category learning: an adaptive network model.
Limitations of ANB Classifier                                      Journal of Experimental Psychology: General, 117 (3),
                                                                   227-247.
   There are classification tasks in which the Augmented         Godin, R. & Missaoui, R. (1994). An incremental
Naive Bayesian model would tend to converge on an                  concept formation approach for learning from
inappropriate representation. For example, the                     databases. Theoretical Computer Science, 133, 387-
competition between singleton and conjunctive feature              419.
makes it more difficult to have representations that consist     Langley, P., Iba, W., & Thompson, K. (1992). An
of both. Note that for learning trend data it did have both        analysis of Bayesian classifiers. From Proceedings of
single and conjunctive representations for different tasks         the Tenth National Conference on Artificial
while using the same parameters. It is possible to have            Intelligence. San Jose: AAAI Press.
tasks that involve both singleton and conjunctions, but the      Medin, D. L., Altom, M. W., Edelson, S. M., & Freko, D.
model may include one over the other. With a more                  (1982). Correlated symptoms and simulated medical
relaxed threshold the Augmented Naive Bayesian model               classification. Journal of Experimental Psychology:
places a large number of conjunctions in the                       Learning, Memory, and Cognition, 8, 37-50.
representation. When there are a large number of                 Nosofsky, R. M., Gluck, M. A., Palmeri T. J., McKinley,
conjunctions, the fit could be good because of each                S. C. & Glauthier, P. (1994). Comparing models of
conjunction contributing to the fit. This would argue that         rule-based classification learning: A replication and
the power of the representation is accounting for variance.        extension of Shepard, Hovland, and Jenkins (1961).
This does not take place in these data sets because each           Memory and Cognition, 22 (3), 352-369.
representation is compressed due to the thresholds biasing       Palmeri, T. J. & Nosofsky, R. M. (1995). Recognition
                                                                   Memory for Exceptions to the Category Rule. Journal
towards small representations.
                                                                   of Experimental Psychology: Learning, Memory, and
                                                                   Cognition, 21(3), 548-568.
                       Conclusion                                Pavel, M., Gluck, M.A., & Henkle, V. (1988).
It is encouraging that the Augmented Naive Bayesian                Generalization by humans and multi-layer networks.
model selected simple consistent feature representations           Proceedings of the 10th Annual Conference of the
for these data sets. It helps to supply a parsimonious             Cognitive Science Society. Hillsdale, NJ: Erlbaum.
account via a smaller feature representation that can be         Press, W. H, Tenkolsky, S. A., Vetterling, W. T. &
tested experimentally. It also supplies novel views on the         Flannery, B.P. (1994). Numerical Recipes in C.
data due to the ordered approach of looking at                     Cambridge University Press.
conjunctions.                                                    Schlimmer, J. C. (1987). Concept Acquisition Through
   For the two data sets, the ANB provides a better                Representational Adjustment (Technical Report
prediction of human categorization phenomena than the              Number 87-19). Irvine CA: University of California,
NB. The NB does not model the behavior well when there             Department of Information and Computer Science.
are violations of the independence assumption in the             Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961).
category structures. The process of space efficient feature        Learning and memorization of classification.
                                                                   Psychological Monographs: General and Applied, 75
construction in the Augmented Naive Bayesian Classifier
                                                                   (13), 1-41.
corrects the Naive Bayesian Classifier’s incorrect ranking       Tenenbaum, J. B. (1999). Bayesian modeling of human
of the categories while at the same time better modeling           concept learning. Advances in Neural Information
human data.                                                        Processing Systems, 11.
                                                             419

