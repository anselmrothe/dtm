UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Role of Embodied Intention in Early Lexical Acquisition
Permalink
https://escholarship.org/uc/item/53n190hz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Yu, Chen
Ballard, Dana H.
Aslin, Richard N.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                The Role of Embodied Intention in Early Lexical Acquisition
                                              Chen Yu (yu@cs.rochester.edu)
                                        Dana H. Ballard (dana@cs.rochester.edu)
                                  Department of Computer Science; University of Rochester
                                                   Rochester, NY 14627 USA
                                       Richard N. Aslin (aslin@cvs.rochester.edu)
                             Department of Brain and Cognitive Sciences; University of Rochester
                                                   Rochester, NY 14627 USA
                             Abstract                               acquisition is solely based on statistical learning of co-
                                                                    occurring data from the linguistic modality and non-
   We examine the influence of inferring interlocutors‚Äô refer-
   ential intentions from their body movements at the early         linguistic context. Richards and Goldfarb (1986) pro-
   stage of lexical acquisition. By testing human subjects          posed that children come to know the meaning of a word
   and comparing their performances in different learning           through repeatedly associating the verbal label with their
   conditions, we find that those embodied intentions fa-           experience at the time that the label is used. Roy and
   cilitate both word discovery and word-meaning associa-
   tion. In light of empirical findings, the main part of this      Pentland (2002) have developed a computational model
   paper presents a computational model that can identify           of infant language learning, in which they used the tem-
   the sound patterns of individual words from continuous           poral correlation of speech and vision to associate spo-
   speech using non-linguistic contextual information and           ken utterances with a corresponding object‚Äôs visual ap-
   employ body movements as deictic references to discover          pearance. It seems quite reasonable to assume that the
   word-meaning associations. To our knowledge, this work
   is the first model of word learning which not only learns        human cognitive system exploits this statistical informa-
   lexical items from raw multisensory signals to closely re-       tion. However, despite the merit of this idea, association-
   semble natural environments of infant development, but           ism is unlikely to be the whole story because it is based
   also explores the computational role of social cognitive         on the assumption that words are always uttered when
   skills in lexical acquisition.
                                                                    their referents are perceived, which has not been verified
                        Introduction                                by experimental studies of infants (Bloom, 2000).
To acquire a vocabulary item, a young language learner                 In addition to temporal co-occurrences of multisen-
must discover the sound pattern of a word from continu-             sory data, recent psycholinguistic studies (e.g., Baldwin
ous speech since spoken language lacks the acoustic ana-            et al., 1996; Bloom, 2000; Tomasello, 2001) have shown
log of blank spaces of written text. Furthermore, learn-            that other major sources of constraints in language acqui-
ing a word involves mapping a form, such as the sound               sition are social cognitive skills, such as children‚Äôs ability
‚Äúcat‚Äù, to a meaning, such as the concept of cat. The child          to infer the intentions of adults as adults act and speak to
senses a multitude of co-occurrences between words and              them. These kinds of social cognitions are called mind
things in the world, and he or she must determine which             reading by Baron-Cohen (1995). Bloom (2000) argued
co-occurrences are relevant.                                        that children‚Äôs word learning actually draws extensively
   In the last ten years, there has been tremendous                 on their understanding of the thoughts of speakers. His
progress in understanding infants‚Äô ability to segment               claim has been supported by the experiments in which
continuous speech, discover words and learn their mean-             young children were able to figure out what adults were
ings. Most research focuses on the role of linguistic in-           intending to refer to by speech. Baldwin et al. (1996) ref-
formation as the central constraint. A number of relevant           erential intent when determining the reference of a novel
cues have been found that are correlated with the pres-             label. showed that infants established a stable link be-
ence of word boundaries and can potentially signal word             tween the novel label and the target toy only when that
boundaries in continuous speech. These include prosodic             label was uttered by a speaker who concurrently showed
patterns (e.g., Cutler & Butterfield, 1992), phonotactic            his attention toward the target, and such a stable map-
regularities (e.g., Mattys & Jusczyk, 2001), allophonic             ping was not established when the label was uttered by a
variations (e.g., Jusczyk, Hohne, & Bauman, 1999)                   speaker who was out of view and hence showed no signs
and distributional probability (e.g., Aslin, Woodward,              of attention to the target toy.
laMendola, & Bever, 1996; Brent & Cartwright, 1996).                   In a complementary study of embodied cognition, Bal-
Recent computational approaches on child-directed cor-              lard, Hayhoe, Pook, and Rao (1997) proposed a cognitive
pora have also revealed that relatively simple statistical          system of implicit reference termed deictic, in which the
learning mechanisms could make an important contribu-               body‚Äôs pointing movements are used to bind objects in
tion to certain aspects of language acquisition (for review         the world to variables in cognitive programs of human
see Brent, 1999).                                                   brain. Also, in the studies of speech production, Cooper
   Recently, a popular explanation of the word learning             (1974) found speakers have a strong tendency to look to-
problem termed associationism assumes that language                 ward objects referred to by speech.
                                                               1293

   By putting together all those ideas on shared atten-        with 9 subjects in each condition.
tion and intention, we propose that speakers‚Äô body move-
                                                               Stimuli. Subjects were exposed to the language by
ments, such as eye, head and hand movements, can re-
                                                               video. In the video, a person was reading the picture
veal their referential intents in verbal utterances, which
                                                               book of ‚ÄúI went walking‚Äù (Williams & Vivas, 1989) in
could possibly play a significant role in early language
                                                               Mandarin. The book is for 1-3 year old children, and the
development. A plausible starting point of learning the
                                                               story is about a young child that goes for a walk and en-
meanings of words is the deployment of speakers‚Äô in-
                                                               counters several familiar friendly animals. The speaker
tentional body movements to infer their referential inten-
                                                               told the story in a way similar to a caregiver describing it
tions which we term embodied intention. This work takes
                                                               to a child. For each page of the book, subjects saw a pic-
some first steps in that direction by examining the prob-
                                                               ture and heard verbal descriptions. The study included
lem through both empirical research and computational
                                                               two video clips that were recorded simultaneously when
modeling with the hope to obtain a more complete pic-
                                                               the speaker was reading the book, and provided differ-
ture. The next section presents the experiments that use
                                                               ent learning conditions for subjects: audio-visual condi-
adult language learners exposed to a second language to
                                                               tion and intention-cued condition. In audio-visual condi-
study the role of embodied intention in infant language
                                                               tion, the video was recorded from a fixed camera behind
acquisition. In light of the human subject study, we then
                                                               the speaker to capture a static view. In the intention-
propose a computational model of word learning to sim-
                                                               cued condition, we recorded video from a head-mounted
ulate the early stage of infant vocabulary learning. The
                                                               camera to get a dynamic view. Furthermore, an eye
implemented model is able to build meaningful semantic
                                                               tracker was utilized to track the course of the speaker‚Äôs
representations grounded in multisensory inputs. The es-
                                                               eye movements and gaze positions were overlapped on
sential structure models the computational role of the in-
                                                               the video to indicate what the speaker was attending to.
ference of speakers‚Äô referential intentions by making use
                                                               Auditory information is same in both videos. Figure 1
of body movements as deictic references (Ballard et al.,
                                                               shows the snapshots from two video clips.
1997), and employs non-linguistic information as con-
straints on statistical learning of linguistic data.
                 Human Simulations
Previous language-learning studies have shown similar
findings for adults exposed to an artificial language and
children or even infants exposed to the same type of lan-
guage (Saffran, Newport, & Aslin, 1996). This suggests
that certain mechanisms involved in language learning          Figure 1: The snapshots when the speaker uttered ‚Äúthe cow is
                                                               looking at the little boy‚Äù in Mandarin. Left: a snapshot from
are available to humans regardless of age. Lakoff and          the fixed camera. Right: a snapshot from a head-mounted cam-
Johnson (1999) argued that children have already built         era with the current gaze position (the white cross).
up pre-linguistic concepts (internal representations of the    Procedure. Subjects were shown video clips on a com-
world) in their brains prior to the development of lexicon.    puter monitor and asked to try to identify both sound
Thus, if we assume that those concepts are already estab-      patterns of individual words and their meanings. They
lished, the lexical learning problem would mainly deal         watched the same video five times before being tested,
with how to find a sound pattern from continuous speech        and were given the opportunity to take a break in the
and associate this linguistic label with a concept previ-      middle of each session, but few did.
ously built up. As pointed out by Gillette et al. (Gillette,
Gleitman, Gleitman, & Lederer, 1999), although the rep-        Test. Subjects were given two written multiple-choice
resentations of concepts of adults may differ from those       tests: a speech segmentation test and a word learning test.
of young children, there should be little difference be-       There were 18 questions in each test. For every ques-
tween adults and children with regard to acquiring sim-        tion in the first test, subjects heard two sounds and were
ple words as long as they are provided with the same           asked to select one that they thought was a word but not
information. In light of this, our first experiment was        a phrase or a syllable. They were given as much time as
conducted with monolingual adults exposed to a second          they wanted to answer each question. A second test was
language to shed light on the role of embodied intention       used to evaluate their knowledge of lexical items learned
in the early stage of infant language learning. The ex-        from the video. The images of 12 objects in the picture
periment consists of two phases. In the training phase,        book were displayed on a computer monitor. Subjects
subjects were asked to watch a video and try to discover       heard one isolated spoken word for each question and
lexical items. In the testing phase, they were given the       were asked to select an answer from 13 choices (12 ob-
tests of both speech segmentation and lexical learning.        jects and also an option for none of the above).
Methods                                                        Results
Participants. 18 monolingual English speaking stu-             Figure 2 shows the average correct answers of two tests.
dents at the University of Rochester participated in this      In the speech segmentation test, subjects made signifi-     
study, and were paid for their participation. Subjects         cantly more errors in the audio-visual condition (
                                                                            
were randomly assigned to two experimental conditions,                                ) than in the intention-cued condition
                                                                              
                                                          1294

                                                                     
(                                                                              ). The further analysis re-
                                                                    
                                                                                                                                           
vealed a significant main effect of conditions
                                                                                                                                    
                                                                                                          
                                                                                                                                                           -- now the little boy is walking.
                                                                                                                                                           -- all the animals follow him.
                                                      
                                                                   . For word learning, a direct compari-
                                                                                           
                                                                                                                                 
                                                                                                                                             
                                                                                                                                                           -- the cat is in the rear.
                                                                                                                                                           -- then the horse and the cow.
son of the intention-cued condition (                                                                                                   
                                                                                                                                                           -- the duck and the pig are
            ) with the audio-visual condition (
                                                                                                                             
                                                                                                                                                               lined up and walking.
                                                                                                                                               
                                                                                                                                                           -- the dog is next to the boy.
            ) also revealed a significant difference (
                                                                                                                               
                                                                                                    
                                                                                                                                                                                              phoneme strings
                                                     
                                                                   ). This human subject study provides                                                                               -- n ow th eh ah l ih t l b oy iy
                                                                                                                                                                                      eh s w aw l k ih ng.
                                                                                                                                                                                                                          -- n ow th eh ah l ih t l b oy iy
                                                                                                                                                                                                                          eh s w aw l k ih ng.
substantial evidence for the hypothesis that embodied in-                                                                                                                             -- aw l hh th eh eh n ih m ow z
                                                                                                                                                                                      f er l ow hh ih m hh.
                                                                                                                                                                                                                          -- aw l hh th eh eh n ih m ow z
                                                                                                                                                                                                                          f er l ow hh ih m hh.
tention plays an important role in language acquisition.                                                                                                                              -- th eh k ae tcl t ih s ih n th eh
                                                                                                                                                                                      r ae er ih.
                                                                                                                                                                                                                          -- th eh k ae tcl t ih s ih n th
                                                                                                                                                                                                                          eh r ae er ih.
This proposal suggests that a formal model that explores                                                                                                                              -- th eh n ih th eh h ao r z eh n
                                                                                                                                                                                      dcl d th ih k aw.
                                                                                                                                                                                                                          -- th eh n ih th eh h ao r z eh
                                                                                                                                                                                                                          n dcl d th ih kk aw.
                                                                                                                                                                                                                                            aw
the computational role of embodied intention in lexical                                                                                                                               -- th eh dcl d uh kcl k eh d th
                                                                                                                                                                                      ih pcl p iy gcl g ih er l ay n uh
                                                                                                                                                                                                                          -- th eh dcl d uh kcl k eh d th
                                                                                                                                                                                                                          ih pcl p iy gcl g ih er l ay n uh
development, should show similar advantages to inten-                                                                                                                                 eh ih hh d w aw l kcl k ih ng.
                                                                                                                                                                                      -- th eh dcl d ao g eh z n eh ix
                                                                                                                                                                                                                          eh ih hh d w aw l kcl k ih ng.
                                                                                                                                                                                                                          -- th eh dcl
                                                                                                                                                                                                                                    dcl dd ao
                                                                                                                                                                                                                                           ao gg eh z n eh ix
tional cues.                                                                                                                                                                          t uw th eh b oy iy.                 t uw th eh b oy iy.iy
                        word discovery                                           word-meaning association                                             Figure 3: The problems in word learning. The raw speech
                                                                                                                                                      is firstly converted into phoneme sequences. The goal of our
                                                                                                                                                      method is to discover phoneme substrings that correspond to
                                                                                                                                                      the sound patterns of words and then infer the meanings of
                                                                                                                                                      those words from non-linguistic modalities.
                                                                                                                                                      Clustering Visually Grounded Meanings The non-
                                                                                                                                                      linguistic inputs of the system consist of visual data from
                                                                                                                                                      a head-mounted camera, head positions and gaze-in-head
                                                                                                                                                      data. Those data provide the contexts in which spoken
                                                                                                                                                      utterances are produced. Thus, the possible referents of
                                                                                                                                                      spoken words that subjects utter are encoded in those
  Figure 2: The mean percentages of correct answers in tests.
                                                                                                                                                      contexts, and we need to extract those word meanings
                                    Computational Simulations                                                                                         from raw sensory inputs. As a result, we will obtain
We supplement our empirical studies with a computa-                                                                                                   a temporal sequence of possible referents depicted by
tional account. By implementing the descriptions of the                                                                                               the box labeled ‚Äúintentional context‚Äù in Figure 4. Our
theories or claims explicitly in computer simulations, we                                                                                             method firstly utilizes eye and head movements as cues
can not only test the plausibility of the theories but also                                                                                           to estimate the subject‚Äôs focus of attention. Attention,
gain the insights of both the nature of the problems and                                                                                              as represented by eye fixation, is then used for spotting
the possible solutions.                                                                                                                               the target object of subject‚Äôs interest. Specifically, at ev-
        To simulate how infants ground semantics, our model                                                                                           ery attentional point in time, we make use of eye gaze as
needs to be embodied in the physical environment and                                                                                                  a seed to find the attentional object from all the objects
sense the environment as a young child. To do so, we                                                                                                  in a scene. The referential intentions are then directly
attached multiple sensors to an adult subject who was                                                                                                 inferred from attentional objects. We represent the ob-
asked to act as a caregiver and perform some everyday                                                                                                 jects by feature vectors consisting of color, shape and
activities, one of which was reading a picture book for                                                                                               texture features. For further information see Yu, Ballard,
a young child. Those sensors include a head-mounted                                                                                                   and Zhu (2002). Next, since the feature vectors extracted
CCD camera to capture visual information of the physi-                                                                                                from visual appearances of attentional objects do not oc-
cal environment, a microphone to sense acoustic signals,                                                                                              cupy a discrete space, we vector quantize them into clus-
an eye tracker to track the course of eye movements, and                                                                                              ters by applying a hierarchical agglomerative clustering
position sensors attached to the head and hands of the                                                                                                algorithm. Finally, for each cluster we select a prototype
caregiver. In this way, our computational model (as a                                                                                                 to represent perceptual features of this cluster.
young language learner) can acquire multisensory data                                                                                                 Comparing Phoneme Sequences We describe our
so that it shares the visual environment with the care-                                                                                               methods of phoneme string comparison in this subsec-
giver, hears infant-directed speech uttered by the care-                                                                                              tion. Detailed descriptions of algorithms can be obtained
giver and observes his or her body movements, such as                                                                                                 from Ballard and Yu (2003). First, the speaker indepen-
gaze and head movements, which are deployed to infer                                                                                                  dent phoneme recognition system is employed to con-
the caregiver‚Äôs referential intentions.                                                                                                               vert spoken utterances into phoneme sequences. To fully
The Model                                                                                                                                             simulate lexical learning, the phoneme recognizer does
To learn words from a caregiver‚Äôs spoken descriptions                                                                                                 not encode any language model or word model. There-
(shown in Figure 3), three fundamental problems needed                                                                                                fore, the outputs are noisy phoneme strings that are dif-
to be addressed are: (1) object recognition to identify                                                                                               ferent from phonetic transcriptions of text. Thus, the goal
grounded meanings of words from visual perception,                                                                                                    of phonetic string matching is to identify sequences that
(2) speech segmentation and word spotting to extract                                                                                                  might be different actual strings, but have similar pro-
the sound patterns of the individual words which might                                                                                                nunciations. In our method, a phoneme is represented
have grounded meanings, (3) association between spo-                                                                                                  by a 15-dimensional binary vector in which every entry
ken words and their meanings.                                                                                                                         stands for a single articulatory feature called a distinc-
                                                                                                                                                 1295

 tive feature. Those distinctive features are indispensable                                                                            are clustered by a hierarchical agglomerative cluster-
 attributes of a phoneme that are required to differentiate                                                                            ing algorithm. The centroids of clusters are associ-
 one phoneme from another in English. We compute the                                                                                   ated with their possible grounded meanings to build
 distance between two individual phonemes as the Ham-                                                                                  hypothesized word-meaning pairs.
 ming distance. Based on this metric, a modified dy-                                                                                   To find correct lexical items from hypothesized lexi-
 namic programming algorithm is developed to compare                                                                                   cal items, the probability of each word is represented
 two phoneme strings by measuring their similarity.                                                                                    as a mixture model that consists of the conditional
                       u1
transcripts the cat looks itself
                                                u2
                                      the little boy gives
                                                                                     u3
                                                                        the horse and the cat
                                                                                                               u4
                                                                                                        the little boy finds a
                                                                                                                                       probabilities of each word given its possible meanings.
                                      the horse a hug                   follow along                    tree
                                                                                                                                       In this way, the Expectation-Maximization (EM) al-
 spoken
 utterances
                                       th eh l ih t l b oy iy gcl
                                                                                                                                       gorithm is employed to find the reliable associations
              th eh kcl k ae ih t l                                      th eh hh ao r z eh n
  phoneme
  strings     uw k s ih t z eh l hh f  g iy v z the ih hh ao aw
                                       r z aa hh uh d
                                                                         d the ih kcl k ae t f
                                                                                                         th ih l ih t l b oy f ih n
                                                                                                         dcl d z aa tcl t iy           of spoken words and their grounded meanings which
                                                                         er l ow ah l ax ng
                                                                                                                                       maximize the probabilities.
   attentioal
     objects
                     cat                     horse      little boy              horse          cat              little boy
                                                                                                                                    Experimental Setup
                                                         intentional contexts                                                       A Polhemus 3D tracker was utilized to acquire 6-DOF
                                                                                                                                                            
                cat
                                 classification
                                  classification              horse                                     little boy                  head positions at           . A subject wore a head-mounted
        u1 th eh kcl k ae ih t l
           uw k s ih t z eh l hh f
                                                 th eh l ih t l b oy iy gcl
                                            u2 g iy v z the ih hh ao aw
                                                                                              th eh l ih t l b oy iy gcl
                                                                                        u2 g iy v z the ih hh ao aw
                                                                                                                                    eye tracker from Applied Science Laboratories(ASL).
        u3 th eh hh ao r z eh n
                                                 r z aa hh uh d                               r z aa hh uh d                        The headband of the ASL held a miniature ‚Äúscene-
                                            u3 th eh hh ao r z eh n d
           d the ih kcl k ae t f
           er l ow ah l ax ng
                                                 the ih kcl k ae t f er l ow
                                                 ah l ax ng
                                                                                        u4    th ih l ih t l b oy f ih n
                                                                                              dcl d z aa tcl t iy
                                                                                                                                    camera‚Äù to the left of the subject‚Äôs head, which provided
                                                                                                                                    the video of the scene. The video signals were sampled
               similar sequence finding                                                                                             at the resolution of 320 columns by 240 rows of pixels
                     cat                          horse                                   little boy
           w1 kcl k ae t
                                                                                                                                    at the frequency of 15Hz. The gaze positions on the im-
                                          w1 hh ao r z                          w1 l ih t l b oy
                                                                                                                                                                                         
           w2 kcl k ae ih t               w2 hh ao aw r z                       w2 l ih t l b oy iy
                                                                                                                                    age plane were reported at the frequency of               . The
                                                                                                                                    acoustic signals were recorded using a headset micro-
                          word-like unit clustering
                                                                                                                                    phone at a rate of 16 kHz with 16-bit resolution. Six sub-
                                                                                                                                    jects participated in the experiment. They were asked to
                    w1
                                                                                                                                    read the picture book (used in the previous experiment)
                                                       w1
                 w2
                                                             w2                               w1
                                                                                                                                    in English. They were also instructed to pretend that they
                                                                                                        w2
                                                                                                                                    told this story for a child so that they should keep verbal
                                                                                                                                    descriptions of pictures as simple and clear as possible.
                           hypothesized lexical items
                                                                                                                                    We collected multisensory data when they performed the
         kcl k ae t                                       hh ao r z                        l ih t l b oy
                                                                                                                                    task, which were used as training data for our computa-
                                                                                                                                    tional model.
                                         EM algorithm                                                                               Results and Discussion
                                         kcl k ae t           hh ao r z        l ih t l b oy                                        To evaluate experimental results, we define the following
                                                                                                                                    three measures: (1) Semantic accuracy is to measure
                                                                                                                                    the accuracy of clustering visual objects (e.g., animals)
 Figure 4: Overview of the method. Spoken utterances are                                                                            in the picture book. (2) Word discovery accuracy is to
 categorized into several bins that correspond to temporally co-                                                                    measure whether the beginning and the end of phoneme
 occurring attentional objects. Then we compare any pair of                                                                         strings of word-like units are correct word boundaries.
 spoken utterances in each bin to find the similar subsequences                                                                     (3) Word learning accuracy is to measure the percent-
 that are treated as word-like units. Next, those word-like units
 in each bin are clustered based on the similarities of their                                                                       age of successfully segmented words that are correctly
 phoneme strings. The EM-algorithm is applied to find lexical                                                                       associated with their meanings.
 items from hypothesized word-meaning pairs.
                                                                                                                                                Table 1: Results of word acquisition
 Word Learning Figure 4 illustrates our approach to
 spotting words and establishing word-meaning associa-                                                                                    Subjects     Semantics         Word         Word
 tions, which consists of the following steps (See Ballard                                                                                                             discovery    learning
 & Yu, 2003 for detailed descriptions):                                                                                                   1                80.3%        72.6%        91.3%
       Phoneme utterances are categorized into several bins                                                                               2                83.6%        73.3%        92.6%
       based on their possibly associated meanings. For                                                                                   3                79.2%        71.9%        86.9%
       each meaning (an attentional object), we find the cor-
                                                                                                                                          4                81.6%        69.8%        89.2%
       responding phoneme sequences uttered in temporal
       proximity, and then categorize them into the same bin                                                                              5                82.9%        69.6%        86.2%
       labeled by that meaning.                                                                                                           6                76.6%        66.2%        83.1%
       The similar substrings between any two phoneme se-                                                                                 Average          80.6%         70.6%       88.2%
       quences in each bin are found and treated as word-like                                                                          Table 1 shows the results of three measures. The
       units.                                                                                                                       recognition rate of the phoneme recognizer we used is
       The extracted phoneme substrings of word-like units                                                                          75% because it does not encode any language model or
                                                                                                                              1296

word model. Based on this result, the overall accuracy of       language knowledge is a challenging problem and has
speech segmentation is 70.6%. Naturally, an improved            been addressed by solely using linguistic information. In
phoneme recognizer based on a language model would              contrast, our method appreciates the importance of non-
improve the overall results, but the intent here is to study    linguistic context in which spoken words are uttered. We
the learning procedure without pre-trained models. The          propose that the sound patterns frequently appearing in
error in word learning is mainly caused by a few words          the same context are likely to have grounded meanings
(e.g., ‚Äúhappy‚Äù and ‚Äúlook‚Äù) that frequently occur in some        related to this context. Thus, by finding frequently ut-
contexts but do not have visually grounded meanings.            tered sound patterns in a specific context (e.g., an object
Considering that the system processes raw sensory data,         that subjects intentionally attend to), the model discov-
and our learning method works in unsupervised mode              ers word-like sound units as candidates for building lex-
without manually encoding any linguistic information,           icons. Secondly, a difficult task of word learning is to
the accuracies for both speech segmentation and word            figure out which entities specific words refer to from a
learning are impressive.                                        multitude of co-occurrences between words and things
        word discovery          word-meaning association        in the world. This is accomplished in our model by uti-
                                                                lizing speakers‚Äô intentional body movements as deictic
                                                                references to establish associations between words and
                                                                their visually grounded meanings.
                                                                Sensory Level Modeling
                                                                The successful implementation of the model suggests
                                                                that with advances in machine learning, speech process-
                                                                ing and computer vision, modeling lexical learning at
                                                                the sensory level is not impossible and it has some ad-
                                                                vantages over symbolic simulations by closely resem-
Figure 5: A comparison of performance of the intention-cued     bling natural environments in which infants develop. Our
method and the audio-visual approach.                           model emphasizes the importance of embodied learning
   To demonstrate the role of embodied intention in lan-        for two main reasons. First, the motivation behind this
guage learning, we process data by another method in            work is that language is grounded in sensorimotor ex-
which eye gaze and head movements are ignored, and              periences with the physical world. Thus, a fundamental
only audio-visual data are used for learning. In this           aspect of language acquisition is to associate the body
approach, we have to classify spoken utterances into            and the environment with words in language (Lakoff &
the bins of all the objects in the scene instead of just        Johnson, 1980). Second, infants learn words by sensing
the bins of attentional objects. Except for this point,         the environments from their perceptual systems and cop-
the method shares other implemented components with             ing with several practical problems, such as the variabil-
the intention-cued approach. Figure 5 shows the com-            ity of spoken words in different contexts and by different
parison of two methods. The intention-cued approach             talkers. To closely simulate infant vocabulary develop-
outperforms the other one in both speech segmentation           ment, the computational model should also have the abil-
and word-meaning association. The significant differ-           ity to remove noise from raw signals and extract durable
ence lies in the fact that there exists a multitude of co-      and generalizable representations instead of simplifying
occurring word-object pairs in natural environments that        the problem by using consistent symbolic representations
infants are situated in, and the inference of referential       (e.g., text or phoneme transcripts).
intents through body movements plays a key role in dis-         Assumptions in the Model
covering which co-occurrences are relevant.
                                                                The range of problems we need to address in model-
                  General Discussion                            ing lexical acquisition in a purely unsupervised manner
The Role of Embodied Intention                                  and from raw multimodal data is substantial, so to make
We propose that the ability of a young language learner         concrete progress, some natural assumptions were made
to infer interlocutors‚Äô referential intentions through the      to simplify the modeling task and allow us to focus on
observations of their body movements may significantly          the key problems in lexical acquisition. First, the model
facilitate lexical learning. This proposal has been verified    mainly deals with how to associate visual representations
by the empirical studies in which adult language learn-         of objects with their spoken object names. This is based
ers exposed to a second language in the intention-cued          on the finding that a predominant proportion of infant
condition outperformed the ones under audio-visual con-         early vocabulary are nouns, which has been confirmed in
dition in both word discovery and word-meaning learn-           various languages and under varying child-rearing condi-
ing tests. Furthermore, in the computational model de-          tions (Caselli, Casadio, & Bates, 2000). Also, the model
scribed in the previous section, a speaker‚Äôs referential in-    is able to learn only object names that are grounded in
tents are estimated and utilized to facilitate lexical learn-   visual perception but not other nouns that represent other
ing in two ways. Firstly, possible referential objects          meanings or abstract notions. We believe that those ini-
in time provide cues for word spotting from a continu-          tial and imageable words directly grounded in the phys-
ous speech stream. Speech segmentation without prior            ical environment serve as a foundation for the acquisi-
                                                           1297

tion of abstract words that become indirectly grounded           the international conference on acoustics, speech and
through their relations to those grounded words. Second,         signal processing. Hong Kong.
the model does not intend to simulate the development of       Baron-Cohen, S. (1995). Mindblindness: an essay on
initial capabilities to recognize phonemes from acoustic         autism and theory of mind. Cambridge: MIT Press.
input. We assume that a language learner has knowl-            Bloom, P. (2000). How children learn the meanings of
edge of the phonetic structure of the language prior to          words. Cambridge, MA: The MIT Press.
lexical development. Third, in natural conditions, a lan-      Brent, M. R. (1999). Speech segmentation and word dis-
guage learner observes the body movements of an inter-           covery: A computational perspective. Trends in Cog-
locutor and infers referential objects by means of mon-          nitive science, 3(8), 294-301.
itoring his/her gaze direction. Due to the difficulties to     Brent, M. R., & Cartwright, T. A. (1996). Distribu-
track the speaker‚Äôs gaze directions and head movements,          tional regularity and phonotactic constraints are useful
and then search for a target object in that direction from       for segmentation. Cognition, 61, 93-125.
the learner‚Äôs perspective, in both empirical studies and       Caselli, M. C., Casadio, P., & Bates, E. (2000). Lexical
the computational simulation, an eye tracker and posi-           development in english and italian. In M. Tomasello &
tion sensors are utilized so that the language learner (i.e.     E. Bates (Eds.), Language development: The essential
a human subject or the computational model) can directly         reading (p. 76-110). Blackwell publishers.
obtain the interlocutor‚Äôs gaze and head movements, and
                                                               Cooper, R. M. (1974). The control of eye fixation by the
also share the visual scene.
                                                                 meaning of spoken language: A new methodology for
                        Conclusion                               the real-time investigation of speech perception, mem-
This work demonstrates a significant role of embod-              ory, and language processing. Cognitive Psychology,
ied intention in infant word learning through both hu-           6, 84-107.
man subject study and computational modeling. In both          Cutler, A., & Butterfield, S. (1992). Rhythmic cues to
cases, no matter a language learner is a human subject           speech segmentation: Evidence from juncture misper-
or a computer program, the intention-cued approach out-          ception. Journal of Memory and Language, 31, 218-
performed the audio-visual approach. We conclude that            236.
the solely statistical learning of co-occurrences in data      Gillette, J., Gleitman, H., Gleitman, L., & Lederer, A.
is less likely to explain the whole story of language ac-        (1999). Human simulations of vocabulary learning.
quisition. The inference of embodied intention, as one           Cognition, 73, 135-176.
of infants‚Äô social cognitive skills, provides constraints to
                                                               Jusczyk, P. W., Hohne, E. A., & Bauman, A. (1999). In-
avoid the large amount of irrelevant computations and
                                                                 fants‚Äô sensitivity to allophonic cues for word segmen-
can be directly applied as deictic reference to associate
                                                                 tation. Perception and Psychophysics, 61, 1465-1476.
words with visually grounded referents in the environ-
ment. Here we do not claim that young children employ          Lakoff, G., & Johnson, M. (1980). Metaphors we live
the exact method presented in this paper. However, as            by. Chicago: University of Chicago Press.
a computational model, this work provides an existence         Lakoff, G., & Johnson, M. (1999). Philosophy in the
proof for a machine learning technique that solves the           flesh: The embodied mind and its challenge to western
lexical acquisition task. It leaves the open question of         thought. New York:Basic Books.
what techniques young children actually use to solve the       Mattys, S. L., & Jusczyk, P. W. (2001). Phonotactic cues
problem by further empirical study. We hope that this            for segmentation of fluent speech by infants. Cogni-
work not only provides a computational account to sup-           tion, 78, 91-121.
plement the existing related theories of language acqui-       Richards, D., & Goldfarb, J. (1986). The episodic mem-
sition but also gives some useful hints to future research.      ory model of conceptual development: An integrative
                        References                               viewpoint. Cognitive Development, 1, 183-219.
Aslin, R. N., Woodward, J. C., laMendola, N., & Bever,         Roy, D., & Pentland, A. (2002). Learning words from
   T. (1996). Models of word segmentation in fluent              sights and sounds: A computational model. Cognitive
   maternal speech to infants. In J. Morgan & K. Demuth          Science, 26(1), 113-146.
   (Eds.), Signal to syntax: Bootstrapping from speech to      Saffran, J. R., Newport, E. L., & Aslin, R. N. (1996).
   grammar in early acquisition. Hillsdale,NJ: Erlbaum.          Word segmentation: The role of distributional cues.
                                                                 Journal of memory and language, 35, 606-621.
Baldwin, D. A., Markman, E. M., Bill, B., Desjardins,
                                                               Tomasello, M. (2001). In M. Bowerman & S. Levinson
   R. N., Irwin, J. M., & tidball, G. (1996). Infant‚Äôs
                                                                 (Eds.), Language acquisition and conceptual develop-
   reliance on a social criterion for establishing word-
                                                                 ment. Cambridge University Press.
   object relations. Child development, 67, 3135-3153.
                                                               Williams, S., & Vivas, J. (1989). I went walking. Har-
Ballard, D. H., Hayhoe, M. M., Pook, P. K., & Rao, R.            court Brance and Company.
   P. N. (1997). Deictic codes for the embodiment of
                                                               Yu, C., Ballard, D. H., & Zhu, S. (2002). Attentional ob-
   cognition. Behavioral and Brain Sciences, 20, 1311-
                                                                 ject spotting by integrating multisensory input. In Pro-
   1328.
                                                                 ceedings of the 4th international conference on multi-
Ballard, D. H., & Yu, C. (2003). A multimodal learn-             modal interface. Pittsburg, PA.
   ing interface for word acquisition. In Proceedings of
                                                          1298

