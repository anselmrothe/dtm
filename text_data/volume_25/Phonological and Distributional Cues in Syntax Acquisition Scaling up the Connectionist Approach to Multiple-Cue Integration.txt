UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Phonological and Distributional Cues in Syntax Acquisition: Scaling up the Connectionist
Approach to Multiple-Cue Integration
Permalink
https://escholarship.org/uc/item/85355384
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Reali, Florencia
Christiansen, Morten H.
Monaghan, Padraic
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                     University of California

                       Phonological and Distributional Cues in Syntax Acquisition:
                Scaling up the Connectionist Approach to Multiple-Cue Integration
                                                   Florencia Reali (fr34@cornell.edu)
                                 Department of Psychology; Cornell University; Ithaca, NY 14853 USA
                                           Morten H. Christiansen (mhc27@cornell.edu)
                                 Department of Psychology; Cornell University; Ithaca, NY 14853 USA
                                    Padraic Monaghan (Padraic.Monaghan@warwick.ac.uk)
                             Department of Psychology, University of Warwick; Coventry, CV4 7AL, UK
                               Abstract                                       Some, perhaps most, aspects of syntactic knowledge
   Recent work in developmental psycholinguistics suggests that          have to be acquired from mere exposure. Acquiring the
   children may bootstrap grammatical categories and basic               specific words and phonological structure of a language
   syntactic structure by exploiting distributional, phonological,       requires exposure to a significant corpus of language input.
   and prosodic cues. Previous connectionist work has indicated          In this context, distributional cues constitute an important
   that multiple-cue integration is computationally feasible for
                                                                         source of information for bootstrapping language structure
   small artificial languages. In this paper, we present a series of
   simulations exploring the integration of distributional and           (for a review, see Redington & Chater, 1998). By eight
   phonological cues in a connectionist model trained on a full-         months, infants have access to powerful mechanisms to
   blown corpus of child-directed speech. In the first simulation,       compute the statistical properties of the language input
   we demonstrate that the connectionist model performs very             (Saffran, Aslin and Newport, 1996). By one year, children’s
   well when trained on purely distributional information                perceptual attunement is likely to allow them to use
   represented in terms of lexical categories. In the second             language-internal probabilistic cues (for reviews, see e.g.
   simulation we demonstrate that networks trained on                    Jusczyk, 1997). For example, children appear to be sensitive
   distributed vectors incorporating phonetic information about          to the acoustic differences reflected by the number of
   words also achieve a high level of performance. Finally, we
                                                                         syllables in isolated words (Shi, Werker & Morgan, 1999),
   employ discriminant analyses of hidden unit activations to
   show that the networks are able to integrate phonological and         and the relationship between function words and prosody in
   distributional cues in the service of developing highly reliable      speech (Shafer, Shucard, Shucard & Gerken, 1998).
   internal representations of lexical categories.                       Children are not only sensitive to distributional information,
                                                                         but they also are capable of multiple-cue integration
                          Introduction                                   (Mattys, Jusczyk, Luce & Morgan, 1999).
Mastering natural language syntax may be one of the most                      The multiple-cue integration hypothesis (e.g.,
difficult learning tasks that children face. This achievement            Christiansen & Dale, 2001; Gleitman & Wanner, 1982;
is especially impressive given that children acquire most of             Morgan, 1996) suggests that integrating multiple
this syntactic knowledge with little or no direct instruction.           probabilistic cues may provide an essential scaffolding for
In adulthood, syntactic knowledge can be characterized by                syntactic learning by biasing children toward aspects of the
constraints governing the relationship between grammatical               input that are particularly informative for acquiring
categories and words (such as noun and verb) in a sentence.              grammatical structure. In the present study we focus on the
The syntactic constraints presuppose the grammatical                     integration of distributional and phonological cues using a
categories in terms of which they are defined; but the                   connectionist approach.
validity of grammatical categories depends on how far they                    In the remainder of this paper, we first provide a review
support syntactic constraints. Thus, acquiring syntactic                 of the empirical evidence suggesting that infants may use
knowledge presents the child with a “bootstrapping”                      several different phonological cues to bootstrap into
problem.                                                                 language. We then present a series of simulations
      Bootstrapping into language seems to be vastly                     demonstrating the efficacy of distributional cues for the
challenging, both because the constraints governing natural              acquisition of syntactic structure. In previous research
language are so intricate, and because young children do not             (Christiansen & Dale, 2001), we have shown the advantages
have the intellectual capacity or explicit instruction                   of multiple-cue models for the acquisition of grammatical
available to adults. Yet, children solve this “chicken-and-              structure in artificial languages. In this paper we are seeking
egg” problem surprisingly well: Before they can tie their                to scale up this model, by training it on a complex corpus of
shoes they have learned a great deal about how words are                 child-directed speech. In the first simulation we show that
combined to form complex sentences. Determining how                      simple recurrent networks trained on lexical categories are
children accomplish this challenging task remains an open                able to predict grammatical structure from the corpus. In the
question in cognitive science.                                           second simulation, we show that a network trained with
                                                                     970

phonetic information about the words in the corpus                 Table 1: Phonological cues that distinguish between
performed better in bootstrapping syntactic structure than a                             lexical categories.
control network trained on random inputs. Finally, we
analyze the networks’ internal representations for lexical                               Nouns and Verbs
                                                                  Nouns have more syllables than verbs (Kelly, 1992)
categories, and find that the networks are capable of
integrating both phonetic and distributional information in       Bisyllabic nouns have 1st syllable stress, verbs tend to have 2nd
the service of developing reliable representations for nouns      syllable stress (Kelly & Bock, 1988)
and verbs.                                                        Inflection -ed is pronounced /d/ for verbs, /@d/ or /Id/ for
                                                                  adjectives (Marchand, 1969)
       Phonological cues to lexical categories                    Stressed syllables of nouns have more back vowels than front
There are several phonological cues that individuate lexical      vowels. Verbs have more front vowels than back vowels
categories. Nouns and verbs are the largest such categories,      (Sereno & Jongman, 1990)
and consequently have been the focus of many proposals for
distinctions in terms of phonological cues. Distinctions have     Nouns have more low vowels, verbs have more high vowels
                                                                  (Sereno & Jongman, 1990)
also been made between function words and content words.
Table 1 summarizes a variety of phonological cues that have       Nouns are more likely to have nasal consonants (Kelly, 1992)
been proposed to distinguish between different syntactic          Nouns contain more phonemes per syllable than verbs (Kelly,
categories.                                                       1996)
     Corpus-based studies of English have indicated that
distinctions between lexical categories based on each of                           Function and Content words
these cues considered independently are statistically             Function words have fewer syllables than content words
significant (Kelly, 1992). Shi, Morgan and Allopenna              (Morgan, Shi & Allopenna, 1996)
(1998) assessed the reliability of several cues when used         Function words have minimal or null onsets (Morgan, Shi &
simultaneously in a discriminant analysis of                      Allopenna, 1996)
function/content words from small corpora of child-directed
                                                                  Function word onsets are more likely to be coronal (Morgan,
speech. They used several cues at the word level (e.g.,
                                                                  Shi & Allopenna, 1996)
frequency), the syllable level (e.g., number of consonants in
the syllable), and the acoustic level (e.g., vowel duration)      /D/ occurs word-initially only for function words (Morgan, Shi
and produced 83%-91% correct classification for each of the       & Allopenna, 1996)
mothers in the study. Durieux and Gillis (2001) considered a      Function words have reduced vowels in the first syllable
number of phonological cues for distinguishing nouns and          (Cutler, 1993)
verbs and, with an instance-based learning system correctly
                                                                  Function words are often unstressed (Gleitman & Wanner,
classified approximately 67% of nouns and verbs from a            1982)
random sample of 5,000 words from the CELEX database
(Baayen, Pipenbrock & Gulikers, 1995).
     Cassidy and Kelly (1991) report experimental data          frequent words in a child-directed speech database
indicating that phonological cues are used in lexical           (CHILDES, MacWhinney, 2000). There were 402 nouns
categorization. Participants were required to listen to a       and 218 verbs in our analysis. We conducted discriminant
nonword and make a sentence including the word. The             analyses on each cue individually, and found that correct
nonword stimuli varied in terms of syllable length. They        classification was only just above chance for each cue. The
found that longer nonwords were more likely to be placed in     best performance was for syllable length, with correct
noun contexts, whereas shorter nonwords were produced in        classification of 41.0% of nouns and 74.8% of verbs
verb contexts.                                                  (overall, with equally weighted groups, 57.4% correct
     Monaghan, Chater and Christiansen (in press) have          classification). When the cues were considered jointly,
shown that sets of phonological cues, when considered           92.5% of nouns and 41.7% of verbs were correctly
integratively, can predict variance in response times on        classified (equal-weighted-group accuracy was 74.7%). The
naming and lexical decision for nouns and verbs. Words that     cues, when considered together resulted in accurate
are more typical of the phonological characteristics of their   classification for nouns, but many verbs were also
lexical category have quicker response times than words         incorrectly classified as nouns (see Monaghan, Chater &
that share few cues with their category.                        Christiansen, submitted).
     We accumulated a set of 16 phonological cues, based             The discriminant analysis indicates that phonological
on the list in Table 1. Some entries in the Table generated     information is useful for lexical categorization, but not
more than one cue. For example, we tested whether reduced       sufficient without integration with cues from other sources.
vowels occurred in the first syllable of the word, as well as   In the following simulations, we first show how a
testing for the proportion of reduced vowels throughout the     connectionist model is capable of learning aspects of
word. We tested each cue individually for its power to          syntactic structure from the distributional information
discriminate between nouns and verbs from the 1000 most         derived from a corpus of child-directed speech. The
                                                            971

subsequent simulation and hidden unit analyses then explore      category to which it belonged. The training set consisted of
how networks may benefit from integrating the kind of            9,072 sentences (29,930 word tokens) from the original
phonetic cues described above with distributional                corpus. A separate test set consisted of 963 additional
information.                                                     sentences (2,930 word tokens).
                       Simulation 1:                             Procedure The ten SRNs were trained on the corpus
     Learning syntactic structure using SRNs                     described above. Training consisted of 10 passes through
We trained simple recurrent networks (SRN; Elman, 1990)          the training corpus. Performance was assessed based on the
to learn the syntactic structure present in a child-directed     networks ability to predict the next set of lexical categories
speech corpus. Previous research has shown that SRNs are         given the prior context.
able to acquire both simple (Christiansen & Chater, 1999;
Elman, 1990) and slightly more complex and                       Results and discussion
psychologically motivated artificial languages (Christiansen     Each lexical category was represented by the activation of a
& Dale, 2001). An important outstanding question is              single unit in the output layer. After training, SRNs trained
whether these artificial-language models can be scaled up to     with localist output representations will produce a
deal with the full complexity and the general disorderliness     distributional pattern of activation closely corresponding to
of speech directed at young children. Here, we therefore         a probability distribution of possible next items.
seek to determine whether the advantages of distributional            In order to assess the overall performance of the SRNs,
learning in the small-scale simulations will carry over to the   we made comparisons between network output probabilities
learning of a natural corpus. Our simulations demonstrate        and the full conditional probabilities given the prior
that these networks are indeed capable of acquiring              occurrence of lexical categories within an utterance
important aspects of the syntactic structure of realistic        (Christiansen & Chater, 1999). Thus, with c i denoting the
corpora from distributional cues alone.                          lexical category of ith word in the sentence we have the
                                                                 following relation:
Method
Networks Ten SRNs were used with an initial weight                                    P(cp  c1 , c2 , . . . cp-1) =
randomization in the interval [-0.1; 0.1]. A different random           Freq (c1 , c2, . . . , cp-1, cp) / Freq (c1 , c2, . . . , cp-1)
seed was used for each simulation. Learning rate was set to
0.1, and momentum to 0.7. Each input to the network              where the probability of getting some member of a lexical
contained a localist representation of the lexical category of   category is conditional on the previous p-1 categories.
the incoming word. With a total of 14 different lexical               To provide a statistical benchmark with which to
categories and a pause marking boundaries between                compare the network performance, we “trained” bigram and
utterances, the network had 15 input units. The network was      trigram models on the Bernstein-Ratner corpus. These
trained to predict the lexical category of the next word, and    finite-state models borrowed from computational linguistics,
thus the number of output units was 15. Each network had         provide a simple prediction method based on strings of two
30 hidden units and 30 context units.                            (bigrams) or three (trigrams) consecutive words.
                                                                      We compared the full conditional probabilities with the
Materials We trained and tested the network on a corpus          network output probabilities and also with bigram and
of child-directed speech (Bernstein-Ratner, 1984). This          trigram predictions. The mean cosine between the full
corpus contains speech recorded from nine mothers                conditional probabilities for the test set and the predictions
speaking to their children over a 4-5 month period when the      of the finite-state models were 0.81 for trigrams and 0.79 for
children were between the ages of 1 year and 1 month to 1        bigrams. We found that the mean cosine between the full
year and 9 months. The corpus includes 1,371 word types          conditional probabilities and network output probabilities
and 33,035 tokens distributed over 10,082 utterances. The        were comparable to the finite-state models’ predictions
sentences incorporate a number of different types of             (mean cosine: 0.86 for the training set and mean cosine:
grammatical structures, showing the varied nature of the         0.79 for the test set). Network predictions for the training set
linguistic input to children. Utterances range from              were better than bigram predictions (p's < .00005) and
declarative sentences ('Oh you need some space') to wh-          trigram predictions (p's < .00005). For the test set we found
questions ('Where's my apple') to one-word utterances ('Uh'      that the trigram model performed better than the networks
or 'hello'). Each word in the corpus corresponded to one of      (p's < .00005), but there was no difference between the
the 14 following lexical categories: nouns (19.5%), verbs        performance of the bigram model and that of the networks
(18.5%), adjectives (4%), numerals (<0.1%), adverbs              (p's < .52).
(6.5%), articles (6.5%), pronouns (18.5%), prepositions               Despite the complexity of child-directed speech –
(5%), conjunctions (4%), interjections (7%), complex             including the many false starts and other ‘ungrammatical’
contractions (8%), abbreviations (<0.1%), infinitive markers     constructions – Simulation 1 demonstrates that the SRN
(1.2%) and proper names (1.2%). Each word in the original        model is able to acquire much of the syntactic structure in
corpus was replaced by a vector encoding the lexical             the corpus from a single cue: distributional information.
                                                             972

Although these results fit with results from computational         if no stress, 1 otherwise), final inflection (0 if none, /@d/ or
linguistics where models are often trained on corpora              /Id/, 1 if present), stress vowel position (from front to back,
encoded in the form of lexical categories, it is also clear that   1-3), vowel position (mean position of vowels, from front to
children are not provided directly with such “tagged” input.       back, 1-3), final consonant voicing (0: vowel, 1: voiced, 2:
Rather, the child has to bootstrap both lexical categories and     unvoiced), proportion of nasal consonants (0-1) and mean
syntactic constraints concurrently. One way of doing this          height of vowels (0-3). The cues that assume only binary
may be to combine distributional information with other            values were encoded using a single unit (e.g., “any stress”,
kinds of cues. Therefore in the next simulation we trained         “initial /D/”). The cues that take on values between 0 and 1
the same type of network but with each word encoded not            (e.g., proportion of vowel consonants) were also encoded
simply by fifteen lexical categories but instead by more than      using a single unit with a decimal number, whereas the cues
a thousand different vectors encompassing different types of       that assume values in a broader range (e.g., number of
phonological information.                                          syllables) were represented using a thermometer encoding
                                                                   - for example, one unit would be on for monosyllabic
                        Simulation 2:                              words, two for bisyllabic words, and so on. Finally we used
    Phonological cues for syntactic acquisition                    a single unit that would be activated at utterance boundaries.
We here take a further step towards providing a more solid               The random-input networks were trained using input
computational foundation for multiple-cue integration. In          for which we randomly permuted the multiple-cue vectors
Simulation 1 we provided evidence of the efficacy of using         among all the words in the corpus. Thus, the random vector
SRNs for learning syntactic structure from the corpus. Our         encoding a given word would be reassigned to an arbitrary
next aim is to determine the extent to which these networks        word in the corpus regardless of its lexical category. Each
are sensitive to the lexical category information present in a     phonological vector was assigned to only one word.
set of phonological cues. To accomplish this task we set up        Moreover, each token of a word was represented using the
two identical groups of networks, each provided with a             same random vector for all occurrences of that word in the
different encoding of the corpus. The encoding of the first        test and training sets.
corpus was based on 16 phonological cues mentioned above
(Table 1). The second set of input was encoded using a             Procedure Ten networks were trained on phonological
random encoding. Possible performance differences in               cues and ten control networks were trained on the random
networks trained with these different input sets would be          vectors. The training consisted of a pass through the training
due to lexical category information available in the               corpus. We used the same ten random seeds for both
phonological cues.                                                 simulation conditions. As in Simulation 1, the networks
                                                                   were trained to predict the lexical category of the next word.
                                                                   The task of mapping phonological cues onto lexical
Method                                                             categories may seem somewhat artificial because children
Networks Ten SRNs were used for the phonetic-input
                                                                   are not provided directly with the lexical categories of the
condition and the random-input condition, with an initial
                                                                   words to which they are exposed. However, children do
weight randomization in the interval [-0.1;0.1]. We used the
                                                                   learn early on to use pragmatic and other cues to discover
same values for learning rate and momentum as in
                                                                   the meaning of words. Given that the networks in our
Simulation 1. Each input to the network contained a
                                                                   simulations only have access to linguistic information, we
thermometer encoding for each of the 16 phonological cues.
                                                                   see lexical categories as a “stand-in” for more ecologically
This encoding required 43 units (each of them in a range
                                                                   valid cues that we hope to be able to include in future work.
from 0 to 1) and a pause marking boundaries between
utterances, resulting in the networks having 44 input units.
Each output was encoded using a localist representation for        Results and discussion
the different lexical categories similarly to Simulation 1.        As in Simulation 1, we recorded the networks’ output
With the 14 different lexical categories and a pause marking       probabilities and computed the full conditional probability
boundaries between utterances, the networks had 15 output          vectors for the two groups of networks. We compared the
units. Each network furthermore was equipped with 88               predictions of the phonetic-input networks with those of the
hidden units and 88 context units.                                 random-input networks1. The mean cosine between the full
                                                                   conditional probabilities and the phonetic-input networks’
Materials We used the same training and test corpora as in         output probabilities was 0.75 for the training set and 0.69
Simulation 1. Each word was encoded according to the               for the test set. For the random-input networks, we found
following sixteen phonological cues: number of phonemes            that the mean cosine was 0.73 for the training set, and 0.67
(1-11), number of syllables (1-5), stress position (0 = no         1
stress, 1 = 1st syllable stressed, etc.), proportion of reduced      Given the absence of explicit lexical category information in the
vowels (0-1), proportion of coronal consonants (0-1),              input combined with the complexity and nature of the phonetic
number of consonants in onset (1-3), consonant complexity          encoding in Simulation 2, network performance is not directly
                                                                   comparable with that of the bigram/trigram models. Thus, the
(0-1), initial /D/ (1 if begins /D/, 0 otherwise), reduced first
                                                                   seemingly better performance of the finite-state models (in terms
vowel (1 if first vowel is reduced, 0 otherwise), any stress (0    of mean cosine) is somewhat deceptive.
                                                               973

for the test set. The phonetic-input networks were               nouns and verbs. This was significantly better than the
significantly better than the random-input networks at           random-input networks, which only achieved 73.15%
predicting the next combination of lexical categories, both      correct separation (t(8) = 5.89, p < .0001). Both sets of
for the training set (p's < .00005) and the test set (p's <      networks surpassed their respective randomized controls
.00005). These results suggest that distributional               (phonetic-input control: 69.05% – t(8) =11.51, p < .0001;
information is generally a stronger cue than phonological        random-input control: 68.20% – t(8 )= 3.92, p < .004). The
information, even though the latter does lead to better          controls for the two sets of networks were not significantly
learning overall. However, phonological information may          different from each other (t(8) = 0.82, p > .43). As indicated
provide the networks with a better basis for processing          by our previous analyses of phonetic cue information in
novel lexical items. Next, we probe the internal                 child-directed speech (Monaghan et al., submitted), the
representations of the two sets of networks in order to gain     phonetic input vectors contained a considerable amount of
further insight into their performance differences.              information about lexical categories, allowing for 67.25%
                                                                 correct separation of nouns and verbs, but still significantly
        Probing the internal representations                     below the performance of the phonetic-input networks (t(4)
Simulation 2 indicated that the phonetic-input networks did      = 25.97, p < .0001). The random-input networks also
not benefit as much as one perhaps would have expected           surpassed the level of separation afforded by their input
from the information provided by the phonological cues.          vectors (59.00% – t(4) = 12.80, p < .0001).
However, the networks may nonetheless use this                         The results of the hidden-unit discriminant analyses
information to develop internal representations that better      suggest that not only did the phonetic-input networks
encode differences between lexical categories. This may          develop internal representations better suited for
allow them to go beyond the phonetic input and integrate it      distinguishing between nouns and verbs, but they also went
with the distributional information derived from the             beyond the information afforded by the phonetic input and
sequential order in which these vectors were presented. To       integrate it with distributional information. Crucially, the
investigate these possibilities, we carried out a series of      phonetic-input vectors were able to surpass the random-
discriminant analyses of network hidden unit activations as      input networks, despite that the latter was also able to use
well as of the phonetic input vectors, focusing on the           distributional information to go beyond the input. Consistent
representations of nouns and verbs.                              phonological information thus appears to be important for
                                                                 network generalization to novel nouns and verbs2.
Method
Informally, a linear discriminant analysis allows us to                                     Conclusions
determine the degree to which it is possible to separate a set   A growing bulk of experimental evidence from
of vectors into two (or more) groups based on the                developmental cognitive science has indicated that children
information contained in those vectors. In effect, we attempt    are sensitive to and able to combine a host of different
to use a linear plane to split the hidden unit space into a      sources of information, and that this may help them
group of noun vectors and a group of verb vectors. Using         overcome the syntactic bootstrapping problem. However,
discriminant analyses, we can statistically estimate the         one important caveat regarding such multiple-cue
degree to which this split can be accomplished given a set of    integration is that the various sources of information are
vectors.                                                         highly probabilistic, and each is unreliable when considered
     We recorded the hidden unit activations from the two        in isolation. Although some headway has been made in the
sets of networks in Simulation 2. The hidden unit                investigation of possible computational mechanisms that
activations were recorded for 200 novel nouns and 200            may be able to integrate multiple probabilistic cues, this
novel verbs occurring in unique sentences taken from other       work has primarily been small in scale (e.g., Christiansen &
CHILDES corpora. The hidden unit activations were labeled        Dale, 2001).
such that each corresponded to the particular lexical                  In this paper, we have presented two series of
category of the input presented to the network (though the       simulations aimed at taking the first steps towards scaling
networks did not receive this information as input). For         up connectionist models of multiple-cue integration to deal
example, a vector would be labeled a noun vector when the        with the full complexity of natural speech directed at
hidden unit activations were recorded for a noun (phonetic)      children. The results of Simulation 1 demonstrated that
input vector. We also included a condition in which the          SRNs are capable of taking advantage of distributional
noun/verb labels were randomized with respect to the             information – despite the many grammatical inconsistencies
hidden unit vectors for both sets of networks, in order to       found in child-directed speech. In Simulation 2, we
establish a random control.
                                                                 2
                                                                   It is possible to object that children do not only get cues that are
                                                                 relevant for syntactic acquisition. Christiansen and Dale (2001)
Results and Discussion                                           specifically addressed this issue by providing networks with
We first compared the two sets of networks. The phonetic-        additional cues that did not correlate with syntax. They found that
input networks had developed hidden unit representations         the networks were able to ignore such “distractor” cues and focus
that allowed them to correctly separate 80.30% of the 400        on the relevant cues.
                                                             974

expanded these results by showing that SRNs are also able        Kelly, M.H. (1996). The role of phonology in grammatical
to utilize the highly probabilistic information found in the        category assignment. In J.L. Morgan & K. Demuth (Eds.)
16 phonological cues in the service of syntactic acquisition.       Signal to Syntax: Bootstrapping from Speech to
Our probing of the networks’ hidden unit activations                Grammar in Early Acquisition. Mahwah, NJ: Lawrence
provided further evidence that the integration of                   Erlbaum Associates.
phonological and distributional cues during learning leads to    Kelly, M.H. & Bock, J.K. (1988). Stress in time. Journal of
more robust internal representations of lexical categories, at      Experimental Psychology: Human Perception &
least when it comes to distinguishing between the two major         Performance, 14, 389-403.
categories of nouns and verbs. Overall the results presented     MacWhinney, B. (2000). The CHILDES Project: Tools for
here underscore the computational feasibility of the                Analyzing Talk (3rd Edition). Mahwah, NJ: Lawrence
multiple-cue integration hypothesis, in particular within a         Erlbaum Associates.
connectionist framework.                                         Marchand, H. (1969). The Categories and Types of Present-
                                                                    day English Word-formation (2nd Edition). Munich,
                   Acknowledgments                                  Germany: C.H. Beck'sche Verlagsbuchhandlung.
This research was supported by the Human Frontiers               Mattys, S.L., Jusczyk, P.W., Luce, P.A. & Morgan, J.L.
Science Program.                                                    (1999). Phonotactic and prosodic effects on word
                                                                    segmentation in infants. Cognitive Psychology, 38, 465-
                        References                                  494.
Baayen, R.H., Pipenbrock, R. & Gulikers, L. (1995). The          Monaghan, P., Chater, N. & Christiansen, M.H. (in press).
   CELEX Lexical Database (CD-ROM). Linguistic Data                 Inequality between the classes: Phonological and
   Consortium, University of Pennsylvania, Philadelphia,            distributional typicality as predictors of lexical
   PA.                                                              processing. In Proceedings of the 25th Annual
Bernstein-Ratner, N. (1984). Patterns of vowel modification         Conference of the Cognitive Science Society. Mahwah,
   in motherese. Journal of Child Language, 11, 557-578.            NJ: Lawrence Erlbaum.
Cassidy, K.W. & Kelly, M.H. (1991). Phonological                 Monaghan, P., Chater, N. & Christiansen, M.H. (submitted).
   information for grammatical category assignments.                Differential Contributions of Phonological and
   Journal of Memory and Language, 30, 348-369.                     Distributional Cues in Language Acquisition.
Christiansen, M.H. & Chater, N. (1999). Toward a                 Morgan, J.L. (1996) Prosody and the roots of parsing.
   connectionist model of recursion in human linguistic             Language and Cognitive Processes, 11, 69-106.
   performance. Cognitive Science, 23, 157-205.                  Morgan, J.L., Shi, R. & Allopenna, P. (1996). Perceptual
Christiansen, M.H. & Dale, R.A.C. (2001). Integrating               bases of grammatical categories. In J.L. Morgan & K.
   distributional, prosodic and phonological information in         Demuth (Eds.) Signal to Syntax: Bootstrapping from
   a connectionist model of language acquisition. In                Speech to Grammar in Early Acquisition. Mahwah, NJ:
   Proceedings of the 23rd Annual Conference of the                 Lawrence Erlbaum Associates.
   Cognitive Science Society (pp. 220-225). Mahwah, NJ:          Redington, M., & Chater, N. (1998). Connectionist and
   Lawrence Erlbaum.                                                statistical approaches to language acquisition: A
Cutler, A. (1993). Phonological cues to open- and closed-           distributional perspective. Language and Cognitive
   class words in the processing of spoken sentences.               Processes, 13, 129-191.
   Journal of Psycholinguistic Research, 22, 109-131.            Saffran, J.R., Aslin, R.N. & Newport, E.L. (1996).
Durieux, G. & Gillis, S. (2001). Predicting grammatical             Statistical learning by 8-month-old infants. Science, 274,
   classes from phonological cues: An empirical test. In J.         1926-1928.
   Weissenborn & B. Höhle (Eds.) Approaches to                   Sereno, J.A. & Jongman, A. (1990). Phonological and form
   Bootstrapping: Phonological, Lexical, Syntactic and              class relations in the lexicon. Journal of Psycholinguistic
   Neurophysiological Aspects of Early Language                     Research, 19, 387-404.
   Acquisition Volume 1. Amsterdam: John Benjamins.              Shaffer, V.L., Shucard, D.W., Shucard, J.L. & Gerken, L.A.
Elman, J.L. (1990). Finding structure in time. Cognitive            (1998) . An electrophysiological study of infants'
   Science, 14, 179-211.                                            sensitivity to the sound patterns of English speech.
Gleitman, L.R. & Wanner, E. (1982). Language acquisition:           Journal of Speech Language and Hearing Research, 41,
   The state of the state of the art. In E. Wanner & L.R.           874-886.
   Gleitman (Eds.) Language Acquisition: The State of the        Shi, R., Werker, J.F., & Morgan J.L. (1999). Newborn
   Art (pp.3-48). Cambridge: Cambridge University Press.           infants sensitivity to perceptual cues to lexical and
Jusczyk, P. W. (1997). The discovery of spoken language.           grammatical words. Cognition, 72, B11-B21.
   Cambridge, MA: MIT Press.                                     Shi, R., Morgan, J.L. & Allopenna, P. (1998). Phonological
Kelly, M.H. (1992). Using sound to solve syntactic                 and acoustic bases for earliest grammatical category
   problems: The role of phonology in grammatical                  assignment: A cross-linguistic perspective. Journal of
   category assignments. Psychological Review, 99, 349-            Child Language, 25, 169-201.
   364.
                                                             975

