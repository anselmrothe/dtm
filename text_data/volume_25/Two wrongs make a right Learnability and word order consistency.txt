UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Two wrongs make a right: Learnability and word order consistency
Permalink
https://escholarship.org/uc/item/5tt20269
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Monaghan, Padraic
Gonitzke, Markus
Chater, Nick
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Two wrongs make a right: Learnability and word order consistency
                                 Padraic Monaghan (Padraic.Monaghan@warwick.ac.uk)
                                  Markus Gonitzke (Markus.Gonitzke@warwick.ac.uk)
                                          Nick Chater (N.Chater@warwick.ac.uk)
                                        Department of Psychology, University of Warwick
                                                      Coventry, CV4 7AL, UK
                              Abstract                             inconsistencies in the language are learnable. As a test case,
                                                                   we focus on word order in German and English. German
   Languages often demonstrate word order inconsistencies, and     and English are particularly interesting for comparison as
   such inconsistencies ought to make languages harder to          they have the same root (Cable & Bough, 1993), however,
   acquire. We present an integrative approach exploring the
                                                                   they differ in several important respects: In main clauses,
   relationship between learnability and word order,
   incorporating syntactic theory, corpus analyses and
                                                                   German has a subject-verb-object (SVO) word order,
   computational modelling. We focus on comparisons between        whereas in subordinate clauses, the order is subject-object-
   English and German, and conclude that inconsistencies may       verb (SOV), as shown in Sentence 1. The sentences are
   be preserved in the language due to the interaction between     subscripted with subject (S), object (O), finite verb (Vf),
   several syntactic structures.                                   infinite verb (Vi), and complementiser (C) to indicate the
                                                                   structures. In English, in contrast, word order is SVO in
                          Introduction                             both main and subordinate clauses (Sentence 2, translation
Inconsistent structures in language are harder to learn than       of Sentence 1). German and English also differ in terms of
consistent structures by computational systems, whether            verb position in infinite verb phrases. In German, the
inconsistencies are at the syntactic level (Christiansen &         infinite verb is sentence final after the object (Sentence 3),
Devlin, 1997), or at the lexical level, in terms of grapheme       whereas in English, the infinite verb occurs after the finite
to phoneme correspondences (Plaut, et al., 1996), or               verb, and before the object (Sentence 4, translation of
semantic ambiguities (Cottrell, 1986). Several languages are       Sentence 3). In subordinate clauses, this is complicated
entirely consistent in terms of head position (e.g., the           further in German: the finite verb moves to after the infinite
position of the verb in verb phrases), such as Japanese or         verb after the object (Sentence 5), but word order is the
Irish. However, a degree of inconsistency is present among         same in English (Sentence 6, translation of Sentence 5).
most languages (Kroch, 2000), even if there is still a high
degree of consistency (Lupyan & Christiansen, 2002; van            [1] SIch Vfbenutzte Odas Werkzeug Cdas Sich Odir Vfgab
Everbroeck, 1999). One possible contributor to learnability        [2] SI Vfused Othe tool Cthat SI Vfgave Oyou
is case-marking, which is particularly useful in languages
with relatively free word order (Lupyan & Christiansen,            [3] SEr Vfhat Oseine Meinung Vigeändert
2002), though this appears to be a necessary rather than a         [4] SHe Vfhas Vichanged Ohis opinion
sufficient condition (Kiparsky, 1996) for the learnability of
such languages.                                                    [5] SEr Vfkauft Oden Teppich Cda Ssie Odie alte Vizerstört
   Viewed in evolutionary terms, languages that are harder              Vfhaben
to learn are more likely to die out (Christiansen & Devlin,        [6] SHe Vfbuys Othe carpet Csince Sthey Vfhave Videstroyed
1997), and given the high rate of change of languages across            Othe old one
time, it is a significant challenge to explain how word order
inconsistencies are learned within languages.                         Such differences are real-world examples in languages
   Our approach to this challenge to account for learnability      with the same origin that have many similarities in common.
of inconsistencies was to bring together syntactic theory          The claims we make from our synthesis of corpus analyses
with analyses of the frequencies of different structures in        and computational modelling generalise to word order
real language corpora, and combine these with                      consistency in general. However, the two languages we
computational modelling. Previous simulations of word              have selected are of especial interest as an example as
order have largely ignored the proportions of different            English used to have the SOV structure in subordinate
syntactic structures (though with notable exceptions, e.g.,        clauses, but this has now changed to SVO which is
MacDonald & Christiansen, 2002). Through the use of real           consistent with main clause word order (Lightfoot, 1991).
language corpora in modelling, we can increase the                 Subordinate clauses constitute only a small proportion of
precision of determining the extent to which the processes         phrases, and so the different word order in a minority of
of sequential learning are engaged in language processing.         clauses is puzzling in its persistence – the greater frequency
   This paper presents a series of corpus analyses and             of SVO has not overwhelmed the SOV structure. Indeed,
simulations that explore the conditions under which                SVO structures are as easily parsed as SOV structures in
                                                                   subordinate clauses in studies on German speakers (Weyerts
                                                               816

et al., 2002). We suggest that general sequential learning
behaviour, as reflected in simple recurrent networks,
contributes towards preserving such inconsistencies in
German word order.
   In this paper, we explore three grammar fragments of
German, compared to the corresponding fragment in
English. We postulate that, though subordinate structures
may be harder to learn in German, the occurrence of verb-
final structures in main clause infinite verb phrases results in
easier learning of these structures. Finite verbs in final
position are rarer than infinite verbs in final position (26%
compared to 74%), and the verb-final position of infinite
verbs is acquired earlier than the position of the finite verb
(Clahsen & Muysken, 1986). This suggests that verb-
ordering in German is influenced by the occurrence of both
finite and infinite verb phrases. Finally, we make
predictions about the scaffolding of relatively infrequent              Figure 1. The simple recurrent network architecture we
word order inconsistencies through interaction with other,          used in the simulations. The model is trained to predict the
more frequent structures. We first detail how we combined            next word in the sentence, given the current word and the
corpus analyses with modelling in our comparisons between                 context of the previous state of the hidden units.
English and German.
                                                                   20,000 hand-tagged sentences from German newspapers.
               Corpus data in modelling                            For English, we employed the British National Corpus
MacDonald and Christiansen (2002) illustrated that the             (Burnard, 1995), composed of 100 million words of
different frequencies of linguistic structures have an impact      automatically tagged English (with an estimated error rate
on their ease of processing. It is, therefore, extremely useful    of 1.7%, with an additional 4.7% of words given ambiguous
to have a representation of the relative frequencies of            tags).
different structures in languages in order to make assertions         We derived simplified versions of the corpora by focusing
about the ease of acquisition of inconsistencies that may          on the NP, PP and VP structures. We omitted all words that
occur within the language. Such frequency information is           modified, but did not alter the NP, PP and VP sentence
not usually employed in modelling syntactic structures, with       structure1. Finally, we omitted any sentences that contained
models training on corpora generated from randomised               ambiguous tags, unclassified words, numerals, alphabetical
proportions of grammatical rules.                                  letters, existential there, conjunctions or postpositions.
   There are two major influences on the ability of simple            These simplifications resulted in 8,814 sentences in the
recurrent networks to learn sequential orders. The first is        NEGRA corpus, and 2,823,034 sentences in the BNC
predictability in word order: if a noun is always followed by      corpus which were comprised entirely of nouns, finite verbs,
a verb, for example, then the verb can be more easily              infinite verbs, prepositions, and complementisers2. Despite
predicted in the grammar. If word order is inconsistent then       the differences in scale and text source, the overall
this will result in greater difficulty of learning. If there are   proportions of different structures in the English and
many branching structures then learning to correctly predict       German corpora are approximately similar, as shown below
the next item will be difficult, though the model will be able     in the more detailed analyses. Given the large number of
to learn the transitional probabilities between elements in        sentences omitted from the corpora, however, the
the sequence. Another influence on learning in simple              proportions given ought to be taken as a general guide only.
recurrent networks is the impact of centre-embeddings in              The corpus data were used to generate sets of sentences
structures (Christiansen & Chater, 1999). The number of            that were plausible approximations to the proportions of
intervening lexical items between the subject noun and the         different phrase structures in English and German. The
verb will affect the accuracy of verb agreement, as long-          models we used were simple recurrent networks (Elman,
distance dependencies are more difficult to learn. This is         1990), or SRNs. SRNs are feedforward backpropagation
one interpretation of the results of Christiansen and Devlin’s     networks with an extra layer of ‘context’ units that record
(1997) study of recursive inconsistencies: Learning                the previous state of the network, and feedback onto the
difficulty relates to the distance between the subject noun        hidden layer of the model (Elman, 1990). This architecture
and the verb (see Figure 3 in Christiansen & Devlin, 1997;
see also Grüning, 2003).
                                                                      1
   We analysed two tagged corpora in order to assess the                 We omitted adjectives, adverbs, articles, interjections,
relative proportions of the different structures in the            possessive markers, infinitive markers, and the negative particle.
                                                                      2
grammars. For German, we examined the NEGRA corpus                      We also analysed the full corpora without omitting any
(Skut et al., 1997), which is composed of approximately            sentences and found very similar proportions of subordinate/main
                                                                   clause as those reported on the cleaned-up corpora.
                                                               817

has been used to learn sequences in a range of tasks,
including sequential learning of linguistic stimuli
(Christiansen & Devlin, 1997). An example of the models
used in our simulations is shown in Figure 1. This model,
for the first simulation, had 7 input and output units, and
eight hidden and context units. The 7 input/output units
corresponded to plural/singular noun (N/n), plural/singular
finite verb (Vf/vf), preposition (P), complementiser (C), and
an end of utterance marker (x). The unit corresponding to
each category is indicated by the letter above the unit. The
model was trained to predict the next token in a sentence, so
in Figure 1, the model is presented with a singular noun, and
predicts that the next element in the sentence will be a
singular verb. Weights were updated using backpropagation
with gradient descent of error with learning rate .1 and              Figure 2. Proportion of words in each category correctly
momentum .9.                                                       predicted by the model for Grammar 1 (see text for details).
    Table 1. Grammar 1 for English and German with main           Elements in parentheses indicate that these are optional, so a
 and subordinate clauses, with proportions of each structure      NP can be composed of a noun (N), or a N and a
           derived from BNC and NEGRA corpora.                    prepositional phrase (PP).
                                                                     The second column of Table 1 indicates the proportions
                                                                  of each structure in the BNC and NEGRA corpora. So, for
            ENGLISH                       PROPORTION              example, in English, 92.3% of sentences are composed of
   S → S S-bar                       7.7%
                                                                  NP VfP, and 7.7% of the time, they are composed of S S-
   S → NP VfP                       92.3%
                                                                  bar. Percentages in parentheses indicate the proportion of
   S-bar → C NP VfPsub              100%
                                                                  times that the corresponding element in parentheses in the
   NP → N (PP)                      76.2% (23.8%)
   PP → P NP                        100%                          grammar occurs. So, the proportions associated with the rule
   VfP → Vf (NP) (PP)               34.3% (48.5%) (17.2%)         VfP → Vf (NP) (PP) are 34.3% (48.5%) (17.2%), which
   VfPsub → Vf (NP) (PP)            32.5% (46.5%) (21.0%)         means that, of the occurrences of VfP, VfP → Vf 34.3% of
            GERMAN                                                the time, VfP → Vf NP 48.5% of the time, and VfP → Vf PP
   S → S S-bar                      10.5%
                                                                  17.2% of the time (in these cases, the rows sum to 100%).
   S → NP VfP                       89.5%                            The grammar rules in Table 1 described approximately
   S-bar → C NP VfPsub              100%                          33.6% of the sentences in the simplified English corpus, and
   NP → N (PP)                      70.4% (29.6%)                 42.3% of the simplified German corpus. Of particular
   PP → P NP                        100%                          interest from this corpus analyses were the proportions of
   VfP → Vf (NP) (PP)              11.3% (72.0%) (16.8%)          the main clause sentences, and those composed of a main
   VfPsub → (NP) (PP) Vf          (69.3%) (10.9%) 19.8%           clause and a subordinate clause. In both English and
                                                                  German, only a small number of each are sentences
                                                                  composed from the rule: S → S S-bar (7.7% in English and
                                                                  10.5% in German). Lightfoot (1991) has proposed that the
            Main and subordinate clauses                          change in English in subordinate clauses from SOV to SVO
                                                                  was due to the overwhelming of the rarer SOV structure by
   The first grammar that we compare between English and
                                                                  the more frequent SVO.
German consisted only of finite verb phrases in subordinate
                                                                     We generated a corpus of 1000 sentences, with branching
clauses. The purpose of this simulation was to test whether
                                                                  according to the proportions of each structure that we found
the grammar with SVO in main clause and SOV in
                                                                  in the corpora for each grammar. We ran 10 simulations for
subordinate clause was harder to learn than the grammar
                                                                  each grammar, with different randomly generated corpora of
with SVO in both main and subordinate clauses. The rules
                                                                  1000 sentences.
of the simple grammars we compared are shown in the first
                                                                     We tested the model for its ability to predict the
column of Table 1. The grammatical rules are read as the
                                                                  proportion of occurrence of the next item in the training
element to the left of the arrow can be composed of the
                                                                  corpus when tested on a different randomly-generated
structures to the right of the arrow. So, a sentence (S) can be
                                                                  corpus of 1000 sentences. A measure of mean squared error
composed of a noun phrase (NP) followed by a finite verb
                                                                  (MSE) for each word in the test corpus, as used by
phrase (VfP). Rules can be recursive, as in the first rule,
                                                                  Christiansen and Devlin (1997), reflected the ability of the
where a sentence (S) can be composed of a sentence
                                                                  model to learn the conditional probabilities in the corpus.
followed by a complementiser (C) followed by a
                                                                  Lower MSE means that the model reflects the conditional
subordinate clause sentence (S-bar notates the
                                                                  probabilities in the corpus with greater accuracy.
complementiser and the subordinate clause sentence).
                                                              818

   For the German grammar, overall MSE was 0.045 (sd =
0.001) for each ‘word’ in the test corpus. For the English
grammar, MSE was 0.013 (sd = 0.002), which was
significantly lower, t(18) = -37.30, p < 0.001.
   We also assessed the ability of the model to correctly
predict the next word for each current lexical category given
the sentential context. For each occurrence of the lexical
category at the input (time t), we scored the proportion of
times that the unit with highest activity in the output
corresponded to the next word in the sentence (t+1). A
higher proportion correctly predicted indicates that the
current lexical category occurs in a more predictable
context. Figure 2 shows the model’s ability to correctly
predict the words occurring after the different categories.
Consistent with our hypothesis, the model learned the
lexical category following nouns in German with greater                 Figure 3. The model’s performance on the English and
difficulty than the category following nouns in English,            German versions of Grammar 2 with finite and infinite verb
t(18) = 22.88, p < 0.001 (adjusted for multiple                                        phrases in main clauses.
comparisons), but responses for no other categories reached
significance. This is due to the subject noun occurring            interposes between the finite verb and the object noun
before the verb in main clauses (SVO), and before the object       (shown in the main clause of Sentence 4). In German, the
noun in subordinate clauses in German (SOV). The                   infinite verb is positioned at the end of the sentence
inconsistency in word order resulted in greater difficulty in      (Sentence 3). Consequently, in German, the object noun is
learning the sequential order of the language, even though         generally preceded by the finite verb. Our second
the inconsistent subordinate clause structure in German            simulations test the hypothesis that this results in greater
occurs a small proportion of the time.                             difficulty in learning the sequential order of the English
   The rare occurrence of the SOV structure in German              grammar.
would put pressure on the survival of the inconsistency, and         The grammar fragments shown in Table 2 accounted for
so we looked to a fuller grammar to see whether other              70.1% of the BNC corpus and 71.2% of the NEGRA
structures may contribute towards the preservation of the          corpus. Infinite verb phrases constitute approximately half
subordinate clause word order. We looked firstly at                of all verb phrases in the German grammar, and almost
differences in word order for finite and infinite verb phrases.    2/3rds of verb phrases in the English grammar. They
                                                                   therefore make a significant contribution towards learning
    Table 2. Grammar 2 for English and German with main            noun-verb sequential order.
  clause finite/infinite verb phrases, with proportion of each       We adapted the SRN model to fit the new grammar, with
          structure from BNC and NEGRA corpora.                    units in input/output layers for plural/singular noun,
                                                                   preposition and end of sentence marker. For the verbs, we
                                                                   included a unit that was activated whenever a verb occurred
            ENGLISH                      PROPORTION
                                                                   in the sequence. One of three other units was also active for
   S → NP VfP                      35.7%
                                                                   each verb, corresponding to whether the verb was finite and
   S → NP ViP                      64.3%
   NP → N (PP)                     60.0% (40.0%)
                                                                   singular, finite and plural, or infinite. This was to ensure that
   PP → P NP                                                       there was some overlap in the representation of each verb.
   VfP → Vf (NP) (PP)              34.3% (48.5%) (17.2%)           The model was trained and tested in exactly the same way
   ViP → Vf Vi (NP) (PP)           35.6% (40.0%) (24.5%)           as for the model trained on the first grammar. After 10
                                                                   epochs of training, MSE for English was 0.038 (sd = 0.004),
            GERMAN                                                 and 0.018 (sd = 0.005) for German.
   S → NP VfP                      50.8%
                                                                     The model trained on the German grammar learned the
   S → NP ViP                      49.2%
                                                                   conditional probabilities with greater accuracy, t(18) =
   NP → N (PP)                     70.0% (30.0%)
                                                                   10.39, p < .001. Figure 3 shows the model’s performance on
   PP → P NP
   VfP → Vf (NP) (PP)               8.5% (73.9%) (17.6%)
                                                                   predicting each category of word for Grammar 2. The
   ViP → Vf (NP) (PP) VI           10.1% (69.5%) (20.4%)           interposition of the infinite verb after the finite verb resulted
                                                                   in greater difficulty in learning for the English grammar.
                                                                   The verb-final structure of the German grammar, in
                 Finite and infinite verbs                         contrast, was learned with relative ease. Figure 3 indicates
                                                                   that this was due to the 100% predictability of the end-of-
The grammar fragments that we employed to assess the               sentence occurring after the infinite verb in German. In
learnability of sentences containing finite and infinite verb      English, the infinite verb can precede a noun, a preposition
phrases are shown in Table 2. In English, the infinite verb        or an end-of-sentence marker. The difference for infinite
                                                               819

verbs was highly significant, t(18) = -48.01, p < 0.001.
However, the difference for finite verbs was also
significantly different, with English being more predictable
than German, t(18) = 10.61, p < 0.001. This was due to the
high frequency of the infinite verb following the finite verb
in English, whereas in German the finite verb can precede a
noun, a preposition, or an infinite verb. No other
comparisons were significant. Finite verb phrases and
infinite verb phrases are learned earlier in main clauses than
word order in subordinate clauses (Clahsen & Muysken,
1986). However, children do not make word-order errors in
constructing subordinate clauses in German (Meisel &
Müller, 1992; Rothweiler, 1993). Does this verb-final
construction in infinite verb phrases assist the acquisition of
the word-order inconsistencies in the subordinate clauses in
German? The next simulation tests this question.
                                                                      Figure 4. Proportion of correct predictions for each word
    Table 3. Grammar 3 for English and German with finite         category for Grammar 3 with finite and infinite verb phrases
  and infinite verb phrases in main and subordinate clauses,                     in main and subordinate clauses.
          showing the proportions of each structure.
                                                                  according to the general proportions of structures found in
                                                                  the language corpora.
            ENGLISH                     PROPORTIONS                  After training for 10 epochs, the model that had been
   S → S S-bar                     18.6%                          exposed to the English grammar resulted in MSE = 0.028
   S → NP VfP                      35.7%                          (sd = 0.006) and the model trained on the German grammar
   S → NP ViP                      64.3%                          had MSE = 0.024 (sd = 0.004). The difference was not
   S-bar → C NP VfPsub             25.2%                          significantly different: t(18) = 1.76, p = 0.09.
   S-bar → C NP ViPsub             74.8%
                                                                     Figure 4 shows the performance of the model on
   NP → N (PP)                     75.7% (25.2%)
                                                                  predicting the word category following each word type.
   PP → P NP                       100%
                                                                  Though the model’s performance did not differ overall
   VfP → Vf (NP) (PP)              34.3% (48.5%) (17.2%)
                                                                  between the English and the German grammar, there were
   ViP → Vf Vi (NP) (PP)           35.6% (40.0%) (24.5%)
   VfPsub → Vf (NP) (PP)           59.9% (28.9%) (11.2%)          differences for the different verb types. For infinite verbs,
   ViPsub → Vf Vi (NP) (PP)        43.1% (45.5%) (27.5%)          German was easier than English, t(18) = -28.47, p < 0.001.
                                                                  For finite verbs, English was easier than German, t(18) = -
            GERMAN                                                13.67, p < 0.001. The lower predictability of words
   S → S S-bar                     18.7%                          following finite verbs in German is consistent with the
   S → NP VfP                      50.8%                          claim that case-marking is especially useful for languages
   S → NP ViP                      49.2%
                                                                  with greater variation in word order (Lupyan &
   S-bar → C NP VfPsub             45.7%
                                                                  Christiansen, 2002). No other comparisons were significant.
   S-bar → C NP ViPsub             54.3%
                                                                     Of particular note is the absence of an effect of prediction
   NP → N (PP)                     59.2% (40.8%)
   PP → P NP                       100%
                                                                  after the noun. For Grammar 1, the sequence following a
   VfP → Vf (NP) (PP)              8.5% (73.9%) (17.6%)           noun is easier to learn for English than German, but for
   ViP → Vf (NP) (PP) Vi           10.1% (69.5%) (19.0%)          Grammar 3 there is no significant difference between
   VfPsub → (NP) (PP) Vf           19.8% (69.3%) (10.9%)          English and German. We interpret this as the different word
   ViPsub → (NP) (PP) Vi Vf       (77.0%) (19.8%) 3.3%            order in subordinate clauses in German being rendered more
                                                                  easily learnable when infinite verb structures are also
                                                                  included in the grammar. This makes sense if one interprets
                                                                  the language learner acquiring one of a competing set of
   Finite/infinite and main/subordinate clauses                   grammars. The verb-final structure appears to affect
We constructed the grammar fragment of English and                performance on learning the word order of subordinate
German that included both finite and infinite verbs in main       clauses.
and subordinate clauses. The grammar is shown in Table 3.
This grammar fragment accounted for 90.8% of the German                                    Conclusion
corpus and 86.2% of the English corpus. The model was             The models indicate that, for the learning of sequential order
adapted from the previous simulation by adding a unit at the      of nouns and verbs, SVO and SOV word order
input and output layers for the complementiser. Once again,       inconsistencies are alleviated by overlapping patterns of
10 simulations of each grammar were trained on training           word ordering in other structures in the grammar. The verb-
sets of 1000 sentences that were randomly generated               final structure in German infinite verb phrases results in less
                                                              820

difference between predictions of the lexical category            Hawkins, J. A. (1990). A parsing theory of word order
following a noun for English and German. We contend that            universals. Linguisitc Inquiry, 21, 223-261.
the survival of the different word order in subordinate           Kiparsky, P. (1996): The shift to head-initial VP in
clauses in German is due, in part at least, to these verb-final     Germanic, in: Epstein, S., Thrainsson, H. & Peters, S.
constructions. The verb-final construction in subordinate           (Eds.): Studies in Comparative Syntax, Volume 2.
clauses is rendered more easily learnable as this pattern of        Dordrecht: Kluwer.
word order is more common in Grammar 3 than in                    Kroch, A. (2000). Syntactic Change, in: Baltin, M., Collins,
Grammar 1. More generally, this indicates that the                  C.: Handbook of Contemporary Syntactic Theory. Oxford:
implications for inconsistencies in structures in languages         Blackwell.
are not sufficient alone to determine whether the language        Lightfoot, D. (1991). How to Set Parameters: Arguments
will survive: the interplay between different structures must       from Language Change. Cambridge, MA: MIT Press.
be considered.                                                    Lupyan, G., Christiansen, M.H. (2002). Case, word order
   We have indicated the importance of melding corpus-              and language learnability: Insights from connectionist
based analyses with computational modelling in order to test        modeling. Proceedings of the 24th Annual Conference of
hypotheses about the contributions of different word orders         the Cognitive Science Society (pp. 220-225). Mahwah, NJ:
towards learnability of sequences in languages. Though              Lawrence Erlbaum.
simplified, the grammars we used to train the models were         MacDonald, M.C., Christiansen, M.H., (2002): Reassessing
based on real corpora. We have also indicated that fine-            Working Memory: Comment on Just and Carpenter
grained analyses of the performance of SRNs, by focusing            (1992) and Waters and Caplan (1996). Psychological
on performance by lexical category, can reveal points at            Review, 109, 35-54.
which learning sequences is improved and impaired when            Meisel, J.M. & Müller, N. (1992). Finiteness and Verb
                                                                    Placement in Early Child Grammars: Evidence from
grammars are varied.
                                                                    Simultaneous Acquisition of French and German in
                                                                    Bilinguals. In: Meisel, J. M. (Ed.). The Acquisition of Verb
                    Acknowledgments                                 Placement: Functional Categories and V2 Phenomena in
The first and third authors were supported by a Human               Language Acquisition. Dordrecht: Kluwer.
Frontiers of Science Program grant. The second and third          Plaut, D.C., McClelland, J.L., Seidenberg, M.S. &
authors were supported by European Commission grant                  Patterson, K. (1996). Understanding normal and impaired
RTN-HPRN-1999-00065.                                                 word reading: Computational principles in quasi-regular
                                                                     domains. Psychological Review, 103, 56-115.
                        References                                Rothweiler, M. (1993). Der Erwerb von Nebensätzen im
Burnard, L. (1995). Users Guide for the British National             Deutschen: Eine Pilotstudie. Tübingen: Niemeyer
   Corpus. British National Corpus Consortium, Oxford                (Linguistische Berichte, Sonderheft 3).
   University Computing Service.                                  Skut, W., Krenn, B., Brants, T. & Uszkoreit, H. (1997). An
Cable, T. & Baugh, A.C. (1993). A History of the English            annotation scheme for free word order languages.
   Language, 4th Edition. London: Routledge.                        Proceedings of the Fifth Conference on Applied Natural
Christiansen, M.H. & Chater, N. (1999). Toward a                    Language Processing. Washington, DC.
  connectionist model of recursion in human linguistic            Van Everbroeck, E. (1999). Language type frequency and
  performance. Cognitive Science, 23, 157-205.                       learnability. A connectionist approach. Proceedings of
Christiansen, M.H. & Devlin, J.T. (1997). Recursive                  the 21st Annual Conference of the Cognitive Science
  inconsistencies are hard to learn: A connectionist                 Society (pp.755-760). Mahwah, NJ: Lawrence Erlbaum.
  perspective on universal word order correlations.
  Proceedings of the 22nd Annual Conference of the
  Cognitive Science Society. (pp.113-118). Mahwah, NJ:
  Lawrence Erlbaum.
Clahsen, H. & Muysken, P. (1986). The availability of
  Universal Grammar to Adult and Child Learners. Second
  Language Research, 5, 1-29.
Cottrell, G.W. (1986). A model of lexical access of
  ambiguous words, in S.I. Small, G.W. Cottrell & M.K.
  Tanenhaus (eds.) Lexical ambiguity Resolution. San
  Mateo, CA: Morgan Kaufman, pp.179-194.
Elman, J.L. (1990). Finding Structure in Time. Cognitive
  Science, 14, 179-211.
Grüning, A. (2003). Why Verb-Initial Languages are not
  Frequent. MPI-MIS Preprint Series, 10. Max Planck
  Institute for Mathematics in the Sciences, Leipzig.
Hawkins, J. A. (1994): A Performance Theory of Order and
  Constituency. Cambridge: Cambridge University Press.
                                                              821

