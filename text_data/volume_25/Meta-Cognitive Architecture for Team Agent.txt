UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Meta-Cognitive Architecture for Team Agent
Permalink
https://escholarship.org/uc/item/7m62f1vf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Samnonovich, Alexei V.
De Jong, Kenneth A.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Meta-Cognitive Architecture for Team Agents
                                         Alexei V. Samsonovich (asamsono@gmu.edu)
                                  Krasnow Institute for Advanced Study, George Mason University
                                   4400 University Drive MS 2A1, Fairfax, VA 22030-4444 USA
                                            Kenneth A. De Jong (kdejong@gmu.edu)
                            Department of Computer Science and Krasnow Institute for Advanced Study
                          George Mason University, 4400 University Drive, Fairfax, VA 22030-4444 USA
                             Abstract                                   work we outline a means for achieving this goal by bringing
                                                                        together recent developments in cognitive science and in
  A key element of our approach is the interpretation of “self”         artificial intelligence: specifically, by relating to each other
  in a meta-cognitive sense: that is, “self” is understood as a         the results of theory of mind studies (Goldman, 2000;
  virtual character representing an agent as the subject of             Nichols & Stich, 2000), of cognitive modeling based on
  experience, as the target of attribution of experiences and
  deliberate actions performed by this agent. Thus understood,
                                                                        production systems (Soar: Newell, 1990; ACT-R: Anderson,
  “self” can be represented as an element in an agent’s                 1993; Anderson & Lebiere, 1998), intelligent agent
  cognitive system and can be used for meta-cognitive                   architectures (belief-desire-intention, or BDI: Bratman,
  processing: i.e., reasoning about one’s own self and other            1987; Ortiz, 1999; Dix et al., 2001; Sabater et al., 2002;
  selves. This general idea reflects a simulationist theory-of-         Panzarasa et al., 2002; Dragoni et al., 2002), and team
  mind viewpoint (Nichols & Stich, 2000), which is taken as the         robotics in situated contexts (Tambe, 1997) of team problem
  basis for our approach. Our model of an agent’s mind                  solving such as search-and-rescue scenarios.
  includes multiple instances of “self” representing notions of I-        The design of a meta-cognitive architecture outlined in
  Now, I-Yesterday, I-Imagine, I-Goal, etc. Each instance of
                                                                        this work allows for its implementation as software that
  “self” is represented by a “chart” with a set of properties and
  mental states attributed to it. Thus, mental states in this
                                                                        could be installed in virtual agents, as well as in mobile
  framework are representations of experiences attributed to a          robot agents, in a form that can be used by individual agents
  particular instance of “self”. This attribution further implies       for a variety of cognitive tasks requiring cooperation within
  certain rules and constraints imposed on the contents and the         a team. We argue by several example scenarios that, in
  dynamics of representations. The result is a general                  order to be successful in real life situations, the agents in the
  architecture that will enable in intelligent agents a meta-           team must possess meta-cognitive awareness of self and
  cognitive “common sense”, which proves to be vital in a               others, episodic memories, and the ability to explore
  variety of paradigms and scenarios requiring cooperation              plausible “what if” scenarios by mental simulations.
  within a team.
                                                                          Specifically, the following abilities will become available
                                                                        in agents based on the proposed architecture: (a) the ability
                         Introduction                                   to be aware of self at a meta-cognitive level, meaning
The area of cognitive teams requires a new approach for                 awareness of current self actions and mental states, goals
creating and managing multi-agent systems that can                      and intentions, of ones personal role in the global scenario,
cooperate, make joint decisions and achieve goals in a                  and the ability to evaluate own behavior; (b) the ability to
coordinated way. Such systems can include robots, virtual               understand a global picture involving multiple agents and
agents and people. The task of achieving successful                     their mental states via a simulationist theory-of-mind
performance of an ad hoc team or network of intelligent                 approach; (c) the ability to remember previous experiences,
agents requires a new technology that would enable self-                to be aware of them and to find among them relevant
awareness, meta-cognition, introspection, theory of mind,               aspects to apply to the current situation (episodic memory,
episodic memory and “what if” capabilities in agents. All               learning from personal experience); (d) the ability to explore
these abilities are closely related to the notion of a “self”.          via mental simulations “what if” scenarios and to plan
  Building artificial cognitive systems that possess a                  actions based on these simulations.
concept of “self” and a notion of “self-awareness” is a                   In addition, the proposed cognitive architecture is
difficult task, but one with tremendous practical significance          sufficiently general and powerful to support the following
and potential with the increasing interest and emphasis on              features: (e) the ability to explain own behavior and to
autonomous, agent-based systems. As we lay the                          accept directions, using human-level communications; (f)
groundwork for teams of robots, agents and people, it is                the ability to mentally simulate emotional states in order to
difficult to imagine effective team dynamics without                    better understand and serve human agents; (g) the ability to
individual team members having a sufficient sense of “self”             learn new general concepts from experience and/or
that allows for self-reflection, self-assessment, self-                 interactions with a teacher; and (h) the ability to exhibit
improvement, and understanding of others. In the present                “voluntary” meta-cognitive rational initiative in order to
                                                                   1029

improve performance via re-designing itself. The design and           An example of a mental state could be a representation of
implementation of these additional features will become            an action attributed to I-Now that is formally interpreted by
possible when, in the near future, solutions to the related        the system as an action caused by the agent at the current
lower-level problems are available, e.g., the problem of           moment of time. Another example could be a representation
natural language processing.                                       of an image attributed to I Previous, that is formally
                                                                   interpreted by the system as an image subjectively perceived
                 Technical Approach                                by the agent at a previous moment of time. Generally, a
The key element of our approach enabling the new meta-             mental state is a pair of a cognitive representation
cognitive abilities in agents is a framework in which the          (understood in a broad sense) and an instance of the agent's
concept of “self” and associated mental states are viewed          "self".
and represented as outlined below (see also Samsonovich &             All cognitive representations in this framework are
Nadel, 2003; Samsonovich & De Jong, 2003).                         created based on schemas. The term "schema" (plural
                                                                   "schemata" or "schemas") was introduced by Kant
Conceptual Framework                                               (1781/1929), and currently has an extremely broad usage in
In this framework each instance of the self is characterized       science, with different semantics in different fields. Within
by its unique mental perspective, including the identity of        computer science alone, the word has perhaps a dozen
the agent (e.g., "I"), the time stamp of the associated            different senses. An advanced theory of schemas can be
experience (the “subjective time”), the status of the instance     found in evolutionary computation (Langdon & Poli, 2002).
(e.g., actual, imaginary, remembered), the position in the         In this text, the notion of a schema is understood in a very
theory-of-mind hierarchy (Baron-Cohen, 1995; Nichols &             general sense applicable to cognition. We define a schema
Stich, 2000), and a general context in which the experience        as an abstract model or a template that can be used to
occurs (e.g., a spatial location). Instances of “self” are         instantiate and to process a certain mental category. The
labeled in this text accordingly: I-Now, I-Previous, I-Next,       categories may include all possible types of elements of the
I-Past, I-Imagined, etc. Thus, instances of the self are           subjective world: concepts and beliefs (e.g., objects,
introduced as abstract entities possessing a set of parameters     properties, events, relations), feelings, sensations,
and properties attributed to them, but lacking any internal        intentions, actions, etc.
structure or mechanisms. In general, a transition from an             We generally say that a schema has a state, when its
ordinary cognitive representation framework to a system of         instance is bound to some given content. For example,
mental states involves (i) partitioning the working memory         seeing a red circle can be described via a state of the schema
by a set of instances of the self and (ii) imposing certain        of red. Logical reasoning can be described in terms of states
constraints and rules based on this partition. Each instance       of the schemas of inference, etc. In our terminology, a state
of the self represents one mental perspective and is
                                                                   is considered “mental”, if it is attributed to a subject of
associated with a domain of the partition called here a
                                                                   experience, i.e., to a “self”. In our framework, schemas are
“chart”. The dynamics of mental states on one chart is called
                                                                   dynamical objects: they can be created and modified
here a mental simulation. In our framework, multiple charts
together with their associated mental simulations (each with       "online", and they all are represented in one universal
its own instance of “self”) may be co-active in the same           format. The entire set of schemas in a given individual
cognitive system at any moment of time. Together they              constitute that individual’s semantic memory, i.e., the
constitute working memory of the system.                           general knowledge about self and the world.
                                                                      In addition to being attributed to a particular mental
                                                                   perspective (e.g., I-Now), each mental state is characterized
                                                                   by an attitude. The word "attitude" here refers to a kind of a
                                                                   functional role of a state on a chart with respect to other
                                                                   content and the instance of the subject’s self. Examples of
                 I-Now
                                                                   attitudes are: intended, desired, believed, dreamed, recalled,
                              Mental state
    instance                                                       “my own”, somebody's, etc. In other words, the attitude
    of “self”                                                      characterizes the kind of a mental position of the subject
                                                                   with respect to the content of the mental state.
                See a
                                                                   Architecture
                 tree                         schema               The macro architecture of an individual agent includes the
                                             of seeing             following components: the input and output buffers,
                                                                   working memory, episodic memory, goal-and-plan memory,
                                               a tree
                                                                   semantic memory, and procedural memory. The input-
 experience                                                        output buffers are special charts labeled I-Input and I-
                                                                   Control, that can be formally considered as a part of the
           Figure 1: An example of a mental state.                 working memory. Episodic memory consists of selected
                                                                   previously active charts that became de-activated and stored
                                                                   in a long-term memory together with all their mental states.
                                                            1030

Goal-and-plan memory is a counterpart of the episodic
memory and is similar to it, except that it consists of
selected previously active simulations of imaginary
scenarios and goal situations, together with a certain system
of values. The semantic memory consists of a set of
schemas, and the procedural memory consists of a set of
drivers: these elements are defined below.
                                                                   Figure 3: Simplest geometrical configurations of schemas:
                                                                   an entity (a), a property (b), an event (c), and a relation (d).
                                                                    Again, states in our framework are bound instances of
                                                                 schemas. One way to represent a schema is to view it as a
                                                                 graph, the nodes of which are associated with “terms”. Each
                                                                 term represents some mental category (and therefore refers
                                                                 to a schema associated with that category). The root term of
                                                                 a schema represents the mental category associated with this
                                                                 schema itself. A simplest design of a schema is just the root
                                                                 term. Each term, either in a schema or in a state, is an object
                                                                 (in the object-oriented-programming sense) with a standard
                                                                 set of slots that specify parameters of the term, including the
                                                                 name of a mental category, a mental perspective, an attitude,
                                                                 the mode and the status of binding, etc. To bind a schema
                                                                 means to assign particular values to parameters of its terms
                                                                 (not necessarily all parameters and all terms). Generally, a
                                                                 schema specifies constraints and relations among the values
                                                                 of parameters of its terms. In addition, it may specify the
          Figure 2: Macro-architecture of the system.            order in which the terms should be bound and “side effects”
                                                                 of binding: e.g., creation of related states, schemas, etc.
                                                                    A chart can be viewed as a container in which a particular
Computational Format                                             mental simulation takes place. It also provides a label (e.g.,
From a computational point of view, we can say that our          “I-Now”) attached to all elements of this mental simulation,
notion of a schema generalizes the notion of a production in     and a domain of the system’s cognitive space with
Soar and the notion of a chunk in ACT-R: schemas and             individual locations in it understood as mental attitudes of
mental states are data structures rather than active elements.   the instance of “self” associated with the chart. In addition,
They need drivers in order to function.                          each chart is characterized by its relations to other charts
  Here by a “driver” we refer to an active object (e.g., an      and its position in the global theory-of-mind hierarchy.
executable function) that performs standard procedures of        These relations determine the rules of information exchange
processing of charts representing mental perspectives,           among charts, which is implemented based on messages. A
schemas, mental states, and the relations among them.            message is another key element of this framework. Each
Procedures are separate elements of our framework: they are      message is characterized by two mental states: the source
scripts associated with drivers that represent basic             and the target. One nice feature of this framework is that the
metacognitive skills. Drivers and procedures constitute the      same format that is used for internal messaging can be used
content of the procedural memory.                                for communications among agents in a team.
  In particular, procedural memory in this framework
includes the following drivers: Clock (chart status updating           Example Scenario of a System in Action
and subjective time flow), Predictor (probing candidates for
                                                                 Any intelligent collaboration within a team of intelligent
mental states), Scanner (binding schemas), Completer
                                                                 units requires understanding other minds. Regardless of
(executing states), Terminator (eliminating states to keep
                                                                 whether a task is to carry a heavy object by joined efforts, to
pre-set memory limits for each chart), Stimulator (goal          keep each other informed about critical, locally available
activation), and Ego, that performs a broad spectrum of          information, or to perform a joined maneuver in capturing
tasks at a meta-cognitive level (voluntary actions, mental       an enemy, it is not possible to be successful in own personal
simulations, internal conflict resolution, self-evaluation,      role without relying on the predictability of partner’s
etc.). All drivers may work in parallel. In addition, most       behavior. Therefore, members of the team must possess
drivers may exist in multiple copies working in parallel.        awareness of the partners’ internal states and cognitive
This circumstance makes the model suitable for                   abilities. Similarly, they must possess a concept of self in
implementation on parallel computers.                            order to understand their own personal role, their own
                                                                 mental state and their own cognitive abilities. One possible
                                                            1031

alternative to this scheme could be to turn local intelligence    representation (template) of the goal that they both have
off and to rely on a centralized control. However, in most        stored in their semantic memory.
scenarios typical for military operations, search and rescue         There are only two possible elementary physical actions
operations and the like, it is not always safe or even possible   that an agent can perform in this world. (1) Pick a letter and
to use centralized control of a mission performed by an ad        put it in its proper position in the virtual template, if this is
hoc, heterogeneous team of robots, software agents and            possible, and if this is not possible, hold it. (2) Pass over the
people in a hostile environment. Units of the team                bridge. Other possible (cognitive rather than physical)
(including those performing centralized control) could be         actions include the following. (3) Mentally allocate a goal
lost or damaged, and global communications could be               template in the environment. (4) Send a message to the
discontinued for a variety of reasons. In addition, there must    partner (see below). In addition, the agents can perform a
be technological and common-sense limitations on the              number of meta-cognitive actions, as described below. Plus,
frequency of long-range communications: e.g., agents
                                                                  they have a concept of a horizontal flip of any object
probably should not continuously broadcast all their video
                                                                  (including self), but cannot perform flips. All details of
and other sensory input. Therefore, intelligence in the team
                                                                  spatial navigation and letter manipulation are presumed to
must be distributed, with certain rules of subordination,
ethics, etc. Individual team members must have a means of         be implemented at a lower (automated) level and do not
local decision making based on an understanding of the            enter the agents’ minds (e.g., agents do not have a concept
minds of their partners and themselves. In addition, they         of spatial coordinates). The time is discrete, and an agent
must be able to learn from personal experience, to reason         can perform at most one physical action at a moment of
about possible future scenarios involving the team, and to        time.
quickly and robustly respond to surprises. In the following
example scenario we demonstrate why these features may
become vital, and why their “traditional” implementation,
e.g., based on mathematical logic, may not be acceptable.
   Consider a possible scenario in which two agents that
perform a collaborative surveillance task capture an
intruder. They get him to cooperate using verbal commands
and warning gun shots. Suppose that, after a warning shot
given by chance simultaneously by both agents, the intruder
falls down and shows no signs of life. An immediate
account of this event given by each agent could be that their
partner killed the intruder. Few seconds later, another
intruder opens fire on the agents from the opposite side. The
agents have to turn around and to respond with their guns.
At this moment the first intruder throws a hand grenade that
destroys them both. This would not happen if one of the                   Figure 4: A virtual reality simulation example.
agents kept an eye on the first intruder considering the
possibility that he was still alive. Robots capable of logical       Agents communicate with each other by transmitting any
reasoning may not succeed in this story, given that they          number of selected own mental states to the partner. This is
have virtually no time for extensive reasoning or for             done mostly automatically, when an agent intends to act or
communications. In order to succeed, the agents need to           becomes aware of something new. In addition, all new
understand each other and to be able to think in terms of a       schemas (e.g., hypotheses) are shared instantly. Transmitted
pretend-play schema attributed to the intruder.                   mental states get represented in the partner’s mind as mental
                                                                  states attributed to the sender. Thus, communications
              And the Miracle Happens…                            amount to direct copying of internal representations of
In order to be more specific, and to demonstrate the              mental states: this simple choice implies absolute trust and
advantages of the proposed meta-cognitive architecture in a       absolute sincerity in the “relationships” among the agents.
trivial virtual reality paradigm, we present a simple                Agents do not possess advanced built-in reasoning
computer simulation result. The paradigm is that two agents,      abilities and follow simple rules. At each step under normal
Circle and Triangle, are placed in a rectangular area, which      conditions, an agent imagines possible actions (i.e.,
has a bridge attached to it (Figure 4 a). The bridge is           generates ideas), simulates expected results of imaginary
implemented as a twisted belt, a part of a Möbius strip,          actions, and checks whether the idea of the first move is
which makes the entire manifold non-orientable. There are         good in the sense that it results in implementing goal
seven letters randomly allocated in the area: M, I, _, A, C,      elements. A good idea is accepted as intent, which is
L, and E. The agents can see and recognize letters, including     communicated to the partner and then executed, if there are
their orientation (one letter, _, is mirror-reflected). Agents    no conflicts. When there are no good ideas, agents try any
cannot flip letters. The task for the team is to spell the word   possible actions provided they do not destroy implemented
“MIRACLE”, with all letters upright, relying on a mental          goal elements. This strategy is selected here for its
                                                             1032

simplicity and may not work in a more general case; still,          until it imagined its own flip. In order to do this, the agent
the main interest in this simulation study was not in               took a third-person perspective I Meta and considered its
cognitive, but in meta-cognitive dynamics of the system,            current instance of self “from an outside”. It could take a
which we describe next discussing a conflict situation as an        substantial amount of traditional logical reasoning to do the
example.                                                            same (e.g., questioning the semantics of many agent’s
   A conflict situation emerges when internal mental                mental representations would be necessary, and the analysis
representations of an agent become inconsistent with each           would be difficult to conduct within the same mental
other: e.g., the expected and the actual experience do not          perspective). On the other hand, an elementary meta-
match. In this case an agent uses its episodic memory, trying       cognitive act allows the agent to see immediately “what is it
to account for the mismatch. It attempts imagining all              like to be flipped” and to compare this vision to the current
possible unknown events that could happen in the                    experience.
environment recently (in our oversimplified world meaning              The particular observed outcome was not “pre-
flips of anything, when the agent was passing the bridge).          programmed” and was not the only possible course of
Next, given a parsimonious account of this sort, the agent          action. For example, given the same initial scenario
starts making hypotheses as to the possible causes of the           (including the first message), it would be likely for Triangle
unknown events, trying to interpret the latter as side effects      to guess independently that Circle got flipped, and to come
of known physical events. During this process, agents may           up with an equivalent hypothesis first.
simulate own and each others’ minds, following the
principle of parsimony: simplest hypotheses are explored
first. When a hypothesis about a side effect is found that                  Connections to Modern ‘Hot Topics’
resolves the conflict, it is accepted as a new semantic
knowledge (schema) that yet has to be tested behaviorally.          Cognitive Psychology and Philosophy of Mind
Based on the new knowledge, agents may change the status
of their present and past mental states (e.g., I-Now may            Currently there are two main competing points of view on
                                                                    human theory of mind: “theory-theorist” and “simulationist”
become I-False-Belief, and I-Meta, where the analysis was
                                                                    (Goldman, 2000). The former assumes that people represent
performed from a “third-person perspective”, may take the
                                                                    mental states in themselves and in others by making
position of I-Now, etc.). In addition, the agents in a case like    inferences from common-sense concepts, while the latter
this are likely to revise their intents, scenarios and, possibly,   assumes that people use their first-hand experience to
goals. Finally, the new knowledge is used in the normal             understand other minds, in other words, perform “mental
process of planning.                                                simulation”.
   This model was implemented and simulated in a                       Our framework viewed as a model of the human theory of
computer, producing the result shown in Figure 4 b. The             mind falls into the simulationist camp. Perhaps, the closest
agents were acting in turn. First, Circle allocated a virtual       to it is the framework proposed by Nichols and Stich
template for the goal, which was accepted and shared by             (2000). Specifically, their notion of a Possible World Box
Triangle. Then agents started filling the template with             (PWB) is similar to our notion of a chart, although PWB
available letters. Because the letter ‘R’ was not available,        does not represent an instance of a self, and therefore does
Circle decided to cross the bridge. This immediately                not explicitly provide a means of separating mental states
resulted in a conflict situation, successfully resolved by the      based on their attribution to different instances of the
system, as it follows from the following output that                subject’s self. Other elements of the model of Nichols and
represents the content of two consecutive messages sent by          Stich (2000) also can be mapped onto our framework: e.g.,
Circle to Triangle:                                                 their UpDater can be related to a subset of our drivers.
                                                                       Remarkably, Nichols and Stich emphasize the advantage
   Message 1: I am surprised to see R upright, and C, L and         of the anthropomorphic approach to self-monitoring over
E flipped.                                                          logical reasoning: “When normal adults believe that p, they
   Message 2: As a parsimonious explanation, I suspect my           can quickly and accurately form the belief I believe that p;
own flip that happened when I was passing over the bridge.          when normal adults desire that p, they can quickly and
I hypothesize that the bridge causes flipping.                      accurately form the belief I desire that p; and so on for the
                                                                    rest of the propositional attitudes. In order to implement this
   After this discovery, the hypothesis together with the pre-      ability, no sophisticated Theory of Mind is required”
existing concept of a flip was used in planning by both             (Nichols & Stich, 2003).
agents, and the goal was quickly achieved (Figure 4 b).                In addition, the proposed framework can be used in the
   This simulation result clearly demonstrates that the agent       field of cognitive psychology of neurological disorders to
was able to solve a nontrivial puzzle with a single meta-           give an account to major agency disorders, including
cognitive act of imagining its own flip that could take place       various forms of hippocampal amnesia, various aspects of
in the past. After detecting a conflict between its internal        schizophrenia, multiple personality, PTSD, and autism
representations, Circle started imagining all possibilities,        (Samsonovich & Nadel, 2003). This topic, however, is
including various flip events that could happen recently,           beyond the scope of the present paper.
                                                               1033

Artificial Intelligence                                              than provide an immediate account of human cognition,
During recent years a tremendous progress has been made in           however, that future theory, when found, might eventually
several fields related to intelligent agents possessing meta-        help us to understand our own mind.
cognitive abilities (e.g., Panzarasa et al., 2002). These fields
primarily consist of logical foundations of artificial                                        References
intelligence and of practical approaches based on production         Anderson, J.R. (1993) Rules of the mind. Hillsdale, NJ:
(or rule-based) systems. The state-of-the-art intelligent agent          Lawrence Erlbaum Associates.
architecture based on the BDI framework (Bratman, 1987)              Anderson, J.R., & Lebiere, C. (1998) The atomic
and its variations allows for implementing a theory of mind              components of thought. Mahwah, NJ: Lawrence
in an agent using logical reasoning. An implementation of                Erlbaum Associates.
this sort would fall into the “theory-theorist” division (see        Baron-Cohen, S. (1995) Mindblindness: An essay on autism
above). It would not, however, provide a natural way of                  and theory of mind. Cambridge, Massachussets:
solving unexpected real-life situations like the examples                Bradford Book, MIT Press.
considered above.                                                    Bratman, M.E. (1987) Intentions, Plans, and Practical
   In addition, unlike its well-known analogs, e.g., Soar                Reason. Harvard University Press: Cambridge, MA.
(e.g., Laird et al., 1987) and ACT-R (Anderson & Lebiere,            Dix, J., Kraus, S., & Subrahmanian, V.S. (2001) Temporal
1998), the system that we propose to build possesses                     agent programs. Artificial Intelligence 127: 87-135.
universality and an unlimited potential of development and           Dragoni, A.F., Giorgini, P., & Serafini, L. (2002) Mental
generalization on the part of its meta-cognitive abilities.              states recognition from communication. Journal of
                                                                         Logic and Computation 12 (1): 119-136.
         Discussion of Further Perspectives                          Goldman, A. (2000) Folk psychology and mental concepts.
We expect the following impact of the proposed architecture              Protosociology 14: 4-25.
on the team agent technology and beyond. (1) The software            Kant, I. (1781/1929) Critique of pure reason. Translated by
agent architecture will be used in teams of mobile robots.               N. K. Smith. New York: St. Martin's Press.
(2) The new technology of meta-cognitive systems will                Langdon, W.B., & Poli, R. (2002) Foundations of genetic
become widely practically available. (3) The results will                programming. Berlin: Springer.
open a broad field of new possibilities in research, allowing        Laird, J.E., Newell, A., & Rosenbloom, P. (1987) Soar: an
for a "quantum leap" from the existing state-of-the-art                  architecture for general intelligence. Artificial
technologies (Soar, ACT-R and BDI agents). The set of                    Intelligence 33: 1-64.
potentially available agent abilities will include the ability to    Nichols, S., & Stich, S. (2000) A cognitive theory of
explain own behavior and to accept directions, using                     pretense. Cognition 74: 115-147.
human-level communications, the ability to mentally                  Nichols, S., and Stich, S. (2003) Reading one's own mind: A
simulate emotional states in order to better understand and              cognitive theory of self-awareness. In Smith, Q., and
serve human agents, the ability to learn new general                     Jokic, A. (Eds.), Aspects of Consciousness (in press),
concepts from experience and/or interactions with a teacher,             Oxford, UK: Oxford University Press.
and the ability to exhibit meta-cognitive rational initiative        Ortiz, C.L. (1999) Introspective and elaborative processes in
and to improve performance via re-designing itself. (4) The              rational agents. Annals of Mathematics and Artificial
theoretical model of a meta-cognitive system will be related             Intelligence 25: 1-34.
to human cognition and to the functional organization of the         Panzarasa, P., Jennings, N.R., & Norman, T.J. (2002)
human brain, thus resulting in a qualitatively new cognitive-
                                                                         Formalizing collaborative decision-making and
psychological model. Mapping of model components onto
                                                                         practical reasoning in multi-agent systems. Journal of
the neuro-anatomical organization of the brain will allow for
a model-based interpretation of “mysterious” neurological                Logic and Computation 12 (1): 55-117.
disorders, as well as for a better understanding of a normal         Sabater, J., Sierra, C., Parsons, S., & Jennings, N.R. (2002)
state of the human mind. (5) Finally, a field of                         Engineering executable agents using multi-context
computational consciousness will be brought to existence.                systems. Journal of Logic and Computation 12 (3): 413-
   Despite many recent speculations, the field of                        442.
computational consciousness has not been born yet. In our            Samsonovich, A.V., & Nadel, L. (2003) Fundamental
view, the idea behind its likely origin is that today we may             Principles and Mechanisms of the Conscious Self.
be in a position to create a new object of study for such                Cortex: Special Issue on Brain, Mind and
abstract disciplines as philosophy of mind and psychology                Consciousness (under review).
of higher cognitive functions. Rather than creating a                Samsonovich, A.V., & De Jong, K.A. (2003) Definition of a
computer simulation of another abstract and oversimplified               computational framework for meta-cognitive systems.
cognitive-psychological model of human mind, our present                 Cortex: Special Issue on Brain, Mind and
ambition is to create a virtual entity emulating human mind              Consciousness (under review).
in its most essential abilities, an entity that by itself might be   Tambe, M. (1997) Towards flexible teamwork. Journal of
of a great scientific and practical interest for us. This entity         Artificial Intelligence Research 7: 83-124.
might further require its own theoretical explanation rather
                                                                1034

