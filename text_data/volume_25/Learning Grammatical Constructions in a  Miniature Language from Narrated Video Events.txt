UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Grammatical Constructions in a Miniature Language from Narrated Video
Events
Permalink
https://escholarship.org/uc/item/51v3c2dn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Author
Dominey, Peter Ford
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                              Learning Grammatical Constructions in a
                        Miniature Language from Narrated Video Events
                                     Peter Ford Dominey (dominey@ isc.cnrs.fr)
                                           Institut des Sciences Cognitives, CNRS
                                         67 Blvd. Pinel, 69675 Bron Cedex, France
                          Abstract                                recognize events including pick-up, put-down, and
                                                                  stack based on their characterization in an event logic.
  The objective of this research is to develop a system for       The use of these intermediate representations renders
  miniature language learning based on a minimum of pre-
                                                                  the system robust to variability in motion and view
  wired language-specific functionality, that is compatible
  with observations of perceptual and language capabilities       parameters. Most importantly, Siskind demonstrated
  in human development.         In the proposed system,           that the lexical semantics for a number of verbs could
  meaning is extracted from video images based on                 be established by automatic image processing.
  detection of physical contact and its parameters.
  Mapping of sentence form to meaning is performed by             Sentence to meaning mapping:
  learning grammatical constructions that are retrieved
  from a construction inventory based on the constellation           Once meaning is extracted from the scene, the
  of closed class items uniquely identifying the target           significant problem of mapping sentences to meanings
  sentence structure. The resulting system displays robust        remains. The nativist perspective on this problem
  acquisition behavior that reproduces certain observations       holds that the <sentence, meaning> data to which the
  from developmental studies, with very modest “innate”           child is exposed is highly indeterminate, and
  language specificity.
                                                                  underspecifies the mapping to be learned.           This
                                                                  “poverty of the stimulus” is a central argument for the
                      Introduction                                existence of a genetically specified universal grammar,
   Feldman et al. (1990) posed the problem of                     such that language acquisition consists of configuring
"miniature" language acquisition based on <sentence,              the UG for the appropriate target language (Chomsky
image> pairs as a "touchstone" for cognitive science.             1995). In this framework, once a given parameter is
In this task, an artificial system is confronted with a           set, its use should apply to new constructions in a
reduced version of the problem of language acquisition            generalized, generative manner.
faced by the child, that involves both the extraction of             An alternative functionalist perspective holds that
meaning from the image, and the mapping of the paired             learning plays a much more central role in language
sentence onto this meaning.                                       acquisition. The infant develops an inventory of
                                                                  grammatical constructions as mappings from form to
Extraction of Meaning                                             meaning (Goldberg 1995). These constructions are
   In this developmental context, Mandler (1999)                  initially rather fixed and specific, and later become
suggested that the infant begins to construct meaning             generalized into a more abstract compositional form
from the scene based on the extraction of perceptual              employed by the adult (Tomasello 1999). In this
primitives.     From simple representations such as               context, construction of the relation between perceptual
contact, support, attachment (Talmy 1988) the infant              and cognitive representations and grammatical form
could construct progressively more elaborate                      plays a central role in learning language (e.g. Feldman
representations of visuospatial meaning. Thus, the                et al. 1990, 1996; Langacker 1991; Mandler 1999;
physical event "collision" is a form of the perceptual            Talmy 1998).
primitive “contact”. Kotovsky & Baillargeon (1998)                   These issues of learnability and innateness have
observed that at 6 months, infants demonstrate                    provided a rich motivation for simulation studies that
sensitivity to the parameters of objects involved in a            have taken a number of different forms. Elman (1990)
collision, and the resulting effect on the collision,             demonstrated that recurrent networks are sensitive to
suggesting indeed that infants can represent contact as           predictable structure in grammatical sequences.
an event predicate with agent and patient arguments.              Subsequent studies of grammar induction demonstrate
   Siskind (2001) has demonstrated that force dynamic             how syntactic structure can be recovered from
primitives of contact, support, attachment can be                 sentences (e.g. Stolcke & Omohundro 1994). From the
extracted from video event sequences and used to                  “grounding of language in meaning” perspective (e.g.
                                                            354

Feldman et al. 1990, 1996; Langacker 1991; Goldberg        vision data were acquired and then processed off-line
1995), Chang & Maia (2001) exploited the relations         yielding a data set of matched sentence – scene pairs
between action representation and simple verb frames       that were provided as input to the structure mapping
in a construction grammar approach, and Cottrell et al.    model. A total of ~300 <sentence, scene> pairs were
(1990) associated sequences of words with simple           tested in the following experiments.
image sequences. In effort to consider more complex
grammatical forms, Miikkulainen (1996) demonstrated                    Visual Scenes and analysis
a system that learned the mapping between relative            For a given video sequence the visual scene analysis
phrase constructions and multiple event representations,   generates the corresponding event description in the
based on the use of a stack for maintaining state          format event(agent, object, recipient).
information during the processing of the next embedded
clause in a recursive manner.                              Single Event Labeling
   In a more generalized approach, Dominey (2000)             Events are defined in terms of contacts between
exploited the regularity that sentence to meaning          elements. A contact is defined in terms of the time at
mapping is encoded in all languages by word order and      which it occurred, the agent, object, and duration of the
grammatical marking (bound or free) (Bates et al.          contact. The agent is determined as the element that
1982). That model was based on the functional              had a larger relative velocity towards the other element
neurophysiology of cognitive sequence and language         involved in the contact. Based on these parameters of
processing and an associated neural network model that     contact, scene events are recognized as follows:
has been demonstrated to simulate interesting aspects of      Touch(agent, object): A single contact, in which (a)
infant (Dominey & Ramus 2000) and adult language           the duration of the contact is inferior to touch_duration
processing (Dominey et al. 2003).                          (1.5 seconds), and (b) the object is not displaced during
                                                           the duration of the contact.
                                                              Push(agent, object): Similar to touch, with a greater
                      Objectives                           contact duration, superior or equal to touch_duration
   The goals of the current study are fourfold: First to   and inferior to take_duration (5 sec), and object
test the hypothesis that meaning can be extracted from     displacement.
visual scenes based on the detection of contact and its       Take(agent, object): A single contact in which (a)
parameters in an approach similar to but significantly     the duration of contact is superior or equal to
simplified from Siskind (2001); Second to determine        take_duration, (b) the object is displaced during the
whether the model of Dominey (2000) can be extended        contact, and (c) the agent and object remain in contact.
to handle embedded relative clauses; Third to                 Take(agent, object, source): Multiple contacts, as
demonstrate that these two systems can be combined to      the agent takes the object from the source. Same as
perform miniature language acquisition; and finally to     Take(a,o), and for the optional second contact between
demonstrate that the combined system can provide           agent and source (a) the duration of the contact is
insight into the developmental progression in human        inferior to take_duration, and (b) the agent and source
language acquisition without the necessity of a pre-       do not remain in contact. Finally, contact between the
wired parameterized grammar system (Chomsky 1995).         object and source is broken during the event.
                                                              Give(agent, object, recipient): Multiple contacts as
                                                           agent takes object, then initiates contact between object
                 The Training Data                         and recipient.
   The human experimenter enacts and simultaneously           These event labeling templates form the basis for a
narrates visual scenes made up of events that occur        template matching algorithm that labels events based on
between a red cylinder, a green block and a blue           the contact list, similar to the spanning interval and
semicircle or “moon” on a black matte table surface. A     event logic of Siskind (2001).
video camera above the surface provides a video image         Complex “Hierarchical” Events: The events
that is processed by a color-based recognition and         described above are simple in the sense that there have
tracking system (Smart – Panlab, Barcelona Spain) that     no hierarchical structure.        This imposes serious
generates a time ordered sequence of the contacts that     limitations on the syntactic complexity of the
occur between objects that is subsequently processed       corresponding sentences (Feldman et al. 1996,
for event analysis (below). The simultaneous narration     Miikkulainen 1996). The sentence “The block that
of the ongoing events is processed by a commercial         pushed the moon was touched by the triangle”
speech-to-text system (IBM ViaVoiceTM). Speech and         illustrates a complex event that exemplifies this issue.
                                                       355

The corresponding compound event will be recognized          Referents Array (PRA). PRA elements are mapped onto
and represented as a pair of temporally successive           their roles in the Scene Event Array (SEA) by the
simple event descriptions, in this case: push(block,         FormToMeaning mapping, specific to each sentence
moon), and touch(triangle, block). The “block” serves        type. This mapping is retrieved from Construction
as the link that connects these two simple events in         Inventory, via the ConstructionIndex that encodes the
order to form a complex hierarchical event.                  closed class words that characterize each sentence type.
  Structure mapping for language learning                    Word Meaning
   Our approach is based on the cross-linguistic                In the initial learning phases there is no influence of
observation that open class words (e.g. nouns, verbs,        syntactic knowledge and the word-referent associations
adjectives and adverbs) are assigned to their thematic       are stored in the WordToReferent matrix (Eqn 1) by
roles based on word order and/or grammatical function        associating every word with every referent in the
words or morphemes (Bates et al. 1982). The mapping          current scene (α =1), exploiting the cross-situational
of sentence form onto meaning (Goldberg 1995) takes          regularity (Siskind 1996) that a given word will have a
place at two distinct levels: Words are associated with      higher coincidence with referent to which it refers than
individual components of event descriptions, and             with other referents. This initial word learning
grammatical structure is associated with functional          contributes to learning the mapping between sentence
roles within scene events (Fig 1). The first level has       and scene structure (Eqn. 4, 5 & 6 below). Then,
been addressed by Siskind (1996), Roy & Pentland             knowledge of the syntactic structure, encoded in
(2000) and Steels (2001) and we treat it here in a           FormToMeaning can be used to identify the appropriate
relatively simple but effective manner. Our principle        referent (in the SEA) for a given word (in the OCA),
interest lies more in the second level of mapping            corresponding to a zero value of α in Eqn. 1. In this
between scene and sentence structure.                        “syntactic bootstrapping” for the new word “gugle,” for
                                                             example, syntactic knowledge of Agent-Event-Object
                                                             structure of the sentence “John pushed the gugle” can
              FormToMeaning      Construction Inventory      be used to assign “gugle” to the object of push.
                                                             WordToReferent(i,j) = WordToReferent(i,j) +
                        WordToReferent                       OCA(k,i) * SEA(m,j) *
                                                             Max(α, FormToMeaning(m,k))                            (1)
           Predicted
           Referents                                         Indices: k(1:6) - words; m(1:6) – scene elements;
           Array (PRA)
                                            Construction     i(1:25), j(1:25) – elements in word and scene item
                                            Index            vectors, respectively.
                Action
                Agent
                Object                                       Open vs Closed Class Word Categories
                Recipient  Open Class       Closed
                           Array (OCA)      class               Newborn infants are sensitive to the perceptual
  Scene Event Array                         words            properties that distinguish these two categories (Shi et
  (SEA)                                                      al. 1999), and in adults, these categories are processed
                                                             by dissociable neurophysiological systems (Brown et al.
          Visual Scene       Speech Input
            Analysis         Processing                      1999). Similarly, artificial neural networks can also
                                                             learn to make this function/content distinction (Morgan
                                                             et al. 1996, Blanc et al. 2003). Thus, for the speech
Figure 1. Structure-Mapping Architecture.
                                                             input that is provided to the learning model open and
                                                             closed class words are directed to separate processing
   Model Overview: Words in sentences, and elements
                                                             streams that preserve their order and identity, as
in the scene are coded as single ON bits in respective
                                                             indicated in Figure 1.
25-element vectors. On input, Open class words
populate the Open Class Array (OCA), and closed class
                                                             Mapping Sentence to Meaning
words populate the Construction index. Visual Scene
Analysis populates the Scene Event Array (SEA) with             In terms of the architecture in Figure 1, this mapping
the extracted meaning as scene elements. Words in            can be characterized in the following successive steps.
OCA are translated to Predicted Referents via the            First, words in the Open Class Array are decoded into
WordToReferent mapping to populate the Predicted             their corresponding scene referents (via the
                                                         356

WordToReferent mapping) to yield the Predicted                    Finally, once this learning has occurred, for new
Referents Array that contains the translated words while      sentences we can now extract the FormToMeaning
preserving their original order from the OCA (Eqn 2).         mapping from the learned ConstructionInventory by
                                                              using the ConstructionIndex as an index into this
                 n                                            associative memory, illustrated in Eqn. 6.
 PRA(m,j) =    ∑ OCA(m,i) * WordToReferent(i,j)        (2)
               i =1                                            FormToMeaning(i) =
   Next, each sentence type will correspond to a specific         n
form to meaning mapping between the PRA and the
SEA. encoded in the FormToMeaning array. The
                                                                ∑ ConstructionInventory(i,j) * ConstructinIndex(j)
                                                                 i=1
problem will be to retrieve for each sentence type, the                                                            (6)
appropriate corresponding FormToMeaning mapping.
To solve this problem, we recall that each sentence type          To accommodate the dual scenes for complex events
will have a unique constellation of closed class words        Eqns. 4-7 are instantiated twice each, to represent the
and/or bound morphemes (Bates et al. 1982) that can be        two components of the dual scene. In the case of
coded in a ConstructionIndex (Eqn.3) that forms a             simple scenes, the second component of the dual scene
unique identifier for each sentence type, shifting the        representation is null.
current contents by the index of the ON bit in                    We evaluate performance by using the
FunctionWord, then ANDing the FunctionWord vector.            WordToReferent and FormToMeaning knowledge to
The appropriate FormToMeaning mapping for each                construct for a given input sentence the “predicted
sentence type can be indexed in ConstructionInventory         scene”. That is, the model will construct an internal
by its corresponding ConstructionIndex.                       representation of the scene that should correspond to
                                                              the input sentence. This is achieved by first converting
ConstructionIndex = fcircularShift(ConstructionIndex,         the Open-Class-Array into its corresponding scene
 FunctionWord)                                          (3)   items in the Predicted-Referents-Array as specified in
                                                              Eqn. 2. The referents are then re-ordered into the
   The link between the ConstructionIndex and the             proper scene representation via application of the
corresponding FormToMeaning mapping is established            FormToMeaning transformation as described in Eqn. 7.
as follows. As each new sentence is processed, we first
reconstruct the specific FormToMeaning mapping for            PSA(m,i) = PRA(k,i) * FormToMeaning(m,k)             (7)
that sentence (Eqn 4), by mapping words to referents
(in PRA) and referents to scene elements (in SEA). The            When learning has proceeded correctly, the predicted
resulting, FormToMeaningCurrent encodes the                   scene array (PSA) contents should match those of the
correspondence between word order (that is preserved          scene event array (SEA) that is directly derived from
in the PRA Eqn 2) and thematic roles in the SEA. Note         input to the model. We then quantify performance
that the quality of FormToMeaningCurrent will depend          error in terms of the number of mismatches between
on the quality of acquired word meanings in                   PSA and SEA.
WordToReferent. Thus, syntactic learning requires a
minimum baseline of semantic knowledge. Given the                             Experimental results
FormToMeaningCurrent mapping for the current                      Hirsh-Pasek & Golinkoff (1996) indicate that
sentence, we can now associate it in the                      children use knowledge of word meaning to acquire a
ConstructionInventory with the corresponding function         fixed SVO template around 18 months, then expand this
word configuration or ConstructionIndex for that              to non-canonical sentence forms around 24+ months.
sentence, expressed in (Eqn 5).            In Eqns 5, 6       Tomasello (1999) indicates that fixed grammatical
FormToMeaning is linearized for simplification.               constructions will be used initially, and that these will
                                                              then provide the basis for the development of more
 FormToMeaningCurrent(m,k) =                                  generalized constructions (Goldberg 1995).            The
            n                                         (4)     following experiments attempt to follow this type of
          ∑ PRA(k,i)*SEA(m,i)                                 developmental progression. Training results in changes
           i=1
                                                              in the associative WordToReferent mappings encoding
                                                              the lexicon, and changes in the ConstructionInventory
ConstructionInventory(i,j) = ConstructionInventory(i,j)       encoding the form to meaning mappings, indexed by
+ ConstructionIndex(i)                                        the ConstructionIndex.
* FormToMeaningCurrent(j)                             (5)
                                                          357

A. Learning of Active Forms for Simple Events               8.    The block pushed the triangle that was touched the
                                                                  moon.
1.   Active: The block pushed the triangle.                 9. The block that was pushed by the triangle touched
2.   Dative: The block gave the triangle to the moon.             the moon.
                                                            10. The block was pushed by the triangle that touched
   For this experiment, 17 scene/sentence pairs were              the moon.
generated that employed the 5 different events, and
narrations in the active voice, corresponding to the           After presentation of 88 scene/sentence pairs, the
grammatical forms 1 and 2. The model was trained for        model performed without error for these 6 grammatical
32 passes through the 17 scene/sentence pairs for a total   forms, and displayed error-free generalization to new
of 544 scene/sentence pairs. During the first 200           sentences that had not been used during the training for
scene/sentence pair trials, α in Eqn. 1 was 1 (i.e. no      all six grammatical forms.
syntactic bootstrapping before syntax is acquired), and
thereafter it was 0. This was necessary in order to avoid   D. Combined Test with and Without Lexicon
the random effect of syntactic knowledge on semantic           A total of 27 scene/sentence pairs, used in
learning in the initial learning stages. The trained        Experiments B and C, were employed that exercised the
system displayed error free performance for all 17          ensemble of grammatical forms 1 – 10 using the learned
sentences, and generalization to new sentences that had     WordToReferent mappings. After exposure to 162
not previously been tested.                                 scene/sentence pairs the model performed and
                                                            generalized without error. When this combined test was
B. Passive forms                                            performed without the pre-learned lexical mappings in
   This experiment examined learning active and             WordToReferent, the system failed to converge,
passive grammatical forms, employing grammatical            illustrating the advantage of following the
forms 1-4. Word meanings were used from Experiment          developmental progression from lexicon to simple to
A, so only the structural FormToMeaning mappings            complex grammatical structure.
were learned.
                                                            E. Some Scaling Issues
3.   Passive: The triangle was pushed by the block.            A small lexicon (n<25) and construction inventory
4.   Dative Passive: The moon was given to the              (n=10) are used, as the objective was to demonstrate the
     triangle by the block.                                 integrated system and grammatical structure learning
                                                            capability. Based on the independent word and
    Seventeen new scene/sentence pairs were generated       construction representations, and their synergistic
with active and passive grammatical forms for the           interaction, the architecture scales well. The model is
narration. Within 3 training passes through the 17          being tested with a larger lexicon, and has learned over
sentences (51 scene/sentence pairs), error free             40 grammatical constructions. Importantly, the system
performance was achieved, with confirmation of error        should extend to all languages in which sentence to
free generalization to new untrained sentences of these     meaning mapping is encoded by word order and/or
types. The rapid learning indicates the importance of       grammatical marking (Bates et al. 1982). In the current
lexicon in establishing the form to meaning mapping         study, deliberate human event production yielded
for the grammatical constructions.                          essentially perfect recognition, though the learning
                                                            model is relatively robust to elevated scene error rates
C. Relative forms for Complex Events                        as documented in Dominey (2000).
  Here we consider complex scenes narrated by relative
clause sentences. Eleven complex scene/sentence pairs                            Conclusion
were generated with narration corresponding to the              The current study demonstrates (1) that the
grammatical forms indicated in 5 – 10:                      perceptual primitive of contact (available to infants at 5
                                                            months), can be used to perform event description in a
5.   The block that pushed the triangle touched the         manner that is similar to but significantly simpler than
     moon.                                                  Siskind (2001), (2) that a novel implementation of
6.   The block pushed the triangle that touched the         principles from construction grammar can be used to
     moon.                                                  map sentence form to these meanings together in an
7.   The block that pushed the triangle was touched by      integrated system, (3) that relative clauses can be
     the moon.                                              processed in a manner that is similar to, but requires
                                                        358

less specific machinery (e.g. no stack) than that in         Feldman JA, Lakoff G, Stolcke A, Weber SH (1990)
Miikkulainen (1996), and finally (4) that the resulting        Miniature language acquisition: A touchstone for
system displays robust acquisition behavior that               cognitive science. In Proceedings of the 12th Ann
reproduces certain observations from developmental             Conf. Cog. Sci. Soc. 686-693, MIT, Cambridge MA
studies with very modest “innate” language specificity.      Feldman J., G. Lakoff, D. Bailey, S. Narayanan, T.
    The goal was to identify minimal event recognition         Regier, A. Stolcke (1996). L0: The First Five Years.
and form-to-meaning mapping capabilities that could be         Artificial Intelligence Review, v10 103-129.
integrated into a coherent system that performs at the       Goldberg A (1995) Constructions. U Chicago Press,
level of a human infant in the first years of                  Chicago and London.
                                                             Hirsh-Pasek K, Golinkof RM (1996) The origins of
development when the construction inventory is being
                                                               grammar:        evidence     from    early   language
built up. This forms the basis for the infant’s
                                                               comprehension. MIT Press, Boston.
subsequent ability to de- and re-compose these
                                                             Kotovsky L, Baillargeon R, The development of
constructions in a truly compositional manner, a topic         calibration-based reasoning about collision events in
of future research.                                            young infants. 1998, Cognition, 67, 311-351
                                                             Langacker, R. (1991). Foundations of Cognitive
                 Acknowledgments                               Grammar. Practical Applications, Volume 2. Stanford
    Supported by the OHLL, EuroCores OMLL, French              University Press, Stanford.
ACI Integrative and Computational Neuroscience, and          Mandler J (1999) Preverbal representations and
HFSP MCILA Projects.                                           language, in P. Bloom, MA Peterson, L Nadel and
                                                               MF Garrett (Eds) Language and Space, MIT Press,
                     References                                365-384
                                                             Miikkulainen R (1996) Subsymbolic case-role analysis
Bates E, McNew S, MacWhinney B, Devescovi A,                   of sentences with embedded clauses. Cognitive
  Smith S (1982) Functional constraints on sentence            Science, 20:47-73.
  processing: A cross linguistic study, Cognition (11)       Morgan JL, Shi R, Allopenna P (1996) Perceptual bases
  245-299.                                                     of rudimentary grammatical categories, pp 263-286,
Blanc JM, Dodane C, Dominey PF (2003) Temporal                 in Morgan JL, Demuth K (Eds) Signal to syntax,
  processing for syntax acquisition: A Simulation              Lawrence Erlbaum, Mahwah NJ, USA.
  Study, In Press, Proceedings of the 25th Ann Conf.         Roy D, Pentland A (2002). Learning Words from Sights
  Cog. Sci. Soc., MIT, Cambridge MA                            and Sounds: A Computational Model. Cognitive
Brown CM, Hagoort P, ter Keurs M (1999)                        Science, 26(1), 113-146.
  Electrophysiological signatures of visual lexical          Shi R., Werker J.F., Morgan J.L. (1999) Newborn
  processing : Open- and closed-class words. Journal           infants' sensitivity to perceptual cues to lexical and
  of Cognitive Neuroscience. 11 :3, 261-281                    grammatical words, Cognition, Volume 72, Issue 2,
Chomsky N. (1995) The Minimalist Program. MIT                  B11-B21.
Chang NC, Maia TV (2001) Grounded learning of                Siskind JM (1996) A computational study of cross-
  grammatical constructions, AAAI Spring Symp. On              situational techniques for learning word-to-meaning
  Learning Grounded Representations, Stanford CA.              mappings, Cognition (61) 39-91.
Cottrel GW, Bartell B, Haupt C. (1990) Grounding             Siskind JM (2001) Grounding the lexical semantics of
  Meaning in Perception. In Proc. GWAI90, 14th                 verbs in visual perception using force dynamics and
  German Workshop on Artificial Intelligence, pages            event logic. Journal of AI Research (15) 31-90
  307--321, Berlin, New York,. Springer Verlag.              Steels, L. (2001) Language Games for Autonomous
Dominey PF, Ramus F (2000) Neural network                      Robots. IEEE Intelligent Systems, vol. 16, nr. 5, pp.
  processing of natural lanugage: I. Sensitivity to serial,    16-22, New York: IEEE Press.
  temporal and abstract structure of language in the         Stolcke A, Omohundro SM (1994) Inducing
  infant. Lang. and Cognitive Processes, 15(1) 87-127          probablistic grammars by Bayseian model merging/
Dominey PF (2000) Conceptual Grounding in                      In Grammatical Inference and Applications: Proc. 2nd
  Simulation Studies of Language Acquisition,                  Intl. Colloq. On Grammatical Inference, Springer
  Evolution of Communication, 4(1), 57-85.                     Verlag.
Dominey PF, Hoen M, Lelekov T, Blanc JM (2003)               Talmy L (1988) Force dynamics in language and
  Neurological basis of language in sequential                 cognition. Cognitive Science, 10(2) 117-149.
  cognition: Evidence from simulation, aphasia and           Tomasello M (1999) The item-based nature of
  ERP studies, (in press) Brain and Language                   children's early syntactic development, Trends in
Elman J (1990) Finding structure in time. Cognitive            Cognitive Science, 4(4):156-163
  Science, 14:179-211.
                                                         359

