UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Perception and Perspective in Robotics
Permalink
https://escholarship.org/uc/item/4q51r4vm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Author
Fitzpatrick, Paul
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                   Perception and Perspective in Robotics
                                           Paul Fitzpatrick (paulfitz@ai.mit.edu)
                              MIT Artificial Intelligence Laboratory, Cambridge, MA 02139 USA
                             Abstract
   To a robot, the world is a sea of ambiguity, in which it will
                                                                                                           Eyes
   sink or swim depending on the robustness of its percep-                                               (3 DOFs)
                                                                                                                                Facial
   tual abilities. But robust machine perception has proven                                                                   (15 DOFs)
   difficult to achieve. This paper argues that robots must be
   given not just particular perceptual competences, but the                                                                 Neck
   tools to forge those competences out of raw physical ex-                                                                (3 DOFs)
   periences. Three important tools for extending a robot’s
   perceptual abilities whose importance have been recog-
   nized individually are related and brought together. The                                                        Head
   first is active perception, where the robot employs motor                                                     (7 DOFs)
   action to reliably perceive properties of the world that it
   otherwise could not. The second is development, where
                                                                                                       Right arm              Left arm
   experience is used to improve perception. The third is                                              (6 DOFs)              (6 DOFs)
   interpersonal influences, where the robot’s percepts are
   guided by those of an external agent. Examples are given
   for object segmentation, object recognition, and orienta-
   tion sensitivity; initial work on action understanding is                                                         Torso
   also described.                                                                                                 (3 DOFs)
                         Introduction
Perception is key to intelligent behavior. While the field                                                                   Stand
                                                                                                                           (0 DOFs)
of Artificial Intelligence has made impressive strides in
replicating some aspects of cognition, such as planning
and plan execution, machine perception remains distress-
ingly brittle and task-specific. This paper directly ad-
dresses this brittleness by supporting perception through            Figure 1: The robots Kismet (top) and Cog (bottom).
active, developmental, and interpersonal means.                      Kismet is an expressive anthropomorphic head useful for
   Suppose there is some property P of the environment               human interaction work; Cog is an upper torso humanoid
whose value the robot cannot usually determine. Further
                                                                     more adept at object interaction.
suppose that in some very special situations, the robot
can reliably determine the property. Then there is the
potential for the robot to collect training data from such
                                                                     “caregiver”. For example, it may be necessary to correct
special situations, and learn other more robust ways to
                                                                     category boundaries or communicate the structure of a
determine the property P . This process will be referred
                                                                     complex activity.
to as “developmental perception” in this paper.
                                                                        By placing all of perception within a developmental
   Active and interpersonal perception both act as
                                                                     framework, perceptual competence becomes the result
sources of the “special situations” that allow the robot
                                                                     of experience evoked by a set of behaviors and predis-
to temporarily reach beyond its current perceptual abil-
                                                                     positions. If the machinery of development is sufficient
ities, giving the opportunity for development to occur.
                                                                     to reliably lead to the perceptual competence in the first
Active perception refers to the use of motor action to
                                                                     place, then it is likely to be able to regenerate it in some-
simplify perception (Ballard, 1991), and has proven its
                                                                     what changed circumstances, thus avoiding brittleness.
worth many times in the history of robotics. It allows the
robot to experience percepts that it (initially) could not
without the motor action. Interpersonal perception refers                                    The robots
to mechanisms whereby the robot’s perceptual abilities               This work is implemented on two robots, Cog and
can be influenced by those around it, such as a human                Kismet (see Figure 1), Cog is an upper torso humanoid
                                                                 408

                                                                                                                     Observe
                                                                              Constrained/Familiar
                                                                                                                 Familiar Objects
                                                                                      Activity
                                                                                                                in Novel Contexts
       (a)                     (b)                       (c)
                                                                                                                     Observe
                                                                                                                  Familiar Actors
                                                                                                                in Novel Contexts
                                                                                     Discover
                                                                                  Novel Objects
                                                                               in Familiar Context               Refine Primitive
                                                                                                                     Features
Moment of    Motion in frame Aligned motion Masking out    Final
impact       immediately     from before    prior motion   segmentation
                                                                                     Discover
             after impact    impact                                               Novel Actors
                                                                               in Familiar Context
Figure 2: Cartoon motivation (top) for active segmenta-
tion (bottom). Human vision is excellent at figure/ground                        Refine Primitive              Constrained/Familiar
separation (top left), but machine vision is not (top cen-                           Features                         Entities
ter). Coherent motion is a powerful cue (top right) and
the robot can invoke it by simply reaching out and pok-                     Figure 3: If the robot is engaged in a known activity
ing around. The lower row of images show the process-                       (left), there may be sufficient constraint to identify novel
ing steps involved. The moment of impact between the                        elements within that activity. Similarly, if known el-
robot arm and an object, if it occurs, is easily detected –                 ements take part in some unfamiliar activity, tracking
and then the total motion after contact, when compared                      those can help characterize that activity. Potentially, de-
to the motion before contact and grouped using a mini-                      velopment is an open-ended loop of such discoveries.
mum cut approach, gives a very good indication of the
object boundary (Fitzpatrick, 2003).
                                                                            arm (Fitzpatrick and Metta, 2002). If an object is within
                                                                            the area swept, then the motion signature generated by
(Brooks et al., 1999) that has previously been applied to                   the impact of the arm with that object greatly simpli-
tasks such as visually-guided pointing (Marjanović et al.,                 fies segmenting that object from its background, and ob-
1996), and rhythmic operations such as turning a crank                      taining a reasonable estimate of its boundary (see Fig-
or driving a slinky (Williamson, 1998). Kismet is an                        ure 2). The image processing involved relies only on
“infant-like” robot whose form and behavior is designed                     the ability to fixate the robot’s gaze in the direction of
to elicit nurturing responses from humans (Breazeal                         its arm. This coordination is easy to achieve either as a
et al., 2001). It is essentially an active vision head aug-                 hard-wired primitive or through learning (Fitzpatrick and
mented with expressive facial features so that it can both                  Metta, 2002). Within this context, it is possible to collect
send and receive human-like social cues.                                    excellent views of the objects the robot pokes, and the
                                                                            robot’s own arm.
                     Active perception                                          Figure/ground separation is a long-standing problem
The most well-known instance of active perception is ac-                    in computer vision, due to the fundamental ambiguities
tive vision. The term “active vision” is essentially syn-                   involved in interpreting the 2D projection of a 3D world.
onymous with moving cameras. Active vision work on                          No matter how good a passive system is at segmentation,
Cog is oriented towards opening up the potentially rich                     there will be times when only an active approach will
area of manipulation-aided vision, which is still largely                   work, since visual appearance can be arbitrarily decep-
unexplored. But there is much to be gained by tak-                          tive.
ing advantage of the fact that robots are actors in their
environment, not simply passive observers. They have                                      Developmental perception
the opportunity to examine the world using causality,                       The previous section showed how, with a particular be-
by performing probing actions and learning from the re-                     havior, the robot could reliably segment objects from
sponse. In conjunction with a developmental framework,                      the background (even if it is similar in appearance) by
this could allow the robot’s experience to expand out-                      poking them. It can determine the shape of an object
ward from its sensors into its environment, from its own                    boundary in this special situation, even though it can-
arm to the objects it encounters, and from those objects                    not do this normally. This is precisely the kind of situa-
both back to the robot itself and outwards to other actors                  tion that a developmental framework could exploit. Fig-
that encounter those same objects.                                          ure 3 shows how an open-ended developmental cycle
   As a concrete example of this idea, Cog was given a                      might be possible. Particular, familiar situations allow
simple “poking” behavior, whereby it selects locations                      the robot to perceive something about objects and actors
in its environment, and sweeps through them with its                        (such as a human or the robot itself) that could not be per-
                                                                        409

           Object          Robot           Foreign
          prototype     manipulator       manipulator
Figure 4: The top row shows sample views of a toy
car that the robot sees during poking. Many such views
are collected and segmented as described in (Fitzpatrick,
2003). The views are aligned to give an average pro-
totype for the car (and the robot arm and human hand         Figure 5: The empirical appearance of edges. Each 4 × 4
that acts upon it). To give a sense of the quality of the    grid represents the possible appearance of an edge, quan-
data, the bottom row shows the segmented views that are      tized to just two luminance levels. The dark line cen-
the best match with these prototypes. The car, the robot     tered in the grid is the average orientation that patch was
arm, and the hand belong to fundamentally different cat-     observed to have in the training data. The upper set of
egories. The arm and hand cause movement (are actors),       patches are the most frequent ones that occur in training
the car suffers movement (is an object), and the arm is      data consisting of about 500 object segmentations. The
under the robot’s control (is part of the self).             lower set of patches are a selection of patterns chosen to
                                                             illustrated the diversity of possible patterns that can oc-
ceived outside those situations. These objects and actors    cur. The oriented features represented include edges, thin
can be tracked into other, less familiar situations, which   lines, thick lines, zig-zags, corners etc. It is difficult to
can then be characterized and used for further discovery.    imagine a set of conventional filters that could respond
Throughout, existing perceptual capabilities (“primitive     correctly to the full range of features seen here – all of
features”) can be refined as opportunities arise.            which appeared multiple times in object boundaries in
   As a specific example of development, the segmented       real images.
views provided by poking of objects and actors by pok-
ing can be collected and clustered as shown in Figure 4.
Such views are precisely what is needed to train up an ob-   jects are judged to be the same depends on which of
ject detection and recognition system, and follow those      their many features are considered essential and which
objects and actors into other, non-poking contexts (Fitz-    are considered incidental. For a robot to be useful, it
patrick, 2003).                                              should draw the same distinctions a human would for a
   As well as giving information about the appearance of     given task. To achieve this, there must be mechanisms
objects, the segmented views of objects can be pooled        that allow the robot’s perceptual judgements to be chan-
to train up detectors for more basic visual features – for   neled and moulded by a caregiver. This is also useful in
example, edge orientation. Once an object boundary is        situations where the robot’s own abilities are simply not
known, the appearance of the edge between the object         up to the challenge, and need a helping hand.
and the background can be sampled along it, and labelled        The most relevant literature at the intersection of com-
with the orientation of the boundary in their neighbor-      munication, perception and development is that of lan-
hood. Figure 5 shows an orientation filter trained up from   guage acquisition. It has been observed that language
such data that can work at much finer scales than nor-       acquisition involves a search through a large search
mally possible when the filter is derived from an ideal      space of models guided by relatively sparse feedback
edge model such as that of (Chen et al., 2000). The “cat-    and few examples. This so-called “poverty of the stim-
alog” of edge appearances found shows that the most fre-     ulus” relative to the complexity of the models being ac-
quent edge appearances is an “ideal” straight, noise-free    quired is taken to imply that infants must have a good
edge, as might be expected (top of Figure 5) – but a re-     search strategy, with biases well matched to the na-
markable diversity of other forms also occur which are       ture of appropriate solution. This is a claim of innate
far less obvious (bottom of Figure 5).                       constraints, and is historically controversial. Examples
                                                             stressing under-determination in language learning in-
              Interpersonal perception                       clude Quine’s “Gavagai” example (Quine, 1960), where
Perception is not a completely objective process; there      Quine invites us to imagine ourselves walking with a
are choices to be made. For example, whether two ob-         native guide in a foreign country, and seeing a rabbit
                                                         410

                 Human speech       Human action          Robot speech      Robot action
                 ...                ...                   ...               ...
                 say yellow         [shows yellow toy]    yen               [looks at toy]
                 say yellow                               yelo
                 yes! say green                           green
                 ...                ...                   ...               ...
                 yellow             [shows yellow toy]    yelo              [looks at toy]
                 left               [moves toy left]      left              [tracks toy]
                 green              [shows green toy]     green             [looks at toy]    Left        Yelo
                 right              [moves toy right]     right             [tracks toy]
                 yellow             [shows yellow toy]    yelo              [looks at toy]
                 left               [moves toy left]      left              [tracks toy]    Right         Green
                 ...                ...                   ...               ...
                 yellow             [shows yellow toy]    yelo              [looks at toy]
                 next?                                    ahm left?         [nods left]
                 yes!               [moves toy left]      left!
                 green              [shows green toy]     green             [looks at toy]
                 next?                                    ahm right?        [nods right]
                 yes!               [moves toy right]     right!
                 ...                                      ...
Figure 6: Extracts from a dialogue with Kismet. The first extract (say yellow...) illustrates how the robot’s active
vocabulary was extended. The second extract shows how a simple sorting activity was annotated for the robot. The
final extract shows the robot being tested on its understanding of the form of the activity. The robot’s utterances were
transcribed phonetically, but are written in a simple form here for clarity. To the right is shown the very simple state
machine model of the activity deduced by the robot.
pass just as the guide says “gavagai” – and then consider             The social-pragmatic approach to the problem of
all the possible meanings this utterance might have. It               referential indeterminacy ... begins by rejecting
is possible over time to learn from such situations (see              truth conditional semantics in the form of the map-
(Steels and Kaplan, 1999) for an example of a robotic im-             ping metaphor (the child maps word onto world),
plementation). Pragmatic constraints can help speed the               adopting instead an experientialist and conceptual-
learner out of this sea of ambiguity. For example, (Mark-             ist view of language in which linguistic symbols are
man, 1989) proposes a set of particular constraints in-               used by human beings to invite others to experi-
fants might use to map words on to meanings. These con-               ence situations in particular ways. Thus, attempt-
straints are along the style of the following (with many              ing to map word to world will not help in situa-
variations, elaborations and caveats) :-                              tions in which the very same piece of real estate
                                                                      may be called: “the shore” (by a sailor), “the coast”
• Whole-object assumption. If an adult labels some-                   (by a hiker), “the ground” (by a skydiver), and “the
   thing, assume they are referring to the whole object               beach” (by a sunbather).
   and not a part of it. categories” as opposed to thematic
   relationships. For example when child is asked to find          Regardless of the utility of Tomasello’s theory for its
   “dog”, may fetch the cat, but won’t fetch dog-food.             proper domain, language acquisition in infants, it seems
                                                                   a useful mindset for tackling interpersonal perception,
• Mutual exclusivity. Assume objects have only one la-
                                                                   which is in essence all about inviting the robot to view
   bel. So look for an unnamed object to apply a new
                                                                   the world in a particular way.
   label to.
                                                                      Tomasello and his collaborators developed a series of
These constraints are intended to explain a spurt in               experiments designed to systematically undermine the
vocabulary acquisition where infants begin to acquire              constraints approach to learning as typified by Markman
words from one or a few examples – so-called fast-                 and others. The experiments investigate word learning
mapping. They are advanced not as absolute rules, but              among children in the context of various games. The ex-
as biases on search.                                               periments are instructive in showing a range of situations
   Tomasello raises several objections to the constraint-          in which simple rules based directly on gaze or affect
based approach represented by Markman (Tomasello,                  would fail in at least one case or other. The experiments
1997). Tomasello favors a “social-pragmatic” model of              all avoid giving children (18-24 months old) ostentative
language acquisition that places language in the context           naming contexts, and rather requiring them to pull out
of other joint referential activity, such as shared attention.     meanings from the “flow of interaction”.
He rejects the “word to meaning mapping” formulation                  For example, in one experiment, an adult makes eye-
of language acquisition. Rather, Tomasello proposes that           contact with a child subject and says “Let’s go find the
language is used to invite others to experience the world          toma.” They then go to a row of buckets, each if which
in a particular way. From (Tomasello, 1997) :-                     contains an object with which the child is not familiar.
                                                              411

   Role in Activity                          Physical Location
                        Object Identity
   Verbal Context                           Physical Appearance
Figure 7: Perceptual judgements are fundamentally
about identity: what is the same, what is different. Iden-
tity judgements should depend (at least) on activity, lo-            Location marked, Robot looks away Robot looks back, Target reappears
                                                                       Target present                   Target is gone
cation, appearance, and verbal context. These in turn can
be influenced by a caregiver.
                                                                    Figure 8: Keeping track of locations. Circles with cross-
                                                                    hairs represent locations that contain a particular object.
                                                                    If the object is removed, this is detected using color his-
One of these objects is randomly designated the “toma”.
                                                                    tograms (Swain and Ballard, 1991), and is indicated by a
If the session is a control, the adult goes directly to the
bucket containing the toma, finds it excitedly and hands            small circle without a cross-hair. The upper row is a car-
it to the child. Otherwise, the adult first goes to two             toon sequence to illustrate what is happening in the views
other buckets in sequence, each time taking out the ob-             below, which are taken directly from Cog’s egocentric
ject, scowling at it, and replacing it, before “finding” the        map. Initially a yellow car is present on the table in front
toma. Later, the child is tested for for the ability to com-        of Cog. The robot looks away to the door, and when it
prehend and produce the new word appropriately. The                 looks back, the car is no longer present. It then reappears
results show equally good performance in the test and               and is immediately detected. This behavior, along with
control scenarios. Tomasello argues that this situation             object tracking (which has also been implemented), give
counts against children using simple word learning rules            the basics of a representation of the robot’s workspace.
such as “the object the adult is looking at while saying
the novel word,” “the first new object the adult looks at
after saying the novel word,” “the first new object the in-             The ability to interact verbally is currently being
fant sees after hearing the novel word,” or such variants.          ported from Kismet to Cog, so that interpersonal percep-
   Tomasello’s theories and experiments are provocative,            tion can be integrated fully with the active and develop-
and suggest an approach quite different from the simple             mental work described earlier. Cog already has a well
associative learning that is most often seen in robotics.           developed means to keep track of physical locations in
Work on interpersonal perception on Cog draws heav-                 an egocentric coordinate frame (see Figure 8). It is antic-
ily on (a grossly simplified caricature of) these ideas.            ipated that this will be important in communicating the
The basic idea for interpersonal perception drawn from              structure of activities to the robot, since even for adult
Tomasello’s work is that information about the iden-                humans cognition can often be traded off with physi-
tity of an object needs to be easily coordinated between            cal space (Pelz, 1995; Kirsh, 1995). Recent work has
perception of activity, location, speech, and appearance            focused on communicating the structure of search ac-
(Figure 7). Without this flexibility, it is hard to imagine         tivity to the robot, and then using that to learn from a
how scenarios such as the experiment described above or             Tomasello-inspired ‘find the toma’ episode (Fitzpatrick,
others proposed (Tomasello, 1997) could be dealt with.              2003).
   It is currently unreasonable to expect the robot to un-
derstand the “flow of interaction” without help. Unaided                                     Conclusions
segmentation of activity is a very challenging problem              This paper presented a snapshot of ongoing work to cre-
(see (Goldberg and Mataric, 1999) for one effort in the             ate an active, developing, malleable perceptual system
robotic domain). The human interacting with the robot               for a robot. There is much remaining work to do. The im-
can greatly simplify the task by making the structure of            mediate technical goal is to further develop mechanisms
the activity unambiguous. Two mechanisms for this are               for communicating the structure of simple activities to a
particularly easy to deal with: vocalizations and location.         robot, translating this structure into a set of supervised
If places and words are used consistently in an activity,           learning problems for parts of the task which are diffi-
then it is straightforward to model the basic “flow of in-          cult to communicate directly, and finally solving those
teraction” they define. Figure 6 shows an example of this           problems with the guidance of a protocol for inducing
for a very simple sorting activity, implemented on the              feature selection. Figure 9 shows a schematic for how
robot Kismet. Note that words are used here without the             this may be achieved. The basic idea is for the robot to
robot needing to know their meanings – it is sufficient             interact with the instructor vocally and through a shared
that they be used consistently enough for the structure of          workspace to acquire a “sequencing model” of an activ-
the task to be made obvious.                                        ity or task, and then to ground that model based on a
                                                                412

                                                                   manoid robot. Lecture Notes in Computer Science,
 Training                                                          1562:52–87.
                            Task Learning Mechanism
     Data                                                     Chen, J., Sato, Y., and Tamura, S. (2000). Orientation
                                                                   space filtering for multiple orientation line segmen-
   Instructor                        Sequencing Model
                                                                   tation. IEEE Transactions on Pattern Analysis and
                      Task                                         Machine Intelligence, 22(5):417–429.
                    Modeling
                                                              Clancey, W. J. (2002). Simulating activities: Relating
                                                                   motives, deliberation, and attentive coordination.
                      Task
                                                                   Cognitive Systems Research, 3(3):471–499.
                    Grounding             State
                                       Grounding
                                                              Fitzpatrick, P. (2003). From First Contact to Close
                                                                   Encounters: A developmentally deep perceptual
                                                                   system for a humanoid robot. PhD thesis, Mas-
                    Perceptual                                     sachusetts Institute of Technology, Cambridge,
                     System                                        MA.
Demonstrated
     Task                                                     Fitzpatrick, P. and Metta, G. (2002).              Towards
                                     Perceptual Network            manipulation-driven vision. In IEEE/RSJ Confer-
                                                                   ence on Intelligent Robots and Systems, volume 1,
                                                                   pages 43–48.
Figure 9: A summary of how task learning will be im-
plemented. The instructor demonstrates the task while         Goldberg, D. and Mataric, M. J. (1999). Augmented
                                                                   markov models. Technical report, USC Institute for
providing verbal and spatial cues. The cues are used to
                                                                   Robotics and Intelligent Systems.
construct a model of the task. Generic machine learning
methods are then used to ground this model in the robot’s     Kirkham, N. Z., Slemmer, J. A., and Johnson, S. P.
perceptual network, guided by previously grounded fea-             (2002). Visual statistical learning in infancy: ev-
ture selection cues. The idea is to avoid ever presenting          idence of a domain general learning mechanism.
the robot with a hard learning problem; the learning algo-         Cognition, 83(2):B35–B42.
rithms are intended to be “decoders” allowing the human       Kirsh, D. (1995). The intelligent use of space. Artificial
to communicate changes in representation, rather than to           Intelligence, 73:31–68.
learn in the conventional sense.
                                                              Marjanović, M. J., Scassellati, B., and Williamson,
                                                                   M. M. (1996). Self-taught visually-guided pointing
demonstration of the task. This goal of this work is not           for a humanoid robot. In From Animals to Animats:
to deal with general-purpose problem solving ability –             Proceedings of 1996 Society of Adaptive Behavior,
for which better models are available (Clancey, 2002) –            pages 35–44, Cape Cod, Massachusetts.
but to capture something of the quite general statistical
                                                              Markman, E. M. (1989). Categorization and naming in
learning abilities of young infants (Kirkham et al., 2002).
                                                                   children: problems of induction. MIT Press, Cam-
                    Acknowledgements                               bridge, Massachusetts.
The author would like to thank the anonymous review-          Pelz, J. B. (1995). Visual Representations in a Natural
ers for their constructive feedback. Funds for this project        Visuo-motor Task. PhD thesis, Brain and Cognitive
were provided by DARPA as part of the “Natural Tasking             Science, University of Rochester.
of Robots Based on Human Interaction Cues” project un-
                                                              Quine, W. V. O. (1960). Word and object. Harvard Uni-
der contract number DABT 63-00-C-10102, and by the
                                                                   versity Press, Cambridge, Massachusetts.
Nippon Telegraph and Telephone Corporation as part of
the NTT/MIT Collaboration Agreement.                          Steels, L. and Kaplan, F. (1999). Collective learning and
                                                                   semiotic dynamics. In European Conference on Ar-
                          References                               tificial Life, pages 679–688.
Ballard, D. H. (1991). Animate vision. Artificial Intelli-
                                                              Swain, M. J. and Ballard, D. H. (1991). Colour indexing.
       gence, 48(1):57–86.
                                                                   International Journal of Computer Vision, 7(1):11–
Breazeal, C., Edsinger, A., Fitzpatrick, P., and Scassel-          32.
       lati, B. (2001). Active vision for sociable robots.    Tomasello, M. (1997). The pragmatics of word learning.
       IEEE Transactions on Systems, Man, and Cybernet-            Japanese Journal of Cognitive Science, 4:59–74.
       ics, A, 31(5):443–453.
                                                              Williamson, M. (1998). Neural control of rhythmic arm
Brooks, R. A., Breazeal, C., Marjanovic, M., and Scas-             movements. Neural Networks, 11(7-8):1379–1394.
       sellati, B. (1999). The Cog project: Building a hu-
                                                          413

