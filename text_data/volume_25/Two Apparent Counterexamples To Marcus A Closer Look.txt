UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Two Apparent “Counterexamples” To Marcus: A Closer Look
Permalink
https://escholarship.org/uc/item/8sw47717
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Vilcu, Marius
Hadley, Robert F.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Two Apparent “Counterexamples” To Marcus: A Closer Look
                                              Marius Vilcu (mvilcu@cs.sfu.ca)
                                    School of Computing Science, Simon Fraser University
                                                     Burnaby, Canada, V5A 1S6
                                            Robert F. Hadley (hadley@cs.sfu.ca)
                                    School of Computing Science, Simon Fraser University
                                                     Burnaby, Canada, V5A 1S6
                           Abstract                                 same network to simultaneously discriminate between
                                                                    sequences of syllables generated by two simple
   Marcus, Vijayan, Bandi Rao, Vishton’s experiment                 grammars (ABA and ABB). Elman claimed that his
   (1999) concerning infant ability to discriminate between         experiment showed that SRNs successfully matched the
   simple syntactic structures has prompted many                    infants’ results. In a recent paper (Vilcu & Hadley,
   connectionists to strive to demonstrate that certain types       2001), after performing numerous experiments on
   of neural networks can replicate those results. In this          Elman’s model (1999), we showed that Elman’s claim
   paper we take a closer look at two such attempts: Shultz         was premature, and his networks performed erratically.
   & Bale (2001) and Altmann & Dienes (1999). We were               We emphasize, however, that Elman’s model had a
   not only interested in how well these two models
   matched the infants’ reported results, but also whether or
                                                                    more difficult task to solve: unlike infants, it was
   not they were able to learn the grammars involved in this        trained to recognize two specific grammars at the same
   process. After performing an extensive set of                    time. Therefore, the fact that Elman’s experiment was
   experiments, we found that, at first blush, Shultz &             questionable does not mean that other connectionist
   Bale’s model replicated the infant’s known data, but the         models cannot effectively match the infants’ results.
   model largely failed to learn the grammars. We also                 In this paper we focus on other two such attempts:
   discovered serious problems with Altmann & Dienes’               Shultz & Bale (2001) and Altmann & Dienes (1999).
   model, which failed to match most of the infant’s results        Shultz & Bale’s experiment was performed on a
   and to learn the syntactic structure of the input patterns.      cascade-correlation network, and they claim that their
                                                                    results “show that an unstructured neural network
                       Introduction                                 model without symbolic rules can simulate infant
The widely known Marcus et al.’s experiment on                      familiarization and novelty results” (2001). They also
infants (1999), involving their ability to differentiate            argue that the network exhibits “extrapolative
between simple grammars, has prompted not a few                     generalization outside the range of the training
connectionists to demonstrate that a variety of neural              patterns” (2001). After performing an extensive set of
networks are indeed capable of matching the infants’                experiments on their model, we came to the conclusion
behavior.                                                           that Shultz and Bale’s claims (2001) are substantially
   Marcus et al. (1999) familiarized 7-month-old infants            over-stated. We found that even though this model
with sequences of syllables (“sentences”) generated by              closely mirrors Marcus et al.’s reported data (1999), it
one of two grammars: ABA or ABB, and, in another                    has limited generalization capabilities. As we
experiment, ABB or AAB (for example, “la ti ti”, “ga                demonstrate below, the network fails to generalize both
ga ti”, etc). During the test phase, infants were                   outside of the training space (extrapolation), and within
presented with novel sequences of syllables generated               the range of the training patterns (interpolation).
by both grammars, and showed an attentional                         Granted, Shultz & Bale never explicitly claim their
preference for sentences that were constructed with the             model learns a grammar. However, in saying that the
unfamiliar grammar. Marcus et al. (1999) argued that                network was able to “recognize a syntactic pattern”
the only explanation for such behavior is that infants              (2001), and had the “ability to learn multiple syntactic
possess a rule-learning mechanism that is not available             forms simultaneously” (2001), Shultz & Bale imply that
to connectionist models.                                            their model learns the underlying syntactic structure of
   There have been repeated attempts to prove that                  the input patterns and is able to successfully apply this
neural networks are capable of doing the same kind of               knowledge to novel items. We believe that Shultz &
discrimination as infants. Among those attempts is                  Bale’s network (2001) behaves more like a typical
Elman’s (1999), who first pre-trained a simple recurrent            pattern recognizer, whose performance is conditioned
network (SRN) to distinguish whether a given syllable               by familiar “shapes” (numerical contours), than a
is identical to a previous syllable. He then trained the            model capable of discovering abstract grammatical
                                                                    relationships. We found that, in general, test sentences
                                                               1188

closest (in Euclidian space) to the training vectors will    between 1 and 8. The choice of the new encoding was
generate the smaller network error, regardless of            based on the fact that sonority represents the “quality of
whether those test sentences had been generated with         vowel likeness” (2001), i.e., some phonemes can be
the familiar or unfamiliar grammar. Therefore, it is         considered to be “more vowel-like” than others. The
Euclidian closeness to the training data, rather than the    sonority scale ranges from “low vowels”, such as /a/
learning of underlying structure of input patterns, which    and /æ/ that were assigned a sonority of +6.0, to
dictates the behavior of this network.                       “voiceless stops”, such as /p/, /t/, and /k/ that were
   Altmann & Dienes (1999) used a modified simple            assigned a sonority of –6.0. For example, ga = -5.0 6.0,
recurrent network, adding a new layer of units between       wo = -1.0 5.0, ti = -6.0 4.0. A sentence consists of three
the input and hidden layers of nodes. This new layer         such syllables, generated using one simple grammar
encodes two different, non-overlapping domains: the          (ABA, ABB, AAB). For example, ga ti ga = -5.0 6.0 -
training and test sets. The common encoding of the two       6.0 4.0 -5.0 6.0, li na na = -1.0 4.0 -2.0 6.0 -2.0 6.0.
domains facilitates the network’s generalization to the
test patterns. Altmann & Dienes (1999) reported good                                               Output layer
results for their simulation. They claimed they found
“significantly higher correlation for congruent
sequences than for incongruent ones (…), and a
significantly smaller Euclidian distance between                                                   Second hidden unit
prediction and target for congruent targets than for
incongruent ones” (1999). We performed numerous
experiments on this model and discovered serious
problems. We found that when the networks were                                                     First hidden unit
trained with the ABB grammar, the Euclidian distance
between the actual and target vectors was consistently
higher for familiar sequences than for unfamiliar ones.
We believe these findings are incompatible with the                                                Input layer
Altmann & Dienes’ assertion that “like the infants (…),
our networks successfully discriminated between the          Figure 1: The cascade architecture (with 2 hidden units)
test stimuli” (1999).
                                                                Similar to Marcus et al.’s experiment (1999), Shultz
           Shultz & Bale’s Experiment                        & Bales’ simulation (2001) consisted of three
Shultz & Bale’s simulation (2001) employs an encoder         experiments, each experiment involving 16 separate
version of the cascade-correlation learning algorithm.       networks (one network corresponds to one infant). The
The cascade-correlation (Fahlman & Lebiere, 1990) is a       first two experiments consisted of training eight
generative algorithm in feed-forward networks. It            networks with sentences generated by the ABA
creates the network topology as it learns, by adding new     grammar, and other eight networks were trained on the
hidden layer units as necessary, in order to minimize        ABB grammar. All 16 networks were then tested with
the network error for the task at hand. Each new unit is     novel sentences corresponding to both grammars.
installed on a separate hidden layer alone, and receives     Experiment 3 was similar to the first two simulations,
data from both the input layer and the existing hidden       except that the grammars involved were AAB and
layers. The hidden unit that gets added to the network is    ABB.
chosen from a pool of candidates: the candidate unit            In our simulation of this model we used the same
whose activations correlate most highly with the             parameters as Shultz & Bale (2001): a score-threshold
network current error gets added to the structure.           of 0.8, input-patience and output-patience of 1. All
   The “encoder” version of the network precludes            other training parameters were identical to Fahlman &
direct input-output connections, in order to avoid           Labiere’s default values [5]. The only difference from
generating networks simply having connections of             Shultz & Bale (2001) was that we ran all experiments
weight 1 between the input and output layers (see            on double the number of networks. This permits a more
Figure 1).                                                   accurate picture of the performance of the model.
   This network is similar to a prior model by Shultz           Initially, we performed the same experiments as
(1999). The only difference between the two                  Shultz & Bale (2001), using identical input patterns.
simulations is the input representation. In the more         Each experiment consisted of training 32 networks, 16
recent experiment, Shultz & Bale (2001) used a               of them with sentences generated by the ABA grammar
sonority scale, each phoneme being assigned a number         (in the case of experiments 1 and 2) or the AAB
between –6.0 and 6.0. They claim that this encoding          grammar (for experiment 3), and the other 16 were
scheme is more “realistic” (2001) than the one used in       trained with patterns generated by the ABB grammar
the previous paper (Shultz, 1999), where each syllable       (in all three experiments). In each experiment, all 32
(collection of two phonemes) was assigned a number           networks were then tested with novel sentences created
                                                        1189

using both the training, familiar grammar (consistent         Table 2: Network error in the three experiments using
patterns), and the unfamiliar grammar (inconsistent).                            altered test patterns
Our results closely resembled Shultz & Bale’s (2001),
as it is shown in Table 1. The table displays the mean                              Test vs.            Altered test
network error over all 32 networks for each experiment.         Experiment          training              patterns
                                                                                   sentences        Inside      Outside
      Table 1: Mean network error in the first three                                                  the          the
                        experiments                                                                training training
                                                                                                    space        space
    Experiment        Test vs.      Shultz &      Our                1            Consistent         8.83        97.29
                      training       Bale’s     results                          Inconsistent        8.46        89.83
                     sentences       results                         2            Consistent         14.56      136.89
                                     (2001)                                      Inconsistent        13.83      122.83
         1           Consistent         8.2       8.32               3            Consistent         14.71      144.82
     ABA vs.                                                                     Inconsistent        14.57      129.09
       ABB          Inconsistent       14.5      16.12
         2           Consistent        13.1      13.92          In experiments 2 and 3, we replaced two instances of
     ABA vs.        Inconsistent       15.8      14.33       the letter “b” (sonority –5.0) with the letter “m”
       ABB                                                   (sonority –2.0) in one ABA sentence (experiment 2),
         3           Consistent        12.9      13.63       and in one AAB sentence (experiment 3): the test
     AAB vs.        Inconsistent       15.3      15.05       sentence ba po ba (-5 6 -6 5 -5 6) became ma po ma (-
       ABB                                                   2 6 -6 5 -2 6) in experiment 2, and ba ba po (-5 6 -5 6
                                                             -6 5) became ma ma po (-2 6 -2 6 -6 5) in experiment
   Given the mapping between the syllables employed          3. We do not know what infants would have done if
by Marcus et al. (1999) and the sonority values used         they were presented with these new test sentences.
here, and assuming that the network error resembles the      However, to an adult the changes seem minimal, and it
time      spent    by      infants    looking    at    the   is entirely credible that infants would still have been
consistent/inconsistent stimuli, it does appear that this    able to differentiate between the familiar and unfamiliar
model is capable of capturing the infants’ reported data.    sentences. As shown in Table 2, the model was not able
But Shultz & Bale (2001) also claim that the network         to distinguish the two categories of sentences, even
was able to generalize to novel sequences of syllables       though the new test sentences were well within the
by learning the underlying structure of input patterns.      training space. Contrary to their claim, our results
As we show below, this is not the case.                      showed that Shultz and Bale’s model (2001) was not
   The problems appeared when we altered the test            able to “generalize (…) to novel sentences within the
patterns for each of the three experiments. When we          range of the training patterns” (2001).
tested interpolation (generalization within the training        In order to test the extrapolation ability of the model,
space), we discovered that changing just one letter in       we re-ran the three experiments on a new test set. We
the test set does make a significant difference in the       picked 6 different values outside the sonority scale: 4 of
distribution of the network error.                           these values were below -6.0 and were used as
   In experiment 1, we replaced one instance of the          consonants: -7.0, -8.0, -9.0, -10.0, and the other 2 were
letter “w” (sonority of –1.0) with “v” (sonority of –3.0)    greater than +6.0 and were used as vowels: 7.0 and 8.0.
in one of the ABB sentences: the test sentence wo fe fe      For instance, an ABA sentence was -10.0 8.0 -9.0 8.0 -
(-1 5 -4 5 -4 5) became vo fe fe (-3 5 -4 5 -4 5). If the    10.0 8.0, and an ABB sentence -10.0 8.0 -9.0 8.0 -9.0
model learned the underlying structure of the input          8.0. As shown in Table 2, the network error for all of
patterns, and if it was trained on ABA patterns, ti          the three experiments was smaller in the unfamiliar
should not have any difficulty distinguishing between        case. This means that the network’s ability to
the unchanged ABA test sentences and the novel ABB           generalize outside the training space is weak, and that
sentences that contained one new consonant. Since this       the model did not reliably “recognize syntactic
new letter was never presented to the network before,        differences in sentences containing words with
the network error was expected to be higher for the          sonorities outside of the training range” (2001).
ABB sentences that contained it, along with an even             The model’s inability to learn the syntactic structure
better differentiation between the more familiar ABA         of the input patterns was also shown in another
sentences and the unfamiliar ABB sentences. In reality,      experiment. We re-ran experiment 1 using two slightly
our results showed that the network error for the            more complex grammars: ABCA vs. ABCB. We used
unfamiliar ABB patterns was smaller than for ABA             the same set of letters as in the original first experiment,
patterns (see Table 2).                                      but the input patterns were generated with the ABCA
                                                             and ABCB grammars. After 16 different runs, the mean
                                                             network error for unfamiliar sentences was smaller than
                                                        1190

for the familiar sentences (30.22 vs. 30.73), which           1.0 -4.0 4.0 -1.0 1.0). For testing, we randomly pic ked 4
means that the model did not learn the grammars.              values within the training set (between 1 and 8 in the
   Finally, in another set of experiments we discovered       first experiment, and between -6.0 and 6.0 in the second
that if the network is “abstracting functions relating        one), and generated two test sets for both experiments.
inputs to outputs” (2001) as Shultz & Bale claim, this        Each test set contained two ABA and two ABB
kind of “abstraction” amounts to recognizing spatial          sentences. The ABB sentences were the same in both
shapes that are similar to the input set, rather than         test sets. The ABA sentences in the first test set had the
understanding “two syntactic forms simultaneously”            same numerical contour as the training patterns (low-
(2001). In other words, we found that this network is         high-low, “peaks”), while in the second test set the
essentially a typical shape-pattern recognition model,        ABA sentences had an opposite contour (high-low-
and not a system capable of learning grammars.                high, “valleys”) (see Table 3). We ran the two
   To clearly show this pattern recognition behavior, we      experiments on 16 different networks, and as shown in
performed two experiments: one experiment used the            Table 3, the networks behaved differently when tested
sonority scale encoding, and the other one used the           with the two sets. When the ABA test vectors
coding scheme that Shultz made use of during a                represented “peaks”, the network error was smaller for
previous simulation (Shultz, 1999). That earlier model        the familiar patterns than for the unfamiliar patterns.
matched the newer model (Shultz & Bale, 2001) in              However, when the ABA test vectors represented
every aspect of network structure and learning                “valleys”, even though they had a “familiar” (ABA)
algorithm. The only difference was the input                  structure, the error was smaller for the unfamiliar
representation. Shultz (1999) assigned an odd number          (ABB) patterns. This means that the grammatical
between 1 and 7 to category “A” syllables, and an even        structure does not play a significant role in the behavior
number between 2 and 8 to category “B” syllables. For         of the model.
example, ga ti ga was represented by 1 2 1.                      We believe these experiments demonstrate that the
   In both experiments we trained the networks on ABA         model was not able to develop the kind of internal
generated sentences, but all training patterns had one        representations that would enable it to actually learn the
additional property. In one experiment, the encoding          syntactic structure of the input patterns. Clearly, it is the
value of the B syllables was always greater than the          numerical contour (shape) of the sentence that dictates
value of the A syllables (for example, 1 2 1, 3 6 3, 4 6      the behavior of the network, and not the grammatical
4, etc). In the other experiment (using the sonority scale    structure of the whole sentence. The network is an
encoding), the absolute values of both consonants and         example of a traditional pattern recognition system,
vowels were greater for the B syllables than for the A        rather than a grammar-learning model.
syllables (for example, -2.0 2.0 -5.0 5.0 -2.0 2.0, -1.0
    Table 3: Mean network error when testing with “peaks” and “valleys” using two different input representations
           Experiment       Input representation                Testing patterns                 Network error
                                                                    -5.0 5.0 -6.0 6.0 -5.0 5.0
                                                     Consistent                                      13.36
                                                                    -4.0 4.0 -5.0 5.0 -4.0 4.0
                           Shultz & Bale (2001)
                                                                    -6.0 6.0 -5.0 5.0 -5.0 5.0
                                                    Inconsistent                                     47.07
             Testing                                                -5.0 5.0 -4.0 4.0 -4.0 4.0
             “peaks”                                                          676
                                                     Consistent                                       1.21
                                                                              454
                               Shultz (1999)
                                                                              766
                                                    Inconsistent                                      3.29
                                                                              544
                                                                    -6.0 6.0 -5.0 5.0 -6.0 6.0
                                                     Consistent                                      55.64
                                                                    -5.0 5.0 -4.0 4.0 -5.0 5.0
                           Shultz & Bale (2001)
                                                                    -6.0 6.0 -5.0 5.0 -5.0 5.0
             Testing                                Inconsistent                                     47.07
                                                                    -5.0 5.0 -4.0 4.0 -4.0 4.0
            “valleys”
                                                                              767
                                                     Consistent                                       4.55
                               Shultz (1999)                                  545
                                                    Inconsistent              766                     3.29
                                                        1191

         Altmann & Dienes’ Experiment                          were frozen, and the network was tested on the second
                                                               domain. Although this training/test procedure may
Altmann & Dienes’ (1999) work on simulating Marcus
                                                               seem biologically implausible, Dienes, Almann, and
et al.’s experiment (1999) was based on a previous
                                                               Gao (1999) argue it mimics an adaptive learning
model of their own: Dienes, Altmann & Gao (1999).
                                                               mechanism, where the learning rate gradually decreases
That previous model represented a version of a simple
                                                               while the learning progresses. We agree that certain
recurrent network that can “transfer its knowledge of
                                                               aspects of the adaptive learning technique may be
artificial grammars across domains” (1999). Later,
                                                               biologically plausible, but doubt that Dienes, Altmann,
Altmann & Dienes (1999) adapted it to simulate
                                                               and Gao’s method (1999) of updating certain
Marcus et al.’s experiment on infants (1999). Since our
                                                               connection weights while keeping others frozen mirrors
intention was to analyze the models that were
                                                               the human brain’s activity during learning of novel
specifically intended to replicate the infants’ results, we
                                                               patterns.
focused on the more recent work of Altmann & Dienes
                                                                  In any case, using the same input representations and
(1999).
                                                               learning parameters1 , we tried to replicate Altmann &
  Altmann & Dienes (1999) used a simple recurrent
                                                               Dienes’ results (1999). We trained 16 networks on
network with an additional layer of units between the
                                                               patterns generated by an ABA grammar, and another 16
input and hidden layers of the SRN. This additional
                                                               networks on patterns following an ABB grammar. We
layer is used to re-encode the input representations of
                                                               then tested the networks on novel sentences having both
two domains (the training and test domains). The
                                                               ABA and ABB structures. It emerged that the Euclidian
function of this extra layer is to provide an abstract,
                                                               distance between target and prediction was always
common encoding of two different input sets (see
                                                               higher for patterns having the ABB structure, regardless
Figure 2).
                                                               of what the training grammar was: after ABA training,
                                                               the network error for consistent/inconsistent test patters
                                                   Output      was 0.84/ 0.85, while after ABB training, the error for
     Output domain 1         Output domain 2
                                                   layer       consistent/inconsistent test patterns was 0.87/ 0.85.
                                                               Therefore, we were not able to replicate Altmann &
                                                               Dienes’ s results (1999). Not only was the network
                 Hidden layer                                  error for consistent test patterns very close to the error
                                          Context layer        for inconsistent patterns, but also the error was higher
                                                               for familiar sentences when the network was trained on
                                                               ABB patterns. We also tried various other learning
                Encoding layer
                                                               parameters (learning rate, momentum, number of
                                                               iterations), but in each case our results showed that the
                                                  Input        model was not able to mirror infants’ performance, and
       Input domain 1        Input domain 2                    it is clear it did not learn the syntactic structure of the
                                                  layer
                                                               input patterns.
     Figure 2: Version of SRN with an extra encoding              In passing, we note that Altmann (2002) employed a
                            layer                              variation on the Altmann & Dienes’ experimental
                                                               design (1999). He pre-trained 16 different networks on
  The connection weights between the encoding and              simple sentences of the form Noun Verb, or Noun Verb
hidden layers, as well as between the context and              Noun. Also, during testing he did not freeze the core
hidden layers, represent the “core weights”, and they          weights (all the connection weights changed freely
are frozen after training. All other connection weights        during both training and testing). After pre-training,
represent the “mapping weights”, and they are allowed          some of the 16 networks were trained on the ABA
to change even during testing, while the test set is           grammar, and others were trained on sentences created
learned.                                                       with the ABB grammar. Altmann found that the model
  Training is performed using back-propagation. The            predicted the familiar test sentences better than the
input vectors are completely orthogonal: just one input        unfamiliar ones, concluding that once the model learns
unit is active at any time, corresponding to a given           a pre-training structure, it is less likely that the test
syllable. Each sentence is presented to the network one        structure will replace the grammar that is learned during
syllable at a time, beginning with the activation of a         habituation. However, the true explanation for this
special “start” unit and concluding with the activation        behavior might stem from the fact that during pre-
of the “end” unit.                                             training the model was presented with patterns
  During “testing” on a new input set, the “core
                                                               1
weights” were frozen, and only the “mapping weights”             Learning rate: 0.5, momentum: 0.01, 10 iterations around
changed for a number of iterations, until the network          each test pattern, 14 input units: 8 of them corresponded to 8
learned the encoding of the new domain. After this             syllables of the first domain, 4 represented the four syllables
additional learning process, all the connection weights        of the second (testing) domain, and 2 were used to signal the
                                                               start and stop of sentences. 10 hidden units were used.
                                                          1192

resembling both ABA and ABB grammars (for                         Dienes, Z., Altmann, G. T. M., Gao, S. J. (1999).
instance, three consecutive sentences of the form                   Mapping across domains without feedback: A neural
Noun1 Verb1, Noun1 Verb2 Noun2, Noun2 Verb3                         network model of implicit learning, Cognitive
resemble an ABA pattern followed by an ABB pattern:                 Science, 23, 53-82
Noun1 Verb1 Noun1, Verb2 Noun2 Noun2), and the                    Elman, J. L. (1999). Generalization, rules, and neural
network became attuned to both grammars. Later,                     networks: a simulation of Marcus et al. ,
during the second habituation phase, the model became               http://www.crl.ucsd.edu/~elman/Papers/MVRVsim.ht
increasingly biased towards the most recently trained               ml
grammar. This grammatical bias would then explain                 Fahlman, S. E., & Lebiere, C. (1990). The cascade-
why it predicted the (most recently) familiar patterns              correlation learning architecture, Advances in Neural
better than unfamiliar ones.                                        Information Processing Systems, 2, Los Altos, CA:
                                                                    Morgan Kaufmann, 524-532
                        Conclusion                                Marcus, G. F., Vijayan, S., Bandi Rao, S., Vishton, P.
                                                                    M. (1999). Rule learning by seven-month-old infants,
In the foregoing, we have examined two connectionist
                                                                    Science, 283, 77-80
models that were specifically designed to simulate
                                                                  Shultz, T. R. (1999). Rule learning by habituation can
Marcus et al.’s experiment on infants (1999). We
                                                                    be simulated in neural networks, Proceedings of the
considered to what degree these models were able to
                                                                    21 st Annual Conference of the Cognitive Science
actually learn the grammars involved. Although we
                                                                    Society (pp. 665-670), Mahwah NJ: Lawrence
believe that the results reported by Marcus et al. (1999)
                                                                    Erlbaum
do not necessarily lead to the conclusion that the infants
                                                                  Shultz, T. R., & Bale, A. C. (2001). Neural network
learned the grammars 2 , we wanted to see whether those
                                                                    simulation of infant familiarization to artificial
connectionist models were capable of doing more than
                                                                    sentences: rule-like behavior without explicit rules
just replicate infants’ results and to generalize both
                                                                    and variables, Infancy, 2, 501-536
within and outside their training sets.
                                                                  Vilcu, M., & Hadley, R. F. (2001). Generalization in
   In contrast with Altmann & Dienes’ model (1999),
                                                                    simple recurrent networks, Proceedings of the 23rd
we found Shultz & Bale’s (2001) very close to
                                                                    Annual Conference of the Cognitive Science Society
matching infants’ performances in the Marcus et al.’s
                                                                    (pp. 1072-1077), Mahwah NJ: Lawrence Erlbaum
study (1999). However, the model fell short of learning
the syntactic structure of the input patterns. We
performed numerous experiments on their model, using
various input patterns and grammars. Our results
showed that their network was only driven by the
numerical contours of the training patterns, and not by
the generality of grammatical structure. Therefore, we
conclude that Shultz & Bale’s model (2001) behaves
like a shape recognition system, and not like a robust
model that is capable of learning grammars.
   Regarding Altmann & Dienes’ model (1999) we were
not able to replicate their results, and we believe their
model lacks consistency and robustness. We ran many
experiments on their network, using various learning
parameters, but we could not even mirror the infants’
results. Because of this, we believe this model does not
learn the syntactic structure of input patterns, and it is
unable to generalize to novel items.
                        References
Altmann, G. T. M. (2002). Learning and development
   in neural networks – the importance of prior
   experience, Cognition, 85 (2), 43-50
Altmann, G. T. M, & Dienes, Z. (1999). Rule learning
   by seven-month-old infants and neural networks,
   Science, 284, 875a
2
  Some connectionists argue that infants could just detect
whether the last two syllables in a sentence are identical, and
they do not necessarily implement a rule.
                                                             1193

