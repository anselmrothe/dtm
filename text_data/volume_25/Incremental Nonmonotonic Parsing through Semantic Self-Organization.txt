UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Incremental Nonmonotonic Parsing through Semantic Self-Organization
Permalink
https://escholarship.org/uc/item/20c5h234
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Mayberry III, Marshall R.
Miikkulaainen, Risto
Publication Date
2003-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

          Incremental Nonmonotonic Parsing through Semantic Self-Organization
                                           Marshall R. Mayberry, III (martym@cs.utexas.edu)
                                                  Risto Miikkulainen (risto@cs.utexas.edu)
                                                         Department of Computer Sciences
                                                    The University of Texas, Austin, TX 78712
                                Abstract                                     nen, 1999), or foregoing parse structures altogether in order
                                                                             to concentrate on more tractable subproblems such as clause
   Subsymbolic systems have been successfully used to model                  identification (Hammerton, 2001) and grammaticality judge-
   several aspects of human language processing. Subsymbolic                 ments (Lawrence et al., 2000; Allen and Seidenberg, 1999;
   parsers are appealing because they allow combining syntactic,
   semantic, and thematic constraints in sentence interpretation             Christiansen and Chater, 1999). However, a promising new
   and revising that interpretation as each word is read in. These           approach scales up a detailed artificial grammar to reflect fre-
   parsers are also cognitively plausible: processing is robust and          quency of structures from the Penn Treebank (Marcus et al.,
   multiple interpretations are simultaneously activated when the            1993) to account for a wide variety of psycholinguistic phe-
   input is ambiguous. Yet, it has been very difficult to scale them
   up to realistic language. They have limited memory capacity,              nomena (Rohde, 2002).
   training takes a long time, and it is difficult to represent linguis-        Why is subsymbolic parsing a desirable goal? The main
   tic structure. In this study, we propose to scale up the subsym-          promise for both cognitive modeling and engineering is that
   bolic approach by utilizing semantic self-organization. The
   resulting architecture, INSOMN ET, was trained on seman-                  it accurately accounts for the holistic nature and nonmono-
   tic representations of the newly-released L IN GO Redwoods                tonicity of natural language processing. Over the course of
   HPSG Treebank of annotated sentences from the VerbMobil                   the parse, the network maintains a holistic parse representa-
   project. The results show that INSOMN ET is able to accu-                 tion at the output. Words processed later in a sentence can
   rately represent the semantic dependencies while demonstrat-              change the developing representation so that the network can
   ing expectations and defaults, coactivation of multiple inter-
   pretations, and robust parsing of noisy input.                            recover from incorrect earlier decisions. This way, the net-
                                                                             work can more effectively resolve lexical ambiguities, attach-
                            Introduction                                     ments, and anaphoric references during the course of pars-
                                                                             ing. Indeed, multiple interpretations are maintained in paral-
A number of researchers utilize neural network (i.e., subsym-
                                                                             lel until disambiguating information is encountered in the in-
bolic) models to gain insight into human language processing.
                                                                             put stream (cf. Onifer and Swinney, 1981; MacDonald et al.,
Such systems develop distributed representations automati-
                                                                             1992; MacDonald, 1993). This is evidently how humans pro-
cally, giving rise to a variety of interesting cognitive phenom-
                                                                             cess natural language, what good parsers should do, and what
ena. For example, neural networks have been used to model
                                                                             subsymbolic parsers promise to deliver.
how syntactic, semantic, and thematic constraints are seam-
lessly integrated to interpret linguistic data, lexical errors re-              The purpose of the present study is to show that deep se-
sulting from memory interference and overloading, aphasic                    mantic parsing of sentences from real-world dialogues is pos-
and dyslexic impairments resulting from physical damage, bi-                 sible using neural networks: a subsymbolic system can be
ases, defaults and expectations that emerge from training his-               trained to read a sentence with complex grammatical structure
tory, as well as robust and graceful degradation with noisy and              into a holistic representation of the semantic features and de-
incomplete or conflicting input (Allen and Seidenberg, 1999;                 pendencies of the sentence. This research breaks new ground
McClelland and Kawamoto, 1986; Miikkulainen, 1997, 1993;                     in two important respects. First, the model described in this
Plaut and Shallice, 1993; St. John and McClelland, 1990).                    paper, the Incremental Nonmonotonic Self-Organization of
   Yet, despite their many attractive characteristics, neural                Meaning Network (INSOMN ET), is the first subsymbolic
networks have proven very difficult to scale up to parsing re-               system to be applied to deep semantic representations derived
alistic language. Training takes a long time, fixed-size vec-                from a hand-annotated treebank of real-world sentences. Sec-
tors make learning long-distance dependencies difficult, and                 ond, whereas almost all previous work has focused on the
the linguistic formalism used can impose architectural con-                  representation and learning of syntactic tree structures (such
straints, such as binary parse trees that are very deep and force            as those in the Penn Treebank), the semantic representations
more information to be compressed in higher nodes, thereby                   taken up in this study are actually dependency graphs. The
making the sentence constituents harder to recover. Progress                 challenge of developing a subsymbolic scheme for handling
has been made by introducing a number of shortcuts such as                   graph structures led to self-organizing the case-role frames
concentrating on small artificial corpora with straightforward               that serve as the graph nodes. This semantic self-organization
linguistic characteristics (Berg, 1992; Ho and Chan, 2001;                   in turn results in a number of interesting cognitive behaviors
Sharkey and Sharkey, 1992), building in crucial linguistic                   that will be analyzed in this paper.
heuristics such as Minimal Attachment and Right Associa-                        The INSOMN ET parser combines a standard Simple Re-
tion (Lane and Henderson, 2001; Mayberry and Miikkulai-                      current Network (SRN; Elman, 1990) with a map of input
                                                                         798

                  I have got time in the morning .
                                                                            MRS in detail. Instead, we illustrate how MRS is used in IN-
                              h0: prop                                      SOMN ET by example. The MRS dependency graph for the
                               SA                                           sentence I have got time in the morning is shown in figure 1.
                    A1                                                      This representation consists of 12 frames connected with arcs
         x0: FRI          h1: have   in
                                                                            whose labels indicate the type of semantic dependency.
      BV      IX           A3           A0     A3                              The MRS graph in figure 1 can be rendered as a set
   h2: def          h3: I
                                EV
                                                            BV
                                                                            of frames to serve as targets for INSOMN ET (figure 2).
             RE
                                      e0: EV
                                                   x2: FRI      h6: the     Each frame has the form [ Handle | Semantic-Relation |
                      x1: FRI                  A3          IX      RE       Subcategorization-Type | Argument-List ]. For example, the
                                        A0                                  graph node labeled h1 in the middle of figure 1 is given as the
                     BV       IX                           h7: morning
                                                                            frame [ h1 | have | A1/A3/EV | x0 x1 e0 ] in figure 2. The first
                 h4: udef
                             RE
                                   h5: time in                              element, h1, is the Handle (node label) of the frame: other
Figure 1: MRS Dependency Graph. This graph represents the                   frames can include this handle in their slots, representing a
sentence I have got time in the morning. The top node in the graph,         dependency to this frame. For instance, h1 fills the state-of-
labeled h0, has value prop, which tells us that this is a declarative       affairs (SA) slot in the topmost node, h0 prop, as indicated
sentence. The have node is the main predication of the sentence,            by the labeled arc in figure 1 (also shown in detail in 2). The
and so serves as the state-of-affairs (the SA arc) for the prop. The
subject (arg1 in MRS; here A1) of have is a full referential index          second element, have, gives the Semantic-Relation (the node
(FRI, with features such as gender, number, and person not shown            value) that this frame represents. The third, A1/A3/EV, rep-
in the figure). It is also the instance (IX) of the I node, and the         resents the Subcategorization-Type and is shorthand for the
bound variable (BV) of the determiner, def node, that governs the I         arguments that the semantic-relation takes. In this case, it in-
node (indicated by the restriction arc, RE). Similarly, the direct ob-      dicates that have is a transitive verb with three arguments:
ject (here A3), of have is also a FRI, the instance of the time node.
Third, have has an event arc EV that refers to an EV node (with             subject A1, object A3, and event EV. The arc labels them-
features, such as aspect, mood, and tense not shown in the figure).         selves are abbreviations for MRS argument names (e.g., A1
There is one final set of nodes: morning, governed by the deter-            is arg1, EV is event, BV is bound variable). The rest of the
miner the, also has an instance that is an object of the preposition        frame x0 x1 e0 lists the handles (fillers) for these arguments.
in. This sentence is ambiguous: in one interpretation the preposition
attaches to the verb have (it is “in the morning” when I have time),        These handles refer to the other nodes in the MRS graph.
and another it attaches to the preceding noun time (it is “time in the         It is important to point out two main properties of handles.
morning” that I have). The two senses are illustrated in the figure         First, a given handle does not uniquely identify a case-role
by literally attaching a node with in to the have node and to the time      frame (node) in the MRS graph. Rather, it can be used to
node. Upon disambiguation, one or the other of these interpretations        refer to several frames. This convention allows represent-
would be selected, but both remain coactivated until then. The dis-
tinction is made in MRS by node-sharing (attachment) and by the in          ing linguistic relations that are optional (in both where and
node’s A0 arc, which points to the EV node in the verb-attachment           whether they may occur), or that may occur more than once
case, or to the time node’s instance FRI in the noun-attachment case.       and therefore may require more than one frame to represent,
                                                                            such as adjuncts (as in the example above), modifiers (such as
words (SARDN ET; Mayberry and Miikkulainen, 1999) and                       adjectives and relative clauses), or verb-particle constructions
a novel self-organized output representation for semantic de-               (e.g., “work something out”). Internally, we do use a unique
pendency graphs. The parser was trained on the Minimal                      designator called a subhandle (which is not part of the MRS
Recursion Semantics (MRS; Copestake et al., 2001) repre-                    formalism) to refer to each frame uniquely.
sentations of sentences from the L IN GO Redwoods Head-                        The second property illustrates an important difference be-
driven Phrase Structure Grammar (HPSG) Treebank (Öpen                      tween symbolic and subsymbolic representations. In the orig-
et al., 2002) under development at the Center for the Study                 inal MRS specification, the handles are arbitrary designators
of Language and Information (CSLI) at Stanford Univer-                      (e.g., the label h1 has no meaning in itself). However, in
sity. This treebank incorporates deep semantic descriptions                 the approach taken in this study, the handles are represented
of sentences taken from the recently completed VerbMobil                    as patterns of activation. These patterns are learned during
project (Wahlster, 2000). We report the performance of the                  training so that the handles actually come to encode semantic
network on the MRS dependency graphs, and illustrate its                    structure. In our example, for instance, the handle h1 actually
cognitive plausiblity on prepositional phrase attachment, ex-               refers to the two frames, [ have | A1/A3/EV | x0 x1 e0 ] and
pectations and defaults, and robustness to dysfluencies and                 [ in | A0/A3 | e0 x2 ]. The pattern corresponding to the han-
grammatical errors in the input. The results demonstrate that               dle h1 is obtained as the average of the subhandles of these
subsymbolic systems can achieve incremental, nonmonotonic                   two frames. The subhandle representations are in turn formed
semantic parsing of sentences of realistic complexity.                      by Recursive Auto-Associative Memory (RAAM; Pollack,
                                                                            1990) of the dependency graph. Starting from the semantic
                   Sentence Representation                                  features at the leaves, this process allows the subhandles to be
In order to process a sentence from the Redwoods Tree-                      generated recursively for each node in the graph. This encod-
bank into its proper semantic representation, we need to be                 ing process results in subsymbolic handles that are similar for
able to represent semantic dependency graphs. These are                     similar structures which allows the system to generalize well
acyclic graphs that represent the Minimal Recursion Seman-                  to new sentences.
tics (MRS; Copestake et al., 2001) interpretation of the sen-
tence. MRS is a flat representation scheme where nodes rep-                                   Network Architecture
resent case-role frames and arcs represent dependencies be-                 The INSOMN ET sentence parsing architecture (figure 2)
tween them. Unfortunately space does not permit reviewing                   consists of four components:
                                                                        799

                               "I have got time in the morning ."
                                        SARDNet input map
                       SRN input layer                        SRN previous hidden layer
                            morning
                                          SRN hidden layer
                                                                                         decoded frames
                                          Self−organized
                                            Frame Map
                                                                                h0         prop          SA          h1
                       Frame Nodes                                                            .
                                                                               h5          time         IX           x1
                                                                  Shared Frame Node Decoder weights
                                                       h1         have      A1/A3/EV         x0           x1          e0
 Figure 2: The I N S OM N ET Architecture. This snapshot shows the network at the end of reading the sentence I have got time in the
 morning, together with three of the decoded MRS frames corresponding to the words have, time and the topmost prop. The shaded units
 represent unit activations between 0.0 and 1.0. The SRN component reads in the sentence one word at a time. The representation for the
 current input word morning is shown at the top left. A unit corresponding to the current input word is activated on the SARDN ET input map
 (at top center) at a value of 1.0 and the rest of the map is decayed by a factor of 0.9. The three output frames shown in the figure are actually
 decodings of the patterns (the multi-shaded squares) in the Frame Map (bottom center). The other patterns in the Frame Map correspond to
 the other nodes in the full MRS dependency graph shown in figure 1; their decodings are not shown to save space. Processing in the network
 proceeds as follows: as each word is read into the input buffer, it is both mapped onto the input map and propagated to the hidden layer. A
 copy of the hidden layer is then saved (as the previous hidden layer) to be used during the next time step. The hidden layer is propagated to
 the Frame Map, which is a 16 × 16 map of Frame Nodes, each consisting of 100 units (shown here as 3 × 3 patterns). The units in each Frame
 Node are connected through a set of shared weights that comprise the Frame Node Decoder to an output layer representing a case-role frame.
 In this way, the Frame Map can be seen as a second hidden layer. Thus, for example, the Frame Node in the top right of the map decodes into
 the case-role frame [ h1 | have | A1/A3/EV | x0 x1 e0 ]. The Frame Map is self-organized with the subhandles representing these case-role
 frame representations. Note that the argument slots and their fillers are bound together by virtue of the shared handle representation (such as
 h1 between have and the SA slot of prop).
1. A SRN trained with BPTT to read in the input sequence.                     occurs more than once, the next closest available unit is ac-
2. A SARDN ET map that retains an exponentially decaying                      tivated. Together with the current input and previous hidden
    activation of the input sequence.                                         layer, the SARDN ET is used as input to the hidden layer.
3. A Self-Organized Frame Map that encodes the MRS de-                        The SARDN ET identifies each input token exactly, informa-
    pendency graph.                                                           tion that would otherwise be lost in a long sequence of SRN
                                                                              iterations (Mayberry and Miikkulainen, 1999).
4. A Frame Node Decoder that generates the output frame
    representations.                                                             The self-organized Frame Map is the main innovation of
                                                                              INSOMN ET. Each node in the map itself consists of a num-
    The Simple Recurrent Network (SRN; Elman, 1990) is the                    ber of units. As a result of processing the input sequence,
 standard neural network architecture for sequence process-                   a number of these nodes will be activated, that is, a par-
 ing, and it forms the basis for the INSOMN ET architecture                   ticular pattern of activation appears over the units of these
 as well. The SRN reads a sequence of distributed word rep-                   nodes. Through the weights in the Frame Node Decoder,
 resentations as input and forms the MRS dependency graph                     these patterns are decoded into the corresponding MRS case-
 of the sentence at the output. At each time step, a copy of the              role frames. The same weights are used for each node in
 hidden layer is saved and used as input during the next step,                the map. This weight-sharing enforces generalization among
 together with the next word. In this way, each new word is in-               common elements across the many frames in any given MRS
 terpreted in the context of the entire sequence read so far, and             dependency graph.
 the final parse result is gradually formed at the output. A par-                The Frame Map is self-organized based on the subhandle
 ticularly effective variant of the SRN uses backpropagation-                 representations. This process serves to identify which nodes
 through-time (BPTT; Williams and Zipser, 1989; Lawrence                      in the Frame Map correspond to which case-role frames in the
 et al., 2000) to improve the network’s ability to process longer             MRS structure. Because the subhandles are distributed repre-
 sequences. With BPTT, the SRN is effectively trained as if it                sentations of case-role frames, similar frames will cluster to-
 were a multi-layer feedforward network, with the constraint                  gether on the map. Determiners will tend to occupy one sec-
 that the weights between each layer are shared.                              tion of the map, the various types of verbs another, nouns yet
    The SARDN ET is included to solve the long-term mem-                      another, and so on. However, although each node becomes
 ory problem of the SRN. SARDN ET is a self-organized map                     tuned to particular kinds of frames, no particular Frame Node
 of word representations (James and Miikkulainen, 1995). As                   is dedicated to any given frame. Rather, through different ac-
 each word from the input sequence is read in, its correspond-                tivation patterns over their units, the nodes are flexible enough
 ing unit in the map is activated at a value of 1.0, and the rest             to represent different frames, depending on what is needed
 of the assembly decayed by a factor of 0.9. If an input word                 to represent the input sequence. For example in figure 2, the
                                                                       800

Frame Map node toward the upper right corner decodes to the           1
[ h1 | have | A1/A3/EV | x0 x1 e0 ] case-role frame for this        0.9
particular sentence. In another sentence, it could represent a
                                                                    0.8
different verb with a slightly different subcategorization type.
This feature of the architecture makes the Frame Map able to        0.7
represent semantic dependency graphs dynamically, enhanc-
                                                                    0.6                                                        N
ing generalization.                                                                                                            T
                                                                                                                               F
   During training of the SRN, the Frame Node serves as a           0.5                                                        H
                                                                                                                               X
                                                                                                                               A
second hidden layer and the case-role frames as the output          0.4
                                                                                                                               S
layer. The appropriate frames are presented as targets for the
Frame Node layer, and the resulting error signals are back-         0.3
propagated through the Frame Node Decoder weights to the            0.2
Frame Node layer and on up to the first hidden layer.
                                                                    0.1
   At the same time, a RAAM network is trained to form
the subhandle representations, and the current representa-            0
                                                                        0        200      400        600        800       1000       1200
tions are used to organize the Frame Map. The input word
representations are developed as part of the SRN training          Figure 3: Sentence Processing Performance. The average pro-
using the FGREP method (Forming Global Representations             portion of frame constituents in the test set that were correctly pro-
                                                                   duced by INSOMNet over four splits of the data during the course
with Extended Backpropagation; Miikkulainen, 1993) and             of training are shown here, broken down by the constituent type.
the SARDN ET map is self-organized with the current repre-         The easiest for the network to learn were the arguments that had
sentations. Eventually all these representations converge, and     no fillers (“N”), subcategorization types (“T”), and features (“F”),
the networks learns to generate the correct MRS dependency         all clustered near the top of the graph. The network also had lit-
                                                                   tle trouble generalizing the handles (“H”). More difficult were the
graph and the corresponding case-role frames as its output.        filled arguments (“A”), and the most troublesome were the semantic
                                                                   (“S”) representations, presumably due to their sparsity in the data.
       Input Data, Training, and Experiments                       The “X” curve (black squares) gives the average of all these com-
                                                                   ponents. After 1200 epochs, the average performance was just over
The subsymbolic word representations developed by FGREP            93%. The performance on the training set was 95%, indicating that
capture how the words are used in the sentences, and               the network indeed generalizes very well.
therefore serve as semantic representations in themselves.
For this reason, the FGREP representations for the input           Subcategorization-Type, Features, and Arguments, as well as
words were used also as the fillers for semantic-relations         Null fillers for those arguments that are not realized in the
in the MRS frames. For instance in our running exam-               case-role frame. The main result is that the network is able
ple, the original semantic relations have rel, time mass rel,      to generate detailed MRS representations in its output. It
 def morning rel, and in temp rel were replaced by the in-         performs very well on all components except semantic rela-
put words have, time, morning, and in, respectively. These         tions, which is not surprising since the data was more sparse
changes reduced the lexicon from over 1100 tokens to just          with respect to semantic relations than the other components.
over 600. All other tokens, such as the semantic relations that    Overall, 93% of the target MRS tokens were correctly gener-
do not correspond to an input word (e.g., prop and def), as        ated, suggesting that the network had indeed learned to parse
well as the 40 subcategorization types (e.g., A0/A3/EV) and        sentences into MRS dependency graphs.
the basic semantic features that occurred in the corpus, were           The most interesting behavior of INSOMN ET, however,
given random representations. All the representations (both        takes place on top of generating the correct output in the end.
FGREP and random) were 40-dimensional vectors between 0            It is these behaviors that make INSOMN ET a potentially use-
and 1.                                                             ful cognitive model.
   All morphemes were represented as separate tokens in the             First, the parsing process is incremental and nonmono-
input sequence. For example, in the sentence it look -s like       tonic. As words are read in, the patterns in the Frame Map
i am go -ing to be pretty busy, the morphemes -s and -ing          fluctuate according to the network’s current interpretation as
are processed in separate steps. Such preprocessing is not         well as its expectation of how the sentence will continue. In
strictly necessary, but it allows focusing the study on semantic   particular, the network can revise its interpretation as it reads
processing without confounding it with morphology.                 more of a sentence in, sometimes to the point of deactivating
   A total of 4000 sentences from the Redwoods corpus were         some frames and activating others.
used: 3200 for training, and the remaining 800 for testing.             Second, INSOMN ET represents ambiguities explicitly,
The shortest sentences had five frames in their MRS repre-         which is apparently also how humans do it in the absence
sentation, the longest had 25. Four separate random splits of      of contextual clues. Several psycholinguistic studies suggest
the data were used to test the INSOMN ET’s performance, as         that multiple interpretations can be coactivated in parallel in
will be described in the next section.                             the face of various types of ambiguity (Onifer and Swinney,
                                                                   1981; MacDonald et al., 1992; MacDonald, 1993). Indeed,
                              Results                              there is evidence that prepositional phrases may modify sev-
Figure 3 shows the average performance on the test set             eral words at the same time (Schütze, 1997). Our recurring
over the four splits measured as the proportion of fillers         example, I have got time in the morning can be used to
generated correctly. Separate plots are shown for the dif-         illustrate this behavior in INSOMN ET as well. Regardless
ferent MRS components, i.e., Handles, Semantic-Relations,          of whether this sentence is new to INSOMN ET (as it was
                                                               801

                       "I have got time in the morning ."
                    morning
                                    Frame Map                                    decoded frames
                                                              h1        have       A1/A3/EV       x0         x1        e0
                                                              h1          in         A0/A3        e0         x2
                                                              h5          in         A0/A3        x1         x2
                                                              h5        time          IX          x1
                                                              h7      morning         IX          x2
Figure 4: Representing Ambiguity. The sentence I have got time in the morning is an example of an ambiguous prepositional phrase
attachment as was shown in figure 1. Both interpretations (i.e., the preposition in attached either to the verb have or to the noun time)
are actually present in the Redwoods corpus, although in separate sentences. The network learns to exhibit both possibilities. The more
likely attachment (i.e., to the verb), yields a preposition frame in with the same handle h1 as the verb frame have to which it attaches. The
other possibility is also activated: in this case the preposition frame shares a handle h5 with the noun frame time. Allowing such multiple
representations to be explicitly activated is one of the main advantages of the Frame Map component of INSOMN ET.
in two of the splits), or INSOMN ET was trained to inter-                    it will substitute a more frequent analogue. Both expecta-
pret it in only one way (i.e., as a noun-attachment or a verb-               tions and semantic illusions are common in human natural
attachment, as in the other two splits), it processes the sen-               language understanding and arise automatically in the IN-
tence the same way: both possible attachments are activated                  SOMN ET model.
in the map (figure 4). Because some sentences in the Red-
woods corpus have noun-attached prepositional phrases while                                 Discussion and Future Work
others have verb attachments, the network properly general-                  The ultimate goal of this research is to develop a subsymbolic
izes to represent both possibilities. This way, INSOMN ET                    parser that can handle realistic language without sacrificing
explicitly activates multiple interpretations for an ambiguous               those characteristics of neural networks that make them pow-
input. This behavior is cognitively valid, but has been diffi-               erful cognitive models. The described method of representat-
cult to capture in artificial parsing systems in general.                    ing MRS dependency graphs permits the network to gradually
   A third significant cognitive feature of INSOMN ET is its                 refine its output to accommodate changes as new information
robustness. A new filler, “um”, not in the original lexicon and              comes in. In this paper, we have shown that this behavior
assigned a random representation, was added to all sentences                 can be preserved while scaling up to the realistic linguistic
in both the training and test sets at random locations. One                  structures present in the L IN GO Redwoods Treebank.
of the networks trained on the Redwoods corpus discussed                        Our future work focuses on three further important devel-
above was then tested on both these new, dysfluent sets, and                 opments of INSOMN ET. First, we will augment the model
performed virtually the same despite this modified input: all                with a gating mechanism that modulates the activations of the
of the MRS case-role frames were properly generated at the                   Frame Node patterns. Preliminary experiments show that this
output, although their activation levels were somewhat de-                   mechanism dramatically enhances the nonmonotonic behav-
graded in some cases. Additionally, besides the grammatical                  ior of INSOMN ET. In particular, gating suppresses the acti-
errors already present in a very few sentences in the Red-                   vations of Frame Nodes that should not be a part of the MRS
woods corpus (e.g., the sentence “here is some clues”), we’ve                dependency graph while at the same time providing a soft
run some preliminary studies wherein we’ve replaced an in-                   threshold for relevant nodes. These experiments also indicate
put word with an ungrammatical variant differing in an agree-                that gating also accentuates coactivation of multiple interpre-
ment feature such as number or person, as well as deleted ran-               tations, as well as expectations and defaults, which will allow
dom articles like “a” and “the”. Early results also show that                a more quantitative assessment of these behaviors.
the network is scarcely affected by these errors because they                   Second, we will replace the tokens in the input with either
occur so infrequently compared to its training history. These                orthographic or phonological representations. The strong ten-
results suggest that INSOMN ET can tolerate noisy, dysflu-                   dency of INSOMN ET to create expectations and its general
ent, and ungrammatical input much like people do.                            robustness should then allow it to process unknown words
   Fourth, the network demonstrates expectations and de-                     systematically. At the same time, the network should also
faults which have become a hallmark of subsymbolic sys-                      learn to identify morphological components in its input rep-
tems. Because the network is trained to output the full rep-                 resentations and map them onto their proper semantic targets,
resentation of the MRS semantic dependency graph, it learns                  removing the need for preprocessing the input data.
to anticipate certain frames before they have been licensed by                  Third, we plan to test INSOMN ET as a robust system
the input. Similarly, the network exhibits defaults and even                 for parsing spoken language. They system will be trained
semantic illusions: when it misses a component in a frame,                   with the actual transcripts in the VerbMobil corpus, which in-
                                                                      802

clude dysfluencies of everyday spoken language, such as false     MacDonald, M. C., Just, M. A., and Carpenter, P. A. (1992).
starts, repairs, hesitations, and fillers. We expect the system     Working memory constraints on the processing of syntactic
to learn their structure, and to learn to compensate for them       ambiguity. Cognitive Psychology, 24:56–98.
in the sentence interpretation. If so, INSOMN ET could serve      Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A.
as a significant step towards scaling up semantics parsing to       (1993). Building a large annotated corpus of English: The
the real world.                                                     Penn treebank. Computational Linguistics, 19:313–330.
                                                                  Mayberry, III, M. R. and Miikkulainen, R. (1999). Using a
                          Conclusion                                sequential SOM to parse long-term dependencies. In Pro-
In this paper, we presented a subsymbolic parser, INSOM-            ceedings of the 21st Annual Conference of the Cognitive
N ET, that is able to parse a real-world corpus of sentences        Science Society, pages 367–372. Hillsdale, NJ: Erlbaum.
into semantic representations. A crucial innovation was to use    McClelland, J. L. and Kawamoto, A. H. (1986). Mechanisms
an MRS dependency graph as the sentence representation, en-         of sentence processing: Assigning roles to constituents. In
coded in a self-organized Frame Map. As is typical of holistic      McClelland, J. L. and Rumelhart, D. E., editors, Parallel
parsers, the parse result is developed nonmonotonically in the      Distributed Processing: Explorations in the Microstruc-
course of incrementally reading in the input words, thereby         ture of Cognition, Volume 2: Psychological and Biological
demonstrating several cognitive behaviors such as coactiva-         Models, pages 272–325. MIT Press, Cambridge, MA.
tion, expectations and defaults, and robustness. These prop-      Miikkulainen, R. (1993). Subsymbolic Natural Language
erties make INSOMN ET a promising foundation for under-             Processing: An Integrated Model of Scripts, Lexicon, and
standing human sentence processing in the future.                   Memory. MIT Press, Cambridge, MA.
                                                                  Miikkulainen, R. (1997). Dyslexic and category-specific im-
                                                                    pairments in a self-organizing feature map model of the
                          References                                lexicon. Brain and Language, 59:334–366.
Allen, J. and Seidenberg, M. S. (1999). The emergence of          Onifer, W. and Swinney, D. A. (1981). Accessing lexical am-
   grammaticality in connectionist networks. In MacWhin-            biguities during sentence comprehension: Effects of fre-
   ney, B. J., editor, Emergence of Language, pages 115–151.        quency of meaning and contextual bias. Memory and Cog-
   Erlbaum, Hillsdale, NJ.                                          nition, 9:225–226.
Berg, G. (1992). A connectionist parser with recursive sen-       Öpen, S., Flickinger, D., Toutanova, K., and Manning, C.
   tence structure and lexical disambiguation. In Swartout,         (2002). LinGO redwoods: A rich and dynamic treebank
   W., editor, Proceedings of the Tenth National Conference         for HPSG. In Beyond PARSEVAL. Workshop of the Third
   on Artificial Intelligence, pages 32–37. Cambridge, MA:          LREC Conference.
   MIT Press.                                                     Plaut, D. C. and Shallice, T. (1993). Perseverative and se-
Christiansen, M. H. and Chater, N. (1999). Toward a connec-         mantic influences on visual object naming errors in optic
   tionist model of recursion in human linguistic performance.      aphasia: A connectionist account. Journal of Cognitive
   Cognitive Science, 23(2):157–205.                                Neuroscience, 5(1):89–117.
Copestake, A., Lascarides, A., and Flickinger, D. (2001). An      Pollack, J. B. (1990). Recursive distributed representations.
   algebra for semantic construction in constraint-based gram-      Artificial Intelligence, 46:77–105.
   mars. In Proceedings of ACL-2001. Toulouse, France.            Rohde, D. T. (2002). A Connectionist Model of Sentence
Elman, J. L. (1990). Finding structure in time. Cognitive           Comprehension and Production. PhD thesis, Computer
   Science, 14:179–211.                                             Science Department, Carnegie Mellon University, Pitts-
Hammerton, J. (2001). Clause identification with long short-        burgh, PA. Technical Report CMU-CS-02-105.
   term memory. In Daelemans, W. and Zajac, R., editors,          Schütze, H., editor (1997). Ambiguity Resolution in Lan-
   Proceedings of CoNLL-2001, pages 61–63.                          guage Learning: Computational and Cognitive Models.
Ho, E. K. S. and Chan, L. W. (2001). Analyzing holistic             CSLI, Stanford University.
   parsers: Implications for robust parsing and systematicity.    Sharkey, N. E. and Sharkey, A. J. C. (1992). A modular de-
   Neural Computation, 13(5):1137–1170.                             sign for connectionist parsing. In Drossaers, M. F. J. and
James, D. L. and Miikkulainen, R. (1995). SARDNET: A                Nijholt, A., editors, Twente Workshop on Language Tech-
   self-organizing feature map for sequences. In Tesauro, G.,       nology 3: Connectionism and Natural Language Process-
   Touretzky, D. S., and Leen, T. K., editors, Advances in          ing, pages 87–96, Enschede, the Netherlands. Department
   Neural Information Processing Systems 7, pages 577–584.          of Computer Science, University of Twente.
   Cambridge, MA: MIT Press.                                      St. John, M. F. and McClelland, J. L. (1990). Learning
Lane, P. C. R. and Henderson, J. B. (2001). Incremental syn-        and applying contextual constraints in sentence compre-
   tactic parsing of natural language corpora with simple syn-      hension. Artificial Intelligence, 46:217–258.
   chrony networks. IEEE Transactions on Knowledge and            Wahlster, W., editor (2000). Verbmobil. Foundations of
   Data Engineering, 13(2):219–231.                                 Speech-to-Speech Translation. Springer-Verlag, Berlin;
Lawrence, S., Giles, C. L., and Fong, S. (2000). Natural            New York.
   language grammatical inference with recurrent neural net-      Williams, R. J. and Zipser, D. (1989). A learning algorithm
   works. IEEE Transactions on Knowledge and Data Engi-             for continually running fully recurrent neural networks.
   neering, 12(1):126–140.                                          Neural Computation, 1:270–280.
MacDonald, M. C. (1993). The interaction of lexical and
   syntactic ambiguity. Journal of Memory and Language,
   32:692–715.
                                                              803

