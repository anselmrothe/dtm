UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bayesian Satisficing Model of Human Adaptive Planning
Permalink
https://escholarship.org/uc/item/9093779g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Author
Fu, Wai-Tat
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          A Bayesian Satisficing Model of Human Adaptive Planning
                                             Wai-Tat Fu (wfu@andrew.cmu.edu)
                                                     Department of Psychology,
                                                     George Mason University
                                                   4400 University Drive, m/s 3f5
                                                      Virginia, VA 22030 USA
                           Abstract                                  A number of successful methods have been proposed
                                                                  to obtain the optimal level of planning given sufficient
  This paper presents a Bayesian satisficing model of when        information about the problem-solving environment
  a problem-solver stops planning and begins acting.              (Sun & Giles, 2001), but most of them adopt some
  Existing knowledge about the environment is                     variations of the dynamic programming approach that
  incrementally updated by new observations, and
  performance improves as a consequence of better
                                                                  requires extensive backward search to find the optimal
  knowledge about the environment. The model aims at              solution (e.g. Kopf, 1988). Although these methods are
  bridging the gap between machine learning and cognitive         often able to lead to optimal solutions, the computations
  science by adopting the bounded rationality framework           involved is clearly beyond the capacity of the human
  (Simon, 1956), which assumes that cognition tends to            cognitive system. Psychological research, however, did
  exploit the characteristics of the environment without          find that people are able to change their level of
  engaging in psychologically implausible computations.           planning in response to the characteristics of the
  Empirical studies were conducted when human subjects            environment to improve problem-solving performance
  learned to find the fastest path in a simple map (only one      (Kirsh & Maglio, Gunnzelman & Anderson, 2002). For
  was reported in this paper). The model fit the human
  learning and performance well and provided insights into
                                                                  example, Kirsh and Maglio found that expert players of
  the mechanisms behind learning and performance in               the interactive video game Tetris outperformed novice
  problem solving.                                                players by better cost-benefit tradeoffs between mental
                                                                  planning and external planning (information-seeking
                       Introduction                               actions). Gunzelmann and Anderson also found that
                                                                  their subjects increased the level of planning in the
Problem-solving research shows that people seldom
                                                                  Tower-of-Hanoi task as they learned that it increased
plan out complete sequences of actions before acting on
                                                                  the efficiency of their solutions. In summary, the results
the world (Hayes-Roth & Hayes-Roth, 1979; Roberson
                                                                  suggest that (1) people are sensitive to the cost and
& Black, 1986). Instead, problem solvers construct
                                                                  benefit of planning, (2) people learn to perform better
partial plans, execute the actions, and plan on further as
                                                                  cost-benefit tradeoffs through experience, and (3) good
they act. Problem-solving behavior can therefore be cast
                                                                  cost-benefit tradeoffs are often critical to performance.
somewhere along the planning continuum. At one
                                                                     The studies of how people perform cost-benefit
extreme, a single action is decided and executed based
                                                                  tradeoffs have been of great interests to researchers in
purely on the current state of the problem. This results
                                                                  the domains of problem solving and decision-making
in highly reactive behavior that can respond quickly to
                                                                  (Christensen-Szalanski, J. J. J., 1980; Beach, L. R., &
the changing external world. However, this may lead to
                                                                  Mitchell, T. R., 1978; Payne, Bettman, & Johnson,
inefficient solutions to the problem. At the other
                                                                  1993). Recently, a growing number of researchers have
extreme of the planning continuum, a complete
                                                                  adopted the rational approach to explain cost-benefit
sequence of actions is planned out before any action is
                                                                  tradeoffs in decision making behavior and strategy
executed. This results in highly efficient performance,
                                                                  selection (Anderson, 1990; Lovett & Anderson, 1996;
as the best sequence of actions can be chosen and
                                                                  O'Hara & Payne, 1998; Fu & Gray, 2000). The major
executed. However, the costs associated with planning
                                                                  assumption of the rational approach is that people are
often nullify the benefits of planning. Interleaving of
                                                                  well adapted to statistical characteristics of their
planning and acting allows a balancing of problem-
                                                                  environment, and computations in the human cognitive
solving performance and planning cost involved in the
                                                                  system perform in ways that are optimal in response to
search of actions.
                                                                  the demand of the environment. Under the rational
  This paper focuses on a class of problems that can be
                                                                  framework, planning should stop as soon as the cost of
solved relatively easily, but requires sufficient planning
                                                                  further planning exceeds the benefit that further
to find an efficient solution path. For example, finding a
                                                                  planning could bring. Since continued search of the
route from Pittsburgh to New York City is relatively
                                                                  problem space takes place at increasing cost, searching
easy (there are many possible routes to choose from),
                                                                  should stop and execution should begin once the
but finding the fastest route may require some planning
                                                                  expected benefit of further search drops below a certain
ahead (to find out traffic conditions in different routes).
                                                                  level. At this point, the problem solver "satisifices" on
                                                             420

the current level of planning with no guarantee that it is   performance, a decision criterion is used that allows the
the global best (Simon, 1956; Russell & Wefald, 1991).       person to choose actions based on the existing
Similarly, biologists have long entertained the              knowledge of the environment. Specifically, when
hypothesis that animals forage for resources in a near-      deciding how much planning one should do during
optimal manner, given the distribution and                   problem solving, the model has a mechanism that
replenishment rate of the resources, and the energy cost     updates the knowledge about the relationship between
to obtain the resources (Krebs & Davis, 1978). For           the amount of planning and the execution cost of
example, hummingbirds have been shown to forage              actions, and a decision criterion on when to stop
flowers in a region until the rate of return is below the    planning given the existing knowledge.
average for all flowers, and then forage another regions        The model assumes that the problem-solver updates
with greater-than-average return (Pyke, 1978).               the existing knowledge about the environment through a
Learning and performance                                     Bayesian combination of new information from the
   One useful approach to study how people adapt their       environment and the existing knowledge. For
behavior to the contingencies of a situation is              simplicity, some assumptions about the environment are
reinforcement learning. During reinforcement learning,       made. First, it is assumed that the problem is solved by
the problem solver learns by observing the                   the combination of general heuristics (such as hill-
consequences of their actions over time, and improves        climbing) and special heuristics based on information
their choice of actions with experience. One of the          obtained from planning. It is assumed that special
challenges that arises in reinforcement learning and not     heuristics are likely to be more efficient than general
in other kinds of learning is the tradeoff between           heuristics, and the more planning the problem solver
exploration and exploitation. In reinforcement learning      does, the less effort will be required to accomplish the
situations, the learner has to take both learning and        task. It is also assumed that the problem can always be
performance into account – the problem solver has to         solved (even with no planning). All other possible
learn about the environment to improve performance in        variables that may influence the perception of the
the long run and take advantage of the knowledge             amount of effort required to solve the problem are
gained to improve immediate performance. A number            assumed to be constant.
of computational methods have been explored (e.g.               Learning in the Bayesian satisficing model is
Sutton & Barto, 1998) to investigate the efficiency of       concerned with estimating the amount of effort (the
learning from interacting with the environment from the      acting costs) required to solve the problem from
machine learning perspectives. The current paper             information observed from the environment. The model
attempts to bridge the gap between computational             assumes that the function describing the amount of
methods in machine learning and the rational approach        effort required to solve the problem has an exponential
in cognitive science by assuming that the human              relationship with the number of steps of planning (n).
cognitive system is well adapted to the demands of the       Mathematically, f(n,B) can be calculated by
environments. By making these assumptions, complex                                   n
                                                                               A −
computations can often be replaced by simple                     f (n, B) = e B (eq1: function of the amount of
mechanisms that exploit the structure of the                                   B
environment while maintaining the same level of                          effort required to solve the problem)
performance (e.g. Anderson, 1990; Gigerenzer &
Selten, 2000). Based on this assumption, the model           where A is a proportionality constant. B is related to the
attempts to characterize both learning and performance       amount of effort saved per each step of planning. For
when human subjects adapt to the environment with            example, for a given n, the higher the value of B, the
some simple, psychologically plausible mechanisms.           lower the value of f(n,B) will be. The exponential
                                                             distribution also has the characteristic that there is
         The Bayesian Satisficing Model                      diminishing return in the amount of effort one can save
The current analysis focuses on how optimal                  per each additional step of planning1.
performance can be achieved through an adaptation               To account for the randomness in the perception of
process in which information is incrementally                the actual effort a noise term σ is added to f(n,B). σ is a
accumulated from the environment. The adaptation             random variable following a normal distribution with
process has the dual goals of (1) learning the
characteristics of the environment and (2) improving         1
                                                               The exponential distribution implies unrealistically that with
performance by choosing better actions to solve the          sufficiently large n, the execution cost will be close to zero.
problem. For learning, the adaptation process has a          However, it is assumed that n will never be too large with the
mechanism that combines new information obtained             stopping criterion. Additionally, Figure 3 suggests that the
from the environment with existing knowledge; for            exponential distribution is a good approximation at least to the
                                                             specific task used in the experiment.
                                                         421

mean equals zero, and standard deviation equals t. The                                                   Problem-solver
value of t can be considered a free parameter. The                                       nopt,
                                                                                                            Learning
                                                                     Execution
uncertainties of B in equation 1 are represented by a                   cost           f(nopt|B)                f(nopt)p(B)
                                                                                                    p(B’) =
gamma distribution. The gamma distribution is a two-                                                         ∫f(nopt|B)p(B) dB
parameter general distribution that describes the
uncertainties of B in a general environment. The gamma              Environment
and exponential distribution are standard non-
informative distributions in Bayesian analysis (e.g. see                                                  Performance
Berger, 1985), which make minimal assumptions on the                    nopt                            f(n) - f(n+1) < wn
structure of the environment. However, based on these
assumptions, models of the mechanisms that respond              Figure 1. The Bayesian satisficing model – learning is
optimally to the characteristics of the environment can         through the Bayes’ theorem, and performance is based
be constructed.                                                 on an optimal stopping criterion. Execution cost is
   The optimal number of steps of planning (nopt) with          sampled from the environment and used as estimate for
respect to the current distributions of B can be                f(nopt |B) for each cycle of learning.
calculated. If wn is the cost of a unit step of planning,          The Bayesian satisficing model therefore nicely
the optimal decision rule to stop planning is when the          combines optimally learning and performance given the
cost of an additional step of planning exceeds its              characteristics of the environment. The Bayesian
expected benefit. Mathematically, planning will stop as         learning equation combines optimally the uncertainties
soon as n satisfies the following equation:                     in existing knowledge and uncertainties in new
                                                                observations. The stopping criterion also assumes that
      f(n-1) – f(n) < wn …..(eq2: the stopping criterion)       the decision on the level of planning is optimal given
After the execution of actions, the acting cost can be          the existing knowledge. These two components of the
obtained. Based on the information, the model updates           model therefore provide a unified account of learning
the distribution of B using Bayes’ theorem.                     and performance in adaptive planning.
Mathematically, if p(B') is the updated distribution, then
                                                                The Task
               f (nopt | B) p( B)                               A simple map-navigation task was chosen as shown in
  p( B' ) =                         (eq3: Bayesian learning)
            ∫ f (n opt | B) p( B)dB                             Figure 2. Simple hill-climbing strategies (i.e. no
                                                                planning) are always sufficient to finish the task, but it
which can be calculated for each nopt chosen according          does not guarantee to yield the fastest path. With
to the stopping criterion stated above. p(B') can then be       sufficient experience, one learns the speeds of different
used as the prior distribution of B for the next cycle of       routes and turns, and will be able to improve
selection of n opt, and p(B') can be calculated, and so on.     performance by a better choice of solution paths.
The updated distributions, with more information, will          Subjects are given a start station (the blue dot) and a
describe the environment better, and therefore generate         destination (the yellow dot) in each trial, and are asked
a better value for nopt. The adaptation process continues       to travel from the start station to the destination.
and nopt will approach a value that is tuned to the             Subjects can choose to go to any one of the adjacent
characteristics of the environment (see Figure 1).              stations directly connected to the current station. To go
   The above stopping criterion is a local decision rule        to one of the adjacent stations, subjects have to point the
that guarantees that performance is optimal given the           mouse cursor to the station, press and hold down the
existing knowledge about the environment. The                   mouse key. A red line will be drawn from the current
Bayesian learning process updates the global                    station (the red dot) to the station clicked. The speeds of
information, f(n), with new information obtained from           the train lines and the transfers are indicated by the
the result of the local decision rule. The Bayesian             speed of the movement of the red line. When the red
satisficing model therefore combines performance and            line reaches the station, the station turns red and
learning through an incremental update of knowledge of          becomes the current station.
the environment. Global representation of the                      Subjects can use the transfer at the intersection of the
                                                                train lines to change direction. When subjects are at a
environment is improved through Bayesian learning and
                                                                transfer station, he/she can go to another train line or
performance is improved through a simple local
                                                                stay on the same train line. Subjects are told that there
decision rule, which greatly simplifies the computations        are two kinds of transfers, the pink transfers and the
involved in many machine-learning approaches.                   orange transfers, and one of them is faster than the
                                                                other. However, they are not told which one is faster.
                                                                When the trial starts, the colors of the transfers are
                                                                covered (i.e. in black). The color of a transfer will be
                                                                shown when the subject is at the transfer station or
                                                                when the subject clicks on the transfer. As soon as the
                                                            422

experiment starts, the subject can check the color of any     transfer (a more comprehensive set of analyses can be
transfer in the map anytime before they reach the             found in Fu, 2003). Each subject solved 8 blocks of 8
destination. Figure 2 shows the two kinds of transfers        maps. The first seven maps of each block had 12 slow
randomly located in the map. The colors are uncovered         transfers and 4 fast transfers, and there was always one
for illustration purpose only. At any time during the         and only one path that contained only fast transfers, and
experiment, the subject can see at most one transfer          it was always the fastest path. Planning was necessary
uncovered.                                                    to find the fastest path (which uses only fast transfers)
   There are two major manipulations of the experiment        on these maps, and the shortest path was never the
– planning and acting costs. The task is constructed so       fastest path. These maps were called the round-about-
that increasing the amount of planning will decrease the      fastest maps. The locations of the start and end stations
cost of acting, and vice versa. Cost is measured by the       were randomized in all maps. The eighth map had no
time to completion of the task, which is the sum of the
                                                              fast transfer and will be called the all-slow maps. The
planning and acting costs. Given the design of the task,
                                                              all-slow maps showed how much planning the subjects
subjects have to trade-off planning costs and acting
costs to maximize performance.                                would do when they could not find any fast transfer, and
                                                              thus allowed the measure of how subjects' decisions to
                     Yellow = End                             stop planning differed in different conditions and how
                                                              they changed through experience.
                                                                 Before the experiment begins, each subject was given
   Pink Transfer                                              a practice trial. The Planning Cost was set according to
                                                              the condition the subject was in (i.e. either with or
                                                              without the 1-second lockout time). Subjects were told
     Red = Current                                            that the task was to go from the start station (the blue
                                                              dot) to the end station (the yellow dot) as fast as
                                                              possible, and they were timed during each map.
                                                              Subjects were told that there were two kinds of transfer,
                                                              one was orange and the other was pink, and that one
                            Blue = Start                      kind was faster than the other. However, they were not
Figure 2. The map used in the experiment. There are           told which one was faster. This was not to bias the
two kinds of transfers, one in orange and the other in        subject on the use of any one kind of the transfer. Half
pink. In the actual experiment, the colors of the transfers   of the transfers (eight) in the practice trial were orange
are covered. There are 4 pink transfers and 12 orange         and the other half (eight) were pink, but the actual speed
transfers in each map.                                        of the orange and pink transfers were the same in the
                                                              practice trial. They were shown how to go from one
   Planning is measured by the number of mouse clicks         station to another, as well as how to uncover the color
that check the colors of the transfers. The cost of           of the transfers. Subjects were then asked to solve the
planning is manipulated by adding a lockout time after        map by themselves. Subjects were instructed to solve
the transfer station is clicked. For example, a 1-second      each map as fast as possible.
lockout time requires the subject to hold down the
mouse button for one second before he/she can see the         Parameters for the environments
color of the transfer. The cost of acting is manipulated
by the speed of the slow transfers. When the slow             Simulations were conducted to estimate the relationship
transfer is much slower than the fast transfer, the effect    between the amount of planning (n) and the execution
of planning will be larger because using the fast transfer    cost (C) for map-navigation task (Figure 3). When no
allows the subject to solve the map much faster than          planning was done (i.e. n=0), the execution cost was
when using the slow transfer.                                 estimated to be the time to go from the start station to
                                                              the end station using the shortest path (i.e. hill-
The Experiment                                                climbing). For n>0, the following simulation was
The experiment was a 2x3 between-subject design. The          conducted to obtain the curves in Figure 3. First, one of
two between-subject independent variables were                the transfers on the same train line of the start station
Planning Cost (information access cost) and Acting            would be randomly selected. This would be counted as
Cost (difference between the speeds of the fast and slow      one planning step, and n will be incremented. For n=1,
transfers). (In the rest of the paper, the high Planning      planning would stop here. If the transfer selected was
Cost and high Acting cost condition will be referred to       fast, the transfer would be used; and from the selected
as Hi-Hi, the low Planning Cost and medium Acting             transfer, the hill-climbing heuristic would be used to
Cost condition will be referred to as Lo-Med, etc). The       find the path from the selected transfer to the end station
dependent variable was the amount of planning, as             (i.e the shortest path from the selected transfer to the
measured by the number of mouse clicks to uncover the         end station would be used). The execution cost could
                                                          423

then be calculated. If the selected transfer was slow,                                  Amount of Planning
                                                                                      12
then the shortest path from the start station to the end                                                               Subject High Cost
station would be selected. For n = 2, planning will                                                                    Subject Low Cost
continue depending on whether the first transfer was                                    9                              Model High Cost
fast or slow. If the first selected transfer was fast, then                                                            Model Low Cost
one of the transfers would be selected on the train line
                                                                                        6
connected by the selected transfer. If the first selected
transfer was slow, another transfer would be selected on
the same train line of the start station. This would                                    3
continue for higher values of n and the corresponding
execution costs could be obtained. The simulations were
                                                                                        0
run 100 times because of the stochasticity involved in                                             High       Medium          Low
the selection of transfer. The curves in Figure 3 were                                                        Benefit
used to approximate the function f(n) in the model fits
presented below.                                                          Figure 4. The amount of planning in the all-slow maps
        Execution cost (seconds)
                                                                          done by the human subjects and the model in the map-
       125                                                                navigation task.
                                                                             The main effects of Cost and Benefit were significant
              High Benefit                                                (F(1, 84) = 37.95, MSE = 2645.53, p < 0.01 and F (2,
       100
                                                                          84) = 34.39, MSE = 2397.386, p < 0.01 respectively).
                                                                          Subject planned more when the cost was low and when
           Med Benefit
        75
                                                                          the benefit was high. The results showed that subjects'
                                                                          decisions on when to stop planning were sensitive to the
                                                                          cost and benefit of planning. The fit of the model to the
        50                                                                empirical data was good, R2 = 0.92, RMSE = 0.21,
              Low Benefit
                                                                          suggesting that the model captured subjects perception
                                                                          of the expected cost and benefit of planning well. It also
        25
                                                                          shows that the model captured the cost-benefit tradeoffs
            0   1  2 3  4  5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
                                                                          well.
                                        Number of steps of planning
                                                                              Amount of planning
Figure 3. The relationship between the number of steps
                                                                             15              Lo-Hi                              Subjects
of planning and the execution cost obtained from the
                                                                                                                                 Model
simulations for the map-navigation task. Each point in                           Lo-Med
the figure represents the mean of the results from 100
simulations.
                                                                             10
                                                                                  Hi-Hi
                                Results
                                                                                 Hi-Med
Since modeling the results from the round-about-fastest
                                                                                Lo-
maps require more elaborate strategies construction, this                     5
                                                                                Lo
paper focuses on the results from the all-slow maps.
Since there was no fast transfer in the all-slow maps, the                         Hi-Lo
major factor affecting the decision on how much to plan
would be affected solely by subjects’ perception of the                       0
                                                                                    Tr8        Tr16      Tr24  Tr32   Tr40       Tr48    Tr54
expected cost and benefit of planning. The dependent                                                                                     Trials
measure was the amount of planning (measured by the                       Figure 5. The amount of planning in the all-slow maps
number of mouse clicks that uncovered the color of the                    across trials done by the human subjects and the model
transfers). The initial parameters for the prior gamma                    in the map-navigation task. R2 = 0.78, RMSE = 2.64.
distribution were set to a = 10, b=2. The value of t (the                    Figure 5 shows the model fit to the amount of
standard deviation of the noise term in f(n|B)) is set to                 planning across trials. The fit to the empirical data was
be 2.5. These were free parameters chosen to better fit                   good, R 2 = 0.78, RMSE = 0.64. It shows that not only
the data. The same set of parameters was used to fit two                  did the model fit the average performance of the
other sets of data, providing constraints to the model.                   subjects well; it also fit the learning of the subjects well
However, due to space limitation, only the first set of                   across different experimental conditions. Given the free
data was presented here. The model fit is shown in                        parameters of the model were mainly the parameters for
Figure 4.                                                                 the prior gamma distribution (which were the same in
                                                                          all experimental conditions) and the noise parameter,
                                                                          the fit for both performance and learning suggest that
                                                                          the Bayesian satisficing model did a good job
                                                                      424

characterizing the behavior of the subjects. The same       Beach, L. R. & Mitchell, T. R. (1978). A contingency
model (with the same parameters) was run to fit two           model for the selection of strategies. Academy of
more sets of data (see Fu, 2003), which provides further      Management Review, 3, 439-449.
constraints to the model. However, due to space             Berger, J. O. (1985). Statistical decision theory and
limitations, they were not reported in this paper.            Bayesian analyses. New York: Springer-Verlag.
                                                            Christensen-Szalanski, J. J. J. (1978). Problem-solving
           Conclusions and discussions                        strategies: A selection mechanism, some implications
                                                              and some data. Organizational Behavior and Human
The above results provided evidence that the                  Performance, 22, 307-323.
interleaving of planning and acting is adaptive. Subjects   Fu, W.-T. (2003). Adaptive planning in problem
were willing to plan more when the benefit was high,          solving. Dissertation. George Mason University,
and plan less when the cost was low. Subjects also            Fairfax, VA.
showed continued adaptation to the environment with         Fu, W.-T. & Gray, W. D. (2000). Memory versus
experience. Overall, the Bayesian satisficing model           perceptual-motor tradeoffs in a blocks world task. In
provides a good account of the cost-benefit tradeoffs in      the proceedings of the 22nd annual conference of the
planning, and it adapts to similar amount of planning as      Cognitive Science Society. Mahwah, NJ: Erlbaum.
the subjects in different environments. The model also      Gunzelmann, G., & Anderson, J. R. (2003). Problem
generated learning curves similar to those of the             solving: Increased planning with practice. Cognitive
subjects.                                                     systems research, 4 (1), 57-76.
  The Bayesian satisficing model is similar to the          Hayes-Roth, B., & Hayes-Roth, F. (1979). A cognitive
reinforcement learning approach in machine learning.          model of planning. Cognitive Science, 3, 275-310.
However, it is rested on the rational assumption, under     Kirsch, D. & Maglio, P. (1994). On distinguishing
which the human cognitive system is assumed to be             epistemic from pragmatic action. Cognitive science,
well adapted to the characteristic of the environment.        18, 513-549.
Specifically, both the Bayesian learning computations       Kopf, R. E. (1988). Optimal path finding algorithms. In
and the stopping criterion were optimal given the             Kanal, L. N. and Kumar, V. (eds), Search in Artificial
                                                              Intelligence, pp. 223-267, Springer Verlag, Berlin
existing knowledge and new observations of the
                                                            Krebs, J. R. & Davies, N. B. (1978). Behavioural
environment. The Bayesian learning takes into account
                                                              Ecology: An Evolutionary Approach. Oxford:
the uncertainties in both the existing knowledge and          Blackwell.
new observation. The stopping criterion takes advantage     Lovett, M. C., & Anderson, J.R. (1996). History of
of the existing knowledge by choosing a level of              success and current context in problem solving:
planning so that further planning does not justify its        combined influences on operator selection. Cognitive
cost. The overall model therefore nicely combines both        psychology, 31 (2), 168-217.
learning of the characteristics of the environment and      Newell, A., & Simon, H. A. (1972). Human problem
immediate performance.                                        solving. NY: Prentice Hall.
  Similarly to other models based on the rational           Payne, J. W., Bettman, J. R., & Johnson, E. J. (1993).
assumption, the human cognitive system does not               The adaptive decision maker. New York: Cambridge
necessarily perform the computations involved as              University Press.
specified in the model. Instead, the computations           Pyke, G. H. (1978). Optimal foraging in hummingbirds:
involved should reflect what the cognitive system             testing the marginal value theorem. American
should do if the system is well adapted to the                Zoologist,18, 739.
characteristics of the environment. For the same reason,    Robertson, S. P., & Black, J. B. (1986). Structure and
the distributions used in the current model may not be        development of plans in computer text editing.
general to all problem-solving situations. However, the       Human-Computer Interaction, 2, 201-226.
current endeavor seems to suggest that with minimal         Russell, S. & Wefald, E. H. (1991). Doing the right
assumptions of the environment, the Bayesian                  thing: studies in limited rationality. Cambridge, MIT
adaptation approach seem to be able to characterize           Press.
human adaptation to new environments well.                  Simon, H. A.(1956). Rational choice and the structure
                                                              of the environment. Psychological Review, 63, 129-
                                                              138.
                 Acknowledgments                            Sun, R., & Giles, L. (2001). Sequence Learning:
The author would like to thank Wayne Gray, John               Paradigms,       Algorithms,     and      Applications.
Anderson, Lael Schooler, and Kevin Burns for their            Heidelberg, Germany: Springer-Verlag.
valuable comments on an earlier version of this paper.      Sutton, R. S. & Barto, A. G. (1998). Reinforcement
                                                              learning: an introduction. Cambridge, MA: MIT
                       References                             press.
Anderson, J. R. (1990). The adaptive character of
  thought. Hillsdale, NJ: Erlbaum.
                                                        425

