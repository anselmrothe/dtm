UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Constraints on Generalization: Why are Past-Tense Irregularization Errors so Rare?
Permalink
https://escholarship.org/uc/item/1gz5z09j
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Taatgen, Niels
Dijkstra, Mariette
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                           Constraints on Generalization:
                       Why are Past-Tense Irregularization Errors so Rare?
                                                 Niels Taatgen (niels@ai.rug.nl)
                                    Department of Artificial Intelligence, Grote Kruisstraat 2/1
                                                   9712 TS Groningen, Netherlands
                                      Mariëtte Dijkstra (m.e.dijkstra@student.rug.nl)
                       Departments of Humanities Computing and English, Oude Kijk in 't Jatstraat 26
                                                  9712 EK Groningen, Netherlands
                             Abstract                                  1. The model should be able to freely add –ed to
                                                                           words of virtually any sound, for example in
   We present an extension of the ACT-R model by                           Yeltsin outgorbacheved Gorbachev.
   Taatgen and Anderson (2002) of learning the English                 2. The model should also be able learn regular
   past tense that can take into account the production of                 inflection in cases where regular inflection has a
   irregularization errors. These errors are produced by                   low frequency, as in the German plural, where less
   using examples of irregular verbs as the basis for                      than 10 percent of inflection is using the regular –s
   analogy. The relative rareness of irregularization errors               suffix.
   puts constraints on the rule generalization process. The            3. The model should avoid making other errors than
   model explains this by the fact that the probability for a              overregularization.
   particular irregularization is too low to establish a rule.       Criterion 3 is based on a study by Xu and Pinker (1995)
                                                                     that investigated what other errors children might make,
                        Introduction                                 except for overregularization errors. They made the
Learning the past tense in English is an example of                  following classification:
generalization in language acquisition that has been a                 - Over-application of irregular forms, such as bring-
central subject in the debate between symbolism and                        *brang in analogy with sing-sang.
connectionism. The phenomenon can be described                         - Blends, where a vowel-change is combined with
easily: the past tense of regular verbs in English can be                  the regular suffix, such as sing-*sanged.
obtained by adding –ed to the stem. Irregular verbs are                - Gross distortions, where the past tense is an
unsystematic: each word has a unique inflection. When                      almost unrecognizable form of the stem, as in
children learn the inflection of the past tense, they go                   mail-*membled.
through three stages. During the first stage, when                   The conclusion of the study was that these errors are
children first start to use the past tense, they inflect             very rare (they found an error rate of 0.0019), and that
irregular verbs correctly or not at all. During the second           gross distortions do not occur at all.
stage, they start overregularization of irregular verbs.                The significance of the low rate of errors other than
Although they might have used broke earlier on, they                 overregularization lies in the fact that it demonstrates
now also use *breaked. The second stage is also                      that human generalization is quite sophisticated.
characterized by an increase in correct inflection of                Apparently only generalizations are attempted that
regular verbs. Once in the third stage all verbs are                 make some sense, and are rooted in experience. As a
inflected correctly. The third stage is more                         consequence, we have to rule out mechanisms for
approximate: even adults occasionally make                           generalization that propose a rather liberal mechanism
overregularization errors. The pattern of learning in the            of producing generalizations, and rely on a feedback or
past tense is usually referred to as U-shaped learning.              a utility mechanism to weed out                  improper
   The motivation to study learning the English past                 generalizations.
tense, or learning inflection in general, is that it is a
clear and well documented example of generalization in               Theories and Models of Learning the Past
language. Somehow the generality of a grammar rule is                Tense
captured by the cognitive system, and in the process                 The central debate around learning the past tense
some over-application occurs. The challenge of a model               focuses on whether a single or a dual system of
of this generalization process is to capture as many of              representation is needed to explain the data. The dual
the details of the learning process as possible.                     representation theory, best explained by Marcus et al.
   In order to better compare models of irregular                    (1992), uses rules and examples to explain the
inflection, Marcus (2001) has formulated three criteria              phenomenon. In this explanation, past tenses are
in addition to the U-shaped performance curve:                       memorized as separate cases, until in stage 2 a general
                                                                     rule is learned, the regular rule (add –ed to the stem).
                                                               1146

From then on the system first attempts to retrieve a past                              ACT-R
tense from memory, and if this fails uses the regular        The basic theoretical foundation of the ACT-R
rule. As more past tenses are properly memorized, the        (Anderson & Lebiere, 1998) architecture is rational
number of overregularization errors decreases.               a n a l y s i s . According to rational analysis, each
   Single representation theory, first expressed in the      component of the cognitive system is optimized with
Rumelhart and McClelland model of the past tense             respect to demands from the environment, given its
(1986), states that only a single representation is needed   computational limitations. The main components in
to model the phenomenon: a neural network. Their             ACT-R are a declarative (fact) memory and a
association network was able to learn inflection of the      production (rule) memory. To avoid confusion with
past tense, while at the same time exhibiting a U-shape      grammatical rules, we will refer to rules in production
in performance on irregular verbs. Their explanation,        memory with production rules. ACT-R is a so-called
and that of many subsequent neural networks (e.g.,           hybrid architecture, in the sense that it has both
Plunkett & Marchman, 1991), states that initially the        symbolic and sub-symbolic aspects. We will introduce
neural network is able to accommodate all individual         these components informally.
examples, but as the size of the vocabulary increases,          Items in declarative memory, called chunks, have
the network is forced to generalize, producing               different levels of activation to reflect their use: chunks
decreased performance on irregular verbs. However,           that have been used recently or chunks that are used
this reliance on vocabulary growth and constitution is       very often receive a high activation. This activation
also the Achilles heel of the model: the input of the        decays over time if the chunk is not used. Activation
model has to be carefully controlled in order to achieve     represents the probability (actually, the log odds) that a
the desired performance. For example, Rumelhart and          chunk is needed and the estimates provided for by
McClelland increased their vocabulary from ten to 420        ACT-R’s learning equations represent the probabilities
words just before the onset of the decrease in               in the environment very well. The level of activation
performance on irregular verbs. More modern neural           has a number of effects. One effect of activation is that
network models have more moderate schemes of                 when ACT-R can choose between chunks, it will
increasing vocabulary, but almost always with some           retrieve the chunk with the highest activation.
growth spurt. Another problem is the constitution of the     Activation also affects retrieval time, and whether the
vocabulary: despite the fact that regular verbs far          chunk can be retrieved at all.
outnumber irregular verbs, the actual use of irregular          Chunks cannot act by themselves, they need
verbs is around 70% (the so-called token-frequency). If      production rules for their application. In order to use a
this raw input of 70% irregular verbs is presented to a      chunk, a production rule has to be invoked that retrieves
neural network, it cannot discover the regularity            it from declarative memory and does something with it.
(Marcus’s criterion 2 refers to this problem). With          Since ACT-R is a goal-driven theory, chunks are
respect to errors, neural network models tend to             usually retrieved to achieve some sort of goal. In the
produce many different types: overregularizations,           context of learning past tense the goal is simple: given
irregularizations, blends, and other unaccountable           the stem of a word, produce the past tense.
errors, thereby violating Marcus’s criterion 3. A final         The behavior of production rules is also governed by
problem, notably for the more advanced three-layer           the principle of rational analysis. Each production rule
backpropagation networks, is that they require feedback      has a real-value quantity associated with its utility. This
on their own production, despite the well-known fact         utility is calculated from estimates of the cost and
that parents do not consistently give feedback on            probability of reaching the goal if that production rule is
syntactic errors.                                            chosen. The unit of cost in ACT-R is time. ACT-R’s
   The pattern that emerges from the problems that           learning mechanisms constantly update these estimates
neural network models have is that they are                  based on experience. If multiple production rules are
underconstrained with respect to generalization. Their       applicable for a certain goal, the production rule is
learning characteristics are too much determined by the      selected with the highest expected outcome.
input, producing a mismatch between learning in                 In both declarative and procedural memory,
networks and actual human learning. In the remainder         selections are made on the basis of some evaluation,
of the paper, we will present an alternative model using     either activation or utility. This selection process is
the ACT-R cognitive architecture. This model extends         noisy, so the item with the highest value has the greatest
and earlier model by Taatgen and Anderson (2002;             probability of being selected, but other items get
Taatgen, 2001) by using a phonological instead of a          opportunities as well. This may produce errors or
symbolic representation of the vocabulary. This              suboptimal behavior, but also allows the system to
extension allows the study of errors other than              explore knowledge and strategies that are still evolving.
overregularization errors.                                      In addition to the learning mechanisms that update
                                                             activation and expected outcome, ACT-R can also learn
                                                             new chunks and production rules. New chunks are
                                                             learned automatically: each time a goal is completed it
                                                        1147

is added to declarative memory. If an identical chunk is         frequency: high frequency words are used often, so
already present in memory, both chunks are merged and            storing an irregular past tense separately pays off, while
their activation values are combined. New production             low-frequency past tenses are better inflected through a
rules are learned on the basis of specializing and               regular rule, where one rule can cover all cases.
merging existing production rules. The circumstance for          Because the frequency distribution of past tenses
learning a new production rule is that two rules fire one        concurs so well with what one would expect on the
after another with the first rule requesting a chunk from        basis of the ACT-R theory, the model is fairly straight-
memory, and the second rule matching that chunk. A               forward.
new production rule is formed that combines the two
into a macro-rule but eliminates the retrieval. The              Representation
macro-rule is specialized to contain the information that        In contrast with earlier ACT-R models (Taatgen &
was retrieved. New rules are not allowed to compete              Anderson, 2002; Taatgen, 2001) we will use a
with old rules right away: they are initially introduced         phonological representation of the verbs. Table 1
with a low utility value1. This utility value is increased       illustrates the contrast between the two representations
each time the rule recreated, until at some point the rule       with an example of the past tense of believe. The
is chosen, and its utility estimate can be based on real         representation is limited to two syllables. In the 478
experience with the rule.                                        verb vocabulary we use, there is only one three-syllable
                                                                 word (which is therefore excluded), so this limitation is
Generalization in ACT-R                                          of no consequence for the results. For the remainder of
The production compilation mechanism in ACT-R only               this paper, we will, for the sake of brevity, pretend that
produces more specialized versions of existing rules.            words are all monosyllabic, and have the structure
How can it then achieve generalization? We will look at          onset-peak-coda-suffix. The model, however,
examples later, but the general idea is as follows. The          implements two syllables.
key to generalization of an example is that it can be
accomplished by specializing even more general rules.             Table 1: Example of the old and the new representation
A very useful strategy that can serve as a basis for this                            of the verb believe.
is analogy. Analogy tries to solve a problem by
retrieving an example of a similar problem from                    Old representation            New representation
memory. This example is then mapped onto the present               Word322                       Word322
problem, and the resulting pattern is used to solve the             isa past-tense                isa past-tense
current problem. If production compilation is applied to            word believe                  onset1 b
the process of analogy, the retrieval of the example is             past-stem believe             peak1 schwa
substituted into the rules themselves, and this produces            past-suffix ed                coda1 blank
a specialized rule that is a generalization of the                                                onset2 l
example. In order for this rule to play a role in the                                             peak2 ii
system, it has to be recreated a number of times, and                                             coda2 v
this will happen more often if different combinations of                                          past-onset1 b
problem and example will produce the same                                                         past-peak1 schwa
generalization. This property will be crucial in                                                  past-coda1 blank
explaining the error patterns in the past tense.                                                  past-onset2 l
                                                                                                  past-peak2 ii
      A Model of Learning the Past Tense                                                          past-coda2 v
                         in ACT-R                                                                 past-suffix d
The ACT-R model is a dual-representation model that
uses ACT-R’s declarative memory to store examples of
past tenses, and that learns a production rule for the           Basic Strategies
regular past tenses. An important assumption of the              The retrieval strategy is to search declarative memory
model is that irregular past tenses have some sort of            for an example that is identical to the current problem.
advantage over regular past tenses, either because               This retrieved example immediately provides the
regular past tenses are slightly longer, because a               (hopefully correct) answer. The analogy strategy also
phoneme or syllable is added instead of vowel-change,            searches declarative memory for an example, but not
or because regular past tenses are phonetically irregular        necessarily identical to the current problem. Pattern
(Burzio, 2002). Based on that assumption, ACT-R’s                matching rules try to find a pattern in the example that
theory of rational analysis can explain why irregular            can be applied to the current problem. Analogy is not
past tenses make up for the words with the highest               necessarily successful, as it can find examples that
                                                                 cannot be mapped on the current problem, or it can
1
  This particular mechanism is not part of the standard ACT-R    make a mapping that produces a wrong answer. If both
distribution, see Taatgen (2002) for details.
                                                            1148

strategies fail, the model will use the present tense as       *brang. Plain and simple: Analogy-2 applies if the
past tense.                                                    current verb and the example rhyme.
   Because the analogy strategy is the key to the
explanation of errors, we will examine it in more detail.      Analogy-3
Analogy in this model is a two-step process:                   IF the goal is to find inflection inflection of the word
  1. Retrieve an example similar to the current                    onset1-peak1-coda1
        problem                                                AND an example has been retrieved of the word
  2. Find the pattern in the example and apply it to               onset1-peak1-coda2 with inflection onset1-peak2-
        the current problem                                        coda2-suffix
Although only one rule is needed for step 1, a set of          THEN set the inflection to onset1-peak2-coda1-suffix
rules is necessary for step 2, one for each possible
combination of patterns in problem and example. In the         This rule is the reverse of Analogy-2: it applies a vowel
model this means we have the following rule for                change whenever the onset and peak of the example and
retrieval2:                                                    current verb match.
                                                                 In the present model we didn’t introduce any other
Retrieve-for-Analogy:                                          analogy rules, but one can image that an even larger set
IF the goal is to find inflection inflection of a word         of rules exists. Another aspect of the analogy rules is
THEN make a retrieval request for an example of                that each of them consists of a combination of copy and
    inflection inflection                                      substitute operations. As such they themselves are the
                                                               result of a compilation process of smaller operations,
Although this rule retrieves an arbitrary past tense,          possibly tuned to analogy patterns that occur often in
ACT-R’s activation mechanism ensures that it is likely         language.
that a high-frequency verb with many phonemes in
common with the current word will be found. Also note          Examples of Production Compilation
that this rule is not specific to the past tense: it can be    As explained before, the production compilation
used for any type of inflection. For the rules in step 2       mechanism merges two rules into one while substituting
we have included the following rules:                          the retrieved chunk from declarative memory. Given
                                                               the rules above, the Retrieve-for-Analogy rule can be
Analogy-1                                                      combined with either Analogy rule and an example to
IF the goal is to find inflection inflection of the word       form a new rule. For example, Retrieve-for-Analogy,
    onset1-peak1-coda1                                         Analogy-1 and the example work-worked will produce
AND an example has been retrieved of the word                  the regular rule:
    onset2-peak2-coda2 with inflection onset2-peak2-
    coda2-suffix                                               Learned-rule-1
THEN set the inflection to onset1-peak1-coda1-suffix           IF the goal is to find the past tense of the verb onset1-
                                                                   peak1-coda1
This rule serves as the basis for the regular rule: if it      THEN set the past tense to onset1-peak1-coda1-ed
retrieves an example in which a certain suffix is added
to the stem, as is the case with all regular past tenses, it   Similarly, Retrieve-for-Analogy, Analogy-2 and the
will also add this suffix to the current verb.                 example sing-sang (and given that the current verb also
                                                               ends in –ing) will produce the following rule:
Analogy-2:
IF the goal is to find inflection inflection of the word       Learned-rule-2
    onset1-peak1-coda1                                         IF the goal is to find the past tense of the verb onset1-i-
AND an example has been retrieved of the word                      ng-blank
    onset2-peak1-coda1 with inflection onset2-peak2-           THEN set the past tense to onset1-a-ng-blank
    coda1-suffix
THEN set the inflection to onset1-peak2-coda1-suffix           During the model simulation, the model will also learn
                                                               rules based on the direct retrieval strategy. These rules
This rule produces vowel-changes: if it retrieves an           are specialized for specific words, for example:
example with the same peak and coda as the current
verb, it will apply the same vowel change (assuming            Learned-rule-3
there is one) to the current verb. This rule would, for        IF the goal is to find the past tense of the verb b-ii-
example, given the example sing-sang, inflect bring to             blank
                                                               THEN set the past tense to w-o-z-blank
2
  Note that we use an informal notation of production rules.
Terms in italics indicate variables.
                                                          1149

                 1
                0.9
   Proportion
                0.8
                                                                                                       Regular Correct
                                                                                                       Overregularization
                0.7
                0.6
                0.5
                      0         5              10              15                                     20               25                      30
                                                        Time (months)
                                Figure 1. Overregularization and regular correct for the model
                          Model Simulation
The input for the model consisted of 477 words (478                                                                    Adam
verbs minus one three-syllable word) from Marcus et al.
                                                                                            1
(1992). Every 2000 simulated seconds two verbs with
their past tense were added to declarative memory. This                                    0.9
represents the past tenses the child hears in its                                          0.8
environment. Also every 2000 simulated seconds one                                         0.7
verb was selected for production: the word was
                                                                      Proportion correct
                                                                                           0.6
presented to the model with the goal to find its past                                                                                     Regular mark rate
tense. Both the added and presented words were                                             0.5                                            Overregularization
randomly drawn from the vocabulary, but based on the                                       0.4
frequency distribution of the words according to Francis                                   0.3
and Kucera (1982). The model received no external
                                                                                           0.2
feedback on its own production, but it did have internal
feedback: the amount of time it took to produce a past                                     0.1
tense. Reflecting the assumption that irregular past                                        0
tenses have some advantage, regular inflection received                                          27   32   37    42       47         52   57        62     67
                                                                                                                      Age (months)
a 400 ms extra cost, and non-inflection received a 600
ms extra cost (the assumption in the case of non-
inflection is that past tense must be indicated in another                        Figure 2. Overregularization and regular correct for
way, for example by adding a word like yesterday).                                                      Adam
   Figure 1 shows the results of 27 simulated months.
What is indicated with overregularization is actually the           number of irregularization errors the model makes is so
proportion of correct irregular past tenses excluding               small that it can hardly be plotted: the model made 10
non-inflection errors. The results clearly show a U-                such errors in 35000 inflections, amounting to .029%
shape in performance, and the onset of                              irregularization errors. The errors the model made
overregularization coincides with the a strong                      varied between runs and very much depended on the
improvement in performance on regular verbs. As such,               combinations of current verb and example. Table 2
the results are quite comparable to for example Adam                shows the errors made in this single run (less than 10
(Figure 2, adapted from Marcus et al, 1992). The                    because some errors were duplicated).
                                                             1150

    Table 2. Irregularization errors made by the model.         examples is a candidate for such a system. It also
                                                                concurs with earlier ideas by Anderson (1987), and also
  Error                          Probably based on              has been successfully used to model completely
  let–*lot                       get–got                        different types of tasks, like Air Traffic Control
  ride–*rid                      hide-hid                       (Taatgen & Lee, in press).
  bring-*brang                   sing-sang                        The symbolic ACT-R model already addressed
  make-*mook                     take-took                      Marcus’s criteria 1 and 2, and with this model we
  carve-*ceerve                  come-came                      showed the type of explanation needed for criterum 3.
  feel-*fel                      feed-fed                       An error type that hasn’t been addressed by this model
                                                                though are the so-called blends, like sing-*sanged. A
   It is clear that the model makes plausible errors, and       possible explanation for this kind of error is that the
at least includes the prototypical bring-*brang. But why        child mistakenly believes that sang is a present tense.
are these errors so much less frequent than over-               The error can then be explained by the application of
regularization errors? To understand this, we have to go        the regular rule. Modeling these kinds of errors is
through the model’s functioning in some more detail.            certainly not impossible, but would require a slightly
   Initially, past tenses are either retrieved, or an attempt   broader scope of representation of the vocabulary.
at analogy is made. Whenever either retrieval or
analogy fails, the other strategy (retrieval or analogy) is     References
tried, and after both have failed the model doesn’t             Anderson, J.R. (1987). Skill acquisition: compilations
inflect at all. Retrieval is only successful when the             of weak-method problem solutions. Psychological
answer can be found in declarative memory. As the                 Review, 94, 192-210.
model gets more and more examples from the outside              Anderson, J. R., & Lebiere, C. (1998). The atomic
world, it will get more and more successful. Analogy              components of thought. Mahwah, NJ: Erlbaum.
never has problems retrieving examples: its problem is          Burzio, L. (2002). Missing players: Phonology and the
to find a mapping between the retrieved example and               Past-tense Debate. Lingua, 112, 157-199.
the current verb. This will fail most of the time. For          Francis, W. N., & Kucera, H. (1982). Frequency
example, the example be-was is retrieved fairly often,            analysis of English usage: lexicon and grammar.
as it is the most frequent verb and therefore has a high          Boston, MA: Houghton Mifflin.
activation, but is virtually useless for analogy.               Marcus, G.F. (2001). The Algebraic Mind. Cambridge,
However, any retrieved regular verb will serve as a               MA: MIT-Press.
basis for Analogy-1 and will produce the regular rule.          Marcus, G. F., Pinker, S., Ullman, M., Hollander, M.,
As it takes some time before the regular rule is actually         Rosen, T. J., & Xu, F. (1992). Overregularization in
learned (remember it takes several re-creations of the            language acquisition. Monographs of the society for
rule), all the overregularizations in the first few months        research in child development, 57(4), 1-182.
of the simulation are due to the application of the             Plunkett, K., & Marchman, V. (1991). U-shaped
analogy strategy. Only later (around month 4) has the             learning and frequency effects in a multi-layered
regular rule’s utility become high enough to be able to           perceptron: Implications for child language
compete with the basic strategies.                                acquisition. Cognition, 38, 43-102.
   The irregularization errors are all produced by              Rumelhart, D.E., & McClelland, J.L. (1986). On
Analogy-2 and Analogy-3. In making these errors,                  learning the past tense of English verbs. In J.L.
rules are learned that correspond to these errors, so in          McClelland & D.E. Rumelhart (Eds.), Parallel
producing bring-*brang, the model learned Learned-                distributed processing: Explorations in the
rule-2. However, this rule, and neither any of the                microstructure of cognition (pp. 216-271).
similarly learned rules, is ever recreated again. The             Cambridge, MA: MIT Press.
pattern is just not frequent enough to make a viable rule       Taatgen, N.A. (2001). Extending the past-tense debate:
in the system. So, all irregularization errors are                a model of the German plural. In: Proceedings of the
produced by accidental combinations of current verb               twenty-third annual conference of the cognitive
and example in the analogy strategy, but are never                science society (pp. 1018-1023). Mahwah, NJ:
frequent enough to produce a rule that can compete.               Erlbaum.
                                                                Taatgen, N.A. & Anderson, J.R. (2002). Why do
                       Conclusions                                children learn to say “Broke”? A model of learning
An important criterion for a system that generalizes              the past tense without feedback. Cognition, 86(1),
rules out of examples is that it should produce the right         123-155.
rules, and also that it avoids producing the wrong rules.       Taatgen, N.A. & Lee, F.J. (in press). Production
Only a sufficiently constraint generalization system is           compilation: a simple mechanism to model complex
able to avoid the latter part of the constraint. In this          skill acquisition. Human Factors.
paper we showed that the ACT-R strategy of                      Xu, F. & Pinker, S. (1995). Weird past tense forms.
specializing the Analogy into a generalization of                 Journal of child language, 22, 531-556.
                                                           1151

