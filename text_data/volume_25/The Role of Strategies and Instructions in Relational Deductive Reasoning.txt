UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Role of Strategies and Instructions in Relational Deductive Reasoning

Permalink
https://escholarship.org/uc/item/2fx7k9bw

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)

Authors
Robinson, A. Emmanuel
Hertzog, Christopher

Publication Date
2003-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Role of Strategies and Instructions
in Relational Deductive Reasoning
A. Emanuel Robinson (gte581m@mail.gatech.e du)
School of Psychology, 654 Cherry Street
Atlanta, GA 30332-0170 USA

Christopher Hertzog (Christopher.hertzog@psych.gatech.edu)
School of Psychology, 654 Cherry Street
Atlanta, GA 30332-0170 USA

Abstract
Deductive reasoning is often seen as being composed of an
immutable mechanism, universal to all reasoning situations
and consisting of either mental models (e.g., Johnson-Laird,
1983) or formal-rules (e.g., Rips, 1994). Many researchers
have questioned whether these positions are truly mutually
exclusive (e.g., Roberts, 1993, 2000). Most deductive
reasoning research has largely ignored the influence of
instructions and strategies on the reasoning process. The
present experiment was conducted to investigate reasoning
strategies along with metacognitive measures of those
strategies. Instructions were given to use a particular strategy
(e.g., spatial, verbal). Items were separated into two levels:
simple and complex, based on the amount of premises.
Premise times, accuracy, and strategy reports were collected.
Instructions had an effect on performance, as seen in premise
times and accuracy. Also, strategy reports indicated a
distribution of strategies utilized by participants. Strategy
reports proved vital in corroborating differential patterns of
performance indicative of varied approaches to solving this
task.

Introduction
We are constantly faced with the need to derive new
information from prior information. Reasoning is a vital
part of that derivation process, whether it is solving a
problem, invoking the proper strategies in memory, or
drawing inferences to test hypotheses.
Individuals can solve a reasoning task in many
different ways. In recent years, there have been two main
theories of representation in reasoning—mental models
(e.g., Johnson-Laird, 1983) and formal-rules (e.g., Rips,
1994). Proponents of the mental models view claim
individuals construct analog representations of the
information presented to them and attempt to find
counterexamples to the purported conclusions, often in the
form of a spatial model. In contrast, rule-based reasoning
advocates claim individuals invoke relatively content-free
algorithms to reason, usually in the form of verbal rules.
These general definitions have quite different implications
for the reasoning process. The mental models definition
would imply the use of a spatial strategy to construct a
representation that is not unlike the information

1000

encountered. In contrast, the formal rules definition implies
a verbal approach to analyzing information, based on
syntactic rules and abstract content. Each theory claims
superiority in most tasks of reasoning, implying an all-ornone situation for reasoning, ignoring the possibility of
individual differences in strategy use or task-related
performance differences.
Formal rules (e.g., Rips, 1994) characterize
reasoning as the application of logical inference rules.
Individuals have a set of syntactic, content-free rules that
can be used to draw inferences (Polk & Newell, 1995).
These rules are seen to be similar to such classical logical
structures as modus tollens and modus ponens (Kahane,
1990; Rips, 1994). Reasoning by rules is highly discrete
and syntactic, with only one right answer (if the problem is
determinate) in an all-or-none manifestation. Difficulty is a
direct result of an increase in the amount of rules (or
repeated steps in using those rules) needed to solve a set of
premises in a reasoning problem. Also, there is no need to
translate information, because the rules and premis es are
presumed to already be in a verbal form. These are points
of divergence between mental models and rule-based
reasoning (cf. Polk & Newell, 1995; Sloman, 1996).
Mental models theorists take a different approach
to the reasoning process. Rather than using strict, syntactic,
abstract rules of reasoning, they propose analog mental
models invoked in reasoning situations (Johnson-Laird &
Byrne, 1993). People are believed to form mental models,
which are representations of the situation, as a starting point
for the reasoning process (Johnson-Laird & Byrne, 1993).
These models are sketches of reality used for deriving
conclusions from the information given (Johnson-Laird,
1999). First, one constructs a model of the premises, then
formulates a putative conclusion, and finally, searches for
counterexamples to refute the initial conclusion (JohnsonLaird & Byrne, 1993). Procedural rules are used to translate
verbal propositions into a spatial or symbolic array (Kurtz,
Gentner, & Gunn, 1999).
An important difference between these competing
claims is that the translation procedure would likely be more
intensive and time consuming than dealing with the verbal
propositions in their verbal form, which is an implication of
rule-based theories, yet the product of the translation would

seem to be easier to manipulate and take less time to
interpret in certain tasks (cf. MacLeod, Hunt, & Mathews,
1978).
Johnson-Laird and colleagues have used a spatial
relations task in several studies (e.g., Byrne & JohnsonLaird, 1989; Ehrlich & Johnson-Laird, 1982), to argue for a
mental models representation. In this task, participants are
presented serially with propositions, such as, “A is on the
right of B; C is on the left of B; D is in front of C; E is in
front of B; What is the relation between D and E” (Byrne &
Johnson-Laird, 1989, p. 568)? Participants are required to
use the information given to deduce the answer—D is on the
left of E. In this case, there is one underlying model:
C B A
D E
Alternatively, changing the above example slightly would
yield a multiple -model problem with an ambiguous
structure. For example, A is on the right of B; C is on the
right of B; D is in front of A; E is in front of B (and asking
for the relation between D and E), would result in the
possibility of two valid models (both yielding the correct
answer that D is on the right of E):
B C A
or
B A C
E
D
E D
By manipulating the validity of the propositions
(valid conclusion is possible or not) and how many
alternative representations could be formed, Byrne and
Johnson-Laird (1989) claimed convincing support for
mental models. This support was based on the assumption
that rule-based theories should not differentiate between
propositions that evoke single- and multiple -models
(because they capture the relationship between key
propositions where the number of rules are the same), where
mental models theories would predict differences based on
the number of models necessary to deduce the conclusion.
The data seem to support of a mental models representation.
Overall, participants had more errors in trying to solve
multiple-model than single-model problems, which is
predicted by the mental models theory.
There are two things to note about the task—1)
participants were instructed to “imagine” a spatial array
(images and mental models are very similar, see JohnsonLaird, 1983), and 2) each item consisted of four premises,
making the item rather difficult to solve. Although there are
studies looking at the role of presentation on strategy
preferences (e.g., Roberts, 2000b), where the argument has
been made that serial presentation forces a spatial strategy,
few studies have investigated the role of instructions and
item complexity (see, Sternberg & Weil, 1980). These two
facets of this task were the impetus behind the present
study. What effects do item complexity and instructions
have on participants’ strategies in solving spatial deductive
reasoning problems? The following experiment attempted
to answer this question.

1001

An underlying argument of both theories is that all
individuals fundamentally reason either using a mental
model or formal rule at all times. This view would assume
there are no individual differences in the fundamental
mechanisms of reasoning. “The problem of individual
differences affects any research intended to identify the
fundamental reasoning mechanism” (Roberts, 2000a, p. 33).
Furthermore, related to the particular task just mentioned,
which was arguably designed to have a spatial strategy
predominate, there is a small percentage of individuals that
still use a verbal strategy (Roberts, 2000b). How can a
“spatial” reasoning task, designed to evoke a dependence on
spatial strategies, fail to completely support the notion of
deductive reasoning by spatial mental models? There is one
“simple” answer—individual differences in strategy choice.
This paper will address inter-strategic individual
differences (different strategies used by different
individuals) as opposed to intra-strategic individual
differences (different levels of the same strategy in different
individuals) (Roberts, 2000a). Also, strategies will be
defined as, “a set of cognitive processes which have been
shown to be used for solving certain types of deductive
reasoning tasks” (Roberts, 1993, p. 576). Roberts (2000a)
proposes three groupings of strategies—a) spatial strategies,
where information is represented spatially and is analogous
to the state of affairs in the world and is exemplified by the
mental models theory of Johnson-Laird (1983), b) verbal
strategies, where information is verbally or abstractly
represented and various content/context free rules of syntax
enable one to draw new conclusions from given
information, with rule-based theories (e.g., Rips, 1994), and
c) task-specific shortcut strategies, which result in massive
gains of performance, rely on various representations, and
are extremely task specific.
A powerful set of studies using the sentencepicture verification task highlighted the potent influence
individual differences can have on task performance. As
Roberts (2000a) noted, this literature is often neglected by
researchers in deductive reasoning, although it has produced
important findings that are useful for the reasoning
strategies literature. Namely, individuals can willingly use
alternative strategies to arrive at the correct problem
solution in the sentence-picture verification task.
Hunt and colleagues (Hunt & Macleod, 1978;
Macleod, Hunt, & Mathews, 1978; Mathews, Hunt, &
Macleod, 1980) performed an elegant set of experiments
using the sentence-picture verification task. In this task,
participants first are presented with a simple sentence (e.g.,
plus is above star) and then a picture (e.g., */+). The goal
was to verify whether the picture is true or false of the
sentence. Response times (comprehension and verification)
were used to infer the type of strategy used.
Individual differences in strategy use during task
performance have been identified across a variety of
domains such as syllogisms (Johnson-Laird, Savary, &
Bucciarelli, 2000; Galotti, Baron, & Sabini, 1986),
categorical syllogisms (Ford, 1995), three-term series (Egan

& Grimes-Farrow, 1982), and relations (Morra, 1989).
Even in a task like Byrne and Johnson-Laird’s (1989)
spatial relations task mentioned earlier, where a spatial
strategy is presumed a necessity, Roberts (2000b) found
10% of participants matched the verbal strategy, which is an
example of the prevalence of individual differences in
reasoning.
How do we determine what type of strategy a
participant uses in reasoning? One method is the analysis of
response times and inferring from them what strategy was
used (e.g., Macleod et al., 1978; Mathews et al, 1980;
Roberts, 2000b). A spatial strategy should entail translation
and integration at the premise level, which would result in
longer comprehension times and shorter solution latencies.
Another approach is to ask individual to report their
strategies as they solve problems. Several of the studies in
deduction that found individual differences used some type
of retrospective report (e.g., Egan & Grimes-Farrow, 1982)
or verbal protocol (e.g., Ford, 1995) to assess strategy
choice and usage.
In a different domain where strategy use can be
important and variable—namely, paired-associate learning,
item-level encoding strategies also have been assessed using
self-reports (Dunlosky & Hertzog, 2001). Dunlosky and
Hertzog (1998, 2001) found that self-reported strategies
highly predicted associative recall performance. From the
previous review of the literature, there are two aspects of
performance on the spatial relation task that des erve greater
attention. The first is the role of instructions on a
participant’s method of reasoning in the task. The second is
the role of strategy choice.

Overview of Experiment
The experiment was based on the spatial relations task
developed by Byrne and Johnson-Laird (1989) previously
discussed in the introduction. Individuals were instructed to
use a certain strategy (e.g., spatial, verbal). The present
experiment manipulated instructions and complexity
(premise and model number), analyzing their effects on
performance and strategy choice.
We predicted instructions and item complexity
would affect strategy choice. Specifically, the spatial group
would take the longest on premise times, and the verbal
group would be the quickest. Also, the spatial group would
perform the best overall, especially on complex items, and
would show performance differences between one- and twomodel problems.
Compliance with strategy instructions was
expected to be imperfect, but it would be highest for the
spatial group. The preferred strategy of the naïve group
would be spatial, but verbal strategies would be selected by
a proportion of participants. Also, there will be a shift to
spatial strategies for the complex items.

1002

Method
Participants. One hundred and thirty undergraduates from
the Georgia Institute of Technology participated for course
credit. Participants were between 18 and 32 years older (M
= 20.04, SD = 1.79)
Materials and procedure. Items were constructed to
contain either two premises (“simple”) or four premises
(“complex”). Complexity is defined by the number of
propositions contained in an item (e.g., two propositions
which vary relation in one-dimension or four propositions
varying relation in two-dimensions). An item with four
propositions, containing related and concrete objects, would
be “A pen is to the left of a pencil. A paperclip is to the
right of a pencil. A ruler is in front of a pen. A notebook is
in front of a pencil. What is the relation between the ruler
and the notebook?” Furthermore, the complex problems
were broken into one- and two-model items (explained in
the introduction).
There were 16 simple items (two-premise) and 16
complex items (four-premise). The complex items were
either one-model (8) or two-model (8). All problems
contained equivalent numbers (16 each) of true and false
verification statements, concrete objects , and were serially
presented on a computer.
Two example problems were given at the
beginning of the simple block and two were given at the
beginning of the complex block. The simple block was
presented first, then the complex block. Each example gave
the correct solution with feedback to the participant, with
the proper method for solving the item (according to what
was instructed). In the case of the naïve group, the problem
was restated and no method of solution was presented.
Before the sample items, individuals in the verbal and
spatial groups were given specific instructions as to the
strategy to use, whereas the control (naïve strategy) group
was not given any instructions regarding the type of strategy
to use.
Individuals in the spatial group were told to use an
image and add objects to that image every time they were
presented with a premise. In contrast, the verbal group was
told to use a set of verbal rules that were provided.
Participants in the naïve group were told to solve the
problems in any manner preferred.
Once all of the items were presented and solved in
the simple block, participants were presented with the items
again and asked to report the strategy used for each item,
using a forced-choice format. The options were spatial,
verbal, other, both and no strategy. These retrospective
reports prevented problems of contamination for the naïve
group, since the strategy report scale will have to list the
possible strategies, and this could contaminate subsequent
strategy selection if other items followed (as in concurrent
strategy reports for the complex block of items).
After the simple block, participants were then
instructed that the next set of items contained two more
premises (for a total of four), and they were reminded of

their particular instructed strategy, if applicable. Strategy
reports were collected after every item.

Results
The significance level was p < .05 for all analyses.
ANOVAs consisted of a between-subjects Instructions
factor (spatial vs. verbal vs. naïve), and either a Complexity
factor (simple vs. complex) or a Model factor (one vs. two),
which were both within-subjects.
Solution Accuracy. Table 1 presents the means and
standard errors of the proportion correct and mean premise
times for correct items in both simple and complex blocks,
with the complex block broken into one- and two-model
problems.
Table 1: Proportion correct and mean premise times for
correct items
Prop
Correct
Simple
Complex
1 Model
2 Model

Spatial
.96 (.01)
.78 (.02)
.80 (.02)
.75 (.02)

Verbal
.89 (.02)
.66 (.02)
.65 (.03)
.67 (.03)

Naïve
.91 (.02)
.74 (.02)
.78 (.02)
.70 (.02)

Mean
Premise
Times
Simple
Complex
1 Model
2 Model

Spatial
4.59(.29)
4.75(.21)
4.37(.20)
5.18(.23)

Verbal
3.06(.26)
3.61(.26)
3.44(.26)
3.75(.28)

Naïve
4.18(.22)
4.11(.19)
3.93(.19)
4.32(.21)

There was a difference between the three
instructional groups: marginal M = .87 for spatial, marginal
M = .78 for verbal, and marginal M = .82 for naïve. As
predicted, there was a significant effect of instructions on
proportion correct, F (2, 130) = 9.38, with the spatial group
performing better than the verbal and naïve groups.
Complexity had a robust main effect on proportion
correct, F (1, 130) = 285.29. Individuals performed much
worse on the difficult items (marginal M = .73) than on the
simple items (marginal M = .92). The number of models
had a significant effect on proportion correct, F (1, 130) =
4.51. A smaller proportion of two -model problems than
one-model problems were solved correctly.

Mean Premise Times for Correct Items . There was a
significant main effect of instructions, F (2, 130) = 7.81.
Planned comparis ons indicate the spatial group was slower
at each premise than the verbal group. If participants are
forming images at each premise, this extra time is likely a

1003

cost of translating the verbal statements into images. Mean
premise times were significantly affected by whether an
item was a one- or two-model problem, F (1, 130) = 40.55.
One-model problems were performed faster, per premise,
than two-model problems .
One-model problems being
performed faster would only be expected for the spatial
group, and by extension, the naïve group (assuming the
majority of people will choose a spatial strategy in the more
difficult complex block). There was a significant Model X
Instruction interaction, F (1, 130) = 4.32. There was a
larger difference between premise times for one- and twomodel problems for the spatial group (and not in the other
groups), which would be expected if participants are trying
to incorporate images into a preexisting model, and then
trying to flesh those images out.

Proportions of strategies reported. Strategy reports
consisted of selections from one of five options: spatial,
verbal, both, other, and none. The “none” and “other”
strategy options were extremely infrequent (less than 1.2%
for simple items and less than 2 % for complex items), and
removed from subsequent analyses. Given the ambiguity of
the meaning of “both” strategies, these will be excluded
from the subsequent analyses.
Table 2 includes the proportions of strategies
reported for all items. As expected for the proportion of
spatial strategies reported, there was a main effect of
instructions, F (2, 130) = 19.43. Also, there was a
significant main effect of complexity, F (1, 130) = 4.75.
Participants shifted slightly toward a spatial strategy on the
more complex items. Complex items are used more
frequently in this type of task, and those items may elicit
spatial strategies because of difficulty.
Similar to the spatial strategies just reported, there
was a main effect of instructions on the proportion of verbal
strategies reported, F (2, 130) = 65.08. Complexity did have
an effect on the proportion of verbal strategies reported, F
(1, 130) = 18.89. Participants shifted slightly away from a
verbal strategy on the more complex items.

Table 2: Proportion of Strategies Reported
Strategy
Simple
Spatial
Verbal

Spatial

Verbal

Naïve

.51 (.06)
.17 (.04)

.20 (.05)
.69 (.05)

.48 (.06)
.29 (.06)

Complex
Spatial
Verbal

.64 (.07)
.02 (.01)

.17 (.04)
.54 (.06)

.57 (.06)
.09 (.03)

Effects of Strategy Choice on Accuracy. Table 3 presents
the conditional probabilities of being correct given a
particular strategy was reported.
Table 3: Conditional Probabilities
P(C)|Strategy
Simple
Spatial
Verbal
Complex
Spatial
Verbal

Spatial

Verbal

Naïve

.97 (.01)
.97 (.02)

.89 (.04)
.89 (.03)

.91 (.03)
.92 (.03)

.76 (.02)
.83 (.13)

.67 (.07)
.69 (.03)

one- and two-model problems , because a verbal strategy
would not be sensitive to the number of underlying spatial
models . Indeed, we find no effects of model number, Fs <
1. Because verbal strategies based on formal rules ignore
multip le model configurations, this finding is yet more
support for individuals accurately reporting the use of a
verbal strategy.

Discussion

.71 (.03)
.68 (.08)

Complexity had a significant main effect on the probability
of being correct given a spatial strategy was reported, F (1,
73) = 32.65. Even though a spatial strategy may be less
working-memory intensive than a verbal strategy,
participants were having difficulties when premise number
was increased.
One of the strong predictions of a mental models
approach is that there should be better performance on onemodel problems than two-model problems. Limiting the
analysis to just those participants in the spatial and naïve
groups produced the expected difference in performance as
a result of model number, F (1, 67) = 6.58. Participants in
these two groups performed significantly better on onemodel problems than two-model problems when they
reported using a spatial strategy.
As would be expected, there was also a significant
effect of complexity on the probability of being correct
given a verbal strategy, F (1, 51) = 12.03. In the simple
block, there was a marginal M = .91, while in the complex
block there was a marginal M = .76. These differences
indicate the cost of using a verbal strategy on the type of
complex items typically used in this task.
Effects of Strategy Choice on Mean Premise Time. One
would expect that using a spatial strategy would show
differences in premise times for two-model problems.
There was a significant effect of model number, F (1, 76) =
14.21, with one-model problems taking less time per
premise than two-model problems. This is intriguing,
because it lends support to the notion of valid self-reports.
Individuals using a spatial strategy would be subject to
delays in processing based on model number as they try to
construct a model that is composed of ambiguous parts (i.e.,
multiple possible configurations that are all true).
The patterns of mean premise times when verbal
strategies were reported seem quite different. There were
no significant main effects or interactions, Fs < 2.27. These
lack of differences lend credence to the validity of the
strategy self-reports, especially when compared to the
spatial strategy reports. If participants are truly using a
verbal strategy, there should not be any differences between

1004

Do instructions affect how individuals approach the task the
spatial relational deduction task? From the data just
presented,
the
overwhelming
response
is —yes.
Instructional effects were seen throughout the analyses.
When collapsing across strategies, the proportion an
individual answered correctly was directly affected by the
instructions given. One claim was that individuals will
naturally prefer to use a spatial, and presumably optimal,
strategy on this type of task, which is why the spatial group
performed the best. If this is true, then the verbal group
should have great difficulty, due to being told to approach
the task in a non-optimal way. This argument seems invalid
for several reasons. First, even the verbal group performed
well above chance, showing it is possible to perform the
task relatively well under these instructions, granted it may
not be the “best” strategy. Second, if the argument was
valid, then performance in the naïve group should have
exactly mirrored performance in the spatial group, which
was not the cas e. Individuals in the naïve group did not
perform identically to the spatial group, which could
indicate not all participants in the naïve group were using a
spatial strategy. Finally, given a valid argument, 100% of
the strategies reported in the spatial and naïve groups should
have been spatial—this did not occur. If the spatial strategy
was the optimal strategy, then individuals should have been
eager to use it every tria l. Instead, many individuals either
never used it or did not use the spatial strategy on all trials,
even under instructions to do so.
The effects of instructions on latency were also
robust and provide convergent evidence for differential
approaches. As predicted, the spatial group took the longest
for each premise and the verbal group had the fastest
latencies. Similar to the argument made by Hunt and
colleagues (Hunt & Macleod, 1978; Macleod, Hunt, &
Mathews, 1978; Mathews, Hunt, & Macleod, 1980),
individuals using a spatial strategy would take longer to
translate the words into a pictorial format. We see support
for this hypothesis.
Do people use various strategies in this task and is
it important to attend to individuals differences in strategy
use? From a purely information-processing view, it is clear
that individuals were not performing the task identically,
with robust differences emerging and resulting from
instructions and the strategy reported. Furthermore, when
we analyzed strategy self-reports, we found a variety of
strategy preferences.

Given that compliance was not perfect, it was
necessary to look at the actual strategy reported in relation
to other variables. The patterns of accuracy and premise
times both lend credence to the validity of strategy selfreports. Namely, spatial strategy reports corresponded to
differences in accuracy and premise times when model
number is considered. Rarely are strategy self-reports
collected in this type of task, yet they can be another
valuable and valid tool in assessing approaches to task
performance by participants.
In sum, even though the spatial relations task has
been cited as a “spatial” reasoning task, strategy self-reports
and instructions indicate this is not the full story.

Acknowledgments
This research was supported by the National Institute on
Aging, NIA 23T2 AG00175-11 Cognitive Aging Training
Grant at the Georgia Institute of Technology, and NIA R37
AG13148 to Christopher Hertzog. Special thanks to
Maxwell Roberts for supplying the basic program used for
this computer task and to Wendy Rogers and Richard
Catrambone for helpful comments and suggestions.

References
Byrne, R., & Johnson-Laird, P. (1989). Spatial reasoning.
Journal of Memory and Language, 28(5), 564-575.
Dunlosky, J., & Hertzog, C. (1998). Aging and deficits in
associative memory: What is the role of strategy
production? Psychology & Aging, 13(4), 597-607.
Dunlosky, J., & Hertzog (2001). Measuring strategy
production during associative learning: The relative
utility of concurrent versus retrospective reports. Memory
& Cognition, 29(2), 247-253.
Egan, D., & Grimes-Farrow, D. (1982). Differences in
mental representations spontaneously adopted for
reasoning. Memory and Cognition, 10(4), 297-307.
Ehrlich, K., & Johnson-Laird, P. (1982).
Spatial
descriptions and referential continuity. Journal of Verbal
Learning and Verbal Behavior, 21(3), 296-306.
Ford, M. (1995). Two modes of mental representation and
problem solution in syllogistic reasoning. Cognition,
54(1), 1-71.
Galotti, K., Baron, J., & Sabini, J. (1986). Individual
differences in syllogistic reasoning: Deduction rules or
mental models. Journal of Experimental Psychology:
General, 115(1), 16-25.
Gentner, D., & Gentner, D. R. (1983). Flowing waters or
teeming crowds: Mental models of electricity. In D.
Gentner, & A. L. Stevens (Eds.) Mental models.
Hillsdale, NJ: Lawrence Erlbaum Associates.
Hunt, E., & MacLeod, C. (1978). The sentence-picture
paradigm: A case study of two conflicting approaches to
individual differences. Intelligence, 2(2), 129-144.
Johnson-Laird, P. (1983). Mental models: Towards a
cognitive science of language, inferences, and
consciousness. Cambridge, MA: Harvard University
Press.

1005

Johnson-Laird, P. (1999). Deductive reasoning. Annual
Review of Psychology, 50, 109-135.
Johnson-Laird, P., and Byrne, R. (1993).
Précis of
deduction. Behavioral and Brain Sciences, 16(2), 323380.
Johnson-Laird, P., Savary, F., & Bucciarelli, M. (2000).
Strategies and tactics in reasoning. In W. Schaeken, G.
De Vooght, A., Vandierendonck, & G. d`Ydewalle (Eds.),
Deductive reasoning and strategies. Mahway, New
Jersey: Lawrence Erlbaum Associates .
Kahane, H. (1990). Logic and philosophy: A modern
introduction (6 th ed.).
Belmont, CA:
Wadsworth
Publishing Company.
Kurtz, K., Gentner, D., & Gunn, V. (1999). Reasoning. In
B. M. Bly & D. E. Rumelhart (Eds.), Cognitive science
(pp. 145-200). San Diego, CA: Academic Press, Inc.
MacLeod, C., Hunt, E., & Mathews, N. (1978). Individual
differences in the verification of sentence-picture
relationships. Journal of Verbal Learning and Verbal
Behavior, 17(5), 493-507.
Mathews, N., Hunt, E., & MacLeod, C. (1980). Strategy
choice and strategy training in sentence-picture
verification. Journal of Verbal Learning and Verbal
Behavior, 19(5), 531-548.
Morra, S. (1989). Developmental differences in the use of
verbatim versus spatial representations in the recall of
spatial descriptions: A probabilistic model and an
experimental analysis. Journal of Memory & Language,
28(1), 37-55.
Polk, T., & Newell, A. (1995). Deduction as verbal
reasoning. Psychological Review, 102(3), 533-566.
Rips, L. (1994). Psychology of proof. Cambridge, MA:
MIT Press.
Roberts, M. (1993). Human reasoning: Deduction rules or
mental models, or both?
Quarterly Journal of
Experimental Psychology:
Human Experimental
Psychology Special Issue, 46 (A), 569-589.
Roberts, M. (2000a). Individual differences in reasoning
strategies: A problem to solve or an opportunity to seize?
In W. Schaeken, G. De Vooght, A., Vandierendonck, &
G. d`Ydewalle (Eds.), Deductive reasoning and
strategies. Mahway, New Jersey: Lawrence Erlbaum
Associates.
Roberts, M. (2000b). Strategies in relational inference.
Thinking and Reasoning, 6(1), 1-26.
Sloman, S. (1996). The empirical case for two systems of
reasoning. Psychological Bulletin, 119(1), 3-22.
Sternberg, R., & Weil, E. (1980). An aptitude X strategy
interaction in linear syllogistic reasoning. Journal of
Educational Psychology, 72(2), 226-239.

