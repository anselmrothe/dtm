UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
LSA: First dimension and dimensional weighting
Permalink
https://escholarship.org/uc/item/0p3526z0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Hu, X.
Cai, Z.
Franceschetti, D.
et al.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                LSA: First dimension and dimensional weighting
                                     Hu, X.1 , Cai, Z.1 ,Franceschetti, D.2 ,
           Penumatsa, P. Graesser, A. C.1 , Louwerse, M. M.1 ,McNamara, D. S.1
                                1,
                                     and the Tutoring Research Group1
                                            1
                                              Department of Psychology;
                                               2
                                                 Department of Physics;
                                   The University of Memphis, Memphis, TN 38152
                         Abstract                           similarity measure, best candidates can be obtained
                                                            and they are assumed to be semantically similar to
   We report two discoveries concerning Latent Se-
   mantic Analysis (LSA). First, we observed the            the query document. Application of this use of LSA
   special properties of the …rst dimension of the          has been documented in several of our current appli-
   LSA space. Second, we observed that dimensional          cations (Olde, Franceschetti, Karnavat, Graesser, &
   weighting plays an important role in LSA analysis.       TRG, 2002; Graesser, Hu, Person, Jackson, & Toth,
   Based on the …rst discovery, we examined the cosine
   matches without the …rst dimension. Based on the         2002).
   second discovery, we explored di¤erent dimensional          Later uses of LSA went beyond document retrieval
   weighting schemes. Based on these observations, we       to measure semantic similarity between documents
   recommend a new algorithm for LSA cosine com-            (Wiemer-Hastings, Wiemer-Hastings, Graesser, &
   putation such that LSA becomes more sensitive to         TRG, 1999). This use of LSA is more e¢cient than
   relevant similarities and di¤erences.
                                                            retrieving documents from the original corpus. To
                                                            determine the semantic similarity of two documents,
                    Introduction                            only one cosine is needed and is computed from
Latent Semantic analysis (LSA) is an application            word vectors, whereas thousands of cosine calcula-
of principal component analysis (PCA) to natural            tions must be completed for document retrieval. In
language understanding. LSA reduces a large body            recent years, researchers have used LSA to compute
of documents into a compact matrix representation           semantic similarities in a variety of applications. For
(with only a few hundred columns), such that each           instance, LSA has been used as an automated essay
row of this matrix represents a word (or a token)           grader, comparing student essays with ideal essays
that appears in the document collection in the form         (Foltz et al., 1998). It has been used as a mea-
of vector. Such a mathematical representation of            surement of coherence between successive sentences
words as vectors captures the semantic relationship         (Foltz et al., 1998). LSA has in fact shown to per-
between words in the following way. If two words            form as well as students on the TOEFL test (Test
appear in the same document environment, accom-             Of English as a Foreign Language) (Landauer & Du-
panied by similar words, then the vector represen-          mais, 1997) and can even be used for understand-
tations of the two words are similar; that is, the          ing metaphors (Kintsch, 2000). Finally, LSA has
normalized dot product (cosine) of the two vectors          played a crucial role in the representation of world
is close to 1. In the same way similarity measures          knowledge in intelligent tutoring systems. For ex-
can be obtained for sentences, paragraphs, or doc-          ample, LSA is used in AutoTutor, an intelligent tu-
uments. For instance, for any document consisting           toring system that has tutorial conversations with
of a list of words, the vector corresponding to a sen-      students on a variety of topics. Currently, AutoTu-
tence, paragraph, or document is simply a weighted          tor has been developed for computer literacy and
vector summation of the vectors for the included            conceptual physics (Graesser, VanLehn, Rose, P., &
words. This property of a reduced semantic matrix           Harter, 2001). AutoTutor uses LSA to give mean-
representation has proven to be extremely useful in         ing to a student answer and to match that answer to
a range of applications in natural language process-        ideal good answers and bad answers (Graesser et al.,
ing (s ee Foltz, K ints ch , an d Landauer, 1998 for an     2000; Franceschetti et al., 2001; Olde et al., 2002).
overview).                                                     In our work on AutoTutor, we observed that when
   LSA was originally developed for information re-         similarity is computed between two documents, the
trieval (Deerwester, Dumais, Furnas, Landauer, &            cosine value is positively related to the size of these
Harshman, 1990). In those cases, for any query doc-         documents. We believe this is an unfortunate ar-
ument, the corresponding vector (called the query           tifact and demonstrate here that the performance
vector) is used to obtain a cosine match with all rows      of LSA can be improved signi…cantly by excluding
of the document vectors corresponding to the docu-          the …rst dimension of the LSA space in calculating
ments in corpus. By using the cosine match as the           the cosines between document vectors. To explain
                                                        587

how the performance of LSA can be improved, we            original m columns) and use the truncated U ma-
…rst brie‡y describe the mathematical foundations         trix, called term-matrix (throughout the remainder
of LSA and how LSA is used in AutoTutor. We               of this paper we will use U k to denote the truncated
then prove two theorems that show how LSA docu-           matrix). Each row of the U k is a k dimensional real-
ment matching can be improved. Finally we provide         valued vector. Similarly, the V matrix is also trun-
some general recommendations on how LSA should            cated into k columns from original n columns. The
be used in natural language understanding applica-        truncated V is called document-matrix (denoted as
tions.                                                    V k ).
                                                             In LSA, words are represented as real-valued vec-
    Mathematical foundations of LSA                       tors from the truncated U matrix. For any para-
The fundamental assumption of LSA is that the             graph, a real-valued vector can be computed from
semantics of any natural language are expressed           the vectors that correspond to the terms in the para-
by the way humans use that language. Accord-              graph. The formula for obtaining the vector is in the
ing to this assumption, the meaning of words is           form
entirely based on co-occurrences with other words.                                     x| WUk ¤ k ;                 (2)
When the language units share a common environ-           where x is an m £ 1 vector with entries either 1
ment, they must be similar in meaning. Two words          or 0, W is a m £ m diagonal matrix and ¤k is a
have similar meaning if they accompanied by similar       diagonal k £ k matrix. After vectors are obtained
words(Landauer & Dumais, 1997). For example, for          for each paragraph, then the similarity of the two
a given corpus of text that contains N distinct docu-     paragraphs are measured as the cosine value of the
ment environments de…ned as sentences, paragraphs,        two vectors:
or clusters of paragraphs, any word that occurs in
                                                                                                                |
the corpus can be expressed as a N -dimensional vec-                              (x|1 WU k ¤ k ) (x|2 WU k ¤k )
tor. Each value in the vector is either non-zero (a            s (x1 ; x2 ) =                                     : (3)
                                                                                  kx|1 WUk ¤ k k kx|2 WU k ¤ k k
function of how the words appear in the paragraph)
or a zero value (in case the word is not in the para-     Accordingly, we can formulate the following de…ni-
graph).                                                   tion:
   Consider Grolier’s (1996) Multimedia Encyclo-
pedia as a corpus. This corpus contains 44; 227           De…nition Assuming all words are indexed from
paragraphs with a total of 76; 948 distinct words.           1 to m; x1 = (x 1;1 ; :::; x 1;m ) and x 2 =
A vector with 44; 227 dimension can be used to               (x 2;1 ; :::; x 2;m ) are two m£1 vectors corresponding
uniquely identify each of the distinct words. In this        two documents, such that
vector, the components along each dimension are                              ½
computed as a function of how often each word is                                1       word j is in document i
used in the corresponding paragraph. For exam-                   x i;j =
                                                                                0 word j is not in document i
ple, assume the mth word is table (in the list of
76; 948 distinct words) and appears in paragraph              The similarity of the two documents is de…ned as
n, then the vector representation for table has a            Equation (3).
non-zero value at the nth element with a value of
f (m; n) £ G (m) £ L (m; n), where: f (m; n) is a            When k = r then U k = U and ¤ k = ¤:
function of the frequency of table occurring in the
m th paragraph; G (m) represents the weight of the                  Observations and Theorems
m th word independent of the paragraph in which
                                                          In this section, we …rst report observations and re-
it occurs; and L (m; n) represents the weight of the      sults related to the special properties of the …rst di-
m th word (i.e., table) depending on the paragraph        mension of LSA space. Then we extend these …nd-
in which it occurs (i.e., the nth paragraph). In the
                                                          ings to explore alternative ways of computing sim-
literature (Deerwester et al., 1990), G (m) is called     ilarity measures using di¤erent dimensional weight-
global weighting and L (m; n) is called local weight-
                                                          ing.
ing.
   Assume matrix A is obtained for a given cor-           The importance of the …rst dimension in
pus, then singular value decomposition (SVD) is per-      LSA
formed on A. SVD will produce three matrices: Two
orthonormal matrices, U and V, and one diagonal           As described earlier, the intelligent tutoring system
matrix § = diag (¾ 1 ; :::; ¾ r), with elements appear-   AutoTutor evaluates student answers by comparing
ing in descending order, where r is the rank of A;        them with ideal good answer information and bad
such that                                                 answers as de…ned by experts in the system’s course
                     A = U§V | :                    (1)   curriculum scripts. Physics textbooks are used to
                                                          create an LSA space (see (Franceschetti et al., 2001;
LSA truncates the U matrix into k columns (from           Olde et al., 2002)). Based on the LSA cosine match
                                                      588

between student answers and ideal good and bad an-              to the eigenvalue ¾ 21 can be written as (Power Ap-
swers, AutoTutor decides whether or not a student               proximation Method)
answered the question correctly and determines its
                                                                                           (AA| )
                                                                                                   k+1
next tutorial dialog move. However, the length of                                                      x
the utterances in the student answer and AutoTu-                             x1 = lim °                  °          (4)
                                                                                    k!1 °        | k+1 °
tor’s expectation show an interesting phenomenon.                                         °(AA )       x°
We randomly selected n (n = 1; 2; 4 : : : 512) words
from a random po ol of student contributions. That              where x is any vector in Rm that is not orthogo-
is, we mimicked student contributions with vari-                nal to x1 . We can always …nd an x not orthogonal
able numbers of words. Next, we randomly selected               to x1 from the m orthonormal vectors e1 ; :::; em ,
m (m = 1; 2; : : : 512) words from the expectations.            where the i th entry of e i; (i = 1; :::; m) is 1 and
Mimicking the authored physics curriculum scripts               all other entries are 0. Therefore, we can assume
with variable numbers of words, entries in Table 1              that all entries of x are non-negative. Because all
show the average cosine match values (100 cosines               entries of A are also non-negative, from (4) we
computed per cell) between the student contribu-                can see that all entries of AA | are non-negative.
tions and the curriculum scripts. Notice that the               Since §x1 are the only two unit eigenvectors cor-
average cosine match between documents with 256                 responding to ¾ 21 , the …rst column of U is one of
words is 0:450, while the average cosine between doc-           these two vectors, their entries are either all non-
uments with 16 words is 0:086. Our …rst observation             negative or all non-positive.
below is:
                                                                To further examine the numerical details, we have
                                                             examined the arithmetic means of the elements in
Observation 1 The cosine match between docu-
   ments is monotonically related to the number of           each column of the U matrix. We observed that the
                                                             …rst column has a signi…cantly larger mean than all
   words contained in documents.
                                                             other dimensions. This can be measured by the ra-
                                                             tio between the mean of the …rst dimension and the
                                                             length of the vector of the means for all the dimen-
           4        16       64      256     512             sions. Denote m i as the mean for ith dimension. For
     4     0:033    0:051    0:093   0:130   0:152           example, forÁ the
                                                                            qP
                                                                               physics LSA space (with 338 dimen-
     16    0:047    0:085    0:146   0:211   0:245                               338
     64    0:079    0:129    0:244   0:345   0:390           sions), m 1         i=1 m i = 0:833:Table 3 shows the
                                                                                        2
     256   0:106    0:167    0:324   0:458   0:509           same observation we found across corpora.
     512   0:107    0:176    0:336   0:483   0:535
                                                                                             1 st Dim        Mean
Table 1: Average cosine values for di¤erent docu-              Space             # Dim                                  Ratio
                                                                                             Mean            length
ment sizes.                                                    Physics           338        0:00528        0:00634      0:833
                                                               Science           365        0:00348        0:00479      0:726
   To understand the above observation, we must                Encyclopedia      496        0:00079        0:00119      0:669
further examine the LSA spaces that are used in                Narrative         277        0:00139        0:00270      0:516
AutoTutor. We observed that the …rst dimension of
each term vector always had the same sign (either            Table 2: Relative magnitude of …rst dimension for
all non-negative or all non-positive). We found the          selected LSA spaces.
same phenomenon within a variety of other corpora
(encyclopedia, science texts, etc.).
   The following theorem provides a formal proof for         Observation 2 The magnitude of the …rst compo-
the above observation.                                          nent of any document vector is signi…cantly larger
                                                                than all other components.
Theorem 1 Let A be an m £ m real matrix with all
entries non-negative. Suppose the singular value de-            Observation 2 and Theorem 1 explain Observation
composition of A is (1), where § = diag (¾ 1 ; :::; ¾ r ).   1. For any document with a large number of terms,
Assume ¾ 1 > ¾ 2 ¸ :::; ¾ r . Then all entries in the        the LSA vector of the document is simply the vec-
…rst column of U are of the same sign (all non-              tor summation of the term vectors. While all other
negative or all non-positive).                               dimensions may cancel out, the …rst dimension just
                                                             keeps adding up, so it is the value of the …rst di-
Proof The …rst column of U is actually an eigen-             mension that drives the cosine value. This critical
   vector with a unit length corresponding to the            observation led us to further examine the numerical
   maximum eigenvalue ¾ 21 of the symmetric matrix           properties of the …rst dimension. We observed that
   AA| . Since ¾ 21 is greater than all other eigenval-      the common words, such as is, the, and an, have
   ues of AA| , an eigenvector of AA | corresponding         larger values in their …rst dimension and rare words
                                                         589

have smaller values. In LSA, a measure of the com-       explored di¤erent dimensional weighting in the com-
monness is measured by weights. We examined one          putation of the cosine match between documents.
of the LSA spaces used in the AutoTutor project.         First, we examined Equation (3). We obtained the
The correlation between the …rst dimension and the       following formal results that link di¤erent measures
weights was as high as ¡0:757 (6679 words). Table        of similarity in a uni…ed formulation.
3 shows corresponding quantities for all other cor-      Theorem 2 From the De…nition before, assuming
pora for which we have created LSA spaces. These         k = r; then
spaces include physics textbooks, science textbooks,
Grollier’s encyclopedia, the encyclopedia augmented     1. If ¤ is a unit matrix, then Equation (3) is equiv-
with WordNet, and a large sample of narrative texts.        alent to weighted word matching.
Observation 3 Weights and the …rst dimension of         2. If ¤ = §; then Equation (3) is equivalent to sim-
   the LSA vectors are negatively related.                  ilarity comparison using original term-document
                                                            matrix.
                Space Name      #1                       Proof : 1) Assume k = r and ¤ is unit matrix,
                                                                                               |
                Physics         ¡0:757                                (x|1 WU¤) (x|2 WU¤)
                Science         ¡0:686                          =
                                                                         |                           |
                                                                      (x1 WU¤¤U | Wx2 ) = (x1 WUU | Wx2 )
                Encyclopedia    ¡0:573                                   |                |        |     |
                                                                =     (x1 WWx2 ) = (x1 W) (x2 W)
                Narrative       ¡0:500
                                                            so,
Table 3: Correlations between the weight for words                                         |                   |
and the …rst dimension for the words.                           (x|1 WU¤) (x|2 WU¤)             (x|1 W) (x|2 W)
                                                                    |          |             =      |       |    :
                                                                kx1 WU¤k kx2 WU¤k                kx1 Wk kx2 Wk
   The above observations and theorems point out
                                                            Notice that
that the …rst dimension of LSA vector needs spe-                             X© ¯                                     ª
cial treatment. Table 4 shows the cosine matches            x|1 WWx2 =            w 2i ¯ word i is in both documents
between document vectors without the …rst dimen-
sion. We observed that removing the …rst dimension          2) Assume k = r and ¤ = §;
made a greater di¤erence when the documents be-
                                                                                              |
ing compared were large. Notice in Table 4, that                     (x|1 WU§) (x|2 WU§ )
when documents have fewer than 64 words (which is                      |                          |
                                                               = x1 WU§ § | U | Wx2 = x1 WU§V | V§ | U | Wx2
more than most student contributions and expecta-                                                      |                |
                                                               = (x|1 WU§V| ) (x|2 WU§V | ) = (x|1 WA) (x|2 WA)
tions in AutoTutor), the LSA cosine matches are no
longer a function of the document size. In contrast,        therefore:
when documents have more than 64 words, the co-                                          |                       |
sine matches increase as a function of document size,         (x|1 WU¤) (x|2 WU¤)             (x|1 WA) (x|2 WA)
                                                                                            =                      :
but in a smaller magnitude than when the …rst di-             kx |1 WU¤k kx|2 WU¤k            kx|1 WAk kx|2 WAk
mension is retained.
                                                            LSA truncates the U matrix into U k ; which only
                4       16      64    256     512        contains a few hundreds columns and is used as an
       4   ¡0:02    ¡0:02   ¡0:06    0:07  ¡0:00         approximation of the original term-document matrix
     16    ¡0:03      0:00    0:03   0:01    0:11        A. Theorem 2 and the De…nition presented before
     64      0:01     0:09    0:06   0:02    0:08        can be understood as the following: (1) without di-
                                                         mensional weighting, the LSA cosine matching ap-
    256      0:05     0:10    0:17   0:12    0:22
                                                         proximates weighted keyword matching, and (2) us-
    512      0:04     0:13    0:14   0:12    0:20
                                                         ing dimensional weighting with singular values, LSA
                                                         cosine matching approximates context similarity.
Table 4: Average cosine values as a function of doc-
                                                            To explore other possible dimensional weighting,
ument size. For these cosines, the …rst dimension        we examined cosines using several di¤erent dimen-
was removed, otherwise, the computations for the         sional weighting schemes: Consider the dimensional
cosines were the same as those in Table 1.               weight matrix ¤ = diag (¸ 1 ; :::; ¸ k ) ;
                                                                       ¸1   = 0; ¸ i = ¾ i ¡ ¾ k+ 1; i ¸ 2        (5)
The importance of dimensional                                          ¸1   = 0; ¸ i = 1; i ¸ 2                   (6)
weighting in LSA                                                       ¸1   = 0; ¸ i = ¾ i; i ¸ 2                 (7)
In the previous section, we showed the importance
                                                                       ¸i   = ¾ i ¡ ¾ k+1 ; i ¸ 1                 (8)
of the …rst dimension of LSA vector. This …nding
motivated us to explore di¤erent ways to obtain co-                    ¸i   = 1; i ¸ 1                            (9)
sine matches between documents. In particular, we                      ¸i   = ¾i ; i ¸ 1                        (10)
                                                    590

Figure 1 gives examples showing LSA performance                                    toTutor pro ject proposal, pair-wise dissimilar to the
for the two dimensional weighting schemes. We sys-                                 sentences in set I. We computed the LSA cosines
tematically changed the number of dimensions used                                  between the similar pairs and dissimilar pairs. The
in the LSA space (from 100 to 400; step size 10).                                  discriminability was then calculated by
Using the weighting in (5) above, for related words                                               Ã           ,r                !
(for example, cat and dog) and non-related words                                                                    sd21 + sd22
(for example, force and blue), the cosine match                                               d=     m1 ¡ m2
                                                                                                                         2
is a smooth function of the number of dimensions
used. This is not true for the weighting method (6)
above.Researchers have previously examined the op-                                 where m1 and sd1 are the mean and standard devia-
                                                                                   tion for the cosine matches between set I and set II
            1                                                                      (cosine for the similar pairs), and m2 and sd2 are the
          0.8                                                                      mean and standard deviation for the cosine matches
          0.6
                                                                                   between set III and set II (cosine for the dissim-
          0.4
                                                                                   ilar pairs) The results of these analysis are shown
          0.2
                                                                                   in Table 5. We observed better performance for the
                                                                                   weighting method (5) in the sense that it provides
            0
                                                                                   the highest discriminability.
         -0.2
         -0.4
         -0.6
                 "Cat" and "Dog": Not Weighted
                 "Blue" and "Force": Not Weighted
                                                  "Cat" and "Dog": Weighted
                                                  "Blue" and "Force": Weighted
                                                                                                        not weighted       weighted
                                                                                                          method (9)        method (8)
                                                                                     keep 1 st dim
                                                                                                          3:408             2:422
Figure 1: Cosines with two di¤erent types of dimen-                                                       method (6)        method (5)
sional weighting.                                                                    remove 1 st dim
                                                                                                          3:573             6:297
timization of the number of dimensions used for an                                 Table 5: Discriminability for the four di¤erent
LSA space (Landauer & Dumais, 1997). It has been                                   weighting methods.
shown that there exists a range of about 300 § 100
dimensions that provide the best overall LSA per-
formance for most corpora. The above observation
                                                                                                      Conclusions
of smooth curves as function of number of dimen-
sions shows that with particular dimensional weight-                               In this paper, we made two important observations
ing scheme, there may be di¤erent ways in which the                                about LSA. The …rst observation concerns the non-
optimum range of dimensions can be selected.                                       negative (or non-positive) …rst component of LSA
   In addition, the smoothness of the LSA similar-                                 vectors. This is the primary reason that the sim-
ity between words when the number of dimensions                                    ilarity measure between any two documents is an
change makes intuitive sense. For example, one                                     increasing function of the document size. The sec-
would not expect huge di¤erences between cosine                                    ond important observation (outlined in Theorem 2)
matches using 300 dimensions and 310 dimensions.                                   is that there is a uni…ed mathematical expression
The smoothness of the curve we observed here is due                                for three di¤erent types of document similarity mea-
to di¤erential weighting of the dimensions. For ex-                                sures, namely (1) LSA cosine match, (2) weighted
ample, if one chose 300 dimensions, then the weight                                word match, and (3) context similarity match. Such
for the 300 th dimension is the smallest (approaching                              a uni…ed representation led us to explore di¤erent di-
zero, it plays the least amount of importance in the                               mensional weighting methods. We observed that the
computation). We have also explored other dimen-                                   weighting methods described in (5) outperformed
sional weighting schemes such as (7), (8), (9), and                                other weighting methods, including the method used
(10) de…ned above. Weighting method (5) performs                                   in the lion’s share of LSA literature. Speci…cally, it
the best in the sense that it provides the smoothest                               provided the smallest cosine changes as a function
function.                                                                          of the dimension of the space and better discrimina-
   We further explored the dimensional weighting on                                tions between sentence pairs. With the above two
3 carefully selected sets of documents from AutoTu-                                …ndings, we have the following recommendations for
tor. Each set had 42 sentences. Sentences in set                                   future use of LSA.
I and set II were from the AutoTutor curriculum                                       Theorem 1 states that the …rst non-zero com-
script for conceptual physics. Each sentence in set                                ponents of all the LSA term vectors will have the
I was an expected answer of a question; the corre-                                 same sign. When constructing a vector to repre-
sponding sentence in set II was an alternative an-                                 sent a document by summing term vectors, the …rst
swer. Therefore sentences in set I and set I I were                                component thus will grow in magnitude while the
similar in meaning in a pair-wise manner. Sentences                                other components need not. We therefore recom-
in set II I were randomly selected from the NSF Au-                                mend against interpreting absolute cosine values as
                                                                               591

a measure of the "match" between documents. In-           Foltz, P. W., Kintsch, W., & Landauer, T. K. (1998).
stead, we recommend establishing statistical distrib-           The measurement of textual coherence with la-
utions of the cosine match values for the target space          tent semantic analysis. Discourse Processes,
and using these relative units to judge the similar-            25, 285-307.
ity. For example, for documents x1 and x2 with
document sizes n1 and n2 , if a baseline (theoreti-       Franceschetti, D., Karnavat, A., Marineau, J., Mc-
cal or empirical) distribution with mean ¹n1; n2 and            Callie, G. L., Olde, B. A., Terry, B. L., &
sdn 1;n 2 is obtained, then the relative similarity can         Graesser, A. C. (2001). Development of
be computed as the following relative score:                    physics text corpora for latent semantic analy-
                                                                sis. In Proceedings of the 23rd annual confer-
                   s (x 1; x2 ) ¡ ¹n1 ;n2                       ence of the cognitive science society (p. 297-
                                                   (11)         300). Mahwah, NJ: Erlbaum.
                           sdn1 ;n2
                                                          Graesser, A., Hu, X., Person, P., Jackson, T., &
where s (x1 ; x2 ) is de…ned in Equation (3). This              Toth, J. (2002). Modules and information re-
recommendation is particularly useful when LSA is               trieval facilities of the human use regulatory af-
used in applications where the similarity measures              fairs advisor (HURAA). In M. Driscoll & T. C.
are used as selection criteria, as in AutoTutor. It             Reeves (Eds.), Proceedings for e-learning 2002:
is not necessary in document retrieval applications,            World conference on e-learning in corporate,
where the similarity measure is primarily used for              government, healthcare, and higher education
ordering the potential candidates.                              (p. 353-360). Montreal, Canada: AACE.
   Even if one uses relative score in the form of Equa-
tion (11), we recommend using dimensional weight-         Graesser, A., VanLehn, K., Rose, C., P., J., & Har-
ing, especially the dimensional weighting matrix in             ter, D. (2001). Intelligent tutoring systems
the form of (5). By using dimensional weighting                 with coversational dialogue. AI Magazine, 22,
scheme outlined in (5), the similarity measures are             39-51.
relatively robust with respect to number of dimen-        Graesser, A., Wiemer-Hastings, P., Wiemer-
sions used the LSA space. Furthermore, weight-                  Hastings, K., Harter, D., Person, N., & TRG.
ing scheme (5) removes the in‡uence of the …rst di-             the. (2000). Using latent semantic analysis to
mension and thereby increases discriminability when             evaluate the contributions of students in Au-
similarity is used in selecting candidates among mul-           toTutor. Interactive Learning Environments,
tiple alternatives.                                             8, 128-148.
                Acknowledgments                           Kintsch, W. (2000). Metaphor comprehension: A
                                                                computational theory. Psyhonomic Bul letin
The Tutoring Research Group (TRG) is an in-
                                                                and Review, 7, 257-266.
terdisciplinary research team comprised of ap-
proximately 35 researchers from psychology, com-          Landauer, T. K., & Dumais, S. T. (1997). A solu-
puter science, physics, and education (visit                    tion to plato’s problem: The latent semantic
http://www.autotutor.org).          This research con-          analysis theory of the acquisition, induction,
ducted by the authors and the TRG was supported                 and representation of knowledge. Psychologi-
by the National Science Foundation (REC 0106965),               cal Review, 104, 211-240.
the DoD Multidisciplinary University Research Ini-
tiative (MURI) administered by ONR under grant            Olde, B., Franceschetti, D., Karnavat, A., Graesser,
N00014-00-1-0600, and Institute of Education Sci-               A., & TRG. (2002). The right stu¤: Do you
ences (IES R3056020018-02). Any opinions, …nd-                  need to sanitize your corpus when using la-
ings and conclusions or recommendations expressed               tent semantic analysis? In W. G. Gray &
in this material are those of the authors and do not            C. Schunn (Eds.), Proceedings of the 24rd an-
necessarily re‡ect the views of ONR, NSF, or IES.               nual conference of the cognitive science society
We would like to thank Tom Landauer and Walter                  (p. 708-713). Mahwah, NJ: Erlbaum.
Kintsch by supplying a number of corpora used in
this study.                                               Wiemer-Hastings, P.,            Wiemer-Hastings, K.,
                                                                Graesser, A., & TRG. (1999). Improving an
                                                                intelligent tutor’  s comprehension of students
                     References                                 with latent semantic analysis. In S. La joie
Deerwester, S., Dumais, S. T., Furnas, G. W., Lan-              & M. Vivet (Eds.), Arti…cial intelligence in
       dauer, T. K., & Harshman, R. (1990). Index-              education (p. 535-542). Amsterdam: IOS
       ing by latent semantic analysis. Journal of the          Press.
       American Society For Information, 141, 391-
       407.
                                                      592

