UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Role of Causal Models in Reasoning Under Uncertainty
Permalink
https://escholarship.org/uc/item/82d1f0wr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Krynski, Tevye R.
Tenenbaum, Joshua B.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                      The Role of Causal Models in Reasoning Under Uncertainty
                                                  Tevye R. Krynski (tevye@mit.edu)
                                                Joshua B. Tenenbaum (jbt@mit.edu)
                          Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology
                                           77 Massachusetts Ave, Cambridge, MA 02139 USA
                               Abstract                                 from anything we have seen before, to support reasoning
                                                                        based on simply looking up frequencies in a table compiled
  Numerous studies of how people reason with statistical data           from past experience.
  suggest that human judgment often fails to approximate                   Here we propose an alternative account of probabilistic
  rational probabilistic (Bayesian) inference. We argue that a
                                                                        reasoning errors in laboratory tasks, based on a different
  major source of error in these experiments may be
  misunderstanding causal structure. Most laboratory studies
                                                                        conception of how uncertain reasoning operates in the real
  demonstrating probabilistic reasoning deficits fail to explain        world. We argue that human reasoning under uncertainty
  the causal relationships behind the statistics presented, or they     naturally operates over causal mental models, rather than
  suggest causal mechanisms that are not compatible with                purely statistical representations, and that statistical data
  people’s prior theories. We propose that human reasoning              typically support correct Bayesian inference only when they
  under uncertainty naturally operates over causal mental               can be incorporated into a causal model consistent with
  models, rather than pure statistical representations, and that        people’s theories of the domain. We will argue that
  statistical data typically support correct Bayesian inference         misunderstanding causal structure is a major source of error
  only when they can be incorporated into a causal model
                                                                        in standard laboratory studies of probabilistic reasoning, and
  consistent with people’s theory of the relevant domain. We
  show that presenting people with questions that clearly
                                                                        then describe two modifications to a standard task which are
  explain an intuitively natural causal structure responsible for a     each capable of eliminating the typical “base-rate neglect”
  set of statistical data significantly improves their performance.     error by clarifying the causal structure of the problem.
  In particular, we describe two modifications to the standard
  medical diagnosis scenario that each eliminates the                        Bayesian Inference and Base-rate Neglect
  phenomenon of base-rate neglect, merely by clarifying the
                                                                        We focus on diagnostic reasoning problems: inferring the
  causal structure behind false-positive test results.
                                                                        probability of a proposition H based on some observed data
                                                                        D. The normative Bayesian approach to diagnostic inference
                           Introduction                                 requires two kinds of probabilities: the prior, P( H ) ,
Can people arrive at correct probability judgments after                representing our degree of belief that the hypothesis is true
reading sufficient statistical data? Decades of experimental            before making the observation, and the likelihoods,
inquiry into intuitive statistical inference have documented             P( D | H ) and P( D | ¬H ) , representing the probabilities
the ways in which human judgment deviates from rational                 that the data would have been observed if the hypothesis
Bayesian norms. Examples include the phenomena of base-                 were true and if the hypothesis were false, respectively.
rate neglect (Kahneman & Tversky, 1982), the conjunction                Bayes’ rule then prescribes an equation for computing the
fallacy (Tversky & Kahneman, 1983), and deviations from                 posterior, our degree of belief in the hypothesis given the
the additivity principle (Villejoubert & Mandel, 2002). Yet             data: P( H | D) = P( H ) × P( D | H ) / P( D) , where P( D) is
in the real world, an environment that is saturated with                computed as P( H ) × P( D | H )+(1 − P( H )) × P ( D | ¬H ) .
useful statistical information and that continually poses                  Bayes’ theorem does not prescribe how one should set the
challenges for reasoning under uncertainty, people function             prior probability or the likelihoods, but most researchers
quite well, and far better than any artificial systems built on         have assumed that experimental participants should set them
                                                                        based on the statistics provided, specifically setting the prior
the norms of probability theory (Russell & Norvig, 2002).
                                                                        equal to the base rate. The term “base rate” refers to a
  One possible explanation for this discrepancy is that
                                                                        statistic summarizing how often H has been true in similar
laboratory studies typically present participants with                  previous situations, independent of whether D was
unnatural forms of information – single-event or epistemic              observed. The label “base-rate neglect” refers to errors in
probabilities instead of naturally sampled frequencies – and            probabilistic reasoning that appear to be due to not setting
that human minds are only designed to operate on                         P( H ) equal to the presented base rate, or to ignoring the
information in the latter, more natural format (Gigerenzer &            influence of the P( H ) term in Bayes’ rule.
Hoffrage, 1995). While we do not dispute the benefits of                   Our primary example of base-rate neglect is a word
presenting people with statistical data in frequency formats,           problem adapted from Eddy (1982) and tested in an
we doubt that the simple frequency-based algorithms of                  influential paper by Gigerenzer and Hoffrage (1995), and
Gigerenzer and Hoffrage (1995) are responsible for most of              several follow-ups (Cosmides & Tooby, 1996; Lewis &
our successful reasoning in everyday life. Real-world                   Keren, 1999; Macchi, 2000). The problem reads as follows
systems are too complex, and often sufficiently different               (from Gigerenzer & Hoffrage, 1995):
                                                                    693

   The probability of breast cancer is 1% for a woman at age forty who        made clear (Kahneman & Tversky, 1980). The frequentist
   participates in a routine screening. If a woman has breast cancer, the     hypothesis does not explain success in these cases.
   probability is 80% that she will get a positive mammography. If a
   woman does not have breast cancer, the probability is 9.6% that she will   Furthermore, the reduction of error in frequency formats
   also get a positive mammography. A woman in this age group had a           could be due to the fact that Bayesian diagnosis problems
   positive mammography in a routine screening. What is the probability       phrased in terms of frequencies are just simpler to solve,
   that she actually has breast cancer? ____ %                                involving only the addition of two whole numbers rather
   The probabilities here are called “single-event                            than the multiplication and division of six decimal numbers.
probabilities”, or “epistemic probabilities”; they refer to                      Gigerenzer and Hoffrage point to the simplicity of the
degrees of belief about an individual case rather than the                    frequency calculation as evidence that people only needed
frequency of an outcome in a series of repeated trials.                       to evolve a simple inference system, but the success of the
People are poor at solving this diagnosis problem, often                      frequentist algorithm in simple word problems is not enough
giving an answer of 70%-90%, while Bayes’ rule prescribes                     to justify the claim that people use this simple frequency
an answer of 7.8% (Gigerenzer & Hoffrage, 1995).                              computation for most real-world inferences. One never
Kahneman and Tversky (1982) used the term “base-rate                          hears a mechanic say: “If you want to estimate the chances
neglect” to characterize errors in this range because in this                 of your car breaking down on a long road trip, first think of
and similar problems people seemed to be neglecting the                       the last 1000 cross-country road trips you took….” This
base rate of 1% (the prior) in favor of the individuating                     approach only works when only one or two variables are
information (the likelihoods), rather than combining the two                  relevant, and ample statistics are available. Real-world
via Bayes’ rule to calculate the posterior. Other explanations                systems present complex patterns of correlation over many
for this phenomenon have been offered, such as the                            variables, and people typically do not have access to enough
tendency to confuse a given conditional probability with its                  observations to warrant drawing conclusions based on a
inverse (Villejoubert & Mandel, 2002). Macchi (1995) has                      simple look-up table of frequencies of previous occurrences.
catalogued incorrect answers to typical inference problems                    Rather than appealing to a large collection of similar past
and found that most instances of “base-rate neglect” are best                 experiences, we typically make judgments about the
described as calculating P( D | H ) , 1 − P( D | ¬H ) , or                    probability of a car breaking down, the chance of getting a
 P( D | H ) − P( D | ¬H ) . Few participants actually carry out a             certain job, or an acquaintance’s intentions, by constructing
Bayesian computation that neglects priors, which would                        and manipulating some kind of domain-specific causal
produce answers equal to P ( D | H ) /[ P ( D | H ) + P ( D | ¬H )] .         mental model. This capacity to reason with causal mental
   Regardless of how one categorizes errors, one thing is                     models may also be responsible for our successes – and
clear: people do not possess a general-purpose probabilistic                  failures – on probabilistic reasoning tasks in the laboratory.
reasoning engine that takes as input single-event
probabilities, sets priors and likelihoods equal to the                             Bayesian Inference with Causal Models
corresponding statistics, and outputs correct posterior                       Both the frequentist algorithms and standard Bayesian
probabilities. But if people do not have such an ability, how                 inference are domain-general and purely statistical
are they generally able to navigate the world so well?                        approaches to uncertain reasoning. We propose that rather
Gigerenzer and Hoffrage (1995) propose that people do                         than possessing a domain-general engine taking statistical
have the ability to make correct Bayesian computations, but                   data as input and producing probabilities of hypotheses as
typical laboratory problems present the statistical                           output, people naturally evaluate and interpret statistical
information in an unnatural format. They have shown that                      information within the framework of a domain-specific
questions provided in a natural frequency format, rather than                 probabilistic causal model, derived from a theory of how
a probabilistic format, can dramatically reduce inference                     particular kinds of causes produce particular kinds of effects
errors such as base-rate neglect. For instance, they tested the               in that domain. An individual’s probabilistic causal model
following “natural frequency” version of the mammography                      encompasses knowledge of which causes produce which
problem:                                                                      effects (the structure), how likely certain causes are to occur
                                                                              (the priors), and how likely a given effect is to follow from a
   10 out of every 1,000 women at age forty who participate in a routine
      screening have breast cancer.
                                                                              given set of causes (the likelihoods). This model provides
   8 of every 10 women with breast cancer will get a positive                 the knowledge base for a causal reasoning engine, which
      mammography.                                                            takes as input (1) a probabilistic causal model and (2)
   95 out of every 990 women without breast cancer will also get a positive   observations or statistical data, and is capable of producing
      mammography.                                                            probabilities of hypotheses as output. The causal reasoning
   Here is a new representative sample of women at age forty who got a
      positive mammography in a routine screening. How many of these          engine can be formally modeled using the tools of Bayesian
      women do you expect to actually have breast cancer? ___ out of ___.     networks (Pearl, 2000), but for the purposes of this short
                                                                              paper, we limit our discussion to informal graphical
   Gigerenzer and Hoffrage (1995) explain these results on                    representations of probabilistic causal models.
evolutionary grounds, arguing that “as humans evolved, the                       Graphical models have figured in many recent accounts of
‘natural’ format was frequencies as opposed to probabilities                  human categorization (Rehder, 2001; Waldmann et. al,
or percentages.” However, we know that people can use                         1995) and causal structure learning (Ahn & Dennis, 2000;
simple probabilities and percentages to reason correctly, and                 Gopnik et al, in press; Steyvers, Tenenbaum et al., in press;
can often solve more complex probabilistic reasoning                          Tenenbaum & Griffiths, 2001), but have not to date made a
problems provided that the causal relevance of all factors is                 large impact on the study of reasoning under uncertainty
                                                                          694

more generally. Yet the connections between real-world               assume that false positives are caused by noise or random
causality and uncertainty run deep – so deep that we doubt           error. Since doctors presumably trust the test, people might
there can be a complete theory of reasoning under                    further assume that the level of noise is low, and this
uncertainty that does not include, and perhaps center                assumption is incompatible with the statistics provided. In
around, causality. Pearl (2000) argues that much of the              fact, the statistics provided are not compatible with the
uncertainty of inference in an otherwise deterministic world         actual causal structure of standard mammogram screenings.
is due to multiple causal influences that can produce the               Gigerenzer and Hoffrage adapted the probabilistic version
same effect. For instance, coffee is occasionally bitter, but        of the breast cancer problem from Eddy (1982), which
this is not due to a stochastic mechanism that unpredictably         describes the true statistical nature of mammograms. We
makes coffee bitter; rather it is due to one of several hidden       found several important discrepancies between the true
causal influences: over-roasting or burning the coffee. If one       statistics in Eddy (1982) and those presented to participants
wishes to know the probability that a given cup of coffee            by Gigerenzer and Hoffrage (1995), which could be at least
will be bitter, the first step should be to identify the potential   partly responsible for their participants’ poor performance.
causes of bitterness and then to investigate them, (e.g., how
long has the pitcher been on the burner, etc.), rather than to       1. In Eddy’s paper, the likelihoods of 80% and 9.6% are
start with a statistical analysis, e.g., estimating the                  not for women receiving routine screenings. The
proportion of bitter cups of coffee you’ve had in your                   numbers come from Snyder (1966, p. 217), whose
lifetime. As all statistical correlations are ultimately a result        statistics are of women who already have a breast mass
of (perhaps very indirect) differential causal influences, we            (a lesion): “The results showed 79.2 per cent of 475
expect reasoning under uncertainty to be sensitive to the                malignant lesions were correctly diagnosed and 90.4 per
causal structures that create uncertainty in the first place.            cent of 1,105 benign lesions were correctly diagnosed”
   The connection between causality and uncertain reasoning              (Snyder, 1966). Gigerenzer and Hoffrage chose to apply
was one of many directions pioneered by Tversky and                      the likelihood of 9.6% to all women without cancer,
Kahneman. Tversky and Kahneman (1980) found that                         rather than just those with benign lesions. Participants
providing “causally relevant” base rates improved                        thus had no indication that benign lesions are actually
probabilistic inference, but they did not explain why, or                the cause of the false positives.
even define what they meant by “causally relevant”. Their            2. The structure of the problem is misleading, by simply
most explicit proposal was that “base-rate information                   giving a probability of 9.6% that a woman without
which is not incorporated into a causal schema, either                   cancer will get a positive mammography. This could be
because it is not interpretable as an indication of propensity           interpreted to mean that if this woman takes the
or because it conflicts with an established schema, is given             mammogram 1000 times, she will receive a positive
little or no weight.” Tversky and Kahneman treated causal                result approximately 96 times. However, the facts of the
schemas as potential sources of error in statistical reasoning,          matter are quite different: the size and density of the
whereas we take them as necessary substrates for                         benign lesion is actually the major determinant of the
probabilistic inference to succeed in complex, everyday                  false positive, and this does not change from moment to
scenarios. The effects of causal schemas are not indicators              moment. So, while it is true that 9.6% of women with
of how some “pure” statistical reasoning engine may go                   benign lesions will receive a positive mammogram, it is
wrong, but the sign that people are not doing “pure”                     not true that any individual will have a 9.6% chance;
statistical reasoning at all; they are doing intrinsically causal        some will have a high chance and others a low chance.
reasoning, by computing probabilities over causal mental
models.                                                                  As Gigerenzer and Hoffrage have described it, the
   Our goal is to go beyond the notion of “causally relevant”        mammogram appears to be an extremely error-prone test:
base rates by examining more precisely how causal mental             the mammogram will come back positive nearly 10% of the
models provide the substrate for reasoning under                     time when testing a woman without cancer, for no reason
uncertainty, and how those models are constructed. We                whatsoever. How could the medical community trust such a
view causal models as transient mental representations               test, with a noise rate 10 times higher than the base rate of
constructed on the fly to solve specific problems, based on          cancer (1%)? We believe a principal reason people perform
both given information and the constraints imposed by                so poorly is that they have difficulty understanding how
people’s domain theories. For instance, one’s theory of              such a high false-alarm rate could result purely from noise
electricity should not allow one to construct a causal model         (the only cause of a false alarm they are aware of) given that
in which taking the batteries out of a device causes it to start     doctors trust this test enough to declare the result “positive”.
working. Our evidence suggests that any piece of given
information – base rates, likelihoods, or qualitative                              Probabilistic Causal Models
statements – will only be used if people can incorporate it          A basic causal model for this scenario is depicted in Figure
into a causal model compatible with their domain theory.             1A, in which a positive mammogram can result from one of
   More specifically, we will argue that the difficulty in the       two independent and stochastic causes: the patient having
probabilistic version of the mammogram problem stems not             cancer or the test having noise. Formally, this model can be
from neglecting the base rate, but from misunderstanding             represented as a Bayes net with a noisy-or parameterization
the causal mechanism behind the false-positive rate. Based           (Cheng, 1997; Pearl, 2000). If there were only one potential
on the information provided in the problem, people may               cause, the probability that the effect occurs is just the base
                                                                 695

rate of the cause times the causal power of that cause (a           Method
conditional probability, between 0 and 1; Cheng, 1997);             Participants. 73 airplane passengers were recruited while
with multiple potential causes, the probability that the effect     waiting for their flights to begin boarding. Their only
occurs is equal to the probability that one or more of its          compensation was temporary alleviation of boredom.
causes occurs and succeeds in causing the effect (treating
both the occurrence of causes and their causal powers as            Design. Participants were given paper-and-pen versions of
independent).                                                       Gigerenzer’s breast cancer question, with the modification
   Suppose a participant believes a positive mammogram to           that the test has three possible results: “positive”,
be tantamount to a doctor’s diagnosis of breast cancer. This        “uncertain”, and “negative” (inspired by Eddy, 1982).
is not implausible: doctors as a rule avoid scaring patients        Participants received one of two versions: in one, a woman
unnecessarily, and it is common for them to say, “You have          gets a “positive” result; in the other she gets an “uncertain”
some indications consistent with disease X, but it’s probably       result. The numbers were exactly the same in both versions,
nothing”. If instead the doctor says, “You’ve tested positive       except that the conditional probabilities for “positive” and
for breast cancer,” there should be a good chance that you          “uncertain” were switched, so the same calculations were
actually have cancer. In this case, people may assume that          required in both versions. The questions follow:
the base rate of noise would not be higher than the base rate
of cancer. Otherwise, the doctor would say, “It could be             “Positive” Question
cancer, but there’s a good chance it was just noise”. This             Women at age 40 are often encouraged by their doctor to participate in a
assumption, however, is inconsistent with the high 9.6%               routine mammography screening for breast cancer. The mammogram
                                                                      has 3 possible results:
false-positive rate and the causal model described above; at              Positive: the patient has breast cancer. This results when tumors are
most, the false alarm rate could equal the base rate of cancer                found that are definitely cancerous.
(1%). People reasoning with this model could become                       Uncertain: the patient may have breast cancer. This result occurs
confused at this point and just look for some way to                          when tissue exists that may be normal breast tissue, benign tumor,
combine the given numbers to obtain a reasonable estimate.                    or cancerous tumor. More testing is needed to determine whether
                                                                              the patient has breast cancer.
   As discussed above, the model in Figure 1A does not                    Negative: the patient does not have breast cancer.
reflect the true causal structure of the test. Figure 1B shows         From past statistics of routine mammography screenings, the following
a more realistic model, in which the source of the false                  is known:
positives is an alternative tissue anomaly: dense benign               1% of the women who have participated in past screenings had breast
                                                                          cancer at the time of the screening.
lesions. Now, the 9.6% false-positive rate can be naturally            Of the 1% who had breast cancer, 20% tested 'uncertain' during the
interpreted as the approximate base rate (for women with a                mammogram (further testing was required to determine that they had
breast mass) of having a benign lesion dense enough to                    breast cancer), and the other 80% tested ‘positive'.
cause a positive mammogram. This interpretation is                     Of the 99% of women who did not have breast cancer, 2% tested
perfectly consistent with people’s background knowledge                   ‘uncertain’ (further testing was required to determine that they did not
                                                                          have breast cancer), 9.6% tested 'positive', and the other 88.4% tested
that tissue anomalies (e.g., pimples, moles, birthmarks, or               'negative'.
bumps) are often harmless.                                             Suppose a woman in this age group participates in a routine
                                                                          mammography screening and the test result is 'positive'. Without
(A)                            (B)                                        knowing any other symptoms, what is the probability that she actually
                                                   Dense Benign           has breast cancer?
      Cancer          Noise          Cancer           Lesion
                                                                    “Uncertain” Question
                                                                       [first 14 lines identical to “positive” question]
             Positive                        Positive                  Of the 1% who had breast cancer, 20% tested 'positive' during the
           Mammogram                       Mammogram                      mammogram, and the other 80% tested 'uncertain' (further testing was
                                                                          required to determine that they had breast cancer).
Figure 1: (A) basic causal model of mammogram, with noise.             Of the 99% of women who did not have breast cancer, 2% tested
   (B) more accurate model with specific alternative cause.               ‘positive’, 9.6% tested 'uncertain' (further testing was required to
                                                                          determine that they did not have breast cancer), and the other 88.4%
                                                                          tested 'negative'.
                       Experiment 1                                    Suppose a woman in this age group participates in a routine
Our first experiment directly tested the idea that people                 mammography screening and the test result is 'uncertain’. Without
might tacitly assume a positive result to be tantamount to a              knowing any other symptoms, what is the probability that she actually
                                                                          has breast cancer?
doctor’s diagnosis of cancer. We hypothesized that people
would better understand the given statistics if most women          Results and Discussion
without cancer who did not test “negative” received an
“uncertain” result rather than a “positive” one. Since a            A one-way ANOVA of the raw responses revealed a
doctor’s report of “uncertain” implies that she believes the        significant difference between the two versions (F(1,71)=
                                                                    21.59, MSE=897.36, p<.0001). We classified as base-rate
test outcome could well be the result of random noise,
                                                                    neglect any answer greater than or equal to 70%. Since the
participants could naturally incorporate the 9.6%
                                                                    exact correct answer of 7.8% is difficult to calculate, we
“uncertain” rate in healthy women as the base rate of the           classified as “close” any answer between 5% and 12%
noise variable in Figure 1A.                                        inclusive. We also classified answers of 1% or 2% as base
                                                                    rate overuse. The result was a significant difference between
                                                                    the two versions (χ2(3) = 16.15, p < .005).
                                                                696

       Table 1: “Positive” versus “Uncertain” Questions                        result. Crucially, both versions required the exact same
   Mammogram         Base-rate        Close           Base-rate      Other     Bayesian formula to calculate the answer. To minimize
                     Neglect          Answer          Overuse                  arithmetic errors, participants were allowed to answer with
   Positive          14               9               6              6
                                                                               either ratios or percentages. We also varied the base rate and
   Uncertain         1                15              14             8
                                                                               false-positive likelihoods (1% and 5% respectively vs. 2%
These results are consistent with our hypothesis that “base-                   and 6%), and the cover story (breast cancer and harmless
rate neglect” may arise in the basic question because people                   cyst vs. colon cancer and harmless polyp), for a total of 8
take the “positive” label to mean that the doctor trusts the                   different questions. Sample questions were as follows (for
test, thus limiting the base rate of noise to a level                          variants, see http://web.mit.edu/tevya/www/CogSci20003):
inconsistent with the high false-alarm rate. An alternative
interpretation is that participants are answering simply based                  “Statistical” Question
                                                                                  The following statistics are known about women at age 60 who
on the meaning of the words “uncertain” and “positive”,                             participate in a routine mammogram screening, an X-ray of the breast
rather than reasoning about likely levels of noise. To test                         tissue that detects tumors:
this, we gave 36 new participants a third “control” question                      About 2% have breast cancer at the time of the screening. Most of those
in which we relabeled the “positive” result “certain” and the                       with breast cancer will receive a positive mammogram.
“uncertain” result “positive”, so that “positive” now means                       There is about a 6% chance that a woman without cancer will receive a
                                                                                    positive mammogram.
the patient may have cancer, and more testing is needed:                          Suppose a woman at age 60 participates in a routine mammogram
                                                                                    screening and receives a positive mammogram. Please estimate the
 “Control” Question                                                                 chance that she actually has breast cancer.
   The mammogram has 3 possible results:
     Certain: the patient has breast cancer. This results when tumors are
        found that are definitely cancerous.                                    “Causal” Question
     Positive: the patient may have breast cancer. This result occurs when        The following statistics are known about women at age 60 who
        tissue exists that may be normal breast tissue, benign tumor, or             participate in a routine mammogram screening, an X-ray of the breast
        cancerous tumor. More testing is needed to determine whether the             tissue that detects tumors:
        patient has breast cancer.                                                About 2% have breast cancer at the time of the screening. Most of those
      [the rest of the question is identical to the “positive” question]             with breast cancer will receive a positive mammogram.
                                                                                  About 6% of those without cancer have a dense but harmless cyst, which
The incidence of base-rate neglect for this “control”                                looks like a cancerous tumor on the X-ray and thereby results in a
                                                                                     positive mammogram.
question was significantly higher than in the “uncertain”                         Suppose a woman at age 60 participates in a routine mammogram
question (6/36 versus 1/38, χ2(1) = 4.25, p < .05), but much                         screening and receives a positive mammogram. Please estimate the
lower than in the “positive” question (6/36 versus 14/35,                            chance that she actually has breast cancer.
χ2(1) = 4.78, p < .05). The only difference in the latter case
was defining “positive” as “may have cancer” rather than                          Note that this experiment did not specify the true positive
“has cancer”. This result suggests that “base-rate neglect” is                 rate, but only that “most women with breast cancer will
due to a mismatch between the information given (a false-                      receive a positive mammogram.” We made this change to
positive rate much higher than the cancer rate) and people’s                   encourage participants to provide answers based on their
domain knowledge (the only cause of false positives they                       intuition rather than memorized mathematical formulas.
are aware of is noise, and a trusted test implies a noise rate
lower than the disease rate).                                                  Results and Discussion
                                                                               Preliminary analyses showed no differences between MIT
                            Experiment 2                                       students and airport passengers, so the two groups were
While Experiment 1 focused on constraints imposed by                           collapsed for the remaining analyses. A three-way ANOVA
domain knowledge, Experiment 2 directly tests the role of                      of raw responses showed no significant interactions, with a
causal reasoning. Specifically, we investigated whether                        significant difference between “Statistical” and “Causal”
people would more easily integrate the high false-positive                     questions (F=8.33, p<.005), and no significant effect of
rate into their causal models if they knew what causes false                   cover story (F=0.43, p=.51) or prior and false-positive
positives: dense benign cysts. They could then use the                         likelihood values (F=.0052, p=.94), all with df=(1,125),
causal model of Figure 1B, assimilating the high false-                        MSE=836. We classified as base-rate neglect any answer
positive rate as the base rate of an alternative kind of tissue                greater than or equal to 70%. We classified as correct any
anomaly, which would not be inconsistent with their domain                     answer equal to between 80% and 100% of the correct ratio
knowledge.                                                                     or percentage. (This range accommodates the fact that most,
                                                                               but not all, women with cancer receive positive results.) The
Method                                                                         causal version significantly reduced base-rate neglect and
                                                                               improved correct responding as compared to the statistical
Participants. 155 people were recruited at the airport or the                  version (χ2(2) = 12.83, p < .0005) (see Table 2).
MIT campus. MIT students were compensated with candy;
airplane passengers were compensated as in Experiment 1.                                Table 2: “Statistical” versus “Causal” Questions
                                                                                  Problem            Base-rate          Correct     Base-rate    Other
Design. We posed two paper-and-pen questions, one with                            Type               Neglect            (or close)  Overuse
only statistical information about false positives and one                        Statistical        19                 24          17           16
with information about an alternative cause for a positive                        Causal             3                  40          16           20
                                                                           697

                   General Discussion                            should play a critical role in explaining how people reason
                                                                 so successfully and efficiently in an uncertain world.
   In two experiments, we gave people a natural way to
make sense of the high false-positive rate in terms of their
causal mental models of the mammogram scenario, and                                       References
thereby essentially eliminated the phenomenon of base-rate       Ahn, W., & Dennis, M. (2000). Induction of causal chains.
neglect. A total of 4 out of 117 participants exhibited base-      Proceedings of the Twenty-second Annual Conference of
rate neglect on our new questions, compared to 33 out of           the Cognitive Science Society. Mahwah, NJ: Erlbaum.
111 people on questions paralleling the original version,        Cheng, P. W. (1997). From covariation to causation: A causal
despite the required calculations being identical. Likewise,       power theory. Psychological Review. Vol 104(2), 367-405.
the incidence of correct or near-correct responses increased     Cosmides, L. and Tooby, J. (1996) Are humans good
from 33 out of 111 participants to 55 out of 117. Experiment       intuitive statisticians after all? Rethinking some
1 showed that diagnostic reasoning could be improved by            conclusions from the literature on judgment under
removing the inconsistency between an apparently high              uncertainty. Cognition, 58, 1-73.
noise rate and an apparently trusted test. Experiment 2          Eddy, D. M. (1982). Probabilistic reasoning in clinical
showed that reasoning could be improved by introducing a           medicine: Problems and opportunities. In D. Kahneman,
compelling non-noise alternative cause for the frequent false      P. Slovic, & A. Tversky (Eds.), Judgment Under
positives. We interpreted these findings as evidence that          Uncertainty: Heuristics and Biases. Cambridge..
human probabilistic reasoning operates over causal mental        Gigerenzer, G. & Hoffrage, U. (1995). How to improve
models rather than purely statistical databases. We also           Bayesian reasoning without instruction: Frequency
argued that this central role for causality in reasoning under     formats. Psychological Review, 102, 684-704
uncertainty should be considered rational and normative,         Gopnik, A., Glymour, C., Sobel D., Schulz L., Kushnir, T.,
contrary to standard assumptions in the Heuristics and             & Danks, D. (in press). A theory of causal learning in
Biases (Kahneman & Tversky, 1982) or Natural Frequency             children: Causal maps and Bayes-Nets. Psych. Review.
research programs (Gigerenzer & Hoffrage, 1995).                 Kahneman, D. & Tversky, A. (1982). Evidential impact of base
   From the standpoint of probabilistic causal models, the         rates. In D. Kahneman, P. Slovic, & A. Tversky (Eds), Judgment
real problem behind “base-rate neglect” errors comes not           Under Uncertainty: Heuristics and Biases. Cambridge.
from having a low base rate for the cause in question, but       Lewis, C., & Keren, G. (1999). On the difficulties
from having a high false-positive rate. Assuming                   underlying Bayesian reasoning: comment on Gigerenzer
independent, probabilistically sufficient causes, as in the        and Hoffrage. Psychological Review, 106, 411–416.
noisy-or model, and assuming that each cause is relatively       Macchi, L. (2000). Partitive Formulation of Information in
rare, suggests a natural interpretation for the true positive      Probabilistic Problems: Beyond Heuristics and Frequency
rate P( D | H ) in terms of the approximate causal strength        Format Explanations. Organizational Behavior and
of H. But the false-positive rate P( D | ¬H ) , while just as      Human Decision Processes, 82, 217–236.
important as the true positive rate in purely probabilistic      Macchi, L. (1995). Pragmatic aspects of the base-rate
reasoning, has no such natural causal interpretation; an           fallacy. Quarterly Journal of Experimental Psychology A:
effect cannot result from the absence of a cause. In causal        Human Experimental Psychology, 48A(1), 188-207.
reasoning, we must come up with one or more alternative          Pearl, J. (2000). Causality: Models, Reasoning, and
causes to account for false positives. Whether that can be         Inference. Cambridge University Press.
done coherently depends on the match between the statistics      Russell, S., & Norvig, P., (1995). Artificial Intelligence: a
given in the problem and our intuitive domain theories,            Modern Approach. Prentice Hall.
which determine what alternative causes are likely to be         Snyder, R. E. (1966) Mammography: Contributions and
considered and constrain their base rates and causal               limitations in the management of cancer of the breast.
strengths. Telling people about an alternate cause whose           Clinical Obstetrics and Gynecology, 9, 207-220.
base rate could plausibly be high enough to account for the      Tenenbaum, J. B. & Griffiths, T. L. (2001) Structure
given false alarm rate, such as the “dense benign cysts” in        learning in human causal induction. Advances in Neural
the breast cancer scenario, could thus make a huge                 Information Processing Systems 13. MIT Press.
contribution to improving uncertain reasoning.                   Tversky, A. & Kahneman, D. (1983). Extensional versus
   Despite the advantages of probabilistic causal reasoning        intuitive reasoning: The conjunction fallacy in probability
over purely statistical reasoning, the successes of real-world     judgment. Psychological Review, 90, 293-315.
inference cannot be explained just by appealing to causal        Tversky, A. & Kahneman, D. (1980). Causal schemas in
models. In order to construct a causal model for a given           judgments under uncertainty. In M. Fishbein (Ed.),
scenario, people must recruit domain-specific theories that        Progress in Social Psychology. Mahwah, NJ: Erlbaum.
specify which kinds of causes are likely to produce which        Waldmann, M. R., Holyoak, K. J., & Fratianne, A. (1995).
kinds of effects. But what does that theoretical knowledge         Causal models and the acquisition of category structure.
consist of, and how is it used to constrain causal model           Journal of Experimental Psychology: General, 124, 181-206.
construction? Understanding how causal models are                Villejoubert, G. & Mandel, D. R. (2002). The inverse fallacy:
constructed through the interaction of domain theories (top-       An account of deviations from Bayes’s theorem and the
down constraints) and statistical data (bottom-up                  additivity principle. Memory and Cognition, 30(2), 171-178
constraints) is a largely open question, and the answer          Acknowledgements We thank Liz Baraff for helping with
                                                                   experiments and Tom Griffiths for statistical assistance.
                                                             698

