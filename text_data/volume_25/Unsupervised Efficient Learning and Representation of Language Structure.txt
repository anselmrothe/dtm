UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Unsupervised Efficient Learning and Representation of Language Structure
Permalink
https://escholarship.org/uc/item/46z7b7hv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Solan, Zach
Horn, David
Ruppin, Eytan
et al.
Publication Date
2003-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

 Unsupervised Efficient Learning and Representation of Language Structure
                 Zach Solan, David Horn, Eytan Ruppin                                Shimon Edelman
                        Faculty of Exact Sciences                              Department of Psychology
                            Tel Aviv University                                     Cornell University
                          Tel Aviv, Israel 69978                                 Ithaca, NY 14853, USA
                  {rsolan,horn,ruppin}@post.tau.ac.il                               se37@cornell.edu
                            Abstract                                units function as components of others, is well-suited to
                                                                    serve as a basis for a psychologically motivated theory of
   We describe a linguistic pattern acquisition algorithm that      language learning, due to its clear parallels with the no-
   learns, in an unsupervised fashion, a streamlined repre-
   sentation of corpus data. This is achieved by compactly          tion of unitization that arises in cognitive psychology [6].
   coding recursively structured constituent patterns, and by       Recent developments in probability and information the-
   placing strings that have an identical backbone and simi-        ory and in computational learning have rendered distri-
   lar context structure into the same equivalence class. The       butional [1] methods of linguistic unitization both more
   resulting representations constitute an efficient encoding
   of linguistic knowledge and support systematic general-          tractable and more readily relatable to grammar-based
   ization to unseen sentences.                                     formalisms [7]. The present paper describes a project
                                                                    that aims to transform the idea of the emergence of dis-
                                                                    tributional syntactic and semantic knowledge into a con-
                         Motivation                                 crete computational model, constrained by psycholin-
Considerations of representational parsimony dictate that           guistic data and striving for biological plausibility.
the explanation for the pattern of acceptable sentences in
a language be as concise as possible. A reduced repre-                                The ADIOS model
sentation of linguistic knowledge need not, however, take
the form of a meta-language such as a prescriptive rule-            The ADIOS (Automatic DIstillation Of Structure) model
set or grammar [1]. Instead, syntax may constitute an               constructs syntactic representations of a sample of lan-
abstraction, emerging from a corpus of language [2], yet            guage from raw, unlabeled corpus data. The model has
coexisting within the same representational mechanism               two components: (1) a Representational Data Structure
that embodies the data. The process of abstraction can              (RDS) graph, and (2) a Pattern Acquisition (PA) algo-
be guided by principles such as complementarity of dis-             rithm that learns the RDS in an unsupervised fashion.
tributions: tokens that function similarly in some sense            The PA algorithm aims to detect patterns — repetitive
(phonological, morphological, syntactic or semantic) but            sequences of “significant” paths (strings) of primitives
represent systematic rather than free variation will form           occurring in the corpus. In that, it is related to prior
complementary distributions or classes (e.g., [1, 3]).              work on alignment-based learning [8] and regular ex-
   In thinking about emergent regularities [2], or                  pression (“local grammar”) extraction [9] from corpora.
syntactic-semantic constructions [4], we adopt Lan-                 We stress, however, that our algorithm requires no pre-
gacker’s vision:                                                    judging either of the scope of the primitives or of their
                                                                    classification, say, into syntactic categories: all the in-
   “. . . particular statements (specific forms) coexist            formation needed for its operation is extracted from the
   with general statements (rules accounting for those              corpus in an unsupervised fashion.
   forms) in a speaker’s representation of linguistic                  In the initial phase of the algorithm, the text is seg-
   convention, which incorporates a huge inventory                  mented into the smallest possible morphological con-
   of specific forms learned as units (conventional ex-             stituents (e.g., ed is split off both walked and bed; the
   pressions). Out of this sea of particularity speakers            algorithm later discovers that bed should be left whole,
   extract whatever generalizations they can. Most of               on statistical grounds).1 This initial set of unique con-
   these are of limited scope, and some forms cannot                stituents is the vertex set of the newly formed RDS
   be assimilated to any general patterns at all. Fully             (multi-)graph. A directed edge is inserted between two
   general rules are not the expected case in this per-             vertices whenever the corresponding transition exists in
   spective, but rather a special, limiting case along a            the corpus (Figure 1(a)); the edge is labeled by the sen-
   continuum that also embraces totally idiosyncratic               tence number and by its within-sentence index. Thus,
   forms and patterns of all intermediate degrees of                corpus sentences and sub-sentences initially correspond
   generality.” [5], p.43.
                                                                        1
                                                                          We remark that the algorithm can work in any language,
Langacker’s conception of grammar as an inventory of                with any set of tokens, including individual characters – or
linguistic units, which is structured in the sense that some        phonemes, if applied to speech.
                                                               1106

                                                  within-sentence index
                                                      Sentence Number
                            (a)                                                    node
                                        (1)   where                                                                   cat
                                                                                                                101
                                                                  edge
                                                            (2)                                                                   (5)
                                104
                                                                                                      (4)                                   (6)
                                     (1)          101               (2)        101 (3)
                          BEGIN                              is           that            a                           dog
                                     (1)          102                          102 (3)                 (4)
                                                                                                           104
                                                                                                                            (5) 102
                                                                                                                            (5)
                                                                                                                                          ? (6)
                                                                                                                                            (6)
                                                                                                                                                END
                                103                                 (3)        103 (4)
                                                           (2)                                                                        103   (7)
                                                                  (3)                                       (5)
                                          (1)   and
                                                                           104   the        (4)                      horse       (6)
                                                                                                                 cat
                            (b)
                                    where     104       is          that            a                            dog
                                                                                                                           104
                                                                                                                                      ?
                                                                                                               horse
                                                                        and
                                                                                              the
                                                                               104
                                                       103    103                                                               103
                          BEGIN                                                                   cat
                                                              102                                                               102             END
                                                                                                  dog
                                                              101                                horse                          101
                                                                                             PATTERN 140        Equivalence Class 200
                            (c)                                       cat                                     eat
                                                       the            dog              is                   sleep               ing
                                                                     horse                                   play
                                                           Equivalence Class 200                new equivalence class 201
Figure 1: (a) A small portion of the RDS directed multi-graph for a simple corpus containing sentences #101 (is that
a cat?) #102 (is that a dog?) #103 (and is that a horse?) #104 (where is the dog?). Each sentence is depicted
by a solid colored line; edge direction is marked by arrows and is labeled by the sentence number and within-sentence
index. The sentences in this example join a pattern is that a {dog, cat, horse} ?. (b). The abstracted pattern and the
equivalence class associated with it are highlighted (edges that belong to sequences not subsumed by this pattern, e.g.,
#104, are untouched). (c) The identification of new significant patterns is done using the acquired equivalence classes
(e.g., #200). In this manner, the system “bootstraps” itself, recursively distilling more and more complex patterns.
This kind of abstraction also supports generalization: the original three sentences (shaded paths) form a pattern with
two equivalence classes, which can then potentially generate six new sentences (e.g., the cat is play-ing and the
horse is eat-ing).
to paths in the graph pathi = {ci1 → . . . , cik }, a path                                       In equations 1-3, L is the length of the typical pattern
being a sequence of edges that share the same sentence                                     the system is expected to acquire, k is the actual length
number. Paths that correspond to entire sentences start at                                 of the candidate pattern, P (k) is the probability of the k th
the ’BEGIN’ node and end at the ’END’ node.                                                order statistics (k-gram) over the path {c1 → . . . → ck }
   In the second phase, the algorithm repeatedly scans                                     that is embedded in the graph, and P (2) is the probability
the RDS graph for Significant Patterns (SP), which are                                     of the second order statistics (bi-gram) of the same path
then used to modify the graph. Each SP consists of a                                       in the graph.2 Thus, P (2) corresponds to the probability
non-empty prefix (a sequence of graph edges), an equiv-                                    of a random walker on the graph to have traversed the
alence class of vertices, and a non-empty suffix (another                                  path c1 → c2 , . . . , → ck , while P (k) corresponds to the
sequence of edges; cf. Figure 1(a)). For each path pathi ,                                 probability of a walker with an unlimited memory span
the algorithm constructs a set sj = {p1 , . . . , pm } of                                  to complete the same path. To calculate the significance
paths of the same length as pathi that together could                                      of the entire set of candidate paths, P (k) and P (2) should
form a pattern. Of the many possible candidate sets, the                                   be summed over all the candidate paths in the set.
algorithm analyzes all those whose size exceeds a certain
fixed threshold. Each candidate set is assigned a score S
                                                                                                  2 (1)
that assesses its likelihood of capturing a significant reg-                                        P (pathi ) corresponds to the “first order” probabil-
ularity rather than a random fluctuation in the data. The                                  ity of choosing the set of nodes c1 , . . . , ck without taking
                                                                                           into account their sequential order along the path. Thus,
definition of the score S (equation 1) combines a syn-
                                                                                           P (1) (pathi ) = P (c1 )P (c2 )P (c3 ) . . . P (ck ). P (2) is a bet-
tagmatic consideration (preferring longer paths) with a                                    ter candidate for identifying significant strings (as opposed to
paradigmatic one (preferring informationally significant                                   mere sets of nodes), because it takes into account the sequence
equivalence classes).                                                                      of nodes along the path.
                                                                               1107

                                                                                                                    .
                                            2
                                                                                                      
         S(pathi )     = e−(L/k) P (k) (pathi ) log P (k) (pathi )/P (2) (pathi ) ,                          pathi = c1 → c2 → . . . → ck (1)
      P (k) (pathi ) = P (c1 )P (c2 |c1 )P (c3 |c1 → c2 ) . . . P (ck |c1 → c2 → . . . → ck−1 )                                               (2)
    CP (2) (path )     = P (c1 )P (c2 |c1 )P (c3 |c2 ) . . . P (ck |ck−1 )                                                                    (3)
      1          i
              C2
   These probabilities can be estimated from frequencies                             must exceed a fixed threshold α) and is added as a new
that are immediately available in the graph. Given the                               vertex to the RDS graph, replacing the constituents and
path pathi = c1 → c2 → c3 → . . . → ck , its prob-                                   edges it subsumes (Figure 1(b)). Note that only those
ability P (k) (pathi ) is the product along the path of the                          edges of the multi-graph that belong to the detected pat-
                                                      C
probabilities p(cj |c1 → cC2 → . . . cj−1 ), 1 < j ≤ k,
                                   3                   4
                                                                                     tern are rewired; edges that belong to sequences not sub-
each of which is equal to the number of edges connect-                               sumed by the pattern are left intact. This highly context-
ing the path c1 → c2 → . . . cj , divided by the number                              sensitive approach to pattern abstraction, which is unique
of edges connecting the path c1 → c2 → . . . cj−1 (see                               to our model, allows ADIOS to achieve a high degree of
Figure 2). P (2) is the product along the path of the prob-                          representational parsimony without sacrificing its gener-
abilities p(cj |cj−1 ), 1 < j ≤ k, each of which is equal                            alization power.
to the number of edges connecting cj−1 → cj , divided                                   During the pass over the corpus, the list of equiva-
by the total number of out-edges at node cj−1 .                                      lence sets is updated continuously; new significant pat-
                                                                                     terns are found using the current equivalence classes. For
                                                                           (a)
         C1                                                                          each set of candidate paths, the algorithm tries to fit one
                                                                                     or more equivalence classes from the pool it maintains.
                                                                                     Because a constituent can appear in several classes, the
                        P(c2|c1) = 3/4                                               algorithm must check different combinations of equiv-
                    C2
                                                                                     alence classes. The winner combination is always the
                                 P(c3|c1    c2) = 2/3
                                                        P(c4|c1  c2   c3) = 1/2      largest class for which most of the members are found
                               C3                    C4                              among the candidate paths in the set (the ratio between
                                                                                     the number of members that have been found among the
                                                                           (b)       paths and the total number of members in the equivalence
         C1                                                                          class is compared to a fixed threshold as the configuration
                                                                                     acceptance criterion). When not all the members appear
                                                                                     in the existing set, the algorithm creates a new equiva-
                    C2
                         P(c2|c1) = 3/4                                              lence class containing only those members that did ap-
                                  P(c3|c2) = 2/3
                                                                                     pear. Thus, as the algorithm processes more and more
                                                           P(c4|c3) = 2/3            text, it “bootstraps” itself and enriches the RDS graph
                               C3                    C4                              structure with new SPs and their accompanying equiva-
                                                                                     lence sets. The recursive nature of this process enables
                                                                                     the algorithm to form more and more complex patterns,
Figure 2: (a) The k-gram model used to calculate P (k) ,                             in a hierarchical manner.
the probability of a walker starting at node c1 to reach the                            The relationships among the distilled patterns can be
node c4 via the path c1 → c2 → c3 → c4 . The k th order                              visualized in a tree format, with tree depth corresponding
                                                                                     to the level of recursion (e.g., Figure 3). This tree can be
probability of a path of length k is the product of k con-
                                                                                     seen as a blueprint for creating valid sequences of con-
ditional probabilities along the graph, each of which is                             stituents (strings). The number of all possible string con-
equal to the number of paths that coincide along the ex-                             figurations can be estimated and compared to the number
act sequence of nodes ending at cj , 1 < j ≤ k, divided                              of examples seen in the training corpus. The reciprocal
by the total number of paths that reach cj−1 . In this ex-                           of their ratio, η, is the generalization factor, which can
ample k = 4, and P (4) (c1 → c2 → c3 → c4 ) = 41 .                                   be calculated for each pattern in the RDS graph (e.g.,
(b) The k-gram model can be compared to the bi-gram                                  in Figure 1(c), η = 0.33). Patterns whose significance
model, in which there is no information about the his-                               score S and generalization factor η are beneath certain
tory of the walker moving along the graph (this is illus-                            thresholds are rejected. The PA algorithm halts if it pro-
trated by using the same color for all the paths in the                              cesses a given amount of text without finding a new SP
graph); P (k) is then calculated using equation 3. Here,                             or equivalence set (in real-life language acquisition this
                                                                                     process may never stop).
P (2) (c1 → c2 → c3 → c4 ) = 13 .
                                                                                        A collection of patterns distilled from a corpus can be
   The most significant set of candidate paths is now                                seen as an empirical grammar of sorts; cf. [5], p.63. The
tagged as a Significant Pattern (its significance value S                            patterns can eventually become highly abstract, thus en-
                                                                                     dowing the model with an ability to generalize to unseen
                                                                                1108

?
      Output: BEGIN George is working extremely far away END
                                                              136                                                     usual. This enables the model to make an educated guess
                                             1                                                                        as to the meaning of the unfamiliar word, by considering
                                                                      12
                                                                                                                      the patterns that become active (Figure 5).
                              87                             6
                                                                                  10                                                           Results
                          3                                              8
                                                                                                                      We now briefly describe the results of several studies de-
                              75 5                               75                       94            137           signed to evaluate the viability of the ADIOS model, in
             2                                                                9
                                                                                                                      which it was exposed to corpora of varying size and com-
                                                                     7                                                plexity.
                      4                                                                   11          13
                                                                                                                         Emergence of syntactic structures. Figure 3 shows
     BEGIN           George     Joe
                                      Pam                   George   Joe
                                                                             Pam    a   cool             car          an example of a sentence from a corpus produced by
             Cindy                     and   Beth   Cindy                    have       fast   boat
      Beth              Jim                                    Jim                                    laptop          a simple artificial grammar and its ADIOS analysis
                                                                                                                      (the use of a simple grammar, constructed with Rmutt,
                                                                                                                      http://www.schneertz.com/rmutt, in these initial ex-
                              BEGIN George and Pam have a fast car
                                                                                                                      periments allowed us to examine various properties of
                                                                                                                      the model on tightly controlled data). The abstract repre-
    Figure 3: The outcome of the acquisition process is a                                                             sentation of the sample sentence in Figure 3 successfully
    set of significant patterns, which can be visualized recur-                                                       identified the grammatical structure used to generate its
    sively as a tree, whose depth corresponds to the level of                                                         data.
    recursion. The tree structure of a pattern can be seen as                                                            Generalization. To measure the capacity of the
                                                                                                                      ADIOS algorithm for true learning, as distinguished
    a blueprint for generating valid strings; here, the arrows
                                                                                                                      from memorization or table lookup [10], we exposed
    illustrate the generation process – a depth-first search.                                                         the algorithm to an artificial corpus generated by a very
    For each non-terminal, the children are scanned from left                                                         simple finite-state grammar. Because this grammar is fi-
    to right; for each equivalence class (underscored num-                                                            nite, so is the set of all possible sentences. We ran sev-
    bers), one member is chosen. The scan continues from                                                              eral learning sessions, in which we varied the propor-
    the node corresponding to that member, with the con-                                                              tion of sentences used to train the algorithm from 10% to
    stituents reached at the terminal nodes being written out.                                                        100%. Figure 4 presents the performance (the precision
    The figure shows a pattern (#136) generated during train-                                                         in accepting unseen sentences) versus the fraction of sen-
    ing on an artificial Context Free Grammar corpus.                                                                 tences in the training data. The model is seen to perform
                                                                                                                      nearly perfectly after exposure to 20% of the corpus.
    inputs. Generalization is possible, for example, when                                                                Novel inputs; systematicity. An important charac-
    two equivalence classes are placed next to each other                                                             teristic of a cognitive representation scheme is its sys-
    in a pattern, creating new paths among the members of                                                             tematicity, measured by the ability to deal properly with
    the equivalence classes. Generalization can also ensue                                                            structurally related items (see [12] for a definition and
    from partial activation of existing patterns by novel in-                                                         discussion). We have assessed the systematicity of the
    puts. This function is supported by the input module,                                                             ADIOS model by training the algorithm on the corpus
    designed to process a novel sentence by forming its dis-                                                          generated by the grammar of Figure 3 and by examin-
    tributed representation in terms of activities of existing                                                        ing the representations of unseen sentences. The gen-
    patterns (Figure 5). These are computed by propagating                                                            eral finding was of Level 3 systematicity according to
    activation from bottom (the terminals) to top (the pat-                                                           the nomenclature of [12]. The ADIOS system’s input
    terns) of the RDS. The initial activities wj of the termi-                                                        module allows it to process a novel sentence by form-
    nals cj are calculated given the novel input s1 , . . . , sk as                                                   ing its distributed representation in terms of activities of
    follows:                                                                                                          existing patterns. Figure 5 shows the activation of pat-
                       
                                       P (sl , cj )
                                                                                                                     tern #185 by a phrase from the test set that contains three
         wj = max P (sl , cj ) log                              (4)                                                   novel words, never before seen by the model.
                l=1..k                P (sl )P (cj )                                                                     Working with real data: the CHILDES corpus. To
    where P (sl , cj ) is the joint probability that sl and cj will                                                   illustrate the scalability of our method, we describe here
    appear in the same equivalence class. while P (sl ) and                                                           briefly the outcome of applying the PA algorithm to a
    P (cj ) are the probabilities that sl and cj will appear in                                                       subset of the CHILDES collection [13], which consists
    any equivalence class. For an equivalence class, the value                                                        of transcribed speech produced by, or directed at, chil-
    propagated upwards is the strongest non-zero activation                                                           dren. The corpus we selected contained 9665 sentences
    of its members; for a pattern, it is the average weight of                                                        (74500 words) produced by parents. The results, one of
    the children nodes, on the condition that all the children                                                        which is shown in Figure 6, were encouraging: the algo-
    were activated by adjacent inputs. Activity propagation                                                           rithm found intuitively significant SPs and produced se-
    continues until it reaches the top nodes of the pattern lat-                                                      mantically adequate corresponding equivalence sets. Al-
    tice. When the algorithm encounters a novel word, all                                                             together, 317 patterns and 404 equivalence classes were
    the members of the terminal equivalence class contribute                                                          established, representing the corpus in terms of these
    a value of  = 0.01, which is then propagated upwards as                                                          constituents.
                                                                                                               1109

S→NVN                                                                                                                                                            185       total activity=0.86
                                                                                                                                                           0.73
S → N and N V N
S → N that N V                                                                                                                                    184             1.00
S → N and N that N V
                                                                                                                                                   0.45
                                                                                                                                                                       1.00
N = {Joe, Beth, Jim, the cat, the dog, the cow}
V = {scolded, loved, adored, worshiped}                                                                              = unseen word                 133
                                                                                                                                           0.67           0.67
                          1
                                                                                                                                86                 0.01                               122
                                                                                                                            0.01                                                 0.01
                         0.9
                                                                                                                                                                              1.00
                         0.8
                                                                                                                     1.00
                                                                                                                                75 1.00                      75                         93          116
                         0.7                                                                                                                                                   1.00           1.00
  Generalization Error
                         0.6
                         0.5                                                                                   BEGIN
                                                                                                                  Beth
                                                                                                                 Cindy
                                                                                                               George
                                                                                                                   Jim
                                                                                                                   Joe
                                                                                                                  Pam
                         0.4                                                                                       and
                                                                                                                  Beth
                                                                                                                 Cindy
                                                                                                               George
                                                                                                                   Jim
                                                                                                                   Joe
                         0.3
                                                                                                                  Pam
                                                                                                                  havea
                                                                                                                  cool
                                                                                                                   fast
                                                                                                                  boat
                                                                                                            1.00            0.01          1.00            0.01           1.00 1.00    0.01   1.00     1.00
                                                                                                                    car
                                                                                                             computer
                         0.2
                                                                                                                laptop
                                                                                                                  END
                         0.1
                                                                                                             BEGIN          Linda
                                                                                                                                                                         have
                                                                                                                                                          Paul                                        END
                          0
                                                                                                                                          and                                         new    car
                               0   0.1   0.2    0.3    0.4     0.5      0.6    0.7   0.8   0.9   1
                                                      fraction of the corpus                                                                                              a
Figure 4: Top: a very simple finite-state grammar that                                                      Figure 5: The input module in action; the most relevant
can generate a corpus of 2016 phrases. Bottom: general-                                                     (i.e., highly active) pattern responding to the novel input
ization error (a measure of precision), defined as 1−hits,                                                  Linda and Paul have a new car. Leaf activation is
where hits is the proportion of the total possible number                                                   determined by equation 4, then propagated up the tree by
of phrases that has been correctly accepted by the model,                                                   taking the average at each junction.
plotted vs. the fraction of the corpus used for training.                                                   sensitivity — in particular, the manner whereby ADIOS
In this case, the model performs nearly perfectly after                                                     balances syntagmatic and paradigmatic cues provided by
exposure to 20% of the corpus. Note that this kind of                                                       the data — is mainly what distinguishes it from other
test is highly relevant to psycholinguistic explorations of                                                 current work on unsupervised probabilistic learning of
productivity, e.g., [11].                                                                                   syntax [8, 14, 15]. The present paper describes a vi-
                                                                                                            able algorithmic approach to unsupervised distributional
                                               Concluding remarks                                           learning of language patterns. The ultimate goal of this
The ADIOS model learns (morpho)syntax on the basis                                                          project is to achieve a computationally explicit, empir-
of distributional information in the “raw” input, and sup-                                                  ically proven, integrated understanding of three aspects
ports the distillation of structural regularities (which can                                                of the representation of linguistic structures:
be thought of as constructions [4]) out of the accrued                                                      theoretical, computational, and psychological.
statistical knowledge. Although our pattern-based rep-                                                         Acknowledgments. Supported by the US-Israel Bina-
resentations may look like collections of finite automata,                                                  tional Science Foundation, the Dan David Prize Founda-
the information they contain is much richer, because of                                                     tion, the Adams Super Center for Brain Studies at TAU,
the recursive invocation of one pattern by another, and                                                     and the Horowitz Center for Complexity Science.
because of the context sensitivity implied by relation-
ships among patterns. Thus, unlike probabilistic finite                                                                                           References
automata such as Hidden Markov Models, ADIOS can                                                             [1] Z. S. Harris. Language and information. Columbia
learn Context Sensitive Languages. ADIOS is also bet-                                                            University Press, New York, 1988.
ter at dealing with the problem of sparse data, because                                                      [2] P. J. Hopper. Emergent grammar. In M. Tomasello,
it bootstraps the relevant statistics using the extracted                                                        editor, The new psychology of language, pages
equivalence classes. Furthermore, the recursive nature of                                                        155–175. Erlbaum, Mahwah, NJ, 1998.
the ADIOS algorithm allows it to capture longer-range                                                        [3] J. Hutchens, M. Alder, and Y. Attikiouzel. Natural
dependencies than those found in any fixed-width win-                                                            language grammatical inference. Technical Report
dow. Finally, the sensitivity to context of pattern ab-                                                          HT94-03, University of Western Australia, 1994.
straction (during learning) and use (during generation)
contributes greatly both to the conciseness of the ADIOS                                                     [4] W. Croft. Radical Construction Grammar: syntac-
representation and to the conservative nature of its gener-                                                      tic theory in typological perspective. Oxford Uni-
ative behavior (note that ADIOS defines patterns in terms                                                        versity Press, Oxford, 2001.
of specific morphemes or combinations of other patterns                                                      [5] R. W. Langacker. Foundations of cognitive gram-
and equivalence classes, rather than in terms of ideal-                                                          mar, volume I: theoretical prerequisites. Stanford
ized “part of speech” categories; cf. [4]). This context                                                         University Press, Stanford, CA, 1987.
                                                                                                     1110

                      (a)                                                                  1903 (0.15)
                              1678 (1)                                                      1904
                                                                         1404 (1)                       1457(0.56)
                                             1679
                                                                   1405                                 1458
                                                                                    1405
                                      have
                                                                      to
                      do        you          like   want
                                                                      go
                                                                   back
                                                                  home
                                                               outside
                                                                  potty
                                                                      up
                                                                    that
                                                                     the
                                                                 ladderin
                                                                    that
                                                                     the
                                                                   back
                                                             bedroom
                                                                 bench
                                                                    box
                                                                     car
                      (b)
                                                                   chair
                                                                  circle
                                                                 closet
                                                                    cup
                                                                garage
                                                                 house
                                                                    one
                                                                   oven
                                                           refrigerator
                                                                  snow
                                                                square
                                                                  truck
                  original sentences from CHILDES                             rephrase sentences by ADIOS
                   I'll play with the eggs and you play with your Mom.        I'll play with the toys and you play with your bib.
                   there's another chicken.                                   there's another bar+b+que.
                   there's a square!                                          there's a chicken!
                   play with the cars and the people?                         play with the dolls and the roof?
                   oh ; the peanut butter can sit right there.                oh ; the peanut butter can go up there .
                   you better eat it.                                         you better finish it.
                   we better finish it ; then.                                we better hold that ; then.
                   yeah ; that's a good one!                                  uh ; that's another little girl!
                   should we put this chair back in the bedroom?              should we put this stuff in in another chick?
Figure 6: (a) Two strongly connected patterns extracted from a subset of the CHILDES collection [13]. Hundreds of
such patterns and equivalence classes (underscored) together constitute a concise representation of the raw data. Some
of the phrases that can be described/generated by patterns #1903 and #1678 are: do you have to go home?; do you
like to go in that truck?; do you want to go up the ladder?; do you like to go the bench?. None of these sentences
appear in the training data, illustrating the ability of ADIOS to generalize. The numbers in parentheses denote the
generalization factor η of the patterns and their components (e.g., pattern #1903 generates 85% new strings, while
pattern #1678 generates none). (b) Some of the phrases generated by ADIOS (left) using sentences from CHILDES
(right) as examples. The generation module works by traversing the top-level pattern tree, stringing together lower-
level patterns and selecting randomly one member from each equivalence class. Extensive testing (currently under
way) is needed to determine whether the acceptability of the newly generated phrases (which is at present less than
ideal) improves with more training data.
 [6] R. L. Goldstone. Unitization during category learn-                     [11] R. L. Gómez and L. Gerken. Infant artificial lan-
     ing. Journal of Experimental Psychology: Human                               guage learning and language acquisition. Trends in
     Perception and Performance, 26:86–112, 2000.                                 Cognitive Science, 6:178–186, 2002.
 [7] F. Pereira. Formal grammar and information the-                         [12] T. J. van Gelder and L. Niklasson. On being sys-
     ory: Together again? Philosophical Transactions                              tematically connectionist. Mind and Language,
     of the Royal Society, 358(1769):1239–1253, 2000.                             9:288–302, 1994.
                                                                             [13] B. MacWhinney and C. Snow. The child language
 [8] M. van Zaanen and P. Adriaans.          Compar-
                                                                                  exchange system. Journal of Computational Lin-
     ing two unsupervised grammar induction systems:
                                                                                  gustics, 12:271–296, 1985.
     Alignment-based learning vs. EMILE. Report 05,
     School of Computing, Leeds University, 2001.                            [14] D. Klein and C. D. Manning.           Natural lan-
                                                                                  guage grammar induction using a constituent-
 [9] M. Gross. The construction of local grammars.                                context model. In T. G. Dietterich, S. Becker, and
     In E. Roche and Y. Schabès, editors, Finite-State                           Z. Ghahramani, editors, Advances in Neural Infor-
     Language Processing, pages 329–354. MIT Press,                               mation Processing Systems 14, Cambridge, MA,
     Cambridge, MA, 1997.                                                         2002. MIT Press.
[10] S. Patarnello and P. Carnevali. Learning networks                       [15] A. Clark. Unsupervised Language Acquisition:
     of neurons with Boolean logic. Europhysics Let-                              Theory and Practice. PhD thesis, COGS, Univer-
     ters, 4:503–508, 1987.                                                       sity of Sussex, 2001.
                                                                   1111

