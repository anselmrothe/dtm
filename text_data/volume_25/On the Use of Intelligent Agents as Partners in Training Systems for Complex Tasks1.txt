UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
On the Use of Intelligent Agents as Partners in Training Systems for Complex Tasks1
Permalink
https://escholarship.org/uc/item/65b2h10k
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 25(25)
Authors
Ioerger, Thomas R.
Sims, Joseph
Volz, Richard A.
et al.
Publication Date
2003-01-01
Peer reviewed
  eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

                             On the Use of Intelligent Agents as Partners
                                 in Training Systems for Complex Tasks1
                                   Thomas R. Ioerger, Joseph Sims, Richard A. Volz
                                   Department of Computer Science, Texas A&M University
                                                  College Station, TX 77843-3112
                                              fioerger,jms3627,volzg@cs.tamu.edu
                                          Judson Workman, Wayne L. Shebilske
                                      Department of Psychology, Wright State University
                                                      Dayton, OH 45435-0001
                                     judwwork@yahoo.com, wayne.shebilske@wright.edu
                            Abstract                                sures to do more with fewer resources. In the com-
                                                                    mercial/manufacturing world, workers constantly need
   Training protocols that involve working with a human             re-training to operate new models of machines as they
   partner have been shown to be beneficial for learning            are developed, or shift processes/jobs as the market de-
   complex tasks. In this paper, we explore emulating the
   function of the partner with an intelligent agent. Given         mands. The military, too, is filled with jobs involving
   a cognitive task analysis, the task can be decomposed            operation of complex, technological equipment, which
   into cognitive components, and these behaviors can be            people (typically young adults with little prior experi-
   independently automated using agent-programming tech-            ence) must be trained rapidly to operate safely and effec-
   niques. Then a trainee and the agent can work together to
   solve practice problems, each taking responsibility for a        tively. In all these environments, the transition between
   different function. We argue that it is desirable not only       textbook knowledge and practical skill must be achieved
   for the agent to produce correct and consistent behavior         efficiently and effectively, minimizing both training re-
   (e.g. demonstrating the optimal strategy), but also to ap-       sources (need for instructors, dedicated training equip-
   pear realistic (human-like, including errors), and we show       ment, simulators, etc.) and time.
   how this can be achieved by introducing randomness in
   an agent’s decisions. We implemented a Partner Agent                Complex tasks present a fundamental challenge for
   for Space Fortress, a laboratory task designed to be repre-      the development of training systems. On the one hand,
   sentative of complex tasks, and found that trainees who          whole-task training (e.g. immersion, on-the-job training)
   swapped roles with this agent during training achieved           is ineffective because the novice is usually over-whelmed
   significantly higher performance scores asymptotically
   than those who trained using a standard (whole-task)             by the complexity of the task. They fail at first, but they
   training protocol. We also simulated 3 different levels          are often unable to comprehend why or make incremen-
   of expertise and found that trainees who worked with an          tal improvements. On the other hand, part-task training
   “expert-level” agent received the most benefit.                  (such as learning to steer a car and operate the pedals sep-
                                                                    arately) can be less effective because novices do not get
                                                                    a chance to experience the inter-play between the parts.
                       Introduction                                 Part-task training does not allow trainees to practice per-
In the modern industrialized and technology-driven                  forming the sub-tasks together, which often requires sig-
world, there is a great need for development of new train-          nificant additional effort to manage shared cognitive re-
ing methods for complex tasks. By complex tasks, we                 sources, such as dividing or shifting attention.
mean tasks that have a cognitive dimension of difficulty,              One interesting training protocol that has shown
such as demands on reasoning, attention, memory, and so             promise for complex tasks is partner-based training, for
on, not just a physical skill based on strength or dexterity        example, the AIM (Active Interlocked Modeling) pro-
(though they may include these). Examples of complex                tocol (Shebilske et al., 1992). AIM involves groups of
tasks include driving vehicles, piloting aircraft, operating        trainees working together to solve a problem (e.g. oper-
machinery, etc. Such tasks require extensive training for           ating a device within a simulated scenario). In the case
operators to learn the necessary details of how a device            of AIM-Dyad, there are two trainees acting as partners.
works, modes of operation, procedures for controlling it,           Each trainee performs part of the task while his partner
regulations and limitations, signals for error/failure con-         does the other part, and later they switch roles. This po-
ditions, and means for recovering from them. Operators              tentially solves the dilemma of complex-task training be-
must not only learn this information by memory, but also            cause it reduces their individual demands, allowing them
be able to apply it in practice, often under real-time con-         to focus on automating one cognitive component at a
straints with competing demands on perception, mem-                 time, which is more tractable, while maintaining the con-
ory, etc. Schneider (Schneider, 1985) gives a characteri-           text of the whole task.
zation of complex tasks and discusses issues for training.             It has been shown that the AIM protocol of partner-
   The need for new training systems is more prevalent              based training can improve performance over standard
today than ever before, especially given economic pres-             individual practice (e.g. whole-task training) for the
    1
      This work was supported in part by MURI grant #F49620-        same amount of time. For example, in Space Fortress,
00-1-0326 from DoD and AFOSR.                                       a computer-based laboratory task designed to emulate
                                                               605

characteristics of a complex task (specifically, flying a     with errors), the trainee will not be able to learn cor-
fighter jet), AIM-Dyad was implemented by alternatively       rect behaviors, while if the agent’s actions are too pre-
having one trainee operate the joystick while the other       cise (i.e. expert), this might be incomprehensible to the
trainee operated the mouse. Over a total of 10 hours of       novice, effectively setting an unattainable goal. There-
practice, trainees using this partner-based protocol per-     fore, we ran a second experiment where we actively ma-
formed as well at the task as individuals who had been        nipulated the level of expertise simulated by the partner
trained using whole-task training. That is, they reached      agent, and we evaluated whether there was any differ-
the same scores (individually) as a control group trained     ence in training with the different agents. The results
by simply giving them instructions and letting them try       show that training in Space Fortress with a partner agent
to maximize their score over the same amount of time          that simulates a human expert provides the most benefit.
(Shebilske et al., 1992; Arthur et al., 1997). Thus AIM-
Dyad produces an increase in efficiency over the stan-                        Partner Agent Design
dard (individual) training protocol by reducing the time,     There are a wide variety of ways in which intelli-
equipment, and trainers needed for each group in half.        gent agents could be used within a computer-based
   The fact that AIM can provide training that is equal to    training system, ranging from automated opponents to
an individualized training method is somewhat surpris-        coaches (Rickel and Johnson, 1999) to performance sup-
ing, given that each trainee experiences only half of the     port. Intelligent agents are software programs that have
hands-on experience on average. The proposed explana-         the following characteristics: 1) they are goal-oriented
tion for this is that, while trainees are performing their    (proactive, seeking to achieve goals given to them),
own part of the task, they are “modeling” the behavior of     2) they are reactive (situated within a dynamic envi-
their partner. In fact, the magnitude of performance im-      ronment in which they need to take actions to change
provement has been shown to correlate with intelligence       the state to achieve their goals), and 3) they are au-
measures of one’s partner (Shebilske et al., 1999), sug-      tonomous (can make decisions without human interven-
gesting that they are learning from each other more than      tion) (Wooldridge and Jennings, 1995). In addition,
just their own part of the task (Bandura, 1986).              agents may have other common characteristics, such as
   However, this benefit of training with a partner does      being adaptive (learning from their experiences) or co-
not intrinsically require interaction between the partners.   operative (interacting with other agents or humans).
In a previous study, the role of social variables (ranging       Intelligent agents have a potential for training that
from verbal communication to visual cues such as body         goes beyond traditional intelligent tutoring systems
language) were investigated by having the dyad perform        (ITS’s) (Anderson et al., 1990). ITS’s are systems de-
the task in physically-separated cubicles with connected      signed for training based on AI techniques, especially
consoles. Still, they reached the same level of proficiency   expert systems and case-based reasoning. The trainee
after training (Shebilske et al., 1999). Furthermore, this    is usually presented with a problem to solve, and the ITS
effect held even when trainees were told that the other       monitors the actions taken. If the problem was not solved
part of the task was being performed by a computer.           correctly, an analysis of the actions is made to identify
   This observation makes an important suggestion: that       gaps in the trainee’s knowledge, to be remediated by fur-
the role of a partner in training could be automated by       ther instruction focussed in that area. The main chal-
using an intelligent agent. Agents are software programs      lenge of an ITS is to interpret the actions the trainee
that can autonomously make decisions and act to achieve       takes and construct plausible explanations of the miss-
goals in a dynamic (real-time) environment. There are         ing/incorrect knowledge that led to those actions. This is
many possible roles that agents could play in a training      often called “user modeling,” and can be performed by
system. In this paper, we describe a principled approach      techniques ranging from abduction (proof completion)
to incorporating agents in training, called the Partner       to plan recognition to probabilistic inference (e.g. with
Agent protocol, based on the arguments for cognitive          Bayesian belief nets).
benefits given above. We discuss a number of design              While intelligent agents may incorporate user-
criteria and implementation issues in developing partner      modeling capabilities too, they go beyond ITS’s by being
agents. To test our hypothesis about the effectiveness of     able to dynamically interact with the problem-solving
this new protocol, we implemented partner agents for the      environment, and thus to drive (alter) the scenario, or
Space Fortress task, and we show that trainees were able      actively participate with the trainee in solving the prob-
to out-perform (reach higher scores than) those trained       lem. An agent can be given goals that involve helping
with the standard control (whole-task) protocol.              (facilitating) the trainee, correcting mistakes, remind-
   The success of our first experiment led to an interest-    ing, off-loading tasks (performance support), providing
ing follow-up question: Does the level of expertise sim-      advice on or explaining correct actions (decision aid),
ulated by the partner agent affect the magnitude of per-      demonstrating correct behavior, adapting the scenario to
formance improvement by the trainee? Because trainees         the trainee’s skill level (modifying events to be more
are believed to “model” the behavior of their partners, it    or less challenging), making the scenario more realis-
might be expected that different behaviors by the agent       tic by simulating elements in the scenario reacting to the
will influence trainees differently. More specifically, if    trainee’s actions (more flexible than scripted scenarios),
the agent’s performance is similar to a novice (i.e. laden    and even creating challenges by intentionally preventing
                                                          606

the trainee from succeeding by easy means and forcing          sub-function of the task, representing cognitively distinct
them to apply deeper knowledge (e.g. a tactical enemy          parts of the overall task (this would have to be based on
that is difficult to defeat in a combat simulation). These     a formal cognitive task analysis). Then, during training,
all require agent techniques such as planning and infer-       the agent would perform one part of the task while the
ence to be able to decide which actions to take to achieve     trainee performs the other.
their goals, given the current state of scenario (including       Although agents can be used to satisfy the correctness
user’s prior actions).                                         and consistency criteria, something else must be added
   Our approach, based on cognitive principles described       to introduce realism and exploration. Our approach to
above, is to use agents as active partners for trainees in     adding realism to an agent’s behavior is to artificially
solving problems. The advantage of using agents for            limit the speed and/or accuracy of responses by intro-
creating virtual partners is that agents can be used to        ducing stochastic errors. For example, random delays
demonstrate correct behavior by giving them knowledge          could be added to response times, a small percentage
of the optimal strategy. Furthermore, agents will apply        of incorrect classifications could be added to a judge-
this knowledge consistently without getting tired, giving      ment/recognition task, or a small amount of imprecision
trainees a stable target behavior to model. This assumes       could be added to a motor control component. The mag-
that: a) sufficient inputs (perceptions) are available from    nitude and frequency of mistakes made by the agent can
the simulation environment to determine the correct be-        be calibrated to measurements of human levels. These
havior, b) the correct action depends on a quantifiable        errors produce a variance in the agent’s behavior that is
judgement (i.e. not nebulous “intuition”), and c) the de-      important for both realism (humans often make mistakes,
cision can be made within the time available (i.e. the up-     especially under high task-load) and exploration (allow-
date cycle-time of the simulation).                            ing trainees to experience different parts of the problem
   The use of agents as active partners places several con-    space and practice recovering from errors).
straints on the design of the agent. To be effective as a
partner for training humans, an agent should have the fol-                           Experiments
lowing qualities:
                                                               Space Fortress
  Correctness - In order for the agent to relate the tar-     To test our hypotheses, we implemented a partner agent
   get strategy to the trainee, the agent must perform the     for training human subjects in Space Fortress. Space
   strategy correctly.                                         Fortress is a laboratory task that was designed by re-
  Consistency - The partner agent should also be con-         searchers at the University of Illinois in the 1980’s as
   sistent in its overall behavior. Inconsistency makes the    representative of complex tasks for experiments with hu-
   learning process harder for the trainee.                    man performance and learning. The subject controls a
  Realism - Relative to humans, agents have the poten-
                                                               “space ship” on a computer screen (see Figure 1). The
                                                               ship may be rotated using a joystick, and it can move
   tial to perform certain actions, make decisions, and re-    forward (in whatever direction it is pointing) by pressing
   spond with unnatural speed and accuracy, which is of        forward on the joystick to fire a thruster. The ship can
   less benefit for the purposes of demonstration. It is de-   also fire “missles” by pressing a button on the joystick.
   sirable for agents to exhibit more human-like behavior      There is a “fortress” in the center of the screen that can-
   so that the performance appears achievable to trainees.     not move, but can rotate and fire shells back at the ship
  Exploration - Exploration refers to how many differ-        to defend itself. The primary goal of the task is to de-
   ent “situations” the agent gets the human into. Without     stroy the fortress (as quickly as possible) without being
   exploration, the trainee cannot make a proper mental        destroyed. It takes ten single-shots (no faster than 250ms
   model or, for example, learn how to recover from er-        apart) followed by a double-shot (within 250ms) to de-
   rors simply because he has not experienced them.            stroy the fortress.
                                                                  In addition to the fortress, another hazard in this en-
   How can a partner agent be designed to meet these de-       vironment consists of mines that appear randomly and
sired criteria? There are a number of intelligent agent        float through the space, attracted toward the ship. When
architectures that could be used to implement intelligent      a mine appears, the subject must make a judgement about
behaviors and decision-making within a simulated en-           whether it is a friend or enemy mine before shooting at
vironment, including SOAR, PRS/dMARS, RETSINA,                 it (IFF: identify friend-or-foe). This is determined by
etc. Each architecture has a different approach to rep-        ASCII characters that appear on screen. Prior to the task,
resenting goals, domain knowledge, and actions. Each           the subject is given three characters to remember. When
architecture defines a different mechanism for determin-       one of the letters in the memory-set appears on screen
ing which sequence of actions it could take that would         with a mine, the mine is a foe; for all other characters, the
lead to accomplishing its goals by transforming the state      mine is a friend. The subject must double-click the right
of the world. Decision-making mechanisms range from            mouse-button within certain time constraints to indicate
reactive rule-based systems, to logical theorem-provers,       a foe, and then shoot the mine. If the subject makes the
to complex planning algorithms. In our approach, the           wrong choice (or does not respond quickly enough), the
appropriate knowledge is given to the agent for each           mine becomes indestructible and will continue to pursue
                                                           607

                                                                        Implementation of the Partner Agent
Session 999
 Pract.Game
                                                           Session 999
                                                            Pract.Game  We implemented an intelligent agent within the Space
  1 of 8                                                     1 of 8
                                                                        Fortress game that could control various parts of the
                                                                        game autonomously. The function-decomposition was
                                                                        based on a cognitive task analysis by Frederikson and
                                                                        White (Frederiksen and White, 1989), who analyzed the
                                                                        cognitive components of the overall task (e.g. navigation,
                                                                        aiming and firing, dealing with mines, managing missle
                                                                        resources). These tend to involve either the mouse or the
                                                                        joystick, without much interaction, so they can be easily
 Emphasize  PNTS  CNTRL VLCTY VLNER IFF INTRVL SPEED SHOTS  Emphasize
                                                                        separated. The agent was made able to control the direc-
   Total                                                      Total
   Score    200   186    203   0               0      100     Score     tion and velocity of the ship and fire missles, as a human
                                                                        could do with the joystick, and the agent is able to man-
                Figure 1: Space Fortress display.                       age IFF and select bonuses, as a human could do with the
                                                                        mouse. These functions can be decoupled and controlled
                                                                        independently, so the agent can act as a partner to the hu-
                                                                        man by controlling one device-function while the human
the ship until it hits it (causing loss of points, and some-
                                                                        manipulates the other.
times destruction of the ship) or times out and disappears.
                                                                           The agent was implemented by modifying the Space-
    Formally, the goal of the game is to maximize the To-               Fortress source code (written in the C language) to mimic
tal score. The Total score is the sum of four sub-scores:               and over-ride inputs from the physical controls (joystick
Points, Velocity, Control, and Speed (indicated at the                  and mouse). The game is designed to run on a 46 ms
bottom of the screen for instant feedback). Points re-                  update cycle, during which: a) inputs from the devices
flects the overt objectives of the game; the Points score               are sampled, b) state parameters (e.g. velocity, orienta-
increases for shooting and destroying the fortress and                  tion) of objects are modified, c) the positions of objects
mines, and decreases for getting hit by or destroyed                    on the screen are updated, and d) the game scores are
by them. Velocity scores are awarded for keeping the                    revised. During an update, the actions available to the
ship below a certain speed threshold. Control points are                agent are: turn the ship, thrust, fire a missle, identify
awarded for keeping the ship within the corridor between                mines as friend or foe, or make a bonus selection. The
the two hexagons on screen (as opposed to flying a lin-                 agent implements a decision-making procedure that in-
ear trajectory and wrapping around the screen). Speed                   volves evaluating a number of conditions, such as speed,
reflects the timing of IFF judgements. In addition, sub-                distance from fortress, appearance of mines, time since
jects must maintain awareness of their supply of ammu-                  last button-press, number of missles left, etc., to deter-
nition (missles). From time to time bonus opportunities                 mine which action is most appropriate to take at any
appear (which the subjects must learn to recognize) and                 given time.
a decision must be made whether to add missiles to the                     The initial implementation of this decision-making
ammunition supply or add points to the score.                           procedure created an agent whose performance was so
    Space Fortress represents a challenging task, both in               good that it did better than even the best-trained humans
terms of motor control as well as cognitive demands.                    (scores around 8000, demonstrating a perfect strategy).
Subjects must learn motor skills such as “tapping” the                  However, we wanted to simulate a more natural level
joystick to orient the ship (e.g. for accurate aiming),                 of expertise. The basic strategy for accomplishing this
timed responses on button presses (e.g. for firing missles,             was to add a degree of randomness to the agent’s sim-
IFF). In addition, Space Fortress has cognitive com-                    ulated control inputs. For example, humans (especially
plexity involving memory, perception, attention, and                    novices) do not fire at the SF at exactly 250ms intervals.
decision-making. Examples include: a) following the                     Furthermore, they do not always thrust in the ideal direc-
rules of the game (different responses for friend vs. foe               tion or for the perfect amount of time to properly control
mines; 10 shots followed by a double shot to destroy                    their trajectory. Thus, the decision procedure was mod-
the fortress, etc.), b) memorizing characters that indicate             ified by adding randomness to following aspects: delay
friend mines, c) keeping track of shots and missles, and                to identify mines - random between 0.2s and 1.0s (uni-
d) deciding how to make bonus selections (for details,                  form); delay in firing - follows a Poisson distribution
see (Mane and Donchin, 1989)). Furthermore, position,                   with a mean of 1s; variance in thrust applied - range of
trajectory, and velocity are manipulated through acceler-               1-3 cycles, with mean of 2 (80ms); precision of aiming -
ation (thrust) only, making it second-order control, which              rotations within 5Æ of desired.
requires complex mental (spatial) calculations of vectors.                 The behavior produced by this second version of the
Because of all these demands, Space Fortress typically                  agent was more realistic and can generate scores at the
takes on the order of 10 hours to learn. Novices typi-                  level that the best humans (i.e. “experts”) can achieve
cally score around -2000 in their first trials, and can reach           after training (around 5000 points, mean total score of
scores as high as 5000-6000 (experts) asymptotically af-                top 5% of humans after 100 games by the standard train-
ter training.                                                           ing protocol). The agent appears to take essentially the
                                                                    608

correct actions, with small but believable imperfections                                  4000
in navigation and firing; it takes a little longer for the
agent to destroy the fortress, but it still usually destroys                              3000
the fortress before making one complete cycle around the
                                                                  Space Fortress Score
                                                                                          2000
hexagon.
Design of the Initial Experiment                                                          1000
In this experiment, the hypothesis we were testing is:                                      0
Does training with a Partner Agent provide increased
performance over training with other types of (individ-                                  -1000
                                                                                                                                    Control
                                                                                                                              Partner Agent
ual) training? For comparison, we evaluated the effects
of training with the Partner Agent with a standard con-                                  -2000
trol training protocol, in which trainees simply practiced
the whole task by themselves, trying to maximize To-                                     -3000
                                                                                                  0     1    2   3   4    5     6    7   8    9   10   11
tal score All trainees received the same standard instruc-                                                               Session
tions (written and on video tape) and two initial practice
sessions for exposure, followed by an opportunity to ask                                    Figure 2: Results from the first experiment.
questions. Then all trainees performed the task for 10
3-hour sessions spread over 4 days, where each session
consisted of 8 three-minute trials, followed by 2 test tri-      Table 1: Final sub-scores after 10 session of training
als, interspersed with rest breaks. Trainees following the       (with standard errors in parentheses).
Partner Agent protocol were randomly assigned to start                                                      Control Protocol        Partner Agent
with either the joystick or the mouse, and thereafter al-                                    Velocity         49.7 (215.7)          709.1 (136.6)
ternated roles with the agent on each trial. As subjects,                                    Control          903.2 (94.8)          1077.9 (50.1)
40 male students from the Department of Psychology at                                        Speed             438.3 (69.7)          639.5 (49.6)
                                                                                             Points           750.2 (174.4)         1143.5 (189.8)
Wright State University were selected who played video                                       Total           2141.4 (435.0)         3570.0 (351.3)
games less than 20 hours per week; 20 subjects were as-
signed randomly to each protocol.
Results                                                          Partner Agent affect the magnitude of performance im-
The results of the experiment are shown in Figure 2. The         provement with training? We hypothesized that it would,
graph shows the average scores for the two test trials           based on evidence that trainees model the behavior of
at the end of each session (the baseline is the score for        their partners, and the performance improvement is cor-
the practice trials before the first session; indicated as       related with the intelligence of their partner. To test this
Session 0 in the graph). The scores are averaged over            hypothesis, we created three variants of the agent, sim-
the 20 subjects in each group, with error bars indicating        ulating three different levels of expertise: novice, inter-
standard error for each measurement. It is clear that the        mediate, and expert. These behaviors were defined oper-
Partner Agent protocol led to a higher asymptotic per-           ationally in terms of the following reference groups: “ex-
formance after 10 sessions of training that the standard         perts” were defined as those who achieved scores in the
control protocol. The final difference in performance be-        top 5% after 10 sessions of training, “novices” were the
tween the two groups is significant at the p < 0:05 level        baseline scores of these subjects prior to training (equiv-
by paired T-test (t = 2:23 > 2:04, df = 38).                     alent to the whole pool of trainees, since those who be-
   When performance is decomposed into sub-scores                came experts were not distinguishable), and “intermedi-
(Table 1), trainees with the Partner Agent were found to         ates” were defined as trainees who had reached a To-
do better on each component after 10 sessions of train-          tal score halfway between novice (baseline) and expert
ing than those who used the standard control protocol,           (which could occur in any session). Novices tend to lack
although the differences were only statistically signifi-        control, fly too fast, go outside the hexagon, and even
cant for the Velocity and Speed scores (lack of signifi-         wrap around, missing the fortress and mines with shots,
cance for Control and Points scores is potentially due to        and getting destroyed more often than destroying the
the high variance and small sample sizes). The reason            fortress, whereas experts rarely get hit, navigate slowly
that the magnitude of improvement was greater for some           around the fortress with many incremental thrusts, and
sub-scores than others may reflect differential impacts of       typically destroy the fortress within one complete pass.
observation versus hands-on practice for sub-skills, de-            The different behaviors of the Partner Agent were gen-
pending on the degree to which they rely on implicit or          erated by adding different amounts of randomness to the
explicit processing during the approach to automaticity          timing and precision of the agent’s actions. For exam-
(Goettl et al., 1997).                                           ple, the time delay in identifying mines was varied from
                                                                 around 0.5s (for experts) up to around 4s (for novices;
Effect of Level of Expertise                                     just at the boundary of when mines time-out). Aim-
The success of the first experiment led to a follow-up           ing accuracy ranged between 5 Æ (experts) and 8 Æ
question: Does the level of expertise simulated by the           (novices). Thrust durations ranged from a mean of 80ms
                                                           609

                                                                 The methods for developing the Partner Agent used in
Table 2: Target skill levels for defining behaviors of        these experiments could potentially be used for building
Novice, Intermediate, and Expert agents.                      intelligent training systems for complex tasks in other
  agent     Points    Control   Velocity  Speed    Total      domains too. The primary requirement is the avail-
  Novice      -863       462        -388     21     -768      ability of a cognitive task analysis to define the sub-
  Inter.       412      1072         673    490     2645      components of the task (at a cognitive level). These func-
  Expert      2314      1229       1132     958     5633      tions would then be automated through agent program-
                                                              ming, e.g. by building a knowledge base of goals and
                                                              actions for achieving them that could be used for making
Table 3: Results of second experiment, showing Total
                                                              decisions. The behavior of the agent can be made more
scores after training (10 sessions) with Partner Agents of    realistic (human-like) by introducing artificial errors (in-
different skill levels.                                       accuracy) and random time delays in actions taken by the
      Protocol          Mean    Std. Err. Participants
      Expert Agt.       3611      514          15
                                                              agent. While this was accomplished in this work by man-
      Intermed. Agt.    2306      475          18             ually tuning internal parameters for randomness to match
      Novice Agt.       2120      587          13             the performance of the agent to target human groups, it
      Control           2034      467          19             might also be possible to use more automated methods to
                                                              capture human-like strategies and behaviors (including
                                                              errors) directly from transcripts of human-performance
(experts; shorter impulses create finer control) to 320ms     data, such as by using reinforcement learning (Kaelbling
(novices). And the delay for bonus selection ranged           et al., 1996).
from around 0.4s (expert) to around 9s (novices), with
intermediates making judgements at around 4s. No er-                                    References
rors were introduced into the correctness of IFF judge-       Anderson, J., Boyle, C., Corbett, A., and Lewis, M. (1990).
ments, as humans are rarely observed to make mistakes               Cognitive modelling and intelligent tutoring. Artificial
                                                                    Intelligence, 42:7–49.
at this. The behavior of each agent was calibrated against
the scores and sub-scores of the target group (shown in       Arthur, W., Day, E., Bennett, W., McNelly, T., and Jordan, J.
                                                                    (1997). Dyadic versus individual training protocols: Loss
Table 2) by adjusting the internal parameters until the             and reacquisition of a complex skill. Jour. of Applied Psy-
scores were within one standard deviation (see (Sims,               chology, 82(5):783–791.
2002)).                                                       Bandura, A. (1986). Social Foundations of Thought and Ac-
   This second experiment was conducted much like the               tion: A Social Cognitive Theory. Prentice Hall, Engle-
first. Male psychology students were drawn as subjects              wood Cliffs, NJ.
and randomly assigned to one of four groups: control          Frederiksen, J. and White, B. (1989). An approach to training
(standard instruction and practice), novice partner agent,          based upon principled task decomposition. Acta Psycho-
intermediate partner agent, and expert partner agent.               logica, 71:89–146.
Trainees in each group performed 10 sessions of train-        Goettl, B., Snooks, S., Day, E., and Shebilske, W. (1997). Em-
                                                                    phasis change and verbal elaboration in skill acquisition:
ing, each consisting of 8 three-minute trials followed by           A tale of two components. In Proceedings of the Hu-
two test trials. Table 3 shows the final scores, after the          man Factors and Ergonomics Society 41st Annual Meet-
10 sessions, averaged over all subjects in each group.              ing, pages 1195–1199.
The results indicate that training with the expert Partner    Kaelbling, L., Littman, M., and Moore, A. (1996). Reinforce-
Agent provided the most benefit, and that simulating in-            ment learning: A survey. Artificial Intelligence Research,
termediate or novice levels of expertise did not provide            4:237–285.
improvements over the standard control protocol.              Mane, A. and Donchin, E. (1989). The Space Fortress game.
                                                                    Acta Psychologica, 71:17–22.
                        Discussion                            Rickel, J. and Johnson, W. (1999). Virtual humans for team
                                                                    training in virtual reality. In Proceedings of Ninth World
The fact that training with a Partner Agent improves the            Conference on AI in Education, pages 578–585.
asymptotic performance of trainees over those who use         Schneider, W. (1985). Training high-performance skills: Falla-
the standard control protocol contrasts with earlier exper-         cies and guidelines. Human Factors, 27(3):285–300.
iments with the AIM-Dyad protocol (with human part-           Shebilske, W., Jordan, J., Goettl, B., and Day, E. (1999). Cog-
ners). AIM-Dyad has consistently been shown to reach                nitive and social influences in training teams for complex
the same levels of performance as the control protocol              skills. Journal of Experimental Psychology: Applied,
(Arthur et al., 1997). The advantage of it is that partner-         5:227–249.
based training requires fewer resources, such as instruc-     Shebilske, W., Regian, J., W. Arthur, J., and Jordan, J. (1992).
tors, workstations, etc. Thus, it can be said the AIM-              A dyadic protocol for training complex skills. Human
                                                                    Factors, 34(3):369–374.
Dyad provides an improvement in training efficiency
but not effectiveness. While the Partner Agent protocol       Sims, J. (2002). The use of partner agents in training systems
                                                                    for complex tasks. MS thesis, Department of Computer
looses the gain in efficiency (since humans are training            Science, Texas A&M University.
individually again), it demonstrates an increase in train-    Wooldridge, M. and Jennings, N. (1995). Intelligent agents:
ing effectiveness, based on the statistically significantly         Theory and practice. Knowledge Engineering Review,
higher scores reached by trainees after 10 sessions.                10(2):115–152.
                                                          610

