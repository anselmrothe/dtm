UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Owls and Wading Birds: Generalization Gradients in Expertise
Permalink
https://escholarship.org/uc/item/2cf83440
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Cottrell, Garrison W.
Nguyen, Nam H.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Owls and Wading Birds: Generalization Gradients in Expertise
                                            Nam H. Nguyen Garrison W. Cottrell
                                                  {nhnguyen, gary}@ucsd.edu
                           UCSD Department of Computer Science and Engineering, 9500 Gilman Drive
                                                   La Jolla, CA 92093-0114 USA
                             Abstract                                 level, and was graded by similarity to previously learned
                                                                      items.
   Recently, Tanaka et al. (in press) have shown that                    In our previous studies, we have developed a
   subjects trained at the subordinate level (henceforth,             neurocomputational model that can explain many face and
   “experts”) exhibit an advantage over subjects trained at           emotion discrimination and perceptual expertise effects. Our
   the basic level on the same stimuli in performing                  neural network model has two modules that learn basic and
   discrimination tasks within their domain of expertise,             subordinate level classification tasks simultaneously. In this
   showing that “mere exposure” to a category is not                  paper, we apply our model to Tanaka et al.’s domain. We find
   enough to induce discrimination behavior consistent with           it necessary to add an auto-encoder module (an unsupervised
   expertise. In addition, experts generalize their                   network that extracts compact representations of its
   discrimination performance in a graded fashion to novel            environment) to our network to account for subjects’
   exemplars from known classes, as well as novel                     discrimination performance on basic-level stimuli, since it
   exemplars from novel, but related classes. We applied              helps to spread out the representation of basic stimuli as well
   our two-component neurocomputational model of                      as subordinate stimuli. The auto-encoder module can be
   perceptual expertise to this domain (Sugimoto & Cottrell,          considered as a model for mere exposure. As Harnad (1987)
   2001; Joyce & Cottrell, 2004). To our surprise, we found           pointed out, there is a need for at least three capacities in
   that we could not match the data with our original model.          order to categorize: discrimination, identification, and an
   Ironically, we needed to add a new component that                  invertible description. In our model, the first capacity falls out
   models “mere exposure” in order to account for the                 of the second two. It is the level of naming or identification
   discrimination performance on basic level categories.              that gives rise to differing levels of discrimination, but being
                                                                      able to describe the object well enough to reproduce it (as in
                         Introduction                                 the hidden layer of an autoencoder) is necessary to account
What is required for perceptual expertise? Is it simple               for discrimination of basic level (but frequently encountered)
exposure to lots of examples of a class, or is more required?         objects.
Tanaka and colleagues have shown recently that frequent
exposure to exemplars of a domain alone is not enough to               Tanaka’s Experimental Design and Results
reach expert levels of discrimination. Instead, they found,            The stimuli consisted of digitized photographs of owls and
subordinate level labeling of stimuli is required, or at least, is     wading birds (Figure 1). All participants had the same
sufficient.                                                            exposure to stimuli. Participants first completed a pre-
   Tanaka et al. (in press) trained subjects on images of owls         assessment “same/different” discrimination task to get the
and wading birds. Half the subjects learned to label owls at           baseline for later comparison. In this task, participants were
the species level (e.g., “Great Grey Owl”), and the wading             shown two bird images sequentially in time, and asked to
birds at the basic level (all were labeled “wading birds”). Half       respond either “same” or “different”. For “same” trials, the
did the opposite. For simplicity, we will describe the results         birds were two different images of the same species (e.g., two
from the point of view of the subjects who were trained to             different images of “screech owls”). For “different” trial, the
discriminate owls at the species level. Subjects were tested           birds were images depicting two species from the same
before training on their ability to discriminate species in a          family (e.g., “screech owl” and “burrowing owl”).
same/different task. The discrimination test was repeated after
training. The results were first, that the subordinate level
training produced greater post-training gains in species
discrimination relative to the basic level training. The
advantage in discrimination also transferred to novel
exemplars of trained owl species by a smaller amount, and
finally, a small, but significant advantage for novel exemplars
of novel species of owl was obtained. The trained images of
wading birds only showed small gains over their pre-training
baseline, and there was no advantage for untrained exemplars
or novel species of wading birds. Transfer of discrimination                               Figure 1: Tanaka’s stimuli.
thus only occurred in the category learned at the subordinate            During training, participants were divided into two groups,
                                                                       one group learned to classify items as a wading bird (basic
                                                                  1636

level), and as one of ten species of owl (subordinate level),
and the other group did the reverse. The participants
performed various tasks such as a naming task, category
verification, and object classification. For the keyboard
naming task, participants were shown a bird image, and asked
to identify the stimulus at either the subordinate level or the
basic level by pressing the corresponding key (e.g., “k” for
eastern screech owl, “w” for wading bird). For the category
verification task, participants were presented first with a
                                                                                      Figure 3: Greeble input stimuli.
subordinate level word label (e.g., “eastern screech owl”) or
basic level label (e.g., “wading bird”), and then a bird image.
                                                                        The images were 8-bit grayscale images consisting of 5
Their task was to identify whether the label and the image
                                                                    basic classes: symmetric, asymmetric Greebles, cups, cans,
were a match. For the object classification task, participants
                                                                    and faces. Within the Greeble classes different exemplars of
were first shown a word label which was similar to the
                                                                    the same species are represented by different surface textures
category verification task, and then two images side by side.
                                                                    applied to the same Greeble. We also shifted the Greebles
Their task was to select the image corresponding to the label.
                                                                    around by 2-3 image pixels to produce more within-species
Following training, participants were given a same/different
                                                                    variations.
sequential matching task which was the same task performed
in the pre-training phase. The matching task was partitioned
into three conditions: 1) old instances of old species (items        Image Preprocessing
seen during training) (OLD/OLD), 2) new instances of old
species (new exemplars of species seen during training)
(NEW/OLD), and 3) new instances of new species
(exemplars of species not seen during training) (NEW/NEW).
These three conditions are pictured in Fig. 1.
   They assessed d’ measures for each of these conditions for
both the basic and subordinate level training which then was
used to compare to baseline performance. The results are
shown in Figure 2.
                                                                                      Figure 4: Image Preprocessing.
                                                                        We followed the procedures introduced by Dailey and
                                                                     Cottrell (1999) to preprocess the images (Figure 4). First,
                                                                     each image was processed using 2-D Gabor wavelet filters (5
                                                                     spatial frequencies at 8 different orientations each), a simple
                                                                     model of complex cell responses in visual cortex. The filters
                                                                     were applied at 64 points in an 8x8 grid, resulting in a vector
                                                                     of 2560 elements (Dailey & Cottrell, 1999). The vectors were
                                                                     then normalized via z-scoring (scaled and shifted so that they
                                                                     had zero mean and unit standard deviation) on a per-filter
             Figure 2: Post-training discrimination.
                                                                     basis, a local operation. A principal component analysis
                                                                     (PCA) was applied to the normalized vectors. The top 40
                          The Model                                  components were saved and renormalized which constituted
Our multi-module neural network was trained to perform two           the input to the neural networks.
different tasks: a basic classification and expert (subordinate)
classification. We began with our standard 2-module neural           Neural Network Parameters
network then add an auto-encoder module. In this section, we         Networks were trained using a learning rate of 0.01 and
will describe the input database, the image pre-processing
                                                                     momentum of 0.5. In all of our experiments, each module
procedure, network configurations and the simulation
                                                                     comprised of 40 input units, 20 hidden units. The number of
procedures.
                                                                     output units was depended on the functionality of each
Input Stimuli                                                        module. Similarly, we trained all of our models until their
                                                                     RMSE reached 0.05.
To simulate Tanaka’s experiment we performed all of our
simulations using symmetric and asymmetric Greebles
                                                                                              Experiment
instead of owls and wading birds. Our network model is not
designed to handle variations in pose, scale and lighting so we      We ran each experiment on 50 networks. The training process
used simpler stimuli since these variations aren’t essential to      is divided into 2 phases. In phase 1, the basic module and the
the story.                                                           expert module are pre-trained separately. The basic module is
                                                                     trained to classify cups, cans, asymmetric, and symmetric
                                                                1637

 Greebles at the basic level, and the subordinate module is                       in Tanaka’s experiment since they didn’t have expert
 trained to classify faces at the subordinate level. This initial                 knowledge of owls and wading birds.
 training is motivated by the prior experience of human                               In phase 2, the hidden layer of the basic module is used to
 subjects before performing the actual experiment. After phase                    learn basic discriminations; and similarly the hidden layer of
 1, networks were tested on the symmetric and asymmetric                          the expert module is used to learn subordinate classification.
 Greeble images that will be trained in the next phase. Then                      Our assumption is that the brain functionally separates the
 we recorded the discrimination baseline for latter comparison.                   processing of basic and subordinate level information. Later
    During phase 2, half of the networks were trained to                          we show how this might be automated. Here we assume that
 classify asymmetric Greebles at subordinate level and                            the discrimination is based upon the module most suited to
 symmetric Greebles at basic level, and the other half did the                    the task:
 reverse. Each network was trained on 100 images: 5
 exemplars within 10 symmetric and 10 asymmetric Greeble
 species.
    Networks were then tested on 300 images comprised of all
 images used during training, 5 new exemplars within each of                       Result and Discussion Subordinate level training produced
 the 20 learned species, and 5 exemplars within 10 new                             greater advantages in the “same/different” discrimination than
 symmetric and asymmetric Greeble species.                                         basic level training. The advantages also carried over to novel
    We model discrimination performance as a function of the                       exemplars of trained species and novel exemplars of novel
 similarity of internal representations (hidden unit activations)                  species. This result is consistent with the effect reported by
 between images. To measure the representational similarity,                       Joyce and Cottrell (2004). They have shown if a category is
 we computed the correlation between the vectors of hidden                         learned at the subordinate level, their neural networks are
 unit activations for two different input patterns.                                more sensitive to the differences among the individuals in that
          similarity (v1 , v2 ) = correlation(v1 , v2 )                            category than the category they have only learned at the basic
                                                                                   level, and that this generalizes to new domains.
    In Tanaka’s experiment, d-prime was used to quantify the
 “same/different” discrimination. As a simple model of d-
 prime we assume that the discrimination performance is
 based on the difference in similarities between images from
 the same species versus the similarities between images of
 different species. Images from the same species will be highly
 similar; images from different species will not have a similar
 representation. We assume subjects’ discrimination scores are
 a function of the difference between these. Hence our
 measure of discrimination between two species is:
                     1                                     1
D(sp1, sp2) =                      ∑
               sp1   sp2  i, j∈sp1,sp2
                                           sim(i, j) −               ∑sim(i, j)
                                                       sp1 * sp2 i∈sp1, j∈sp2
                +     
                2   2                                                            (a) “Same” and “Diff” are the average correlations between
 where sp1 and sp2 are the two species being compared. The                            exemplars within the same species and between exemplars
 first term is the average similarity of exemplars within the                           from different species within 3 conditions: OLD/OLD,
 two species, averaged together, and the second term is the                                            NEW/OLD, NEW/NEW.
 average similarity of exemplars between the species.
 Experiment 1
 From the modeling stand point, it’s hard to train a single
 module network to perform both the basic and subordinate
 classification. In addition, we want separate representation of
 basic level classification and the fusiform face area recruited
 for expertise tasks since we assume that the brain functionally
 separates the processing of basic and subordinate level
 information. Therefore, we started with a 2-module neural
 network model (Figure 4) which includes a basic classifier
 and an expert classifier.
 Similarity Calculation There are two sets of hidden unit
 activations which come from the two modules of the network.                                      (b) Discrimination = (Same - Diff).
 During phase 1, the hidden layer of the basic module is used                          Figure 5: Post-training discrimination. Error bars denote
 as the internal representation of the neural network to assess                                           standard deviations.
 the baseline discrimination. This is similar to human subjects
                                                                              1638

   However within the basic level the “same/different”
discrimination measures of the training exemplars and novel
exemplars of trained species were below the baseline. In
addition, the discrimination of the novel exemplars of novel
species was higher than training exemplars and novel                Result and Discussion Our multi-module neural network
exemplars of trained species. This results from the basic level     model has demonstrated the same results as human subjects
network compressing the representations of classes. This            do. Even though we have achieved our goal to model
makes sense, since the main function of the basic module is to      Tanaka’s results with this multi-module network we would
ignore the differences among individuals and group similar          like to automate the process of choosing which combination
stimuli together. However, subjects in Tanaka’s experiment          of hidden layers to represent an input image.
not only learned to classify stimuli, but also experienced the
stimuli through mere exposure. This suggests that we should
have an additional module in our network modeling this
phenomenon.
Experiment 2
Based on the previous experiment, we know that basic level
training compressed the representation of basic stimuli. We
added an autoencoder as a model of perceptual learning from
mere exposure (Figure 6). Autoencoders try to reproduce their
input on their output, thus learning the statistics of their
environment in a passive way. The side effect of this is that
the representations of items so trained will be separated in
hidden unit space.                                                    (a) “Same” and “Diff” are the average correlations between
                                                                       exemplars within the same species and between exemplars
                                                                         from different species within 3 conditions: OLD/OLD,
                                                                                        NEW/OLD, NEW/NEW.
     Figure 6: Multi-module neural network configuration.
Similarity Calculation Now there are three hidden layers
from each of the basic, subordinate, and auto-encoder
modules. To decide which hidden layer is used as the internal
representation of the network, we based our decision on two
motivations. First, the brain functionally processes the basic
                                                                                   (b) Discrimination = (Same - Diff).
level information and the subordinate level information
                                                                        Figure 7: Post-training discrimination. Error bars denote
separately. Second, while learning a classification task,
                                                                                           standard deviations.
subjects are also experienced stimuli through mere exposure.
Therefore, it’s most likely that the brain accumulates
                                                                    Experiment 3
information from the classification process and mere
exposure to perform the “same/different” discrimination.            To automate the process of selecting which combination of
   In phase 1, the concatenation of the basic and autoencoder       hidden layers to represent an input image, we exploit a
hidden layer is used as the internal representation of the          mixture network model (Figure 8). These networks
neural network to assess the baseline discrimination. In phase      implement a soft competition between the networks to
2, we concatenate the hidden layers of the basic and                process the data. The competition is based upon the output of
autoencoder module and the hidden layers of the subordinate         a gating network that mediates the learning and activation
and autoencoder module to represent the internal                    flow in the two networks. Thus we set up a competition
representation of basic stimuli and subordinate stimuli             between the basic and expert level classifiers. The complete
respectively.                                                       model is composed of the mixture network and a standard
                                                                    feed-forward network. The mixture model that we used is a
                                                                    modified version of the one described by Dailey & Cottrell
                                                               1639

(1999). The modification is that the output teaching signals of
the gating network depend on the subordinate output
activations. If any subordinate outputs are on, we turn on the
teaching signal of the gating network that connects to the
subordinate module. Hence, whenever subordinate
classification is the goal, the system is forced to choose the
expert network. We imagine that this is the role of the frontal
lobe system. The mixture network learning rule is described
in Appendix A.
                                                                     (a) “Same” and “Diff” are the average correlations between
                                                                      exemplars within the same species and between exemplars
                                                                        from different species within 3 conditions: OLD/OLD,
                                                                                       NEW/OLD, NEW/NEW.
     Figure 8: Multi-module neural network configuration.
Similarity Calculation In phase 1, similarly to experiment 2,
the concatenation of the basic and autoencoder hidden layer is
used as the internal representation of the neural network to
assess the baseline discrimination.
   In phase 2, we use the hidden layers of the auto-encoder
module with either the basic or subordinate module to
represent input stimuli. To select between the hidden layers of
the basic and subordinate module, we depend on the higher
output activation of the gating network. The gating network                       (b) Discrimination = (Same - Diff).
has two output units which are pre-assigned to the basic and           Figure 9: Post-training discrimination. Error bars denote
subordinate module.                                                                       standard deviations.
                                                                      Our multi-module network has similar performance as
Result and Discussion This result (Figure 9) indicates that        human subjects. Furthermore, it also supported the concept
we have again successfully modeled Tanaka’s experiment.            that the brain functionally separates the processing of basic
Moreover, we also have suggested a mechanism for the brain         and subordinate level information. Indeed, in experiments not
to choose between the levels of processing between the two         reported here, we could not model this behavior in a single
networks.                                                          network. However, our model contained no mechanism for
                                                                   simply learning good (invertible) representations of
                                                                   experienced stimuli. The network actually saw basic level
                         Conclusion                                stimuli as “all looking alike:” In order to prevent this, we
                                                                   added a process, implemented here as an autoencoder, to
Tanaka’s study has shown that classification task at               learn faithful perceptual representations of frequently
subordinate level, not mere exposure, is the most important        encountered stimuli. The autoencoder helps spread out the
ingredient in becoming an expert. Subordinate level training       representation of the basic stimuli as well as the subordinate
produced greater advantages in species discrimination relative     stimuli, and it can be considered as a model for mere
to basic level training. Furthermore, transfer occurs in a         exposure. In our final multi-module networks, we found it
graded fashion, to novel exemplars of known categories and         necessary to further assume that the brain can choose the
novel exemplars from novel, but similar, categories.               representations it will use to compare stimuli. This is a novel
                                                                   prediction, as far as we know, and one that could be verified
                                                                   via fMRI.
                                                               1640

                     Acknowledgments                                Joyce, C., & Cottrell, G. W. Solving the visual expertise
                                                                        mystery. In Proceedings of the Neural Computation and
We would like to thank Gary’s Unbelievable Research Unit,
                                                                        Psychology Workshop 8, Progress in Neural Processing.
Carrie Joyce for a preliminary experiment. This was
                                                                        World Scientific, London, UK, 2004.
supported by NIMH grant MH57075 to GWC.
                                                                    Lecun, Y., Bottou, L., Orr, G. B., & Muller, K. R. Efficient
                                                                        backprop. In Neural Networks-Tricks of the Trade,
                         References                                     Springer Lecture Notes in Computer Sciences (1998), vol.
Bishop, C. M. Neural networks for pattern recognition.                  1524, pp. 5-50.
  Oxford University Press, 1995.                                    Sugimoto, M., & Cottrell, G. W., (2001) Visual Expertise is a
Cottrell, G. W., Brandon, K. M., & Calder, A. J. Do                     General Skill. Proceedings of the 23rd Annual Cognitive
  expression and identity need separate representations? In             Science Conference, pp. 994-999.
  Proceedings of the 24th Annual Conference of the                  Tanaka, J. W., Curran, T., Sheinberg, D.L. (in press). The
  Cognitive Science Society (Mahwah, New Jersey, 2002),                 training and transfer of real-world, perceptual expertise.
  The Cognitive Science Society.                                        Psychological Science.
Dailey, M. N., & Cottrell, G. W. Organization of face and
  object recognition in modular neural network models.                      Appendix A: Mixed hidden layer network
  Neural Networks 12 (1999), 1053-1073.
                                                                    In the feed-forward phase, we first compute the input
Dailey, M. N., Cottrell, G. W., Padgett, C., & Adolphs, R.
                                                                    weighted sum of the hidden unit uij (i is the module number
  Empath: A neural network that categorizes facial
                                                                    and j is the unit number in the hidden layer):
  expressions. Journal of Cognitive Neuroscience 14, 8
  (2002), 1158-1173.                                                 net ij = ∑ wijk xk
Daugman, J. G. Uncertainty relation for resolution in space,                     k
  special frequency, and orientation optimized by two              Then we apply the sigmoid function to the weighted sum:
  dimensional visual cortical filters. Journal of the Optical                                2
  Society of American A 2 (1985), 1160-1169.                         z ij = 1.7159 tanh( net ij )
Devalois, R. L., & Devalois, K. K. Spatial Vision. Oxford                                    3
  University Press, 1988.                                          We also compute the input weighted sum of the output unit in
Gauthier, I., Skudlarski, P., Gore, J., & Anderson, A. (2000).     the gating network and then apply the softmax function to
  Expertise for cars and birds recruits brain areas involved in    that weighted sum:
                                                                                                             exp(neti )
  face recognition. Nature Neuroscience, 3, 191-197.
                                                                     net i = ∑θ ik xk ,              gi =
Gauthier, I., Curran, T., Curby, K. M., & Collins, D. (2003).
  Perceptual interference supports a non-modular account of
                                                                                k                         ∑ exp(net j )
                                                                                                            j
  face processing. Nature Neuroscience, 6, 428-432.                Afterward, we compute the activations of the output units:
Gauthier, I., & Tarr, M. J. (1997). Becoming a ‘Greeble’
  expert: exploring the face recognition mechanism. Vision           oi = ∑ ( g m ∑ wmij net mj )
  Research, 37. 1673-1682.                                                    m         j
Gauthier, I., Tarr, M. J., Anderson, A. W., Skudlarksi, P., &       The network is trained by online back-propagation of error
  Gore, J. C. (1999). Activation of the middle fusiform “face       with the generalized delta rule. First, we compute the update
  area” increases with expertise in recognizing novel objects.      weights for the output layer:
  Nature Neuroscience, 2, 568-573.                                  δ o = 2(t i − oi )
                                                                         i
Gauthier, I., Williams, P., Tarr, M. J., & Tanaka, J. W.
  (1998). Training “Greeble” experts: a framework for                ∆wmij (t ) = η * z mj * δ oi + α * ∆wmij (t + 1)
  studying expert object recognition processes. Vision             Second, we compute the update weights for the hidden layer:
  Research, 38, 2401-2428.                                                           dzij
Harnad, Stevan (1987) Uncomplemented Categories, or,
  What Is It Like To Be a Bachelor. Presidential address to
                                                                    δ u = gi
                                                                         ij
                                                                                   dnetij
                                                                                          ∑δ
                                                                                           p
                                                                                                op w pij
  the 1987 meeting of Society for Philosophy and
  Psychology. http://cogprints.org/2134/index.html                   ∆wmij (t ) = η * x j * δ uij + α * ∆wmij (t + 1)
Johnson, K., & Mervis, C. (1997). Effects of varying levels of     Let g1 and g2 wire to the basic module and subordinate
  expertise on the basic level of categorization. Journal of        module respectively. Finally, we compute the teaching
  Experimental Psychology: General, 126, 248-277.                   signals for the output units of the gating network:
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E.
  (1991). Adaptive mixtures of local experts. Neural                         [0,1], max(subordinate(oi )) ≥ 0.5
                                                                    tg = 
  Computation, 3, 79-87.                                                     [1,0], max(subordinate(oi )) < 0.5
Jones, J. P., & Palmer, L. A. An evaluation of the two
                                                                   Then we compute the update weights for the gating network:
  dimensional gabor filter model of simple receptive fields in
  cat striate cortex. Journal of Neurophysiology 58, 6 (1987),      δ g = 2(t g − oi )
                                                                          i        i
  1233-1258.
                                                                     ∆θ ij (t ) = η * x j * δ gi + α * ∆θ ij (t + 1)
                                                               1641

