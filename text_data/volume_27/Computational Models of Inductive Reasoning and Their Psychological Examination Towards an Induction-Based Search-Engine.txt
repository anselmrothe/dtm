UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Computational Models of Inductive Reasoning and Their Psychological Examination: Towards
an Induction-Based Search-Engine
Permalink
https://escholarship.org/uc/item/3p1457z4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Naveh, Isaac
Sun, Ron
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Computational Models of Inductive Reasoning
                                       and Their Psychological Examination:
                                  Towards an Induction-Based Search-Engine
                                  Kayo Sakamoto (SAKAMOTO@Nm.Hum.Titech.Ac.Jp)
                                          Tokyo Institute of Technology, 2-21-1 O-okayama,
                                                 Meguro-ku, Tokyo, 152-8552 JAPAN
                                        Asuka Terai (ASUKA@Nm.Hum.Titech.Ac.Jp)
                                          Tokyo Institute of Technology, 2-21-1 O-okayama,
                                                 Meguro-ku, Tokyo, 152-8552 JAPAN
                              Masanori Nakagawa (NAKAGAWA@Nm.Hum.Titech.Ac.Jp)
                                          Tokyo Institute of Technology, 2-21-1 O-okayama,
                                                 Meguro-ku, Tokyo, 152-8552 JAPAN
                            Abstract                                     based on their features, has an important role. This model is
   The purpose of the present study is to propose computational          called the feature-based model. Sloman argues that the
   models of human inductive reasoning, using a statistical              feature-based model is better able to account for the results
   analysis of Japanese linguistic data, and to develop a search-        from an experiment involving inductive reasoning for
   engine based on inductive reasoning. Osherson, et al. (1990)          conditionals. Sloman’s model is a kind of neural network.
   provided a psychological model of inductive reasoning based           In the model, input nodes represent the feature pattern of the
   on the similarity between the premise and the conclusion and          conclusion, and the output node indicates the similarity of
   on knowledge of the category including the premise and the            the conclusion to the premises. The strengths of weights
   conclusion. Models of this kind are known as category-
                                                                         between the input nodes and the output node are computed
   based models. In contrast, Sloman (1993) proposed a model
   where the inductive reasoning is based only on the features           from the feature patterns of the premises according to a kind
   of arguments (the feature-based model). These models were             of delta rule.
   constructed based on the result from psychological                       However, while this model is only capable of handling
   evaluations concerning the relationship between arguments             positive premises that include affirmative expressions, and
   and their attributes which were selected in advance.                  so cannot accept negative premises consisting of negative
   However, it is difficult to objectively identify all the              expressions, the category-based model is valid for negative
   attributes that cover general human knowledge, which is               premises (Osherson, et al., 1991). Furthermore, Sloman’s
   necessary in order to construct a general model that simulates        model requires experimental data to determine the feature
   the human process of inductive reasoning. Moreover, the               strengths of the premises and conclusion.
   costs involved in conducting psychological evaluations for
                                                                            However, it is difficult to identify all the features that
   the sheer numbers of features means that they are
   prohibitively impractical. In order to avoid such problems,           cover general human knowledge that is necessary in order
   the present study proposes three types of models (a neural            to construct a general model that simulates the human
   network model, a subjective probabilistic model, and a                inductive reasoning process. Beyond this problem, costs
   Bayesian model) that utilize the results of statistical analysis      involved in conducting psychological evaluations for the
   for a language corpus in computing co-occurrence                      sheer numbers of features means that they are prohibitively
   probabilities for two words, rather than using psychological          impractical.
   evaluation. A psychological experiment concerning inductive              Osherson’s model also faces similar problems, because it
   reasoning was conducted to evaluate the models. In                    also needs experimental data to determine the strength of
   comparisons of the experimental results and the simulation            similarity between the premises and the conclusion, as well
   results for the neural network model and the subjective
                                                                         as the similarity between the premises and the category
   probabilistic model, good correlations were observed. We
   also successfully implemented a trial version of a search-            including the premises and the conclusion.
   engine based on inductive reasoning using the subjective                 In order to solve these problems for the construction of a
   probabilistic                                            model.       general model of inductive reasoning, the present study
                                                                         proposes three types of models that use the results from the
                          Introduction                                   statistical analysis of a language corpus, instead of
Osherson, et al. (1990) provided a psychological model of                psychological evaluations of the relationships between
inductive reasoning based on the similarity between the                  objects and their attributes. The analysis results are used
premise and the conclusion and on knowledge of the                       to compute co-occurrence probabilities between two words
category including the premise and the conclusion.                       (in the case of the feature-based model) and to compute co-
Models of this kind are known as category-based models.                  occurrence probabilities between a word and a category (in
   In contrast, Sloman (1993) developed another type of                  the case of the category-based models).
psychological model for inductive reasoning, where only                    The feature-based model proposed here has an input layer
the similarity between the premise and the conclusion,                   and one output node, where the weights between input
                                                                    1907

nodes and the output node are estimated by the usual delta
method using the feature strengths of a given premise
                                                                                                   where   ∑    Ni
                                                                                                                   P( N i | C k ) = 1 and     ∑ Aj
                                                                                                                                                   P( A j | C k ) = 1
(either positive or negative), which are computed as a                                             and N(Ni,Aj) is the co-occurrence frequency of term “Ni”
conditional probability of a word representing a feature                                           and term “Aj”. Using the estimated parameters to compute
given a word representing the premise or the conclusion.                                           the probabilistic relationship between concepts and their
  Two types of category-based model are also proposed.                                             attribute in Sloman’s model, this method makes it possible
One is the category-based subjective probability model.                                            to simulate the inductive reasoning process based only on
This model employs the conditional probability of the latent                                       the results from the statistical analysis of a language corpus.
semantic class assumed within the model that represents a                                          For example, an adjective that has a large conditional
category given the word representing the premise or the                                            probability given a particular term as a concept may be
conclusion. The other is the category-based Bayesian model.                                        selected as the feature of the concept with a probability
This model employs the conditional probability of the word                                         representing the strength of the relation between the feature
representing the premise or conclusion given a category.                                           and the concept.
  A psychological experiment on inductive reasoning was                                              It is also possible to construct category-based models
conducted to evaluate the three types of the models.                                               using the same analysis results. For example, a latent
                                                                                                   semantic class with a large conditional probability for a
 The Probabilistic-Expression of Meaning Based on                                                  given term as a concept may be regarded as the category to
                                                                                                   which the concept belongs.
       a Statistical Analysis of a Language Corpus                                                   However, there is no guarantee that the results of the
                                                                                                   models constructed using the estimated parameters
Statistical Method                                                                                 correspond to the actual human process of inductive
For the statistical analysis of a language corpus, the present                                     reasoning. In order to investigate this issue, it is necessary
study employs a statistical method that is similar in                                              to verify predictions for inductive reasoning from the
structure to the Pereira’s model or PLSI (Pereira, et al. 1993,                                    models, based on comparisons with actual experimental
Hofmann 1999, Kameya & Sato 2005). The model assumes                                               data.
that the co-occurrence probability of a term “Ni” and a term
“Aj”, P(Ni,Aj), is computed using formula (1).                                                     Results of Statistical Language Analysis
 P( N i , Aj ) = ∑ P( N i | Ck ) P( Aj | Ck ) P(Ck )
                     k
                                                                                     (1)
where P(Ni|Ck) is the conditional probability of Ni, given Ck                                      As a linguistic corpus, the co-occurrence frequencies for
which indicates the latent semantic class assumed in the                                           adjectives and nouns, and for predicates and nouns that
model. Each of the parameters in the model, P(Ck), P(Ni|Ck)                                        have a modifying relationship were measured from
are estimated as the values which maximize the likelihood                                          Japanese Newspaper “MAINICHI SHINBUN” 1993-2002
of co-occurrence data measured from a language corpus                                              using a modification analysis system, called “Cabocha
using the EM algorithm, as follows;                                                                (Kudoh & Matsumoto, 2000)”. The results include a set of
                                                                                                   2,736 adjectives and 14,807 nouns, and a set of 83,176
E-step P(C | N , A ) = P ( N i | Ck ) P( Aj | Ck ) P(Ck )                                          predicates and 21,206 nouns in Japanese.
                                     ∑
                  k     i   j
                                        k
                                          P( N i | Ck ) P ( A j | Ck ) P (Ck )                       The number of latent classes was fixed at 50 in the case of
M-step P(Ck ) = ∑                    P(Ck | N i , Aj )N ( N i , Aj )                    (2)        adjectives and nouns, and at 200 in the case of predicates
                              N ,A
                               i   j
                                                                                                   and nouns. The co-occurrence frequencies were analyzed
                                 ∑   Aj
                                         P (C k | N i , A j ) N ( N i , A j )                      using the method described above. The meanings of the
              P ( N i | Ck ) =                                                                     latent classes were identified from the conditional
                                                 P (Ck )
                                                              Table 1: Probability Patterns in the Latent Classes
                                            Class of Valuable assets                                               Class of Jobs
                                             P(Cvalues|Jj)                    P(Cvalues|Nj)                     P(Cjob|Aj)                 P(Cjob|Nj)
                                 1    stock                   0.929     issue            0.929        1  soldier           0.773    young             0.999
                                      government                                                      2  police            0.744    brilliant         0.942
                                 2                                      list                             officer
                                      bonds                   0.862                      0.916
                                 3    estate                  0.791     increase         0.899        3  clerk             0.696    great             0.941
                                 4    building estate         0.780     release          0.884        4  journalist        0.689    capable           0.851
                                      real                                                            5  office            0.665    competent         0.813
                                 5                                      vend out                         worker
                                      estate                  0.757                      0.877
                                 6    cruiser                 0.662     sell             0.852           woman
                                                                                                      6  office            0.607    kind              0.753
                                 7    farmland                0.657     borrow on        0.841           worker
                                      foreign                                                         7  prosecutor        0.550    sturdy            0.623
                                 8                                      not sell
                                      bond                    0.628                      0.802        8  monk              0.546    promising         0.557
                                 9                                      buy                              business-
                                      house                   0.594     and add          0.802        9  man               0.543    indifferent       0.544
                               10     currency                0.555     keep             0.781       10  bank clerk        0.530    humble            0.457
                                                                                              1908

probability of the latent class Ck given an adjective Aj                       P(conclusion) according to the Bayesian theorem, as
(P(Ck|Aj)) or given a predicate Jl (P(Ck|Jl), and the                          follows.
conditional probability of the latent class Ck given a noun                     P(conclusion ) = Pp (1 − Pn )                                     (8)
Ni (P(Ck|Ni)). For example, the classes named as “Valuable                            n
Assets” and “Jobs” are shown in Table 1. The listed                                  ∑ {P ( N conci | Ck ) ⋅ Π N i ∈N p P ( N i | Ck ) ⋅ P(Ck )}  (9)
adjectives or predicates and nouns that have relatively                         Pp =  k
                                                                                              n
strong probabilities P(Ck|Aj) or P(Ck|Jl) and P(Ck|Ni) may be                                ∑Π      N i ∈N p P ( N i | Ck ) ⋅ P(Ck )}
considered to be closely related to each class Ck. So these                                   k
classes can be regarded as categories of concepts                                    n
represented by the listed nouns.                                                     ∑ {P ( N conci | Ck ) ⋅ Π N i ∈N n P ( N i | Ck ) ⋅ P (Ck )} (10)
                                                                                Pn = k
                                                                                              n
                          Construction of Models                                             ∑Π
                                                                                              k
                                                                                                     N i ∈N n P ( N i | Ck ) ⋅ P (Ck )}
The present study proposes three types of models. The first                    where Nn refers to a nominal group for the subject of
type of model is a feature-based neural network. The                           negative premises and Np refers to the nominal group for the
feature-based model proposed here has an input layer and                       subject of positive premises.
one output node, where the weights between input nodes                             The other category-based model utilizes similar
and the output node are estimated by the usual delta method                    processes, except that it uses the probability P(Ck|Ni) /Sum,
using the features strengths of positive and negative                          instead of P(Ni|Ck), where Sum=ΣiP(Ck|Ni). In this model, it
premises, which are computed as the conditional probability                    is assumed that P(conclusion)=Pp-Pn. In this case,
of a word representing a feature, given a particular word                      P(Ck|Ni)/Sum may be regarded as representing the
representing the premise, according to following formulas,                     subjective probability of the noun N belonging to a given
W ( N i ) = W ( N i −1 ) + [1 − O ( N i )]O ( N i )                            category Ck. For example, taking into account the
                                  N i ∈ N p ; positive premises, (3)           conditional probabilities, P(Ck|Ni) in Table 1, it is not
                                                                               difficult to subjectively decide which latent class Ck
W ( N i ) = W ( N i −1 ) + [0 − O ( N i )]O ( N i )                            includes the noun Ni. “Stock” and “government bonds” are
                                  N i ∈ N n ; negative premises, (4)           strongly related to the latent class of valuable assets and
              W ( N i ) ⋅ I ( Ni )                                (5)          “soldier,” “police officer” and “clerk” are strongly related to
O( N i ) =                  2                                                  the latent class of jobs. Thus, the latent classes of valuable
                   I ( Ni )
                                                                               assets and jobs are considered as categories. “Stock” and
where W(Ni) indicates the weight and I(Ni) is the feature                      “government bonds” are regarded as members of the
vector of Ni.                                                                  category “valuable assets,” while “soldier,” ”police officer”
The conditional probability of an adjective given a noun,                      and “clerk” are taken as members of the category “jobs”.
P(Aj|Ni), and the conditional probability of a predicate                       Hereafter, this category-based model will be referred to as
P(Jl|Ni) given a noun are computed using the following                         category-based subjective probabilistic model, while the
formulas                                                                       remaining model, based on the Bayesian theorem, will be
 P( A | N ) = k
                   ∑ P( Aji | Ck ) P( N ij | Ck ) P(Ck )              (6)      called the category-based Bayesian model.
      ij     i
                            ∑   k
                                  P ( N i | Ck ) P (Ck )
                    ∑     P( J l | Ck ) P ( N i | Ck ) P (Ck )        (7)            Psychological Experiment of Inductive
 P( J li | N j ) =      k
                           ∑   k
                                 P ( N ij | Ck ) P (Ck )                                                          Reasoning
High values for P(Aj|Ni) and P(Jl|Ni) indicate a strong                        A psychological experiment concerning inductive reasoning
relationship between a noun Ni and an adjective Aj, and                        was conducted to evaluate these models.
between a noun Ni and a predicate Jl, respectively. This
feature-based neural network model actually uses adjectives                    Method
and predicates that have relatively strong conditional                         In the experiment, participants (about 40 undergraduate
probabilities given words that represent positive and                          students) were asked to rate the likelihood of a conclusion,
negative premises. For example, imagine a piece of                             such as “Currency is valuable for Mr. A.”, given premises,
inductive reasoning that has as its conclusion “A designer is                  such as “Stock is valuable for Mr. A.” The premises also
accepted into club B,” and as its positive premise “An artist                  included negative ones, such as “Outdoor activities are not
is accepted into club B.” and as its negative premise “An                      valuable for Mr. A.” Participants rated the likelihood
office worker isn’t accepted into club B,” then, P(Aj|                         according to a 5-point scale (see Figure 1). Three sets of
“artist”) and P(Aj| “office worker”), which have relatively                    premises and conclusions shown in Table 2 were used in the
stronger values, will be selected as feature values. These                     experiment. The premises and conclusions in each set are
values for P(Aj|Ni) are used as terms for the vector I(Ni) in                  selected from two different latent classes. For instance, the
this model.                                                                    positive premises and some of conclusions in the set of
The other two models are both category-based probabilistic                     “Wear” are chosen from the class of “Clothes,” and the
models, which employ similar conditional probabilities for                     negative premises and the other conclusions are chosen
both the category and the premise. One of the category-                        from the class of “Hats”.
based models computes the probability of the conclusion,
                                                                          1909

             Now you know the following premises.
                                      Stock is valuable for Mr. A.
                                      Place is valuable for Mr. A.
                                   Walking is not valuable for Mr. A.
                              Outdoor activities
                                              are not valuable for Mr. A.
             Please estimate how likely the following conclusion is true given above premises.
              “Currency is valuable for Mr. A.”
                   strongly                                                             strongly
                    likely          likely             neutral         unlikely         unlikely
                               Figure1: Example of the rating tasks
  Table 2: Examples of the premises and conclusions used in the experiments
                                                                                                 Valuable and
                                   Jobs                             Wear                               leisure
 nouns used in                    scholar                           uniform                              stock
    positive                      painter                          polo shirt                            place
   premises
                                   actor
 nouns used in                office worker                          helmet                             walking
   negative              female office worker                    straw boater                    outdoor activities
   premises
                                  soldier                           kimono                       government bonds
                              police officer                      regimentals                      building estate
                                   clerk                              cloak                           real estate
                                journalist                         haori coat                           cruiser
nouns used
                                prosecutor                           jacket                              farm
       in
conclusions                        king                               mask                          tea ceremony
                              photographer                            hood                             shopping
                                 general                           straw hat                            hiking
                               commander                              beret                   swimming in the sea
                                    chef                            silk hat                           billiards
                             Table 3: The Results of the Experiment
                                                                                     Valuable and
            Jobs                                 Wear                                      leisure
      scholar              5.000            uniform            5.000                   stock                   5.000
   office worker           1.000            helmet             1.000                  walking                  1.000
       soldier             2.522            kimono             2.885           government bonds                4.821
   police officer          2.023        regimentals            2.577             building estate               4.750
        clerk              1.681             cloak             2.269                real estate                4.607
     journalist            2.955          haori coat           2.885                  cruiser                  3.036
        king               3.250             mask              2.154              tea ceremony                 2.071
   photographer            4.068             hood              2.308                 shopping                  2.393
      general              3.023           straw hat           2.115                  hiking                   1.214
                                                                Experimental              Results
    commander              2.841             beret             2.654         swimming in the sea               1.393
                                                       1910

Average rated values for each conclusion are shown in                                                    Development of Search-engine Based on
Table 3. The total averages for each set of premises and
conclusions are 3.208 (SD = 1.190) for Jobs, 2.846 (SD =
                                                                                                                 Inductive Reasoning
0.985) for wear, and 3.084 (SD = 1.460) for valuable and                                           The present study has developed a trial version of a search-engine
leisure.                                                                                           based on inductive reasoning using the category-based
                                                                                                   probabilistic model, which provided the best results of the three
                                                                                                   models. The search-engine outputs words as conclusions that are
       Comparison Between Psychological                                                            inductively reasoned from given words as premises according to
                                                                                                   the category-based psychological model. Nouns with relatively
      Experiment and Models’ Simulations                                                           larger output values from the model are considered as typical
Simulations of all three models were executed. Correlations                                        results of the search engine. An example of a search given positive
between the results from the psychological experiment and                                          and negative keywords (premises) is shown in Table 5
the simulation results, together with F ratios (the fitness
indices for the models) are shown in Table 4. From this                                                                     Discussion
table, it is clear that the results for the feature-based model                                    From the comparisons of the simulation and experimental
and the category-based subjective probabilistic model are                                          results, it is clear that the feature-based model and the
well correlated with the experimental results, and are better                                      category-based subjective probabilistic model are well
than the simulation results for the category-based Bayesian                                        correlated with the results of the experiments, and that their
model. The relationship between average evaluations in the                                         results were better than those of the category-based
experiment (valuable assets) and the output from of the                                            Bayesian model. Using the category-based subject
category-based subjective probabilistic model is represented                                       probabilistic model, we have successfully implemented a
in Figure 2.                                                                                       trial version of a search-engine based on inductive
                                                                                                   reasoning.
                                  Average Evaluations in the
                                                                   6.000
                                                                   5.000
                                                                   4.000
                                                                   3.000
                                Experiments(Valuable and leisure
                                                                   2.000
                                                                   1.000
                                                                   0.000
                                                                           -0.01   -0.005          0           0.005     0.01        0.015
                                                                                             Output of the model
                            Figure 2: Comparison between the Experiment results (Valuable and leisure)
                                          and the Category-based Psychological model
                                                             Table 4: Correlations between Experiments and Models
                                                                                                                                                Valuable
                                                                                                           Jobs                 Wear           and leisure
                                                                                   correlation
        Feature-based model                                                        coefficients        0.761   P<0.001   0.624   P<0.001      0.906   P<0.001
                                                                                   F ratios            42.53   P<0.001   19.11   P<0.001     137.49   P<0.001
                              Subjective                                           correlation
                              probabilistic                                        coefficients        0.729   P<0.001   0.661   P<0.001      0.932   P<0.001
                              model                                                F ratios            35.25   P<0.001   23.22   P<0.001     199.35   P<0.001
        Category-based
                                                                                   correlation
                              Bayesian model                                       coefficients        0.535   P<0.01    0.483   P<0.01       0.450   P<0.01
                                                                                   F ratios            12.18   P<0.01     9.15   P<0.01        7.62   P<0.01
                                                                                            1911

                    Table 5: Outputs from the Search-Engine                                    Acknowledgments
                                                                                 The present study is supported by the Tokyo
      Positive keywords                         Output                           Institute of Technology 21COE Program,
                                             1  place                            “Framework         for    Systematization         and
            “Place”                                                              Application      of     Large-scale       Knowledge
                                             2  stock                            Resources”.
            “Stock”                          3  credit                           The authers would like to thank Dr. Joyce, T.,
                                                                                 the post doctor of 21COE-LKR, for his critical
                                             4  wealth
                                                                                 reading of manuscripts and valuable comments
                                             5  government bonds                 on the earlier draft.
                                             6  real estate
      Negative keywords                      7  bequest
            “Walk”                           8  farm                                                References
                                             9  condominium                      Kameya, Y., & Sato, T. (in press). Computation
            “Outdoor life”                  10  dollar                           of probabilistic relationship between concepts
                                                                                 and their attributes using a statistical analysis of
                                            11  inheritance
                                                                                 Japanese corpora. Proceedings of Symposium on
                                            12  bond                             Large-scale Knowledge Resources: LKR2005
                                            13  yen                              Kudoh, T., & Matsumoto, Y. (2000). Japanese
                                            14  remains                          Dependency Analysis Based on Support Vector
                                                                                 Machines. Proceedings of Joint SIGDAT
                                            15  family business
                                                                                 Conference on Empirical Methods in Natural
                                                                                 Language Processing and Very Large Corpora
                                                                       :EMNLP/VLC2000
   However, the corpus used in the present study consists
only of newspaper articles. A newspaper has its own style            Hofmann, T. (1999). Probabilistic latent semantic indexing.
of representation and its own content bias. The style and              Proceedings of the 22nd International Conference on
content naturally influence the statistical analysis results in        Research and Development in Information
the present study, which may, in turn, create certain biases           Retrieval :SIGIR ’99. (pp. 50-57).
in the simulation results. For example, the category of              Osherson, D. N., Smith, E. E., Wilkie, O. Lopez, A., and
‘sports’ and the category of ‘crime’ were typically                    Shafir, E. (1990). Category-Based Induction.
identified as latent semantic classes in the present study,            Psychological Review, 97, 2, 185-200.
which may reflect the contents of the newspaper. In order to         Osherson, D. N., Stern J., Wilkie O., Stob M., and Smith E.
avoid this kind of bias, it is necessary to collect various
                                                                       E. (1991) Default Probability. Cognitive Science, 15,
corpora including a wide range of style and a variety of
                                                                       251-269.
content. Now, we are planning to analyze many kinds of
corpora, not only newspapers, but other materials like               Pereira, F., Tishby, N., and Lee, L. (1993). Distributional
literature, dictionaries, or encyclopedias.                            clustering of English words. Proceedings of the 31st
   Other models beyond those discussed in this paper, for              Meeting of the Association for Computational Linguistics.
example a neural network for a category-based model,                   (pp.183-190).
should also be used in future studies, in order to identify the      Sloman, A. T. (1993). Feature-Based Induction. Cognitive
most appropriate model for natural inductive reasoning.                Psychology, 25, 231-280.
   An important future plan is to develop a practical search-
engine through inductive reasoning, based on the results
from these future analyses.
                                                                1912

