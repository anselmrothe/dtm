UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Computational Models of Inductive Reasoning and Their Psychological Examination: Towards
an Induction-Based Search-Engine

Permalink
https://escholarship.org/uc/item/3p1457z4

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Authors
Naveh, Isaac
Sun, Ron

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Computational Models of Inductive Reasoning
and Their Psychological Examination:
Towards an Induction-Based Search-Engine
Kayo Sakamoto (SAKAMOTO@Nm.Hum.Titech.Ac.Jp)
Tokyo Institute of Technology, 2-21-1 O-okayama,
Meguro-ku, Tokyo, 152-8552 JAPAN

Asuka Terai (ASUKA@Nm.Hum.Titech.Ac.Jp)
Tokyo Institute of Technology, 2-21-1 O-okayama,
Meguro-ku, Tokyo, 152-8552 JAPAN

Masanori Nakagawa (NAKAGAWA@Nm.Hum.Titech.Ac.Jp)
Tokyo Institute of Technology, 2-21-1 O-okayama,
Meguro-ku, Tokyo, 152-8552 JAPAN
based on their features, has an important role. This model is
called the feature-based model. Sloman argues that the
feature-based model is better able to account for the results
from an experiment involving inductive reasoning for
conditionals. Sloman’s model is a kind of neural network.
In the model, input nodes represent the feature pattern of the
conclusion, and the output node indicates the similarity of
the conclusion to the premises. The strengths of weights
between the input nodes and the output node are computed
from the feature patterns of the premises according to a kind
of delta rule.
However, while this model is only capable of handling
positive premises that include affirmative expressions, and
so cannot accept negative premises consisting of negative
expressions, the category-based model is valid for negative
premises (Osherson, et al., 1991). Furthermore, Sloman’s
model requires experimental data to determine the feature
strengths of the premises and conclusion.
However, it is difficult to identify all the features that
cover general human knowledge that is necessary in order
to construct a general model that simulates the human
inductive reasoning process. Beyond this problem, costs
involved in conducting psychological evaluations for the
sheer numbers of features means that they are prohibitively
impractical.
Osherson’s model also faces similar problems, because it
also needs experimental data to determine the strength of
similarity between the premises and the conclusion, as well
as the similarity between the premises and the category
including the premises and the conclusion.
In order to solve these problems for the construction of a
general model of inductive reasoning, the present study
proposes three types of models that use the results from the
statistical analysis of a language corpus, instead of
psychological evaluations of the relationships between
objects and their attributes. The analysis results are used
to compute co-occurrence probabilities between two words
(in the case of the feature-based model) and to compute cooccurrence probabilities between a word and a category (in
the case of the category-based models).
The feature-based model proposed here has an input layer
and one output node, where the weights between input

Abstract
The purpose of the present study is to propose computational
models of human inductive reasoning, using a statistical
analysis of Japanese linguistic data, and to develop a searchengine based on inductive reasoning. Osherson, et al. (1990)
provided a psychological model of inductive reasoning based
on the similarity between the premise and the conclusion and
on knowledge of the category including the premise and the
conclusion. Models of this kind are known as categorybased models. In contrast, Sloman (1993) proposed a model
where the inductive reasoning is based only on the features
of arguments (the feature-based model). These models were
constructed based on the result from psychological
evaluations concerning the relationship between arguments
and their attributes which were selected in advance.
However, it is difficult to objectively identify all the
attributes that cover general human knowledge, which is
necessary in order to construct a general model that simulates
the human process of inductive reasoning. Moreover, the
costs involved in conducting psychological evaluations for
the sheer numbers of features means that they are
prohibitively impractical. In order to avoid such problems,
the present study proposes three types of models (a neural
network model, a subjective probabilistic model, and a
Bayesian model) that utilize the results of statistical analysis
for a language corpus in computing co-occurrence
probabilities for two words, rather than using psychological
evaluation. A psychological experiment concerning inductive
reasoning was conducted to evaluate the models. In
comparisons of the experimental results and the simulation
results for the neural network model and the subjective
probabilistic model, good correlations were observed. We
also successfully implemented a trial version of a searchengine based on inductive reasoning using the subjective
probabilistic
model.

Introduction
Osherson, et al. (1990) provided a psychological model of
inductive reasoning based on the similarity between the
premise and the conclusion and on knowledge of the
category including the premise and the conclusion.
Models of this kind are known as category-based models.
In contrast, Sloman (1993) developed another type of
psychological model for inductive reasoning, where only
the similarity between the premise and the conclusion,
1907

nodes and the output node are estimated by the usual delta
method using the feature strengths of a given premise
(either positive or negative), which are computed as a
conditional probability of a word representing a feature
given a word representing the premise or the conclusion.
Two types of category-based model are also proposed.
One is the category-based subjective probability model.
This model employs the conditional probability of the latent
semantic class assumed within the model that represents a
category given the word representing the premise or the
conclusion. The other is the category-based Bayesian model.
This model employs the conditional probability of the word
representing the premise or conclusion given a category.
A psychological experiment on inductive reasoning was
conducted to evaluate the three types of the models.

The Probabilistic-Expression of Meaning Based on
a Statistical Analysis of a Language Corpus
Statistical Method
For the statistical analysis of a language corpus, the present
study employs a statistical method that is similar in
structure to the Pereira’s model or PLSI (Pereira, et al. 1993,
Hofmann 1999, Kameya & Sato 2005). The model assumes
that the co-occurrence probability of a term “Ni” and a term
“Aj”, P(Ni,Aj), is computed using formula (1).
(1)
P( N i , Aj ) = ∑ P( N i | Ck ) P( Aj | Ck ) P(Ck )
k

where P(Ni|Ck) is the conditional probability of Ni, given Ck
which indicates the latent semantic class assumed in the
model. Each of the parameters in the model, P(Ck), P(Ni|Ck)
are estimated as the values which maximize the likelihood
of co-occurrence data measured from a language corpus
using the EM algorithm, as follows;
E-step P(C | N , A ) = P ( N i | Ck ) P( Aj | Ck ) P(Ck )
k

i

∑

j

k

P( N i | Ck ) P ( A j | Ck ) P (Ck )

M-step P(Ck ) = ∑
P(Ck | N i , Aj )N ( N i , Aj )
N ,A
i

P ( N i | Ck ) =

(2)

j

∑

Aj

P (C k | N i , A j ) N ( N i , A j )
P (Ck )

where

∑

Ni

P( N i | C k ) = 1 and

∑

Aj

P( A j | C k ) = 1

and N(Ni,Aj) is the co-occurrence frequency of term “Ni”
and term “Aj”. Using the estimated parameters to compute
the probabilistic relationship between concepts and their
attribute in Sloman’s model, this method makes it possible
to simulate the inductive reasoning process based only on
the results from the statistical analysis of a language corpus.
For example, an adjective that has a large conditional
probability given a particular term as a concept may be
selected as the feature of the concept with a probability
representing the strength of the relation between the feature
and the concept.
It is also possible to construct category-based models
using the same analysis results. For example, a latent
semantic class with a large conditional probability for a
given term as a concept may be regarded as the category to
which the concept belongs.
However, there is no guarantee that the results of the
models constructed using the estimated parameters
correspond to the actual human process of inductive
reasoning. In order to investigate this issue, it is necessary
to verify predictions for inductive reasoning from the
models, based on comparisons with actual experimental
data.

Results of Statistical Language Analysis
As a linguistic corpus, the co-occurrence frequencies for
adjectives and nouns, and for predicates and nouns that
have a modifying relationship were measured from
Japanese Newspaper “MAINICHI SHINBUN” 1993-2002
using a modification analysis system, called “Cabocha
(Kudoh & Matsumoto, 2000)”. The results include a set of
2,736 adjectives and 14,807 nouns, and a set of 83,176
predicates and 21,206 nouns in Japanese.
The number of latent classes was fixed at 50 in the case of
adjectives and nouns, and at 200 in the case of predicates
and nouns. The co-occurrence frequencies were analyzed
using the method described above. The meanings of the
latent classes were identified from the conditional

Table 1: Probability Patterns in the Latent Classes
Class of Valuable assets
P(Cvalues|Jj)

Class of Jobs

P(Cvalues|Nj)

1

stock

0.929

2

government
bonds

0.862

issue
list

P(Cjob|Aj)

0.929
0.916

3

estate

0.791

increase

0.899

4

building estate

0.780

release

0.884

5

real
estate

0.757

6

cruiser

0.662

sell

0.852

7

farmland

0.657

borrow on

0.841

8

foreign
bond

0.628

house

0.594

currency

0.555

9
10

vend out

0.877

1
2
3
4
5
6

0.802

7
8

buy
and add

0.802

9

keep

0.781

10

not sell

1908

soldier
police
officer
clerk
journalist
office
worker
woman
office
worker
prosecutor
monk
businessman
bank clerk

P(Cjob|Nj)
0.773

young

0.999

0.744

brilliant

0.942

0.696
0.689

great
capable

0.941
0.851

0.665

competent

0.813

0.607

kind

0.753

0.550
0.546

sturdy
promising

0.623
0.557

0.543

indifferent

0.544

0.530

humble

0.457

P(conclusion) according to the Bayesian theorem, as
follows.
(8)
P(conclusion ) = Pp (1 − Pn )

probability of the latent class Ck given an adjective Aj
(P(Ck|Aj)) or given a predicate Jl (P(Ck|Jl), and the
conditional probability of the latent class Ck given a noun
Ni (P(Ck|Ni)). For example, the classes named as “Valuable
Assets” and “Jobs” are shown in Table 1. The listed
adjectives or predicates and nouns that have relatively
strong probabilities P(Ck|Aj) or P(Ck|Jl) and P(Ck|Ni) may be
considered to be closely related to each class Ck. So these
classes can be regarded as categories of concepts
represented by the listed nouns.

n

Pp =

∑ {P ( N

n

∑Π
k

n

Pn =

∑ {P ( N

N i ∈ N p ; positive premises, (3)
W ( N i ) = W ( N i −1 ) + [0 − O ( N i )]O ( N i )

N i ∈ N n ; negative premises, (4)
W ( N i ) ⋅ I ( Ni )
(5)
O( N i ) =
2
I ( Ni )

where W(Ni) indicates the weight and I(Ni) is the feature
vector of Ni.
The conditional probability of an adjective given a noun,
P(Aj|Ni), and the conditional probability of a predicate
P(Jl|Ni) given a noun are computed using the following
formulas
∑ P( Aji | Ck ) P( N ij | Ck ) P(Ck )
(6)
P( A | N ) = k
P( J li | N j ) =

∑

k

∑

k

P ( N i | Ck ) ⋅ P(Ck )}

| Ck ) ⋅ Π N i ∈N n P ( N i | Ck ) ⋅ P (Ck )}

∑Π

N i ∈N n

(9)

(10)

P ( N i | Ck ) ⋅ P (Ck )}

where Nn refers to a nominal group for the subject of
negative premises and Np refers to the nominal group for the
subject of positive premises.
The other category-based model utilizes similar
processes, except that it uses the probability P(Ck|Ni) /Sum,
instead of P(Ni|Ck), where Sum=ΣiP(Ck|Ni). In this model, it
is assumed that P(conclusion)=Pp-Pn. In this case,
P(Ck|Ni)/Sum may be regarded as representing the
subjective probability of the noun N belonging to a given
category Ck. For example, taking into account the
conditional probabilities, P(Ck|Ni) in Table 1, it is not
difficult to subjectively decide which latent class Ck
includes the noun Ni. “Stock” and “government bonds” are
strongly related to the latent class of valuable assets and
“soldier,” “police officer” and “clerk” are strongly related to
the latent class of jobs. Thus, the latent classes of valuable
assets and jobs are considered as categories. “Stock” and
“government bonds” are regarded as members of the
category “valuable assets,” while “soldier,” ”police officer”
and “clerk” are taken as members of the category “jobs”.
Hereafter, this category-based model will be referred to as
category-based subjective probabilistic model, while the
remaining model, based on the Bayesian theorem, will be
called the category-based Bayesian model.

W ( N i ) = W ( N i −1 ) + [1 − O ( N i )]O ( N i )

P ( N i | Ck ) P (Ck )

∑ k P( J l | Ck ) P( Ni | Ck ) P(Ck )

N i ∈N p

n
k

The present study proposes three types of models. The first
type of model is a feature-based neural network. The
feature-based model proposed here has an input layer and
one output node, where the weights between input nodes
and the output node are estimated by the usual delta method
using the features strengths of positive and negative
premises, which are computed as the conditional probability
of a word representing a feature, given a particular word
representing the premise, according to following formulas,

i

conci

k

Construction of Models

ij

| Ck ) ⋅ Π N i ∈N p P ( N i | Ck ) ⋅ P(Ck )}

conci

k

Psychological Experiment of Inductive
Reasoning

(7)

P ( N ij | Ck ) P (Ck )

A psychological experiment concerning inductive reasoning
was conducted to evaluate these models.

High values for P(Aj|Ni) and P(Jl|Ni) indicate a strong
relationship between a noun Ni and an adjective Aj, and
between a noun Ni and a predicate Jl, respectively. This
feature-based neural network model actually uses adjectives
and predicates that have relatively strong conditional
probabilities given words that represent positive and
negative premises. For example, imagine a piece of
inductive reasoning that has as its conclusion “A designer is
accepted into club B,” and as its positive premise “An artist
is accepted into club B.” and as its negative premise “An
office worker isn’t accepted into club B,” then, P(Aj|
“artist”) and P(Aj| “office worker”), which have relatively
stronger values, will be selected as feature values. These
values for P(Aj|Ni) are used as terms for the vector I(Ni) in
this model.
The other two models are both category-based probabilistic
models, which employ similar conditional probabilities for
both the category and the premise. One of the categorybased models computes the probability of the conclusion,

Method

In the experiment, participants (about 40 undergraduate
students) were asked to rate the likelihood of a conclusion,
such as “Currency is valuable for Mr. A.”, given premises,
such as “Stock is valuable for Mr. A.” The premises also
included negative ones, such as “Outdoor activities are not
valuable for Mr. A.” Participants rated the likelihood
according to a 5-point scale (see Figure 1). Three sets of
premises and conclusions shown in Table 2 were used in the
experiment. The premises and conclusions in each set are
selected from two different latent classes. For instance, the
positive premises and some of conclusions in the set of
“Wear” are chosen from the class of “Clothes,” and the
negative premises and the other conclusions are chosen
from the class of “Hats”.

1909

Now you know the following premises.

Stock is valuable for Mr. A.
Place is valuable for Mr. A.
Walking is not valuable for Mr. A.
Outdoor activities
are not valuable for Mr. A.
Please estimate how likely the following conclusion is true given above premises.

“Currency is valuable for Mr. A.”
strongly
likely

likely

neutral

strongly
unlikely

unlikely

Figure1: Example of the rating tasks
Table 2: Examples of the premises and conclusions used in the experiments

Jobs

Wear

Valuable and
leisure

scholar

uniform

stock

painter

polo shirt

place

office worker

helmet

walking

female office worker

straw boater

outdoor activities

soldier

kimono

government bonds

police officer

regimentals

building estate

clerk

cloak

real estate

journalist

haori coat

cruiser

prosecutor

jacket

farm

king

mask

tea ceremony

photographer

hood

shopping

general

straw hat

hiking

commander

beret

swimming in the sea

chef

silk hat

billiards

nouns used in
positive
premises

actor
nouns used in
negative
premises

nouns used
in
conclusions

Table 3: The Results of the Experiment

Jobs
scholar
office worker
soldier
police officer
clerk
journalist
king
photographer
general
commander

Valuable and
leisure

Wear
5.000
1.000
2.522
2.023
1.681
2.955
3.250
4.068
3.023
2.841

uniform
helmet
kimono
regimentals
cloak
haori coat
mask
hood
straw hat
beret

5.000
stock
1.000
walking
2.885
government bonds
2.577
building estate
2.269
real estate
2.885
cruiser
2.154
tea ceremony
2.308
shopping
2.115
hiking
Experimental
Results
2.654
swimming in the sea

1910

5.000
1.000
4.821
4.750
4.607
3.036
2.071
2.393
1.214
1.393

Average rated values for each conclusion are shown in
Table 3. The total averages for each set of premises and
conclusions are 3.208 (SD = 1.190) for Jobs, 2.846 (SD =
0.985) for wear, and 3.084 (SD = 1.460) for valuable and
leisure.

Development of Search-engine Based on
Inductive Reasoning
The present study has developed a trial version of a search-engine
based on inductive reasoning using the category-based
probabilistic model, which provided the best results of the three
models. The search-engine outputs words as conclusions that are
inductively reasoned from given words as premises according to
the category-based psychological model. Nouns with relatively
larger output values from the model are considered as typical
results of the search engine. An example of a search given positive
and negative keywords (premises) is shown in Table 5

Comparison Between Psychological
Experiment and Models’ Simulations

Average Evaluations in the
Experiments(Valuable and leisure

Simulations of all three models were executed. Correlations
between the results from the psychological experiment and
the simulation results, together with F ratios (the fitness
indices for the models) are shown in Table 4. From this
table, it is clear that the results for the feature-based model
and the category-based subjective probabilistic model are
well correlated with the experimental results, and are better
than the simulation results for the category-based Bayesian
model. The relationship between average evaluations in the
experiment (valuable assets) and the output from of the
category-based subjective probabilistic model is represented
in Figure 2.

Discussion

From the comparisons of the simulation and experimental
results, it is clear that the feature-based model and the
category-based subjective probabilistic model are well
correlated with the results of the experiments, and that their
results were better than those of the category-based
Bayesian model. Using the category-based subject
probabilistic model, we have successfully implemented a
trial version of a search-engine based on inductive
reasoning.

6.000
5.000
4.000
3.000
2.000
1.000
0.000
-0.01

-0.005

0

0.005

0.01

0.015

Output of the model

Figure 2: Comparison between the Experiment results (Valuable and leisure)
and the Category-based Psychological model

Table 4: Correlations between Experiments and Models

Jobs
Feature-based model

Category-based

Subjective
probabilistic
model
Bayesian model

correlation
coefficients
F ratios
correlation
coefficients
F ratios
correlation
coefficients
F ratios

1911

Wear

Valuable
and leisure

0.761

P<0.001

0.624

P<0.001

0.906

P<0.001

42.53

P<0.001

19.11

P<0.001

137.49

P<0.001

0.729

P<0.001

0.661

P<0.001

0.932

P<0.001

35.25

P<0.001

23.22

P<0.001

199.35

P<0.001

0.535

P<0.01

0.483

P<0.01

0.450

P<0.01

12.18

P<0.01

9.15

P<0.01

7.62

P<0.01

Acknowledgments

Table 5: Outputs from the Search-Engine

Output

Positive keywords
“Place”
“Stock”

Negative keywords
“Walk”
“Outdoor life”

1

place

2

stock

3

credit

4

wealth

5

government bonds

6

real estate

7

bequest

8

farm

9

condominium

10

dollar

11

inheritance

12

bond

13

yen

14

remains

15

family business

The present study is supported by the Tokyo
Institute of Technology 21COE Program,
“Framework
for
Systematization
and
Application
of
Large-scale
Knowledge
Resources”.
The authers would like to thank Dr. Joyce, T.,
the post doctor of 21COE-LKR, for his critical
reading of manuscripts and valuable comments
on the earlier draft.

References
Kameya, Y., & Sato, T. (in press). Computation
of probabilistic relationship between concepts
and their attributes using a statistical analysis of
Japanese corpora. Proceedings of Symposium on
Large-scale Knowledge Resources: LKR2005
Kudoh, T., & Matsumoto, Y. (2000). Japanese
Dependency Analysis Based on Support Vector
Machines. Proceedings of Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora
:EMNLP/VLC2000
Hofmann, T. (1999). Probabilistic latent semantic indexing.
Proceedings of the 22nd International Conference on
Research and Development in Information
Retrieval :SIGIR ’99. (pp. 50-57).
Osherson, D. N., Smith, E. E., Wilkie, O. Lopez, A., and
Shafir, E. (1990). Category-Based Induction.
Psychological Review, 97, 2, 185-200.
Osherson, D. N., Stern J., Wilkie O., Stob M., and Smith E.
E. (1991) Default Probability. Cognitive Science, 15,
251-269.
Pereira, F., Tishby, N., and Lee, L. (1993). Distributional
clustering of English words. Proceedings of the 31st
Meeting of the Association for Computational Linguistics.
(pp.183-190).
Sloman, A. T. (1993). Feature-Based Induction. Cognitive
Psychology, 25, 231-280.

However, the corpus used in the present study consists
only of newspaper articles. A newspaper has its own style
of representation and its own content bias. The style and
content naturally influence the statistical analysis results in
the present study, which may, in turn, create certain biases
in the simulation results. For example, the category of
‘sports’ and the category of ‘crime’ were typically
identified as latent semantic classes in the present study,
which may reflect the contents of the newspaper. In order to
avoid this kind of bias, it is necessary to collect various
corpora including a wide range of style and a variety of
content. Now, we are planning to analyze many kinds of
corpora, not only newspapers, but other materials like
literature, dictionaries, or encyclopedias.
Other models beyond those discussed in this paper, for
example a neural network for a category-based model,
should also be used in future studies, in order to identify the
most appropriate model for natural inductive reasoning.
An important future plan is to develop a practical searchengine through inductive reasoning, based on the results
from these future analyses.

1912

