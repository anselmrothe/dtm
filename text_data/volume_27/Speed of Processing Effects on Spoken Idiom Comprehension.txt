UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Speed of Processing Effects on Spoken Idiom Comprehension
Permalink
https://escholarship.org/uc/item/1nt3q2ng
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Bennetto, Loisa
Cacciari, Cristina
Campana, Ellen
et al.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Real-time Integration Of Gesture And Speech During Reference Resolution
                                          Ellen Campana (ecampana@bcs.rochester.edu)
                                  Department of Brain and Cognitive Sciences, University of Rochester
                                                         Rochester, NY 14627, USA
                                         Laura Silverman (lauras@psych.rochester.edu)
                          Department of Clinical and Social Sciences in Psychology, University of Rochester
                                                         Rochester, NY 14627, USA
                                        Michael K. Tanenhaus (mtan@bcs.rochester.edu)
                                  Department of Brain and Cognitive Sciences, University of Rochester
                                                         Rochester, NY 14627, USA
                                         Loisa Bennetto (bennetto@psych.rochester.edu)
                          Department of Clinical and Social Sciences in Psychology, University of Rochester
                                                         Rochester, NY 14627, USA
                                        Stephanie Packard (sp002m@mail.rochester.edu)
                                  Department of Brain and Cognitive Sciences, University of Rochester
                                                         Rochester, NY 14627, USA
                               Abstract                                  immediately integrated with the content of the co-occurring
                                                                         speech (McNeill, Cassell, and McCullough, 1994).
   There is some disagreement among researchers about the role              One factor that may have contributed to the lack of
   of gesture in comprehension; whether it is ignored, processed         consensus is that gesture researchers have tended to focus on
   separately from speech, used only when speakers are having            answering the question: is gesture communicative? (see
   difficulty, or immediately integrated with the content of the co-     Kendon, 1994 for a review). In doing so, they may have
   occurring speech. The present experiment provides evidence in
                                                                         confounded two questions that could be considered
   support of immediate integration. In our experiment
   participants watched videos of a woman describing simple
                                                                         independently: 1) do speakers intend to communicate using
   shapes on a display in which the video was surrounded by four         gesture in natural interaction? and 2) do comprehenders
   potential referents: the target, a speech competitor, a gesture       benefit from gesture when it naturally co-occurs with speech?
   competitor, and an unrelated foil. The task was to “click on the      For instance, some results suggesting a lack of intentionality
   shape that the speaker was describing”. In half of the videos         have been taken as evidence that listeners’ do not use gesture
   the speaker used a natural combination of speech and gesture.         during comprehension (Krauss et al., 1994).
   In the other half, the speaker’s hands remained in her lap.              In the current study, we focus only on comprehenders. Our
   Reaction time and eye-movement data from this experiment              goal is to investigate how and when comprehenders make use
   provide a strong demonstration that as an utterance unfolds,          of naturally co-occurring gesture and speech (leaving aside
   listeners immediately integrate information from naturally co-        for the moment the question of whether speakers intend for
   occurring speech and gesture.                                         gesture to be communicative). There is a growing body of
                                                                         evidence that listeners integrate many forms of extra-
                           Background                                    linguistic information when comprehending co-occurring
Non-deictic manual gestures are ubiquitous in language                   speech, including embodiment constraints (Chambers,
production. People produce gesture even when their                       Tanenhaus, and Magnuson, 2004) gaze (Hanna and Brennan,
interlocuters cannot see them (Alibali, Heath and Myers,                 in prep), and disfluency information (Ferreira, Lau and
2001) and can experience difficulty speaking when their                  Bailey, in press; Arnold, Tanenhaus, Altmann, and Fagnano,
                                                                         2004). Listeners take advantage of this information even
hands are restricted (Graham and Heywood, 1975). There is
                                                                         when it is not intended by the speakers to be communicative.
some consensus that the production system allows different
                                                                         We would therefore expect listeners to take account of
aspects of a single message to be expressed simultaneously               gestures as well. Much of this recent work has been done
over, and distributed between, linguistic utterances and                 using the Visual World Paradigm (Tanenhaus et al. 1995;
gestures (Alibali, Kita and Young., 2000) However,                       Cooper, 1974). Here, we apply this methodology to determine
researchers disagree about the role of gesture in                        whether or not comprehenders benefit from gesture that co-
comprehension; whether it is ignored (Krauss, Dushay, Chen               occurs with speech
and Rauscher., 1995), processed separately from language
(Goldin-Meadow and Singer, 2003), used only when speakers
are having difficulty (Rauscher, Krauss and Chen, 1996), or
                                                                     378

                                                                       both speech + gesture videos and speech – only videos. The
                                                                       speech-only videos were directly modeled on the natural
                                                                       speech + gesture videos and contained identical verbal
                                                                       descriptions of the target objects. For example, for the set of
                                                                       potential referents shown in figure 2, the speaker in the video
                                                                       verbally described the target as “a mitten and a curved line
                                                                       with one loop.” This verbal description was the same for both
                                                                       the speech + gesture and the speech-only conditions.
                                                                       However, in the speech + gesture condition the speaker also
                                                                       traced the shape of the loop in the air while in the speech-only
                                                                       condition her hands remained in her lap. The next two
                                                                       sections describe the experimental manipulation we did in
                                                                       more detail, focusing on: 1) the sets of potential referents that
                                                                       participants had to choose between for each trial and 2) the
                                                                       videos, specifically how we elicited natural gestures for this
                                                                       task.
  Figure 1: Volunteer modeling the ASL head-mounted eye-               Stimuli: Potential Referents
tracker that participants wore during the experiment. The eye-
     tracker recorded the position of the participant’s gaze.          As outlined in the previous section, for each trial in our
                                                                       experiment there was a set of four potential referents. Each
                                                                       potential referent was comprised of two individual objects or
                            Method                                     features, one of which was chosen to be easy to describe
We collected data from 10 participants, who were paid $7.50            verbally, and the other of which was chosen to be difficult to
for an hour of their time. Participants wore a lightweight             describe verbally, but easy to gesture. For each trial there
head-mounted eye-tracker (figure 1) as they watched videos             were two of the former and two of the latter. The features
surrounded by four potential referents (figure 2).                     were combined in such a way that each potential referent had
                                                                       one of each type of feature. Based on these features, the
                                                                       potential referents were classified as target, speech
                                                                       competitor, gesture competitor, or unrelated foil (Table 1).
                                                                           Table 1: This table describes the four types of potential
                                                                           referents that appeared in each trial. “Objects” were the
                                                                         contents of a single quadrant on the participants’ screen, but
                                                                          they were sometimes composed of several line drawings.
                                                                       Object            Object Type
                                                                                         Target
                                                                                         Consistent with both speech and gesture for
                                                                                         the entire description
                                                                                         Speech Competitor
                                                                                         Consistent with speech, but not gesture, up
                                                                                         until a point near the end of the description
                                                                                         Gesture Competitor
      Figure 2: Example screen from the speech + gesture                                 Consistent with gesture, but not with speech
 condition. The actor in the center of the screen describes the
 contents of the top left-hand quadrant, saying “a mitten and a
  curved line with one loop” as she gestures the shape of the                            Foil
                              loop.                                                      Inconsistent with both speech and gesture
The participants’ task was to “click on what the speaker
described.” In half of the 14 trials the speaker in the video          The remainder of this section will go into more detail
used a natural combination of speech and gesture (Campana              concerning what we meant by these descriptions, taking each
et al, 2004). In the other half of the trials, the speaker’s hands     one in turn.
remained in her lap.                                                      The target item was the one that the speaker described – at
   The stimuli were counterbalanced in two lists such that             the end of the description, this is the object that participants
across participants each set of visual stimuli occurred with           were expected to click on. The other types of items were
                                                                   379

chosen based on their relationship to the speech and gesture             In the speech-only condition, videos were captured in a
as the speakers description unfolded.                                 second session. In order to replicate the speech + gesture
   The speech competitor was consistent with the speech up            videos without any hand movements, the volunteer model
until a certain point in the utterance (we’ll call this point the     was provided with both written transcripts and sound clips of
“point of disambiguation,” or POD). In the example from               the speech + gesture condition prior to recording each trial.
figure 2, in which the description was “a mitten and a curved         Videos for the speech-only condition were chosen on the
line with one loop,” the speech competitor is consistent with         basis of identical verbal description and similar facial
the first part of the description, “a mitten and a curved line        expression, articulation, intonation, and timing.
with”, but it is inconsistent with the description as a whole
given the final words “one loop.” It should also be noted that                                    Results
the speech competitor in our study was always inconsistent            This experiment yielded both coarse-grained reaction-time
with the gesture that the speaker in the video made in the            data and fine-grained eye-movement data. Both sources of
speech + gesture condition.                                           data are relevant for interpreting the results of the experiment,
   The gesture competitor was inconsistent with the speech            and both support the conclusion that during reference
even very early on in description, but it was consistent with         resolution, naturally co-occurring gesture and speech
the gesture in the gesture + speech condition. In the example         information are immediately integrated. In this section we
from figure 2, the speaker traces the shape of the loop in the        will first describe the reaction time data and then go on to
speech + gesture condition. The gesture competitor is                 describe the eye-movement data.
consistent with this gesture even though it is inconsistent with
“a mitten” in the verbal description (it has a key instead).
                                                                      Results: Reaction Time
   Finally, the foil was unrelated to both the verbal description
and the gesture in the gesture + speech condition. It was,            We analyzed the reaction times with respect to the point of
however, systematically related to the other potential referents      disambiguation (POD) in each individual video. As described
in the trial. It had one feature in common with the gesture           in the previous section, the POD is the point in time at which
competitor (the one that was not traced in the gesture +              the utterance (just the literal component) goes from being
speech condition, e.g. the key) and one feature in common             consistent with several potential referents to being consistent
with the speech competitor (the one that was not included in          with only one. For the trial shown in figure 2 the POD would
the verbal description e.g. the curved line with two loops).          be the onset of the word “one” in the description “a mitten
The rationale for this was that we wanted to reduce the               and a curved line with one loop.” The POD is relevant
predictability of the target.                                         theoretically to the reference comprehension literature and
                                                                      has been used in similar studies investigating the time course
Stimuli: Videos                                                       of reference resolution (Brown-Schmidt, Campana, and
                                                                      Tanenhaus, 2004; Chambers, Tanenhaus, Eberhard, Filip, &
For each trial, the potential referents surrounded the visual         Carlson, 2002, Hanna, Tanenhaus & Trueswell 2003,
stimuli, which were counterbalanced between two lists such            Eberhard, Spivey-Knowlton, Sedivy & Tanenhaus, 1995;
that each list consisted of seven trials with speech + gesture        Tanenhaus, Spivey-Knowlton, Eberhard, & Sedivy, 1995 ).
videos and seven trials with speech-only videos. The main                For our experiment, we chose to analyze reaction times
distinction between the two video conditions is the presence          with respect to the POD both because of it’s theoretical
or absence of gesture. The verbal descriptions of the target          relevance and because it allows us to interpret reaction times
objects are identical. In the speech + gesture condition, the         even in the face of variation in length between trials and
speaker uses a natural combination of speech and gesture              between tokens in the two conditions (an inevitable
(Campana et al, 2004); whereas, in the speech-only condition,         consequence of using naturally-produced stimuli). We found
the speaker’s hands remain in her lap.                                that participants who saw the speech + gesture version of a
   For the speech + gesture condition, videos were captured in        given trial more often clicked on the correct target referent
a single session. In the first portion (CC1), a volunteer model       prior to the POD than participants who saw the speech – only
was run in our production experiment (Campana et al, 2004).           version of the same trial (T1(9)=4.87, T2(13)=3.21, p<.05).
This involved briefly showing her a set of four objects exactly
like the visual stimuli described above, except that one was
                                                                      Results: Eye-movement Data
highlighted. Her task was to “get her partner to click on the
square that is highlighted.” Her partner had a similar display        The eye-tracking methodology can provide very fine-grained
without the highlighting and was sitting at a table opposite her      information about language processing in context. In
(off-camera) .After completing all of the trials, she was asked       reference resolution experiments similar to ours, language-
to do the entire set of trials a second time (CC), but to be sure     driven eye-movements have been observed as early as 250 ms
to use her hands if she felt like it.                                 after disambiguating information is encountered, little more
   We selected videos from both sets as stimuli for the               than the time required to program and execute the required
comprehension experiment. After reviewing all the videos,             motor commands (Allopenna, Magnuson, & Tanenhaus,
the spontaneous descriptions (CC1) were chosen over the               1998).
second descriptions (CC) unless the speech was disfluent.                In the context of our experiment participants’ looks to the
Both of the two lists contained two videos from the first data        target, speech competitor, gesture competitor, and unrelated
collection (CC1) and five videos from the second data                 item can provide information about which entities they are
collection (CC).                                                      considering as potential referents of the speaker’s description.
                                                                  380

In our experiment we observed that participants who saw the                                                                                     produced the reference, and correspondingly the context in
speech + gesture version of a given trial were less likely to                                                                                   which our participants’ understood them.
look at the speech competitor between the onset of the                                                                                             The visual stimuli consisted of potential referents that each
description and the POD than participants who saw the                                                                                           had two features that were relevant to the task. In the speech
speech – only version of the same trial (T1(9)=2.27,                                                                                            + gesture condition, both of the target’s features were
T2(13)=2.64, p<.05).                                                                                                                            described in speech and one was also gestured. Given the
  Graphs of the eye-movements in the two conditions reveal                                                                                      information contained in each channel, and the properties of
longer-lasting competition from the speech competitor in the                                                                                    gesture described above, if participants attended to both
speech – only condition (graph 1) compared to the speech +                                                                                      speech and gesture, and immediately integrated the two
gesture condition (graph 2). By competition we mean that                                                                                        sources of information, they would be able to identify the
even after looks to the target have surpassed looks to the                                                                                      target from among the set of potential referents much earlier
speech competitor, looks to the speech competitor still remain                                                                                  than if they attended to just one channel information source.
above baseline (looks to the unrelated foil).                                                                                                   This is, in fact, what we found: participants were able to
                                                                                                                                                identify the target faster in the gesture + speech condition
Graph 1: Proportion of looks over time to the target, speech                                                                                    than in the speech - only condition. The eye-movement data is
competitor, and foil in the speech-only condition. Time is in                                                                                   also consistent with the hypothesis that integration is
frames (30/sec). The average POD is at 62 frames.                                                                                               immediate, and it provides little data in support of the
                                                                             Speech-Only Condition                                              competing hypotheses. Thus, our results demonstrate that as
                         0.6
                                                                                                                             S-O Target
                                                                                                                                                an utterance unfolds, listeners integrate information from
                                                                                                                                                naturally co-occurring speech and gesture.
  Proportion of Looks
                         0.5
                         0.4                                                                                                 S-O Speech
                                                                                                                             Competitor
                         0.3                                                                                                 S-O Foil
                         0.2
                         0.1
                                                                                                                                                                       Future Work
                              0
                                  1       8    15    22    29    36    43     50    57    64    71    78    85    92    99
                                                                                                                                                We are currently utilizing the methodology from this study to
                                                                      Time (Frames)                                                             examine the time course of gesture and speech
                                                                                                                                                comprehension in individuals with high-functioning autism
                                                                                                                                                and Asperger Syndrome. Individuals with autism have
Graph 2: Proportion of looks over time to target, speech                                                                                        significant impairments in social and communicative
competitor, and foil in the speech + gesture condition. Time is                                                                                 domains. Research has established that they experience
in frames (30/sec). The average POD is at 66 frames.                                                                                            difficulties comprehending the nonverbal behaviors of others,
                                                                           Speech+Gesture Condition
                                                                                                                                                such as eye-gaze, pointing, and facial expressions (e.g., Klin,
                        0.6
                        0.5
                                                                                                                             S+G Target         Jones, Schultz, Volkmar & Cohen, 2002; Goodhart & Baron-
                        0.4                                                                                                  S+G Speech
                                                                                                                             Competitor
                                                                                                                                                Cohen, 1993; Blair, Frith, Abell & Cipollotti, 2002). This is
                        0.3
                        0.2
                                                                                                                             S+G Foil
                                                                                                                                                important since, the ability to decode nonverbal information
                        0.1                                                                                                                     is critical for appropriate behavior during social interactions.
                          0
                              1       8       15    22    29    36    43    50     57    64    71    78    85    92    99
                                                                                                                                                For example, Boyatzis & Sataprasad (1994) examined the
                                                                     Time (Frames)                                                              relationship between children’s peer popularity and their
                                                                                                                                                abilities to decode gestures and facial expressions. This study
                                                                                                                                                found that non-verbal abilities, and specifically gesture
                                                                       Implications                                                             decoding skills were significantly related to peer popularity.
At the beginning of this paper we described several                                                                                             Other evidence suggests that people entirely deprived of
hypotheses that have been advanced in the gesture literature                                                                                    exposure to gestures (i.e., children who are congenitally
concerning the relationship between gesture and speech                                                                                          blind) have social and communicative impairments that
during comprehension: 1) gesture could be ignored by                                                                                            resemble those observed in sighted children with autism (e.g.,
comprehenders, 2) gesture could be processed separately and                                                                                     Brown, 1997). Hobson (1993) has proposed an overlap in the
independently from speech, 3) gesture could be used by                                                                                          developmental psychopathology of autism and congenital
                                                                                                                                                blindness, based on a shared inability to process the outward
comprehenders only in situations where speakers seem to be
                                                                                                                                                physical expressions of others’ social-emotional experiences.
having difficulty, and 4) gesture and speech could be
                                                                                                                                                Taken together, these studies suggest an important link
immediately integrated during the process of comprehension.                                                                                     between gesture comprehension abilities and social outcomes
Our experiment provides evidence in support of the 4th                                                                                          that may be critical to understanding the social and
hypothesis: immediate integration.                                                                                                              communicative difficulties observed in autism.
  The videos we used as stimuli in our study were naturally-                                                                                       The goal of our future work involves, first establishing
produced. Like most natural examples of this phenomena, the                                                                                     whether individuals with autism attend to and successfully
gestures were aligned with or slightly preceded speech                                                                                          decode meaningful gestures in the absence of speech. Next, it
(McClave, 1994). In addition, because gestures are holistic                                                                                     involves establishing how and when individuals with autism
and spatial they either encode different features of the referent                                                                               process gestures that occur in the presence of speech. Based
than speech, or they encode the same features as speech, but                                                                                    on the findings of the present study, we know that typical
with a different timecourse (McNeill, 2000).              In our                                                                                adults immediately integrate information that is presented via
experiment we took advantage of these properties by                                                                                             gesture and speech. We expect that individuals with autism
carefully constructing the original context in which our model                                                                                  will show a different time-course and pattern of gesture and
                                                                                                                                          381

speech processing. Specifically, empirical evidence suggests           skills and with popularity. Journal of Nonverbal Behavior,
that individuals with autism have difficulties processing              18(1), 37-55.
information from two modalities. For example, DeGeldger et           Brown, R., Hobson, R.P., Lee, A. & Stevenson, J. (1997).
al., (1991) found that individuals with autism experienced             Are there “autistic-like” features in congenitally blind
significant difficulty matching speech sounds with their               children: Journal of Child Psychology, Psychiatry, and
corresponding mouth and lip movements. Similarly, Bryson               Allied Disciplines, 38, 693-703.
(1972) found that individuals with autism performed                  Brown-Schmidt, S., Campana, E. & Tanenhaus, M.K. (2005)
significantly less well on a task where they matched items             Real-time reference resolution by nave participants during
across verbal and visual modalities compared to a task where           a task-based unscripted conversation. In J.C. Trueswell &
they matched objects within modalities. Research on attention          M.K. Tanenhaus (eds.), World-situated language
also provides evidence that individuals with autism                    processing: Bridging the language as product and language
demonstrate an inability to shift attention between modalities         as action traditions. Cambridge, MA: MIT Press.
(e.g., Courchesne et al., 1994).                                     Bryson, C. (1972). Short-term memory and cross-modal
   Limitations of these studies are that none of them use              information processing in autistic children. Journal of
spontaneous, naturally occurring social stimuli, and none of           Learning Disabilities, 5, 81-91.
these studies look at the natural time course of verbal and          Campana, E., Silverman, L., Tanenhaus, M. K., and Bennetto,
non-verbal processing in autism. The stimuli used in our               L. (2004). Iconic Gesture Production in Controlled
study (as described earlier) involve samples of spontaneous            Referential Domains. In Proceedings of the 26th Annual
instances of gesture and speech collected during a gesture             Meeting of the Cognitive Science Society. Chicago, IL,
production task. In addition, the eye-tracking technology              August 2004.
allows us to examine on-line gesture and speech processing           Chambers, C.G., Tanenhaus, M.K. & Magnuson, J.S. (2004).
and integration as it occurs.                                          Action-based affordances and syntactic ambiguity
   This research has the potential to provide answers to               resolution. Journal of Experimental Psychology: Learning,
questions central to understanding social and communicative            Memory & Cognition, 30, 687-696.
functioning in autism. Furthermore it has clear implications         Chambers, C.G., Tanenhaus, M.K., Eberhard, K.M., Filip, H.,
for treatment. If gesture comprehension in autism is affected          & Carlson, G.N. (2002). Circumscribing referential
by difficulties with cross-modal processing, then                      domains during real-time language comprehension.
interventions can focus on teaching children with autism to            Journal of Memory and Language, 47, 30-49.
attend to gestures in the presence of speech.                        Cooper, R. M. (1974). The control of eye fixation by the
                                                                       meaning of spoken language: A new methodology for the
                     Acknowledgements                                  real-time investigation of speech perception, memory, and
This research was supported by NIH grant HD-27206 and                  language processing. Cognitive Psychology, 6, 84-107.
(NIMH) F31 MH70119. We would like acknowledge several                Courchesne, E., Townsend, J.P., Akshoomoff, N.A., Yeung-
individuals for their hard work and contributions to this              Courchesne, R., Press, G.A., Murakami, J.W., Lincoln,
research: Rebecca Webb, Hsi-Mei (Betty) Huang, Kathryn                 A.J., Jamies, H.E., Saitoh, O., Egaas, B., Haas, R.H., &
McNamara, Kelley Knoch and Cheryl Carmichael.                          Schreibman, L. (1994). A new finding: Impairment in
                                                                       shifting attention in autistic and cerebellar patients. In
                                                                       Atypical cognitive deficits in developmental disorders:
                           References                                  Implications for brain function, (ed. S.H. Broman & J.
Alibali, M. W., Heath, D. C., and Myers, H. J. (2001).                 Grafman). Hillsdale, NJ: Erlbaum.
   Effects of visibility between speaker and listener on gesture     de Gelder, B., Vroomen, J., & van der Heide, L. (1991).
   production: Some gestures are meant to be seen. Journal of          Face recognition and lipreading in autism. European
   Memory and Language, 44, 1-20.                                      Journal of Cognitive Psychology, 3, 69-86.
Alibali, M. W., Kita, S., and Young, A. J. (2000). Gesture and       Eberhard, K.M., Spivey-Knowlton, M.J., Sedivy, J.C., &
   the process of speech production: We think, therefore we            Tanenhaus, M.K. (1995). Eye movements as a window into
   gesture. Language and Cognitive Processes, 15, 593-613.             real-time spoken language comprehension in natural
Allopenna, P.D., Magnuson, J.S., & Tanenhaus, M.K.                     contexts. Journal of Psycholinguistic Research, 24, 409-
   (1998). Tracking the time course of spoken word                     436.
   recognition using eye movements: Evidence for continuous
   mapping of models. Journal of Memory and Language, 38,            Ferreira, F., Lau, E.F., & Bailey, K.G.D. (in press).
   419-439.                                                            Disfluencies, parsing, and tree-adjoining grammars.
Arnold, J.A., Tanenhaus, M.K. Altmann, R.J., & Fagnano, M.             Cognitive Science.
   (2004). The old and, theee, uh, new: Disfluency and               Goldin-Meadow, S, and Singer, M. A. (2003). From
   reference resolution. Psychological Science.                        children’s hands to adults’ ears: Gesture’s role in the
Blair, R.J.R., Frith, U., Smith, N., Abell, F. & Cipolotti, L.         learning process. Developmental Psychology, 39, 509-520.
   (2002). Fractionation of visual memory: Agency detection          Goodhart, F., & Baron-Cohen, S. (1993). How many ways
   and its impairment in autism. Neuropsychologia, 40, 108-            can the point be made? Evidence from children with and
   118.                                                                without autism. First Language, 13, 225-233.
Boyatzis, C. J., & Satyaprasad, C. (1994). Children’s facial         Graham, J. A., and Heywood, S. (1975). The effects of
   and gestural decoding and encoding: relations between               elimination of hand gestures and of codability on speech
                                                                 382

  performance. European Journal of Social Psychology, 2,          Krauss, R. M., Dushay, R. A., Chen, Y., and Rausher, F.
  189-195                                                           (1995). The communicative value of conversational hand
Hanna, J.E., Tanenhaus, M.K. & Trueswell, J.C. (2003). The          gestures. Journal of Experimental Social Psychology, 31,
  effects of common ground and perspective on domains of            533-553.
  referential interpretation.     Journal of Memory and           McNeill, D. (Ed.) (2000). Language and Gesture.
  Language., 49, 43-61                                              Cambridge: Cambridge University Press.
Hobson, R.P. (1993). Autism and the development of mind.          McNeill, D., Cassell, J., and McCullough, K.-E. (1994).
  Hillsdale, NJ: Lawrence Erlbaum Associates.                       Communicative effects of speech-mismatched gestures.
Kendon, A. (1994). Do gestures communicate? A review.               Research on Language and Social Interaction, 27, 223-
  Research on Language and Social Interactions, 27, 175-            237.
  200.                                                            Rauscher, F. H., Krauss, R. M. and Chen Y. (1996). Gesture,
Klin, A., Jones, W., Schultz, R., Volkmar, F.R. & Cohen, D.J.       speech, and lexical access: The role of lexical movements
  (2002).     Visual fixation patterns during viewing of            on speech production. Psychological Science, 7, 226-231.
  naturalistic social situations as predictors of social          Tanenhaus, M.K., Spivey-Knowlton, M.J., Eberhard, K.M.,
  competence in individuals with autism. Arch. Gen.                 & Sedivy, J.C. (1995). Integration of visual and linguistic
  Psychiat., 59, 809-816.                                           information in spoken language comprehension. Science,
                                                                    268, 1632-1634.
                                                              383

