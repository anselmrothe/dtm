UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards a Quatitative Corpus-based Evaluation Measure for Syntactic Theories
Permalink
https://escholarship.org/uc/item/9jm0f2cx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Chang, Franklin
Lieven, Elena
Tomasello, Michael
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

    Towards a Quantitative Corpus-based Evaluation Measure for Syntactic Theories
                       Franklin Chang (chang@eva.mpg.de), Elena Lieven (lieven@eva.mpg.de),
                                             and Michael Tomasello (tomas@eva.mpg.de)
                                             Max Planck Institute for Evolutionary Anthropology
                                                Deutscher Platz 6, Leipzig 04103, Germany
                               Abstract                                  words in actual utterances in corpora. To insure that our test
                                                                         sentences are representative of the syntactic knowledge that
  It is difficult to quantitatively evaluate the match between           is typically present in production, we will use the utterances
  syntactic theories and human utterance data. To address this           in spoken corpora between children and adults (rather than
  problem, we propose an automatic evaluation measure based
                                                                         artificial linguistic examples, or highly edited utterances in
  on a syntactic theory’s ability to predict the order of words in
  actual utterances in corpora. To test this measure, a lexicalist
                                                                         written corpora). By measuring the difference between the
  theory of syntax acquisition is utilized and shown to achieve          syntactic learner’s predictions and the actual utterances, we
  relatively high levels of prediction accuracy on both adult and        can quantitatively evaluate the learner’s viability as theory
  child data from five typologically-different languages.                of syntax acquisition. To determine if this measure of
  Application of this evaluation measure to different input and          syntactic knowledge is useful, we will use it to examine
  output sets shows that this measure can address important              several criteria that are used to evaluate syntactic theories:
  linguistic criteria such as computational sufficiency,                 computational sufficiency, typological flexibility, and
  learnability, and typological-flexibility.                             learnability from the input.
                                                                            One of Chomsky’s (1957) early claims was that the
  Keywords: Corpora, Syntax Acquisition, Evaluation
                                                                         computational sufficiency of a learner for representing
                                                                         human languages was an important determinant in selecting
           Comparison of Syntactic Theories                              a language learner. If our computational learner is not
There is a fairly large gap between computational                        appropriate for accounting for the utterances in real corpora,
implementations of syntactic theories and human syntactic                then we should expect that the learner will never achieve a
knowledge. For example, computational systems often use                  high level of prediction accuracy. However, if it is possible
tagged corpora for either training or evaluation, while                  to find a set of conditions where it is possible to predict
humans are able to demonstrate syntactic competence by                   most of the syntactically-appropriate word orders in a
processing        or     producing        untagged       utterances.     corpora, then such an algorithm could be said to be
Computational systems are tested on large written corpora                computationally sufficient for accounting for the syntactic
or hand-crafted test sentences which may not be                          constraints implicit in these orderings.
representative of the utterances that humans actually                       Another important criterion for a linguistic theory is that
produce. Also children use their developing syntactic                    is can accommodate constraints that are embodied in the
knowledge to produce utterances, but existing                            syntax of typologically-different languages (fixed/free word
computational systems cannot be easily compared to these                 order, argument omission, rich/poor morphology). Because
sometimes ungrammatical utterances (e.g., Anne, 2;7 “can                 these typological features have an impact on the set of
you know doesn't be in there”). Finally, children can                    words and their order, our prediction measure can examine
automatically learn any human language, while                            whether a particular learner can deal with the structural
computational systems are often language-specific and are                changes that these features induce. A typologically-flexible
typically not tested with typologically different languages.             algorithm should yield similar magnitudes of improvements
To address these issues, we will present an algorithm that               for languages with different typological features.
can automatically learn syntactic constraints from untagged                 The third criterion relates to learnability or the
corpora, and such knowledge can help in sentence                         relationship between the input and the syntactic abstractions
production of utterances from 5 typologically different                  that need to be learned. Traditionally, syntactic abstractions
languages from both children and adults.                                 have been presumed to be difficult to learn because the
    Syntax in sentence production helps to order the                     input does not directly model the syntactic abstractions that
concepts that one wants to convey. For example, in                       are thought to be needed (this is called the poverty of the
Japanese, the verb must follow its arguments (e.g., “nobuko-             stimulus argument). Our measure can evaluate learnability
ga inu-o suki” is ok, but not “nobuko-ga suki inu-o”). In                by examining the prediction accuracy for the child’s
German, articles must precede the nouns that which they                  utterances when trained on the adult utterances from the
agree in gender and which they mark the case of (e.g., “Die              same corpus. If the small sample of adult utterances in
Tigerente isst den Döner” is fine, but not “Tigerente den isst           these corpora (small relative to the years of input that
Döner die”). In English, “the ball red” is ungrammatical,                children use to learn to produce their utterances) can predict
unlike the canonical “the red ball”. Our evaluation measure              a substantial proportion of the child’s utterances, then that
will measure how well a syntactic learner can order the                  would suggest that the input provides much of what the
                                                                     418

learner needs to predict syntactic orderings, and that would      makes it harder to use the previous word to create a
militate against the view that the input is impoverished. In      category. Because of these issues with distributional
sum, word order prediction accuracy can evaluate how well         approaches to syntax, it is not clear that there is an
a computational syntax learner can predict actual utterances,     automatic way to learn syntactic knowledge cross-
and by comparing different types of inputs, outputs, or           linguistically from distributional information in corpora.
languages, we can use this measure to examine issues                 To summarize, we have proposed an evaluation measure
related to computational sufficiency, learnability, and           that should allow us to examine various linguistic criteria
typological-flexibility.                                          that are important in comparing syntactic theories. But
   To demonstrate the viability of our evaluation measure,        because existing computational instantiations of syntactic
we will test a simple computational theory of syntax              theories do not do word order prediction, we will present a
acquisition that is inspired by connectionist approaches to       simple example of the syntax acquisition learner that can
syntax (Elman, 1990). To avoid the complications of               accomplish the mapping that we are suggesting that children
learning and using abstract syntax, this approach will only       are learning. This learner, which we called the lexical
represent the lexically-specific aspects of these theories.       producer, will be shown to predict the order of words in the
Lexically-specific representations are compatible with most       speech of adults and children in a set of typologically-
linguistic approaches and are easily collected from language      different languages. Then we will show that some of these
corpora. But while there are good motivations for taking          constraints can be learned from just a small subset of the
this approach, there are many reasons to think it will not be     adult speech that these children actually receive. This will
very successful at predicting syntactically-appropriate word      demonstrate that it is possible to create a system that
orders. While many linguistic theories emphasize the role         achieves reasonable performance when evaluated by this
of lexical entries in syntax (Pollard & Sag, 1994), these         measure. In the future, when existing generative and
lexical entries get their power from links to more abstract       statistical syntactic theories are adapted to word order
information (e.g. verb subcategorization, parameters). And        prediction, this evaluation measure will allows us to see
while there is evidence showing that children make                how much different syntactic abstractions (e.g., parameters,
extensive use of lexical information early on in language         syntactic categories) improve prediction over the lexical
acquisition (Bates & Goodman, 1997; Tomasello, 1992), it          producer.
is      also      thought     that     abstract     syntactic
structures/constructions and meaning, are also needed for                      The lexical producer algorithm
acquisition of the appropriate constraints (Gleitman, 1990;
Lieven, Behrens, Speares, & Tomasello, 2003; Pinker,              Since our goal is to model the acquisition of the constraints
1989; Tomasello, 2003). Also, distributional corpus-based         needed for ordering words in sentence production, our
approaches that attempt to discover syntactic categories like     approach needs to fit with work in sentence production
nouns and verbs have not yielded techniques that are              (Bock, 1995). First, sentence production begin with a
comparable across languages (part of speech taggers often         message or meaning that the person wants to convey. This
use different sets of tag categories for each language,           message is important in determining the words that are used.
Manning & Schütze, 1999) and approaches that attempt to           Second, production is an incremental process where
be psycholinguistically realistic have not been shown to          structure selection is determined by the words that are
yield the same level of accuracy in typologically-different       selected early on in an utterance. A third feature of sentence
languages (Mintz, 2003; Redington, Chater, & Finch, 1998).        production is that we need to “deactivate the past” (Dell,
   Computational distributional learning approaches have          Burger, & Svec, 1997), to keep recently produced (and
been mainly tested in English, a language that is ideal for       therefore activated) words from being reactivated. To
these algorithms. English has relatively rigid word order,        incorporate these aspects of production into the lexical
limited morphology, a small set of articles, and no argument      producer, we formalized the problem in this way. The
omission. These properties make the previous word a               lexical producer was given the words that the speaker
particularly good cue for classifying words into categories       actually produced, but as an unordered list that we called the
like nouns and verbs in English. But it is not clear whether      candidate set. The words that a speaker uses reflects
these techniques will work for other languages. In relatively     features of the message that they are trying to convey (e.g.
free word-order languages like Japanese, German, or               articles encode discourse information), and since speakers
Croatian, there might be more variability in these word           know what they want to say before they speak, the candidate
transitions.    In languages with rich morphology like            set captures the influence of message knowledge. The task
Croatian, nouns can have different forms for each                 of the producer is to create an ordered sequence of words
combination of case, gender, and number, and that means           from this set. This ordering will be done incrementally,
that a larger number of word transitions have to be               where the system has to choose the next word in the
abstracted over in order to discover a category. And in           utterance from the candidate set. Then the word that was
languages that allow all arguments to be omitted like             actually produced next by the speaker will be removed from
Japanese (relatively free word-order) and Cantonese               the candidate set, and then the system will again attempt to
(relatively fixed word-order), the words that appear before       choose the next word from this shorter set. The removal of
the word to be categorized will be more variable, and this        this word accomplishes the suppression of the past, and
                                                              419

allows us to test the system’s accuracy against each of the        member A was before the second member B (A>B). So in
choices that the speaker actually made. This is crucial for        the above utterance, the access counters for look>at,
evaluating the system against actual utterances in different       look>me, or at>me pairs would all be 1. The paired counter
languages.                                                         recorded how often each pair of words in the utterance
   Before describing the algorithm in more detail, it is first     occurred together in any order. The paired counters for
important to mention two issues related to the preprocessing       .=look, .=at, .=me, look=., look=at, look=me, at=., at=look,
of the utterances. Before processing each utterance (both          at=me, me=., me=look, and me=at pairs would all be 1.
for production and for recording the statistics), the              These three types of counters were used to create the
punctuation (period, question mark, exclamation mark) was          context and access biases that will be used by the lexical
moved from the end of the utterance to the first position.         producer. The context bias for an ordered pair of words was
This made it the first word in the utterance, and the system       just the context counter divided by the paired counter for
could use this punctuation word to predict the first real word     those two words (e.g. context-bias(look-me) = 0/1 = 0). The
in the utterance (e.g. “? who did this”). Second, repeated         access bias for an ordered pair of words was just the access
words within an utterance were given number indexes (e.g.          counter divided by the paired counter for these two words
the-1) to distinguish them from other tokens, but these            (e.g. access-bias(look>me) = 1/1 = 1). Dividing by the
indexes started from the first repeated word starting from         paired counter removed the pair frequency from the biases.
the end of the utterance (e.g., “. i-‘m the-1 king of the             Now the production algorithm will be described. Before
castle”). This made production of the latter part of an            production began, the producer started with the candidate
utterance (e.g. “of the castle”) similar to other utterances       set (e.g. “at”, “me”, “look”) and the punctuation word (e.g.
with those words.                                                  “.”) as the previous word. For each word in the candidate
   The lexical producer algorithm was inspired by a                set, the system calculated a context score and access score
particular connectionist architecture for sentence                 and then summed them together to get a choice score. The
production.       This architecture, called the Dual-path          context score for each word in the candidate set was just the
architecture, has been argued to be superior for sentence          context bias from the previous word at this point in
production models, because it allows these models to               production of this utterance. The access score for each word
generate utterances with words in novel sentence positions         in the candidate set was just the sum of all the access biases
(Chang, 2002).         This architecture is composed of            to that word from all the words in the candidate set divided
sequencing pathway, which is a simple recurrent network            by the number of candidate words. Hence, the context and
that learns which sets of words occur in different sentence        access scores captured different aspects of ordering. The
positions (Elman, 1990), combined with a message pathway           context score ranked the candidate words in terms of how
that activates message-appropriate words. By combining             likely they were to occur after the previous word. The
the output of these two pathways, the system could use             access score ranked the candidates in terms of how likely
novel meanings to sequence novel utterances, but in a way          they were produced before the other candidates.
that was consistent with the syntactic properties of the              The context bias is a type of bigram statistic that is
language. To simulate the two pathways in the Dual-path            commonly used in distributional learning approaches to
model, two types of information will be used by the lexical        syntax. The access statistic on the other hand is relatively
producer to sequence the words in the candidate set. One           novel, because it assumes that the existence of a candidate
type of information, referred to here as context information,      set of words that are in competition for selection. Since
will record how well the context (the previous word)               most distributional learning systems that use corpora take a
predicts the next word in the sentence, much like the              comprehension approach to syntax (utterance -> abstract
sequencing network in the Dual-path model. The other               representation), they have not made use of access-type
information, referred to here as access information, will          statistics.
record how often a word precedes other words in an
utterance. When the access information for all the candidate       The corpora
words is combined together, it simulates the competition
between the words that are activated by the message in the         To allow us to compare typologically-different languages,
message pathway in the Dual-path model.                            corpora for English, German, Croatian, Japanese, and
   The lexical producer uses only lexically-specific               Cantonese were selected from the CHILDES database
information in the input corpora to derives the context and        (MacWhinney, 2000). Our goal for this comparison was
access information. To do this, it collects three types of         just to see if the algorithm could work at a similar level of
lexical statistics from the corpora: the context counters, the     accuracy for languages that should be more difficult for
access counters, and the paired counters. The context              distributional approaches than English. The databases that
counters count how often a pair of adjacent words in an            were chosen were those that attempted to gather substantial
utterance occur together in utterances in the corpora.             amounts of recorded data from a single child. Table 1
Taking as an example the utterance “. look at me”, the             shows the children from the different databases that were
context counters for .-look, look-at, and at-me pairs would        chosen as the corpora to be used in our analysis. The
all be 1. The access counter incremented its counter for all       utterances in each database were separated into child
word pairs (excluding the punctuation word) where the first        (utterances from the target child) and adult (all other
                                                               420

utterances in the corpora). The final column in Table 1                with the same data. Self-prediction tests how well the
specify the number of child utterances that were used. The             learner can memorize and reproduce the test set, and is akin
utterances in each database had extra coding of missing                to repetition tasks that have been used to test syntactic
sounds, pause information, and a variety of other codes.               knowledge in children and adults (Chang, Bock, &
Utterances which had uncodeable parts were excluded.                   Goldberg, 2003). Because it provides all the words and the
Other codes were stripped so that the original utterance as            orders that are needed in the test utterances, self-prediction
the child and adult said it was preserved. Morphologically             should tell us whether the lexical producer is sufficient for
marked words were left unchanged (e.g. boy-s), and in                  the task of predicting word order in raw corpora.
general all space-separated strings were treated as different
                                                                                              Figure 1: Utterance Self-prediction
words. All words were converted into lowercase.
                                                                            100
                                                                             90
   Table 1. Corpora from individual children in 5 languages.                 80
                                                                             70
 Corpora/                Database                 Age       # of             60
                                                                             50
   Child                                                   Child             40
                                                            Utt.             30
                                                                             20
English       Manchester (Theakston, Lieven,     1;10-    19943              10
                                                                              0
Anne          Pine, & Rowland, 2001)             2;9                             English   English-   German    German-    Croatian    Japanese  Cantonese
                                                                                            Dense                Dense
English-      MPI-EVA (Lieven et al., 2003)      2;0-     174110        Corpora
                                                                                         Adult-Adult  Chance-Adult   Child-Child    Chance-Child
Dense                                            3;11
Brian
German        Nijmegen (Miller, 1976)            1;9-     28561           Figure 1 shows the utterance accuracy during self-
Simone                                           4;0                   prediction. Overall, the algorithm improves utterance
German-       MPI-EVA                            1;11-    139540       prediction over chance by 43% for adults and 58% in the
Dense                                            4;11                  children. What these results show is that both children and
Leo
                                                                       adults tend to be fairly consistent in the order of words that
Croatian      Kovacevic (Kovacevic, 2003)        0;10-    20875
Vjeran                                           3;2
                                                                       they use. For example, the English-Dense child said “Bill
Japanese      Miyata (Miyata, 1992, 1995)        1;4-     11778        and Sam”, but never “Sam and Bill”. If the child had used
Ryo                                              3;0                   both grammatical orders for a large proportion of their
Cantonese     CanCorp (Lee, Wong, Leung,         2;8-     10021        utterances, their utterance accuracy would be closer to 50%.
Jenny         Man, Cheung, Szeto, & Wong,        3;8                   Instead, the lexical producer can predict 81% of the child
              1996)                                                    utterances (Child-Child) with a single order and 53% of the
                                                                       adult utterances (Adult-Adult). The fact that the children
The utterances from all seven corpora were used to train and           were more likely to use one order for any set of words
test individual versions of the model. Utterance accuracy              suggests that their representations are more lexically-
was the dependent measure used in all the analyses and it              specific than the adults around them. The adult utterances
was the percentage of utterances correctly predicted (where            are more order variable, presumably because the adults have
each word in an utterance had to be correct for the utterance          a more abstract syntax which allows them to produce differ
to be correct). The same word with different number                    word orders to convey different meanings. But overall,
indexes were treated the same for accuracy (e.g. the-1 =               these results show that the two lexically-specific statistics in
the). Since candidate sets with only one word are trivial to           the lexical producer are sufficient to account for most of the
produce and inflate the accuracy results, these situations             syntactically-appropriate word orderings in both adults and
(both one-word utterances and the last position of multi-              the children.
word utterances) were excluded from the results. To get                   The next question is whether the lexical producer system
measure of chance for each of these corpora, a Chance                  has the right learnability characteristics for syntax
model was created that made a random word choice from                  acquisition from sparse amounts of adult data from real
the candidate set at each position in each utterance in each           corpora. To test this, the same algorithm was applied to the
corpus. For example, if you have only two words in your                adult data to extract statistics that were used to predict the
candidate set, then you have a 50% chance to get them in               child’s utterances. It would be surprising if this were
the right order.                                                       possible, since children and adults use different words and
   The first step in evaluating the lexical producer is to see if      talk about things in different ways. Furthermore, our
it is computationally sufficient for predicting word order.            samples of adult speech are only a small sample of the input
As mentioned earlier, it is not clear that it is possible to find      the child actually receives (Cameron-Faulkner, Lieven, &
a single set of lexically-specific statistics that can predict the     Tomasello, 2003).
order of words in actual utterances in different languages.               However when we test the lexical producer on the child’s
To address this issue, we will test whether the adult’s or the         data when trained on the adult input (Adult-Child), we find
child’s data can be used to predict itself. To do this for both        that it can use the adult data to increase utterance accuracy
the adult and the child, we collect our two lexical statistics         36% over chance (Figure 2). While it is not surprising that
by passing through the data, and then we test the system               the adult input is useful to predict the child’s output (since
                                                                   421

the child is learning the language and the words from these                           between the order of words in utterances and computational
adults), it surprising that more than half of the distance                            learner of syntax that allowed us to examine linguistic
between chance (Chance-Child) and self-prediction (Child-                             criteria related to computational sufficiency, learnability,
Child, same as Figure 1) can be predicted from a small                                and typological-flexibility. The measure was high when the
sample of adult speech to the child without abstract                                  training and test data were the same, low when the system
categories or structures like syntactic trees, constructions,                         was randomly choosing the order of words, and
meaning, or discourse. Surprisingly, the amount of input                              intermediate when trained on adult data to predict the
does not seem to change performance, as the dense corpora                             child’s utterances. And the measure was able to achieve
have the same accuracy levels as the non-dense versions.                              high levels for the five typologically-diverse languages
Even though the dense corpora have more utterances to                                 suggesting that it can be used to compare the learning of
learn from, they also have more utterances to account for,                            different languages.
and therefore the percentage accounted for does not change                               In addition, we introduce a simple lexicalist theory (the
much.                                                                                 lexical producer) and showed that it was able to achieve a
                                                                                      good fit to the data. The lexical producer was inspired by
     Figure 2: Utterance Accuracy on Child's Data depending on Input
                                                                                      the     way      that    the    psycholinguistically-motivated
     100
      90
                                                                                      connectionist model of sentence production, the Dual-path
      80                                                                              model (Chang, 2002), learned different types of information
      70
      60                                                                              in each of its pathways. The lexical producer during self-
      50
      40                                                                              prediction was able to predict a large percentage of word
      30
      20
                                                                                      order patterns, which suggests that lexically-specific
      10
       0
                                                                                      knowledge can do much of the work of predicting the order
            English   English-
                       Dense
                                  German   German-
                                            Dense
                                                     Croatian  Japanese Cantonese     of words in human language. When trained on adult speech
 Corpora
                             Child-Child Adult-Child Chance-Child
                                                                                      to the child, the algorithm was able to predict more than half
                                                                                      of the utterances that the child produced. This suggests that
   To examine typological differences between languages,                              the constraints that govern the child’s utterances are
multiple corpora from each language will need to tested and                           learnable from the meager input that this algorithm was
statistically compared. But within the seven corpora tested,                          given. And finally when tested on languages which might
there does not seem to be a large variation in the prediction                         be difficult for distributional learning (flexible word order
accuracy of the lexical producer across the different                                 languages like German, or morphologically rich languages
languages. One reason for this is that that evaluation                                like Croatian, or argument-omitting languages like
measure and the lexical producer balances out some of the                             Japanese), the lexical producer was able to acquire word
typological differences. In argument-omission languages,                              order constraints at a level that approximates its behavior in
one has less distributional information, but one has also                             English. This suggests that it is a typologically-flexible
fewer word orders to predict. In languages with rich                                  algorithm for syntax acquisition.
morphology, the context and access statistics in these                                   There are three respects in which this work is novel.
languages should also be more specific and reliable, but any                          First, it shows that in the utterances from children in five
particular pairing of words is less likely to be represented in                       typologically-different languages, there is a tendency to use
the input corpus.                                                                     a single word order for any set of words. Previous work in
   To summarize, the lexical producer is able to account for,                         English suggests that children tend to be conservative and
averaging over all the corpora, 58% of the child’s utterances                         lexically-specific in their sentence production (Lieven, Pine,
that are two words or longer when trained on a small subset                           & Baldwin, 1997; Tomasello, 1992), but this is the first
of the adult data that the actual child receives. If we include                       typologically-diverse demonstration. Second, it shows that a
all the one-word utterances, the lexical producer’s accuracy                          small sample of the adult-speech to children can predict
rises to 76%, which amounts to 309,559 correctly produced                             more than half of the child’s utterance orderings. This is
utterances in five typologically-different languages. By                              important cross-linguistic counterevidence to the claim that
themselves, these results may not seem impressive. But if                             there is not enough input to explain syntax acquisition
we realize that all existing linguistic theories assume that                          without innate knowledge (poverty of the stimulus). The
word order would require some abstraction over words, it is                           third result is that lexical access can play an important role
surprising that more than three quarters of the utterances                            in computational approaches to distributional learning.
produced by children in five languages are predictable from                           Lexical access is often thought to be a performance
a small sample of adult data with no abstractions at all.                             component of sentence production, rather than a part of
                                                                                      syntax acquisition. But since speakers must learn to order
                                                                                      their ideas (and the words that they activate) and order of
                                    Conclusion                                        words in the input is useful for learning these constraints, it
Theory evaluation in linguistics is not a quantitative science.                       suggests that lexical access knowledge should play a part in
To have a quantitative science, one needs a way to                                    theories of syntax acquisition and use.
numerically evaluate the fit between data and theories.                                  The lexical producer only represented the lexical aspects
Here, we have provided an evaluation measure of the fit                               of the Dual-path model, and hence it is not a complete
                                                                                  422

syntactic theory. In particular, it is not sufficient for          Kovacevic, M. (2003). Acquisition of Croatian in
generating utterances where meaning is used to generate a            Crosslinguistic Perspective. Zagreb.
novel surface form. Its success has more to do with the            Lee, T. H. T., Wong, C. H., Leung, S., Man, P., Cheung, A.,
conservative nature of language that people actually use,            Szeto, K., et al. (1996). The development of grammatical
rather than its learning mechanism. That is why we think             competence in Cantonese-speaking children. Hong Kong:
that the important aspect of this work is the evaluation             Dept. of English, Chinese University of Hong Kong.
measure, which allows us to measure syntactic knowledge              (Report of a project funded by RGC earmarked
in untagged utterances by children or adults in different            grant,1991-1994).
languages.      Hopefully, this evaluation measure will            Lieven, E., Behrens, H., Speares, J., & Tomasello, M.
encourage advocates of particular syntactic theories to show         (2003). Early syntactic creativity: A usage-based
that their representations can be learned from untagged              approach. Journal of Child Language, 30(2), 333-367.
input in different languages, and that these representations       Lieven, E. V. M., Pine, J. M., & Baldwin, G. (1997).
can be use to order the words in sentence production.                Lexically-based learning and early grammatical
                                                                     development. Journal of Child Language, 24(1), 187-219.
                     Acknowlegements                               MacWhinney, B. (2000). The CHILDES Project: Tools for
                                                                     analyzing talk, Vol 2: The database (3rd ed. ed.).
   Preparation of this article was supported by a postdoctoral     Manning, C., & Schütze, H. (1999). Foundations of
fellowship from the Department of Developmental and                  statistical natural language processing. Cambridge, MA:
Comparative Psychology at the Max Planck Institute for               The MIT Press.
Evolutionary Anthropology. We thank Kirsten Abbot-                 Miller, M. (1976). Zur Logik der frühkindlichen
Smith, Gary Dell, Miriam Dittmar, Evan Kidd, and Danielle            Sprachentwicklung: Empirische Untersuchungen und
Matthews for their helpful comments on the manuscript.               Theoriediskussion. Stuttgart: Klett.
Correspondence should be addressed to Franklin Chang at            Mintz, T. H. (2003). Frequent frames as a cue for
chang@eva.mpg.de.                                                    grammatical categories in child directed speech.
                                                                     Cognition, 90(1), 91-117.
                           References                              Miyata, S. (1992). Wh-questions of the third kind: The
                                                                     strange use of wa-questions in Japanese children. Bulletin
Bates, E., & Goodman, J. C. (1997). On the inseparability of
                                                                     of Aichi Shukutoku Junior College, 31, 151-155.
  grammar and the lexicon: Evidence from acquisition,
                                                                   Miyata, S. (1995). The Aki corpus -- Longitudinal speech
  aphasia, and real-time processing. Language & Cognitive
                                                                     data of a Japanese boy aged 1.6-2.12. Bulletin of Aichi
  Processes, 12(5-6), 507-584.
                                                                     Shukutoku Junior College, 34, 183-191.
Bock, K. (1995). Sentence production: From mind to mouth.
                                                                   Pinker, S. (1989). Learnability and cognition: The
  In J. L. Miller & P. D. Eimas (Eds.), Handbook of
                                                                     acquisition of argument structure. Cambridge, MA: MIT
  perception and cognition: Speech, language, and
                                                                     Press.
  communication (Vol. 11, pp. 181-216). Orlando, FL:
                                                                   Pollard, C., & Sag, I. A. (1994). Head-driven phrase
  Academic Press.
                                                                     structure grammar. Chicago: University of Chicago
Cameron-Faulkner, T., Lieven, E., & Tomasello, M. (2003).
                                                                     Press.
  A construction based analysis of child directed speech.
                                                                   Redington, M., Chater, N., & Finch, S. (1998).
  Cognitive Science, 27(6), 843-873.
                                                                     Distributional information: A powerful cue for acquiring
Chang, F. (2002). Symbolically speaking: A connectionist
                                                                     syntactic categories. Cognitive Science, 22(4), 425-469.
  model of sentence production. Cognitive Science, 26(5),
                                                                   Theakston, A. L., Lieven, E. V. M., Pine, J. M., & Rowland,
  609-651.
                                                                     C. F. (2001). The role of performance limitations in the
Chang, F., Bock, K., & Goldberg, A. E. (2003). Can
                                                                     acquisition of verb-argument structure: An alternative
  thematic roles leave traces of their places? Cognition,
                                                                     account. Journal of Child Language, 28(1), 127-152.
  90(1), 29-49.
                                                                   Tomasello, M. (1992). First verbs: A case study of early
Chomsky, N. (1957). Syntactic structures. The Hague:
                                                                     grammatical development. Cambridge: Cambridge
  Mouton.
                                                                     University Press.
Dell, G. S., Burger, L. K., & Svec, W. R. (1997). Language
                                                                   Tomasello, M. (2003). Constructing a language: A usage-
  production and serial order: A functional analysis and a
                                                                     based theory of language acquisition. Cambridge, MA:
  model. Psychological Review, 104(1), 123-147.
                                                                     Harvard University Press.
Elman, J. L. (1990). Finding structure in time. Cognitive
  Science, 14(2), 179-211.
Gleitman, L. (1990). The structural sources of verb
  meanings. Language Acquisition: A Journal of
  Developmental Linguistics, 1(1), 3-55.
                                                               423

