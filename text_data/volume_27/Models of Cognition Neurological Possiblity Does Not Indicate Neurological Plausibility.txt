UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Models of Cognition: Neurological Possiblity Does Not Indicate Neurological Plausibility
Permalink
https://escholarship.org/uc/item/26t9426g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Kriete, Trenton E.
Noelle, David C.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

                                                Models of Cognition:
      Neurological possibility does not indicate neurological plausibility.
                                     Peter R. Krebs (peterk@cse.unsw.edu.au)
                                                   Cognitive Science Program
                                           School of History & Philosophy of Science
                                               The University of New South Wales
                                                  Sydney, NSW 2052, Australia
                            Abstract                              level cognitive functions like the detection of syntactic
                                                                  and semantic features for words (Elman, 1990, 1993),
   Many activities in Cognitive Science involve complex           learning the past tense of English verbs (Rumelhart and
   computer models and simulations of both theoretical
   and real entities. Artificial Intelligence and the study       McClelland, 1996), or cognitive development (McLeod
   of artificial neural nets in particular, are seen as ma-       et al., 1998; Schultz, 2003). SRNs have even been
   jor contributors in the quest for understanding the hu-        suggested as a suitable platform “toward a cognitive
   man mind. Computational models serve as objects of             neurobiology of the moral virtues” (Churchland, 1998).
   experimentation, and results from these virtual experi-        While some of the models go back a decade or more,
   ments are tacitly included in the framework of empiri-
   cal science. Cognitive functions, like learning to speak,      there is still great interest in some of these ‘classics’,
   or discovering syntactical structures in language, have        and similar models are still being developed, e.g. Rogers
   been modeled and these models are the basis for many           and McClelland (2004). I argue that many models in
   claims about human cognitive capacities. Artificial neu-       this class explain little at the neurological level about
   ral nets (ANNs) have had some successes in the field of
   Artificial Intelligence, but the results from experiments      the theories they are designed to support, however I do
   with simple ANNs may have little value in explaining           not intend to offer a critique of connectionism following
   cognitive functions. The problem seems to be in re-            Fodor and Pylyshyn (1988).
   lating cognitive concepts that belong in the ‘top-down’
   approach to models grounded in the ‘bottom-up’ con-
   nectionist methodology. Merging the two fundamentally
   different paradigms within a single model can obfuscate
   what is really modeled. When the tools (simple artifi-
   cial neural networks) to solve the problems (explaining                                                  Input nodes
   aspects of higher cognitive functions) are mismatched,
   models with little value in terms of explaining functions
   of the human mind are produced. The ability to learn
   functions from data-points makes ANNs very attractive
   analytical tools. These tools can be developed into valu-
   able models, if the data is adequate and a meaningful                                             Hidden nodes
   interpretation of the data is possible. The problem is,
   that with appropriate data and labels that fit the desired
   level of description, almost any function can be modeled.
   It is my argument that small networks offer a univer-
                                                                                                    Output nodes
   sal framework for modeling any conceivable cognitive
   theory, so that neurological possibility can be demon-
   strated easily with relatively simple models. However, a               Figure 1. Feed forward network architecture
   model demonstrating the possibility of implementation
   of a cognitive function using a distributed methodol-
   ogy, does not necessarily add support to any claims or         Instead, this paper concerns models where ANNs act
   assumptions that the cognitive function in question, is        merely as mathematical, or analytical, tools. The fact
   neurologically plausible.
                                                                  that mathematical functions can be extracted from a
                                                                  given set of data, and that these functions can be success-
                        Introduction                              fully approximated by an ANN (neurological possibility),
Several classes of computational model and simulation             does not provide any evidence that these functions are
(CMS) used in Cognitive Science share common ap-                  capable of being realized in similar fashion inside human
proaches and methods. One of these classes involves               brains (neurological plausibility).
artificial neural nets (ANNs) with small numbers of
nodes, particularly feed forward networks (Fig. 1) and                         Bridging the Paradigms
simple recurrent networks (SRNs)1 (Fig. 2). Both of               Theories in Cognitive Science fall generally into two dis-
these architectures have been employed to model high              tinct categories. Some theories are offered as explana-
    1
                                                                  tions of aspects of human cognition in terms of what
      SRNs have a set of nodes that feed some or all of the
previous states of the hidden nodes back. The nodes are           ‘short term memory’ that becomes part of the input in the
often described as context nodes. They provide a kind of          next step of the simulation.
                                                              1184

brains do, and the implementation at neural level is usu-         that purpose although there are many technical and con-
ally of little concern. Arguably, these theories are all          ceptual issues unresolved4.
about psychological phenomena and the physical brain              Other attempts to bridge the divide between the two
should not even be considered in the context of this ap-          paradigms involve computational models which aim to
proach. Bennett and Hacker (2003), for example, believe           explain how higher cognitive functions could possibly
that                                                              be supported by a distributed architecture. In order to
                                                                  achieve this, descriptive elements from different levels
   [. . . ] it makes no sense to ascribe such psychological       are brought together in an attempt to present unified
   functions [i.e. perceiving and thinking] to anything           and coherent CMSs of cognitive processes. In the case
   less than the animal as a whole. It is the animal that         of models that are based on simple feed forward ANNs
   perceives, not parts of its brain, and it is human             and SRNs, theoretical and conceptual elements are sub-
   beings who think and reason, not their brains. The             jected to a set of neurologically inspired mathematical
   brain and its activities make it possible for us - not         tools. An important contribution to the apparent success
   for it - to perceive and think, to feel emotions, and          of these models is that the analysis and interpretation of
   to form and pursue projects (Bennett and Hacker,               experimental results can be framed in the language of
   2003, 3).                                                      the theoretical and conceptual entities concerning the
                                                                  cognitive function. Building models using ANNs is not
Essentially, the top-down 2 approach deals with high              a difficult task, particularly if the ANN is small, because
level cognitive functions, and the brain, or entire being,        many of the technical and methodological details need
is viewed as a single black box, or a collection of black         not to be dealt with5 .
boxes with certain functional properties.
                                                                                  A Universal Framework
                                                                  Artificial neural networks are trained using algorithms
                                             Input nodes
                                                                  that adjust the weights between units, i.e. model neu-
                                                                  rons, so that the error between the ANN’s computed
                                                                   output and the expected output is minimized for the
                                                                  given input. This process is repeated for all possible
                                                                   input-output pairs many times over. For example, to
                                                                   implement the XOR-function
                                                                                    On = ( I1 ∧ ¬I2 ) ∨ ( ¬I1 ∧ I2 )
                                         Context nodes
                                                                   the network will be presented with values for I1 and I2 ,
                                                                   i.e. ‘0, 0’, ‘0, 1’, ‘1, 0’ and ‘1, 1’. The weights are adjusted
                                Output nodes
                                                                   using an appropriate algorithm to minimize the error be-
                                                                   tween the network’s output and the output of the train-
                   Figure 2. SRN architecture                      ing set, i.e. ‘0’, ‘1’, ‘1’, and ‘0’ respectively. Once the
                                                                   network is trained, it will compute the output On from
                                                                   the inputs I1 and I2 according to the XOR-function. In
The bottom-up approach, in contrast, deals with the base
                                                                   many discussions about ANNs in the context of cogni-
elements, namely neurons, and their physiological and
                                                                   tive modeling, the inputs are labeled with terms other
functional properties and processes. Functional aspects
                                                                   than ‘0’s or ‘1’s. Because we can use these labels freely,
of brains, or parts of brains, are investigated by look-
                                                                   there is always the danger of introducing ‘wishful’ ter-
ing at individual neurons and structures of groups and
                                                                   minology not only for labels, but also for methodological
networks of neurons. Cognitive Neuroscience and some
                                                                   terms. But, as Fodor and Pylyshyn (1988) have pointed
work in Artificial Intelligence is concerned with how cog-
                                                                   out,
nitive functions might be implemented in brains.
Currently, methods are explored to connect the top-down
                                                                      [. . . ] the labels play no role at all in determining
and the bottom-up approaches in attempts to ground
                                                                      the operation of a Connectionist machine; in par-
high-level psychological phenomena in neuro-physiology.
                                                                      ticular, the operation of the machine is unaffected
One such research program concerns the mapping or lo-
                                                                      by the syntactic and semantic relations that hold
calizing of cognitive functions in the brain. Modern tech-
                                                                      among the expressions that are used as labels. To
nologies such as PET and fMRI3 are commonly used for
    2
      I am using the terms top-down and bottom-up in favor of     emitted from decaying atoms of a radioactive tracer (typically
high-level and low-level to suggest that these are not static     H2 O15 ), while fMRI detects different levels of oxygenated and
research programs, but that they are dynamic endeavors aim-        deoxygenated hemoglobin.
                                                                       4
ing to close the divide between them.                                    See for example Uttal (2001) for a discussion of issues
    3
      Positron Emission Tomography (PET) and functional            surrounding current methods in mapping cognitive functions
Magnetic Resonance Imaging (fMRI) are based on the as-            onto the brain.
                                                                       5
sumption that mental activity causes an increase in the                  Many computer programs that implement various ANNs
metabolic rate of neurons, and therefore an increase in the       are freely available, and little technical expertise is required
flow of blood. PET detects the locations where positrons are      to use them.
                                                              1185

   put this another way, the node labels in a Connec-           anything we want to model, as long as we have the ap-
   tionist machine are not part of the causal structure         propriate training set for the particular selection of la-
   of the machine (Fodor and Pylyshyn, 1988, 13).               bels for the inputs and outputs of the ANN. As a result
                                                                we will have to accept that even simple ANNs provide
Only the activation levels of the input nodes and the            a universal, but uninformative, framework for cognitive
connection strengths in the network matter for an ANN            models.
to produce the appropriate output for the function it            There is a further methodological issue to consider. It
is trained to approximate. Nevertheless, labels for              may be surprising to learn that neural nets in some mod-
the nodes and terminology for other parts of the net-           els are not necessarily composed of neurons. Elman et al.
works are introduced whenever models are constructed.           (1998) offer as a “note of caution” that
Schultz (2003), for example, maps terms from neural nets
onto terms from developmental psychology (here, Piage-              . . . [m]ost modelers who study higher-level cognitive
tian theory).                                                       processes tend to view the nodes in their models as
                                                                    equivalent not to single neurons but to larger popu-
   Accommodation, in turn, can be mapped to                         lations of cells. The nodes in these models are func-
   connection-weight adjustment, as it occurs, in the               tional units rather than anatomical units (Elman
   output phase of cascade-correlation learning. [. . . ]           et al., 1998, 91).
   More substantial qualitative changes, corresponding
   to reflective abstraction, occur as new hidden units          Can models still be considered as ‘bottom-up’ neural
   are recruited into the network. [. . . ] Then the net-        nets, if they are composed of functional units? I sug-
   work reverts back to an output phase in which it              gest that such models do not belong in the realm of
   tries to incorporate the newly achieved representa-           connectionism, because the replacement of model neu-
   tions of the recruited unit into a better overall solu-       rons with “functional units” re-introduces exactly those
   tion. This, of course, could correspond to Piaget’s           black boxes that we trying to eliminate in the ‘bottom-
   notion of reflection (Schultz, 2003, 128, original ital-      up’ approach.
   ics).                                                         The kinds of models that I am describing here, i.e. sim-
                                                                 ple feed forward ANNs and small SRNs, do not rely
The terminology from Piagetian theory clearly belongs            on special neural ‘circuitry’, unlike structured models in
to a higher level of description than the true descriptions      which the models’ architectures reflect a particular part
of the network’s structure and dynamics.                         of brain physiology. The architectures are universal in
Churchland (1998) suggests that a recurrent network              the sense that only the number of neurons and connec-
could model more challenging cognitive functions. He             tions vary from model to model. The diversity of models
considers that a recurrent network may have appropri-            that have been described in the literature is the product
ate architecture for simulating the acquisition of moral         of applying ANNs as analytical tools to a diverse set of
virtues in humans. He argues that a network would be             problems where suitable data sets for training of the net-
able to map concepts like cheating, tormenting, lying, or        works are available. The universal architecture and the
self-sacrifice within a n-space of classes containing di-        freedom to choose labels and terminology fitting the par-
mensions of morally significant, morally bad, or morally         ticular model explains the proliferation of ANN inspired
praiseworthy actions, by learning through “repeated ex-          models. Traditional mathematical (symbol based) mod-
posure to, or practice of, various examples of perceptual        els may be more constrained as far as the selection of
or motor categories at issue” (Churchland, 1998, 83).            representations is concerned7 . How then are explanatory
Churchland says that                                             links maintained between representations in distributed
                                                                 models and real world phenomena?
   [t]his high-dimensional similarity space [...] displays
   a structured family of categorical “hot spots” or                        Symbols and Representations
   “prototype position”, to which actual sensory in-             Classic CMS are representational systems using sym-
   puts are assimilated with varying degree of closeness         bols, which carry arbitrarily assigned semantic content.
   (Churchland, 1998, 83).                                       Haugeland (1985, 1981) and others have argued that
                                                                 these semantics remain meaningful during processing, as
It is beyond the scope of this paper to discuss whether          long as the syntactical structure is appropriate and suit-
a model of such a calculus of moral virtues is appropri-         ably maintained. Haugeland notes that in an interpreted
ate, but Churchland certainly demonstrates that it can           formal system with true axioms and truth-preserving
at least in principle be modeled with an ANN. However,           rules, the semantics will take care of itself, if you take
feed forward networks and SRNs with suitable numbers             care of the syntax (Haugeland, 1981). The symbol ‘5’,
of inputs and outputs and a reasonable number of hidden          for example, carries different semantics in a positional
units6 can be trained to implement almost any function.          number system. Whether ‘5’ means ‘500’ in ‘1526’, or
The point here is that a network can implement almost            ‘50’ in ‘1257’ is a function that is governed by the syn-
                                                                 tactic and semantic rules of the number system. Tying
    6
      Note that the number of hidden nodes and the number
                                                                     7
of connections within the network are largely determined by            Dretske (1981, 1988), among others, has dealt with ques-
experience and experiment. There are no definite methods        tions of representations and their semantics in representa-
or algorithms for this.                                         tional systems.
                                                            1186

semantics to symbols is much more problematic in con-           are adjusted until the system’s behavior comes to model
nectionist models, because it is part of the connectionist      the statistical properties of its inputs” (my italics).
doctrine that representations (symbols) are distributed         Elman (1990), for example, presented 29 words in the
in the structures of neural nets. It is therefore more dif-     human language one at a time to a simple recurrent net-
ficult to produce a trace of what happens semantically           work in the form of binary vectors I1 . . . In , such that
in an ANN, because no syntactical structure exists.             a single bit represented a particular word. The words
ANNs are usually described as having distinct and dis-          themselves were presented in sequences forming two and
crete inputs and outputs8 , each labeled as having a dis-        three word sentences that had been generated accord-
tinct and discrete meaning. Such labels may be words,            ing to a set of 15 fixed templates. A cluster analysis
like boy, girl, read, book, or, the labels may be concepts       of the hidden nodes revealed that the trained network
such as phonemes, or visual inputs. Such labels have             exhibits similar activation patterns for inputs (words)
their own set of problems associated with them. Attach-          according to their relative position in the sequence (sen-
ing the value ‘grandmother ’ to one of the input nodes           tence) and their probability of occurring in relation to
illustrates my concern. While nearly everyone rejects            other words. The analysis of these activation patterns
the existence of a grandmother-neuron in the brain as            allowed for the classification of inputs into categories like
a rather naı̈ve concept, boy-, girl-, or book- neurons are      nouns or verbs. Moreover, the categories of internal rep-
willingly accepted in models.                                    resentations could be broken down into smaller groups
Localized representations are no longer available once           like human, non-human, large animals, or edibles, and so
the focus shifts on to hidden nodes within the network,          on.
and the ‘representations’ are now described in terms of          Cluster analysis is used as a method to gain insights into
weights, or synaptic strengths, between individual units.       the internal representations of ANNs, but is not with-
However, for a meaningful interpretation of the network          out some conceptual problems. Clark (2001) argues that
and its dynamics, it is necessary to convey content and          cluster analysis is an analytic technique to provide an-
meaning in terms of non-distributed (localized) symbols,         swers to the crucial question of what kinds of representa-
because is not sufficient for a discussion of what goes on       tions the network has acquired. However, cluster analy-
in ANNs to assign semantic content merely to inputs              sis does not reveal anything that is not already contained
and outputs. In order to track the flow of information           in the raw data of the model. The relationships and pat-
through the networks, some descriptions are needed, be-          terns in the input datasets and training datasets become
cause explaining the processes in the ANN in terms of            embedded in the structure of the network during train-
connection-weights between neurons is tedious and un-            ing10 . What counts are the mathematical and statistical
suitable for the kinds of models in question. Discussing         relations that are contained in the training datasets. In
representations in terms of connection weights is tedious,       many cases the relations may just be tacitly accepted. In
because the number of connections can be considerable,           other models these relations are purposefully introduced
even in small networks9. A distributed representation R,         from the outset. Under these conditions, the relations
i.e. the activation pattern for a particular input I1...k ,      are part of the model’s design. Elman (1990), for ex-
could be specified in the form of a matrix, or as a vector,      ample, states that “13 classes of nouns and verbs were
with as many elements as there are connections in the            chosen” for generating the datasets. Whether the rela-
network.                                                         tions in the data are introduced by design, or whether
         R(I1...k ) = (.8234, .9872, .1290, . . ., .0012).       the experimenter is unaware of these statistical artifacts,
                                                                 there should be no surprise that the analysis will reveal
In any case, it it necessary to specify all of the numeric       these relations later during the experiment. The imple-
values to capture every single activation pattern. Repre-        mentation of a model as an ANN and the subsequent ex-
sentations and descriptions in this form are unsuitable,         traction of results that are already in the data may have
because they reveal little in terms of the cognitive func-       little value in terms of obtaining empirical evidence. The
tion that is modeled. Where do new and helpful descrip-         training set of pairs of input and output vectors already
tions come from?                                                 contains all there is to the model, and the ANN does
                                                                 not add anything that could not be extracted from the
                  Interpreting models                           training sets through other mathematical or computa-
The representations for words, concepts, phonemes, vi-          tional methods.
sual inputs, and so on, are usually coded in binary, or         Green (2001) argues that
as real values, in paired input and output vectors in the
training set for the ANN. During the training the rela-
tionships between the input and output vectors are en-              these results are just as analytic as are the results
coded in the hidden layers of the ANN, or as Fodor and              of a mathematical derivation; indeed they are just
Pylyshyn (1988) put it, ”the weights among connections              mathematical derivation. It is logically not possible
                                                                    that [the results] could have turned out other than
    8
      Inputs and outputs of ANNs can also have continuous           they did (Green, 2001, 109).
values. The kinds of models I am discussing here have typi-
cally discrete values.
    9                                                               10
      A fully connected feed forward network with 20 input             The patterns and relationships in these datasets can ei-
nodes, 10 hidden nodes, and 5 output nodes has 250 connec-      ther be carefully designed or might be an unwanted by-
tions.                                                          product.
                                                            1187

A trained ANN implements a mapping from the input                 output following the current input11 . The analysis of
nodes (I1−n ) to the output nodes (O1−i ). The power of           the experiment is framed in the language of the higher
the ANN is in its ability to implement some function              cognitive function that is the subject of the model.
                      O1−i = f (I1−n )                            For the interpretation and the analysis of the results,
                                                                  the output nodes are neglected and new ‘output’ for
from the training data set. Hoffmann (1998) emphasizes            the model is generated by methods that belong to a
this point and says that                                          higher level of description than the ANN. New ‘insights’
   [t]he greatest interest in neural nets, from a prac-           are synthesized from distributed representations by
   tical point of view, can be found in engineering,              means and methods external to the ANN. Figure 3
   where high-dimensional continuous functions need               illustrates the disconnectedness of the ANN and the
   to be computed and approximated on the basis of a              newly gained ‘insights’ emerging from the network’s
   number of data points (Hoffmann, 1998, 157).                   internal representations. The experimenter performs
                                                                  the task of extracting information about the activation
The modeler does not need to specify the function                 pattern using a new tool, cluster analysis for example,
f , in fact, the modeler does not even need to know               however the network has no part in this - ANNs do not
anything about f . Knowledge extraction (KE) from                 perform cluster analysis. The work is clearly performed
ANNs is concerned with providing a description of the             by the modeler’s neurons with the aid of a statistical
function f that is approximated by the trained ANN.               procedure and not by the model’s neural structure.
The extraction of the function “lies in the desire to have
explanatory capabilities besides the pure performance”
(Hoffmann, 1998, 155). The ability to determine f may
or may not add to the explanatory value of the model.
For moderately sized networks and relatively simple                                                         Input nodes
functions it is quite feasible to describe the model in a
series of simple logic statements or with some high level
programming language. In this step by step description
of the network in terms of its input-output relations,
knowledge of the function that will be ultimately
implemented is not necessary. A particular relationship
could be expressed, albeit awkwardly, in the form
if I1 = 0 and I2 = 1 then      On = 1
else if I1 = 1 and I2 = 0      then On = 1                                             ?         ?
else if I1 = 1 and I2 = 1      then On = 0
else if I1 = 0 and I2 = 0      then On = 0                                     Figure 3. Actual model architecture
The simple XOR-function is easily recognized in                  If the ANN is meant to be a model of what might happen
this example. This approach may not be practical for             at the neural level, then the question arises, what mech-
more complicated functions, however it would be possi-           anism could be responsible for the equivalent (cluster)
ble in principle. ANNs can even offer a convenient way           analysis of activation patterns in the brain? In order to
of implementing complicated functions approximately,             make this information accessible to the rest of the brain,
if some data points of these functions are known. The            we will have to introduce some other neural circuit to do
number of data points that are available for the training        such an analysis of the hidden nodes. Such a new addi-
of the network determine how close the approximation              tion to the network could possibly categorize words into
of the functions can be, unless the function is known            verbs and nouns, but then we need another circuit to
to be linear. The ability to process even relatively             categorize words into humans, non-humans, inanimates,
large data sets make ANNs valuable analytical tools to            or edibles, and another to categorize words into mono-
reveal something about the data. Even employing KE                syllabic and multi-syllabic. In fact, we will need an very
methods that may help to determine the function f                large number of neural circuits just for the analysis of
does not overcome the limitation that the ANN cannot              word categories, provided the training dataset contains
deliver anything new for the cognitive model. Regres-             the appropriate relations to allow for such categoriza-
sion analysis (curve fitting) performed on the training           tions.
dataset will provide a more exact description of f than           The class of simple ANNs that I have discussed here can-
to teach an ANN and to perform KE subsequently.                   not provide any new ‘insights’ in any meaningful sym-
There is a further complication as the data revealed in           bolic12 or coded form on some output nodes. This, how-
the cluster analysis is not accessible within the model.          ever, would have to be a crucial function of the model
In other words, the results of the analysis are not fur-             11
nished by the ANN. Rather, they are interpretations of                  The desired output, which follows the input in the train-
                                                                 ing set, is used as the target to determine the error for back
the internal structures at a different level of description.     propagation during the training phase.
The actual role of the network is that of a predictor,               12
                                                                        I do not think that ‘distributed’ or ‘sub-symbolic’ rep-
where the trained network attempts to guess the next             resentations are helpful here. Moreover, this alternative ap-
                                                             1188

to be considered neurologically plausible. For a model            Branquinho, J. (2001). The foundations of cognitive sci-
to be neurologically plausible, it would need to deduce             ence. New York: Oxford UP.
new information about itself. More importantly, it would          Churchland, P. M. (1998). Toward a Cognitive Neurobi-
be necessary to signal the newly obtained knowledge to              ology of the Moral Virtues. In Branquinho (2001).
other neurons by changing the state of some nodes. Both           Clark, A. (2001). Mindware: An Introduction to the
cluster analysis and current methods of KE clearly fail             Philosophy of Cognitive Science. New York: Oxford
to do this, although more recent developments in KE                 UP.
can deliver much more accurate description of f . How-
                                                                  Cummins, R. and Delarosa Cummins, D. (2000). Minds,
ever the renewed and possibly accurate synthesis of re-
                                                                    Brains, and Computers: The Foundations of Cognitive
lations that were present in a training dataset does not
warrant claims that the ANN ‘discovered’, ‘learned’, or             Science. Malden, Mass.: Blackwell.
‘recognized’ something or other, even if these relations          Dretske, F. (1981). Knowlegde & the Flow of Informa-
were not evident to the experimenter before. The abil-              tion. Cambridge, Mass.: MIT Press.
ity to determine a function f that is contained in some           Dretske, F. (1988). Representational Systems. In
dataset illustrates the power of ANNs as analytical tools.          O’Connor and Robb (2003).
However, it should be clear that a different analytical           Elman, J. (1993). Learning and development in neural
tool could also have been used to detect the function               networks: The importance of starting small. Cogni-
f . We must conclude then that the model has failed to              tion, 48:71–99.
explain any processes at the neural level. Instead, the           Elman, J. L. (1990). Finding structure in time. Cognitive
network model has only succeeded in offering an alterna-            Science, 14:179–211.
tive method to encode the data, and the cluster analysis          Elman, J. L., Bates, E. A., Karmiloff-Smith, A., Parisi,
provides an alternative method to analyze the data.                 D., and Plunkett, K. (1998). Rethinking Innateness:
                                                                    A Connectionist Perspective on Development. Cam-
                      Conclusions                                   bridge, Mass.: MIT Press.
Computational models and simulations, and models us-              Fodor, J. and Pylyshyn, Z. (1988). Connectionism and
ing ANNs in particular, are commonly used in support of             cogintive architecture: A critical analysis. Cognition,
theories about aspects of human cognition. Some mod-                28:3–71.
els deal with high level psychological functions where            Green, C. D. (2001). Scientific models, connectionist
the operations at the neural level are of little interest,          networks, and cognitive science. Theory & Psychology,
and some models are concerned with the implementa-                  11(1):97–117.
tion of cognitive functions at neural level. I have ar-
gued that neurological possibility can be demonstrated            Haugeland, J. (1981). Semantic Engines: An Introduc-
for nearly any conceivable psychological theory due to              tion to Mind Design. In Cummins and Delarosa Cum-
the universality of simple ANNs. However using the lan-             mins (2000).
guage and symbolism of neural nets does not support              Haugeland, J. (1985). Artificial Intelligence: the Very
any claims for neurological plausibility. The mistake,              Idea. Cambridge, Mass.: MIT Press.
I believe, is to bring the top-down psychological model           Hoffmann, A. (1998). Paradigms of Artificial Intelli-
and the bottom-up neural environment together and to                gence. Singapore: Springer.
treat the result as a coherent and meaningful demonstra-          McLeod, P., Plunkett, K., and Rolls, E. T. (1998). Intro-
tion. ANNs can be used successfully as models, provided             duction to Connectionist Modelling of Cognitive Pro-
a clear description of the aims, assumptions and claims             cesses. Oxford: Oxford UP.
are presented. However, when simple ANNs with small               O’Connor, T. and Robb, D. (2003). Philosophy of Mind:
numbers of nodes are employed to model complex high                 Contemporary Readings. London: Routledge.
level cognitive functions, the experimenter should eval-
                                                                  Rogers, T. T. and McClelland, J. L. (2004). Seman-
uate whether the simplicity of the network can provide
                                                                    tic Cognition: A Parallel Distributed Processing Ap-
a plausible implementation, because it is all too easy to
                                                                    proach. Cambridge, Mass.: MIT Press.
provide a neurologically possible model.
                                                                  Rumelhart, D. E. and McClelland, J. L. (1996). On
                 Acknowledgments                                    Learning the Past Tense of English Verbs. In Cummins
                                                                    and Delarosa Cummins (2000).
I would like to thank Anthony Corones for comments               Schultz, T. R. (2003). Computational Developmental
and valued suggestions on earlier drafts of this paper.             Psychology. Cambridge, Mass.: MIT Press.
                                                                 Uttal, W. R. (2001). The New Phrenology: The Limits of
                                                                    Localizing Processes in the Brain. Cambridge, Mass.:
                       References                                   MIT Press.
Bennett, M. R. and Hacker, P. M. S. (2003). Philo-
   sophical Foundations of Neuroscience. Malden, Mass.:
   Blackwell.
proach of dealing with network structures is usually not em-
ployed by modelers either.
                                                             1189

