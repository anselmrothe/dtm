UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Simulating the Cross-Linguistic Development of Optional Infinitive Errors in MOSAIC
Permalink
https://escholarship.org/uc/item/6md6q9j2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Freudenthal, Daniel
Gobert, Fernand
Pine, Julian
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                     University of California

                Simulating the Cross-Linguistic Development of Optional Infinitive
                                                         Errors in MOSAIC
                                           Daniel Freudenthal (D.Freudenthal@liv.ac.uk)
                                                  Julian Pine (Julian.Pine@liv.ac.uk)
                                               School of Psychology, University of Liverpool
                                                            L69 7ZA Liverpool, UK
                                           Fernand Gobet (Fernand.Gobet@Brunel.ac.uk)
                                            School of Social Sciences and Law, Brunel University
                                                     Uxbridge, Middlesex, UB8 3PH, UK
                               Abstract                                  Bloom, 1990; Pinker, 1984; Valian, 1991). More recently,
                                                                         however, it has been argued that they reflect the child’s
  The Optional Infinitive (OI) phenomenon in children’s speech           optional use of (root) infinitives (e.g. go) in contexts where a
  has attracted a great deal of attention due to its occurrence in a     finite form (e.g. went, goes) is obligatory in the adult
  variety of languages (including English, Dutch and German),
                                                                         language (Wexler, 1994, 1998).
  and its apparent absence in other languages (such as Spanish
  and Italian). Wexler (1998) explains this pattern of results in
                                                                            This interpretation reflects the fact that children produce
  terms of a Unique Checking Constraint that interacts with              (root) infinitives not only in English, where the infinitive is a
  cross-linguistic differences in the underlying grammar to result       zero-marked form, but also in languages such as Dutch
  in Optional Infinitive errors in obligatory subject languages          (Wijnen et al. 2001) and German, where the infinitive carries
  (which require double-checking), but not in pro-drop languages         its own infinitival marker (-en). For instance, children
  (which do not require double-checking). While Wexler’s                 learning Dutch may produce utterances such as:
  account explains the cross-linguistic data, it attributes a great
  deal of innate linguistic knowledge to the child, and ignores the         (3a) Pappa eten*           (Daddy (to) eat)
  possibility that the cross-linguistic data may be equally well
                                                                            (4a) Mamma drinken*       (Mummy (to) drink)
  explained by the interaction between a simple distributional
  learning mechanism and the surface characteristics of the
  language. This paper presents simulations of the Optional              Instead of
  Infinitive phenomenon across 4 languages (English, Dutch,
  German, and Spanish) using MOSAIC, a simple distributional                (3b) Pappa eet            (Daddy eats)
  analyser with no built-in syntactic knowledge. MOSAIC clearly             (4b) Mamma drinkt         (Mummy drinks)
  simulates the different rates of Optional Infinitive errors across
  the languages, suggesting (a) that it is possible to explain the         According to Wexler (1998), the Optional Infinitive
  basic OI phenomenon without assuming large amounts of                  phenomenon can be explained as follows. By the time
  innate linguistic knowledge, and (b) that cross-linguistic
                                                                         children begin to produce multi-word utterances, they have
  differences in the OI phenomenon may be related to differences
  in the surface characteristics of the languages being learned.
                                                                         already set all the basic inflectional and clause structure
                                                                         parameters of their language. However, their grammars are
                          Introduction                                   governed by a Unique Checking Constraint that is
                                                                         ‘genetically specified (and withering away in time)’ (Wexler,
Between two and three years of age, children learning English            1998: 27). The Unique Checking Constraint may prevent the
often produce utterances that appear to lack inflections, such           child from checking the D-feature of the subject DP against
as past tense markers or third person singular agreement                 more than one D-feature (in this case the D-features of Tense
markers. For example, children may produce utterances as:                and Agreement). As a result, Tense and Agreement can be
                                                                         optionally under-specified in the underlying representation of
  (1a) That go there*                                                    the sentence, and the child may produce non-finite verb forms
  (2a) He walk home*                                                     (forms that are not marked for tense or agreement) in contexts
                                                                         in which a finite verb form is required.
instead of                                                                 The main strength of Wexler’s account is that it can explain
                                                                         data from a range of different languages. Thus, it can explain
  (1b) That goes there                                                   why children produce Optional Infinitive errors at high rates
  (2b) He walks home                                                     in obligatory subject languages like English, Dutch and
                                                                         German, which require the child to check against two D-
Traditionally, such utterances have been interpreted in terms            features: Tense and Agreement. However, it can also explain
of absence of the appropriate knowledge of inflections                   why children make few Optional Infinitive errors in INFL-
(Brown, 1973) or the dropping of inflections as a result of              licensed null subject languages like Spanish and Italian,
performance limitations in production (L. Bloom, 1970; P.
                                                                     702

which (usually) only require the child to check against one D-        MOSAIC consists of a simple network of nodes that
feature: Tense.                                                     incrementally encode words and phrases that have been
  On the other hand, Wexler’s account also has certain              presented to the model. As the model sees more input it will
weaknesses. First, while the account makes qualitative              encode more and longer phrases and will consequently be
predictions about the occurrence or non-occurrence of OIs in        able to generate more and longer output. Figure 1 shows a
a number of different languages, it makes no (detailed)             sample MOSAIC network. Learning in MOSAIC is anchored
quantitative predictions about the rate at which children will      at the sentence-initial and sentence-final positions: MOSAIC
make OI errors or how these rates will change as the child’s        will only encode a new word or phrase when all the material
language develops. Wexler invokes the concept of maturation         that either follows or precedes it in the utterance has already
to explain the decline in OI errors, but the concept is             been encoded in the network. When presented with the
relatively unspecified, and does not give rise to quantitative      utterance He wants to go to the shops for instance, the model
predictions.                                                        may in first instance encode the words He and shops. At a
   Second, where the account makes qualitative predictions          later stage it may encode the phrases He wants and the shops,
(e.g. about the lack of Optional Infinitive errors in pro-drop      until the point where it has encoded the entire phrase He
languages), it does so with reference to deep structural            wants to go to the shops. When the model processes an
differences in the grammar of the languages, thereby ignoring       utterance, and a sentence-final and sentence-initial phrase for
the possibility that the interaction between an input-driven        that utterance have already been encoded in the network,
learning mechanism and the surface characteristics of the           MOSAIC associates the two nodes encoding these phrases, to
language may explain the data equally well. Freudenthal, Pine       indicate the two phrases have co-occurred in one (longer)
and Gobet (2002, submitted) have already shown that                 utterance. In Figure 1, the model has associated the phrases
MOSAIC, a simple distributional analyser that learns from           He wants and Go home.
child-directed speech and has no built-in syntactic knowledge
can provide a close quantitative fit to the basic Optional
Infinitive phenomenon in Dutch and English.
   This paper presents a new version of MOSAIC which
addresses some weaknesses of earlier versions and explains
OI errors in terms of the omission of auxiliaries or modals
from constructions containing a modal/auxiliary and an
infinitive. For example, the phrase that go there might be
produced by omitting should from that should go there, and
he go home might be produced by omitting wants to from he
wants to go home. Similarly, the Dutch phrase Pappa eten,              Figure 1: A partial MOSAIC model. The sentence-initial
might be produced by omitting wil from Pappa wil eten                phrase he wants, and the sentence-final phrase go home have
(Daddy wants to eat). MOSAIC will be applied to Optional             been associated, allowing the model to produce the utterance
Infinitive data from Dutch and English, as well as German                                  He wants go home.
and Spanish. MOSAIC’s ability to simulate the data across
these languages, which show rather different levels of              Learning in MOSAIC takes place by adding nodes that
Optional Infinitive errors, serves as a strong test of its          encode new words and phrases to the model. Learning is
mechanisms for producing Optional Infinitive errors, and the        relatively slow. The formula governing the probability of
feasibility of distributional approaches to language                creating of a node in MOSAIC is as follows:
acquisition in general.
                            MOSAIC                                                             1             d
A major change from earlier versions of MOSAIC is that the           NCP =                                   
model now learns from both edges of the utterance and                              1+ e 0.5((m*c )−u) 
associates sentence-initial and sentence-final phrases, leading
to the omission of sentence-internal elements. This change          where: ncp = node creation probability
brings MOSAIC more in line with general psychological                         m = a constant, set to 20 for these simulations.
theorizing (MOSAIC now shows a primacy as well as a                           c = corpus size (number of utterances)
recency effect). It also allows the model to simulate a wider                 u = (total number of) utterances seen.
range of phenomena than the previous version of MOSAIC,                       d = distance to the edge of the utterance
which only learnt from the right edge of the utterance. An
additional difference is that MOSAIC now distinguishes              The formula results in a basic sigmoid function, with the
between questions and declaratives, resolving the problem           probability of creating a node increasing as a function of the
that earlier versions of MOSAIC relied too heavily on               number of times the input has been presented. The input
questions as the source for Optional Infinitive errors              corpus (which consists of realistic child-directed speech) is
(Freudenthal, Pine, & Gobet, 2005a).                                fed through the model iteratively, and output can be generated
                                                                703

after every presentation of the input corpus. Making the node           Likewise, a sentence-final phrase can only be concatenated if
creation probability dependent on the number of times the               the first word in that phrase has occurred in sentence-initial
corpus has been seen allows for comparison across corpora of            position. Since the word the will not occur in sentence-final
differing sizes. The distance to the edge (or length of the             position, the phrase Give the a hand will not be generated.
utterance being encoded) features in the exponent in the                The rationale behind this restriction is that, to the extent that
formula, and lowers the likelihood of encoding long                     children concatenate phrases/omit sentence-internal elements,
utterances. As a result, MOSAIC will initially only learn               they will rarely break up syntactic units. Restricting
sentence-initial and sentence–final words. Only when the                concatenation to phrases where the internal edges are
base probability in the formula starts to increase (as a result of      anchored effectively achieves this, as an anchored word is
seeing more input), will longer phrases start being encoded.            unlikely to be a partial unit.
                                                                           The rote output of MOSAIC thus consists of a mixture of
Due to node-creation being probabilistic, a word or phrase
                                                                        sentence-final phrases and concatenations of sentence-initial
must normally be seen several times before it will be
                                                                        and sentence-final phrases. Both types of utterances are
encoded. Frequent words or phrases therefore have a higher              apparent in child language. An example of a phenomenon that
probability of being encoded than infrequent words or                   might be explained through omission of sentence-initial
phrases.                                                                elements is the omission of subjects from the sentence-initial
   MOSAIC maintains an utterance-final bias in that learning            position (Bloom, 1990). Due to MOSAIC’s learning
from the right edge of the utterance is faster than learning            mechanism and faster right-edge learning, MOSAIC’s output
from the left edge. This is accomplished by adding 2 to the             will initially contain a large proportion of sentence-final
length of a left edge phrase1 (the parameter d) that is                 fragments. As the Mean Length of Utterance (MLU) of the
considered for encoding (The parameter d designates distance            model increases, concatenations will become more frequent.
from the left edge of the utterance for left edge learning, and         The concatenations themselves will be slowly replaced by
distance to the right edge of the utterance for right edge              complete utterances.
learning). This learning mechanism results in a model that is              The two mechanisms described so far produce output that
biased towards learning sentence-initial words and a few                directly reflects the utterances present in the input (with the
(high-frequency) sentence initial phrases coupled with                  potential omission of sentence-initial or sentence-internal
comparatively long utterance-final phrases. As a result, the            material). These two mechanisms are complemented by a
sentence-internal elements that MOSAIC omits will tend to               third mechanism which is responsible for the generation of
be located near the left edge of the utterance.                         novel utterances through the substitution of distributionally
                                                                        similar words. When two words tend to be followed and
Generating output from MOSAIC                                           preceded by the same words in the input, they are considered
MOSAIC has two mechanisms for producing (rote) output.                  equivalent, and can be substituted for each other. Thus, the
The first mechanism is (almost2) identical to that in earlier           model is capable (in principle) of producing the utterance She
versions of MOSAIC. In generation, the model traverses the              run by omitting will from He will go, and substituting She for
network, and generates the contents of branches that encode             He, and run for go. A more in-depth discussion of
sentence-final phrases. (Sentence-initial fragments are not             MOSAIC’s mechanism for substituting distributionally
generated as these may end in the middle of the sentence, and           similar items is given in Freudenthal, Pine and Gobet
often do not resemble child speech).                                    (2005b), though the chunking mechanism described in that
   The second mechanism, which is new to this version of                paper has not yet been implemented in the present version of
MOSAIC, is the concatenation of sentence-initial and                    the model.
sentence-final phrases. When MOSAIC builds up the
network, it associates the sentence-initial and sentence-final
fragments from each utterance (c.f. He wants go home in                                        The Simulations
Figure 1). Since the concatenation of phrases could result in           All the simulations reported in this paper used the same
many implausible utterances, not all possible concatenations            version of MOSAIC together with corpora of realistic, child-
are produced. A source utterance like Give the man a hand,              directed speech. For English and Dutch, corpora available
for example, could potentially give rise to the concatenated            through the CHILDES database (MacWhinney, 2000) were
phrase Give the a hand. This utterance is awkward (and not              used. The English child (Becky) was part of the Manchester
typical of child speech) because it breaks up the unit the man.         corpus (Theakston, Lieven, Roland & Pine, 2001). The Dutch
MOSAIC prevents such concatenations by only                             child (Peter) was part of the Groningen corpus (Bol, 1995).
concatenating phrases that are anchored: a sentence-initial             Additional simulations for one Dutch and one additional
phrase can only be used for concatenation if the last word in           English child can be found in Freudenthal, Pine & Gobet,
that phrase has occurred in a sentence-final position.                  2005a). The Spanish simulations were conducted using the
                                                                        corpus of Juan (Aguado Orea, 2004). For German, the corpus
1
   The utterance-final bias applies to phrases, but not words.          of Leo (Behrens, in press) was used. For all simulations, the
Sentence-initial and sentence-final words are equally likely to be      same (automatic) coding scheme was used: utterances that
encoded.                                                                only contained verbs matching non-finite forms were classed
2                                                                       as non-finite. Utterances containing only finite forms were
  In line with the restriction discussed under concatenation, only
utterance-final phrases that start with a word that has occurred in     classed as simple finite. Utterances containing both finite and
utterance-initial position are produced.
                                                                    704

non-finite forms were classed as compound finites. The                                                        Fig. 2a: Data for Becky
analyses for English deviated slightly from the other analyses.
As English has an impoverished inflectional system, it is                                          1
necessary to restrict the analysis to utterances containing a 3rd
                                                                                              0.8                                       Non-finite
                                                                                Proportion
singular subject in order to identify Optional Infinitives. Also,
since many verb forms (e.g. walked) are ambiguous with                                        0.6                                       Simple Finite
respect to whether they are non-finite (past-participle) or                                   0.4                                       Comp. Finite
finite (past tense), utterances in which such forms were the                                  0.2                                       Ambiguous
only verb were classed as ambiguous. The children’s output
                                                                                                   0
was analysed at different stages of increasing MLU. The
Child-Directed speech for each child was then fed through the                                             2.4         3.1     3.5
model several times. Output from the model was generated                                                             MLU
after every presentation of the input. The output files that
most closely matched the child’s MLU were then selected.
                                                                                                       Fig. 2b: Simulations for Becky
For both the simulations and the children, the analysis was
performed on utterance types. The size of the input corpora
varied. For Becky, it consisted of approximately 25,000                                       1
utterances, Peter’s input consisted of approximately 13,000                                  0.8                                     Non-finite
                                                                           Proportion
utterances, and the size of Juan’s input was 34,000 utterances.                              0.6                                     Simple Finite
For Leo a random sample of 30,000 utterances was chosen
                                                                                             0.4                                     Comp. Finite
from the entire corpus, which consists of nearly 110,000
utterances.                                                                                  0.2                                     Ambiguous
                                                                                              0
English simulations                                                                                      2.1         2.8     3.7
Figure 2 gives the data and simulations for English. As can be
seen, the model provides a close fit to the data with respect to                                                 MLU
the rates at which Optional Infinitives are produced.
However, at the lowest MLU point, the proportion of simple                Fig. 2: Data and simulations for an English child.
finites that the model produces is too high. The model
generates Optional Infinitives because it is capable of                                                    Fig. 3a: Data for Peter
omitting modals and auxiliaries from phrases such as He
wants to go. As the model learns to produce longer                                            1
utterances, such omissions become less frequent and the
                                                                                             0.8
                                                                           Proportion
proportion of Root Infinitives decreases.                                                                                            Non-finite
                                                                                             0.6
                                                                                                                                     Simple Finite
Dutch simulations                                                                            0.4
Figure 3 displays the data and simulations for a Dutch child.                                                                        Comp. Finite
                                                                                             0.2
It is apparent that the Dutch child starts out with relatively
high levels of Optional Infinitives, which drop quite quickly.                                0
                                                                                                        1.5     2.2    3.1   4.1
MOSAIC simulates the high levels of OI errors as a result of                                                     MLU
its utterance-final bias. In Dutch, non-finite verb forms take
sentence-final position, while finite verbs take second
position. Early in development, the model will produce                                                 Fig. 3b: Simulations for Peter
mostly sentence-final phrases (and a few concatenations
including sentence-initial words). The sentence-final phrases                                 1
will contain many non-finite verb forms, the sentence-initial                                0.8
                                                                           Proportion
words will mostly consist of (pro)nouns, as subjects tend to                                                                         Non-finite
                                                                                             0.6
take first position in declaratives. As the model starts to                                                                          Simple Finite
produce longer utterances, finite verb forms start appearing,                                0.4
                                                                                                                                     Comp. Finite
leading to an increase in the proportion of simple and                                       0.2
compound finites.                                                                             0
                                                                                                        1.3      2     2.9   3.9
                                                                                                                 MLU
                                                                           Fig. 3: Data and simulations for a Dutch child.
                                                                    705

German Simulations                                                              Spanish. For all languages discussed so far, Optional
The results for German are shown in Figure 4. German                            Infinitives are generated by the omission of
grammar is identical to Dutch as far as the relation between                    modals/auxiliaries from compound finites. While these occur
verb placement and finiteness is concerned. Thus, in both                       at roughly equal rates (.31, .22, and .25 for Dutch, German
Dutch and German, finite verbs take second position, whereas                    and Spanish respectively3), the verb forms that occur in
non-finite verbs take utterance-final position. As with the                     sentence-final position (and are thus learned most easily)
Dutch data, MOSAIC simulates the patterning of the German                       differ across the languages. In Spanish, a large majority of
data quite well. When comparing the results for Dutch and                       these are finite (74%). For Dutch and German, only 18% and
German, it is apparent that the rates of OI errors in the early                 35% of the utterance-final verbs are finite.
German child data and simulations are quite a bit (16%)                            The fact that Spanish is a pro-drop language also
lower than they are for Dutch, and the decrease in levels of OI                 contributes to the low levels of Optional Infinitives: in
errors is not as pronounced as it is for Dutch. While this effect               situations where the subject is dropped, the utterance is likely
may reflect individual rather than cross-linguistic variation in                to start with a (finite) verb. Concatenations involving a
the children’s speech, it also raises the interesting possibility               subjectless verb are therefore likely to result in a finite
that although verb placement is subject to the same                             utterance.
grammatical rules in Dutch and German, there are
nevertheless subtle differences between the two languages                                                    Fig. 5a: Data for Juan.
that affect the relative frequency with which certain
constructions are used. If this is the case, it suggests a greater                                    1
role for input-driven learning than has so far been assumed by
                                                                                                     0.8
                                                                                        Proportion
Wexler.                                                                                                                                   Non-fin
                                                                                                     0.6
                                                                                                                                          Simple fin.
                                 Fig. 4a: Data for Leo.                                              0.4
                                                                                                                                          Com. Fin
                                                                                                     0.2
                      1                                                                               0
                     0.8                                                                                    2.18     2.85   3.78
        Proportion
                                                          Non-finite
                     0.6
                                                          Simple Finite                                             MLU
                     0.4
                                                          Comp. Finite
                     0.2                                                                                   Fig. 5b Simulations for Juan
                      0
                           1.3     2.2    3    4.6                                                    1
                                     MLU                                                             0.8
                                                                                        Proportion
                                                                                                                                          Non-fin
                                                                                                     0.6
                                                                                                                                          Simple fin.
                           Fig. 4b: Simulations for Leo.                                             0.4
                                                                                                                                          Com. Fin
                                                                                                     0.2
                      1                                                                               0
                     0.8                                                                                    2.1      2.99   3.5
        Proportion
                                                          Non-finite
                     0.6
                                                          Simple Finite                                             MLU
                     0.4
                                                          Comp. Finite
                     0.2                                                               Fig. 5: Data and simulations for a Spanish child.
                      0
                           1.5     2.1   3.2   4.1
                                                                                                                   Conclusions
                                     MLU                                        MOSAIC clearly simulates the basic Optional Infinitive
                                                                                phenomenon in four languages that differ considerably in
       Fig. 4: Data and simulations for a German child.                         terms of their underlying grammar and in the rates of
                                                                                Optional Infinitive errors that children in these languages
Spanish Simulations                                                             display. Since MOSAIC does not use any built-in linguistic
Figure 5 presents the data and simulations for Spanish. It is                   knowledge, and learns from child-directed speech that has a
apparent from Figure 5 that the Spanish child produces OIs at                   realistic frequency distribution, this result strongly suggests
                                                                                that cross-linguistic differences in the Optional Infinitive
a considerably lower rate than the children in the other
languages. Again, MOSAIC simulates the basic rate of
Optional Infinitives quite well. MOSAIC simulates this low                      3
                                                                                  The English input is ignored here as the inclusion of a subject is
rate of Optional Infinitives because of the structure of                        required in order to identify an Optional Infinitive.
                                                                          706

phenomenon are related to the surface characteristics of the          Bloom, P. (1990). Subjectless sentences in child language.
languages. Unlike Wexler’s account, which invokes the                    Linguistic Inquiry, 21, 491-504.
relatively unspecified concept of maturation, MOSAIC also             Bol, G.W. (1995). Implicational scaling in child language
provides a plausible explanation for the gradual decrease of            acquisition: the order of production of Dutch verb
Optional Infinitive errors. Optional Infinitive errors are              constructions. In M. Verrips & F. Wijnen, (Eds.), Papers
produced through the omission of modals and auxiliaries                 from the Dutch-German Colloquium on Language
from compound finites. Early in development, MOSAIC will                Acquisition, Amsterdam Series in Child Language
omit these elements at a high rate. As the model’s MLU                  Development, 3, Amsterdam: Institute for General
increases, omission rates decrease and Optional Infinitives are         Linguistics.
replaced by compound finites.                                         Brown, R. (1973). A first language. Boston, MA: Harvard
   An interesting finding is that despite there being no
                                                                        University Press.
difference between Dutch and German in verb placement and
                                                                      Freudenthal, D., Pine, J.M. & Gobet, F. (2002). Modelling the
its relation to finiteness, the analyses of the children and
simulations show a difference of 15-20% in the proportion of            development of Dutch Optional Infinitives in MOSAIC. In:
Optional Infinitives at the earliest stage. A similar asymmetry         W. Gray & C. Schunn (Eds.), Proceedings of the 24th
is apparent in the input files. Compound finites are less               Annual Meeting of the Cognitive Science Society pp. 322-
common in the German input (by 9%), and non-finite verbs                327 Mahwah, NJ: LEA.
are more common in sentence-final position in the Dutch               Freudenthal, D., Pine, J.M. & Gobet, F. (2005a). Simulating
input (by 17%). Whilst it is possible that this asymmetry               Optional Infinitive Errors in Child Speech through the
reflects individual differences in the children’s speech, this          Omission of Sentence-Internal Elements. This Volume.
finding also raises the possibility that subtle differences can       Freudenthal, D., Pine, J.M. & Gobet, F. (2005b). On the
exist between same family languages that affect the relative            resolution of ambiguities in the extraction of syntactic
frequency of certain constructions, and the subsequent rates            categories through chunking. Cognitive Systems Research,
of Optional Infinitive errors that children learning these              6, 17-25.
languages display. Thus, cross-linguistic differences in the          Freudenthal, D., Pine, J.M. & Gobet, F. (submitted).
rates at which children produce Optional Infinitives appear to          Modelling the development of Children’s use of Optional
be graded, quantitative differences which reflect the statistical       Infinitive in Dutch and English using MOSAIC. Submitted
properties of the input, and can be explained without recourse          to Cognitive Science.
to differences in the deep structural properties of the               MacWhinney, B. (2000). The CHILDES project: Tools for
language’s grammar.                                                     analyzing talk. Third Edition. Mahwah, NJ: Lawrence
                                                                        Erlbaum Associates.
                                                                      Pinker, S. (1984). Language learnability and language
                    Acknowledgements
                                                                        development. Cambridge, MA: Harvard University Press.
This research was funded by the ESRC under grant number               Theakston, A.L., Lieven, E.V.M., Pine, J.M. & Rowland,
RES000230211. We thank Javier Aguado-Orea for making                    C.F. (2001). The role of performance limitations in the
the Spanish corpus available and for aiding in the simulations          acquisition of Verb-Argument structure: an alternative
and analysis of Spanish. We thank the Max Planck Institute              account. Journal of Child Language, 28, 127-152.
for Evolutionary Anthropology in Leipzig for making the
                                                                      Valian, V. (1991). Syntactic subjects in the early speech of
German corpus available.
                                                                        American and Italian children. Cognition, 40, 21-81.
                                                                      Wexler, K. (1994). Optional infinitives, head movement and
                         References                                     the economy of derivation in child grammar. In N.
                                                                        Hornstein & D. Lightfoot (Eds.), Verb Movement.
Aguado Orea, J. J. (2004). The acquisition of morpho-syntax
                                                                        Cambridge: Cambridge University Press.
   in Spanish: Implications for current theories of
                                                                      Wexler, K. (1998). Very early parameter setting and the
   development. Unpublished doctoral thesis, University of
                                                                        unique checking constraint: a new explanation of the
   Nottingham.
                                                                        optional infinitive stage. Lingua, 106, 23-79.
Behrens, H. (in press). The input-output relationship in first
                                                                      Wijnen, F., Kempen, M. & Gillis, S. (2001). Root infinitives
   language acquisition. Language and Cognitive Processes.
                                                                        in Dutch early child language. Journal of Child Language,
Bloom, L. (1970). Language development: Form and function
                                                                        28, 629-660.
    in emerging grammars. Cambridge, MA: MIT Press.
                                                                  707

