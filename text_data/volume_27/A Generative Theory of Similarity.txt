UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Generative Theory of Similarity
Permalink
https://escholarship.org/uc/item/66b344s3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Bernstein, Aaron
Kemp, Charles
Tenenbaum, Joshua B.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                   A Generative Theory of Similarity
                         Charles Kemp, Aaron Bernstein & Joshua B. Tenenbaum
                                              {ckemp, aaronber, jbt}@mit.edu
                                          Department of Brain and Cognitive Sciences
                                             Massachusetts Institute of Technology
                           Abstract                                 a generative theory of similarity, a notion often invoked
                                                                    by models of high-level cognition. We argue that two
   We propose that similarity judgments are inferences
   about generative processes, and that two objects appear          objects are similar to the extent that they seem to have
   similar when they are likely to have been generated by           been generated by the same underlying process.
   the same process. We present a formal model based                    The literature on similarity covers settings that ex-
   on this idea, and suggest that it may be particularly            tend from the comparison of simple stimuli like tones and
   useful for explaining high-level judgments of similarity.
   We compare our model to featural and transformational            colored patches to the comparison of highly-structured
   accounts, and describe an experiment where it outper-            objects like narratives. The generative approach is rele-
   forms a transformational model.                                  vant to the entire spectrum of applications, but we are
     Keywords: similarity; generative processes;                    particularly interested in high-level similarity. In par-
                     computational theory                           ticular, we are interested in how similarity judgments
                                                                    draw on intuitive theories, or systems of rich conceptual
   Every object is the outcome of a generative process.             knowledge (Murphy and Medin, 1985). Intuitive theories
An animal grows from a fertilized egg into an adult, a              and generative processes are intimately linked: Murphy
city develops from a settlement into a metropolis, and              (1993), for example, defines a theory as “a set of causal
an artifact is assembled from a pile of raw materials ac-           relations that collectively generate or explain the phe-
cording to the plan of its designer. Observations like              nomena in a domain.” Our generative framework should
these motivate the generative approach, which proposes              therefore help to explain how similarity judgments are
that an object may be understood by thinking about the              guided by intuitive theories. Others have recognized the
process that generated it. The promise of the approach              importance of this issue: Murphy and Medin (1985) sug-
is that apparently complex objects may be produced by               gest, for example, that “the notion of similarity must be
simple processes, an insight that has proved productive             extended to include theoretical knowledge.”
across disciplines including biology (Thompson, 1961),                  We develop a formal theory of similarity and compare
physics (Wolfram, 2002), and architecture (Alexander,               it to two existing theories. The featural account (Tver-
1979). To give two celebrated examples from biology,                sky, 1977) suggests that the similarity of two objects is
the shape of a pinecone and the markings on a cheetah’s             a function of their common and distinctive features, and
tail can be generated by remarkably simple processes of             the transformation account suggests that similarity de-
growth. These patterns can be characterized much more               pends on the number of operations required to transform
compactly by describing their causal history than by at-            one object into the other (Hahn et al., 2003). We show
tempting to describe them directly.                                 that versions of both approaches emerge as special cases
   Leyton has argued that the generative approach pro-              of our model, and present an experiment that directly
vides a general framework for understanding cognition.              compares our model with the transformation account.
Applications of the approach can be found in generative
theories of memory (Leyton, 1992), categorization (An-                    Generative processes and similarity
derson, 1991; Feldman, 1997; Rehder, 2003), visual
perception (Leyton, 1992), speech perception (Liber-                Before introducing our formal model, we describe several
man et al., 1967), syntax (Chomsky, 1965)1 , and mu-                cases where the assessment of similarity relies on infer-
sic (Lehrdahl and Jackendoff, 1996). This paper offers              ences about generative processes. Suppose we are shown
    1
      The approach we have described should be distinguished         sume that structural descriptions have derivational histories.
from two usages of “generative” that are found in the linguis-       Generative grammars, however, are typically expressed us-
tics literature. Generativity sometimes refers to the infinite       ing formalisms that do assign derivational histories to struc-
use of finite means: for us, a generative process need not meet      tural descriptions, and theories that assume the psychological
this criterion, although many interesting processes will. The        reality of these histories are instances of the generative ap-
second (and more central) usage refers to a grammar’s abil-          proach. Chomsky (1995) has rejected theories of this sort:
ity to generate the set of grammatical sentences: Chomsky            “the ordering of operations [in grammatical theory] is ab-
(1965) defines a generative grammar as “a system of rules            stract, expressing postulated properties of the language fac-
that in some explicit and well-defined way assigns structural        ulty of the brain, with no temporal interpretation implied.”
descriptions to sentences.” A system of this sort need not           Others, however, argue for linguistic theories that are gener-
be generative in our sense — in particular, it need not as-          ative in our sense (Marantz, To appear)
                                                                1132

          i)            ii) Prototype       iii)                 they are deeply related to essentialism. Medin and
                                                                 Ortony (1989) note that “surface features are frequently
                                                                 constrained by, and sometimes generated by, the deeper,
                                                                 more central parts of objects.” Even if we observe only
                                                                 the surface features of two objects, it may make sense to
                                                                 judge their similarity by comparing the deeper proper-
                                                                 ties inferred to generate the surface features. Yet we can
Figure 1: Three bugs. Given that the prototype has               say more: just as surface features are generated by the
been observed, which is more likely to exist — i or iii?         essence of the object, the essence itself has a generative
                                                                 history. Surface features are often reliable guides to the
a prototype object and asked to predict what similar ob-         essence of an object, but the object’s causal history is
jects might exist in the world. There are two kinds of           a still more reliable indicator, if not a defining criterion
predictions: small perturbations of the prototype, or ob-        of its essence. Keil (1989) discusses the case of an ani-
jects produced by small perturbations of the process that        mal that is born a skunk, then undergoes surgery that
generated the prototype. The second strategy is likely to        leaves it looking exactly like a raccoon. Since the animal
be more successful than the first, since many perturba-          is generated in the same way as a skunk (born of skunk
tions of the prototype will not arise from any plausible         parents), we conclude that it remains a skunk, no matter
generative process, and thus could never appear in prac-         how it appears on the surface.
tice. By definition, however, an object produced by a               These examples suggest that the generative approach
small perturbation of an existing generative process will        may be broadly useful in explaining high-level similarity
have a plausible causal history.                                 judgments. Yet it is unlikely that the approach will be
   To give a concrete example, suppose the prototype is a        able to account for all kinds of similarity judgments. We
bug generated by a biological process of growth (Figure          claim only that there is an important class of judgments
1ii). The bug in i is a small perturbation of the proto-         that is better explained by the generative approach than
type, but has no plausible generative history. The bug           by previous approaches to similarity.
in iii does have a plausible generative history — it is             We now present a formal model that attempts to cap-
easy to imagine how a perturbation of the process that           ture the intuitions behind the examples described thus
produced ii could produce a bug with an extra segment.           far. The rigor of a computational theory is bought at
If we hope to find a bug that is similar but not identical       a price, however, and we will only apply the theory to
to the prototype, we should look for iii rather than i.          examples much simpler than those already given.
   A sceptic might argue that this prediction task can be
solved by taking the intersection of the set of objects sim-         A computational theory of similarity
ilar to the prototype and the set of objects that are likely     Given a domain D, we develop a theory that specifies the
to exist. The two sets could be computed by indepen-             similarity between any two samples from D. A sample
dent mental modules: the second set depends critically           from D will usually contain a single object, but work-
on generative processes, but the first set (and therefore        ing with similarities between sets of objects is useful for
the similarity module) need not. We think it more likely         some applications. We formalize a generative process as
that the notion of similarity is ultimately grounded in          a probability distribution over D that depends on pa-
the world, and that it evolved for the purpose of com-           rameter vector θ.
paring real-world objects. If so, then knowledge about              Suppose that s1 and s2 are samples from D. We con-
what kinds of objects are likely to exist should be deeply       sider two hypotheses: H1 holds that s1 and s2 are in-
bound up with the notion of similarity.                          dependent samples from a single generative process, and
   This prediction task is of practical importance, but is       H2 holds that the samples are generated from two inde-
not the standard context in which similarity is discussed.       pendently chosen processes. Similarity is defined as the
More commonly, subjects are shown a pair of objects and          log likelihood ratio (Good, 1984), which measures the
asked to rate the similarity of the pair. Note that both         weight of evidence for H1 compared to H2 :
objects are observed to exist and the previous argument
does not apply. Yet generative processes are still im-                                  ·
                                                                                          P (s1 , s2 |H1 )
                                                                                                           ¸
portant, since they help pick out the features critical for         sim(s1 , s2 ) = log
the similarity comparison. Suppose, for instance, that a                                  P (s1 , s2 |H2 )
                                                                                    ·      R                            ¸
forest-dweller discovers a nutritious mushroom. Which                                        P (s1 |θ)P (s2 |θ)p(θ)dθ
is more similar to the mushroom: a mushroom identical                        = log    R                    R              (1)
                                                                                        P (s1 |θ)p(θ)dθ P (s2 |θ)p(θ)dθ
except for its size, or a mushroom identical except for
its color? Knowing how mushrooms are formed suggests                Equation 1 is not the only way to formalize the gen-
that size is not a key feature. Mushrooms grow from              erative approach to similarity, and Jebara et al. (2004)
small to large, and the final size of a plant depends on         describe an alternative that is motivated by similar intu-
factors like the amount of sunlight it received and the          itions. Our model has a clearer probabilistic interpreta-
fertility of the soil that it grew in. Reflections like these    tion than theirs, but the two may well perform similarly
suggest that the differently-sized mushroom should be            in practice.
judged more similar.                                                For some applications, Equation 1 may be difficult to
   A final reason why generative processes matter is that        calculate and we will approximate it by replacing the
                                                             1133

                                                                                                               ³          ´                  ³               ´
integrals with likelihoods at the maximum a posteriori                                                           α+β+1                          α+β+1
                                                                         where k1 = log α+1
                                                                                             ¡          ¢
                                                                                                  α       −log    α+β        ,  k 2 =  log         α+β         ,
(MAP) values of θ:
                                                                        and F (X) = |X| is the cardinality of X.
                        ·
                           P (s1 |θ12 )P (s2 |θ12 )p(θ12 )
                                                             ¸              Under a suitable choice of generative process, then,
    sim(s1 , s2 ) = log                                         (2)      our model becomes equivalent to a version of the con-
                          P (s1 |θ1 )p(θ1 )P (s2 |θ2 )p(θ2 )             trast model where γ2 = γ3 and F (·) = | · |. Our rederiva-
where θ12 = argmaxθ P (s1 , s2 |θ), θ1 = argmaxθ P (s1 |θ),              tion of Tversky’s result makes at least two contributions.
and θ2 = argmaxθ P (s2 |θ).                                              First, it provides an interpretation of k1 and k2 : these
   Similarity is symmetric under this measure:                           parameters are functions of α and β, which make state-
                                                                                                                                  α
sim(s1 , s2 ) = sim(s2 , s1 ).         Whether a symmetric              ments about properties of the world. α+β                      is the a priori
measure is suitable will depend on the context in subtle                 probability that an object has any given feature, and
ways. Consider, for example, the difference between the                  α + β measures the confidence we should place in this
questions ‘How similar are s1 and s2 ?’ and ‘How similar                 probability. In contrast, the parameters γ1 , γ2 and γ3 in
is s1 to s2 ?’ If an asymmetric measure is required, the                 Tversky’s model are free parameters with no real mean-
similarity of s1 to s2 could be defined as the probability               ing independent of the model. A second contribution is
that s1 is produced by the process that generated s2 , or                that our approach automatically provides a setwise sim-
that s2 is produced by the process that generated s1 .                   ilarity measure if s1 and s2 are sets of feature vectors
This paper, however, will focus on the symmetric case.                   rather than single objects. Setwise measures are needed
   We now demonstrate our generative framework in ac-                    by some psychological models (Osherson et al., 1990),
tion by deriving a featural model and a transformational                 but cannot be derived from the contrast model without
model as special cases. 2 Understanding the formal rela-                 additional assumptions.
tionships between these models is important for choosing
                                                                         Transformational models
between them, an issue we will soon address.
                                                                         The transformational approach holds that s1 is similar
Featural models                                                          to s2 if s1 can be readily transformed into s2 . Suppose
Suppose that objects are represented as binary feature                   we are given a set of objects D and a set of transfor-
vectors, and let s1 and s2 be two objects, s1 ∪ s2 be the                mations T . We assume that every transformation is re-
set of features shared by both objects, and s1 − s2 and                  versible — if there is a transformation mapping s1 into
s2 − s1 be the sets of features possessed by one object                  s2 , there must also be a transformation mapping s2 into
but not the other. Tversky’s contrast model proposes                     s1 . A generative process over D is specified by a pro-
that                                                                     totype θ ∈ D chosen from a uniform (and possibly im-
                                                                         proper) distribution over D. To generate an object s
sim(s1 , s2 ) = γ1 F (s1 ∪ s2 ) − γ2 F (s1 − s2 ) − γ3 F (s2 − s1 )      from this process, we sample k from a geometric dis-
                                                                         tribution, choose k transformations at random from T ,
where γ1 , γ2 , and γ3 are positive constants and F (·)                  then apply them to the prototype:
measures the saliency of a feature set.
   Let n be the number of features possessed by one or                                            θ ∼ Uniform(D)
both of the objects. To apply our generative framework,                                          k ∼ Geometric(λ)
let the domain D be the set of all n-place binary vectors.
A generative process over D is specified by a n-place                                            ti ∼ Uniform(T )
vector θ, where θi is the probability that an object has                                          s = tk · tk−1 . . . · t1 (θ)
value 1 on feature i. We place independent beta priors
on each θi :
                                                                         where λ is a constant, and ti is the ith transformation
                        i
                      θ ∼ Beta(α, β)                                     chosen. Intuitively, this process tends to generate small
                                                                         variations of the chosen prototype θ, where the permis-
                      si ∼ Bernoulli(θi ),                               sible variations depend on the set of transformations.
                                                                            We use Equation 2, and approximate each term in the
where si is the ith feature value for object s, α and β                  expression using MAP settings of k and t. The denom-
are hyperparameters and Beta(·, ·) is the beta function.3                inator drops out, and the numerator is approximated
This generative process is known by statisticians as the                 using
beta-Bernoulli model, and has previously appeared in
the psychological literature as part of Anderson’s ratio-                   P (s1 |θ12 )P (s2 |θ12 )
nal analysis of categorization (Anderson, 1991).
   Using Equation 1, we can show that                                           ≈ P (s1 |θ12 , kˆ1 , tˆ1 )P (s2 |θ12 , kˆ2 , tˆ2 )P (kˆ1 , kˆ2 , tˆ1 , tˆ2 )
   sim(s1 , s2 ) = k1 |s1 ∪ s2 | − k2 |s1 − s2 | − k2 |s2 − s1 |                = P (kˆ1 , kˆ2 , tˆ1 , tˆ2 )
                                                                         where kˆ1 is the number of transformations needed to
    2
      Spatial models also emerge as a special case: see the sup-
plementary information at www.mit.edu/~ckemp/ for details,               generate s1 from the prototype θ12 , tˆ1 is the set of these
and for derivations of all results presented here.
    3
      If we want to weight features differently, a different α           transformations, and θ12 , kˆ1 , kˆ2 , tˆ1 and tˆ2 are set to val-
and β can be used for each feature.                                      ues that maximize P (θ12 , kˆ1 , kˆ2 , tˆ1 , tˆ2 |s1 , s2 , H2 ). Since
                                                                    1134

there is a cost for each transformation (the geometric                                    Experiment
distribution encourages kˆ1 and kˆ2 to be small), the MAP          Models: We compared the transformational approach
settings for kˆ1 and kˆ2 minimize the sum kˆ1 + kˆ2 . The          to the generative approach in the domain of colored
minimal value is achieved when kˆ1 + kˆ2 is the length of          strings. An advantage of choosing this domain is that
the shortest path joining s1 and s2 , and θ12 lies some-           there are instances of the competing approaches that
where along this path. It is now straightforward to show           seem natural but make different predictions. Two indi-
that sim(s1 , s2 ) is inversely related to kˆ1 + kˆ2 , or the      cations that the models are natural are that both draw
transformation distance between s1 and s2 . We suspect             on previously published work, and that neither was de-
that a similar analysis can be given if we relax the as-           veloped specifically for this comparison.
sumption that transformations are reversible, although                The transformation model for binary strings uses
we leave the details for future work.                              the five transformations proposed by Imai (1977) and
                                                                   adopted by Hahn et al. (2003): insertion, deletion, phase
                                                                   shift (shifting all squares one position to the right or left),
Choosing between models of similarity
                                                                   mirror-imaging (reflection about the central axis), and
We believe that the featural model, the transformational           reversal (the transformation that maps white squares
model and our generative model offer precisely the same            into black squares and vice versa). We extend these
expressive power. The featural model can capture an                transformations to ternary strings in the natural manner.
arbitrary dataset perfectly if we have complete freedom            All of the transformations are weighted equally, and the
to choose the features, and so can the other models if we          dissimilarity between two strings is defined as the num-
have complete freedom to choose the transformations or             ber of transformations required to transform one into the
generative processes.4 It follows that all of the models           other.
can mimic each other — given a particular choice of fea-              We implement the generative approach using Hidden
tures for the featural model, for example, there will be           Markov Models (HMMs), a class of generative processes
transformations and generative processes that allow the            that is standard in fields including computational biology
other models to make exactly the same predictions.                 and computational linguistics. A HMM is determined by
   Even though the models have the same expressive                 a set of internal states, a matrix of transition probabili-
power, we can choose between them on grounds of ex-                ties q that specifies how to move between the states, and
planatory power. Whenever these models are applied,               a matrix of observation probabilities o that specifies how
advocates of each approach need to explain why they                to generate symbols from each state. To generate a se-
chose the features, transformations, and generative pro-           quence from a HMM, we choose an initial state from a
cesses that they did, and these explanations are unlikely          distribution π, probabilistically generate a color using o,
to be equally convincing. Suppose, for example, that the           then probabilistically choose the next state using q. We
features needed by the featural model seem rather com-             continue until some stopping criterion has been satisfied.
plicated, and share only one property: all of them are                A HMM can be represented using a vector θ =
signatures of an underlying generative process. Keil’s             {π, o, q}. Any given θ induces a probability distribution
skunk example (Keil, 1989) is one case where this seems            over the set of all strings, and we can therefore apply the
to be true, and where the generative approach should               formal model developed above. For simplicity, we use
probably come out on top. We expect there to be other              uniform priors on each component of θ and follow the
cases where the featural model is judged superior, and             MAP approach in Equation 2. MAP values of θ were
others still where the transformational approach gives            computed using the EM algorithm (Murphy, 1998).
the most natural account of the data.                             Task: We used a forced-choice triad task. Subjects
   It may be possible to characterize the settings where          were shown a prototype string, and asked to decide
each of the three models is likely to prove the model             which of two strings was most similar to the prototype.
of choice. We do not attempt that here, but suggest               One of these strings was the ‘HMM string,’ the string
only that the generative approach is uniquely well-suited         most similar to the prototype according to the genera-
to explaining high-level similarity judgments. High-level         tive model. The other was the ‘transformation string,’
judgments are likely to rely on intuitive theories, and           the string most similar to the prototype according to
intuitive theories often specify exactly the kind of infor-       the transformational model. Each subject assessed 20
mation needed by the generative approach: information             binary triads then 16 ternary triads. Five binary triads
about the causal histories of objects.                            are shown in Figure 2, and the full set is available from
   We also believe that there are low-level applications          www.mit.edu/~ckemp/.
where the generative approach is more explanatory than                The triads were chosen systematically to cover most
the other approaches. To support this point and to com-            kinds of strings that can be represented using HMMs
pare our approach to a published model, we designed an             with a handful of states. We generated a comprehen-
experiment using colored strings as stimuli.                       sive set of HMM types, then designed a few triads for
                                                                   each type. A HMM type includes an architecture (a
    4                                                              graph with arrows indicating probable transitions be-
      The model specified by Equation 1 can only capture sym-
metric similarity measures, but as mentioned earlier, there        tween states) and a purity parameter for each state. A
are versions of the generative approach that are not subject       pure state generates only one color, but a noisy state
to this limitation.                                                generates multiple colors. Figure 2 shows several of the
                                                              1135

              Transformation string          Prototype          HMM string                                HMM type
   a)
   b)
   c)
   d)
   e)
Figure 2: Five binary triads used in the experiment. For each triad, at least 9 of 12 subjects chose the HMM
string. The prototype and HMM strings are consistent with the HMM types shown on the right. Arrows indicate
high-probability transitions, and the darkness of a state shows its probability of generating the color black.
HMM types used to generate binary strings. The HMM                      Data              Judgments        Triads
type in 2e moves between a state that generates white                   Binary triads           73           85
squares and another that generates black squares, and                   Ternary triads          63           69
tends to generate several squares from each state.                      All triads              69           78
   Given a HMM type, we chose a prototype string and a
HMM string consistent with the type. The HMM string            Table 1: Percentages of judgments and of triads that
was usually, but not always the same length as the pro-        favored the generative model. A triad favored the gen-
totype string. The transformation string was created by        erative model if more than half of the subjects chose the
transforming the prototype string at a few key points.         HMM string.
Two or three transformations were used to create most
of the binary transformation strings. The ternary strings                            Discussion
are longer, and between three and five transformations
                                                               Our results suggest some conclusions about the gener-
were used in most cases.
                                                               ative and transformational approaches that apply well
Results: Table 1 shows results for 12 subjects. There          beyond the domain of strings. A major problem with the
were 240 judgments overall for the binary strings (20 for      transformational account is that it does not distinguish
each subject), and 73% of these judgments favored the          between generic and non-generic configurations (Jepson
generative model. For 17 out of the 20 triads, a ma-           and Richards, 1993). Consider the strings in Figure 2a.
jority of subjects chose the generative string, and no no      The transformation string is only two transformations
triad clearly favored the transformation model (7 out of       away from the prototype string, but the transformation
12 subjects chose the transformation string on the most        string is non-generic : since the dark squares appear in a
successful triad for this model). The general pattern          clump, it has a Gestalt property that is not shared by the
of results was similar for the ternary strings, but this       prototype string. Figure 3a shows another example. The
time a handful of triads clearly favored the transforma-       difference between a.i and a.ii is that all the dots have
tion model. Overall, these results suggest that similarity     been shifted by a small amount, but a.i is non-generic —
judgments between sequences are sensitive to regularities      it has a striking property that is missing from a.ii.
that can be expressed using HMMs.                                 The generative approach deals neatly with generic and
   A possible response is that all of the prototype strings    non-generic configurations. The configuration in a.i is
were consistent with simple HMMs, and it is not surpris-       most likely to have been generated by a process that pro-
ing that a model based on HMMs should perform better           duces dots arrayed along a line, and this process has no
than an alternative model. It is true that our sample of       chance of producing a.ii. The configuration in a.ii is most
strings was biased towards strings generated by simple         likely to have been generated by a process that produces
processes, and is therefore unrepresentative of the set        a line-shaped cloud of dots, and generating a stimulus
of all possible strings. We suggest, however, that sam-        like a.i would be an astonishing coincidence under such
ples from real-world domains are biased in precisely the       a process. It follows that a.i and a.ii are unlikely to have
same way — indeed, that is one of the motivations for          been generated by the same process, even though a very
our approach. Consider the set of all possible animals,        small transformation will convert one into the other.
which includes creatures like the manticore, a beast with         Another way to state the problem is that simple trans-
a man’s face, a lion’s body and a scorpion’s tail. We can      formations will not suffice for the transformational ap-
imagine animals that are much more bizarre than the            proach. Consider the stimuli in Figure 3b. Removing
manticore, but any sample of real-world animals will be        an edge between a pair of nodes must be an acceptable
biased towards animals generated by a relatively simple        transformation, since b.ii is very similar to b.iii, which is
process — descent with modification.                           identical except for a missing edge. Yet the remove edge
                                                           1136

        a) i)              ii)             iii)                                        References
                                                                Alexander, C. (1979). The timeless way of building. Oxford
                                                                  University Press, Oxford.
                                                                Anderson, J. R. (1991). The adaptive nature of human cat-
                                                                  egorization. Psychological Review, 98(3):409–429.
        b)                                                      Chomsky, N. (1965). Aspects of the theory of syntax. MIT
                                                                  Press, Cambridge, MA.
                                                                Chomsky, N. (1995). Categories and transformations. In The
                                                                  Minimalist Program. Cambridge.
                                                                Feldman, J. (1997). The structure of perceptual categories.
        c)                                                        Journal of Mathematical Psychology, 41:145–170.
                                                                Gentner, D. (1983). Structure-mapping: A theoretical frame-
                                                                  work for analogy. Cognitive Science, 7:155–170.
                                                                Good, I. J. (1984). The best explicatum for weight of evi-
                                                                  dence. Journal of Statistical Computation and Simulation,
                                                                  19:294–299.
Figure 3: Each central object is a small perturbation           Hahn, U., Chater, N., and Richardson, L. B. C. (2003). Sim-
of the object on the left, but seems more similar to the          ilarity as transformation. Cognition, 87:1–32.
object on the right.                                            Imai, S. (1977). Pattern similarity and cognitive transforma-
                                                                  tions. Acta Psychologica, 41:433–447.
                                                                Jebara, T., Kondor, R., and Howard, A. (2004). Probability
transformation must be highly context-sensitive: since            product kernels. Journal of Machine Learning Research,
b.ii is more similar to b.iii than b.i, it must be more ex-       5:819–844.
pensive to convert b.ii into b.i. This example suggests        Jepson, A. and Richards, W. (1993). What makes a good fea-
that each transformation must be assigned a cost that             ture? In Harris, L. and Jenkin, M., editors, Spatial vision
                                                                  in humans and robots, pages 89–121. Cambridge University
depends on global properties of the stimulus.                     Press, Cambridge.
   Colored strings are relatively unstructured objects,        Keil, F. C. (1989). Concepts, kinds, and cognitive develop-
but we can handle more complex domains using pro-                 ment. MIT Press, Cambridge, MA.
cesses that generate structured objects. Kemp et al.           Kemp, C., Griffiths, T. L., and Tenenbaum, J. B. (2004).
(2004), for example, describe a process that generates            Discovering latent classes in relational data. AI Memo
                                                                  2004-019, MIT.
systems of relations. Analogies form one special family of
                                                               Lehrdahl, F. and Jackendoff, R. (1996). A generative theory
comparisons between relational systems, and we believe            of tonal music. MIT Press, Cambridge, MA.
that the generative approach offers a view of analogy          Leyton, M. (1992). Symmetry, causality, mind. MIT Press,
that is is intriguingly different from previous approaches.       Cambridge, MA.
Existing models generally assume that systems are anal-         Liberman, A. M., Cooper, F. S., Shankweiler, D. P., and
ogous to the extent that there is a structure-preserving          Studdert-Kennedy, M. (1967). Perception of the speech
one-to-one map between their elements (Gentner, 1983).            code. Psychological Review, 74:431–361.
The generative approach, however, allows analogous sys-         Marantz, A. (To appear). Generative linguistics within the
tems to have very different numbers of elements, as long          cognitive neuroscience of language. The Linguistic Review.
as they appear to have been produced by the same pro-           Medin, D. and Ortony, A. (1989). Psychological essentialism.
                                                                  In Vosniadou, S. and Ortony, A., editors, Similarity and
cess. Consider, for instance, the graphs in Figure 3c.            analogical reasoning, pages 179–195. Cambridge University
Even though there is a better structure-preserving map            Press, Cambridge.
between c.ii and c.i, c.ii seems more analogous to c.iii.       Murphy, G. L. (1993). Theories and concept formation. In
This is only one suggestive example, but we believe that          Categories and concepts: theoretical views and inductive
the generative approach to analogy deserves further in-           data analysis, pages 173–200.
vestigation.                                                    Murphy, G. L. and Medin, D. L. (1985). The role of theories
                                                                  in conceptual coherence. Psychological Review, 92:289–316.
   We have argued that similarity judgments are infer-         Murphy, K. (1998). Hidden Markov Model (HMM) toolbox
ences about generative processes, and suggested how this          for matlab.
idea applies to domains ranging from the simple (fea-          Osherson, D. N., Smith, E. E., Wilkie, O., Lopez, A., and
ture vectors) to the complex (graphs and other struc-             Shafir, E. (1990). Category-based induction. Psychological
tured objects). The generative processes formalized here          Review, 97(2):185–200.
have been simpler than the processes that appear in peo-       Rehder, B. (2003). A causal-model theory of conceptual rep-
ple’s intuitive theories, but we are optimistic that our          resentation and categorization. Journal of Experimental
                                                                  Psychology: Learning, Memory, and Cognition, 29:1141–
framework will help explain how similarity judgments              1159.
are guided by sophisticated theoretical knowledge.             Thompson, D. W. (1961). On growth and form. Cambridge
                                                                  University Press.
Acknowledgments We thank Ashish Kapoor for point-              Tversky, A. (1977). Features of similarity. Psychological Re-
ing us to (Jebara et al., 2004), and NTT Communication            view, 84:327–352.
Science Laboratories and DARPA for research support.           Wolfram, S. (2002). A new kind of science. Wolfram media,
                                                                  Champaign, IL.
JBT was supported by the Paul E. Newton Career De-
velopment Chair.
                                                           1137

