UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Evolution of Minimal Catastrophic Forgetting in Neural Systems
Permalink
https://escholarship.org/uc/item/0b5276tv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Bullinaria, John A.
Seipone, Tebogo
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

          The Evolution of Minimal Catastrophic Forgetting in Neural Systems
                                            Tebogo Seipone (T.Seipone@cs.bham.ac.uk)
                                        School of Computer Science, The University of Birmingham,
                                                         Birmingham, B15 2TT, UK
                                        John A. Bullinaria (J.A.Bullinaria@cs.bham.ac.uk)
                                       School of Computer Science, The University of Birmingham,
                                                         Birmingham, B15 2TT, UK
                              Abstract                                  learning (Ratcliff, 1990; McClelland, McNaughton &
                                                                        O’Reilly, 1995). The need to store the old patterns, which
   It is well known that neural systems can suffer catastrophic         may be impractical (and also rather implausible for
   forgetting of previously learned patterns when trained on new
   patterns, and that this renders many cognitive models
                                                                        cognitive systems), can be avoided by ‘pseudo-rehearsal’
   unrealistic. However, through evolution, humans have                 which involves creating pseudo-items (that approximate the
   arrived at mechanisms which minimize this problem, and so            original items) to learn with the new items (Robins, 1995).
   in this paper we aim to show how simulated evolution can be          Of course, this still requires storage of the pseudo-items, and
   used to generate neural network models with significantly less       it is far from obvious how best to create pseudo-items that
   catastrophic forgetting than traditionally formulated models.        represent the old items sufficiently well.
                                                                           Given that the main cause of catastrophic forgetting is
                          Introduction                                  interference in the shared weights, many approaches have
It is normal for humans to gradually forget what they have              attempted to reduce that interference. For example, one can
previously learned, particularly during the learning of new             restrict the way in which the hidden unit activations are
information. However, in traditional artificial neural                  distributed, and hence the connection usage. French (1991)
networks, the forgetting is considerably more catastrophic,             has used activation sharpening algorithms to reduce the
and this proves to be a serious limitation of such cognitive            hidden unit activation overlaps. The Sharkey & Sharkey
models (McCloskey & Cohen, 1989; Ratcliff, 1990; French,                (1995) HARM model uses a neural network implementation
1999). Human brains have presumably evolved by natural                  of a lookup table. This divides the learning task into two
selection to minimize this problem. The aim of this paper is            sub-tasks; first eliminating the overlap in the input patterns,
to show how simulated evolution can be used to minimize                 and then producing appropriate outputs from the hidden
the problem in artificial neural networks too.                          nodes. Other approaches have involved allowing two sets
   We shall begin by describing the problem of catastrophic             of weighted connections between nodes. Hinton & Plaut
forgetting in more detail, and outline the principal previous           (1987) used dual-additive weights, with fast weights to learn
approaches to reduce it. We then discuss the possibilities              new patterns and slow weights for long-term storage.
for evolving artificial neural networks, and explain the                   Related approaches have been based on the belief that
particular approach that we have adopted for our study.                 humans do not suffer from catastrophic forgetting because
Then, in our largest section, we present a series of                    their brains have evolved two distinct areas to deal with the
simulation results, and compare each of our evolved systems             problem (McClelland, McNaughton & O’Reilly, 1995).
against the baseline of traditionally built systems. We end             The hippocampal system deals with learning new
with some discussion and conclusions.                                   information, whilst the neocortical system slowly
                                                                        consolidates that new information with the old for long-term
                  Catastrophic Forgetting                               storage, using some form of interleaved learning. Dual-
                                                                        model architectures consisting of two distinct networks, one
After a neural network has been trained on one set of                   for early processing and another for long-term storage of
patterns, training on a new set can seriously disrupt, or               previously learnt information, together with an interfacing
cause loss of, the previously learned patterns. This                    mechanism, have been developed to simulate this separation
‘catastrophic forgetting’ is a direct result of the stability/          (French, 1997; Ans & Rousset, 1997).
plasticity dilemma which was first investigated by                         All these approaches have been based on variations of
McCloskey & Cohen (1989) and Ratcliff (1990). Since                     traditional neural networks, with the designers themselves
then, various approaches have been studied in an attempt to             deciding on the architecture, node activation functions,
reduce or eliminate it (French, 1999).                                  learning algorithms, the various parameter values, and so
   Several approaches were based on the idea that if some,              on. In this paper, we aim to show how evolutionary
or all, of the previous information is re-learned together              computation techniques can be used to evolve neural
with any new information, then the network will not forget              systems that suffer less catastrophic interference than
the old information. This process is called interleaved                 traditionally built systems.
                                                                   1991

          Evolving Neural Network Models                                                   Simulation Results
Evolutionary algorithms are a class of non-deterministic               To begin we need to fix a convenient training set that is
optimization techniques that apply the basic principles of             small enough for the simulations to run reasonably quickly,
evolution by natural selection to find high performance                yet large enough to be representative. Consistency with
solutions. They maintain a population of individuals, each             earlier work led us to a variation of that used by Hinton &
encoding a potential solution to the problem at hand, and              Plaut (1987), namely random associations of 12 bit random
use some form of fitness function to determine which                   binary patterns with 6 bits ‘on’. New random data sets of
solutions to discard and which to keep to form the next                this specification were generated for each generation. Each
generation. Cross-overs and random mutations are applied               network was trained on 20 such patterns until the error on
to the remaining solutions to create new individuals, and the          each output bit was less than 0.1, or the maximum of 1000
process continues, with increasingly fit populations. This             epochs was reached. It was then trained on a different set of
approach has been combined with artificial neural networks             4 such patterns, and the number remembered correctly from
in a number of ways to create highly successful learning               the original 20 was measured using a tolerance of 0.2. All
systems. It has been used to select optimal network                    our populations consisted of 100 networks.
topologies, choices of input features, numbers of hidden                  It is appropriate to start by establishing the baseline
units, node transfer functions, learning algorithms and                performance levels obtained for standard neural network
parameters, and even the connection weights themselves,                training parameters. Figure 1 shows the mean percentages
with results reported as being superior to traditionally built         remembered over 2500 populations with different random
systems (e.g. Yao, 1999; Bullinaria, 2003).                            training data sets, trained using traditional back-propagation
   In this study, the underlying network architecture and              learning rates of 0.2 and random initial weights uniformly
learning algorithm are fixed to be Multi-Layer Perceptrons             distributed in the range [–1, +1]. We see the extent of the
with one hidden layer, trained by gradient descent weight              variance due to some data sets being ‘easier’ than others,
updating (back-propagation) with the Cross Entropy error               and that the performance increases with the number of
measure (Bullinaria 2003). The aim is to evolve the various            hidden units. The first test of our simulated evolution was
other neural network topology and learning parameters to               therefore to allow the number of hidden units NHid to evolve,
produce systems that suffer minimal catastrophic forgetting.           and on the right of Figure 1 we see that there is a steady rise
Evolution is simulated by using populations of individual              towards the maximum allowed (10,000 in this case). We
neural networks, each initialized with random weights from             shall return to this issue again later, but for the bulk of our
their own innately specified ranges. The initial population            simulations we keep the number of hidden units fixed at the
has random innate parameters. At each generation, each                 more computationally feasible number of 50, for which the
network is trained on the same set of initial patterns until it        baseline remembering performance is 68.7%.
has learnt all those patterns (i.e. has all its output activations        The next thing we wanted to explore was the suggestion
within a particular tolerance of their target outputs) or until        of French (1991) that hidden unit activation sharpening
a maximum number of epochs of training is reached. They                could reduce the forgetting by developing semi-distributed
are then trained on a new set of patterns in the same manner.          representations in the hidden layer. The idea is that, at each
After the new patterns have been learned, each network is              epoch of training, the input to hidden weights are modified
tested again to see how well the original patterns are still           to bring the NH highest activation hidden units closer to one,
remembered, that is, the number of output units ‘correct’              and the NL lowest activations closer to zero, by some
(within a particular tolerance) over all of the initial patterns.      ‘sharpening factor’ of α times the difference. There are two
The fittest individuals are naturally those with the highest           variations to consider. First, when we force NH + NL = NHid
number remembered. The least fit half of the population is             so that all hidden activations get changed, the sharpening
then removed, and each of the remaining individuals                    factor invariably evolves to zero, leaving us with our
randomly select a mating partner to produce one child, thus            standard network. If we let N H and NL evolve freely, they
restoring the population size. The children inherit innate             both evolve very quickly to zero, again leaving us with our
characteristics (i.e. parameter values) from the range                 standard network. It seems that node sharpening does not
spanned by both parents, with random Gaussian mutations                really help with catastrophic forgetting, at least for our class
added to allow values outside that range (Bullinaria, 2003).           of training data. As a check, these parameters were left free
For each new generation, a new global random set of                    to evolve in all our subsequent simulations, but in each case
training/remembering data is generated, and all the networks           node sharpening was quickly ‘turned off’.
are started with new random initial weights according to                  There are several traditional network parameters that one
their innately specified ranges.                                       can evolve with the hope of improving performance. To get
   The following Section will make these ideas more                    a feel for which were most effective, we considered each
concrete by specifying our simulations in more detail, and             one in turn before evolving them all at once. First we tried
presenting the results from a systematic sequence of                   evolving the learning rates. It is now well established that
experiments that explore the issues involved.                          allowing separate gradient descent step sizes ηL for each
                                                                  1992

      84                                                               10 4
      76                                                               10 3
   Remem.                                                              NumHid
      68                                                               10 2
      60                                                               10 1
              10      100   Hidden   Units 1000        10000                     0           100    Generation 200            300
Figure 1: Remembering performance improves with the number of hidden units (left), and simulated evolution consequently
causes the number of hidden units to increase to the maximum allowed (right).
10 4                                                                   10 3
                                                                                                                      iwtHO
10 3
                                                    etaHO              10 2
10 2
                                                                                     iwtIH
                                                                       10 1
10 1
10
      0                                                                10 0
                                                    etaOB
10 -1                                                                  10 -1
10 -2
                                                    etaHB              10
                                                                            -2
10 -3
                                                                       10 -3
10 -4                                               etaIH
10 -5                                                                  10 -4
              0      1500   Generation 3000             4500                     0           1500   Generation 3000           4500
100                                                                    450
   90
                                                                       300
 Remembered
                                                                       Epochs
   80
                                                                       150
   70
   60                                                                       0
              0      1500   Generation 3000             4500                     0           1500   Generation 3000           4500
 Figure 2: Simultaneous evolution of the learning rates and initial weight distributions (top), and the associated improvements
in remembering performance and learning speed (bottom).
layer and bias set L is more efficient than a single parameter        these, such as means and standard deviations of Gaussian
to control them all (Bullinaria, 2003). Evolving the four             distributions (µ L, σ L), or as the lower and upper limits of
ηL resulted in a significant improvement in remembering               uniform distributions (–lL, uL). Evolving the upper and
performance from the baseline 68.7% up to around 79%.                 lower limits resulted in an improvement in remembering
  Associated with each learning rate is a random initial              from the 68.7% baseline up to around 76%.
weight distribution. There are several options for specifying            A major advantage of evolutionary approaches is their
                                                               1993

  1.0                                                                   0.5
                                           HO Connectivity                                               Remembering
  0.8                                                                   0.4
                                                                                                           Tolerance
  0.6                                                                   0.3
  0.4                                                                   0.2
                                            IH Connectivity
  0.2                                                                   0.1
                                                                                                                  Training
                                                                                                                 Tolerance
  0.0                                                                   0.0
       0               1500 Generation 3000                4500             0               5000  Generation 10000           15000
Figure 3: Evolution of the connectivity proportions between layers results in significantly reduced connectivity between the
input and hidden layer (left), and evolution of better training and remembering tolerances (right).
ability to evolve simultaneously a number of parameters that          almost full connectivity between the hidden and output
have highly complex interactions. Allowing the learning               layer, but only about one third of the input to hidden layer
rates and initial weight distributions to evolve together leads       connections are used. However, this only resulted in about
to a rather different patterns of results from when they are          one percent improvement in the remembering performance
evolved separately. Figure 2 shows what happens. By                   over the evolved fully connected networks. As a check, we
coordinating their values, our system has now improved its            tried evolving just the connectivities, with all the other
remembering performance up to around 88%. Note the                    parameters held at the baseline values used for Figure 1, but
large differences in size between the four learning rates, and        there was a similarly small remembering improvement.
how far removed they are from the values traditionally used.             Two final parameters that could affect our results are the
It is clear that the sudden improvement in performance                output error tolerances that determine when a particular
corresponds to a ten-fold widening of the input to hidden             output activation is deemed ‘correct’. Previously, these
initial weight distributions, and the thousand fold increase in       allowed an error on the binary targets of up to 0.1 for
learning rate for the hidden to output connections.                   training, and 0.2 for testing/remembering, but other values
   It is interesting to note that the improved remembering            could conceivably give better performance. We see on the
automatically brings with it faster learning, so there is no          right of Figure 3 what happens if we let these parameters
need to build that explicitly into the evolutionary fitness           evolve, in addition to those already considered above. The
function. However, as can be seen in the final graph of               remembering tolerance comes out at just under 0.5, meaning
Figure 2, during the evolutionary history there can be                that almost any output on the right side of 0.5 is deemed
periods of very slow learning. These can be avoided, if               correct. This makes sense, as it renders the remembering as
necessary, by including the number of epochs of learning in           easy as possible without affecting the learning. The training
the fitness sorting. For example, instead of taking for               tolerance is very erratic and fails to settle down, with no
breeding the 50% of the population that remembers best, we            noticeable effect on the remembering performance. With
can take the best 60% and then reduce that to 50% by                  the original and new patterns trained to the same tolerance,
removing the slowest learners.                                        it seems to make little difference what that tolerance
   Two more learning parameters that one might expect to              actually is. It turns out that evolving these parameters only
affect our results are the Sigmoid Prime Offset which                 produces a marginal improvement in performance (about
prevents saturation and poor learning at the hidden layer,            0.5%), and closer investigation reveals some unwanted side-
and weight decay regularization which prevents over-fitting           effects, such as learning times which are as erratic as the
of the training data (Bullinaria, 2003). However, if we               training tolerances, and this can cause the learning rates to
allow these to evolve, their parameters both take on values           drift with an eventual deterioration in performance.
that are so low that they have no significant effect on the              We now move on to the evolution of more sophisticated
learning or remembering performance.                                  networks, allowing them to have two sets of additive
   Another factor that could be expected to influence the             weights along the lines of Hinton & Plaut (1987), with one
interference that causes forgetting is the connectivity               standard set as above, and one set of ‘fast weights’ that has
between layers. We can evolve parameters that specify the             learning rates larger by some scale factor and a weight
proportion of possible connections that are used by the               decay rate that prevents them having long term memory.
network, and find that proportions significantly less than            Evolving the scale factor and decay rate, along with all the
one do emerge as seen on the left of Figure 3. There is               other details described above, results in a further level of
                                                                 1994

100                                                                    0.5
                                             Scale Factor              0.4                            Remembering Tolerance
  10
                                                                       0.3
     1
                                                                       0.2
    .1                                       Decay Rate
                                                                       0.1
                                                                                                            Training Tolerance
 .01                                                                   0.0
          0         1500   Generation 3000              4500                       0      1500
                                                                                                 Generation 3000              4500
Figure 4: Evolution of the dual weight system: the learning rate scale factor and fast-weight decay rate (left), and the evolved
training and testing tolerances (right) that now take on a somewhat different form to the single weight systems.
10 4                                                                  100
                                                                        90
10 3
                                                                      Remembered
 NumHid
                                                                        80
10 2
                                                                        70
10 1                                                                    60
          0          100   Generation 200               300                        0       100   Generation 200               300
Figure 5: The final run evolving everything including the number of hidden units: the number of hidden units rising towards
the maximum allowed (left), and the associated proportion remembered (right).
performance improvement up to 94.8%. Figure 4 shows the               10,000 hidden unit baseline of 78.8%, and the 94.8%
evolving parameter values that achieve this. Interestingly,           maximum achieved with only 50 hidden units. Fortunately,
the evolved training tolerance takes on a much lower value            large parameter interactions are relatively rare, though we
than before (around 0.01), with an associated large increase          do frequently find complex interactions with significant
in the number of epochs training required (around 500 rather          effects that are automatically resolved by evolutionary
than 70). Of course, the importance of slow consolidation is          approaches, but are very difficult to get right ‘by hand’.
not a new idea to memory modelling.                                      So far our study was based on how many of 20 original
   Finally, we return to the matter of the number of hidden           patterns were remembered after training on 4 new patterns.
units. Evolving all the other network parameters, with the            Now we need to explore the extent to which the number of
number of hidden units fixed at 50, has led to improvements           new patterns affects the results. Figure 6 shows what
far superior to the improvement achievable by simply                  happens for our fixed 50 hidden unit case (left graph), and
increasing the number of hidden units by a factor of 100.             when the number of hidden units is free to evolve up to
We now need to check whether allowing more hidden units               10,000 (right graph). Not surprisingly, the baseline degree
can improve our evolved performance even further, or if we            of forgetting as a percentage (left bar of each triple)
have reached a performance ceiling. Figure 5 shows that,              increases with the number of new patterns. Also, consistent
even with all the other parameters evolved at the same time,          with our earlier results, for each number of new patterns, the
the population still takes on increasing numbers of hidden            baseline decreases with the number of hidden units. The
units, though noticeably slower than before. Now the                  important result is that for every case, evolving the network
remembering performance rises to 98.5%, compared to the               parameters, as described above, leads to a massive reduction
                                                               1995

  50                                                                    50
             50 Hidden Units                                                       10000 Hidden Units
                  Baseline                                                               Baseline
  40              Evolved                                               40               Evolved
                  DualWeight                                                             DualWeight
 Forgotten                                                             Forgotten
  30                                                                    30
  20                                                                    20
  10                                                                    10
      0                                                                     0
                 2              4   NewPat 10       20                                 2              4   NewPat 10   20
Figure 6: The more new patterns learned, the more of the original set are forgotten, but in each case the evolved networks and
the evolved dual weight networks have improved remembering performance over the baseline.
in the amount of forgetting (middle bars), with even further          Bullinaria, J.A. (2003). Evolving Efficient Learning
reductions for evolved dual weight systems (right bars).                Algorithms for Binary Mappings. Neural Networks, 16,
                                                                        793-800.
                 Discussion and Conclusions                           French, R.M. (1991). Using Semi-Distributed
                                                                        Representations to Overcome Catastrophic Forgetting in
We have taken ideas from natural evolution and shown,                   Connectionist Networks. Proceedings of the Thirteenth
through a series of simulations, how they can significantly             Annual Conference of the Cognitive Science Society (pp.
reduce the well known problem of catastrophic forgetting in             173-178). Hillsdale, NJ: LEA.
artificial neural network systems trained on simple pattern           French, R.M. (1997). Pseudo-Recurrent Connectionist
association memory tasks. Simply evolving the traditional               Networks: An Approach to the “Sensitivity-Stability”
neural network parameters (such as numbers of hidden                    Dilemma. Connection Science, 9, 353-379.
units, degrees of connectivity, initial weight distributions,         French, R.M. (1999). Catastrophic Forgetting in
learning rates and tolerances) leads to big improvements in             Connectionist Networks. Trends in Cognitive Sciences, 4,
remembering performance, and allowing dual weight                       128-135.
architectures provides even further increases.                        Hinton, G.E. & Plaut, D.C. (1987). Using Fast Weights to
   It is always difficult to relate simplified cognitive models         Deblur Old Memories. Proceedings of the Ninth Annual
to real brain processes. However, our results are robust
                                                                        Conference of the Cognitive Science Society (pp. 177-
across numbers of hidden units and problem complexity (i.e.
                                                                        186). Hillsdale, NJ: Erlbaum.
number of new patterns learned), so we can expect them to
                                                                      McClelland, J.L., McNaughton, B.L. & O’Reilly, R.C.
scale up well. Reliably extrapolating up to human type
                                                                        (1995). Why There Are Complementary Learning
memory tasks and brain like numbers of hidden units is very
                                                                        Systems in the Hippocampus and Neocortex: Insights
difficult, of course, but it does seems that the problem of
                                                                        From the Successes and Failures of Connectionist Models
catastrophic forgetting in neural networks may not be quite
                                                                        of Learning and Memory. Psychological Review, 102,
as problematic as previously thought.
                                                                        419-457.
   The next stage will be to extend our models to include
                                                                      McCloskey, M. & Cohen, N.J. (1989). Catastrophic
more esoteric factors, such as the dual model architectures
of French (1997) and Ans & Rousset (1997), and let                      Interference in Connectionist Networks: The Sequential
evolution decide on the solution it prefers. Our models                 Learning Problem. The Psychology of Learning and
already contain the necessary factors to prevent over-fitting,          Motivation, 24, 109-165.
and preliminary results indicate that we are able to evolve           Ratcliff, R. (1990). Connectionist Models of Recognition
appropriate parameters for networks to work as well on                  and Memory: Constraints Imposed by Learning and
generalization tasks as they have here on memory tasks.                 Forgetting Functions. Psychological Review, 97, 205-308.
                                                                      Robins, A. (1995). Catastrophic Forgetting, Rehearsal and
                               References                               Pseudorehearsal. Connection Science, 7, 123-146.
                                                                      Sharkey, N.E. & Sharkey, A.J.C. (1995). An Analysis of
Ans, B. & Rousset, S. (1997). Avoiding Catastrophic                     Catastrophic Interference. Connection Science, 7, 301-
 Forgetting by Coupling Two Reverberating Neural                        329.
 Networks. CR Academie Science Paris, Life Sciences,                  Yao, X. (1999). Evolving Artificial Neural Networks.
 320, 989-997.                                                           Proceedings of the IEEE, 87, 1423-1447.
                                                               1996

