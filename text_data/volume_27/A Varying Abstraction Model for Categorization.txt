GCM framework                                                    where N is the number of exemplars of J. Using equa-
                                                                 tions (2) and (3), we find that
Probability The traditional GCM states that, in a
categorization task with two categories A and B, the                            (N )
                                                                                X
probability of responding A given a stimulus i equals                    ηiJ =       exp(−dα  ij )
                                                                                j∈J
                                 βA ηiA
             P (A, i) =                            ,     (1)                    (N )
                                                                                X               X D
                         βA ηiA + (1 − βA )ηiB
                                                                             =       exp(−(c[        wk |xik − xjk |r ]1/r )α ).   (5)
                                                                                j∈J             k=1
where βA is the response bias towards category A and
ηiJ is the similarity of the stimulus i to the category J.       Prototype model A prototype model can easily be
Similarity A necessary condition to be able to calcu-            formulated within the framework of the GCM. In this
late the similarity of the stimulus i to the category J is       model, a category is assumed to be represented as an
a clear specification of what makes up a category. It is         abstract summary representation of all the encountered
exactly at this specification that exemplar and prototype        exemplars of the category. This abstract summary of the
models diverge, as will become clear immediately. They           category is called the category prototype and is denoted
do agree however on the definition of the similarity of          as PJ . Hence, the similarity of the stimulus i to the
the stimulus i to another stimulus j. It is assumed to be        category J equals the similarity of the stimulus i to the
related to (psychological) distance d via                        category prototype, that is:
                      ηij = exp(−dα                                                          ηiJ ≡ ηiPJ .                          (6)
                                      ij ).              (2)
                                                                 The coordinates of PJ are simply the averaged coordi-
Two special cases are popular: the one where α = 1,              nates of all the exemplars within the category J on each
resulting in the exponential decay function, and the one         of the underlying coordinate axes:
where α = 2, resulting in the Gaussian function.
                                                                                                      (N )
Distance The distance between the two stimuli i and                                                 1 X
j in turn is calculated from the coordinates of the two                                  xPJ k =           xjk .                   (7)
                                                                                                   N
stimuli. There are several versions of how to compute                                                 j∈J
a distance from the coordinates. The most common ex-
pression for distance is                                         Using equations (2), (3) and (7), we find that
                        D                                            ηiJ = exp(−dα    iPJ )
                        X
               dij = c[    wk |xik − xjk |r ]1/r ,       (3)                             D                  (N )
                                                                                        X                1 X
                       k=1                                                = exp(−(c[        wk |xik −            xjk |r ]1/r )α ). (8)
                                                                                                        N
                                                                                        k=1                 j∈J
where xik is the coordinate of stimulus i on dimension
k. This is a so-called weighted distance: wk denotes the         The unifying model
proportion of attention allocated to dimension k and so          Principle In the above presentation of the exemplar
PD
   k=1 wk = 1. The parameter c is a scaling parameter.           and prototype models within the GCM framework, the
This distance is called a city-block distance when r = 1         extreme positions of these models are easily seen: in the
and Euclidean when r = 2.                                        exemplar model, the distance of i towards all N exem-
                                                                 plars is calculated, while in the prototype model, the
Exemplar and prototype models                                    distance of i towards just one single exemplar (i.e., the
                                                                 category centroid) is calculated. The varying abstraction
Both prototype and exemplar models share the above               model now assumes that the number of items to which
assumptions and definitions. They differ however in their        i is compared can vary, in principle, from 1 to N . This
exact understanding of what makes up ‘the category J’.           means that the varying abstraction model not only in-
Therefore, they differ in the way the similarity of the          corporates the two traditional models, but also invokes
stimulus i to the category J is calculated.                      new intermediate models.
Exemplar model In the exemplar model, a category
is assumed to be represented by memory traces of all             Formalization One can consider a category J as a
the encountered exemplars of the category. Hence, the            set of N elements jn .2 The basic idea is to make up a
similarity of the stimulus i to the category J is calculated     partition for each category. A set partition, or simply
by summing the similarity of the stimulus i to all N             a partition, of a set S is defined as a collection of dis-
stored exemplars of J, that is:                                  joint, nonempty subsets of S whose union is S. Each
                                                                 subset in such a partition is called an equivalence class
                               (N )
                                                                 or a block. For every block one can easily construct the
                               X
                                                                     2
                        ηiJ ≡       ηij ,                (4)           To simplify the notation, the index n is dropped most of
                               j∈J                               the time.
                                                            2278

prototype of this block by averaging over all the exem-                        A key role is played by Q: it denotes the number of
plars in that block. Such a block prototype is called a                     blocks and hence the number of block prototypes (or
pseudo-exemplar.3                                                           pseudo-exemplars) that are used to represent a category.
   In general, a partition of J consists of Q different                     On a conceptual level, Q indexes the level of abstraction
blocks Fq , where q ranges from 1 to Q. Q itself ranges                     (higher Q means lesser abstraction). Since Q is allowed
from 1 (when F equals J, the set of all exemplars) to N                     to vary from 1 to N for each category and Q counts the
(when every Fq is a singleton). The number of elements                      number of prototypes used to represent a category, the
in a block Fq is denoted as Rq . The block prototype in                     varying abstraction model is a formalization of the idea
a block Fq is denoted as eq .4 The set of all the block                     that people use multiple prototypes.
prototypes of a certain partition of J, is denoted as E                        The varying abstraction model makes clear that the
and has a set size of Q.                                                    exemplar and prototype model are extreme pseudo-
   How is a category defined according to the varying                       exemplar models and are therefore of the same nature
abstraction model? In the model, it is assumed that a                       as the intermediate pseudo-exemplar models. However,
category is represented by a number of abstract sum-                        for ease of exposition, the term pseudo-exemplar model
mary representations of some of the encountered exem-                       will only be used to refer to the intermediate models, not
plars of the category. These abstract summaries of the                      to the extreme ones.
category are called the category pseudo-exemplars and                       The traditional models Formally, the expressions
are denoted as eq . Hence, the similarity of the stimulus                   (4) and (6) can be considered as special cases of the
i to the category J is calculated by summing the simi-                      general expression (9). The same holds for expressions
larity of the stimulus i to all Q pseudo-exemplars of J,                    (5) and (8): they are special cases of expression (11).
that is:                                                                    The varying abstraction model reduces to the traditional
                                 (Q)
                                 X                                          models when a specific partition is chosen for each cate-
                          ηiJ ≡      ηie .                          (9)
                                                                            gory.
                                e∈E
                                                                               The exemplar model follows when each category J
The coordinates of eq are simply the averaged coordi-                       with N elements is partitioned in N subsets of one el-
nates of all the Rq exemplars within the block Fq on                        ement each. The block prototypes therefore equal the
each of the underlying coordinate axes:                                     exemplars. More formally:
                                  (Rq )
                               1 X                                          • Q=N
                      xek =             xjk .                     (10)
                              Rq
                                  j∈Fq
                                                                            • Fq = {jq } so Rq = 1 for every q
Making use of equations (2), (3) and (10), we finally find
that                                                                        • E = J.
           (Q)
           X                                                                   The prototype model follows when each category J
   ηiJ =       exp(−dα ie )                                                 with N elements is partitioned in only one subset of N
          e∈E                                                               elements. There is only one block prototype which equals
           (Q)             D                                                the category prototype PJ . More formally:
           X             X
       =       exp(−(c[      wk |xik − xek |r ]1/r )α )
                                                                            • Q=1
          e∈E            k=1
           (Q)             D                  (Rq )                         • Fq = J so Rq = N for every q
           X             X                 1 X
       =       exp(−(c[      wk |xik −              xjk |r ]1/r )α ).
                                        Rq                                  • E = {PJ }.
          e∈E            k=1                  j∈Fq
                                                                  (11)
                                                                            The pseudo-exemplar models Every partition de-
Model family One specific partition of each category                        fines or corresponds to a certain model. How many non-
picks out one specific model. Such a model is called                        extreme (i.e. 1 < Q < N ) partitions (and hence models)
a pseudo-exemplar model. The two extreme partitions                         should one consider when fitting the model to a data set?
(i.e. Q = N for each category and Q = 1 for each cate-                         In the ideal case, one might explore all possible par-
gory) correspond to the two extreme pseudo-exemplar                         titions of the N stimuli in a category J. However, this
models (i.e. the exemplar model and the prototype                           strategy is not always feasible. The number of possible
model, respectively). The other, intermediate parti-                        partitions of a set of N elements is given by the Bell
tions correspond to intermediate pseudo-exemplar mod-                       number of N (denoted as BN ). This number increases
els. Therefore, the varying abstraction model is not just                   very rapidly.5 In a categorization task with two cate-
a model but it is a family of models.                                       gories A and B with NA and NB exemplars respectively,
    3                                                                       this implies fitting BNA × BNB different models. When
      Alternative names for pseudo-exemplar are super-
exemplar or sub-prototype. They are all used interchange-                   more than, say, eight, stimuli per category are used, the
ably.                                                                       number of possibilities to consider becomes intractable.
    4
      To simplify the notation, the index q is dropped most of
                                                                                5
the time.                                                                         For instance, B6 equals 203 and B10 equals 115975.
                                                                       2279

   A straightforward way to keep the number of parti-             where pn is proportion of A responses for stimulus n.
tions within practically feasible boundaries is to select            The maximum likelihood algorithm looks for the θ
at every level of abstraction only one partition. Instead         that most likely have produced the observed responses.
of blindly considering all partitions, we could limit our-        Therefore, it should seek those parameter values that
selves to only one partition for each category for every          maximize this likelihood. Assuming a binomial distribu-
value of Q. This partition can for example be selected            tion, this likelihood equals
through clustering. This reduces the number of different
models to be fit to NA × NB . An application of this ap-                       YN µ ¶
                                                                                      S
proach in the context of natural language can be found               Lik(θ) =             P (A, n | θ)yn P (B, n | θ)S−yn , (13)
                                                                                      y n
in Verbeemen, Storms and Verguts (in press).                                   n=1
   When practically possible, fitting all possible models
is a fruitful strategy. It has the clear advantage over           where yn denotes number of subjects choosing category
the clustering approach that no assumptions have to be            A for stimulus n. For computational efficiency, it is bet-
made about spatial representation or about which ex-              ter to look for the θ that maximizes the natural loga-
emplars should or should not be merged together. What             rithm of this likelihood. Hence the function to be maxi-
drives clustering is inferred, not imposed.                       mized in the maximal likelihood algorithm is the loglike-
   The main advantage of the pseudo-exemplar models               lihood7 :
is that they allow for sensitive modeling. They allow for                     N
                                                                              X
adaptation to category complexity, category experience            Loglik =       [yn ln P (A, n | θ) + (S − yn ) ln P (B, n | θ)].
and individual skills.                                                       n=1
                                                                                                                               (14)
                Evaluating the model                                 When fitting the varying abstraction model to a data
A first data set of a category learning experiment with           set, the algorithm seeks, among all possible partitions of
artificial stimuli has been analyzed using the vary-              each category, the parameter values that minimize SSE
ing abstraction model and corresponding Matlab al-                or maximize Loglik. The partition yielding the smallest
gorithms. This preliminary analysis indicated that a              minimal SSE or largest maximal Loglik of all the possi-
pseudo-exemplar model outperformed the traditional                ble partitions corresponds to the pseudo-exemplar model
prototype and exemplar models.                                    that best accounts for the categorization process.8
Matlab fitting algorithms                                         Category learning experiment
Determining to what extent the model can account for              The proposed model’s performance was tested in a cate-
category-related behavior is done by fitting all the mod-         gorization experiment using the well-known 5-4 structure
els of the varying abstraction model family to a data             (Medin & Schaffer, 1978; Smith & Minda, 2000).
set.
   A typical categorization experiment consists of a train-       Subjects Twenty-four first year university students
ing phase and a test phase. In the test phase, N stimuli          participated for course credit.
are presented to S subjects. Each subject classifies every        Stimuli Stimuli were constructed according to the 5-
stimulus as either A or B. In the following, P (A, i | θ) is      4 category structure from Medin and Schaffer (1978).
the varying abstraction model’s estimate of the proba-            The stimuli were artificial sheep, varying on four dimen-
bility of responding A to i, given the parameters θ. The          sions: eyes (open or closed), fleece (four or five curls),
expression for this probability is obtained by combining          feet (black or white) and tail (rounded or starred). Dur-
expressions (1) and (11).                                         ing the training phase, subjects only encountered the five
   We have developed two Matlab algorithms to fit pro-            stimuli of category A and the four stimuli of category B.
totype, exemplar and all pseudo-exemplar models and               During the transfer phase, all 16 stimuli were presented.
compare the performance of the different models. Both
least squares and maximum likelihood methods are used             Procedure To motivate the subjects, the categoriza-
to estimate the model’s unknown parameters.6 These                tion task was presented as a sheepdog game. The par-
parameters are the response bias βA , the scaling param-          ticipants were asked to drive the sheep to the correct
eter c and D − 1 attention weights wk . All these pa-             meadow. A-sheep should be driven to the left and B-
rameters are, for the sake of brevity, summarized in the          sheep to the right. In each trial, a sheep appeared on
parameter vector θ.                                                                           P
   The least squares algorithm looks for the θ that most
                                                                      7
                                                                        The additive constant N  n=1 [ln S! − ln(S − yn )! − ln yn !]
                                                                  does not depend on θ and therefore is dropped.
accurately describes the observed responses. The algo-                8
                                                                        Both measures only take descriptive accuracy into ac-
rithm seeks those parameter values that minimize the              count. To avoid overfitting, model complexity should be
sum of squared errors, that is                                    taken into account as well. Although all the models of
                                                                  the varying abstraction model family have the same num-
                          XN
                                                                  ber of parameters with the same range, the functional form
             SSE(θ) =         (pn − P (A, n | θ))2 ,     (12)     of these parameters differ (i.e. the way the parameters are
                          n=1                                     combined). There are tools available combining goodness-of-
                                                                  fit with model complexity, such as Bayes factors and mini-
    6
      A very useful tutorial on maximum likelihood estimation     mum description length (Myung & Pitt, 1997; Pitt, Myung
is Myung (2003).                                                  & Zhang, 2002).
                                                             2280

the screen, staring at a certain direction. The partici-          Model performance
pant had to evaluate the correctness of the initial staring       Analysis of a categorization experiment indicated that
direction, by pressing button 1 for a correct and button 2        a pseudo-exemplar model outperformed the traditional
for a wrong direction. Each sheep appeared twice, once            models. It is important to note that this finding does
in the correct direction and once in the incorrect direc-         not lead to the conclusion that the varying abstraction
tion. Hence there were 18 trials in the training phase            model outperformed the exemplar and prototype mod-
and 32 trials in the transfer phase. The order of appear-         els. In fact, we would be able to make that conclusion
ance of the sheep was randomized. In the training phase,          even before having a look at any data set at all. For,
every trial was followed by “good” or “false”. Feedback           the varying abstraction model includes both traditional
was omitted in the transfer phase.                                models, so its performance is at least as good as the per-
Results Both categories are small, so all possible par-           formance of the traditional models. Of course, this com-
titions/models could be examined. Since there are 16              parison would not be a fair one. We do conclude however
possible partitions for a set of four elements and 52 for a       that the varying abstraction model can single out a new
set of five, 780 different models were fit to the data. In        pseudo-exemplar model (or several new pseudo-exemplar
principle, four different families of models could be fit-        models), that clearly outperforms the traditional ones.
ted, since both r and α can take the values 1 or 2. In the        The comparison between this specific pseudo-exemplar
preliminary analysis presented here, both r and α were            model and the traditional models is a fair one, or at least
set to 1. The summary fits are presented in Table 1.              as fair as the comparison between the exemplar model
                                                                  and the prototype model.
Table 1: Summary fits of the prototype (PM), exemplar             Future Directions
(EM) and the best pseudo-exemplar (PE) model under                Due to the pseudo-exemplar models, the varying abstrac-
the least squares estimation method.                              tion model allows for sensitive modeling. This sensitiv-
                                                                  ity makes the varying abstraction model highly useful
                               PM      EM    PE                   for investigating in full detail Smith and Minda’s (1998)
              minimal SSE      0.13    0.09  0.03                 findings that category representation may change during
              r2               0.93    0.95  0.99                 the learning process. In fact, they found that with large
              parameter βA     0.58    0.52  0.09                 categories, a prototype model yields better accounts of
              parameter c      7.53    6.88  11.31                the initial phases of categorization, while exemplar mod-
              parameter w1     0.24    0.20  0.17                 els yield better accounts in later stages of the learning
                                                                  process. One can expect that, using the varying abstrac-
              parameter w2     0.00    0.10  0.14
                                                                  tion model, better fits will be obtained for models with
              parameter w3     0.37    0.47  0.24                 more pseudo-exemplars as learning proceeds.
              parameter w4     0.39    0.24  0.44                    Apart from doing more elaborate modeling of catego-
                                                                  rization decisions, the varying abstraction model can eas-
                                                                  ily be extended to account for typicality ratings. Nosof-
Discussion This preliminary analysis with small cat-              sky (1988) contrasted prototype and exemplar accounts
egories revealed that, as expected, the exemplar model            of rated typicalities. Further research will investigate
outperformed the prototype model. More importantly,               whether intermediate abstraction levels account better
it also revealed that a pseudo-exemplar model outper-             for rated typicalities than extreme abstraction levels do.
formed both traditional models.9 The best pseudo-                    Another extension could be the account of response
exemplar model corresponds to the following partition             times for exemplars in a speeded categorization task, for
of the categories, using the labels as described in Medin         instance along the lines of Nosofsky and Palmeri’s (1997)
and Schaffer (1978): category A is divided in three clus-         exemplar-based random walk model.
ters (A1 and A4; A2; A3 and A5) and category B re-                   A theoretical issue is the connection between the pro-
mains one simple cluster. It is not surprising at all that        posed varying abstraction model and related models.
A2 is singled out since A2 is the stimulus that is the least      The idea of considering the prototype and exemplar
similar to the prototype of category A.                           models as extremes along a continuum has been adopted
                                                                  in other models. The rational model (Anderson, 1991),
                   General Discussion                             SUSTAIN (Love, Medin & Gureckis, 2004) and the mix-
                                                                  ture model (Rosseel, 2002) share with the varying ab-
The model proposed in this paper elegantly unifies two            straction model the idea that a category is represented
traditional formal models for categorization. Further, it         by multiple prototypes, but show clear differences with
gives rise to a set of new models, called pseudo-exemplar         the varying abstraction model too. The varying abstrac-
models. All these models, traditional and new, are for-           tion model is also closely related to a general-rule-plus-
malized along the continuum of abstraction.                       exception model such as RULEX (Nosofsky, Palmeri &
                                                                  McKinley, 1994). Investigating exactly how these mod-
    9
      In fact, there were several pseudo-exemplar models that     els and the varying abstraction model differ in principle,
outperformed the traditional models. Only the best one is         formalization and performance is important work for the
reported here.                                                    future.
                                                             2281

                 Acknowledgments                               Nosofsky, R. M. (1986). Attention, similarity, and the
                                                                 identification-categorization relationship. Journal of
The research reported here is part of an interdisciplinary
                                                                 Experimental Psychology: General, 115, 39–57.
research project sponsored by the Research Council of
the University of Leuven (IDO/02/004), given to the sec-       Nosofsky, R. M. (1988). Exemplar-based accounts of
ond author. The authors wish to thank Michael D. Lee             relations between classification, recognition, and typi-
and Daniel Navarro.                                              cality. Journal of Experimental Psychology: Learning,
                                                                 Memory, and Cognition, 14, 700–708.
                      References                               Nosofsky, R. M. (1992). Exemplars, prototypes, and
Anderson, J. (1991). The adaptive nature of human cat-           similarity rules. In A. F. Healy, S. M. Kosslyn, &
  egorization. Psychological Review, 98, 409–429.                R. M. Shiffrin (Eds.), From learning theory to connec-
                                                                 tionist theory: Essays in honor of William K. Estes,
Ashby, F. G., & Maddox, W. T. (1993). Relations be-              Vol. 1. Lawrence Erlbaum, Hillsdale, NJ.
  tween prototype, exemplar, and decision bound mod-
  els of categorization. Journal of Mathematical Psy-          Nosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-
  chology, 37, 372–400.                                          based random walk model of speeded classification.
                                                                 Psychological Review, 104, 266-300.
Borg, I., & Groenen, P. J. F. (1997). Modern multidi-
                                                               Nosofsky, R. M., Palmeri, T. J., & McKinley, S.C.
  mensional scaling. New York: Springer.
                                                                 (1994). Rule-plus-exception model of classification
Hampton, J. A. (1993). Prototype models of concept               learning. Psychological Review, 101, 53–79.
  representations. In I. Van Mechelen, J. A. Hampton,          Pitt, M. A., Myung, I., & Zhang, S. (2002). Toward a
  R. S. Michalski, & P. Theuns (Eds.), Categories and            method of selecting among computational models of
  concepts: Theoretical views and inductive data analy-          cognition. Psychological Review, 109, 472–491.
  sis. London: Academic Press.
                                                               Rosch, E. (1978). Principles of categorization. In E.
Lakoff, G. (1987). Women, fire and dangerous things:             Rosch & B. B. Lloyd (Eds.), Cognition and catego-
  What categories reveal about the mind. Chicago: Uni-           rization. Erlbaum, Hillsdale, NJ.
  versity of Chicago Press.
                                                               Rosseel, Y. (2002). Mixture models of categorization.
Lee, M. D. (2001). Determining the dimensionality of             Journal of Mathematical Psychology, 46, 178–210.
  multidimensional scaling representations for cognitive
                                                               Smith, J. D., & Minda, J. P. (1998). Prototypes in the
  modeling. Journal of Mathematical Psychology, 45,
                                                                 mist: The early epochs of category learning. Journal
  149–166.
                                                                 of Experimental Psychology: Learning, Memory, and
Love, B. C., Medin, D. L., & Gureckis, T. M. (2004).             Cognition, 24, 1411–1436.
  SUSTAIN: A network model of category learning.               Smith, J. D., & Minda, J. P. (2000). Thirty categoriza-
  Psychological Review, 111, 309–332.                            tion results in search of a model. Journal of Exper-
Medin, D. L., Altom, M. W., & Murphy, T. D. (1984).              imental Psychology: Learning, Memory, and Cogni-
  Given versus induced category representations: Use of          tion, 26, 3–27.
  prototype and exemplar information in classification.        Smith, J. D., & Minda, J. P. (2002). Distinguishing
  Journal of Experimental Psychology: Learning, Mem-             prototype-based and exemplar-based processes in dot-
  ory, and Cognition, 10, 333–352.                               pattern category learning. Journal of Experimental
Medin, D. M., & Schaffer, M. M. (1978). Context theory           Psychology: Learning, Memory and Cognition, 28,
  of classification learning. Psychological Review, 85,          800–811.
  207–238.                                                     Smits, T., Storms, G., Rosseel, Y., & De Boeck, P.
Minda, J. P., & Smith, J. D. (2001). Prototypes in cat-          (2002). Fruits and vegetables categorized: An appli-
  egory learning: The effects of category size, category         cation of the generalized context model. Psychonomic
  structure, and stimulus complexity. Journal of Experi-         Bulletin and Review, 9, 836–844.
  mental Psychology: Learning, Memory and Cognition,           Verbeemen, T., Storms, G., & Verguts, T. (in press).
  27, 775–799.                                                   Varying abstraction in categorization: a K-means ap-
                                                                 proach. Proceedings of the 27th Annual Conference of
Myung, I. J. (2003). Tutorial on maximum likelihood
                                                                 the Cognitive Science Society.
  estimation. Journal of Mathematical Psychology, 47,
  90–100.
Myung, I. J., & Pitt, M. A. (1997). Applying Occam’s
  razor in modeling cognition: A Bayesian approach.
  Psychonomic Bulletin & Review, 4, 79–95.
Nosofsky, R. M. (1984). Choice, similarity, and the con-
  text model of classification. Journal of Experimen-
  tal Psychology: Learning, Memory, and Cognition, 10,
  104–114.
                                                          2282

