GCM framework
Probability The traditional GCM states that, in a
categorization task with two categories A and B, the
probability of responding A given a stimulus i equals
P (A, i) =

βA ηiA
,
βA ηiA + (1 − βA )ηiB

where N is the number of exemplars of J. Using equations (2) and (3), we find that
ηiJ =

(N )
X

exp(−dα
ij )

j∈J

(1)
=

where βA is the response bias towards category A and
ηiJ is the similarity of the stimulus i to the category J.

(N )
X

exp(−(c[

j∈J

D
X

wk |xik − xjk |r ]1/r )α ).

(5)

k=1

Similarity A necessary condition to be able to calculate the similarity of the stimulus i to the category J is
a clear specification of what makes up a category. It is
exactly at this specification that exemplar and prototype
models diverge, as will become clear immediately. They
do agree however on the definition of the similarity of
the stimulus i to another stimulus j. It is assumed to be
related to (psychological) distance d via

Prototype model A prototype model can easily be
formulated within the framework of the GCM. In this
model, a category is assumed to be represented as an
abstract summary representation of all the encountered
exemplars of the category. This abstract summary of the
category is called the category prototype and is denoted
as PJ . Hence, the similarity of the stimulus i to the
category J equals the similarity of the stimulus i to the
category prototype, that is:

ηij = exp(−dα
ij ).

ηiJ ≡ ηiPJ .

(2)

Two special cases are popular: the one where α = 1,
resulting in the exponential decay function, and the one
where α = 2, resulting in the Gaussian function.
Distance The distance between the two stimuli i and
j in turn is calculated from the coordinates of the two
stimuli. There are several versions of how to compute
a distance from the coordinates. The most common expression for distance is
dij = c[

D
X

(6)

The coordinates of PJ are simply the averaged coordinates of all the exemplars within the category J on each
of the underlying coordinate axes:
xPJ k =

(N )
1 X
xjk .
N

(7)

j∈J

Using equations (2), (3) and (7), we find that
ηiJ = exp(−dα
iPJ )

wk |xik − xjk |r ]1/r ,

(3)
= exp(−(c[

k=1

where xik is the coordinate of stimulus i on dimension
k. This is a so-called weighted distance: wk denotes the
proportion of attention allocated to dimension k and so
PD
k=1 wk = 1. The parameter c is a scaling parameter.
This distance is called a city-block distance when r = 1
and Euclidean when r = 2.

Exemplar and prototype models
Both prototype and exemplar models share the above
assumptions and definitions. They differ however in their
exact understanding of what makes up ‘the category J’.
Therefore, they differ in the way the similarity of the
stimulus i to the category J is calculated.
Exemplar model In the exemplar model, a category
is assumed to be represented by memory traces of all
the encountered exemplars of the category. Hence, the
similarity of the stimulus i to the category J is calculated
by summing the similarity of the stimulus i to all N
stored exemplars of J, that is:

ηiJ ≡

(N )
X

ηij ,

(4)

j∈J

2278

D
X

k=1

(N )
1 X
xjk |r ]1/r )α ).
wk |xik −
N

(8)

j∈J

The unifying model
Principle In the above presentation of the exemplar
and prototype models within the GCM framework, the
extreme positions of these models are easily seen: in the
exemplar model, the distance of i towards all N exemplars is calculated, while in the prototype model, the
distance of i towards just one single exemplar (i.e., the
category centroid) is calculated. The varying abstraction
model now assumes that the number of items to which
i is compared can vary, in principle, from 1 to N . This
means that the varying abstraction model not only incorporates the two traditional models, but also invokes
new intermediate models.
Formalization One can consider a category J as a
set of N elements jn .2 The basic idea is to make up a
partition for each category. A set partition, or simply
a partition, of a set S is defined as a collection of disjoint, nonempty subsets of S whose union is S. Each
subset in such a partition is called an equivalence class
or a block. For every block one can easily construct the
2
To simplify the notation, the index n is dropped most of
the time.

prototype of this block by averaging over all the exemplars in that block. Such a block prototype is called a
pseudo-exemplar.3
In general, a partition of J consists of Q different
blocks Fq , where q ranges from 1 to Q. Q itself ranges
from 1 (when F equals J, the set of all exemplars) to N
(when every Fq is a singleton). The number of elements
in a block Fq is denoted as Rq . The block prototype in
a block Fq is denoted as eq .4 The set of all the block
prototypes of a certain partition of J, is denoted as E
and has a set size of Q.
How is a category defined according to the varying
abstraction model? In the model, it is assumed that a
category is represented by a number of abstract summary representations of some of the encountered exemplars of the category. These abstract summaries of the
category are called the category pseudo-exemplars and
are denoted as eq . Hence, the similarity of the stimulus
i to the category J is calculated by summing the similarity of the stimulus i to all Q pseudo-exemplars of J,
that is:
(Q)
X
ηiJ ≡
ηie .
(9)
e∈E

The coordinates of eq are simply the averaged coordinates of all the Rq exemplars within the block Fq on
each of the underlying coordinate axes:
xek =

(Rq )
1 X
xjk .
Rq

(10)

j∈Fq

ηiJ =

=

exp(−(c[

e∈E

=

(Q)
X
e∈E

D
X

wk |xik − xek |r ]1/r )α )

D
X
k=1

• E = J.

• Q=1

k=1

exp(−(c[

• Q=N

The prototype model follows when each category J
with N elements is partitioned in only one subset of N
elements. There is only one block prototype which equals
the category prototype PJ . More formally:

exp(−dα
ie )

e∈E
(Q)
X

The traditional models Formally, the expressions
(4) and (6) can be considered as special cases of the
general expression (9). The same holds for expressions
(5) and (8): they are special cases of expression (11).
The varying abstraction model reduces to the traditional
models when a specific partition is chosen for each category.
The exemplar model follows when each category J
with N elements is partitioned in N subsets of one element each. The block prototypes therefore equal the
exemplars. More formally:

• Fq = {jq } so Rq = 1 for every q

Making use of equations (2), (3) and (10), we finally find
that
(Q)
X

A key role is played by Q: it denotes the number of
blocks and hence the number of block prototypes (or
pseudo-exemplars) that are used to represent a category.
On a conceptual level, Q indexes the level of abstraction
(higher Q means lesser abstraction). Since Q is allowed
to vary from 1 to N for each category and Q counts the
number of prototypes used to represent a category, the
varying abstraction model is a formalization of the idea
that people use multiple prototypes.
The varying abstraction model makes clear that the
exemplar and prototype model are extreme pseudoexemplar models and are therefore of the same nature
as the intermediate pseudo-exemplar models. However,
for ease of exposition, the term pseudo-exemplar model
will only be used to refer to the intermediate models, not
to the extreme ones.

wk |xik −

(Rq )
1 X
xjk |r ]1/r )α ).
Rq

• Fq = J so Rq = N for every q
• E = {PJ }.

j∈Fq

(11)
Model family One specific partition of each category
picks out one specific model. Such a model is called
a pseudo-exemplar model. The two extreme partitions
(i.e. Q = N for each category and Q = 1 for each category) correspond to the two extreme pseudo-exemplar
models (i.e. the exemplar model and the prototype
model, respectively). The other, intermediate partitions correspond to intermediate pseudo-exemplar models. Therefore, the varying abstraction model is not just
a model but it is a family of models.
3
Alternative names for pseudo-exemplar are superexemplar or sub-prototype. They are all used interchangeably.
4
To simplify the notation, the index q is dropped most of
the time.

2279

The pseudo-exemplar models Every partition defines or corresponds to a certain model. How many nonextreme (i.e. 1 < Q < N ) partitions (and hence models)
should one consider when fitting the model to a data set?
In the ideal case, one might explore all possible partitions of the N stimuli in a category J. However, this
strategy is not always feasible. The number of possible
partitions of a set of N elements is given by the Bell
number of N (denoted as BN ). This number increases
very rapidly.5 In a categorization task with two categories A and B with NA and NB exemplars respectively,
this implies fitting BNA × BNB different models. When
more than, say, eight, stimuli per category are used, the
number of possibilities to consider becomes intractable.
5

For instance, B6 equals 203 and B10 equals 115975.

A straightforward way to keep the number of partitions within practically feasible boundaries is to select
at every level of abstraction only one partition. Instead
of blindly considering all partitions, we could limit ourselves to only one partition for each category for every
value of Q. This partition can for example be selected
through clustering. This reduces the number of different
models to be fit to NA × NB . An application of this approach in the context of natural language can be found
in Verbeemen, Storms and Verguts (in press).
When practically possible, fitting all possible models
is a fruitful strategy. It has the clear advantage over
the clustering approach that no assumptions have to be
made about spatial representation or about which exemplars should or should not be merged together. What
drives clustering is inferred, not imposed.
The main advantage of the pseudo-exemplar models
is that they allow for sensitive modeling. They allow for
adaptation to category complexity, category experience
and individual skills.

where pn is proportion of A responses for stimulus n.
The maximum likelihood algorithm looks for the θ
that most likely have produced the observed responses.
Therefore, it should seek those parameter values that
maximize this likelihood. Assuming a binomial distribution, this likelihood equals
N µ ¶
Y
S
Lik(θ) =
P (A, n | θ)yn P (B, n | θ)S−yn , (13)
y
n
n=1

where yn denotes number of subjects choosing category
A for stimulus n. For computational efficiency, it is better to look for the θ that maximizes the natural logarithm of this likelihood. Hence the function to be maximized in the maximal likelihood algorithm is the loglikelihood7 :
Loglik =

N
X

[yn ln P (A, n | θ) + (S − yn ) ln P (B, n | θ)].

n=1

A first data set of a category learning experiment with
artificial stimuli has been analyzed using the varying abstraction model and corresponding Matlab algorithms. This preliminary analysis indicated that a
pseudo-exemplar model outperformed the traditional
prototype and exemplar models.

(14)
When fitting the varying abstraction model to a data
set, the algorithm seeks, among all possible partitions of
each category, the parameter values that minimize SSE
or maximize Loglik. The partition yielding the smallest
minimal SSE or largest maximal Loglik of all the possible partitions corresponds to the pseudo-exemplar model
that best accounts for the categorization process.8

Matlab fitting algorithms

Category learning experiment

Determining to what extent the model can account for
category-related behavior is done by fitting all the models of the varying abstraction model family to a data
set.
A typical categorization experiment consists of a training phase and a test phase. In the test phase, N stimuli
are presented to S subjects. Each subject classifies every
stimulus as either A or B. In the following, P (A, i | θ) is
the varying abstraction model’s estimate of the probability of responding A to i, given the parameters θ. The
expression for this probability is obtained by combining
expressions (1) and (11).
We have developed two Matlab algorithms to fit prototype, exemplar and all pseudo-exemplar models and
compare the performance of the different models. Both
least squares and maximum likelihood methods are used
to estimate the model’s unknown parameters.6 These
parameters are the response bias βA , the scaling parameter c and D − 1 attention weights wk . All these parameters are, for the sake of brevity, summarized in the
parameter vector θ.
The least squares algorithm looks for the θ that most
accurately describes the observed responses. The algorithm seeks those parameter values that minimize the
sum of squared errors, that is

The proposed model’s performance was tested in a categorization experiment using the well-known 5-4 structure
(Medin & Schaffer, 1978; Smith & Minda, 2000).

Evaluating the model

SSE(θ) =

N
X

(pn − P (A, n | θ))2 ,

(12)

n=1
6
A very useful tutorial on maximum likelihood estimation
is Myung (2003).

2280

Subjects Twenty-four first year university students
participated for course credit.
Stimuli Stimuli were constructed according to the 54 category structure from Medin and Schaffer (1978).
The stimuli were artificial sheep, varying on four dimensions: eyes (open or closed), fleece (four or five curls),
feet (black or white) and tail (rounded or starred). During the training phase, subjects only encountered the five
stimuli of category A and the four stimuli of category B.
During the transfer phase, all 16 stimuli were presented.
Procedure To motivate the subjects, the categorization task was presented as a sheepdog game. The participants were asked to drive the sheep to the correct
meadow. A-sheep should be driven to the left and Bsheep to the right. In each trial, a sheep appeared on
P
The additive constant N
n=1 [ln S! − ln(S − yn )! − ln yn !]
does not depend on θ and therefore is dropped.
8
Both measures only take descriptive accuracy into account. To avoid overfitting, model complexity should be
taken into account as well. Although all the models of
the varying abstraction model family have the same number of parameters with the same range, the functional form
of these parameters differ (i.e. the way the parameters are
combined). There are tools available combining goodness-offit with model complexity, such as Bayes factors and minimum description length (Myung & Pitt, 1997; Pitt, Myung
& Zhang, 2002).
7

the screen, staring at a certain direction. The participant had to evaluate the correctness of the initial staring
direction, by pressing button 1 for a correct and button 2
for a wrong direction. Each sheep appeared twice, once
in the correct direction and once in the incorrect direction. Hence there were 18 trials in the training phase
and 32 trials in the transfer phase. The order of appearance of the sheep was randomized. In the training phase,
every trial was followed by “good” or “false”. Feedback
was omitted in the transfer phase.
Results Both categories are small, so all possible partitions/models could be examined. Since there are 16
possible partitions for a set of four elements and 52 for a
set of five, 780 different models were fit to the data. In
principle, four different families of models could be fitted, since both r and α can take the values 1 or 2. In the
preliminary analysis presented here, both r and α were
set to 1. The summary fits are presented in Table 1.
Table 1: Summary fits of the prototype (PM), exemplar
(EM) and the best pseudo-exemplar (PE) model under
the least squares estimation method.
minimal SSE
r2
parameter βA
parameter c
parameter w1
parameter w2
parameter w3
parameter w4

PM
0.13
0.93
0.58
7.53
0.24
0.00
0.37
0.39

EM
0.09
0.95
0.52
6.88
0.20
0.10
0.47
0.24

PE
0.03
0.99
0.09
11.31
0.17
0.14
0.24
0.44

Discussion This preliminary analysis with small categories revealed that, as expected, the exemplar model
outperformed the prototype model. More importantly,
it also revealed that a pseudo-exemplar model outperformed both traditional models.9 The best pseudoexemplar model corresponds to the following partition
of the categories, using the labels as described in Medin
and Schaffer (1978): category A is divided in three clusters (A1 and A4; A2; A3 and A5) and category B remains one simple cluster. It is not surprising at all that
A2 is singled out since A2 is the stimulus that is the least
similar to the prototype of category A.

General Discussion
The model proposed in this paper elegantly unifies two
traditional formal models for categorization. Further, it
gives rise to a set of new models, called pseudo-exemplar
models. All these models, traditional and new, are formalized along the continuum of abstraction.
9
In fact, there were several pseudo-exemplar models that
outperformed the traditional models. Only the best one is
reported here.

2281

Model performance
Analysis of a categorization experiment indicated that
a pseudo-exemplar model outperformed the traditional
models. It is important to note that this finding does
not lead to the conclusion that the varying abstraction
model outperformed the exemplar and prototype models. In fact, we would be able to make that conclusion
even before having a look at any data set at all. For,
the varying abstraction model includes both traditional
models, so its performance is at least as good as the performance of the traditional models. Of course, this comparison would not be a fair one. We do conclude however
that the varying abstraction model can single out a new
pseudo-exemplar model (or several new pseudo-exemplar
models), that clearly outperforms the traditional ones.
The comparison between this specific pseudo-exemplar
model and the traditional models is a fair one, or at least
as fair as the comparison between the exemplar model
and the prototype model.

Future Directions
Due to the pseudo-exemplar models, the varying abstraction model allows for sensitive modeling. This sensitivity makes the varying abstraction model highly useful
for investigating in full detail Smith and Minda’s (1998)
findings that category representation may change during
the learning process. In fact, they found that with large
categories, a prototype model yields better accounts of
the initial phases of categorization, while exemplar models yield better accounts in later stages of the learning
process. One can expect that, using the varying abstraction model, better fits will be obtained for models with
more pseudo-exemplars as learning proceeds.
Apart from doing more elaborate modeling of categorization decisions, the varying abstraction model can easily be extended to account for typicality ratings. Nosofsky (1988) contrasted prototype and exemplar accounts
of rated typicalities. Further research will investigate
whether intermediate abstraction levels account better
for rated typicalities than extreme abstraction levels do.
Another extension could be the account of response
times for exemplars in a speeded categorization task, for
instance along the lines of Nosofsky and Palmeri’s (1997)
exemplar-based random walk model.
A theoretical issue is the connection between the proposed varying abstraction model and related models.
The idea of considering the prototype and exemplar
models as extremes along a continuum has been adopted
in other models. The rational model (Anderson, 1991),
SUSTAIN (Love, Medin & Gureckis, 2004) and the mixture model (Rosseel, 2002) share with the varying abstraction model the idea that a category is represented
by multiple prototypes, but show clear differences with
the varying abstraction model too. The varying abstraction model is also closely related to a general-rule-plusexception model such as RULEX (Nosofsky, Palmeri &
McKinley, 1994). Investigating exactly how these models and the varying abstraction model differ in principle,
formalization and performance is important work for the
future.

Acknowledgments
The research reported here is part of an interdisciplinary
research project sponsored by the Research Council of
the University of Leuven (IDO/02/004), given to the second author. The authors wish to thank Michael D. Lee
and Daniel Navarro.

References
Anderson, J. (1991). The adaptive nature of human categorization. Psychological Review, 98, 409–429.
Ashby, F. G., & Maddox, W. T. (1993). Relations between prototype, exemplar, and decision bound models of categorization. Journal of Mathematical Psychology, 37, 372–400.
Borg, I., & Groenen, P. J. F. (1997). Modern multidimensional scaling. New York: Springer.
Hampton, J. A. (1993). Prototype models of concept
representations. In I. Van Mechelen, J. A. Hampton,
R. S. Michalski, & P. Theuns (Eds.), Categories and
concepts: Theoretical views and inductive data analysis. London: Academic Press.
Lakoff, G. (1987). Women, fire and dangerous things:
What categories reveal about the mind. Chicago: University of Chicago Press.
Lee, M. D. (2001). Determining the dimensionality of
multidimensional scaling representations for cognitive
modeling. Journal of Mathematical Psychology, 45,
149–166.
Love, B. C., Medin, D. L., & Gureckis, T. M. (2004).
SUSTAIN: A network model of category learning.
Psychological Review, 111, 309–332.
Medin, D. L., Altom, M. W., & Murphy, T. D. (1984).
Given versus induced category representations: Use of
prototype and exemplar information in classification.
Journal of Experimental Psychology: Learning, Memory, and Cognition, 10, 333–352.
Medin, D. M., & Schaffer, M. M. (1978). Context theory
of classification learning. Psychological Review, 85,
207–238.
Minda, J. P., & Smith, J. D. (2001). Prototypes in category learning: The effects of category size, category
structure, and stimulus complexity. Journal of Experimental Psychology: Learning, Memory and Cognition,
27, 775–799.
Myung, I. J. (2003). Tutorial on maximum likelihood
estimation. Journal of Mathematical Psychology, 47,
90–100.
Myung, I. J., & Pitt, M. A. (1997). Applying Occam’s
razor in modeling cognition: A Bayesian approach.
Psychonomic Bulletin & Review, 4, 79–95.
Nosofsky, R. M. (1984). Choice, similarity, and the context model of classification. Journal of Experimental Psychology: Learning, Memory, and Cognition, 10,
104–114.

2282

Nosofsky, R. M. (1986). Attention, similarity, and the
identification-categorization relationship. Journal of
Experimental Psychology: General, 115, 39–57.
Nosofsky, R. M. (1988). Exemplar-based accounts of
relations between classification, recognition, and typicality. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 14, 700–708.
Nosofsky, R. M. (1992). Exemplars, prototypes, and
similarity rules. In A. F. Healy, S. M. Kosslyn, &
R. M. Shiffrin (Eds.), From learning theory to connectionist theory: Essays in honor of William K. Estes,
Vol. 1. Lawrence Erlbaum, Hillsdale, NJ.
Nosofsky, R. M., & Palmeri, T. J. (1997). An exemplarbased random walk model of speeded classification.
Psychological Review, 104, 266-300.
Nosofsky, R. M., Palmeri, T. J., & McKinley, S.C.
(1994). Rule-plus-exception model of classification
learning. Psychological Review, 101, 53–79.
Pitt, M. A., Myung, I., & Zhang, S. (2002). Toward a
method of selecting among computational models of
cognition. Psychological Review, 109, 472–491.
Rosch, E. (1978). Principles of categorization. In E.
Rosch & B. B. Lloyd (Eds.), Cognition and categorization. Erlbaum, Hillsdale, NJ.
Rosseel, Y. (2002). Mixture models of categorization.
Journal of Mathematical Psychology, 46, 178–210.
Smith, J. D., & Minda, J. P. (1998). Prototypes in the
mist: The early epochs of category learning. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 24, 1411–1436.
Smith, J. D., & Minda, J. P. (2000). Thirty categorization results in search of a model. Journal of Experimental Psychology: Learning, Memory, and Cognition, 26, 3–27.
Smith, J. D., & Minda, J. P. (2002). Distinguishing
prototype-based and exemplar-based processes in dotpattern category learning. Journal of Experimental
Psychology: Learning, Memory and Cognition, 28,
800–811.
Smits, T., Storms, G., Rosseel, Y., & De Boeck, P.
(2002). Fruits and vegetables categorized: An application of the generalized context model. Psychonomic
Bulletin and Review, 9, 836–844.
Verbeemen, T., Storms, G., & Verguts, T. (in press).
Varying abstraction in categorization: a K-means approach. Proceedings of the 27th Annual Conference of
the Cognitive Science Society.

