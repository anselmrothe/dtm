UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Extended Theory of Human Problem Solving
Permalink
https://escholarship.org/uc/item/246193x3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Langley, Pat
Magnani, Lorenzo
Schunn, Christian
et al.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     An Extended Theory of Human Problem Solving
                                     Pat Langley (langley@csli.stanford.edu)
                                     Seth Rogers (srogers@csli.stanford.edu)
                                            Computational Learning Laboratory
                                    Center for the Study of Language and Information
                                       Stanford University, Stanford, CA 94305 USA
                          Abstract                                dynamics, as well as addressing the observed behavioral
                                                                  differences between domain experts and novices.
   Human problem solving has long been a central topic
   in cognitive science. We review the established theory            The theory makes a number of claims about hu-
   in this area and note some phenomena it does not ad-           man cognition. The most basic is that problem solv-
   dress. In response, we present an extended framework,          ing involves the mental inspection and manipulation of
   cast within a theory of the cognitive architecture, that       list structures. Newell and Simon (1976) later refined
   provides one account of these phenomena and that we            this into their physical symbol system hypothesis, which
   illustrate using the familiar Tower of Hanoi puzzle. We
   conclude by discussing other extensions to the standard        states that symbolic processing is a necessary and suffi-
   theory and directions for future research.                     cient condition for intelligent behavior. Another central
                                                                  claim, termed the problem space hypothesis, is that prob-
                                                                  lem solving involves search through a space of candidate
               Introductory Remarks                               states generated by operators.
Research on human problem solving has a venerable his-               A more detailed aspect of the theory is that, in
tory that played a central role in the creation of cognitive      many cases, problem solvers utilize means-ends analysis
science. Early studies of problem-solving behavior on             (Newell & Simon, 1972). This class of search methods
puzzles and other novel tasks led to many insights about          involves a combination of selecting differences between
representation, performance, and learning that now un-            the desired and current states, selecting operators that
derpin the field. Moreover, computational models devel-           will reduce the chosen differences, and either applying
oped from these studies remain some of the most detailed          the operators or creating subproblems to transform the
and precise accounts of human cognition.                          current states into ones in which they can apply. This
   However, the past decade has seen considerably less            requires one to chain backward from aspects of the goal
attention paid to this important topic, presumably be-            state to find relevant operators and determine useful sub-
cause many researchers believe that it is sufficiently well       goals. However, with experience this novice strategy is
understood. In contrast, we maintain that the standard            replaced in experts with forward chaining that leads di-
theory of problem solving, although basically accurate,           rectly to the goal (Larkin et al., 1980).
is still incomplete, and we need additional work, at the             Nevertheless, closer analyses of human behavior on
level of both theoretical principles and specific computa-        novel tasks have suggested that this story is incomplete
tional models, to extend our understanding of this com-           and that the actual situation is more complicated. Here
plex cognitive activity.                                          we make some additional observations that are not ad-
   In this paper, we review traditional accounts of prob-         dressed by the standard theory.
lem solving and note some important omissions that re-
quire further effort. After this, we present an extended             • Problem solving occurs in a physical context. Puz-
framework that we have embedded in Icarus, a compu-                     zles like the Tower of Hanoi are typically presented
tational theory of the human cognitive architecture. We                 in some physical form, with solutions relying on
then illustrate our points using a classic task, the Tower              manual actions and tests for legal moves requiring
of Hanoi. In closing, we discuss other variations on the                visual inspection. This physical setting simplifies
traditional theory and topics for future research.                      the task by providing an external memory, but it
                                                                        also introduces irrelevant features.
  The Standard Problem Solving Theory                                • Problem solving abstracts away from physical de-
The standard theory of problem solving, initially out-                  tails, yet must return to them to implement the solu-
lined by Newell, Shaw, and Simon (1958), focuses on                     tion. For instance, when solving the Tower of Hanoi,
how humans respond when they are confronted with un-                    humans appear to search through an abstract prob-
familiar tasks. Early work focused on abstract problems                 lem space that describes states in terms of disk-peg
like proving theorems in propositional logic and solving                configurations and operators as transitions between
the Tower of Hanoi puzzle. Later research adapted the                   them. They ignore the details of grasping and mov-
framework to explain cognition in semantically rich do-                 ing required to demonstrate the solution, but they
mains like solving word problems in physics and thermo-                 can execute these actions when necessary.
                                                             1242

  • Problem solving is seldom a purely mental activity,         has attributes that describe its height, width, and posi-
     but rather interleaves reasoning with execution. In        tion, and the hand also has a status of open or closed.
     the Tower of Hanoi, the problem solver may reason              Like other cognitive architectures, Icarus operates in
     backward to select an intended move, but typically         cycles, but processing involves more than cognitive ac-
     makes that move as soon as it is legal, without wait-      tivity. On each cycle, the agent perceives objects in its
     ing to make sure that he can solve the remainder of        immediate environment and their descriptions are de-
     the problem from that point.                               posited into a short-lived perceptual buffer. The sys-
  • Eager execution of partial plans can lead the problem       tem may also carry out actions associated with primi-
     solver into physical dead ends that require restarting     tive skills; for the Tower of Hanoi, these correspond to
     the task . For the Tower of Hanoi, initially moving        grasping and ungrasping a disk, as well as moving the
     the smallest disk to the goal peg on an even-disk          hand up, down, left, or right. Naturally, we could model
     problem means one cannot solve the puzzle without          both perception and action at a finer granularity, but
     later retracting this objective. However, once the         this level is sufficient to make our point about the em-
     problem solver has made such an execution error,           bodied nature of problem solving.
     he is unlikely to repeat it on later attempts.                 Of course, the situated cognition movement has also
  • Learning from successful solutions transforms back-         argued that human behavior occurs in a physical setting,
     ward chaining search into informed skill execution.        with Zhang and Norman (1994) even reanalyzing behav-
     When a person first solves the Tower of Hanoi, he          ior on the classic Tower of Hanoi puzzle in these terms.
     resorts to means-ends analysis, but sufficient expe-       Newell and Simon (1972, pp. 800–803) were quite aware
     rience on the task replaces this with an automatized       that much of their subjects’ behavior occurred in this
     execution procedure that involves no search.               context; they simply chose not to distinguish between
                                                                internal and external states in their models. However,
Naturally, because these facets of human problem solving        we have developed a cognitive architecture that treats
have not received attention as interesting phenomena,           the physical side of problem solving as a central tenet, as
computational models have also tended to ignore them.           contrasted with frameworks like Soar and ACT-R, which
In the next section, we present an expanded theory of           view embodiment as peripheral to cognitive processing.
problem solving that begins to remedy these oversights.
   We should clarify that do not we view our framework          Abstraction from Physical Details
as inconsistent with the standard theory, but rather as         As noted, although humans have the perceptual-motor
augmenting it. Neither are we the first to raise the need       abilities necessary to execute solutions to tasks like the
for such extensions, as we note below when we discuss           Tower of Hanoi, their mental problem solving occurs at
each issue at more length. However, we are the first            a more abstract level. Icarus models this capability in
to address them in a consistent manner within a the-            two ways. One involves a long-term conceptual mem-
ory of the human cognitive architecture. Moreover, we           ory that, on each cycle, recognizes instances of generic
note that the two best-known frameworks – Soar (Laird,          situations and adds them to a short-term memory that
Rosenbloom, & Newell, 1986) and ACT-R (Anderson,                contains current beliefs about the agent’s environment.
1993) – have been augmented to address some of these            For instance, our model for the Tower of Hanoi includes
issues, but we view these changes as retrofits rather than      concepts for noting when a peg is empty and when a disk
deeper revisions at the architectural level.                    is the smallest on a peg.
                                                                    The other involves a long-term skill memory that de-
  An Extended Problem Solving Theory                            scribes higher-level skills, like moving a disk, in terms of
                                                                subskills like grasping a disk, lifting it off a peg, carry-
We are now ready to present an extended theory of hu-
                                                                ing it from one peg to another, lowering the disk onto a
man problem solving that moves beyond the abstract
                                                                peg, and ungrasping it. On each cycle, the architecture
framework provided by the standard version. This forms
                                                                selects for execution a path through the skill hierarchy
a key part of Icarus, an account of the human cogni-
                                                                with application conditions that match. These skills are
tive architecture. We will present only those aspects
                                                                durative, in that they may take many cycles to complete;
of Icarus relevant to problem solving, but we have pre-
                                                                for example, this holds for carrying a disk from one peg
sented more complete descriptions elsewhere (Choi et al.,
                                                                to another in our Hanoi environment. We assume the ex-
2004). We will use the phenomena listed above to orga-
                                                                ecution of such complex skills occurs in an automatized
nize our points.
                                                                way that demands very few attentional resources.1
Physical Setting of Problem Solving                                 Thus, Icarus’ sensory-motor control cycle provides
                                                                both abstract descriptions of the environment and ways
First, Icarus is an architecture for physical, embod-           to execute abstract operators. This lets much of prob-
ied agents. Although the framework incorporates many            lem solving occur at an abstract level, which verbal
ideas from traditional work on cognitive modeling, it           protocols consistently suggest is how humans deal with
maintains that cognition is closely tied to perception and      such tasks. However, our framework is more constrain-
action, and one cannot build an Icarus model that is            ing than the physical symbol system hypothesis, in that
not linked to some environment external to the agent.
For the Tower of Hanoi, we have developed a simulated               1
                                                                      Because Icarus’ hierarchical skills are interpreted reac-
world that contains disks, pegs, and a hand. Each object         tively, they are much less rigid than traditional hierarchies.
                                                            1243

problem states are always grounded in real or imagined          Problem Restarts
physical states, and problem-space operators always ex-
                                                                Of course, the disadvantage of this mixed strategy is
pand to primitive skills with executable actions. We be-
                                                                that the problem solver may make an incorrect choice,
lieve a more accurate claim is that cognition relies on a
                                                                such as which subgoal it should achieve first. Heuristics
symbolic physical system, which comes closer to Johnson-
                                                                can reduce the chances of this happening, but they can
Laird’s (1989) views on thinking with mental models.
                                                                seldom eliminate it entirely. Traditional systems, includ-
                                                                ing the General Problem Solver (Newell & Simon, 1972),
Interleaved Cognition and Execution                             responded by backtracking in the problem space. This
In addition to modules for conceptual inference and skill       produces a systematic strategy of depth-first search that
execution, Icarus includes a module for means-ends              does not match the details of human problem solving.
analysis. As in traditional accounts, we assume the prob-          Icarus engages in a limited form of depth-first search,
lem solver begins with a declarative goal that describes        but only if it can accomplish this by pushing subgoals
aspects of the desired state. This must be stated as an         onto, or popping them from, the current goal stack.3
instance of some known concept, so that the agent can-          Once it has made a physical move, the system will
not even formulate a problem unless it has a long-term          not consider retracing this step as part of a system-
structure that can recognize its solution. This assump-         atic search. Moreover, the goal stack retains information
tion is novel, but not a difference we will focus on here.      about subgoals the system has accomplished in pursuit
   Skills contain not only application conditions and sub-      of higher-level goals, and it does not consider skills that
skills, but also descriptions of their expected effects. If     would undo them. Thus, the agent may find itself in sit-
the agent has a skill in long-term memory that would            uations where it has achieved some subgoals necessary
achieve the goal, it checks to see whether that skill can       for a solution but where it has no actions available.
be applied in the current setting. If so, then the agent           In such cases, the problem solver gives up on its cur-
simply executes the stored solution in the environment.         rent attempt and starts the problem again. Anzai and
But if it lacks such a skill, as it would for an unfamiliar      Simon (1979) note two instances of this strategy in their
task, it resorts to means-ends problem solving.                  protocols, and many readers will recall taking such steps
   In this case, two situations can arise. The agent may         on complex or tricky tasks. Restarting on a problem re-
know a skill that would achieve the goal concept, but            quires physically moving it to the initial configuration,
it cannot yet be applied. Here the module adds the in-          which can be viewed as a form of backtracking, but it
stantiated precondition of the skill, which is stated as        is certainly not the systematic version usually discussed
a single concept, to a goal stack, then repeats the pro-        in the literature. Icarus assumes that this restarting
cess by looking for skills that will achieve this subgoal.      process plays a key role in human problem solving.
Alternatively, the agent may have no skill that achieves           We should note one earlier model, reported by Jones
the goal concept, in which case it examines the concept         and Langley (1995), combined restarts with means-ends
definition and adds one of its unsatisfied subconcepts to       analysis. However, their Eureka system carried out
the goal stack, making it the focus of problem solving.         purely mental problem solving and had limited access to
   This process follows the standard theory fairly closely,     memories of previous solution attempts, which meant it
but it diverges when the agent retrieves a skill that can       could make the same errors in choice repeatedly. Hu-
be applied in the current state. Unlike traditional mod-        mans may retrace some errorful steps, but they also try
els, Icarus does not apply the skill mentally and con-           to avoid these choices on later attempts. To model this
tinue reasoning from the new internal state. Instead, it        tendency, Icarus stores the goal stack associated with
executes the skill physically in the environment, inter-        a move later judged to be mistaken and uses it to elim-
rupting cognitive activity for as many cycles as needed,        inate the move from consideration on successive trials.
then resumes internal problem solving in the context of         One might view this as systematic search across problem
the new situation. This may lead to more execution, if          attempts, but it still differs from most models of problem
the precondition of the next skill on the stack is met, or      solving and, we hold, comes closer to human strategies.
to more subgoaling, if more remains to be achieved.
   As a result, Icarus never retains in goal memory a
                                                                Learning from Problem Solutions
complete plan about how it will solve the problem, but          Icarus’ storage of long-term structures about what
only a single path backward through its concept and skill       moves to avoid constitutes a simple form of learning, but
definitions that would lead toward its initial goal. The        the traces are highly specific and meant only to avoid
advantage of this scheme is that it places a much lighter       previous errors. However, people can also acquire more
memory load on the problem solver by using the envi-            general knowledge about how to solve a class of prob-
ronment as an external store.2 Again, this account does         lems, and the architecture incorporates a mechanism
not conflict with Newell and Simon, who never claimed           with this capability. This learning method is interleaved
that problem solving took place entirely in the human           with performance, it operates in an incremental man-
mind, but it does contrast with most follow-on efforts.         ner, and it is cumulative in that it builds on knowledge
                                                                acquired earlier, all characteristics of human behavior.
    2
      We should note that humans can handle simple problems
                                                                    3
mentally, so a compromise model like that reported by Gun-            We are also exploring memory-limited goal stacks that
zelmann and Anderson (2003) may offer a better account.          are more consistent with recent studies of problem solving.
                                                            1244

   The learning process is driven by impasses that occur               Table 1: Some Icarus skills for the Tower of Hanoi.
during execution and problem solving, which in turn lead
to pushing entries onto the goal stack. Because Icarus              (pickup-putdown (?disk ?from ?to)
aims to avoid such impasses in the future, the archi-                 :start     ((pickup-putdownable ?disk ?from ?to))
tecture creates a new skill whenever it pops the stack                :ordered ((grasp-disk ?disk)
upon achieving its current goal. However, recall that the                         (lift-disk ?disk)
means-ends module creates two distinct types of subgoal                           (carry-disk ?disk ?from ?to)
                                                                                  (lower-disk ?disk ?to)
that require slightly different forms of learning.                                (ungrasp-disk ?disk))
   One type occurs when the agent selects a skill instance            :effects ((no-disk-on ?from)
S2 to achieve a goal G, but, upon finding its start con-                          (on-peg ?disk ?to)
dition unsatisfied, selects another skill instance S1 so as                       (only-disk-on ?disk ?to)))
to achieve it. When the agent has executed both skills              (unstack-putdown (?d1 ?d2 ?from ?to)
successfully and reached the goal, Icarus constructs a                :start     ((unstack-putdownable ?d1 ?d2 ?from ?to))
                                                                      :ordered ((grasp-disk ?d1)
new skill that has the same name and arguments as the                             (lift-disk ?d1)
goal concept G, the two skills S1 and S2 as ordered sub-                          (carry-disk ?d1 ?from ?to)
skills, and a start condition that is the same as that for                        (lower-disk ?d1 ?to)
S1. In addition, arguments and other peceived objects                             (ungrasp-disk ?d1))
are replaced by variables in a consistent manner.                     :effects ((on-peg ?d1 ?to)
                                                                                  (top-disk-on ?d2 ?from)
   The other form of learning occurs when the agent finds                         (only-disk-on ?d1 ?to)))
no skill to achieve a goal G, and thus creates subgoals             (lift-disk (?disk)
for the unsatisfied conditions of G’s concept definition.             :start     ((on-peg ?disk ?from)
If the agent achieves each subgoal in turn, the architec-                         (no-smaller-disk-on ?disk ?from)
ture constructs a new skill that has the same name and                            (grasped ?disk))
arguments as the goal concept G, the subgoals that are                :actions ((*vertical-move ?disk 1))
                                                                      :effects ((above-peg ?disk ?from)))
achieved as ordered subskills, and a start condition based
on the subconcepts of G that held initially. Both mecha-            (carry-disk (?disk ?from ?to)
                                                                      :percepts ((peg ?from xpos ?fxpos)
nisms name new skills after the goals they achieve, which                         (peg ?to xpos ?txpos))
leads naturally to disjunctive and recursive definitions.             :start     ((above-peg ?disk ?from)
   After learning, when the agent encounters a problem                            (grasped ?disk))
that is isomorphic to one it has solved before, it avoids             :actions ((*horizontal-move ?disk ?fxpos ?txpos))
                                                                      :effects ((above-peg ?disk ?to)))
an impasse by accessing the stored skill. The architec-
ture instantiates this skill and executes it, continuing           (lower-disk (?disk ?to)
                                                                      :start     ((above-peg ?disk ?to)
until achieving the goal that led to its retrieval. Similar                       (no-disk-on ?to)
events transpire if the system encounters any subprob-                            (grasped ?disk))
lem it has solved previously, which can produce transfer              :actions ((*vertical-move ?disk -1))
to new tasks with familiar subtasks. Note that the agent              :effects ((on-base ?disk ?to)))
transitions from the backward-chaining search of means-            (lower-disk (?disk1 ?to)
ends analysis to forward execution. This is consistent                :start     ((above-peg ?disk1 ?to)
                                                                                  (on-peg ?disk2 ?to)
with observations of novice-expert shifts (Larkin et al.,                         (no-smaller-disk-on ?disk1 ?to)
1980), but clarifies that the “forward chaining” occurs                           (no-smaller-disk-on ?disk2 ?to)
in the environment, rather than in the agent’s mind.                              (grasped ?disk1))
                                                                      :actions ((*vertical-move ?disk1 -1))
                                                                      :effects ((on-disk ?disk1 ?disk2)))
        A Model for the Tower of Hanoi
To illustrate these ideas further, we will consider an
Icarus model for the Tower of Hanoi puzzle. The sys-                percepts. These do not satisfy its goal of having a tower
tem begins with some 14 concepts that describe config-              composed of the same disks on the rightmost peg, so it
urations of disks and pegs, organized in a hierarchy that           attempts to retrieve a skill that will transform its current
has five levels. We also provide the system with five               state into the desired one. Failing to retrieve such a skill,
primitive skills for manipulating disks, along with four            it decomposes the tower goal into three subgoals, each of
higher-level skills that correspond to problem-space op-            which involves getting a separate disk on the rightmost
erators,4 some of which appear in Table 1. In addition,            peg, and selects among them.
we embed the system in a simulated environment with                    Because the model knows it can move the smallest
three disks and three pegs, which it perceives on each cy-         disk to the goal immediately, it selects this alternative,
cle and can alter using the actions in its primitive skills.       although the choice causes problems later. The agent
   The agent begins by perceiving three disks stacked on           selects a skill that will achieve this subgoal and, since its
the leftmost peg, along with two other empty pegs, and             conditions are satisfied, executes it in the environment.
inferring a number of higher-level concepts from these             This skill invokes subskills for grasping, lifting, carrying,
    4
      The system requires four operators because it must dis-      lowering, and ungrasping the disk, most of which take
tinguish between cases in which the moved disk is/is not alone     multiple cycles to complete. When it has finished the
on the ‘from’ peg and between similar cases for the ‘to’ peg.      move, it pops the subgoal from the goal stack.
                                                               1245

   The system next selects the subgoal of getting the sec-        edge compilation in ACT-R (Anderson, 1993). All three
ond smallest disk on the goal peg, along with the skill in-       create new structures that reduce an agent’s efforts on
stance that would move it there. However, the start con-          future tasks. A key difference is that the older architec-
dition for this skill is not matched, so the model pushes         tures determine conditions on learned rules by analyz-
a subgoal onto the stack to achieve it. But it cannot find        ing dependencies in reasoning traces, whereas Icarus
any skill that would accomplish it and not undo the sub-          simply stores generalized elements from its goal stack.
goal regarding the smallest disk it has already achieved.         Moreover, their learning mechanisms eliminate interme-
   As a result, it pops the goal stack and marks the pro-         diate structures to reduce internal processing, whereas
posed skill as failed, then selects another that it hopes         Icarus’ mechanisms operate in a cumulative manner
will move the second smallest disk to the rightmost peg.          that leads to the creation of hierarchical structures.
However, it finds that this skill leads to the same prob-            We have also mentioned Jones and Langley’s (1995)
lem and abandons it as well. Thus, the system pops the            Eureka, which incorporated a modified means-ends
subgoal for the second smallest disk and replaces it with         problem solver that combined search down a single path
the subgoal of moving the largest disk to the goal peg.           with restarts upon failure. Their model did not learn
But this encounters the same problems as before, which            either generalized chunks or hierarchical skills, but it
eventually causes it to abandon this subgoal.                     did include a form of analogical search control based on
   Lacking any further options, the model realizes its first      problem-solving traces accessed through spreading acti-
subgoal was an error, so it stores the context in which it        vation. Icarus stores instances of specific failures, but
made this selection in order to avoid the subgoal in future       uses them in a more limited way to keep from repeat-
attempts. The agent then resets the environment to the            ing mistakes. Eureka also utilized a flexible version of
initial configuration and tries the puzzle again. This time       means-ends analysis which preferred operators that re-
it avoids the previous mistake but instead attempts to            duced more differences and fell back on forward-chaining
get the second smallest disk onto the goal peg, which             search when no other choices were available. We intend
leads to similar problems. After some effort, it decides          to incorporate this idea into future versions of Icarus.
to abandon this attempt and restart the problem again.               Jones and Langley also used their model to account
   The model continues in this fashion, chaining back-            for insight in problem solving, a topic which Ohlsson
ward to select subgoals and skills and executing the lat-         (1992) has discussed at more length. In his treatment,
ter whenever they are applicable. On its eighth attempt,          problem solving sometimes leads to impasses that can-
the agent selects the subgoal involving the largest disk          not be overcome in the initial problem space. Insight
first, followed by the second largest disk, which together        occurs when the problem solver restructures the space
let it solve the problem successfully. Along the way, the         in some manner that makes a solution possible. Ohlsson
system creates a new skill whenever it achieves a subgoal         suggested three distinct restructuring mechanisms that
that requires executing two or more component skills.             can produce this effect. In some ways, his proposal con-
   For the three-disk puzzle, the model acquires 13 skills        stitutes a more radical extension to the standard theory
that build on initial hierarchy. When given the same              than our own, but it also expands on Newell et al.’s orig-
problem, the agent retrieves the relevant learned skill           inal analysis without rejecting its basic contributions.
and executes it without invoking means-ends analysis.                Other researchers have extended the standard frame-
There are two ways to instantiate this skill, one that            work to explain the benefits of diagrams in problem solv-
moves disks to the goal peg and another that moves them           ing. For example, Larkin and Simon (1987) have argued
to the other peg. The model makes the right choice half           that, besides serving as external memories, diagrams re-
the time, in which case it makes the correct sequence of          duce search by grouping elements that are used together,
moves in a reactive manner. When it makes the wrong               utilize location to group information about a given ele-
choice, the model fails and restarts, either to repeat its        ment, and support perceptual inferences that are easy
error or make the right one on the next round. Humans             for humans. This extension seems closely related to our
often make similar errors even after practice on the task.        concern with the embodied context of problem solving,
   We have included this example to illustrate the ways in       although diagrams have a somewhat different character
which Icarus’ behavior is qualitatively consistent with           from the physical structures in many puzzles and games.
the phenomena discussed earlier. Other researchers have
presented models of behavior on the Tower of Hanoi,
including some (e.g., Anderson, 1993) that match closely                          Concluding Remarks
the timing observed on the task. Such detailed modeling
has benefits, but it should not be expected in the early          Although our extended theory of problem solving is
stages of developing a unified theory that covers new             roughly consistent with the phenomena we presented
phenomena, when qualitative evaluation is appropriate.            earlier, our framework would clearly benefit from further
                                                                 evaluation and refinement. There remain many ques-
         Related Theoretical Extensions                           tions about how much humans abstract away from the
                                                                 physical details of the problem setting, how often they re-
We are hardly the first to propose extensions to the stan-       turn to this setting during their search, how tightly they
dard theory of problem solving. For example, Icarus’              integrate cognition with execution, exactly when they
learning mechanisms are closely related to the chunk-            restart on difficult problems, and how rapidly learning
ing process in Soar (Laird et al., 1986) and to knowl-           lets them acquire automatized strategies. Icarus makes
                                                             1246

specific predictions about these issues, but testing them                             References
will require carefully studying traces of problem-solving
behavior with them in mind.                                     Anderson, J. R. (1993). Rules of the mind . Hillsdale,
   We also know the current theory remains incomplete             NJ: Lawrence Erlbaum.
in some ways. For instance, when humans first encounter         Anzai, Y., & Simon, H. A. (1979). The theory of learning
a novel problem, they appear to spend time observing              by doing. Psychological Review , 86, 124–140.
and manipulating the objects involved, presumably to
                                                                Choi, D., Kaufman, M., Langley, P., Nejati, N., & Shap-
construct an abstract representation of the task. Icarus
                                                                  iro, D. (2004). An architecture for persistent reactive
does not model this early learning process, although Go-
                                                                  behavior. Proceedings of the Third International Joint
bet and Simon’s (2001) work on the role of discrimina-
                                                                  Conference on Autonomous Agents and Multi Agent
tion learning in game playing suggests one promising av-
                                                                  Systems (pp. 988–995). New York: ACM Press.
enue. Moreover, people seem to form generalizations not
only about what steps to take in solving problems, but          Gobet, F. & Simon, H. A. (2001). Human learning in
about what steps to avoid. Icarus stores specific mem-            game playing. In M. Kubat & J. Furnkranz (Eds.),
ories to avoid repeating errors, but, unlike Soar and             Machines that learn to play games. Huntington, NY:
Prodigy, it does not form generic rejection knowledge.            Nova Science.
   Modeling these aspects of learning is important, but         Gunzelmann, G., & Anderson, J. R. (2003). Problem
we should also remedy some drawbacks of our perfor-               solving: Increased planning with practice. Cognitive
mance framework. Although Icarus cannot backtrack                 Systems Research, 4 , 57–76.
over execution errors, it does carry out depth-first search
                                                                Johnson-Laird, P. N. (1989). Mental models. In M. I.
within each problem-solving attempt, whereas humans
                                                                  Posner (Ed.), Foundations of cognitive science. Cam-
seem far less systematic. Future versions should incor-
                                                                  bridge, MA: MIT Press.
porate the memory-limited strategy used by Eureka,
which seems closely related to the progressive deepen-          Jones, R., & Langley, P. (1995). Retrieval and learn-
ing scheme observed in chess players. Finally, although           ing in analogical problem solving. Proceedings of the
means-ends analysis has been widely implicated in prob-           Seventeenth Annual Conference of the Cognitive Sci-
lem solving, it is not the only method that humans use            ence Society (pp. 466–471). Pittsburgh, PA: Lawrence
to solve problems. Forward-chaining search appears to             Erlbaum.
dominate on complex tasks like chess in which backward          Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987).
chaining from the goal is impractical. We should add this         Soar: An architecture for general intelligence. Artifi-
ability to Icarus and specify when this form of search is         cial Intelligence, 33 , 1–64.
invoked and how it interacts with means-ends analysis.
                                                                Larkin, J. H., McDermott, J., Simon, D. P., & Simon, H.
   In summary, our extensions to the standard theory
                                                                  A. (1980). Expert and novice performance in solving
have added some important items to our store of knowl-
                                                                  physics problems. Science, 208 , 1335–1342.
edge about human problem solving. These include the
claims that problem solving typically occurs in a physi-        Larkin, J. H., & Simon, H. A. (1987). Why a diagram
cal context yet abstracts away from physical details, that        is (sometimes) worth ten thousand words. Cognitive
problem solving interleaves mental reasoning with exe-            Science, 4 , 317–345.
cution, that this interaction can lead to dead ends which       Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka,
require problem restarts, and that learning transforms            D., Etzioni, O., & Gil, Y. (1989). Explanation-based
backward chaining into informed execution. Moreover,              learning: A problem solving perspective. Artificial
we have embedded these ideas in Icarus, a cognitive               Intelligence, 40 , 63–118.
architecture that we have used to develop models for            Newell, A., Shaw, J. C., & Simon, H. A. (1958). Ele-
behavior on the Tower of Hanoi and other domains.                 ments of a theory of human problem solving. Psycho-
   However, we should remember that the scientific pro-
                                                                  logical Review , 65 , 151–166.
cess itself involves problem solving. The extensions we
have reported here are simply further steps toward a            Newell, A., & Simon, H. A. (1972). Human problem
full understanding of human cognition. These must be              solving. Englewood Cliffs, NJ: Prentice-Hall.
followed by the heuristic application of additional op-         Newell, A., & Simon, H. A. (1976) Computer science as
erators, and possibly even by backtracking, before we             empirical enquiry: Symbols and search. Communica-
achieve this worthwhile but challenging goal.                     tions of the ACM , 19 , 113–126.
                                                                Ohlsson, S. (1992). Information processing explanations
                 Acknowledgements                                 of insight and related phenomena. In M. Keane & K.
This research was funded in part by Grant HR0011-04-              Gilhooly (Eds.), Advances in the psychology of think-
1-0008 from DARPA IPTO and by Grant IIS-0335353                   ing (Vol. 1). London: Harvester-Wheatsheaf.
from the National Science Foundation. Discussions with          Zhang, J., & Norman, D. A. (1994). Representations
Glenn Iba, David Nicholas, Dan Shapiro, and others con-           of distributed cognitive tasks. Cognitive Science, 18 ,
tributed to many of the ideas presented here.                     87–122.
                                                            1247

