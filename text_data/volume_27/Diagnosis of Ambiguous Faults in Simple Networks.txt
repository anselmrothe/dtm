UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Diagnosis of Ambiguous Faults in Simple Networks

Permalink
https://escholarship.org/uc/item/02n5k1c4

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Authors
Goodwin, Geoffrey P.
Johnson-Laird, P.N.

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Diagnosis of Ambiguous Faults in Simple Networks
Geoffrey P. Goodwin (ggoodwin@princeton.edu)
Department of Psychology, Green Hall, Princeton University
Princeton, NJ 08544 USA

P. N. Johnson-Laird (phil@princeton.edu)
Department of Psychology, Green Hall, Princeton University
Princeton, NJ 08544 USA

i.e., in order for it to transmit a signal, it had to receive
activation from every one of its input nodes. Faults were
failures in one or more output nodes to yield a signal. The
participants’ task was to locate the cause of a fault by
performing tests on single connections between pairs of
nodes. They needed to find the single faulty node that
accounted for all and only the observed output failures.
These studies showed that several factors increased the
difficulty of the task, but they did not reveal much about the
initial generation of hypotheses to explain the faults. In order
to investigate this process, we adopted a modified version of
the network task. Our networks were much simpler than those
previously investigated: they had only six nodes (Experiment
1) or seven nodes (Experiment 2), and only a single output
node. We allowed that the connecting nodes could be one of
three sorts of Boolean operator – AND, OR, and OR ELSE.
And the participants were not required to determine the cause
of the fault definitively, but only to formulate a preliminary
hypothesis about what to investigate first in the network in
order to find the fault.

Abstract
We propose a theory of how individuals diagnose faults, and
we report two experiments that tested its application to the
diagnosis of faults in simple Boolean systems. Participants
were presented with simple network diagrams in which a signal
was transmitted from a set of input nodes to an output node, via
a set of connecting nodes. Their task was to detect and diagnose
faults. Experiment 1 showed that individuals tend to diagnose
events closest to an observed inconsistency as the cause of the
fault. Experiment 2 replicated this proximal effect, but also
demonstrated that participants tend to target the proximal node
most often when it fails to transmit a signal. This phenomenon
may occur because individuals construct models of those
situations in which a node works, but leave implicit those
situations in which it does not work. The present results
extend the mental model theory to diagnostic reasoning.

How do individuals diagnose faults in simple systems? If
something goes wrong, what guides their initial hypotheses
about the cause of the fault? In this paper we propose a theory
that explains the diagnosis of faults in simple Boolean
networks, and we report experimental tests of the theory. The
theory assumes that individuals diagnose faults by mentally
simulating the network in a dynamic mental model (see
Johnson-Laird, 1983). It postulates three main principles for
diagnosis. First, individuals assume that causes of faults occur
prior to the fault and as close to it as possible. Hence, they
should locate faults as close as possible to the output of a
network. We refer to this sort of diagnosis as a “proximal”
bias, i.e., the proximal cause is the event that occurs nearest to
the effect. Second, individuals should be more likely to
diagnose faults in the proximal node when it ought to transmit
a signal than when it ought not to. Third, individuals assume
by default that complex components are more likely to go
wrong than simple components. One index of complexity is
the ease of understanding how a component works.
Prior research has investigated fault finding in network
tasks (see e.g., Morrison & Duncan, 1988; Rouse, 1978;
Rouse & Rouse, 1979). Participants were presented with a
matrix of nodes, connected in a variety of different ways. A
set of input nodes was connected by intermediary nodes to a
set of output nodes. Typically, the networks consisted of a
matrix of about 49 nodes (7 by 7). The input nodes each
transmitted a signal through the system, and in the basic form
of the task, each connecting node acted as an AND operator,

Experiment 1
The purpose of the study was to examine the ability of naïve
individuals to detect inconsistencies in the behavior of a
simple network and the nature of their diagnoses. All the
problems used the network shown in Figure 1.
Input nodes

Connecting Nodes

1
2

Output
Node

B
A

3
Direction of signal
Figure 1: The network used in Experiment 1.
The participants were told that inputs are fed into the three
input units, and a signal is transmitted from left-to-right to the
output node. It may or may not produce an output depending
on the particular inputs and on the particular logical
connectives (AND, OR, or OR ELSE) in the two nodes (A
791

the eight possible input patterns (there are eight possible
patterns, given three inputs with two settings each). Each
input pattern was presented twice – once when the network
produced a correct output, and once when it produced an
incorrect output. The full set of the 29 problems that had
incorrect outputs is presented in Table 1. The six sorts of
problem were presented in six separate blocks in a random
order for each participant, as was the order of the problems in
each block. There were six practice problems using the same
connective in both nodes.

and B). On a given trial, the participants were presented with
the inputs, the logical connectives in each node, and the
output, and they first judged whether the input-output
configuration was correct or incorrect. If they judged it as
incorrect, they next indicated which of the two nodes, A or B,
they would prefer to investigate first in an attempt to diagnose
the cause or causes of the fault. This question was designed to
elucidate the principles underlying their diagnostic intuitions.
The theory predicts that they should focus on the node
closest to the output. It further predicts that this bias should be
strongest when that node ought to transmit a signal but in fact
does not. This prediction stems from a known bias to
represent only what is true in reasoning: individuals construct
mental models of propositions in which each model
represents a true possibility, and within each of these true
possibilities, only those clauses that are true within that
possibility are represented (see principle of truth, JohnsonLaird & Savary, 1999). Extending the first component of this
principle to the present domain, we predicted that individuals
should be more likely to construct explicit models of the
conditions under which each node transmits a signal rather
than the conditions under which it does not transmit a signal.
As participants try to diagnose a fault in a network, their
attention should focus on nodes that ought to transmit a
signal, but which in fact do not. There is a mismatch between
the participants’ models of the node, which explicitly
represent only the transmitting possibilities, and their models
of the node’s actual functioning, which is not to transmit a
signal (see the mismatch principle, Johnson-Laird, Girotto, &
Legrenzi, 2004). Of course, there is also a mismatch in the
case where a node ought not to transmit a signal, but in fact
does transmit one, but the theory predicts an asymmetry in
diagnoses because of the tendency to represent explicitly only
the transmitting possibilities. Finally, the theory predicts that
nodes that are the most difficult to process should be
diagnosed as faulty most often.

Table 1: The full set of inconsistent problems used in
Experiment 1, with the percentages of node choices alongside
each node.
No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

Method
Participants. Thirty-nine participants (13 male, 26 female)
from Princeton University were paid $10 or received course
credit for their participation.
Design and Materials. Participants acted as their own
controls, and each performed 58 test problems. All problems
used the network shown in Figure 1. The problems were
divided into six different categories according to the logical
connectives (AND, OR, or OR ELSE) in the two main nodes
in the network. An AND node is one that transmits a signal if
and if only if it receives a signal from all of its inputs. An OR
node is one that transmits a signal if and only if it receives a
signal from at least one of its inputs, or both. An OR ELSE
node is one that transmits a signal if and only if it receives a
signal from at least one, but not both, of its inputs. Given the
three connectives and the two nodes, there are six sorts of
problem given that we examined only those problems in
which the connectives in the two nodes were different. Within
each sort of problem, we selected a representative subset from

Inputs: 1
2 3
0 0 0*
1 0 0
1 1 0
1 1 1
0 0 0
0 1 0
0 1 1
1 1 1*
0 0 0*
1 0 0
1 1 0
0 1 1*
1 1 1
0 0 0
1 0 0
0 1 0
1 1 0
0 1 1
1 1 1
0 0 0
0 1 0
1 1 0*
0 1 1
0 0 0
1 0 0
0 1 0
1 1 0
0 1 1
1 1 1

Output
1
1
0
0
1
1
0
0
1
1
0
1
1
1
0
1
0
0
1
1
0
0
1
1
0
0
1
0
1

Node A

Node B

OR (16)
OR (45)
OR (29)
OR (26)
AND (24)
AND (82)
AND (26)
AND (0)
XOR (21)
XOR (47)
XOR (32)
XOR (34)
XOR (89)
AND (16)
AND (11)
AND (66)
AND (21)
AND (24)
AND (24)
XOR (42)
XOR (45)
XOR (16)
XOR (89)
OR (29)
OR (13)
OR (37)
OR (34)
OR (21)
OR (26)

AND (79)
AND (53)
AND (63)
AND (66)
OR (76)
OR (16)
OR 71)
OR (87)
AND (79)
AND (53)
AND (58)
AND (66)
AND (8)
XOR (84)
XOR (89)
XOR (24)
XOR (55)
XOR (68)
XOR (76)
OR (55)
OR (50)
OR (82)
OR (8)
XOR (61)
XOR (79)
XOR (61)
XOR (63)
XOR (71)
XOR (74)

Note. Consistent with standard logical notation, OR ELSE in
Table 1 is represented by XOR. Problems marked with an
asterisk are ones where the error could be explained only by
an error in Node B. Problems in bold produced aberrant
results (see results and discussion).
Procedure. The participants were tested individually with a
computer running the Eprime program. Each problem
appeared on the screen as a static diagram of the network
(showing the inputs, the logical operator within each
connecting node, and the output), with yellow input and
output nodes indicating that they were “on”, and white nodes
indicating that they were “off”. The participants judged
whether the network was correct or incorrect by pressing “c”

792

there are two potential confounds. First, the proximal node, B,
was always at the top of network. Second, it was connected to
both an input node and the output node, whereas the other
node, A, was not directly connected to the output node.
We examined factors that may modify the proximal bias.
As the theory predicted, the proximal node was chosen more
frequently when it ought to have transmitted a signal but
failed to do so, than when it ought not to have transmitted a
signal, and yet did so (68% vs. 57%, Wilcoxon test, z = 3.13,
p < .01). In other words, the proximal node was more liable to
be diagnosed as faulty when it produced a “miss” rather than
a “false positive”. This difference may arise from
participants’ explicitly constructing a model of the conditions
under which the node should transmit a signal rather than
when it should not.
As the theory also predicts, the participants were more
likely to locate the fault in the proximal node when it was an
OR ELSE node (67%), than when it was an AND node (59%,
Wilcoxon test, z = 2.53, p < .02) or an OR node (55%,
Wilcoxon test, z = 3.2, p < .01). But, there was no difference
between AND and OR nodes (Wilcoxon test, z = .41, p =
.684). As the latency results above show, OR ELSE appears
to be harder to understand than the other two connectives.
Hence, it is a more complex connective for our participants,
and so the participants should infer that a fault is more likely
to occur in its nodes. It may be that mere fluency of
processing exerts a direct effect on diagnosis (see Schwarz,
2004). There were four problems in which node A was
chosen more frequently than node B (nos. 6, 13, 16, 23, in
bold). But, for each of these four networks, the proximal node
transmitted output when it should not have. As we have
already seen, the proximal bias was attenuated for such
problems. In addition, node A was an OR ELSE node for
problems 13 and 23, which made it more liable to be
diagnosed as being in error.
Experiment 1 established that individuals are proficient at
judging the correctness of simple networks. They are faster to
make accurate judgments about consistent networks than
about inconsistent networks. Their preliminary diagnoses
corroborated the theory in three ways. First, they tended to
locate faults in proximal nodes, i.e., those that were closest to
the output revealing that a fault had occurred. Second, they
were more likely to do so when the proximal node failed to
produce output when it should have than when it produced
output when it should not have. Third, they also tended to be
biased towards locating the fault in the proximal node when it
was an OR ELSE node. These nodes are harder to
understand, and so that difficulty may indicate that the node is
complicated, and hence more likely to go wrong.

or “i”, on the keyboard. After each practice problem
participants received feedback about whether they had or had
not made a correct evaluation of the network. In order to
proceed to the main stage of the experiment, participants were
required to perform five out of six practice problems
correctly. For these problems, participants judged whether the
network was behaving correctly or incorrectly, but did not go
on to make a diagnosis for the incorrect networks. They
repeatedly cycled through the same set of practice problems
(in a new random order each time) until this criterion was
achieved. Participants required a mean of 1.79 cycles, and the
modal number of cycles was 1.
In the experiment proper, the participants judged whether
or not a network was correct. But, when they judged that it
was incorrect, they then made a preliminary diagnosis. They
typed “a” or “b” to select node, A or B, as the one that they
would test first in a preliminary investigation into what was
wrong with the network. This judgment was hypothetical, i.e.,
participants did not go on to conduct the test, nor were they
presented feedback about which node was in fact responsible
for the error. The network remained on the screen while the
diagnosis was made.

Results and discussion
The data from one participant were removed as a result of a
computing error. The participants were accurate in their
judgments about whether or not the network was correct
(95% correct). They were faster to make accurate judgments
about correct circuits than about incorrect circuits (6.60s vs.
7.43s, Wilcoxon test, z = 2.82, p < .01). Problems that
included an OR ELSE node appeared to pose an extra
difficulty. The participants took longer to make judgments of
correctness for these circuits (7.25s) than for problems
without an OR ELSE node (6.25s; Wilcoxon test, z = 2.4, p <
.02). This effect occurred for judgments of both correct and
incorrect networks (Wilcoxon tests, z = 2.76, p < .01; z = 2.08
p < .05, respectively). Networks that included OR ELSE
nodes therefore appeared somewhat more difficult to
simulate.
The percentage of node choices for each problem is
displayed in Table 1 (the percentages do not always sum to
100 owing to some participants’ failure to identify the
network as inconsistent, or to their pressing the wrong
button). There was a consensus about which node to
investigate first for the circuits that were incorrect. As the
theory predicts, the participants tended to select the node
closest to the output. This proximal bias was greater than 20%
for 21 out of the 29 incorrect circuits. For problems where
either A or B may have been in error (there were 24 such
problems; and the five remaining problems were ones where
only B could account for the error, see table 1), the difference
between the percentages of A and B choices was 20% across
all subjects (Wilcoxon test, z = 3.3, p < .01), and across all
problems (Wilcoxon test, z = 2.06, p < .05). This proximal
bias may occur because nodes earlier in the network are
considered more reliable, or because individuals prefer to
locate causes as close as possible to their effects. However,

Experiment 2
In order to eliminate the confounds in the previous
experiment, we carried out a second experiment using a more
complicated network of seven nodes (see Figure 2). The
theory yields three predictions. First, individuals should show
the proximal bias, i.e., they should be biased to select node,
D, as the cause of the fault.
793

Connecting
Nodes

Input Nodes

1

with the constraint that the four possible types of input
pattern (0 0; 0 1; 1 0; 1 1) were nearly equally represented
across the whole experiment.

Output
Node

Procedure. The procedure was the same as in Experiment 1.
The mean number of required cycles through the practice
segment was 1.33, and the modal number was 1. When the
participants made their diagnoses in the main part of the
experiment, they indicated which node from A, B, C, or D,
they would test first (by typing “a”, “b”, “c”, or “d”).

B
A

D

C

2

Results and Discussion
Overall accuracy in detecting inconsistencies was high, as in
the first experiment. All problems were performed at greater
than or equal to 75% accuracy. Participants correctly judged
90% of the networks: 89% of the consistent networks and
90% of the inconsistent networks (there was no reliable
difference between these percentages).
Table 2 illustrates the diagnoses made for the main classes
of problems in the experiment (percentages do not sum to 100
owing to participants’ errors). The proximal bias was again
reliable. Across all problems, the percentages of selections of
each node were as follows:
A: 22%
B: 11%
C: 13%
D: 43%
The last node, D, was chosen more often than each of the
other three nodes (all three pairwise Wilcoxon comparisons
were significant, p < 0.01).

Direction of signal
Figure 2: The network used in Experiment 2.
Second, the network again allowed us to investigate how
the tendency to diagnose the proximal node is affected by
mental models of its functioning. As before, we predicted
that the proximal node should be diagnosed as the faulty
component more often when it ought to transmit a signal but
does not do so, than when it ought not to transmit a signal but
does so. This tendency should arise from a bias to represent
only the transmitting possibilities of the proximal node.
Third, networks that include OR ELSE nodes should take
longer to evaluate than those that do not, and hence an
account based on processing fluency predicts that the
proximal node will be identified as the cause of the fault more
often when it is an OR ELSE node.

Method

Table 2: The percentages of node choices depending on the
functioning of the proximal node, D, in Experiment 2.

Participants. Twenty-four participants (15 male, 9 female)
from Princeton University participated for course credit.

Type of problem

Design and Materials. Participants acted as their own
controls, and each performed 27 test problems that all used
the network in Figure 2 and the same three connectives as in
Experiment 1. Of the 27 problems, 23 were incorrect
networks and four were correct networks.
We suspected that the first and last nodes, A and D, would
be most salient, and would be diagnosed as faulty more
frequently than either B or C. B and C are likely to be less
salient since they occupy symmetrical positions in the overall
network structure, and it should therefore be harder to
motivate a choice of one over the other as the cause of the
fault. Hence, in order to test the proximal bias, the
connectives in the first and last nodes were always identical,
whereas the connectives in the middle nodes were always
selected to be different from those in the end nodes. The
problems were further divided into three main categories
according to whether the first and last connectives were: OR,
AND, or OR ELSE. Within each of those three categories,
there were three further sub-types, depending on the
connectives in the middle nodes. Problems were sampled
from each of these nine categories. Different input patterns
are also possible, and we selected these roughly at random,

Networks in which the proximal
node should transmit output but
fails to do so.
Networks in which the proximal
node should not transmit output
but does so in error.
Networks in which the proximal
node was OR ELSE.
Networks in which the proximal
node was not OR ELSE.

Node
A
13

Node
B
10

Node
C
9

Node
D
54

27

13

15

38

15

8

12

53

29

13

13

39

Note. The horizontal line separates the two main sets of
comparisons. Numbers in bold indicate the key comparisons
within each set.
The predictions based on the mismatch principle were
confirmed. The participants were more likely to diagnose the
proximal node, D, when it should have transmitted a signal
but in fact did not, than when it should not have transmitted
but in fact did so (54% vs. 38%, Wilcoxon test, z = 2.3, p <
.05). This again supports our theory that a mismatch between
an explicit model of the situations in which a node will
transmit a signal, and a model of the node’s actual non794

probably relies less on the temporal order of a mental
simulation, and more on the greater cognitive effort required
to change an earlier (as opposed to a later) component of a
network. An earlier change may call for a later change,
whereas a later change need have no effect on what happened
earlier in the network. There may thus be a rational element
to this tendency.
There is less room for a rational interpretation of the
tendency to diagnose faults in the proximal node when it
produced a “miss” rather than a “false positive”. A failure to
transmit a signal is just as much an error than as the mistaken
transmission of an signal, and there is no logical reason why
the proximal node is implicated to a greater extent in the first
sort of error. The bias, however, was predicted from an
extension of the model theory’s principle of truth and its
mismatch principle. According to this extension, participants
are influenced by the match between their model of how the
proximal node ought to be operating, and their model of its
actual operation. The model of how the proximal node ought
to be operating represents explicitly only the possibilities in
which the node transmits a signal, leaving implicit the
possibilities in which it does not transmit a signal. Hence,
while both misses and false positives produce mismatches,
only misses give rise to a mismatch with an explicit model of
the node’s functioning, and thus lead to an increased
proclivity to attribute error. The tendency occurred only for
the proximal node, and not for the other nodes. However, this
difference makes sense, because the participant knows for
certain only the output of the proximal node. Mismatches for
the other nodes are a matter of inference. If, as we claim, the
mismatch bias is responsible for the phenomenon, its
operation must be largely unconscious, because in postexperimental questioning, no participant ever put it into
words.
We found support for the theory’s third principle: complex
nodes should tend to be diagnosed as the cause of faults. Both
experiments showed that OR ELSE gates were most liable to
be diagnosed as the source of error, and that they also took
the longest time to process. To our knowledge, this
demonstration is the first to show an effect of processing
fluency in diagnostic reasoning (see Schwarz, 2004).
A skeptic might argue that the diagnostic choices we
observed, and particularly the effects of mismatch, result
from posing a question to the participants that did not have a
correct answer. Such effects are arguably likely to be fleeting
and unimportant. We agree that the effects may not have a
persistent impact when individuals have to discover a single
and unambiguous cause of a fault (as in Rouse and his
collaborators’ experiments). But, such tasks are unlikely to
match the diagnosis of faults in the real world. Evidence for
them is rarely clear and unambiguous (see e.g., Dörner, 1996,
on the Chernobyl incident). Ambiguous evidence is liable to
be assimilated to match prior hypotheses and to be processed
in a distorted way (e.g., Darley & Gross, 1983; Lord, Ross, &
Lepper, 1979). In such situations, the phenomena that we
observed may prevent people from using new evidence to
reach a correct diagnosis.

transmission, leaves the node particularly liable to be
diagnosed as the cause of fault.
As in Experiment 1, for inconsistent networks in which at
least one input was on, those that included at least one OR
ELSE node took longer to correctly evaluate than those that
had no such nodes (17.4s vs. 14.4s., Wilcoxon test, z = 2.43,
p < .01, one-tailed). The theory predicts that participants
should choose OR ELSE nodes with a greater frequency than
they choose either AND or OR nodes. And participants were
more likely to diagnose the proximal node as faulty when it
was an OR ELSE node (53%) than when it was an AND node
(39%, Wilcoxon test, z = 2.59, p < .01), or an OR node (38%,
Wilcoxon test, z = 3.18, p < .01). Participants again seemed to
prefer diagnosing more complex nodes as the cause of error.

General Discussion
The model theory of diagnosis proposes that individuals
base their intuitions on a mental simulation of a network. It
postulates three main principles. First, individuals focus on
proximal causes when forming diagnoses. Second, they tend
to diagnose the fault in the proximal node more often when it
ought to transmit a signal. And, third, they tend to diagnose
faults in more complex nodes. We tested these principles in
two experiments which investigated how individuals form
intuitive diagnostic preferences when diagnosing a faulty
network.
Experiment 1 showed that individuals tended to diagnose
the fault in the node that was as close as possible to its
occurrence. This result could have been explained by other
factors, but it held up in Experiment 2, in which such
alternative explanations were not available. This
demonstration of a proximal effect parallels work showing
that more recent events are more likely to be “undone” in
counterfactual thinking (see Byrne, Segura, Culhane, Tasso,
& Berrocal, 2000; Miller & Gunesegarm, 1990; Teigen,
Evensen, & Samoilow, 1999; Walsh & Byrne, 2004, for the
temporal order effect). And it also parallels work on belief
revision, which has shown that information that is presented
earlier in a sequence tends to be regarded as less corrigible
(although the opposite effect has also been demonstrated in
some studies; see Hogarth & Einhorn, 1992, for a review).
Our findings extend this previous work by demonstrating a
proximal effect in a Boolean domain, where information is
presented simultaneously rather than sequentially, but a
mental model must reconstruct the temporal sequence of
events.
One interpretation of the proximal effect is that the parts of
the network that are mentally simulated first tend to be
regarded as the least revisable. This account attributes the
result to memory processes – initial information may be the
most salient (see e.g., Anderson, 1981; Hogarth & Einhorn,
1992; Schlottmann & Anderson, 1995). We have offered an
alternative interpretation: individuals tend to mentally undo
the event closest in causal proximity to an observed
inconsistency. These two interpretations are different, but our
present data are not able to distinguish between them. Yet, the
second interpretation seems more plausible. The effect
795

Johnson-Laird, P. N., Girotto, V., & Legrenzi, P. (2004).
Reasoning
from
inconsistency
to
consistency.
Psychological Review, 111, 640-661.
Lord, C. G., Ross, L., & Lepper, M. R. (1979). Biased
assimilation and attitude polarization: The effects of prior
theories on subsequently considered evidence. Journal of
Personality and Social Psychology, 37, 2098-2109.
Miller, D. T., & Gunasegaram, S. (1990). Temporal order and
the perceived mutability of events: Implications for blame
assignment. Journal of Personality & Social Psychology,
59, 111-1118.
Morrison, D. L. & Duncan, K. D. (1988). Strategies and
tactics in fault diagnosis. Ergonomics, 31, 761-784.
Rouse, W. B. (1978). Human problem solving performance in
a fault diagnosis task. IEEE Transactions on Systems, Man,
and Cybernetics, 8, 258-271.
Rouse, W. B., & Rouse, S. H. (1979). Measures of
complexity of fault diagnosis tasks. IEEE Transactions on
Systems, Man, and Cybernetics, 9, 20-727.
Schlottmann, A., & Anderson, N. H. (1995). Belief revision
in children: Serial judgment in social cognition and decisionmaking domains. Journal of Experimental Psychology:
Learning, Memory,
and Cognition, 21, 1349-1364.
Schwarz, N. (2004). Metacognitive experiences in consumer
judgment and decision making. Journal of Consumer
Psychology, 14, 332-348.
Teigen, K. H., Evensen, P. C., & Samoilow, D. (1999). Good
luck and bad luck: How to tell the difference. European
Journal of Social Psychology, 29, 981-1010.
Walsh, C. R., & Byrne, R. M. J. (2004). Counterfactual
thinking: The temporal order effect. Memory & Cognition,
32, 369-378.

Acknowledgements
This research was supported by a grant from the National
Science Foundation to the second author to study strategies in
reasoning (BCS-0076287), and by a fellowship awarded to
the first author from the Woodrow Wilson School, Princeton
University. We thank Louis Lee and three anonymous
reviewers for their advice.

References
Anderson, N. H. (1981). Foundations of information
integration theory. New York: Academic Press.
Byrne, R. M. J., Segura, S., Culhane, R., Tasso, A., &
Berrocal, P. (2000). The temporality effect in
counterfactual thinking about what might have been.
Memory & Cognition, 28, 264-281.
Darley, J. M., & Gross, P. H. (1983). A hypothesisconfirming bias in labeling effects. Journal of Personality
and Social Psychology, 44, 20-33.
Dörner, D. (1996). The logic of failure: Why things go wrong
and what we can do to make them right (R. Kimber & R.
Kimber, Trans.). New York: Metropolitan Books.
Hogarth, R. M., & Einhorn, H. J. (1992). Order effects in
belief updating: The belief-adjustment model. Cognitive
Psychology, 24, 1-55.
Johnson-Laird, P. N. (1983). Mental models. Cambridge:
Cambridge University Press.
Johnson-Laird, P. N., & Savary, F. (1999). Illusory
inferences: A novel class of erroneous deductions.
Cognition, 71, 191-229.

796

