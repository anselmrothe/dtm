UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Varying Abstraction in Categorization: a K-means Approach
Permalink
https://escholarship.org/uc/item/06d5c0fm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Storms, Gert
Verbeemen, Timothy
Verguts, Tom
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                       Varying Abstraction
                                       in Categorization: a K-means Approach
                              Timothy Verbeemen (timothy.verbeemen@psy.kuleuven.ac.be)
                                    Departement Psychologie, University of Leuven, Tiensestraat 102
                                                          B-3000 Leuven, Belgium
                                          Gert Storms (gert.storms@psy.kuleuven.ac.be)
                                    Departement Psychologie, University of Leuven, Tiensestraat 102
                                                          B-3000 Leuven, Belgium
                                               Tom Verguts (tom.verguts@ugent.be)
                              Vakgroep Experimentele Psychologie, Ghent University, H. Dunantlaan 2
                                                           B-9000 Ghent, Belgium
                             Abstract                                   sum of its members, and membership is not just defined by
  In this paper we propose, instead of the traditional distinction      the relation to other members and non-members. Rather, a
  between prototype and exemplar models, a generic model that           new “object” that does not necessarily correspond to a
  assumes a continuum between prototypes and exemplars. The             concrete real-world-object, but is an abstraction over
  model is based on the very successful GCM and an associated           previously encountered category members, arises.
  prototype model that both assume a representation on
  continuous dimensions. Abstractions are obtained by taking
                                                                        Categorization processes are then assumed to operate on
  for each category the centroids of the clusters as produced by        these centroids, rather than on all possible stored members
  K-means clustering, effectively producing the GCM and the             of the categories in question. This second way of thinking
  Single-Prototype Model as extreme cases. The model was fit            about categorization has in fact been the dominant approach
  on a set of unknown, to-be–classified fruits and vegetables           in most research dealing with natural language (e.g.,
  (Smits et al., 2002). Better fit values were clearly obtained for     Hampton, 1979; Storms, De Boeck, & Ruts, 2000, 2001).
  the intermediate solutions indicating a strategy where people            A second line of research (Medin & Schaffer, 1978;
  compare the test stimuli to a set of multiple prototypes rather       Nosofsky, 1986, 1992) focused more on the formal
  than just one prototype or all stored exemplars.                      definition of the categorization process. In this tradition one
  Keywords: prototype; exemplar; categorization; varying                typically uses artificial stimuli that were created in the lab
  abstraction; clustering.                                              and that have the obvious advantage of being completely
                                                                        under the experimenter’s control. Typically, a limited set of
                          Introduction                                  training stimuli belonging to two competing categories is
Since the ground breaking work of Rosch and Mervis                      presented until people classify these items sufficiently
(1975) in the mid-seventies, the idea has gained ground                 correctly. Consequently, a set of transfer stimuli is presented
among researchers that one of the most important aspects                whose items have to be classified in one of the earlier
that define human categorization decisions is similarity.               trained categories. Finally, rivaling formal models are fit to
Contrary to what has been called the classical view, people             explain the categorization proportions. These competing
do not seem to base their decisions as to which stimulus                models express the distinction that was mentioned earlier.
belongs to which category on a predetermined set of singly              Models that assume no abstraction at all, but see
necessary and jointly sufficient characteristics (Komatsu,              categorization as a process that is based on the similarity
1992). In fact, for many of our everyday concepts it appears            towards all items that were previously stored in a category,
quite impossible to even formulate such a definition. Rosch             are contrasted with models that assume one central
showed that several measures related to categorization were             representation for each category. In this tradition the first
in fact related to the similarity of a particular, to-be-               kind of models are called exemplar models, the second kind
categorized, item with its own category and other related               are called prototype models. Whereas research in the
categories. In this view, called the family resemblance view,           tradition of natural language has been interested in the
there are two possible interpretations of this notion of                representation of concepts in general, the formal approach
similarity. A first way is to assume that a category is simply          focused specifically on the distinction between the two
a relational structure, and that membership of an item is               notions of categorization, the exemplar and prototype view
simply determined by the similarity relation towards other              (Smith & Minda, 1998, 2000).
members and non-members of that particular category. But
it is often assumed that categories in fact provide a certain                         Abstraction and Similarity
summary or centroid that is determined by a number of                    In the distinction mentioned above between prototype and
weighted characteristics. As such, a category is not just the            exemplar models, the emphasis is on the question whether
                                                                    2301

there is total abstraction or no abstraction at all. Such a          subordinates. As the relative similarity of stimuli in a
distinction may seem plausible in the case of only a limited         category decreases when one goes up a level of abstraction,
set of relatively similar stimuli with relatively few                so should the chance of having one unifying abstraction.
characteristics to be recalled, as was most often the case in
research contrasting the prototype and the exemplar                                      A Generic Framework
approach. The idea of a single prototype is plausible                In order to formulate a model that can accommodate the
because of the relative similarity of the stimuli. The idea of       idea of varying abstractions as discussed above, we will first
no abstraction is sufficiently plausible because of the small        define one of the most successful modeling frameworks that
number of stimuli and their simple and obvious structure.            incorporates the exemplar/prototype distinction.
This reasoning breaks down, however, when one looks at                  In the generalized context model (GCM; Nosofsky, 1986,
natural language concepts.                                           1992), an exemplar model, categorization is assumed to be a
   Take for example the concept fruit. First, we can ask             function of similarity towards all relevant stored exemplars.
ourselves whether it is possible to have no abstraction at all.      The model was formulated as a generalization of the
In a traditional laboratory experiment, only a few stimuli are       Context Model proposed by Medin and Schaffer (1978) to
presented, in exactly the same way. They constitute the full         incorporate stimuli that differ on continuous characteristics
set of exemplars. In natural language, it is not so clear what       rather than binary dimensions. In case (physical) dimensions
an exemplar of fruit is. Take, for instance, an apple. If we         are unavailable, the GCM fitting procedure starts with a
assume no abstraction at all, then classifying an item as            multidimensional scaling procedure (MDS; see, e.g.,
belonging to the concept fruit would require us to compare           Takane, Young & De Leeuw, 1977) on proximity measures
it to, among other stimuli, all real-life apples that we have        of all stimulus pairs involved. The coordinates of these
encountered. If no abstraction occurs then one must                  stimuli are then used as input for the model. In the case of
compare to every specific instance that was ever                     two categories, A and B, the probability that stimulus x is
encountered. To represent a category such as fruit, that             classified in category A is given by:
would amount to an enormous amount of instances of its
members. It appears implausible, therefore, to assume no                                β Aη XA
abstraction at all.                                                   P(A|X) =                                               (1.1)
                                                                                β Aη XA + (1 − β A)η XB
   A second question we can ask ourselves is whether it is
possible to have a single abstraction for all category
members. Returning to our apples, one could argue that              where βΑ lies between 0 and 1 and serves as a response
abstraction does take place at a certain level. A granny smith      bias parameter towards category A. The parameters ηXA and
could for example be an abstraction over many encountered            ηXB denote the similarity measures of stimulus x toward all
instances. Or it could be that an abstraction for “apple”           stored exemplars of category A and B, respectively:
exists based on different types of apples. But when we look
at a higher category level, this reasoning should at least feel                                                          q
                                                                                             D               r 
                                                                                                                  1/ r
                                                                                                                       
uncomfortable. What would be for instance the abstract
                                                                     η XA  = ∑ exp− c  ∑ wd yxd − y jd                    (1.2)
representation of the collection of an apple, a litchi and a
banana? It seems hard to think of anything that is not absurd
                                                                              j∈A            d =1              
or comical.
   In more complex natural categories, therefore, it seems          with yxd and yjd as the coordinates of stimulus x and the j-th
that at least some abstraction would have to take place. The         stored exemplar of category A (or B for ηXB, respectively)
question is how. There seems to be no clear reason why              on dimension d. The weight of the d-th dimension is
abstraction should only take place at the category level. This      denoted by wd, with all weights restricted to sum to 1. The
is all the more clear in natural language where there are            power metric, determined by the value of r, is usually given
categories at different levels. Why, then, should an abstract        a value of either 1 or 2, corresponding to city-block and
representation be based on a predetermined category level            Euclidean distance, respectively. The sensitivity parameter c
or an item name, when it is more plausible to say that both          determines the overall scaling of the distances. The
abstraction and categorization are similarity-based? When            parameter q determines the decay of similarity as a function
we assume that such abstraction is similarity based, we are          of distance, where typically the values 1 or 2 are used,
saying at the same time that the chance of actually forming          corresponding to an exponential or a Gaussian decay
an abstract representation is also a function of similarity.         function.
One would therefore expect that the amount of abstractions              Much of the traditional research that was based on
in any group of stimuli would be determined by their                 artificial stimuli used a very limited set of training stimuli
internal similarity. In the context of natural language this         that varied on a set of binary dimensions (or features). It
translates to the idea of level of abstraction. Typically,           would of course be impossible to average over discrete
categories such as fruit are called superordinates, categories       features, so the prototype was conceived as an ideal
such as apple are seen as basic level categories, and                example of a category and was granted modal values for
categories such as Granny Smith apples would be called               that category. One of the advantages of the GCM is exactly
                                                                     that it allows one to derive a similarity structure from
                                                                2302

similarities between stimuli as obtained from actual human          partition of the stimulus set. With a partition defined as an
judgments. The subsequent representation in terms of a              exhaustive set of nonoverlapping subsets, where category J
multidimensional space makes it very easy to translate the          is partitioned in K different sets Sk one obtains, using the
idea of a prototype as the central tendency of a category            same reasoning we used in the case of the prototype model,
(Rosch & Mervis, 1975) into a formal model. The object               K different centroids for each dimension d of a particular
created by taking, on each dimension, the average                   category:
coordinate over all members of the category, is a
straightforward definition of the prototype. The similarity                        1
function changes to:                                                     y.kd =
                                                                                  Nk
                                                                                     ∑y
                                                                                     j∈Sk
                                                                                            jkd                              (1.4)
                                                 q
                  D                  r 
                                          1/ r
                                               
η XA  = exp − c  ∑ wd yxd − y.d                       (1.3)
                                                                     Using the above formula, we can formulate a generic model
                                                                     for (1.2) and (1.3):
                 d =1                  
                                                                                                                         q
                                                                                           D                 r 
                                                                                                                  1/ r
                                                                                                                       
where   y.d denotes the mean value of all stored members of          η XA   = ∑ exp− c  ∑ wd yxd − y.kd                   (1.5)
category A on dimension k. We will refer to (1.3) in                          k∈ A         d =1                
combination with (1.1) as the Single-Prototype Model.
   A number of studies have already been conducted that             It is easy to see that this model incorporates the special
compared prototype and exemplar models (e.g., Nosofsky,             cases where K=1 and K=N. In the first case, the partition is
1992; Nosofsky & Zaki, 2002). In many, the GCM                       made up of all stored category members and hence the mean
performed better than prototype models (but see also e.g.,           weights correspond to the Single-Prototype Model as
Smith & Minda, 1998, 2000). Recently, we have also                   described earlier. In the second case, each partition contains
applied formal models to the categorization of natural               exactly one exemplar, and hence the mean weights on each
language stimuli (Smits et al., 2002; Storms et al., 2000,           dimension are the original exemplar weights. We will refer
2001; Verbeemen, Storms, & Verguts, 2003, 2004;                      to this model as the Varying Abstraction Model. (See also
Verbeemen, Vanoverberghe, Storms, & Ruts, 2001). In                  Vanpaemel, Storms, & Ons, submitted).
these studies too, we found an overall advantage of
exemplar models.                                                     Defining the Partitions using K-means Clustering
                                                                     The Varying Abstraction Model seeks to determine which
Varying Abstraction
                                                                     partition gives the best fit to the data, instead of merely
In the above presented models, two extremes can be found.            comparing the two most extreme cases. The obvious way to
First there is the prototype that is seen as the one unifying        do this is to fit the model to the data, with all possible
centroid for the whole category. The other extreme, the              partitions, and to pick out that model which uses the optimal
exemplar model, corresponds not necessarily to the idea of           partition with regards to the categorization data. This would
no abstraction at all, but rather to the lowest level of             be feasible for datasets with only a limited number of
abstraction under investigation. In laboratory experiments,          training stimuli or supposed stored members, but for large
the exemplars would of course refer to the presented stimuli,        datasets this would become unpractical or even impossible
and would be truly the lowest level. In natural language             as the number of possible partitions of a set increases
research it would be impossible to actually trace all stimuli        drastically with the set size. To give only a small example,
at the lowest level, i.e. the actual real-life stimuli that were     the number of partitions for a set of 5 stimuli equals 52.
encountered by people. One will therefore have to establish          With two categories with five stored members each this
a lower level of interest that is still feasible to obtain. For      would amount to 52x52=2704 models to be fitted, a large
obvious practical reasons, the approach that is most often           but still computationally feasible number. The number of
used is to define the exemplar level as being one level lower        partitions for a set of 10 stimuli already equals 115975 and
than the category level.                                             would amount, in the case of two categories with 10 stored
   However, in a category with N stimuli, there is a whole           members, to 13450200625 models to be compared. For
spectrum of intermediate abstractions varying from exactly           categories with a large number of stored members, such as
one abstraction, corresponding to the Single-Prototype               natural language categories, a different approach will
Model presented earlier, to N ‘abstractions’ corresponding           therefore be required.
to the exemplar model (where there is in fact no abstraction            We mentioned already that abstraction, if it takes place at
at all save perhaps at the exemplar level). Given the fact,          all, should be based on the same principles as
then, that there appears to be no reason why the exemplar            categorization: similarity. Partitions of a category, and the
model should be contrasted only with a model that assumes            associated centroids, should therefore be based on the
abstraction over the set of stored category members, what            internal similarity of that category. Not only should very
should be the right approach? Abstraction, as defined in             similar stimuli be allowed to merge into a single prototype,
formula (1.3), could in fact also be based on any other
                                                                2303

but very dissimilar stimuli should be allowed to remain              the most frequently generated features ensures that the
separate as a reference object for the to-be-classified items.       analysis is not clouded by potentially unreliable features that
Such an approach naturally leads us to consider clustering           are important to only a few subjects.) A similarity matrix
techniques of some sort. Given the fact that cluster centers         was then obtained by correlating the feature applicability
for each partition in the varying abstraction model are based        vectors for all 109 stimuli, after summing over participants.
on the mean values of the stimuli belonging to that partition,       A different group of thirty participants classified the well-
an immediate choice would be K-means clustering (see,                known stimuli as belonging to either fruit or vegetables. A
e.g., Hastie, Tibshirani and Friedman, 2001).                         group of twenty different participants did the same for the
    In K-means clustering, one seeks to optimally partition a         novel stimuli.
set of N items in a predefined number of Sk subsets so as to              In order to obtain dimensions, the derived similarities
minimize the criterion                                                between the 109 old and novel fruits and vegetables were
                                                                      analyzed with ALSCAL (Takane et al., 1977). A three-
  K    Nk   D             2                                           dimensional ordinal solution was chosen that explained
 ∑∑∑ y
 k =1 j∈Sk d =1
                 jd − y.d                                  (1.6)      approximately 96 percent of the variance.
                                                                          Smits et al. then predicted category decisions based on the
                                                                      geometric versions of the GCM and the Single-Prototype
This minimum is reached by first assigning the stimuli                Model and found a clear advantage of the GCM over the
randomly to the K clusters, and then computing the cluster            prototype model.
centers. Consequently, the items are reassigned to the
closest cluster center and the cluster means are computed             Fitting the K-means Varying Abstraction Model
again anymore. This process continues until the assignments           In the illustration presented here we fitted the different
do not change. Because K-means clustering is based on the             models of the generic family to the classification data of the
Euclidean distance between items on a number of predictor             30 novel stimuli. The first step is to apply K-means
dimensions, it can simply operate on dimensions that are              clustering to each category separately in order to find the
prespecified or obtained through multidimensional scaling             cluster centers, and hence the prototype coordinates. The
techniques. It can be seen from (1.4) and (1.6) that the              well-known items are seen as the stored items, as they were
cluster centers obtained by K-means clustering follow the             generated from memory by actual subjects. This means that
previously mentioned definitions of the (multiple)                    there are 35 stored exemplars in the category fruit and 44
prototypes. The most straightforward way, then, to                    stored exemplars in the category vegetables. To make the
incorporate the K-means approach into the Varying                     comparison even more feasible, we chose not to examine
Abstraction Model is to simply use the coordinates of the             every possible clustering ranging from 1 cluster to N
cluster centers as returned by K-means clustering into the            clusters, but to work in steps of 4. Hence, for each of the
model.                                                                two categories, clustering was applied to the well-known
    This effectively leaves us with a total number of N               stimuli based on the three dimensions as produced by
partitions per category, where N is the number of stored              ALSCAL resulting in 1, 5, 9, 13,....,N clusters for each
category members. In the case of two rivaling categories A            version of the model. Thus we have a total of 10 successive
and B this leaves us with a maximum of NA x NB models to              steps for fruit and 12 for vegetables, including the extreme
be evaluated.                                                         cases where K=1 and K=N. This leaves us with an effective
                                                                      number of 10 x 12 models to be fitted corresponding to the
                An Illustration of the Model                          different combinations of the cluster levels.
In this section, we will present an application of the K-                 In the next step, we simply used the coordinates as
means Varying Abstraction Model to a natural language                 produced by K-means clustering as model input to define
dataset consisting of the two superordinate categories fruit          the coordinates of the K reference objects for each separate
and vegetables taken from Smits et al. (2002). The choice of          model.
a natural language set of the superordinate level allows us to            Consequently, the models were fitted to the categorization
optimally test the model as it seems most relevant for                data. As we are dealing with the scaling of response
categories that posses, intuitively, different subsets of items       probabilities in two categories, the obvious way is to
that are relatively similar within the subsets but rather             maximize the Likelihood assuming the binomial
different between subsets.                                            distribution1. In order to compare the models, we use the
    Smits et al. (2002) analyzed a stimulus set consisting of
pictures of 79 well-known items, retained after an exemplar           1
                                                                         This amounts to maximizing the binomial probability of the data
generation task for the categories fruit and vegetables, and          arising under a specific parametrization of the model,
                                                                                       ()
30 fruits or vegetables, mostly exotic, that were completely
                                                                          (     )
                                                                                         nj  rj            n j −r j
unknown to participants. Ten participants completed a                   p D M =∏            p j (1 − p j )          ,
                                                                                     j rj
feature applicability task for all stimuli, for the 17 most
frequently generated features for fruit and vegetables,              where the index j refers to the j-th to-be-classified exemplar. The
                                                                     number of trials for each stimulus corresponds to nj. Here, we use
generated by a different group of thirty participants. (Taking       the proportion of classifications in the category fruit as a dependent
                                                                 2304

       170
       166
       162
       158
 BIC
       154
       150
       146
       142
       138
             1   5   9   13   17   21   25      29   33   37   41   44
                                   vegetables                                    Figure 1b: BIC values for all models as seen on a surface
                         fruit1                       fruit5                     plot. The number of clusters for each category is indicated
                                                                                                        on the axis.
                         fruit9                       fruit13
                         fruit17                      fruit21                   from these results that the actual optimum is not situated at
                         fruit25                      fruit29                   the full exemplar model. The BIC value of the exemplar
                         fruit33                      fruit35                   model, which corresponds to the classical GCM, is 149.22.
                                                                                For the Single-Prototype Model, the fit value was 167.97.
    Figure 1a: BIC values for all models. The number of                         The model that fits best in our analyses is the model with 21
 clusters for vegetables is indicated on the X-axis while the                   clusters for fruit and 17 clusters for vegetables. It has a BIC
    different curves represent the cluster levels for fruit.                    value of 138.71. This difference is large enough to
                                                                                decidedly reject the full exemplar model as the best-fitting
Bayesian Information Criterion (BIC) that is most suitable                      model. Furthermore, as can be seen from figure 1a and 1b,
for nonnested models as is the case here2.                                      there is a relatively smooth decrease towards this minimum,
                                                                                indicating that the optimum is not just due to spurious
Results and Discussion We will only discuss models fitted                       factors. We can therefore safely assume that the true optimal
with an exponential decay function (q=1) and Euclidean                          value is situated at least somewhere around this minimum.
distances (r=2) as this resulted in clearly better fit values.                  Thus, the categorization performance of people seems more
The results are summarized in Figures 1a and 1b. It is clear                    likely to be explained by a strategy where people in fact use
                                                                                intermediate abstraction. It does seem to be the case,
                                                                                however, that the models that are relatively more close to
                                                                                the full exemplar model perform better than the models that
variable, so rj corresponds to the number of trials that the j-th item          are relatively close to the Single-Prototype Model.
was classified as belonging to fruit. pj corresponds to the predicted              There seems also to be a certain asymmetry concerning
proportion of classification of the j-th item in the category fruit.
Practically, the natural logarithm of the Likelihood function is used
                                                                                the amount of clusters that are required for the category fruit
as this produces identical parameter estimates.                                 and the category vegetables. To reach the optimal value,
2
  BIC = -2 ln(L) + k ln(n), where L is the likelihood value, k is the           fruit requires more clusters than vegetables even though it
number of free parameters, and n is the number of data points. As               has less stored exemplars to be compared with.
such, the measure is a trade-off between model fit and model                    Furthermore, the fit values as a function of the amount of
complexity Lower means better, and only the difference in free                  clusters decrease faster as a function of the number of
parameters needs to be taken into account, hence the models                     clusters in fruit than in vegetables. An explanation may by
presented here are evaluated using only -2 ln(L). The                           suggested by looking at the actual cluster assignments as
absolute difference |∆| between two models can be roughly                       produced by K-means clustering. In the case of vegetables,
interpreted on a scale of e|∆|/2 where this approximates the                    there are always less singular exemplars that make up a
probability ratio of the best fitting model over the worst fitting              cluster in any of the cluster levels, except of course for the
model (For an extensive discussion, see Kass & Raftery, 1995.)
                                                                                case where K=N. In the case of fruit, a large portion of the
                                                                         2305

clusters are singular exemplars, with for instance rhubarb as       Medin, D. L., & Schaffer, M. M. (1978). Context theory of
a single cluster from the start on. In the actual optimum, 14         classification learning. Psychological Review, 85, 207-
of the 21 clusters for fruit consist of exactly one exemplar.         238.
For vegetables, only 5 of the 17 clusters consist of a singular     Nosofsky, R. M. (1986). Attention, similarity, and the
exemplar. Furthermore, most of the instances of fruit that            identification-categorization relationship. Journal of
were kept as singular clusters were highly atypical such as           Experimental Psychology: General, 115, 39-57.
meddler or pomegranate whereas this was much less the               Nosofsky, R. M. (1992). Exemplars, prototypes and
case for vegetables. It appears that especially in the case of        similarity rules. In A F. Healy, S. M. Kosslyn, & R. M.
fruit, the capacity of the model to both keep similar items in        Shiffrin (Eds.), From learning theory to connectionist
one cluster and on the other hand keep outliers separate as a         theory: Essays in honour of William K. Estes, Vol. 1.
reference object is a crucial feature to explain the                  Lawrence Erlbaum, Hillsdale, NJ.
categorization data.                                                Nosofsky, R. M., & Zaki, S. R. (2002). Exemplar and
                                                                      prototype models revisited: Response strategies, selective
                         Conclusion                                   attention, and stimulus generalization. Journal of
In the present paper, we present a generic model with                 Experimental Psychology: Learning, Memory and
varying levels of abstraction that incorporates the                   Cognition,                   28,                   924-940.
Generalized Context Model on the lowest level of                    Rosch, E., & Mervis, C. B. (1975). Family resemblances:
abstraction, and the Single-Prototype Model on the highest            Studies in the internal structure of categories. Cognitive
level of abstraction, as special cases. Abstraction is based on       Psychology, 7, 573-605.
similarity and is formally implemented by using K-means            Smith, J. D., & Minda, J. P. (1998). Prototypes in the mist:
clustering to find the appropriate partitions within each             The early epochs of category learning. Journal of
category. The model therefore allows one to analyze large             Experimental Psychology: Learning, Memory and
datasets as it does not require all partitions to be examined.        Cognition, 24, 1411-1436.
  In an application on a set of unknown to-be-classified           Smith, J. D., & Minda, J. P. (2000). Thirty categorization
fruits and vegetables, the model clearly favors the                   results in search of a model. Journal of Experimental
intermediate levels of abstraction rather than one of the two         Psychology: Learning, Memory and Cognition, 26, 3-27.
extremes proposed by the classical models. It seems that           Smits, T., Storms, G., Rosseel, Y., & De Boeck, P. (2002).
especially in the case of larger categories with sufficiently         Fruits and vegetables categorized: An application of the
different stimuli, such as natural language categories, this          generalized context model. Psychonomic Bulletin and
model provides a promising approach to modeling people’s              Review, 9, 836-844.
categorization decisions.                                          Storms, G., De Boeck, P., & Ruts, W. (2000). Prototype and
  Finally, the general framework of the model can easily be           exemplar-based information in natural language
expanded to other models that use a multidimensional                  categories. Journal of Memory and Language, 42, 51-73.
representation. In general, the framework of basing                Storms, G., De Boeck, P., & Ruts, W. (2001).
partitions on a similarity-based heuristic could also be              Categorization of unknown stimuli in well-known natural
expanded to models that use other representational                    language concepts: a case study. Psychonomic Bulletin
assumptions than the multidimensional models (e.g.,                   and Review, 8, 377-384.
Verbeemen et al., 2004).                                           Takane, Y., Young, F. W., & De Leeuw, J. (1977).
                                                                      Nonmetric individual differences multidimensional
                    Acknowledgments                                   scaling: An alternating least squares method with optimal
                                                                      scaling features. Psychometrika, 42, 7-67.
The first author is a research assistant of the Fund for           Vanpaemel, W., Storms, G. , & Ons, B. (2005). A varying
Scientific Research – Flanders. This project was in part              abstraction model for categorization. Manuscript
sponsored by grant OT/01/15 of the University of Leuven               submitted for publication.
research council to Gert Storms.                                   Verbeemen, T., Storms, G., & Verguts, T. (2003).
                                                                     Determinants of Speeded Categorization in Natural
                         References                                  Concepts. Psychologica Belgica, 43, 139-151.
Hampton, J. A. (1979). Polymorphous concepts in semantic           Verbeemen, T., Storms, G., & Verguts, T. (2004). Similarity
  memory. Journal of Verbal Learning and Verbal                      and taxonomy in categorization. In K. Forbus, D. Gentner
  Behavior, 18, 441-461.                                             & T. Regier, (Eds.), Proceedings of the 26th Annual
Hastie, T., Tibshirani, R., & Friedman, J. (2001). The               Conference of the Cognitive Science Society, pp. 1393-
  elements of statistical learning: Data mining, inference           1398. Mahwah, NJ: Erlbaum.
  and prediction. New York: Springer-Verlag.                       Verbeemen, T., Vanoverberghe, V., Storms, G., & Ruts, W.
Kass, R. E. , & Raftery, A. E. (1995). Bayes factors. Journal         (2001). The role of contrast categories in natural language
  of the American Statistical Association, 90, 773-795.               concepts. Journal of Memory and Language, 44, 618-643.
Komatsu, L. K. (1992). Recent views of conceptual
  Structure. Psychological Bulletin, 3, 500-526.
                                                               2306

