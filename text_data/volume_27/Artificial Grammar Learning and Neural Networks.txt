UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Artificial Grammar Learning and Neural Networks
Permalink
https://escholarship.org/uc/item/3q63c4bh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Forkstam, Christian
Grenholm, Peter
Petersson, Karl Magnus
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                           Artificial Grammar Learning and Neural Networks
                           Karl Magnus Petersson (karl.magnus.petersson@fcdonders.ru.nl)
                                          F.C. Donders Centre for Cognitive Neuroimaging,
                                           Radboud University Nijmegen, The Netherlands
                               CSI, Center for Intelligent Systems, Universidade do Algarve, Portugal
                                         Peter Grenholm (peter_grenholm@yahoo.se)
                                  Cognitive Neurophysiology Research Group, Karolinska Institutet
                                                      171 76 Stockholm, Sweden
                                      Christian Forkstam (christian.forkstam@cns.ki.se)
                                  Cognitive Neurophysiology Research Group, Karolinska Institutet
                                                      171 76 Stockholm, Sweden
                             Abstract                                 transition (Figure 1). The modified RM1 outputs an infinite
                                                                      symbol string: first an end-of-string symbol '#', then a Reber
   Recent FMRI studies indicate that language related brain           string, a ‘#’, a new string, and so on indefinitely. It turns out
   regions are engaged in artificial grammar (AG) processing. In      that to recognize all possible output strings of RM1, a
   the present study we investigate the Reber grammar by means
                                                                      necessary and sufficient condition is to know a set, TG, of
   of formal analysis and network simulations. We outline a new
   method for describing the network dynamics and propose an          48 trigrams. A string is generated by RM1 if and only if the
   approach to grammar extraction based on the state-space            string starts with ‘#’ and only contains trigrams in TG. To
   dynamics of the network. We conclude that statistical              show this, we observe that the Reber grammar yields
   frequency-based and rule-based acquisition procedures can be       exactly the same strings as RM2 (Figure 2). Assume that we
   viewed as complementary perspectives on grammar learning,          know RM1 and that we know the latest two symbols in an
   and more generally, that classical cognitive models can be         output string from RM2. This determines the internal state
   viewed as a special case of a dynamical systems perspective        of RM1 and RM2. The set of possible bigrams is
   on information processing.                                         {MT,MV,…,#V}.
   Keywords: artificial grammar learning, neural network,
   dynamical systems.                                                                                 #
                         Introduction                                                     T
According to Chomsky a core feature of natural language                                            V
processing is the ‘infinite use of finite means’. The family
of right-linear phrase structure grammars, implementable in                        M
the finite-state architecture (FSA), is a simple formal model                                                    T
of this idea. The work of Reber (1967) suggested that                    #              X
                                                                                                    R
humans can learn AGs implicitly and that the relevant
structure is abstracted from the input. Reber (1967)
                                                                                                                 M
proposed that this process is intrinsic to natural language                        V
learning and it has been suggested that AG learning (AGL)
is a relevant model for aspects of language acquisition
                                                                                                     X
(Gomez & Gerken, 2000). Recent FMRI studies indicate
                                                                                                               R
that language related brain regions are engaged in AG
processing (Petersson et al., 2004). Here we investigate the
Reber grammar (Figure 1) by means of formal analysis and
                                                                          Figure 1: The transition graph representation of RM1
network simulations and outline an approach to grammar                             corresponding to the Reber grammar.
extraction based on the network state-space dynamics.
                                                                     We obtain the set of possible trigrams from RM2 if we take
               Elementary formal analysis                            these 21 bigrams and extend them according to Figure 2,
We begin by showing that the Reber grammar, and in                   yielding TG={MTT,MTV,…,#VX}. It is clear that a string
certain respects similar AGs, can be learned by acquiring a          is an output of RM1, if it starts with ‘#’ and only contains
finite set of n-grams (for details see Grenholm, 2003). To           trigrams in TG. Conversely, assume that a string begins
this end, we modify the Reber machine (RM0) by                       with one of the 48 trigrams, the first symbol of which is ‘#’.
connecting the final to the initial state with a #-labeled           The only possibilities are ‘#MT’, ‘#MV’, and ‘#VX’, which
                                                                 1726

determine unique states in RM2. Recursively, assume that          MTVT is transformed to MT–TV–VT). The limit
the last three symbols were C1C2C3. Then C2C3 is a possible       distribution of the new Markov process was computed by an
bigram, hence determines a unique internal state S in RM2.        iterative procedure yielding a numerical limit distribution.
If the next symbol is C4, by assumption C2C3C4 is a possible      The probabilities were rounded and verified to be stationary.
trigram and C4 must be on one of the transitions from S, and      The entropy of Ψ was then calculated according to H(Ψ) =
thus assigns a unique state to RM2. In this way, we can           −Σijπijlog(pij), where πij is the stationary distribution and
traverse RM2 in a way that yields the desired output string.     pij the transition probabilities of Ψ. We can thus measure
It follows that the string is a possible output of RM1. This      how much information that is contained in the last and next-
suggests one reason for the success of fragment-learning          to-last symbol etc. Since the process takes its values among
methods in acquiring the Reber grammar and similar AGs: a         six symbols, the entropy of a single symbol = 2.46 ( ≅ log
machine that memorizes TG can in principle recognize the          6), implies that these are approximately equally distributed.
Reber language.                                                   If we know the latest symbol, the entropy of the next
                              #                                   decreases to 1.54 (≅ log 3), while given 2 symbols, which is
                                                                  all there is to know for a second-order Markov process, the
                                                                  entropy = 1.00 bits/symbol. This makes intuitive sense,
                   T                                              since most states of RM1 have two exit transitions.
                            V
                                                                           Table 1: Measures of the conditional entropy.
            M                              T                              Machine     0      1      2       3      4
                 T                R
   #
                         V                                                RM1         2.46   1.54   1.00    1.00   1.00
                                  M                                       RM2         2.25   1.58   1.19    1.14   1.13
            V    X
                                           M                      We see that more of the information is contained in the last
                                  R                               compared to the next-to-last symbol, implying that one can
                                                                  expect reasonably good performance by recognizing
                                                                  bigrams alone. Similar results hold for RM2.
                                        R
                                                                                   Neural network model
             Figure 2: RM2 – The modified RM1.                    We proceed to outline a series of computer experiments
                                                                  using predictive networks based on the simple recurrent
We can further characterize the properties of RM1 by              network (SRN) architecture (Elman, 1990). The networks
                                                                  were trained on acquisition data generated from RM1
information theoretic tools. We associate a random process
                                                                  (Grenholm, 2003) with a gradient-descent error back-
Ψ with RM1 by setting Ψ[0]=‘#’ with probability one. Then         propagation algorithm with adaptable learning rate and
beginning at the start state of RM1 and randomly selecting        momentum. The discrete-time SRN can be viewed as a
one of the possible transitions with equal probability. Let       simple network analogue of the FSA. More specifically, at
the value of Ψ[1] be the corresponding symbol. Continuing         time point n+1, the output λ(n+1)=N[ω(n),σ(n)] of the SRN
in this manner, randomly traversing RM1 yields a sequence        is a function of the input σ(n) and the previous internal state
of random symbols Ψ[n]. The random process obtained will         ω(n). The internal state ω(n)=[h(n),c(n)] consist of two
be referred to as the Reber process, a hidden Markov             components: the state of the hidden layer of computational
process. We note that while RM1 and RM2 generate the             units h(n) and the state of a 1-step short-term memory layer
same output strings, the corresponding random processes          c(n)=h(n-1). If we introduce the time-shift operator ∆ (i.e.,
have different probability distributions.                        ∆β(n)=β(n+1)), the dynamics of the state transitions are
                                                                  described by: ∆h(n):=A[ω(n),σ(n)], ∆c(n):=C[ω(n)]:=h(n);
            Information theoretic analysis                       equivalently,
                                                                            ∆ω(n) = U[ω(n),σ(n)]                         [1]
In certain cases it is possible to define the entropy of a                  ∆λ(n) = N[ω(n),σ(n)]                         [2]
discrete-time random process. This procedure starts with the     where U[ω(n),σ(n)]:=(A[ω(n),σ(n)],C[ω(n)]). It is clear that
entropy of Ψ[n] that remains after conditioning on Ψ[0],…,       the form of [1,2] is identical to that of the dynamical
Ψ[n-1] and this conditional entropy will in our case             formulation of the FSA (cf., Eqs. [4,5]). This is an example
converge as n→∞. The elementary analysis implies that,           of a well-known result of McCulloch and Pitts (1943), who
given two symbols of Ψ, the probabilities for the next           showed that a particular class of networks is equivalent to
symbol are determined. Hence, Ψ is second-order Markov           the FSA.
and consequently we can determine its entropy (Goldie &               Generally, learning and development can be
Pinch, 1991). In computing entropies, we followed the            implemented by making the system function, for example U
procedures in section 2.7 of Goldie & Pinch (1991). Higher-      and N of equation [1,2], dependent on adaptive parameters
order Markov processes were transformed into Markov              α, thus yielding a model space M:={Uα, Nα| α realizable by
processes of order one by considering strings as output of       the system}. Learning (development) can then be
the process instead of individual symbols (e.g., the output
                                                             1727

conceptualized as a trajectory in the model space M,                inverse tangent; output layer: linear). With randomly
determined by a learning dynamics L of the system:                  initialized weights, the mean squared error was used as the
           ∆α(n) = L(α(n), σ(n), n)                     [3]         objective cost function for the learning process (gradient-
given an initial state α=α0; here L is explicitly dependent on      descent back-propagation with adaptable learning rate and
time in order to capture the idea of innately specified             momentum). The symbols were translated into a vector
developmental processes and (possible) dependence on the            representation with six elements (0.8 in one position and
previous developmental history. In general, information             else -0.2). The networks were trained for 200 epochs using
processing and learning can be viewed as a coupled                  one symbol of the acquisition sequence as input and the next
dynamical system, where the processing dynamics [1,2] is            symbol as target. The results of the first series of
coupled with the learning dynamics [3]. If we replace the           experiments (mean learning error on the acquisition data for
discrete time with a continuous time and specify the                10 networks of each size) indicate that the smaller networks
differential time changes, we have the general continuous-          (2, 4 hidden nodes) have smaller error in the beginning,
time case. We note that the SRN differs in processing               while the larger networks perform better in the end (8-32).
capacity from the FSA to the extent that it can be viewed as        There was no performance gain when the number of hidden
an analog model; in other words, to the extent that the SRN         nodes increased above 8. In epochs 40-200, the performance
takes advantage of infinite processing precision. If this is        curves continue almost horizontally, essentially replicating
not the case, or proper care is not taken with finite-precision     previous results (data not shown). In a second series of
simulations, the SRN simulation reduces to a FSA                    experiments, the networks were evaluated using methods A-
simulation, which might or might not capture relevant               D on an independent string set. Evaluation by method A and
model behavior (McCauley, 1993).                                    B is straightforward and in order to evaluate the networks
                                                                    by method C and D we generated a list of Reber and non-
Performance evaluation                                              Reber strings. The latter were generated from a first-order
Four different methods for performance evaluation were              Markov process with the same bigram statistics as the Reber
used: (A) The output of the network was compared with the           process. The list contained 1000 symbols (50% strings from
next symbol in the test sequence. This difference is                the Reber process). In method C, we interpreted the
precisely what the learning process aims to minimize. Since         network’s output as probabilities and multiplied these for
the next symbol of the test sequence is a random variable,          each symbol in the test string, yielding a maximum-
perfect prediction performance is impossible, and one               likelihood score for each string. These were normalized by
approach is to compute the theoretically best performance in        taking the logarithm and dividing by string-length. We
terms of generalization performance and compare the actual          measured performance according to how many non-Reber
performance with this. Here the output was interpreted using        strings that scored better than Reber strings. In order to
a winner-take-all competitive architecture in the output            improve performance, resetting was used in C1 by feeding
layer. (B) It is possible to interpret the normalized output of     MV#MV#MV#MV as input between each test string. In this
the network as a probability distribution for the next              way, the network settles in its start-of-word position. For
symbol. This is then compared with the theoretical                  method D, we first trained a competitive network with 24
probability distribution of RM1. (C) The third method               nodes for 10000 epochs on the activation pattern of the SRN
mimics human experiments on AGL, in which the network               hidden nodes. The adaptive algorithm used was developed
is set to discriminate between correct and incorrect strings in     from the algorithms described in section 2.5 of Haykin
a grammaticality classification task. In method C1, we              (1998).
introduced a grammatical string between strings to be
classified (cf., below), in order to force the network to settle       Table 2: Performance evaluation according to methods A
in a start-of-word state. We denote method C by C2 when                    and B. Mean from 8 networks of each size (± SD).
we do not use this reset procedure. (D) Here we attempted
to measure how far the internal state of the network deviates               #Hidden Nodes            A              B
from what can be considered the typical behavior in terms                   2                   .114±.002       .037±.002
of its state-space dynamics. This is described in greater                   4                   .094±.004       .017±.004
detail below and attempts to analyze the internal behavior of               8                   .088±.002       .010±.002
trained network. The basic question is whether the nodes in                 16                  .086±.003       .009±.003
the hidden layer capture features that are interpretable with               32                  .085±.002       .008±.002
respect to underlying generative mechanism. To this end,                    Best possible          .076              0
we constructed a competitive network, which was trained to
classify the outputs of the hidden nodes.                            We then measured, for each symbol, the state-space distance
                                                                     from the activation of the hidden nodes to the closest node
Simulation results                                                   in the competitive network. We computed the mean value of
From the Reber process, a sequence of 1000 symbols was               these distances for all symbols in a test string. Heuristically,
generated (127 strings). In the first series of experiments,         if the mean distance is large, the word is likely to be non-
five SRNs were defined with the following architecture: 6            grammatical. The rest of the evaluation was performed as in
nodes in the input/output layers, and 2k nodes (k = 1-5) in          method C. We evaluated the generalization capacity on an
the hidden layer (transfer function in the hidden layer:
                                                                1728

independent string set, using the methods A-D described                                      Discussion
above, see Table 2 and 3.
                                                                     There was no significant performance increase with more
                                                                     than 8 hidden nodes on the training data. These nodes have
   Table 3: Performance evaluation according to methods C
                                                                     two main tasks: process memory and computation. The
      and D. Mean from 8 networks of each size (± SD).
                                                                     information theoretical analysis suggests that the memory
                                                                     required is on the order of 2-3 bits (~2-3 nodes; cf., Table
      #Hidden Nodes            C1          C2          D
                                                                     1). The reason performance improved with 8 hidden nodes
      2                     60±4%        60±5%      61±2%            is likely related to the requirements of the output
      4                     79±6%        81±5%      68±6%            representation. In general, there is a risk of over-learning
      8                     91±3%        87±5%      75±5%            using large networks on small acquisition samples, leading
      16                    93±2%        74±6%      83±5%            to sub-optimal generalization performance and less efficient
      32                    94±2%        73±4%      81±5%            acquisition of the underlying structure in the input (Haykin,
      Best possible          100%         100%       100%            1998). In the present study, this was not a major problem
                                                                     (cf., Table 2 and 3), and is likely related to the close match
                                                                     between the SRN architecture and Markov processes. As
                                                                     noted above, the SRN architecture can be viewed as a time-
                                                                     discrete analog version of the FSA. To make this explicit,
                                                                     we take a dynamical systems perspective on the classical
                                                                     computational models (Davis, Sigal, & Weyuker, 1994); let
                                                                     Σ be the input space (σ∈Σ), Ω the space of internal states
                                                                     (ω∈Ω), and Λ the output space (λ∈Λ). The possible
                                                                     transitions between internal states are determined by a
                                                                     transition function T:ΩxΣ→Ω and the outputs by a function
                                                                     R:ΩxΣ→Λ. In other words, at processing step n, the system
                                                                     receives input σ(n) in state ω(n), and changes its state into
                                                                    ω(n+1) while generating the output λ(n+1) according to:
                                                                               ∆ω(n) = T[ω(n), σ(n)]                         [4]
                                                                               ∆λ(n) = R[ω(n), σ(n)]                         [5]
                                                                    In this way, the system traces a trajectory in state-space,
                                                                    forced by the input, and thus generating an output sequence.
                                                                    Within the framework of Church-Turing computability Σ,
                                                                    Ω, and Λ are all finite and thus T and R are finitely
Figure 3: The upper part shows the activation of the hidden         specified. It is clear then that these models represent a
nodes plotted along the first three principal axes obtained by      special class of dynamical systems (Petersson, 2004b).
PCA. The dots are coded to correspond to the seven internal         Furthermore, we note that the form of [4,5] is identical with
states of RM1. The diagram shows that dots corresponding            that of equations [1,2]. Here we have not explicitly
to the same internal state are geometrically close (dashed          described the system’s memory organization. In all classical
ellipses). The filled circles in the diagram are centered at the    architectures, the transition function T:ΩxΣ→Ω can be
24 nodes of a trained competitive network. The lower part           realized in a FSA. Thus, with respect to the mechanism
of the diagram plots all activation points according to their       subserving transitions between internal states there is no
distance from the closest node in the competitive network.          fundamental distinction in terms of machine complexity
                                                                    between the different computational architectures. However,
The theoretically best performance was calculated from the          as indicated by the Chomsky hierarchy (Davis, Sigal, &
full knowledge of the state transitions between internal            Weyuker, 1994), there are differences in structural
states of RM1. The behavior of the competitive network,             expressivity. These differences are fundamentally related to
characterized by the first three principal components, is           the interaction between the generative mechanism and the
illustrated in Figure 3. We see that the competitive network        available memory organization. The most important
is able to discriminate between activation states of the            determinant of structural expressivity is the availability (or
hidden nodes corresponding to different states of RM1: to           not) of infinite storage capacity. Thus, it is the
every RM1-state corresponds one region of state-space.              characteristics of the memory organization, which in a
Thus the state-space dynamics of the network reproduces             fundamental sense, allow the architecture to recursively
the dynamics of the Reber machine, and the combined                 employ its processing capacities inherent in T, to realize
approach of analyzing the behavior of the hidden nodes with         functions of high complexity or complex levels of
a competitive network and PCA provides one approach to              expressivity.
grammar extraction from the state-space dynamics of the                 In addition, we evaluated network performance on test
recurrent network. We note that grammar extraction as               data with the same bigram statistics as the Reber process.
outlined here is related to the symbolic dynamics approach          This is a harder task than commonly employed in AGL
used in the analysis of non-linear dynamical systems (cf.,          experiments on humans or with computers. We were able to
Badii & Politi, 1997).                                              obtain better than 90% network performance as
                                                                1729

characterized by method C1, which is not surprising on              complementary perspective is offered by network models of
theoretical grounds but suggests that the lower SRN                 language processing. The network perspective represents a
performance reported in the literature is related to the            special case of the recently revived dynamical systems
evaluation method C2. In particular, when a large SRN is            perspective on analog information processing (Siegelmann
tested using C2, a single incorrect symbol can induce               & Fishman, 1998) as a model for cognition (e.g., Petersson,
activation patterns that are quite distant from the regions of      2004). From a neurophysiological perspective, the natural
state-space on which it was trained. The state-space                generalization of the classical finiteness in this context is the
trajectories then remain distant for several subsequent input       property of state-space compactness (Petersson, 2005a). If
symbols from 'typical' trajectories to which the SRN has            one assumes that the brain sustains some level of noise or
been exposed. A grammatical string that is processed                does not utilize infinite precision processing (either of these
subsequently to an incorrect string can thus obtain a low           assumptions seems necessary for physically realizability), it
grammaticality score due to transient network behavior.             can be argued on qualitative grounds that 'natural language',
                                                                    viewed as a neurobiological system, might be well-
Grammar learning                                                    approximated by finite-state behavior (Petersson, 2005a).
Grammar learning, whether natural or artificial, is                    Returning to the issue of grammar learning, it is possible
commonly conceptualized either in terms of a structure-             to take a view that is placed somewhere between the two
based ('rule') acquisition mechanisms or statistical learning       more common conceptualizations. The generative
mechanisms. Our analysis of activation patterns suggests            mechanisms of the Reber machine is easily translated into a
that the SRN architecture can learn to represent the rules of       Minimalist-type or unification-based framework (Chomsky,
the grammar in its state-space dynamics. Conversely, the            2005; Joshi & Schabes, 1997) as suggested in Petersson et
elementary analysis shows that a finite set of string               al. (2004). Specifically, given a transition from state sj to sk
fragments is sufficient for Reber language recognition.            when the terminal symbol T is recognized (sj→Tsk in the
Thus, it appears that these perspectives on grammar learning       transition graph), this is translated into a lexical item or
are complementary rather than mutually exclusive, at least         feature vector [sj,T,sk], where sj and sk should be interpreted
in the case of finite-state grammars. Some aspects of natural      as 'syntactic' or 'control' features ('specifier' feature: sj, and
language (e.g., syntax) are amenable to an analysis within         'complement' feature sk) and T as a 'surface' or
the classical cognitive science framework, which suggests          'phonological' feature. A finite transition graph thus
that isomorphic models of cognition can be found within the        generates a finite number of lexical items. The syntactic
framework of Church-Turing computability (Davis, Sigal, &          features of these representations could very well be
Weyuker, 1994). These language models typically allow for          generated or estimated based on a statistical learning
a greater structural expressivity than can be (strictly)           mechanism. Moreover, there is no need for a specific 'rule'
implemented in the FSA. For example, the different                 acquisition mechanism, because the parsing process might
architectures of the Chomsky hierarchy allow for different         use general structure integration mechanisms already in
types of recursion, a feature thought to be at the core of the     place for merging or unifying structured representations. We
language faculty (Hauser et al., 2002). Unlimited                  note that the syntactic features of lexical items have
embedding recursion is supported by the push-down                  acquired a particular functional role in this picture. This can
architecture, while unlimited cross-dependency recursion           be described in terms of governing or monitoring of the
requires a linearly-bound architecture (Davis et al., 1994);       integration process based on selecting pieces of information
none of these are characteristic of human performance. In          that can be merged. In other words, the finite-state control
contrast, the FSA supports unlimited concatenation                 has been distributed over a mental lexicon (long-term
recursion and can support finite recursion of general type.        memory) among the lexical items in terms of control
This is also characteristic of human performance. It is well       features. In the unification picture, the lexical item [sj,T,sk]
accepted that neurobiological and functional brain                 corresponds to stored trees ('treelets') with foot- (i.e., sk) and
constraints have important implications for the                    root-nodes (i.e., sj), which are merged or unified in a
characteristics of the language faculty (Hauser et al., 2002).     unification space. Thus, again, important aspects of the
It seems natural to assume that the brain is finite with           finite-state control is allocated to the mental lexicon and the
respect to its memory organization. Now, if one assumes            foot- and root-nodes of the lexical items now come to
that the brain implements a classical model of language,           govern the unification process based on selecting the
then it follows immediately from the assumption of a finite        representations that can be recursively integrated. This view
memory organization that this model can be implemented in          on grammar acquisition is more akin to lexical learning in
a FSA, although a context-sensitive or any other suitable          that it suggests that simple structured representations
formalism might be used as long as the finite memory               [sj,T,sk] are created during acquisition. In essence, this re-
organization is appropriately handled. In short, a finite-state    traces a major trend in theoretical linguistics in which more
machine will behave as a Turing machine as long as the             of the grammar is shifted to the mental lexicon and the
memory limitations are not encountered. However, one can           distinction between lexical items and grammatical rules is
take the view that classical cognitive models of language are      beginning to evaporate.
approximate abstract descriptions of brain properties. A
                                                               1730

Information processing in dynamical systems                          Fundamentals of Theoretical Computer Science (2 ed.).
                                                                     San Diego, CA: Academic Press.
It is well-known that the class of symbolic processing
                                                                   Elman, J. L. (1990). Finding structure in time. Cognitive
models can be captured within the Church-Turing
                                                                     Science, 14, 179-211.
framework, which is computationally equivalent to the class        Goldie, C. M., & Pinch, R. G. E. (1991). Communication
of partially recursive functions (Davis, Sigal, & Weyuker,           Theory. Cambridge, UK: Cambridge University Press.
1994; Rogers, 2002). Hence it is possible to simulate all          Gomez, R. L., & Gerken, L. (2000). Infant artificial
finitely specified symbolic models as processes on numbers.          language learning and language acquisition. Trends in
Furthermore, it is known that these can be emulated in               Cognitive Sciences, 4, 178-186.
dynamical systems, including low-dimensional smooth                Grenholm, P. (2003). Artificial Grammar Learning - Rules
dynamical systems (Moore, 1991), analog recurrent                    and Statistics. Stockholm, Sweden: Karolinska Institutet.
networks (Siegelmann & Fishman, 1998), and spiking                 Hauser, M. D., Chomsky, N., Fitch, W. T. (2002). The
networks (Maass, 1996). Generally, network approaches                faculty of language. Science 198, 1569-1579.
offer interesting possibilities to model cognition within a        Haykin, S. (1998). Neural Networks: A Comprehensive
non-classical dynamical systems framework. For example, a            Foundation, 2nd ed. Upper Saddle River, NJ: Prentice
rich class of dynamical systems can be implemented in                Hall.
recurrent network architectures (Siegelmann & Fishman,             Joshi, A. K., & Schabes, Y. (1997). Tree-adjoining
1998; Legenstein & Maass, 2005). The recurrent network               grammars. In A. Salomaa (Ed.), Handbook of Formal
architecture can be viewed as a finite set of analog registers       Languages (Vol. 3: Beyond words). Berlin: Springer
(e.g., membrane potentials) that processes information               Verlag.
concurrently and interactively. It is natural to model             Lasota, A., & Mackey, M. C. (1994). Chaos, Fractals, and
cognitive brain functions in terms of spiking recurrent              Noise: Stochastic Aspects of Dynamics. New York:
networks and since the brain only represents 'numbers' in            Springer-Verlag.
terms of membrane potentials, inter-spike-intervals, or any        Legenstein, R., & Maass, W. (2005, preprint). What makes
appropriate set of dynamical variables, it is clear that the         a dynamical system computationally powerful? In S.
                                                                     Haykin, J. C. Principe, T. J. Sejnowski, and J. G.
human brain does not represent cognitive structures in a
                                                                     McWhirter (eds.), New Directions in Statistical Signal
simple transparent manner. However, general dynamical
                                                                     Processing: From Systems to Brain. Cambridge, MA:
system theory is obviously too rich as a framework for               MIT Press.
formulating explicit models of cognitive brain functions.          Maass, W. (1996). Lower bounds for the computational
For example, it turns out that for any given state-space one         power of networks of spiking neurons. Neural
can find a universal dynamical system whose traces will              Computation 8, 1-40.
generate any dynamics on the state-space (Lasota &                 McCauley, J. L. (1993). Chaos, Dynamics, and Fractals: An
Mackey, 1994). Thus, what is needed is a specification of            Algorithmic Approach to Deterministic Chaos.
cognitively relevant constraints as well as processing               Cambridge, UK: Cambridge University Press.
principles relevant for neurobiological networks subserving        McCulloch, W. S., & Pitts, W. (1943). A logical calculus of
information processing in the brain (Petersson, 2005b). Any          the ideas immanent in nervous activity. Bull. Math.
real progress on this front would represent a significant            Biophysics 5, 115-133.
generalization of Chomsky's concept of knowledge and               Moore, C. (1991). Generalized shifts: Unpredictability and
competence. On a final note, the existence of universal              undecidability in dynamical systems. Nonlinearity, 4,
dynamical systems, which in general are infinite                     199-230.
dimensional, suggests an additional possibility. In general,       Petersson, K. M., Forkstam, C., & Ingvar, M. (2004).
mapping data non-linearly into a high-dimensional space              Artificial syntactic violations activate Broca's region.
typically makes the data linearly separable and thus easier to       Cognitive Science, 28, 383-407.
learn. The dynamical version of this in conjunction with the       Petersson, K. M. (2004). The human brain, language, and
possibility that the neural infrastructure supports dynamics         implicit learning. Impuls - Journal of Psychology
of very high-dimensionality might suggest that brains can            (Norwegian), 58(3), 62-72.
learn 'traces' and thus acquire a rich spectrum of cognitive       Petersson, K. M. (2005a). On the relevance of the
skills using what appears to be surprisingly stereotypic             neurobiological analogue of the finite-state architecture.
architecture at a microscopic level (Petersson, 2005b).              Neurocomputing, 65-66, 825-832.
                                                                   Petersson, K. M. (2005b). Learning and Memory in the
                                                                     Human Brain. Stockholm, Sweden: Karolinska University
                        References                                   Press.
Badii, R., & Politi, A. (1997). Complexity: Hierarchical           Reber, A. S. (1967). Implicit learning of artificial grammar.
   Structures and Scaling in Physics. Cambridge, UK:                 J. Verb. Learn. & Verb. Behav., 6, 855-863.
   Cambridge University Press.                                     Rogers, H. (2002). Theory of Recursive Functions and
Chomsky, N. (2005). Three factors in language design.                Effective Computability. Cambridge, MA: MIT Press.
   Linguistic Inquiry, 36, 1 - 22.                                 Siegelmann, H. T., & Fishman, S. (1998). Analog
Davis, M. D., Sigal, R., & Weyuker, E. J. (1994).                    computation with dynamical systems. Physica D, 120,
   Computability,       Complexity,       and     Languages:         214-235.
                                                              1731

