UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bayesian Model for Recursive Recovery of Syntactic Dependencies
Permalink
https://escholarship.org/uc/item/5v172907
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Phaf, R. Hans
Rotteveel, Mark
Wendte, Robert
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   A Bayesian Model for Recursive Recovery of Syntactic Dependencies
                                          Virginia Savova (savova@jhu.edu)
                                    Department of Cognitive Science, 3300 N. Charles St
                                    Johns Hopkins University, Baltimore, MD 21218 USA
                                    Leonid Peshkin (pesha@hms.harvard.edu)
                                     Department of Systems Biology, 200 Longwood Ave
                                         Harvard University, Boston, MA 02115 USA
                            Abstract                              Markov models in syntax-related tasks like parsing. Syn-
                                                                  tactic dependencies in natural language are unboundedly
   Bayesian inference on graphical models can account for         non-local, in the sense that no fixed amount of context
   a variety of psychlogical data in non-linguistic domains.      is guaranteed to contain the members of a given con-
   Recent proposals have touted its biological plausibility,
   which raises the question to what extent it may capture        stituent. For example, consider the sentences in exam-
   the learning and use of grammar. We propose a way to           ples (1 - 3). In the first sentence, the subject king and
   structure the parsing task in order to make it amenable        verb bought are adjacent to one another. Thus, the de-
   to local classification methods. This allows us to build a     pendecy between them would be captured by a bigram
   Dynamic Bayesian Network which uncovers the syntac-
   tic dependency structure of English sentences. Exper-          (order-1) model. However, the same model would be un-
   iments with a small test corpus demonstrate that the           able to represent the dependendency in the second exam-
   model successfully learns from labeled data. We discuss        ple, because the subject and verb are separated by two
   what this approach may tell us about the way syntax            words. To capture this dependency, we need a 2nd-order
   may be encoded in the brain and about the modularity           Markov model (trigram). Similarly, the 2nd-order model
   of the language faculty.
                                                                  would prove inadequate for the third example, where the
                                                                  subject and verb are separated by four words.
Keywords
                                                                     (1) The king bought a camel.
Parsing, Finite-state models, Bayesian networks, Biolog-
ical plausibility, Dependency Grammars, Grammar in-                  (2) The king of Prussia bought a camel.
duction, Cognitive modeling.
                                                                     (3) The king of some strange country bought a
                                                                          camel.
                        Introduction
Bayesian graphical models have become an im-                         Our solution to this problem relies on representing sen-
portant explanatory strategy in cognitive science                 tences with non-local dependencies like (2, 3) as derived
( [Knill and Richards, 1996], [Kording and Wolpert, 2004],        from their local dependency variants, akin to (1). This
[Stocker and Simoncelli, 2005]). Recent work strongly             intuition is based on the formal notion that a string with
supports their biological plausibility in general and that        non-local dependency is obtained from a dependency
of dynamic Bayesian models in particular [Rao, 2005].             tree via a recursive linearization procedure. The string
Dynamic models are geared towards prediction and clas-            obtained at each step of the linearization procedure con-
sification of sequences. As such, they are naturally suit-        tains new local dependencies, which push apart local de-
able for language modeling and have already been aplied           pendencies from previous levels. This way of conceptu-
to tasks like speech recognition [Livescu et al., 2003] and       alizing the linearization of syntactic structure allows us
part-of-speech tagging [Peshkin et al., 2003]. However,           to use a Dynamic Bayesian Network despite its Markov
grammar learning and parsing with such models gener-              properties. We construct a DBN parser which decides
ally appears out-of-reach, because of their Markovian             only on local attachments. We then call the parser
character.                                                        recursively to uncover the underlying dependency tree.
   Markov models restrict possible dependencies to a              Our results show that the model captures grammatical
bounded, local context. At one extreme, the context               knowledge for all levels of the derivation. The biologi-
is confined to the symbol occupying the current position          cal plausibility and remarkable compactness of learned
in the sequence (order-0 or unigram models). In more              representation may suggest that parsing in the brain is
relaxed versions, context may include a fixed number of           accomplished in a similar manner.
positions before the current symbol (k-order), typically
no more than two (trigram models). The restricted space                         Dependency grammar
of possible dependencies allows transition probabilities          Tree-based linguistic representations of natural language
to be infered from the data and stored in a look-up table         syntax treat non-local dependencies as local in the two-
with relatively little technical sophistication.                  dimensional tree structure, of which the string is a one
   Not surprisingly however, the restricted space of rep-         dimensional projection. The dependency grammar rep-
resentable dependencies is also the main disadvantage of          resentation of (1) captures the dependency between the
                                                              1931

subject, the object and the verb, and the dependency be-        local dependency relationships (links). At each position,
tween the determiners and their respective nouns (Figure        we choose between setting the link to left, right, or
1).                                                             none, where left/right means the word is dependent
                                                               on its left/right neighbor. none means the search for
               bought                                          this word’s head should be postponed until later stages
               HH                                            of compression. The output of the classifier is a labeled
                       H                                      string, which can be compressed by removing linked de-
         king          camel                                   pendents. It is fed through recursively, until the string
       HH                                                    is compressed to the ROOT.
     the       of        a
            Prussia
                                                               The Dynamic Bayesian Network classifier
                                                               The first step towards building the classifier is coming up
     Figure 1: Dependency structure of example (2)             with a feature representation. We will briefly motivate
                                                               the choice of feature set with linguistic arguments. It is
   More formally, a dependency grammar consists of a           easy to determine that the linking pattern of a word de-
lexicon of terminal symbols (words), and an inventory          pends on its part of speech (PoS) and the part of speech
of dependency relations specifying inter-lexical require-      of its neighbor. For example, English determiners only
ments. A string is generated by a dependency grammar           link to the right, and adverbs link almost exclusively to
if and only if:                                                verbs. However, the parts of speech alone are not suffi-
• Every word but one (ROOT) is dependent on another            cient to determine linking behavior. In some cases, the
   word.                                                       identity of the adjacent word is required - bought accepts
                                                                links from nouns to the right, while slept does not.
• No word is dependent on itself either directly or indi-          Another decisive factor is how many dependents the
   rectly.                                                     current word has acquired so far. Since once the cur-
• No word is dependent on more than one word.                  rent word is linked it will become unavailable as a future
• Dependencies do not cross.                                   linking target to other words, it is important to acertain
                                                               that its valency has already been satisfied. Valency refers
In a dependency tree, each word is the mother of its de-       to the minimal number of dependents a word actively
pendents, otherwise known as their head. To linearize          seeks to license. In English and other SVO languages,
the dependency tree in Figure 1 into a string, we intro-       the word has particular requirements with respect to the
duce the dependents recursively next to their heads:           number of left and right dependents. Thus, in our fea-
Step I: bought                                                 ture representation, valency is indirectly captured by two
Step II: king bought camel                                     variables, which reflect the number of dependents which
Step III: The king of bought a camel                           had already been linked to the current word from ei-
Step IV: The king of Prussia bought a camel.                   ther side - left and right composite (comp). The
                                                               comp variables affect not only the linking behavior of
  Recursive parsing as local classification                    the current token, but that of its neighbor as well. If
Parsing in the dependency grammar framework is the             the word has already received many dependents from
task of uncovering the dependency tree given the sen-          one side, the probability of accepting yet another one
tence. Suppose that instead of searching for a complete        becomes smaller, since its valency is already satisfied.
parse given a complete sentence, we restricted our task            Finally, the current label depends on the labels of its
to compressing the string up the linearization path. Note      neighbor, because if the previous label is right, then
that linearization is essentially dependency parsing in re-    the current label cannot be left, and if the next label
verse. In other words, we can uncover the dependency           is left, the current label cannot be right. Thus, our
structure by labeling the local head-dependent relation-       full feature representation consists of the word and its
ships at the bottom linearization level (i.e. the sentence)    PoS tag, the words and PoS tags of its neighbor, the two
and erasing from the string the words whose heads are          valencies of the current word, the right valency of its left
already found. We recursively process the output until         neighbor and the left valency of its right neighbor, as
the root level. Thus, if as a first step in parsing (2),       well as the neighboring links.
we pick the head of Prussia to be the preposition of,              The Word and Next Word feature vocabulary contain
we can compress the string to a form virtually equiva-         the 2500 most frequent words in the data. An additional
lent to linearization Step III. Picking king as the head of    value was allocated for all remaining out-of-vocabulary
the preposition leads us to compress the string further,       words. The PoS, and Next PoS vocabulary contain 36 of
to the equivalent of step II. To compress the string, we       the original 45 Penn Treebank Tagset, after all punctu-
must simply identify which words in the string occupy a        ation P oS tags were removed. The left and right comp
position adjacent to their heads.                              features had tree values: none, one and many.
   The attractive feature of this representation is that           This feature representation is used as the basis of the
the parsing decisions taken at each step are local. Hence,     Dynamic Bayesian Network (dbn). After we briefly in-
parsing can be converted into a local classification task.     troduce the essential aspects of dbns, we wil expand on
The task is to chose the best sequence of labels denoting      the structure of the network for parsing. For more infor-
                                                           1932

mation on the general class of models, we refer the reader
to a recent dissertation [Murphy, 2002] for an excellent                         C   o n   t r     o     l
survey.
General notes on DBNs                                                      L i
                                                                               n   k
A dbn is a Bayesian network unwrapped in “time” (i.e.
over a sequence), such that it can represent dependencies                                                                         W
                                                                                                                                                  o       r   d
between variables at adjacent position. More formally, a
dbn consists of two models B 0 and B + , where B 0 defines
the initial distribution over the variables at position 0,                                                                            P
                                                                                                                                            o       S
by specifying:
• set of variables X1 , . . . , Xn ;                                                                                                    L     e       f
                                                                                                                                                            t   C     o   m   p
• directed acyclic graph over the variables;
• for each variable Xi a table specifying the conditional                                                                       R         i     g       h
                                                                                                                                                                  t C   o   m   p
    probability of Xi given its parents in the graph
    P r(Xi |P ar{Xi }).                                                                  s     l i   c e   K
                                                                                                                  s l i c e K +     1
The joint probability distribution over the initial state
is:
                                 Yn                                                              Figure 2: The parsing DBN.
           Pr(X1 , ..., Xn ) =        Pr(Xi |P ar{Xi }).
                                  1
                              +                                        Learning the parameters of a dbn from data is gen-
The transition model B specifies the conditional prob-
                                                                    erally accomplished using the EM algorithm. However,
ability distribution (cpd) over the state at time t given
                                                                    in our model, learning is equivalent to collecting statis-
the state at time t−1. B + consists of:
                                                                    tics over cooccurrences of feature values and link labels.
• directed acyclic graph over the variables X1 , . . . , Xn         This is implemented in gawk scripts and takes minutes
    and their predecessors X1− , . . . , Xn− — roots of this       on a large corpus. While in large dbns, exact inference
    graph;                                                         algorithms are intractable, and are replaced by a vari-
                                                                   ety of approximate methods, the number of hidden state
• conditional probability tables Pr(Xi |P ar{Xi }) for all         variables in our model is small enough to allow exact al-
    Xi (but not Xi− ).                                             gorithms to work. For the inference we use the standard
                                                                   algorithms, as implemented in a recently released toolkit
The transition probability distribution is:                        [Bilmes and Zweig, 2002].
                                        Yn                         Structure of the DBN parser
     Pr(X1 , ..., Xn X1− , ..., Xn− ) =     Pr(Xi |P ar{Xi }).
                                                                   Each slice of our DBN parser is a representation
                                         1
                                                                   of the joint probability distribution of word, pos,
Together, B 0 and B + define a probability distribution            left/right comp, and the hidden variable link Fig-
over the realizations of a system through time, which              ure 2. In our model, the link determines the value of
justifies calling these bns “dynamic”. In our setting, the         all variables and they are independent of one another.
word’s index in a sentence corresponds to time, while              Of course, this is not truly the case, but among those
realizations of a system correspond to correctly tagged            variables link is the only unobserved, hence modeling
English sentences. Probabilistic reasoning about such              all other dependencies is inconsequential. In addition to
system constitutes inference.                                      the intra-slice dependencies, we model dependencies be-
    Standard inference algorithms for dbns are similar to          tween the current, previous and next position. The link
those for hmms. Note that, while the kind of dbn we                variable infulences all aforementioned variables in neigh-
consider could be converted into an equivalent hmm, that           boring slices. Finally, we introduce a control variable
would render the inference intractable due to a huge re-            which deterministically ensures that at least one link in
sulting state space. In a dbn, some of the variables will           the sequence will be set to something other than none.
typically be observed, while others will be hidden. The             This forces the parser to trully compress the string at
typical inference task is to determine the probability dis-         each recursive parsing step.
tribution over the states of a hidden variable over time,
given time series data of the observed variables. This                                 Experiments and results
is usually accomplished using the forward-backward al-              For the results presented here we used the WSJ10 cor-
gorithm. Alternatively, we might obtain the most likely             pus [Klein and Manning, 2004]. It is a subset of the WSJ
sequence of hidden variables using the Viterbi algorithm.           Penn Treebank ([Marcus et al., 1993]), consisting of all
These two kinds of inference yield resulting link tags.             sentences shorter than eleven words with punctuation
                                                               1933

removed 1 . Eliminating the punctuation was done to                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              L         i n         k         :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             R                 i g   h           t                                                       L         e   f t
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0
simulate parsing in the oral modality. The dependency                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    W             o       r     d             :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             p     r     e     d             e               c             e       s           s   o r s   u   f       f   e                 r     e         d             b r e a           k     d     o   w n       .
annotation was obtained through automatic conversion                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           P         o         S           :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       n       o                 u   n
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             v     e           r   b
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n     o       u   n
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 r
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   o     o t
of the original treebank annotation. The relatively short
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2                         0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         L     e   f t C   o       m             p       :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0                         0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       R   i g   h   t C     o       m             p       :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0
sentences make this corpus a good approximation to ca-
sual speech and limit the effects of misattachments due
to the conversion. Note that the parser is in principle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Figure 6: Third layer representation.
capable of handling longer sentences.
Encoding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 L             i       n             k         :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     R           i     g       h     t
The corpus was encoded in our feature representation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             W                   o             r           d             :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         s       u           f         f       e           r     e       d               .
as follows. For each sentence, a number of feature files                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   P             o           S                     :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     v           e         r         b
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     r o         o     t
were produced containing the feature representation of                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               L     e     f   t     C       o             m                     p               :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0
the sentence at each linearization level. The encoding of                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    R         i g     h       t     C       o             m                     p               :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0
an actual sentence-structure pair from our corpus (Fig-
ure 3), is illustrated in Figures 4 to 7.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Figure 7: Top layer representation.
        h       e       r                     i     m                   m               e               d                   i     a           t   e                 p           r       e               d             e     c     e         s       s               o           r               s     u     f   f   e     r       e               d                       a     n             e         r       v       o           u         s                       b       r   e       a         k               d         o   w     n           .
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            all levels with the correct labels. It was trained directly
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            on the annotations, with no additional smoothing. The
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            result achieved was 79% correct link attachment for di-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rected dependencies, and 82% for undirected. We com-
                                                                                                                                  Figure 3: Dependency structure.                                                                                                                                                                                                                                                                                                                                                                                           pare the results to two baselines given for this corpus by
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [Klein and Manning, 2004], Table 1.
            At the lowest level, no word has any discovered depen-
dents, hence the comp values are zero everywhere. All
links of words whose heads are not adjacent are labeled                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 1: DBN results against baseline.
none (0).
        At the next level, words whose labels were left or                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Model                                                                                                                                                                          Accuracy
                                  L       i     n             k     :
                                                                                                                                  0                                                 R             i g         h       t                                                                     0                                                                           0                                     0                             R             i   g     h     t                                                 0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Dir                                                                                         Undir
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      DBN                                                                                                                                                                            79                                                                                          82
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Random                                                                                                                                                                         30                                                                                          46
                          W         o             r         d         :
                                                                                    h           e                   r                     i     m         m e             d               i   a           t     e               p     r e         d             e             c       e       s     s   o     r       s       u       f       f         e       r     e       d   a                         n     e         r   v         o           u         s               b   r e       a             k           d       o   w n         .
                                P           o             S         :
                                                                                      p       r       o           n                                           a         d           j                                                                 n           o         u     n                                             v         e           r     b                       d       e     t                               a     d   j                                               n     o       u           n                   r o     o   t
  R
    L
      i   g
            e
              h
                  f t
                    t
                      C
                      C
                            o
                              o
                                      m
                                        m
                                                        p
                                                        p
                                                                    :
                                                                      :
                                                                                                  0
                                                                                                  0
                                                                                                                                                                      0
                                                                                                                                                                      0
                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                                                                                                0
                                                                                                                                                                                                                                                                                                                                                0
                                                                                                                                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Adjacent heuristic                                                                                                                                                             34                                                                                          57
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               More detailed results for our model are shown in Ta-
                                                                                                      Figure 4: First layer representation.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ble 2 . The results unequivocally surpass the random
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            baseline, and the best available heuristic, which amounts
right are removed from the structure and the comp                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           to linking every word to its right neighbor. This suggests
counters for their head are incremented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    our model has learned at least some of the non-trivial
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            dependencies which govern the choice of link structure.
                                                                                                                L         i     n           k         :                               R               i     g       h   t
                                                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                                                                                                                                  0
                                                                                                                                                                                                                                                                                                                                                                                          R         i     g         h     t
                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The minimal difference between the vocabulary and out-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            of-vocabulary scores imply that the network can recover
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            the syntactic properties of an unknown word in context.
                                                                                                                                                                  h           e                 r                 p       r   e     d         e           c               e         s     s       o       r       s     u         f     f           e         r     e       d         a                       b       r       e         a         k         d           o     w         n                       .
                                                                                                    W                 o             r     d             :
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The fact that the root accuracy is higher than the non-
                                                                                                              P               o       S             :
                                                                                                                                                                p         r       o         n
                                                                                                                                                                                                                                          n     o           u         n                                                     v       e           r         b                             d       e     t                               n       o         u         n                             r       o         o       t
                                                                                                                                                                            0
                                                                L         e   f t         C               o             m               p             :
                                                                                                                                                                                                                                                                                                                                            0                                             0                                                                                                                   0
                                                      R           i   g     h     t         C               o           m               p             :
                                                                                                                                                                            0
                                                                                                                                                                                                                                                        0
                                                                                                                                                                                                                                                              1
                                                                                                                                                                                                                                                                                                                                            0                                             0                                                     0
                                                                                                                                                                                                                                                                                                                                                                                                                                                    1
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            root accuracy allows us to conclude that the network cor-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rectly learns to postpone decisions about the root word
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            in all cases, and about its dependent in most cases.
                                                                                Figure 5: Second layer representation.
The same procedure produces the subsequent levels (Fig-
ures 6, 7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Table 2: Detailed results for the DBN.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Measure                                                                                                                                                                                                                       Accuracy
Testing
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Root dependency                                                                                                                                                                                                               83
The corpus was split randomly 9:1 into a training and
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Non-root dependency                                                                                                                                                                                                           78
testing section. In training mode, the DBN was given
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Out-of-Vocabulary                                                                                                                                                                                                             75
                1
                      the dot in our figures stands for an abstract ROOT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Sentence                                                                                                                                                                                                                      36
symbol
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1934

                       Discussion                              networks of noisy integrate-and-fire neurons can per-
                                                               form approximate Bayesian inference for dynamic and
Our results show that combining a DBN model with re-           hierarchical graphical models. According to him, the
cursive application is a reasonable parsing strategy. This     membrane potential dynamics of neurons corresponds
opens the door to the hypothesis that Bayesian infer-          to approximate belief propagation in the log domain,
ence is a possible mechanism for parsing in the brain,         and the spiking probability of a neuron approximates
despite the Markovian properties of the corresponding          the posterior probability of the preferred state encoded
dynamic models. The high ROOT accuracy suggests                by the neuron, given past inputs. This seems to suggest
that the model has captured some fundamental princi-           that our parsing model can be implemented in a neu-
ples defining the local dependency structure at all levels     ral circuit. Furthermore, since the same DBN is used to
of the derivation. We take this result as evidence that        uncover local dependencies throughout all levels of the
graphical models with Markov properties are capable of         derivation, such implementation would address Hum-
handling unbounded non-local dependencies through re-          boldt’s characterization of language as a system that
cursive calls on their own output. The implication of this     makes “infinite use of finite means” at the neurophys-
finding transcend Bayesian graphical models and speak          iological level. The same neural aparatus could be used
to the general issue of how relevant other biologically        to recursively uncover the dependency structure of a sen-
plausible Markov models can be to language processing          tence level by level.
and learning. For example, Elman networks have been               Another implication of our work is that the nature
criticized for their a priori limitation in handling un-       of the processing architecture may constrain the kind of
bounded dependencies [Frank et al., 2005]. It is possible      grammar human languages permit. If indeeed parsing is
that such type of models may be adapted to discover            accomplished through recursive processing of the output
locality in the hierarchical structure through recursive       of previous stages, some types of long-distance depnden-
application.                                                   cies would be impossible to detect. In particular, if the
   One exciting implication of this hypothesis is the          material intervening between a head-dependent pair (H,
domain-generality of Bayesian inference and learn-             D) is not a constituent whose own head depend on ei-
ing mechanisms. Previous work has proposed that                ther H or D, our model would not be able to uncover
these mechanisms are involved in visual perception             it because H and D will not be adjacent at any point
[Knill and Richards, 1996], [Kersten and Yuille, 2003],        in the derivation. In other words, this parser is inca-
motor control [Kording and Wolpert, 2004],            and      pable of handling strictly context-sensitive languages. to
attention         modulation         [Yu and Dayan, 2005].     the extent that such dependencies exist, they are fairly
[Kersten and Yuille, 2003] proposes Bayesian graphical         limited [Shieber, 1985]. Such cases will need to be re-
model of object detection which rely on estimating hid-        solved through some reordering in pre-processing, possi-
den variables such as relative depth and 3-D structure         bly based on case marking.
from observables they influence -shadow displacement,
2-D projection. [Kording and Wolpert, 2004] suggests
that subjects ina sensory-motor experiment internally                                Future work
represent both the statistical distribution of the task        One deficiency of our model is that decisions at lower
and their sensory uncertainty, combining them in               levels cannot be reversed in the interest of more optimal
a manner consistent with a performance-optimizing              choices at higher levels. There are however important
bayesian process. In our work, the hidden links are            reasons why this might be necessary. For example, a
estimated from observable word and PoS, along with a           prepositional phrase subcategorized for by the verb may
prior label distibution.                                       be mistakenly attached to a preceding noun phrase, leav-
   The parallelism in the proposed cognitive strategies for    ing the verb with a missing dependent (4)
all these different modalities may shed light on the issue
whether and how modular the language faculty is. The              (4) The king put *[the camel in the trunk].
modularity hypothesis states that the cognitive mech-
anisms underlying linguistic competence are specific to        In the future, we hope to address this problem through a
language. If Bayesian inference proves to be a plausi-         form of beam search - retaining the k-best parses at each
ble uniting principle behind visual, motor and linguistic      level and choosing among them based on what happens
abilities, this hypothesis is seriously undermined. At the     at the next level.
same time, it is important to note that the generality            Another important issue that we need to address is
of the mechanism does not necessarily negate the mod-          the total loss of information about the dependents that
ularity of language completely. The feature represen-          have been linked to a word at previous levels. Some
tation which our model used already encodes language-          well-known cases pose a problem for this aspect of our
specific knowledge. Further research is needed to deter-       model. For example, the sentences in (6) and (5) are
mine whether the feature representation and the struc-         structurally distinct solely becase the complement of the
ture of the network can be induced through structure           prepositional phrase in the second sentence is an instru-
learning algorithms.                                           ment appropriate for seeing.
   Our approach is particularly appealing in light of re-         (5) The king saw [the camel with two humps] .
cent work suggesting that Bayesian type inference is bi-
ologically plausible. [Rao, 2005] shows that recurrent            (6) The king saw *[the camel with a telescope].
                                                           1935

   In our current model, once the complement is linked to      [Knill and Richards, 1996] Knill, D. C. and Richards,
the preposition, the two sentence will become identical,          W., editors (1996). Perception as Bayesian inference.
and one of them will be assigned the wrong structure.             Cambridge University Press.
This concern can be addressed through introducing new
variables, which keep track not only of the number of          [Kording and Wolpert, 2004] Kording, K. and Wolpert,
linked dependents but of their semantic category (e.g.            D. (2004). Bayesian integration in sensorimotor learn-
instrument, animate etc.)                                         ing. Nature, 427:244–247.
   A natural way to extend our model in a different di-        [Livescu et al., 2003] Livescu, K., Glass, J., and Bilmes,
rection is to combine it with the Bayesian PoS tagger             J. (2003). Hidden feature models for speech recogni-
developed in [Peshkin et al., 2003]. Allowing the model           tion using dynamic bayesian networks. In 8th Euro-
to infer PoS tags and structure simultaneously will be            pean Conference on Speech Communication and Tech-
a significantly better approximation to the parsing task          nology (Eurospeech).
humans are faced with. Last but not least, we would
like to implement semisupervised learning. One way to           [Marcus et al., 1993] Marcus, M., Santorini, B., and
do this would involve starting off with a small labeled           Marcinkiewicz, M. A. (1993). Building a large an-
set of sentences at all parsing depths, followed by pre-          notated corpus of english: the penn treebank. Com-
senting unparsed whole sentences. The parses suggested            putational Linguistics, 19.
by the model would in their turn be used for learning in
a bootstrap fashion.                                           [Murphy, 2002] Murphy, K. (2002). Dynamic Bayesian
                                                                  Networks: Representation, Inference and Learning.
                                                                  PhD thesis, Univ. of California at Berkeley.
                      Conclusion
                                                               [Peshkin et al., 2003] Peshkin, L., Pfeffer, A., and
In our closing remarks, we would like to emphasize sev-           Savova, V. (2003). Bayesian nets for syntactic catego-
eral aspects of our parsing model which make it inter-            rization of novel words. In Proceedings of the NAACL.
esting from the perspective of cognitive science. First,
it belongs to a class of models which have been used re-       [Rao, 2005] Rao, R. P. N. (2005). Hierarchical bayesian
cently to capture cognitive mechanisms in non-linguistic          inference in networks of spiking neurons. In Saul,
domains. Second, it naturally utilizes the overwhelming           L. K., Weiss, Y., and Bottou, L., editors, Advances
“disguised locality” of natural language syntax - in other        in Neural Information Processing Systems 17. MIT
words, it benefits from the fact that string-non-local de-        Press, Cambridge, MA.
pendencies are tree-local. Third, it is biologically plau-
sible because it has been shown to be implementable in         [Shieber, 1985] Shieber, S. (1985). Evidence against the
a neural circuit. And finally, it takes seriously the ques-       context-freeness of natural language. Linguistics and
tion how the finite amount of brain hardware is capable           Philosophy, 8:333–343.
of encoding structures of unbounded depth. While there
                                                               [Stocker and Simoncelli, 2005] Stocker, A. and Simon-
is much room for improvement, we believe these qualities
                                                                  celli, E. (2005). Constraining a bayesian model of hu-
make it an important step on the difficult road toward
                                                                  man visual speed perception. In Saul, L. K., Weiss, Y.,
understanding how the mind emerges from the brain.
                                                                  and Bottou, L., editors, Advances in Neural Informa-
                                                                  tion Processing Systems 17. MIT Press, Cambridge,
                                                                  MA.
                      References                               [Yu and Dayan, 2005] Yu, A. J. and Dayan, P. (2005).
[Bilmes and Zweig, 2002] Bilmes, J. and Zweig, G.                 Inference, attention, and decision in a bayesian neural
   (2002). The graphical models toolkit: An open source           architecture. In Saul, L. K., Weiss, Y., and Bottou,
   software system for speech and time-series processing.         L., editors, Advances in Neural Information Process-
   IEEE Int. Conf. on Acoustics, Speech, and Signal Pro-          ing Systems 17. MIT Press, Cambridge, MA.
   cessing, June 2002. Orlando Florida.
[Frank et al., 2005] Frank, R., Mathis, D., and
   Badecker, W. (2005). The acquisition of anaphora by
   simple recurrent networks. unpublished manuscript.
[Kersten and Yuille, 2003] Kersten, D. and Yuille, A.
   (2003). Bayesian models of object perception. Cogni-
   tive Neuroscience, 13:1–9.
[Klein and Manning, 2004] Klein, D. and Manning, C.
   (2004). Corpus-based induction of syntactic structure:
   Models of dependency and constituency. In Proceed-
   ings of the 42nd Annual Meeting of the ACL.
                                                           1936

