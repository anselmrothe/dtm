UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Incremental Effects of Mismatch during Picture-Sentence Integration: Evidence from Eye-
Tracking
Permalink
https://escholarship.org/uc/item/7173d5tj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Crocker, Matthew W.
knoeferle, Pia
Publication Date
2005-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

 Incremental Effects of Mismatch during Picture-Sentence Integration:
                                        Evidence from Eye-tracking
                                     Pia Knoeferle (knoeferle@coli.uni-sb.de)
                                          Department of Computational Linguistics,
                                     Saarland University, 66041 Saarbrücken, Germany
                                Matthew W. Crocker (crocker@coli.uni-sb.de)
                                          Department of Computational Linguistics
                                     Saarland University, 66041 Saarbrücken, Germany
                          Abstract                                that when there is a match between a picture and a sen-
                                                                  tence, their integration should be faster than when a
   A model of sentence-picture integration developed              picture and a sentence do not match.
   by Carpenter and Just (1975) predicts that picture-
   sentence integration ease/difficulty depends on picture-
   sentence match/mismatch respectively. Recent find-                          A Model of Incremental
   ings by Underwood, Jebbet, and Roberts (2004), how-                     Sentence-Picture Comparison?
   ever, fail to find a match/mismatch difference for se-
   rial picture-sentence presentation in a sentence veri-         The CCM has received strong support from off-line re-
   fication study. In a sentence comprehension study              sponse latencies in verification tasks, and is primarily a
   with serial picture-sentence presentation we find no           model of sentence-picture verification. The model spec-
   match/mismatch effect in total sentence inspection
   times. However, inspection times for individual sentence       ifies - at least to some extent - how the integration of
   regions reveal a mismatch effect at the very sentence          picture and a written sentence proceeds incrementally:
   constituent for which the corresponding picture con-           by serially comparing the representations of sentence and
   stituent mismatches, and this in a study with a sentence       corresponding picture constituents.
   comprehension rather than verification task. Drawing              Reaction times are approriate for testing the complex-
   on insights about spoken sentence comprehension dur-
   ing the inspection of concurrent scenes, we suggest that       ity of sentence-picture integration steps. However, for
   the absence of a mismatch effect in the Underwood et al.       truly examining the incremental integration of picture-
   studies might be due to grain size of gaze time analyses.      and sentence-based mental representations they are less
                                                                  informative than other, on-line measures such as eye-
                                                                  tracking. In sentence-picture integration research, few
                      Introduction                                studies have monitored eye-movements during sentence
How do we integrate what we see in a scene with a                 reading (e.g., Carroll et al., 1992; Underwood et al.,
sentence that we read? Answering this question is of              2004). Among recent studies in the sentence-picture
interest in various types of comprehension situations             verification paradigm that have employed eye-tracking,
such as when we read comic books (Carroll, Young,                 findings by Underwood et al. (2004) have challenged the
& Guertin, 1992), newspaper advertisements (Rayner,               validity of the CCM. They have further identified im-
Rotello, Stewart, Keir, & Duffy, 2001), or inspect sci-           portant additional factors (e.g., order of picture-sentence
entific diagrams (Feeney, Hola, Liversedge, Findlay, &            presentation) that affect their integration.
Metcalf, 2003).                                                      In two eye-tracking studies with a sentence-picture
   One account of how a picture and sentence are inte-            verification task, Underwood et al. (2004) examined the
grated is the “Constituent Comparison Model” (CCM)                effect of presentation order for real-world photographs
by Carpenter and Just (1975). They suggest that peo-              and captions. They report total inspection time, num-
ple build a mental representation of sentence and picture         ber of fixations, and durations of fixations for the en-
constituents, and that the corresponding constituents of          tire sentence and picture in addition to response la-
sentence and picture are then serially compared with              tencies. In Experiment 1, picture and caption were
one another. Their model of sentence verification ac-             presented together, and congruence was manipulated
counts for response latencies in a number of sentence-            (match/mismatch). Results confirmed the established
picture verification studies by attributing differences in        match/mismatch effect: Response latencies were longer
the response latencies to congruence/incongruence be-             for the mismatch than for the match condition. Total in-
tween sentence and picture (e.g., Gough, 1965, Just &             spection times and number of fixations further confirmed
Carpenter, 1971).                                                 this finding.
   In a sentence verification task, Just and Carpenter               In Experiment 2, order of presentation (picture-first,
(1971) presented people with a picture of either red or           sentence-first) was introduced as a condition in addition
black dots, followed by a related written sentence. Sen-          to the match/mismatch manipulation. Crucially, and in
tence verification response latencies were shorter when           contrast to Experiment 1, there was no match/mismatch
the colour adjective in the sentence (red ) matched the           effect in Experiment 2 in either response latencies or in-
colour of the depicted dots (red) than when it did not            spection times for the entire sentence. Response accu-
match their colour (black). The CCM predicts precisely            racy was relatively high (83.6 and 79.2 percent for match
                                                             1166

and mismatch responses respectively, with no reliable             monitored attention in scenes to investigate the interac-
difference between match/mismatch conditions). Fur-               tion of scene and spoken sentence.
ther findings were faster reaction times for the sentence-           Studies using concurrent scene-utterance presentation,
first in comparison with the sentence-last condition. The         have revealed important insights into the incremental in-
reaction time findings were confirmed by total inspection         tegration of an immediate scene context with an unfold-
times.                                                            ing spoken sentence. Shortly after a word in the utter-
   The findings by Underwood et al. challenge the gen-            ance identifies a scene constituent as its referent, peo-
eral validity of the CCM as a model of sentence-picture           ple inspect the relevant scene object (e.g., Tanenhaus,
integration. They lend support to the view that the               Spivey-Knowlton, Eberhard, & Sedivy, 1995). Sedivy,
sentence-picture integration process cannot be explained          Tanenhaus, Chambers, & Carlson (1999) show the incre-
by one factor (e.g., match/mismatch), but might rather            mental influence of contrast between objects in the scene
be the product of complex interactions between a num-             on semantic interpretation. Knoeferle et al. (2005) have
ber of factors.                                                   in turn found a rapid influence of the immediately de-
   Despite their merits in advancing our knowledge of             picted scene events on on-line thematic role assignment
sentence-picture integration, existing eye-tracking stud-         once the verb in the sentence had identified the relevant
ies in the sentence verification paradigm (e.g., Carroll          depicted action and its associated role relations.
et al., 1992; Underwood et al., 2004) offer limited in-              The close time-lock of utterance and attention in a
sights into how picture and sentence are integrated on a          scene has importantly been extended to serial picture-
constituent-by-constituent basis. Underwood et al., re-           utterance presentation. In Altmann (2004) people in-
port, for instance, only total reading times for the entire       spected an image with a man, a woman, a cake, and
sentence rather than reading times for individual sen-            a newspaper, and then the screen went blank (see also
tence regions.                                                    Richardson & Spivey, 2000; Spivey, Richardson, & Fit-
   It is gaze times in individual sentence regions, how-          neva, 2004). Two seconds later, people heard a sentence
ever, which prior psycholinguistic research on sentence           that described part of the previously-inspected scene
processing has established as a reliable measure for the          (e.g., The man will eat the cake). Once people had heard
incremental processing of relevant syntactic, semantic,           the verb in the sentence, they rapidly looked at the loca-
and pragmatic information (see Rayner, 1998 for re-               tion on the blank screen where previously there had been
view). We suggest that a more fine-grained analysis of            a cake. The time-course of eye-movements in the serial-
gaze-durations may reveal more about the incremental              presentation study closely ressembled the time-course of
nature with which sentence and picture are integrated             gaze-patterns in an earlier study (Altmann & Kamide,
when they match and when they are incongruent.                    1999) with concurrent scene and utterance presentation.
                                                                  These findings are consistent with the CCM. The close
      Does the Model Generalize Across                            time-lock between utterance comprehension and scene
                           Tasks?                                 inspection observed in all of these studies reflects a serial
An important issue for on-line sentence comprehension             comparison and integration of the unfolding utterance
concerns the claims that the CCM makes in situations              constituents and corresponding scene constituents. The
where the sentence-picture integration process is not             findings from spoken sentence comprehension in scenes
modulated by a verification task. Serial picture-sentence         even go beyond the serial constituent-by-constituent in-
constituent comparison is predicted to take place “when-          tegration predicted in the CCM and show a certain de-
ever a person answers a question, follows an instruction,         gree of anticipation in sentence-picture integration. The
or incorporates statements into his belief system” (Car-          important insight from the above studies for the present
penter & Just, p. 72).                                            paper is, however, that a serial picture-utterance inte-
   Recall that there was no match/mismatch effect in              gration was observed during both concurrent and serial
the serial-order presentation study by Underwood et               picture-utterance presentation.
al. (2004). Underwood and colleagues suggest that its                The present paper directly explores whether the CCM
absence may be due to task requirements. They rea-                is valid for serial presentation, whether it generalizes
son that in other studies (e.g., Goolkasian, 1996), the           from sentence verification to a sentence comprehension
match/mismatch effect was only present when partici-              task, and whether a more fine-grained analysis of the
pants were asked to make a verbatim comparison be-                eye-gaze data provides more detailed insights into the
tween picture and words. If task requirements were                incremental integration of picture and written sentence
indeed responsible for the presence or absence of the             for serial picture-sentence presentation. To this end, we
match/mismatch effect, then this would seem to question           designed a study which combined a sentence comprehen-
the general validity of the CCM as a model of sentence-           sion task (e.g., Altmann & Kamide, 1999; Knoeferle et
picture processing.                                               al., 2005) and a match/mismatch manipulation during
   To further investigate issues of task-specificity, let us      serial presentation.
consider sentence-picture integration in passive sentence            If serial presentation and/or task eliminates the
comprehension and act-out tasks. While few psycholin-             match/mismatch effect, then we should see no such ef-
guistic studies on on-line sentence comprehension have            fect in the eye-gaze data of our study. Finding a dif-
examined the integration of a written sentence and a pic-         ference, however, in sentence or word-region gaze-times
ture (e.g., Carroll et al., 1992), a number of studies have       would support the view that the Carpenter and Just
                                                             1167

model does generalize to tasks other than verification            Design From the 108 images and 72 sentences, we cre-
and across presentation order. Furthermore, if the grain-         ated 36 item sets. An item consisted of three images
size of gaze-analyses is one reason why Underwood et              (e.g., Fig. 1) and two written sentences (e.g., Table 1).
al. (2004) failed to find a difference between picture-           A three-by-two within-subject design crossed congruence
sentence integration in the match/mismatch conditions,            (match, mis1, mis2) with sentence type (SVO, OVS), re-
then we would expect to observe a difference in finer-            sulting in six conditions. Each of the two sentences in
grained word-region gaze-times while replicating their            Table 1 was presented with each of the images (a, b, and
finding that total sentence inspection times showed no            c in Fig. 1).
effect of match/mismatch.
   To investigate in addition the influence of other fac-
tors on sentence-picture integration, we manipulated the                       Table 1: Example Item Sentences
word order of the sentence. In this respect, the present
picture-written sentence study continues research by                Cond.   Sentence
Knoeferle et al. (2005) who investigated comprehension              SVO     Die Oma (subj.) filmt soeben den Handelskaufmann (obj.)
of spoken German sentences with local structural and                        nach dem Vertragsabschluss.
thematic role ambiguity. German is a language with rel-                     The granny (subj.) films currently the businessman (obj.)
atively flexible word order. Both a subject-verb-object                     after the signing of the contract.
(SVO), and an object-verb-subject (OVS) ordering of                         ‘The granny films currently the businessman
                                                                            after the signing of the contract.’
constituents is grammatical, with the SVO order being
preferred. People inspected a scene in which a princess
                                                                    OVS     Die Oma (obj.) filmt soeben der Handelskaufmann (subj.)
both paints a fencer and is washed by a pirate while hear-                  nach dem Vertragsabschluss.
ing either an initially ambiguous SVO or OVS sentence.
                                                                            The granny (obj.) films currently the businessman (subj.)
Late disambiguation occurred through case-marking on                        after the signing of the contract.
the second noun phrase, and earlier disambiguation was                      ‘The businessman films currently the granny
only possible through depicted events. When the verb                        after the signing of the contract.’
identified an action, its associated depicted role relations
disambiguated towards either an agent-patient (SVO) or
patient-agent role relation (OVS), as evidenced by antic-            When the SVO sentence was presented with Fig.
ipatory eye-movements to the patient (pirate) or agent           1a, it matched the depicted agent-patient role rela-
(fencer) respectively. A further goal of the present study       tions (granny-filming-businessman) in the scene (SVO-
is thus to investigate the integration of depicted event         match). In contrast when it was presented with Fig. 1b,
scenes and reading mechanisms for initially ambiguous            the scene contained constituents that matched individual
written German sentences.                                        sentence constituents (NP1, verb, NP2), however, the
                                                                 role relations in the scene (businessman-filming-granny)
                      Experiment                                 were the opposite of the thematic relations expressed by
                                                                 the SVO sentence in Table 1 (SVO-mis1). When pre-
Method                                                           sented with Fig. 1c, the scene did not contain explicitly
Participants Thirty-six German native speakers with              depicted actions, and was incongruent with the sentence
normal or corrected-to-normal vision received each 7.50          through an absence of the thematic relations expressed
euro for taking part in the experiment.                          by the sentence (SVO-mis2). When the OVS sentence in
                                                                 Fig. 1, appeared together with Fig. 1a the thematic rela-
Materials We created 108 images using commercially               tions in the sentence (businessman-filming-granny) were
available clipart and graphics programs. An image ei-            the opposite of the role relations in the scene (granny-
ther depicted a female-action-male (e.g., granny-filming-        filming-businessman) (OVS-mis1). In contrast, present-
businessman, Fig. 1a), a male-action-female event                ing the OVS sentence with Fig. 1b resulted in a match of
(businessman-filming-granny, Fig. 1b), or there was              depicted and thematic role relations (OVS-match). For
no explicitly depicted (granny, businessman, Fig. 1c).           Fig. 1c, the scene was incongruent with the sentence
There were 72 sentences, 36 of which described a female-         since it did not explicitly depict role relations (OVS-
action-male, and 36 of which described a male-action-            mis2).
female event. The experiment was carried out in Ger-
man. Since the feminine character was always men-                Procedure An SMI Eye-Link head-mounted eye-
tioned first, and the first noun phrase was case- and             tracker monitored participants eye-movements at a fre-
role-ambiguous, sentences were locally structurally am-           quency of 250 Hz. Images were presented on a 21-inch
biguous. One sentence of an item had a subject-verb-              multi-scan color monitor at a resolution of 1024 x 768
object (SVO) order (see Table 1), and the second sen-             pixel. Pictures and sentences were presented serially,
tence always had an object-verb-subject (OVS) order.              and the picture was always displayed first. For each
The only difference between the two sentences was the             trial, participants first focussed a centrally-located fix-
disambiguating definite determiner of the second noun             ation dot on the screen. The experimenter then trig-
phrase, which was in the accusative case (den, ‘the’) for         gered the image, which participants inspected. Once
SVO, and in the nominative case (der, ‘the’) for OVS              they had inspected and understood the picture, they in-
sentences.                                                        dicated this by pressing a button. A template with a
                                                             1168

                                                               rated by at least one filler item. The entire experiment
                                                               lasted approximately 45 min with a short break after
                                                               half the trials.
                                                               Results
                                                               Figs 2, 3, 4, and 5 show the mean total reading times
                                                               (duration of all fixations in a region) per condition for
                                                               the NP1 (e.g., ‘The granny’), verb (‘films’), adverb (‘cur-
                                                               rently’), and NP2 (‘the businessman’) sentence regions
                                                               respectively. For the inferential analyses, we carried out
                                                               repeated measures ANOVAs. Analysis by participants
                                                               are reported as F1, and by items as F2.
                                                                 There was no effect of congruence (match, mis1, mis2)
                                                               on the NP1 region in total time (Fig. 2), both F s < 1,
                                                               nor was there an interaction between congruence and
                                                               sentence type. We found an effect of sentence type on
                                                               NP1, F 1(1, 35) = 12.39, p < 0.01, F 2(1, 35) = 18.89, p <
                                                               0.001.
                                                                                                              1400
                                                                                                              1200
                                                                            Mean Total Inspection Time (ms)
                                                                                                              1000
                                                                                                              800
                                                                                                              600
                                                                                                              400
                                                                                                              200
                                                                                                                 0
                                                                                                                      SVOmatch   SVOmis1   SVOmis2   OVSmatch   OVSmis1   OVSmis2
                                                                  Figure 2: Mean total reading time for the NP1 region
            Figure 1: Example Item Images
                                                                                                                800
                                                                            Mean Total Inspection Time (ms)
                                                                                                                600
black fixation square at the position of the first word in
the subsequent sentence appeared automatically. Once
participants fixated the square, the experimenter pre-                                                          400
sented the sentence. Participants read the sentence, and
indicated that they had read and understood it by press-                                                        200
ing a button. The only task was to read and understand
the sentences, i.e., there was no explicit sentence ver-                                                        0
ification task. There were no questions on any of the
images, and no questions on the item sentences. How-                                                                  SVOmatch   SVOmis1   SVOmis2   OVSmatch   OVSmis1   OVSmis2
ever, for 24 of the 48 filler trials participants answered a
yes/no question which always referred to the sentence.
   The items were randomized and one version of each
item was assigned to one of six experimental lists. Items         Figure 3: Mean total reading time for the verb region
were rotated across lists such that an equal number
of each condition (SVO-match, SVO-mis1, SVO-mis2,                For the verb region, where the mismatch occurred
OVS-match, OVS-mis1, OVS-mis2) appeared in each list           (Fig. 3), in contrast, the key finding was a main effect of
and such that no participant saw more than one version         congruence (match, mis1, mis2), F 1(2, 34) = 16.51, p <
of each item. Each list consisted of 36 experiment and         0.001, F 2(2, 34) = 7.22, p < 0.01. There was further a
48 filler items. Consecutive experiment trials were sepa-      main effect of sentence type (SVO, OVS) on the verb,
                                                           1169

F 1(1, 35) = 17.19, p < 0.001, F 2(1, 35) = 23.40, p <                                                                  In contrast to the analyses for the individual regions,
0.001, in the absence of an interaction between congru-                                                               neither the duration of inspection for the whole sentence
ence and sentence type, ps > 0.2.                                                                                     F s < 1, nor mean total sentence inspection time (ps >
   In the adverb region (Fig. 4), total reading time                                                                  0.2) showed a reliable match/mismatch effect.
analyses revealed a main effect of congruence by par-
ticipants (match, mis1, mis2), F 1(2, 34) = 3.37, p <                                                                                       Discussion
0.05, F 2(2, 34) = 2.69, p = 0.08. There was further a                                                                The eye-gaze data that we report provide strong support
main effect of sentence type (SVO, OVS), F 1(1, 35) =                                                                 for the Constituent Comparison Model, and furthermore
18.29, p < 0.001, F 2(1, 35) = 40.28, p < 0.001, and a                                                                allow us to integrate predictions by the model with prior
marginal interaction of congruence and sentence type,                                                                 research on picture-sentence integration during concur-
F 1(2, 34) = 2.75, p = 0.08, F 2(2, 34) = 1.52, p = 0.23.                                                             rent scene and utterance presentation (Knoeferle, 2004,
   For the NP2 region (see Fig. 5) analyses revealed only                                                             Knoeferle & Crocker, 2004; Knoeferle et al., 2005; Sedivy
a main effect of sentence type, F 1(1, 35) = 44.12, p <                                                               et al., 1999; Tanenhaus et al., 1995).
0.001, F 2(1, 35) = 90.54, p < 0.001, no effect of congru-                                                               The fact that we observed a match/mismatch effect
ence, and no interaction between sentence type and con-                                                               in a sentence comprehension task shows that the CCM
gruence (all ps > 0.1).                                                                                               generalizes to tasks other than verification, and supports
                                                                                                                      its validity as a model of picture-sentence integration.
                                                                                                                      In more detail, analyses of gaze durations (at the verb)
                                              1000
                                                                                                                      revealed a match/mismatch effect and that even for a
                                                                                                                      presentation type (serial picture-sentence presentation)
                                              800                                                                     for which Underwood et al. (2004) had failed to find a
            Mean Total Inspection Time (ms)
                                                                                                                      match/mismatch effect in their gaze measures. Reading
                                              600
                                                                                                                      times on the verb were highest when the scene did not
                                                                                                                      explicitly depict the thematic relations described in the
                                              400
                                                                                                                      sentence (mis2), lowest when the role relations depicted
                                              200
                                                                                                                      in the scene matched those described in the sentence
                                                                                                                      (match), and intermediate, when scene relations were op-
                                              0                                                                       posite to the thematic relations expressed in the sentence
                                                                                                                      (mis1). The picture-sentence match/mismatch effects
                                                     SVOmatch   SVOmis1   SVOmis2   OVSmatch   OVSmis1   OVSmis2
                                                                                                                      that we observed on the very constituent (the verb) at
                                                                                                                      which the mismatch occurred clearly support the CCM
                                                                                                                      prediction that picture-sentence integration is a process
                                                                                                                      of serially comparing sentence constituents with mental
Figure 4: Mean total reading time for the adverb region                                                               representations of the corresponding picture parts.
                                                                                                                         Crucially, while fine-grained, constituent-based read-
                                                                                                                      ings times in our experiment did reveal a significant effect
                                                                                                                      of match/mismatch, we - just as Underwood et al. (2004)
                                              1200
                                                                                                                      - failed to find an effect of match/mismatch in total sen-
                                                                                                                      tence inspection times. Our findings therefore suggest
                                              1000                                                                    that the lack of a mismatch as reported by Underwood
            Mean Total Inspection Time (ms)
                                              800
                                                                                                                      et al. (2004) derives from their reliance on total sentence
                                                                                                                      times rather than on finer, constituent-based analyses of
                                              600                                                                     the sentence.
                                              400
                                                                                                                      Perspectives on Scene-Sentence Integration
                                              200
                                                                                                                      Findings from the present study lend strong support
                                              0                                                                       to the CCM, while also providing an explanation for
                                                                                                                      why Underwood et al. failed to find confirming sup-
                                                     SVOmatch   SVOmis1   SVOmis2   OVSmatch   OVSmis1   OVSmis2
                                                                                                                      port. The CCM specifies two key steps in the integra-
                                                                                                                      tion process. After the acquisition of mental representa-
                                                                                                                      tions from scene and sentence, the second step concerns
                                                                                                                      the constituent-based comparison and integration of the
 Figure 5: Mean total reading time for the NP2 region                                                                 acquired representations. Indeed, it was precisely the
                                                                                                                      constituent-by-constituents analyses of total gaze dura-
  The main effect of congruence (match, mis1, mis2)                                                                   tions which revealed the mismatch effect, total sentence
that appeared in total reading times on the verb region,                                                              times, as reported by Underwood et al., did not.
was also apparent in the first fixation measure on the                                                                   The success of the CCM in explaining our findings de-
adverb region, F 1(2, 34) = 4.99, p < 0.02, F 2(2, 34) =                                                              spite the lack of an explicit verification task in our study
6.64, p < 0.01 (F s < 1 for congruence effects on the                                                                 leads us to speculate about the applicability of CCM as
other analysis regions for first fixation).                                                                           a more general model of scene-sentence comprehension.
                                                                                                                   1170

   Closer consideration of the CCM suggests that the              V. Haarslev (Eds.), Theory and application of dia-
mechanisms which it proposes have been heavily shaped             grams: First international conference, Diagrams 2000
by the nature of the verification paradigm within which           (pp.149-161). Berlin: Springer-Verlag.
the model has been developed. Crucially, we suggest            Goolkasian, P. (1996). Picture-word differences in a sen-
that full scene-sentence comprehension goes beyond the            tence verification task. Memory & Cognition, 24, 584-
two CCM steps, and crucially relies upon interactive and          594.
interpretative processes. This is particularly evidenced
by the studies on spoken comprehension of Tanenhaus            Gough, P. B. (1965). Grammatical transformations and
et al. (1995), Sedivy et al. (1999), and Knoeferle et al.         speed of understanding. Journal of Verbal Learning
(2005). These studies reveal the dynamic influence of             and Verbal Behavior, 5, 107-111.
the scene on utterance comprehension processes which           Just, M. A., & Carpenter, P. A. (1971). Comprehen-
manifests itself by anticipatory eye-movements to likely          sion of negation with qualification. Journal of Verbal
scene elements, while or even before they are mentioned.          Learning and Verbal Behavior, 10, 244-253.
   An account of scene-sentence integration which is com-
                                                               Knoeferle (2004). The role of visual scenes in spoken
patible with the CCM, but which furthermore includes
                                                                  comprehension. Unpublished doctoral dissertation,
such interpretative processing has been proposed by               Saarland University.
Knoeferle & Crocker (2004) based on findings from utter-
ance comprehension during scene inspection. Their “Co-         Knoeferle & Crocker (2004). The coordinated processing
ordinated Interplay Account” describes scene-sentence             of scene and utterance: evidence from eye-tracking in
interaction as highly incremental, with a close time-lock         depicted events. In: Proceedings of the International
between scene and sentence processing. Importantly, it            Conference on Cognitive Science, (pp. 217-222), Alla-
further predicts that once the utterance has identified           habad, India.
relevant scene information (e.g., a depicted action), that     Knoeferle, Matthew Crocker, Martin Pickering, &
scene information can then in turn influence comprehen-           Christoph Scheepers (2005). The influence of the im-
sion and interpretation processes - a process which is            mediate visual context on incremental thematic role-
missing from the Constituent Comparison Model. This               assignment: evidence from eye-movements in depicted
last step, from a simple incremental integration of sen-          events. Cognition, 95, 95-127.
tence and scene towards an interactive and interpretative
account of these comprehension processes is an impor-           Rayner, K. (1998). Eye movements in reading and infor-
                                                                  mation processing: 20 years of research. Psychological
tant first step towards a complete and general model of
                                                                  Bulletin, 124, 372-422.
comprehension in visual environments.
                                                                Rayner, K., Rotello, C. M., Stewart, A. J., Keir, J., &
                 Acknowledgements                                 Duffy, S. A. (2001). Integrating text and pictorial
We thank Martin J. Pickering for his helpful comments,            information: Eye movements when looking at print
and Nicole Kühn for her assistance in running and                advertisements. Journal of Experimental Psychology:
analysing the experiment. This research was funded by             Applied, 7, 219-226.
a PhD scholarship to the first, and by SFB 378 project          Richardson, D. C., & Spivey, M. J. (2000). Representa-
ALPHA to the second author, both awarded by the Ger-              tion, space and Hollywood squares: Looking at things
man Research Foundation (DFG).                                    that arent there anymore. Cognition, 76, 269-295.
                                                                Sedivy, J. C., Tanenhaus, M. K., Chambers, C. G., &
                      References                                  Carlson, G. N. (1999). Achieving incremental seman-
Altmann, G. T. M. (2004). Language-mediated eye                   tic interpretation through contextual representation.
   movements in the absence of a visual world: the blank          Cognition, 71, 109-148.
   screen paradigm. Cognition, 93, B79-B87.
                                                                Spivey, M. J., Richardson, D. C., & Fitneva, S. A.
Altmann, G. T. M. & Kamide, Y. (1999). Incremen-                  (2004). Thinking outside the brain: Spatial indices to
   tal interpretation at verbs: Restricting the domain of         visual and linguistic information. In J. M. Henderson,
   subsequent reference. Cognition, 73, 247-264.                  & F. Ferreira (Eds.), The interface of language, vision,
Carpenter, P. A. & Just, M. A. (1975). Sentence compre-           and action: Eye movements and the visual world. New
   hension: a psycholinguistic processing model of verifi-        York: Psychology Press.
   cation. Psychological Review, 82, 45-73.                     Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard,
Carrol, P. J., Young, J. R., & Guertin, M.S. (1992). Vi-          K. M., & Sedivy, J. C. (1995). Integration of visual
   sual analysis of cartoons: A view from the far side.           and linguistic information in spoken language compre-
   In K. Rayner (Ed.), Eye movements and visual cog-              hension. Science, 268, 1632-1634.
   nition: Scene perception and reading (pp. 444-461).          Underwood, G., Jebbett, L., & Roberts, K. (2004). In-
   NewYork: Springer-Verlag.                                      specting pictures for information to verify a sentence:
Feeney, A., Hola, A. K. W., Liversedge, S. P., Findlay, J.        eye movements in general encoding and in focused
   M., & Metcalf, R. (2000). How people extract infor-            search. The Quarterly Journal of Experimental Psy-
   mation from graphs: Evidence from a sentence-graph             chology, 56, 165-182.
   verification paradigm. In M. Anderson, P. Cheng, &
                                                           1171

