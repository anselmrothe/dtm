UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Similarity Between Semantic Spaces
Permalink
https://escholarship.org/uc/item/8mx2s8qb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Cai, Zhiqiang
Graesser, Arthur C.
Hu, Xiangen
et al.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Similarity Between Semantic Spaces
                                         Xiangen Hu (xhu@memphis.edu)
                                  Department of Psychology; The University of Memphis
                                                     Memphis, TN 38152
                                        Zhiqiang Cai (zcai@memphis.edu)
                                  Department of Psychology; The University of Memphis
                                                     Memphis, TN 38152
                                 Arthur C. Graesser (agraesser@memphis.edu)
                                  Department of Psychology; The University of Memphis
                                                     Memphis, TN 38152
                                 Matthew Ventura (mventura@memphis.edu)
                                  Department of Psychology; The University of Memphis
                                                     Memphis, TN 38152
                          Abstract                               to compute similarities between documents (Hu et al.,
                                                                 2003, 2003; Hu, Cai, Wiemer-Hasting, Graesser, & Mc-
   One of the challenges in Latent Semantic Analysis (LSA)       Namara, 2005). The issue of evaluating the quality of
   is deciding which corpus is best for a speci¯c applica-
   tion.Imp ortant factors of LSA in°uence the generation        semantic space is very important, not only at the level
   of high quality LSA space including the size of the cor-      of theoretical importance, but also at the level of speci¯c
   pus, the weight (local or global) functions, number of        applications.
   dimensions to keep, etc. These factors are often di±cult         Curren tly, the quality of semantic spaces is usually
   to determine and as a result hard to control for. In this
   paper, we provide a general method to measure simi-           evaluated by human experts. This is done by compar-
   larity between semantic spaces. Using this method, one        ing performances between applications that use a spe-
   can evaluate semantic spaces (such as LSA spaces) that        ci¯c semantic space and have human experts perform
   are generated from di®erent sets of parameters or di®er-      some benchmark tests. For example, to evaluate an
   ent corpora. The method we have develop ed is generic         LSA space with a given set of parameters (e.g., num-
   enough to evaluate di®ering types of semantic spaces.
                                                                 ber of dimensions), an LSA similarity measure between
                                                                 texts is compared with experts' judgement of the sim-
   Keywords: Semantic Space, Latent Semantic Analy-
                                                                 ilarity of those texts (Olde, Graesser, & Tutoring Re-
sis, Similarity Measures Between Texts
                                                                 search Group, 2002). There are many possible variables
                                                                 that are involved in creating semantic spaces, which
                      Introduction                               makes it impractical for human experts to evaluate all
The use of higher dimensional semantic spaces, such as           semantic spaces.
Latent Semantic Analysis (LSA) (Landauer & Littman,                 In this paper, we present a systematic method to au-
1990; Dumais, 1990; Laham, 1997; Landauer, Laham,                tomatically evaluate semantic spaces. This method al-
Rehder, & Schreiner, 1997; Landauer, Foltz, & La-                lows us to measure similarity between semantic spaces
ham, 1998), Hyperspace Analogue to Language (HAL)                that are created from di®erent sets of parameters (do-
(Burgess, Livesay, & Lund, 1996; Burgess & Lund, 1997;           main, corpus size, dimensions, etc.). Furthermore, this
Burgess, 1998), Non-Latent Similarity (NLS) algorithm            metho d can even be used to ¯nd di®erences between se-
(Cai et al., 2004), is very common in computational lin-         mantic spaces that are created using entirely di®erent
guistics. The semantic spaces have been used in applica-         metho ds, such as LSA, HAL, and NLS.
tions that involve information retrieval(Dumais, 1990),
essay grading, and text comparison (Foltz, Laham, &                                  Observations
Landauer, 1999). The scope and depth of the appli-               We ¯rst provide a mathematical model for Semantic
cations are so diverse that di®erent semantic spaces are         Space. This model is simply an abstraction of some
needed for di®erent purposes (Franceschetti et al., 2001).       commonly used semantic spaces such as LSA , HAL,
Furthermore, the process of generating semantic spaces           and NLS. Based on this model, we provide a measure of
is very complicated (Deerwester et al., 1990). Some of           similarity between semantic spaces. At the end of the
them involve careful selection of corpora (Franceschetti         paper, we outline pro cedures of how to use the similar-
et al., 2001). From all the previous studies and applica-        ity measures to evaluate semantic spaces. The metho d
tions of semantic spaces such as LSA, HAL, and NLS,              presented in this paper is based on the following obser-
we observed that there are several key parameters (such          vations from semantic spaces, such as LSA , HAL, and
as dimensions, domain, corpus size, etc.) that need to           NLS:
be set before generating an appropriate space for an ap-
plication. There are di®erent metho ds that can be used         1. Semantics is a property that applies to ¯ve di®er-
                                                             995

    ent levels of language entities: word, phrase, sentence,     ² Si (x1 ; x2 ) < 1 if x 1 and x2 have only ¯nite non-zero
    paragraph, and document. In any given language,                 elements,
    ² the smallest semantic units are words. For example,        ² Si (x1 ; x2 ) > 0 if x1 = x2 6= 0;
       "this", "is", "a", "big", "table".
                                                                 ² Si (x1 ; x2 ) = 0 if x1 = 0 or x2 = 0:
    ² a phrase is an ordered array of words. For example,
       "big table".                                                 The similarity measure si (x 1 ; x2 ) between x 1 ; x2 2 Xi
    ² a sentence is an ordered array of words and phrases.       is de¯ned in as
       For example, "This is a big table."
                                                                                        Si (E i (x1 ) ; Ei (x 2 ))
    ² a paragraph is an ordered array of sentences. For                   p                            p                             ;
       example, "This is a big table. It was broken."                       Si (E i (x1 ) ; Ei (x 1 )) S i (E i (x 2 ) ; E i (x 2 ))
    ² a document is an ordered array of paragraphs.              where E i (x 1 ) and E i (x2 ) are not zero vectors.
                                                                    For maps from lower level representations to higher
2. Semantics of any level of the language entities can be
                                                                 level representations follow the following constraints:
    represented numerically or algebraically.
                                                                 ² if x = (y 1 ; :::; yk ) 2 Xi; y1 ; :::; y k 2 Xi¡ 1 : for some
    ² Semantics and the numerical or algebraic represen-
                                                                    k > 0, then
       tation are synonymous.
                                                                               E i (x) = Hi (E i¡ 1 (y 1 ) ; :::; E i¡ 1 (y k )) ;
3. Semantics of di®erent levels of the language entities
    may be represented di®erently, but                                                                             k
                                                                    where Hi is a function Hi : [R1 ] ¡! R1 ;
    ² Semantics of a higher level language entity is com-        ² For x 1 = (y11 ; :::y1u ) 2 Xi and x 2 = (y21 ; :::y2v ) 2 Xi;
       puted as a function of semantics of its lower level
                                                                    where y 11 2 Xi¡ 1 ; Si (E i (x1 ) ; Ei (x 2 )) is in the form
       language entities.                                                                                                              u
                                                                    of Eqn. (1), where Ui is a function Ui : [R1 ] £
    ² Semantic relations between any two entities at the                1 v
                                                                    [R ] ¡! R; for some u, v > 0:
       same level can be numerically measured as a func-
       tion of the semantics of the entities.                                      Si (E i (x1 ) ; Ei (x 2 )) = Ui (U; V)               (1)
4. The meaning of any word is represented by its (numer-            where U = (Ei¡ 1 (y11 ) ; E i¡ 1 (y 12 ) ; :::; E i¡ 1 (y 1k 1 ))
    ically measurable) relations with other words in the            and V = (E i¡ 1 (y 21 ) ; E i¡ 1 (y22 ) ; :::; Ei¡ 1 (y2k2 )) and
    same semantic space. We call such a relation induced            x 1 ; x2 2 Xi; y11 ; :::;y 1k 1 ; y21 ; :::; y 2k 2 2 Xi¡ 1
    semantic structur e of the word in the given semantic
    space.                                                          De¯nition 1 is similar with the four components model
                                                                 of Lowe (2001). The di®erence that arises between Lowe
                                                                 and out de¯nition is that this de¯nition considers not
                         De¯nitions                              only the word level, but also all other levels with assumed
 To formalize the above assumptions and the concept of           mapping from lower layers to higher layers. To under-
 semantic structure, we have a formal de¯nition of vector-       stand the above de¯nition, consider the ¯ve language
 based semantic spaces.                                          entities, namely, word, phrase, sentence, paragraph, and
 De¯nition 1 A vector-based semantic space contains              document. Each corresponds to a di®erent layer. X1 is
 ¯ve components:                                                 a set of phrases, X2 is a set of sentences, etc. For every
                                                                 element, there is a vector representation in R1 . In Def-
1. A set of words X0 = fx 1 ; x2 ; :::; x N g ;                  inition 1, we do not specify a limited dimensionality for
                                                                 the vector representation. Instead we assume there is an
2. A hierarchy of layers, X1 ; :::; XM ; where an element        in¯nite dimensional vector with only a ¯nite number of
    in the set Xi is a ¯nite ordered array of elements in        non-zero entries. To understand 5 and 4 of De¯nition 1,
    Xi¡ 1 (i = 1; :::; M );                                      one can take LSA as a simple example, where the seman-
                                                                 tic vector of a sentence is simply a vector summation of
3. Vector representation for elements in each of the lay-        the vectors of the words in the sentence. Furthermore,
    ers.                                                         the similarity between two words (or two sentences) is a
4. Measure of similarity between elements within each of         function of the two word (sentence) vectors. 5 of De¯ni-
    the layers.                                                  tion 1 emphasizes the relations between di®erent layers.
                                                                 The similar relations can be seen from LSA, where the
5. Maps from lower level representations to higher level         computation of similarity between documents is a func-
    representations                                              tion of the vectors of the words.
                                                                    For the purpose of this paper, we next generalize the
    For vector representations of elements in each layer, i.     idea of "near neighbor" of LSA in the new framework of
 e., 8x 2 Xi; there exists a vector E i (x) 2 R1 with only       semantic space. From this concept, we further introduce
 ¯nite non-zero entries; i = 1; :::; M ;                         the idea of induced semantic structure. These two con-
    For measure of similarity for each layer: Assume S i :       cepts will serve as the foundation for the remainder of
 R1 £ R1 ¡! R; i = 1; :::; M: such that                          the paper.
                                                             996

De¯nition 2 Given a semantic space with layers                                semantic structures in each of the semantic spaces. De-
X0 ; :::; XM ; 8x          2 Xi ; the neighbor of x is                        note them as Sx1 and Sx2 : Assume N 1 and N 2 are the
f (y; s i (x; y))j y 2 Xi g :                                                 number of words in the two semantic spaces ( de¯ned in
   The neighbor of any element in any of the layer Xi is                      1 of De¯nition 1), respectively, where T · min (N1 ; N 2 ) :
simply a partial ordered set. We call such an ordered set                     Furthermore, assume 1 Sx;T        1
                                                                                                                      and S2x;T are the top T near-
induced semantic structur e.                                                  est neighbor of word x: The combinatorial similarity for
                                                                              word x between the two semantic spaces is de¯ned as
De¯nition 3 Given a semantic space with layers                                                                  °                   °
X0 ; :::; XM ; 8x 2 Xi ; the induced semantic structur e                                                        °S 1 \ S 2 °
                                                                                                        T            x;T       x;T
Sx;i ½ Xi £ Xi is a partial order de¯ned in Eqn. (2).                                                Cx = °                         °                    (3)
                                                                                                                ° 1            2 °
½             ¯                                                         ¾                                       °Sx;T [ Sx;T        °
              ¯                (x1 ; x 2 2 Xi ) and
  (x 1 ; x2 ) ¯
              ¯ (si (E i (x) ; E i (x1 )) ¸ S i (E i (x) ; E i (x2 )))    :
                                                                              where kXk is the number of items in set X: Given
                                                                      (2)     T · min (N 1 ; N2 ) ; with such a de¯nition of seman-
                                                                              tic similarity for any word x. One can obtain similar-
                          Assumptions                                         ity for any collection©of words,     ¯          W ½ X0 \ ª       Y 0 ; as sta-
With the above de¯nitions, we have the following as-                          tistical properties of CxT ¯x 2 W ½ X0 \ Y 0 : For sim-
sumptions. These assumptions serve as the theoretical                         plicity,
                                                                                 © we    ¯ only consider mean      ª        and standard derivation
foundation for our similarity measure of vector based se-                     of C xT ¯x 2 W ½ X0 \ Y 0 ; although we may consider
mantic spaces.                                                                other characteristics. Furthermore, we have the similar-
                                                                              ity de¯ned as a function of the value T : In fact
Assumption 1 The meaning of a word is embedded in                                © T¯                                                                ª
its relations with other words.                                                      C x ¯x 2 W ½ X0 \ Y 0 ; 1 · T · min (N 1 ; N2 )                     (4)
   As an illustrative example (see Fig 1), the word "life"
                                                                              contains all information between the two semantic spaces
has di®erent near neighbors for di®erent LSA spaces.                          at the "combinatorial sense". Statistical properties of
Assumption 2 If a given word is shared in di®erent                            (4) can be used to measure the Combinatorial Similarity
semantic spaces, the relation between the semantics of                        between two spaces. For example, if W is a collection
the word in di®erent semantic spaces is a function of the                     of physics glossory terms, then statistical properties of
corresponding induced semantic structur es.                                   (4); namely, mean and standard deviation, would be a
   In Assumption 2, we consider only the algebraic (or-                       measure of semantic similarity of these terms between
dering) nature (as in Eqn. (2) ) of the near neighbors.                       the two spaces.
Assumption 3 The relations between any two semantic                           Permutational Similarity
spaces are a function of the relations of the semantic                        Permutational similarity is de¯ned in the same way as
structur es of all the shared words                                           Combinatorial Similarity , except the comparison of the
   Assumption 3 extends Assumption 2 from the level of                        top T nearest neighbors of x in the two semantic spaces
the word to the entire semantic space.                                        is not only combinatorial,       n but      alsoopermutational. Con-
                                                                                      ¡ 1             ¢             0        0         ©              ª
                                                                              sider Sx;T \ S x;T = x1 ; :::; x ¿ = x"1 ; :::; x "¿ and
                                                                                                  2
                    Similarity Measures                                                                                                       ³ 0        0
                                                                                                                                                           ´
With the above assumptions, we are able to measure                            the orders of the nearest neighbors for x: x 1 ; :::; x¿
                                                                                     ¡           ¢
similarity between semantic spaces at three di®erent lev-                     and x"1 ; :::; x "¿ for the two semantic spaces, resp ectively.
els: Combinatorial Similarity, Permutational Similarity ,                     d is a function that measures the permutational distance
and Quantitative Similarity. From 5 of De¯nition 1, we                        between two orders. It is assumed that
see that all layers Xi; :::; XM of a semantic space actu-
ally depend on X0 and the mappings from lower layers                                           d ((x1 ; :::; x ¿ ) ; (x1 ; :::; x ¿ )) = 0;
to higher layers. This makes it easier to intro duce the
general measure of semantic similarity between semantic                       and
spaces.. In this paper, we only consider similarity mea-                        ³³ 0         0
                                                                                                ´ ¡               ¢´         ³³ 0           0
                                                                                                                                              ´ ³ 0          0
                                                                                                                                                               ´´
sures that are derived at the layer of the basic items,                       d x 1 ; :::; x ¿ ; x"1 ; :::; x "¿ · d x 1 ; :::; x¿ ; x¿ ; :::; x 1 :
namely, X0 :
                                                                              We de¯ne the quantity
Combinatorial Similarity                                                                   0         ³³ 0               ´ ¡               ¢´1
                                                                                                                      0
Based on Assumption 1, the meaning of a word is deter-                                             d x 1 ; :::; x¿ ; x "1 ; :::; x"¿
mined by its relations with all other words in a semantic                         PxT = @ 1 ¡ ¡¡ 0                    0
                                                                                                                        ¢¡0             0
                                                                                                                                          ¢¢ A C xT      (5)
                                                                                                    d x 1 ; :::; x¿ ; x ¿ ; :::;x 1
space. Using Assumption 2, we ¯rst have the Combinato-
rial Similarity at the level of individual word. Applying                         1
Assumption 3, we will have the Combinatorial Similarity                             In some cases, there is no unique T top neighbours, be-
                                                                              cause the induced semantic structure is only a partial order.
at the level of semantic spaces.                                              We only consider the simplest case here. We will not consider
   Assume X0 and Y 0 are layers in two semantic spaces.                       the cases where no unique top T nearest neighbours in this
For any given item x 2 X0 \ Y 0, there are two induced                        paper.
                                                                          997

 as permutational similarity of x in two semantic spaces                        Assume that we have two LSA spaces 2 L1 =
 for a given T . Similarly, permutational similarity can be                 (X0 ; X2 ; X3 ; X4 ; X5 ) and L2 = (Y 0 ; Y 2 ; Y 3 ; Y4 ; Y 5 ) ; and
 de¯ned at the level of semantic spaces for any given set                   a common set of words W = fx 1 ; x 2 ; :::; xN g = X0 \ Y0 .
 of words, W ½ X0 \ Y0 ;                                                    Two matrices can be obtained by considering near neigh-
     © T¯                                                      ª            bors (De¯nition 2) for all words in W : S1 = (s1ij )
       P x ¯x 2 W ½ X0 \ Y 0 ; 1 · T · min (N 1 ; N2 )              (6)     and S2 = (s2ij ) ; where s kij = cosk (x i; xj ) is the co-
                                                                            sine match between xi and x j within the LSA space k;
 contains similarity between the two semantic spaces at
 the permutational level. Consequently, statistical prop-                   i; j = 1; :::; N ; k = 1; 2: Notice that such two matrices
                                                                            contain all necessary information needed for all three dif-
 erties of (6), such as mean and standard deviation can
                                                                            ferent levels of similarities.
 be used for such purposes.
                                                                                For the purp ose of illustration, and due to space lim-
 Quantitative Similarity                                                    itation of the paper, we compute C xT ; P xT ; and QTx for
 Combinatorial Similarity and Permutational similarity                      the word "life".
 are based on algebraic properties of the induced se-                           Table 1 lists near neighbors for several LSA spaces.
 mantic structure as a partial order. Quantitative Sim-                     We computed C xT , P xT ; and QTx (as de¯ned in Eqn. (3),
 ilarity is based on quantitative property of the near-                     (5) and (7)) with the value T = 50 (see Tables 2, 3,
 est neighbor (De¯nition 2). For any x 2 X0 \ Y0 ;                          and 4); We observed that the meaning of "life" is most
 and T · min (N1 ; N 2©¡        ) ; there is ¢a¯ simple quanti-             similar between 6th grade and 9th grade and between 9th
                                                                      ª     grade and 12th grade in the Touchstone Applied Science
 tative relation between           y; s10 (x; y) ¯y 2 Sx;T 1
                                                              \ S2x;T
       ©¡ 2            ¢¯                     ª                             Associates (TASA) corpus.
 and       y; s0 (x; y) ¯y 2 Sx;T1         2
                                      \ Sx;T     . For example, one             In this example, we have ¯ve di®erent spaces gener-
 could use Pearson correlation r between the two set of                     ated from TASA corpus. The same method can be used
 quantities. In the same way,                                               to ¯nd semantic similarities for a group of words be-
              0                                            1                tween di®erent corpora. When using a group of words,
                          P 1              2
                              s   (x; y)  s  (x;  y)                        instead of using quantities CxT , P xT ; and Q Tx as similarity
     QTx = @ 1 ¡ qP             0          0               A CxT    (7)     measures, one needs to use statistical properties of core-
                             1         2P 2              2
                          [s 0 (x; y)]       [s 0 (x; y)]                   sponding sets of quantities (de¯ned in (4),(6),(8)) such
                                                                            as mean and standard deviation.
                                                           1       2
 where the sum is obtained for all y 2 Sx;T                    \ Sx;T :
 Furthermore, a set of quantities can be obtained for
 W ½ X0 \ Y 0;                                                              Table 1: Top 20 nearest neighbours of "life" for dif-
    © T¯                                                      ª             ferent LSA spaces (only showing 6th{12th, due to the
      Q x ¯x 2 W ½ X0 \ Y 0 ; 1 · T · min (N 1 ; N2 ) : (8)
                                                                            space limitation of the paper). The rank is taking from
 Quantitative similarity is de¯ned as a set of statistical                  http://lsa.colorado.com.
 properties of (8). As usual, mean and standard deviation                                 6th                9th                12th
 can be used for such purp oses.                                                           life              life                 life
                                                                                    reincarnation     contemplated              death
                         An Example                                                   premiums         reincarnation          lifetime
                                                                                     policyholder            sai               hamlin
 In this section, we apply the de¯nitions, assumptions,
 and similarity measures to LSA spaces. The following is                               premium              pipal               pipal
 true for LSA spaces:                                                                      sai            nirvana             nirvana
                                                                                         cycles           lifetime         zarathustra
1. LSA contains a set of words.                                                        holdover            death          ahuramazda
2. At the layer corresponding to phrases, sentences, and                             condemning          hinduism            ahriman
    do cuments, LSA does not consider ordering of lower                                chekhov            afterlife      policyholder
    layer items. One may view the "bag of words" as a                                   captial          excerpted          romantics
    equivalent class of the ordered arrays containing the                                pipal            ribman             essayists
    same set of items.                                                                  nirvana           rea±rm                  sai
                                                                                      hinduism           militarily     beaumarchais
3. The representation of LSA is a ¯nite dimensional vec-
    tor. It can be viewed as in¯nite vector with ¯nite                                    span            kindless               1658
    non-zero entries.                                                                    priori         condemning         pseudonym
                                                                                       maturity          premiums            poquelin
4. The similarity measure S i (x1 ; x2 ) is simply the dot-                            immoral           premium              ribman
    pro duct of the vectors                                                             humane          policyholder         kindless
5. The maps between lower layers to higher layers is a                           2
                                                                                   In applications of LSA, only X0 and very small portion
    pool of the items from the lower layers. The vector                     of other layers are meaningful. Due to the simple algorithm
    representation of higher layer items is simply a vector                 of combining word vectors to sentence or document vectors,
    summation of the vectors from the lower layers.                         items in other layers can be computed very easily.
                                                                        998

                                                                ² Type of corpus : LSA has been used in di®erent do-
                                                                   mains. For example, LSA has been used in tutoring
                                                                   systems that teach computer literacy and qualitative
Table 2: Combinatorial similarity of the word "life" be-           physics(Graesser et al., 2002). Usually, di®erent cor-
tween several LSA spaces.                                          pora are used for di®erent target application domains.
             3rd       6th      9th      12th     College          There are also general purpose corpora, such as the
  3ed        1                                                     TASA corpus. One question is how di®erent the LSA
  6th        0:0526 1                                              spaces are that are created from these di®erent cor-
                                                                   pora. The similarity measure o®ered here can be used
  9th        0:0204 0:2987 1
                                                                   to evaluate the LSA spaces, such as LSA spaces gen-
  12th       0:0000 0:0989 0:2500 1                                erated from physics text, computer literacy texts, and
  College 0:0000 0:0989 0:1111 0:2048 1                            TASA.
                                                                                        Summary
Table 3: Permutational Similarity of    the word "life" be-     In this paper, we provide a general approach to mea-
tween several LSA spaces.                                       sure similarity between semantic spaces. We ¯rst o®er
             3rd       6th      9th      12th     College       some observations of commonly used semantic spaces.
                                                                From these observations, we intro duce a set of general
  3ed        1
                                                                assumptions. Finally we have a mathematical model
  6th        0:0491 1
                                                                of semantic spaces. Based on this model, we are able
  9th        0:0136 0:2078 1                                    to derive three quantitative measures between semantic
  12th       0:0000 0:0478 0:2130        1                      spaces. Finally, we have used the similarity measures to
  College 0:0000 0:0462 0:0626           0:1450   1             examine semantic similarity of a single word ('life") in
                                                                several LSA spaces.
                                                                   The space limitation of this paper does not permit us
                                                                to o®er more, elaborated applications of these similar-
Table 4: Quantitative Similarity of the word "life" be-         ity measures. However, we have o®ered several possible
tween several LSA spaces.                                       applications of the metho d. We argue that the metho d
             3rd       6th      9th      12th     College       introduced in this paper will help researchers to select
  3ed        1                                                  parameters in the pro cess of creating semantic spaces.
  6th        0:0524 1
  9th        0:0202 0:2975 1                                                      Acknowledgments
  12th       0:0000 0:0979 0:2495 1                             The Institute for Intelligent Systems (IIS) is an interdis-
  College 0:0000 0:0975 0:1101 0:2043 1                         ciplinary research group comprised of approximately 100
                                                                researchers from psychology, computer science, physics,
                                                                engineering, linguistics, education and other disciplines
Possible applicationsof the method                              (visit http://www.iismemphis.org). This research, con-
                                                                ducted by the authors and the IIS, was supp orted by
Although we only have shown a simple application of the         the National Science Foundation (REC 0106965) and
semantic similarity measures, namely, only at the level         the DoD Multidisciplinary University Research Initiative
of single word ("life "), we argue that the method can be       (MURI) administered by ONR under grant N00014-00-
easily applied to evaluate similarities between semantic        1-0600. Any opinions, ¯ndings and conclusions or rec-
spaces. For example, several parameters need to be set          ommendations expressed in this material are those of
for any give LSA space. The method introduced in this           the authors and do not necessarily re°ect the views of
paper can be used to measure di®erences due to di®erent         ONR or NSF. We would like to thank Tom Landauer
values of those parameters.                                     and Walter Kintsch by supplying a number of corpora
                                                                used in this study.
² Size of Documents : LSA created a word-do cument
   matrix as the original matrix for later SVD. It is not
   very clear what the ideal document size is. Using the
                                                                                       References
   similarity measure we have here, we can systemati-           Burgess, C. (1998). From simple associations to the
   cally vary the document size and measure the similar-           building blocks of language mo deling meaning in mem-
   ity among LSA spaces with di®erent document sizes.              ory with the hal model. Behavior Research Methods,
                                                                   Instruments, and Computers , 30, 188-198.
² Selection of number of dimensions to keep : After SVD,
                                                                Burgess, C., Livesay, K., & Lund, K. (1996). Mo del-
   only dimensions corresponding to the largest singular
                                                                   ing parsing constraints in high-dimensional semantic
   values are kept. The number of dimensions kept is only
   about 1~3% of the total number of dimensions. One               space: On the use of proper names. In Proceedings of
   question is to address the robustness of the selection          the cognitive science society. Hillsdale, N.J.: Erlbaum.
   of the dimensions. Using the similarity measure, we
   can compare LSA spaces with di®erent dimensions.
                                                            999

Burgess, C., & Lund, K. (1997). Modeling parsing                 Cognitive Science Society (p. 979). Mawhwah NJ: Erl-
  constraints with high-dimensional context space. Lan-          baum.
  guage and Cognitive Processes , 12, 177-210.                 Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
Cai, Z., McNamara, D. S., Louwerse, M., Hu, X., Rowe,            Introduction to latent semantic analysis. Discourse
  M., & Graesser, A. C. (2004). Nls: Non-latent similar-         Processes , 259-284.
  ity algorithm. In K. Forbus, D. Gentner, & T. Regier         Landauer, T. K., Laham, D., Rehder, B., & Schreiner,
  (Eds.), Proceedings of the 26th annual meeting of the          M. E. (1997). How well can passage meaning be de-
  cognitive science society (p. 180-185). Mahwah, NJ:            rived without using word order? a comparison of la-
  Erlbaum.                                                       tent semantic analysis and humans. In M. G. Shafto
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer,          and P. Langley (Eds.), Proceedings of the 19th annual
  K., T., & Harshman, R. (1990). Indexing by latent              meeting of the Cognitive Science Society, 412-417.
  semantic analysis. Journal of the American Society           Landauer, T. K., & Littman, M. L. (1990). Fully auto-
  For Information, 141, 391-407.                                 matic cross-language document retrieval using latent
Dumais, S. (1990). Enhancing performance in latent               semantic indexing. In Proceedings of the 6th Annual
  semantic indexing (lsi) retrieval (TM-ARH-017527               Conferenceof the Centre for the New Oxford English
  Technical Report). Bellcore.                                   Dictionaryand Text Research, 31-38.
Foltz, P. W., Laham, D., & Landauer, T. K. (1999).             Lowe, W. (2001). Towards a theory of semantic space. In
  Automated essay scoring: Applications to educational           J. D. Mo ore & K. Stenning (Eds.), Proceedings of the
  technology. In proceedings of EdMedia '99.                     twenty-thir d annual conference of the cognitive science
                                                                 society (pp. 576{581).Mahwah NJ: Lawrence Erlbaum
Franceschetti, D., Karnavat, A., Marineau, J., McCal-
                                                                 Associates.
  lie, G. L., Olde, B. A., Terry, B. L., Graesser, A., &
                                                               Olde, B. A., Graesser, A. C., & Tutoring Research Group
  C. (2001). Development of physics text corpora for
                                                                 the. (2002). Latent semantic analysis: What is it and
  latent semantic analysis. In J. Mo ore & K. Stenning
                                                                 how can it improve and assess student learning? Paper
  (Eds.), Proceedings of the 23rd annual conference of
                                                                 presented at the North East Regional Conference on
  the cognitive science society (p. 297-300). Mahwah,
                                                                 Excellence in Learning and Teaching, Oswego, NY.
  NJ: Erlbaum.
Graesser, A. C., Hu, X., Olde, B. A., Ventura, M., Ol-
  ney, A., Louwerse, M., Franceschetti, D. R., & Person,
  N. (2002). Implementing latent semantic analysis in
  learning environments with conversational agents and
  tutorial dialog. In W. G. Gray & C. D. Schunn (Eds.),
  Proceedings of the 24th annual meeting of the cognitive
  science society (p. 37). Mahwah, NJ: Erlbaum.
Hu, X., Cai, Z., Franceschetti, D., Penumatsa, P.,
  Graesser, A., Louwerse, M., McNamara, D., & TRG.
  (2003). Lsa: The ¯rst dimension and dimensional
  weighting. In R. Alterman & D. Hirsh (Eds.), Pro-
  ceedings of the 25rd annual conference of the cognitive
  science society (p. 1-6). Boston, MA: Cognitive Sci-
  ence Society.
Hu, X., Cai, Z., Graesser, A. C., Louwerse, M. M., Penu-
  matsa, P., Olney, A., & TRG. (2003). An improved
  lsa algorithm to evaluate student contributions in tu-
  toring dialogue. In G. Gottlob & T. Walsh (Eds.),
  Proceedings of the eighteenth international joint con-
  ference on arti¯cial intel ligence (p. 1489-1491). San
  Francisco: Morgan Kaufmann.
Hu, X., Cai, Z., Wiemer-Hasting, P., Graesser, A., & Mc-
  Namara, D. S. (2005). Strengths, limitations, and ex-
  tensions of lsa. In D. S. S. W. Landauer T.;McNamara
  (Ed.), Lsa: A road to meaning. Mahwah, NJ: Erl-
  baum. (in press)
Laham, D. (1997). Latent semantic analysis approaches
  to categorization. In M. G. Shafto and P. Langley
  (Eds.), Proceedings of the 19th annual meeting of the
                                                          1000

