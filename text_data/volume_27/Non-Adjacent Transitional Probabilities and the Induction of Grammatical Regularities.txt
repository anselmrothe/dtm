UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Non-Adjacent Transitional Probabilities and the Induction of Grammatical Regularities
Permalink
https://escholarship.org/uc/item/1wn8n7kn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Author
Garzon, Francisco Calvo
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                       Non-adjacent Transitional Probabilities and the Induction
                                                 of Grammatical Regularities
                                             Francisco Calvo Garzón (fjcalvo@um.es)
                                             Departamento de Filosofía, Campus de Espinardo
                                                           Murcia, 30100 SPAIN
                               Abstract                                 familiarization. The results reported show that subjects
                                                                        favour words over part-words (P < 0.0005; see Peña et al.,
   According to Peña et al. (2002), statistical computations based      2002, for details). This behaviour backs up the hypothesis
   on nonadjacent transitional probabilities of the sort that are       that statistics alone suffice to segment a linguistic stream,
   exploited in speech segmentation cannot be used in order to          since what counts as a word is defined as a function of the
   induce existing grammatical regularities in the speech stream.       higher transitional probabilities between specific non-
   In their view, statistics are insufficient to support the            adjacent items (in this case, between Ai and Ci).
   discovery of the underlying grammatical regularities. In this
   note I argue that a single statistical mechanism can account
   for the data Peña et al. report.                                     Experiment B
                                                                        In experiment B, Peña et al. wanted to know whether
   Keywords: statistical computations; rule-based learning;             subjects are simply segmenting the stream by exploiting
   speech segmentation; neural networks.                                differential transitional probabilities between words and
                                                                        part-words, or whether they are also tuning to some more
                           Introduction                                 abstract underlying grammatical regularity. In order to
Peña et al. (2002) consider whether statistical computations            answer this question, after having been familiarized to the
based on nonadjacent transitional probabilities of the sort             same speech stream of experiment A, subjects were asked to
that are exploited in speech segmentation (Saffran et al.,              choose between a part-word and what Peña et al. dubbed a
1996) can be used in order to induce existing grammatical               rule-word. A rule-word is a sequence of three syllables of
                                                                        the form AiX*Ci, where X* stands for a familiar syllable
regularities in the speech stream. To answer this question,
                                                                        that occurs in familiarization, although never between Ai
they designed an experimental paradigm and performed five
                                                                        and Ci. In this way, although a rule-word represents a
different experiments. The first three are briefly reviewed             novelty, it is congruent with the structure of actual words by
here. Under the light of these experiments Peña et al.                  means of the preservation of a non-adjacent transitional
conclude that statistics are insufficient to support the                probability of 1.0 between Ai and Ci. This time,
discovery of the underlying grammatical regularities, and               interestingly, subjects prefer part-words over rule words.
that their results imply knowledge of rules. Contra Peña et               Under the light of experiments A and B, Peña et al.
al., I shall argue that a single statistical mechanism can              conclude that a “computational mechanism sufficiently
account for the data they report.                                       powerful to support segmentation on the basis of
                                                                        nonadjacent transitional probabilities [experiment A] is
Peña et al.’s experimental paradigm                                     insufficient to support the discovery of the underlying
Peña et al.’s experimental paradigm involves asking adult               grammatical-like regularity embedded in a continuous
subjects to listen to a sequence of trisyllabic artificial words        speech stream [experiment B]” (p. 605).
for 10 minutes. Words have the form AiXCi and are
identified on the basis of their nonadjacent transitional               Experiment C
probabilities, such that the transitional probability between           Peña et al. then introduced a 25-ms subliminal segmentation
any Ai and the following Ci is 1.0; between Ai and the                  gap between words during familiarization. This time
intermediate X, and between X and the final Ci is 0.33; and             subjects manifested preference for rule-words over part-
between Ci and the next word’s first syllable is 0.5. After             words, identifying them with above-chance accuracy (P <
familiarization, subjects are confronted with two linguistic            0.0005; see Peña et al., 2002, for details).
stimuli and asked to offer a judgement as to which stimulus               In their view, these results imply knowledge of rules
is more similar to chunks of the familiarization stream.
                                                                        insofar as the very notion of an abstract rule-word underlies
Experiment A
                                                                        the successful discrimination of rule-words and part-words.
In experiment A, subjects were familiarized to a continuous
                                                                        Thus, they claim: “This seems to be due to the fact that the
speech stream as described above. In the test phase, they
were exposed to a word (AiXCi) and a part-word (XCiAj)1,                selected items are compatible with a generalization of the
both of them contained in the stream being heard during                 kind “If there is a [pu] now, then there will be a [ki] after an
                                                                        intervening X” (p. 606).
1
  Part-words can also be of the form CkAiX (see below).
                                                                    767

   Summing up, Peña et al. contend that two different               preferred, and experiment C, where the rule-word is
computational mechanisms must be responsible for the                favoured.
results of experiments A-C: Namely, a statistical mechanism            Thus, Peña et al. consider the prediction that subjects
for performing speech segmentation (experiment A), and a            would choose rule-words (#A1X*C1#) over part-words
rule-governed mechanism responsible for the induction of            (#XC2@A3#), once we consider “probabilities over
grammatical structural regularities in the corpus (experiment       syllables, pauses, and absence of pauses in the stream and
C).                                                                 the test items”, since “[t]ransitional probabilities between
                                                                    adjacent elements favours rule words over part words”
             Subliminal segmentation gaps                           (emphasis added).
In a footnote, however, although they consider a potential
rejoinder according to which a single statistical mechanism         Non-adjacent transitional probabilities
may be responsible for the induction of the structural              The problem with this comment is that no reason is offered
regularity in experiment C, they dismiss that alternative. As       as to why the only transitional probabilities to be computed
I shall try to show in what follows there’s a                       must be those between adjacent elements in the speech
misunderstanding in Peña et al.’s line of reasoning that            stream. They arbitrarily assume that a statistical learning
invalidates the conclusion they reach. Let’s first of all           mechanism can only be sensitive to immediately adjacent
rescue their whole footnote into the main text for clarity’s        patterns. However, there’s no reason not to believe that such
sake:                                                               mechanisms can be sensitive to higher order (i.e., non-
                                                                    immediately adjacent) regularities. This is so, especially
“Participants might have included the gaps as separate              since Peña et al.’s experimental setting was precisely
elements for computing transitional probabilities. As a             designed by constructing a lexicon mainly characterized in
result, they may have preferred rule words, not because they        terms of nonadjacent transitional probabilities; probabilities
extracted the structure of the stream, but because they             which, as they themselves acknowledge, are the cornerstone
computed probabilities over syllables, pauses, and absence          of the segmentation task in experiment A: “[We] explore
of pauses in the stream and the test items. Thus participants       whether participants can segment a stream of speech by
may have analyzed the rule words in the test as having the          means of nonadjacent transition probabilities, and we also
structure #A1X*C1# and the part words as having the                 ask whether the same computations are used to promote the
structure #XC2@A3# (where # indicates a pause and @ the             discovery of its underlying grammatical structure” (pp. 604-
absence of a pause). In this case, the transitional                 605; emphasis added).
probabilities between adjacent elements would favor rule               One first hurdle before we can decide whether the
words over part words and no sensitivity to the structure of        experimental results of Peña et al. point towards a non-
the rule words would be needed to prefer rule words. This           statistical route, is to specify exactly what it is that we are
hypothesis makes a prediction that has not been confirmed           comparing in terms of probability. Since, they simply
in a control experiment. Though in experiment [C] the test          comment that adjacent transitional probabilities for rule-
items do not contain pauses, in this control experiment we          words are higher than for part-words, but do not especify
tested participants (n = 14) with items including the pauses.       which generalizations support that choice, it is not clear how
Thus, participants compared rule words with structure               neat the prediction of their working hypothesis is.
#A1X*C1# to part words with structure #XC2#A3#. In this                We may then ask which test items subjects “should”
case, the presence of the pause in the part words makes the         choose once both adjacent as well as nonadjacent
transitional probability of the part word higher than that of       transitional probabilities are considered. Table 1 shows
the rule word. Therefore, if pauses counted as separate             (adjacent and nonadjacent) transitional probabilities
events in the computation, participants should favor the part       between syllables in the familiarization stream, and the
words over the rule words. Nevertheless, contrary to this           corresponding probability values. By taking into account
prediction, participants still preferred rule words to part         these probabilities, we can see if Peña et al.’s comments in
words” (fn. 27, p. 607).                                            footnote 27 are justified.
                                                                       Let us first take the rule-word #PUBEKI# and the part-
   In effect, there is no principled reason to exclude the          word #RAKI@BE#. According to Peña et al., a choice
possibility that subliminal segmentation gaps can be                based on the computation of adjacent transitional
exploited statistically. Notice that the very fact that these       probabilities should favour the rule-word. Their prediction
gaps are subliminal doesn’t prevent them from carrying              is correct. Notice that whereas the rule-word #PUBEKI# is
potentionally relevant information. It simply means that            backed up by predictions 1 and 4 in table 1 (summed
their presence is not available to conscious access. As a           transitional probabilities = 1.5), the part-word #RAKI@BE#
matter of fact, as Peña et al. well observe, they must carry        is only backed up by prediction 3 (which has a transitional
the critical piece of information for the mastery of structural     probability of 0.33).
induction since the inclusion of the gaps is the one and only
difference between experiment B, where the part-word is
                                                                768

    Table 1: Some adjacent and nonadjacent transitional         exclusively. Once nonadjacent transitional probabilities are
 probabilities between syllables/pauses extracted from Peña     taken into account, the transitional probability of the rule-
      et al.’s (2002) familiarization stream, and their         word becomes higher than that of the part-word. This means
              corresponding probability values.                 that participants in the control experiment may be
                                                                computing statistical information about segmentation gaps.
      Familiarization stream:                                   The prediction would be that they should favour rule-words
                                                                over part-words, which is exactly what happens in Peña et
      .....#PURAKI#BELIGA#TAFODU#PUFOKI#                        al.’s control experiment. Therefore, statistical computations
      TALIDU#BERAGA#TARADU#.....                                can, not only perform speech segmentation, but also
      Predictions between            Transitional               promote the discovery of the underlying structural
      adjacent items                 probability                regularities in the corpus.
                                                                Table 2: Adjacent and nonadjacent transitional probabilities
      1. # predicts PU               0.5                          for part-word #RAKI#BE# and for rule-word #PUBEKI#
      2. PU predicts RA              0.33                                                Predictions
                                                                                         between
      3. RA predicts KI              0.33                                                adjacent
                                                                        Experiment
                                                                                         items
      4. KI predicts #               1.0                                C
                                                                                         supporting
                                                                                         test
      5. # predicts BE               0.5                                                 preferences
                                                                                                        Summed
      6. BE predicts LI              0.33                                                               probabilities
      Predictions between non        Transitional                       Rule-word        1, 4           1.5
      adjacent items                 probability                        #PUBEKI#
                                                                        Part-word        3, 4, 5        1.83
                                                                        #RAKI#BE#
      7. # _ predicts RA             0.33                                                Predictions
                                                                                         between
      8. # _ _ predicts KI           0.33                                                non-
                                                                                         adjacent
      9. # _ _ _ predicts #          1.0                                                 items
                                                                                         supporting
      10. # _ _ _ _ predicts BE      0.5                                                 test
                                                                                         preferences
      11. PU _ predicts KI           1.0                                Rule-word        8, 9, 11, 12   3.33
                                                                        #PUBEKI#
      12. PU _ _ predicts #          1.0                                Part-word        14, 15, 16     2
                                                                        #RAKI#BE#
      13. PU _ _ _ predicts BE       0.5
                                                                                         TOTAL          adjacent +
      14. RA _ predicts #            1.0                                                                nonadjacent
                                                                                                        probability
      15. RA _ _ predicts BE         0.5                                                                based
                                                                                                        predictions
      16. KI _ predicts BE           0.5                                Rule-word                       4.83
                                                                        #PUBEKI#
      17. # _ predicts LI            0.33                               Part-word                       3.83
                                                                        #RAKI#BE#
      18. KI _ _ predicts LI         0.33
                                                                   On the other hand, part-words may be of two different
  What happens then in Peña et al.’s control experiment         types. They can be constructed by taking the last two
when test items include segmentation gaps? They contend         syllables of a word and the first one of the next word
that the transitional probability of the part-word              (XCiAj), or by joining the last syllable of a word and the
(#XC2#A3#) is higher than that of the rule-word                 first two syllables of the next word (CkAiX). Peña et al.
(#A1X*C1#). Table 2 shows that their claim is correct only      only consider part-words of the first sort. But the
if adjacent transitional probabilities are computed             aforementioned results can be consistently extended to the
                                                            769

second sort of part-words. As table 3 shows, were we to              both the hidden and context layers. In the familiarization
insert segmentation gaps in part-words of the second sort            phase the network was fed with 5,000 syllable tokens. The
(#KI#BELI#), predictions would still favour rule-words               task was to predict the next item in the sequence.2
over them in statistical terms.
                                                                             Output layer (10U)
Table 3: Adjacent and nonadjacent transitional probabilities
              for part-word (type 2) #KI#BELI#.
                                                                             Hidden layer (3U)
                         Predictions
                         between                                                                         Context layer (3U)
                         adjacent                                            Input layer (10U)
        Part-word
                         items
        (type 2)
                         supporting
                         test                                              Figure 1: Architecture of SRN used to illustrate the
                         preferences                                   exploitation of statistically-driven information. (the dashed
                                         Summed                                       line represents a copy connection).
                                         probabilities
        #KI#BELI#        4, 5, 6         1.83
                                                                     Results
                         Predictions
                                                                     To confirm the robustness of the results, 5 extended test
                         between
                                                                     corpora were created to investigate the predictions of Peña
                         non-
                         adjacent                                    et al: (i) words; (ii) part-words of type 1 (XCiAj); (iii) part-
                         items                                       words of type 2 (CkAiX); (iv) rule-words; and (v) part-
                         supporting                                  words that include segmentation gaps in between of the sort
                         test                                        considered in footnote 27 of Peña et al., (2002). With the
                         preferences                                 weights from the familiarization phase frozen, networks
        #KI#BELI#        16, 17, 18      1.16                        were tested on these five corpora. Figures 2-6 show
                                                                     differences in prediction root-mean-square (rms) error on
                         TOTAL           adjacent      +             test items for all five corpora.3
                                         nonadjacent
                                         probability                                               Part-words type 1
                                         based                               Words                 (XCiAj)
                                         predictions
        #KI#BELI#                        2.99
           Grammatical induction in SRNs
In order to back up empirically these results, I run a series of
connectionist simulations that illustrate the exploitation of
statistically-driven information. Following Elman (1990), I
trained a simple recurrent network (SRN) on a prediction
task to test if it could generalize to novel rule-words in the
line of Peña et al.’s experiments A, B and C, and their
control experiment.
                                                                         Figure 2: Network performance (RMS error) on words
Stimuli                                                                              versus “type 1” part-words (XCiAj).
Familiarization corpus
The familiarization corpus consisted of the same strings of
syllables used by Peña et al. (table 1). The corpus thus
consisted of CV syllables formed by concatenating all legal
combinations of consonants and vowels. CV syllables were
encoded in a localist way.
Network architecture and Task                                        2
                                                                       SRNs were trained with a learning rate of 0,1 during habituation.
The network had 10 input and output units, and 3 units in            3
                                                                         Although calculating error measures of probability-based
                                                                     predictions against likelihood vectors would have been more
                                                                     informative, for current purposes sum_rms values suffice.
                                                                 770

                             Part-words type 2                     congruent patterns should be smaller. This prediction is
          Words                                                    confirmed (figs. 2-6). The results of this simulation show
                             (CkAiX)
                                                                   that simple recurrent networks can generalize the abstract
                                                                   patterns embodied in their training set and gain an
                                                                   advantage in processing subsequent patterns of the same
                                                                   grammatical type (i.e., rule-words).
                                                                              Rule-words               #radu#be#
   Figure 3: Network performance (RMS error) on words
            versus “type 2” part-words (CkAiX).
    Part-words type 1
    (XCiAj)                        Rule-words
                                                                    Figure 6: Network performance (RMS error) on rule-words
                                                                      with structure #A1X*C1# and part-words with structure
                                                                       #XC2#A3# (see fn. 27 from Peña et al., 2002, above).
                                                                                             Conclusion
                                                                   The results reported here show that frequency and
                                                                   distributional properties in the corpus not only serve to
                                                                   segment statistically the data into its constitutive legal
                                                                   words, but also to explain the choices made by subjects that
                                                                   apparently involve manipulation of non-statistical
                                                                   information.
  Figure 4: Network performance (RMS error) on “type 1”               In this way, statistical computations based on nonadjacent
    part-words (XCiAj) versus rule-words with structure            transitional probabilities of the sort that are exploited in
                         #A1X*C1#.                                 speech segmentation (Saffran et al., 1996) may be used in
                                                                   order to induce existing grammatical regularities in the
    Part-words type 2                                              speech stream.4
    (CkAiX)                        Rule-words                         The arguments offered here don’t attempt to show that
                                                                   this indeed is the case, but rather to illustrate that this is an
                                                                   option that cannot be discarded beforehand.
                                                                                       Acknowledgements
                                                                   This work was supported by DGICYT Project BFF2003-
                                                                   129, and by a Ramón y Cajal research contract (Spanish
                                                                   Ministry of Science and Technology). The material draws
                                                                   out of preliminary work presented at the First Joint
                                                                   Conference of the Society for Philosophy & Psychology and
                                                                   the European Society for Philosophy & Psychology, in
                                                                   4
  Figure 5: Network performance (RMS error) on “type 2”               There is in fact a fairly clear literature demonstrating that
    part-words (CkAiX) versus rule-words with structure            recurrent networks can induce grammars from examples of
                                                                   context-free and context-sensitive languages; grammars that are
                         #A1X*C1#.
                                                                   precisely of a form in which there are long-distance dependencies.
                                                                   See for example Boden & Wiles (2000), and Chalup & Blair
  If the network has abstracted the structural regularities        (2003). Many thanks to Jeff Elman for bringing this to my
that underlie the familiarization corpus, prediction errors in     attention.
                                                               771

Barcelona, Spain. I thank Luca Bonatti, Jeff Elman, Toni          first order recurrent neural networks to predict a context-
Gomila and Javier Marín for helpful comments and                     sensitive language”, Neural Networks, 16, 955-972.
discussions.                                                      Elman, J. (1990). Finding Structure in Time. Cognitive
                                                                     Science, 14, 179-211.
                        References                                Peña, M., Bonatti, L., Nespor, M., & Mehler, J. (2002).
Boden, M., & Wiles, J. (2000). Context-free and context-             Signal-Driven computations in Speech Processing.
  sensitive dynamics in recurrent neural networks.                   Science, 298, 604-607.
  Connection Science, 12, 197-210.                                Saffran, J., Aslin, R., & Newport, E. (1996). Statistical
Chalup, S. K., & Blair, A. D. (2003). Incremental training of        learning by 8-month-old infants. Science, 274, 1926-1928.
                                                              772

