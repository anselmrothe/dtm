UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Probabilistic Model of Early Argument Structure Acquisition
Permalink
https://escholarship.org/uc/item/85z0g20q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Alishahi, Afra
Stevenson, Suzanne
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

        A Probabilistic Model of Early Argument Structure Acquisition
                                      Afra Alishahi and Suzanne Stevenson
                                              Department of Computer Science
                                                    University of Toronto
                                             {afra,suzanne}@cs.toronto.edu
                          Abstract                               Akhtar, 1999; Tomasello, 2000; Demuth et al., 2002). In
                                                                 support of this view is evidence that children initially
   We present a computational model of usage-based learn-        learn verb-argument patterns on an item-by-item (verb-
   ing of verb argument structure in young children. The         by-verb) basis, before forming a conceptualization of
   model integrates Bayesian classification and prediction
   to learn from utterances paired with appropriate seman-       more general syntactic structures (Tomasello, 2000). In-
   tic representations. The model balances item-based and        deed, Goldberg (1999) claims that it is the form-meaning
   class-based knowledge in language use, demonstrating          mappings, or constructions, of a set of high-frequency
   appropriate word order generalizations, and recovery          verbs that serve as the basis for generalization of their
   from overgeneralizations with no negative evidence or
   change in learning parameters.                                associated forms to other verbs. Moreover, experimental
                                                                 evidence has shown that the frequency of an individual
                                                                 verb influences the likelihood that children accept it in
        Argument Structure Acquisition                           an overgeneralized usage (Theakston, 2004).
Verb argument structure is a complex aspect of language             To further test these ideas, explicit usage-based mod-
for a child to master, as it requires learning the rela-         els must be explored, both of the underlying learning
tions of arguments to a verb, and how those arguments            mechanisms and of the use of the acquired knowledge.
are mapped into valid expressions of the language. Chil-         Here, we present a computational model that elaborates
dren, however, learn to correctly use common verbs quite         a specific mechanism for how children learn the argument
early. Moreover, they grasp argument structure regular-          structure of individual items, and how this knowledge is
ities at a young age, producing novel utterances that            generalized to new forms in language use.
obey the mapping of arguments to syntactic positions                Generalization in our model is achieved through a
in their language (Demuth et al., 2002; MacWhinney,              Bayesian classifier that groups similar argument struc-
1995). Children even “correct” experimenters’ non-SVO            ture frames. Each frame represents a combination of
(subject-verb-object) usage of novel verbs to fit the SVO        semantic and syntactic properties of a verb and its argu-
order of their ambient language as early as age 3 (Akhtar,       ments in a particular usage. Frames with shared syntax
1999).                                                           are grouped according to probabilities over their seman-
   The ability to generalize observed argument structure         tic properties. The result is that the semantic primitives
patterns to novel situations sometimes leads to overgen-         most frequently used across all frames in a class have
eralization (Bowerman, 1982). For example, children of-          the highest probabilistic association with the syntactic
ten use an intransitive-only verb such as fall in a transi-      form. The emergent classes capture form-meaning pair-
tive construction, as in:                                        ings that generalize over the fine-grained semantics of
      Adam fall toy. [Adam 2;3, CHILDES MacWhinney (1995)]       both the verb and its arguments. While the resulting
Such usages are not arbitrary errors, but rather general-        classes have a similar function to the constructions of
izations of the association between causative action and         Goldberg (or the event structure templates of Rappa-
transitive form, to an intransitive action verb.                 port Hovav and Levin (1998)), in our case, the syntactic
   Thus, acquisition of argument structure exhibits a U-         pattern is associated with a range of verb and argument
shaped learning curve seen in other areas of language            meanings probabilistically.
learning (Marcus et al., 1992): correct use of a verb may           A key property of our computational model is the in-
be followed by a period containing incorrect (overgener-         teraction between item-based and class-based informa-
alized) usages, before convergence on adult behaviour.           tion, and how it plays a role in language use—both com-
Moreover, negative evidence (corrective feedback) plays          prehension and production. Here, we focus on simula-
little to no role in this process; only additional positive      tions of the child in utterance production and demon-
evidence of correct usages is necessary for “unlearning”         strate how the classes of argument structure frames en-
of overgeneralized rules (Marcus, 1993).                         able the model to generalize over observed forms. Exam-
   Learning in this domain has been suggested to rely            ples of the model experiencing a period of overgeneral-
on rich innate knowledge of argument structure regu-             ization illustrate that the probabilistic balance of item-
larities (Pinker, 1989). However, recent psycholinguistic        based and class-based information may shift toward class
evidence has questioned this assumption, and a number            knowledge when knowledge of particular verbs is infre-
of usage-based proposals have argued that children learn         quent. Subsequent recovery occurs with additional posi-
such regularities from the input alone (Bowerman, 1982;          tive evidence due to the increased strength of item-based
                                                             97

  Scene-Utterance Input Pair                                             Algorithm 1 Learning an argument structure frame
  TAKE[cause,move] (TIMhagenti , BOOKShthemei , TO[] (SCHOOLhgoali ))
  Tim took books to school.                                              1. Extract frame F from a scene-utterance pair (see Figure 1),
                                                                            and access lexical entry L of the head verb of F .
  Extracted Frame
  head verb                           take                               2. If F is incomplete, then use both the frame structure of
  semantic primitives of verb         hcause, movei                         L and the information stored in the classes to predict the
                                                                            best value for the missing part(s) of F .
  arguments roles                     hagent, theme, goali
                  categoriesa         hhuman, concrete, dest-predi       3. Update the lexicon:
  syntactic pattern                   arg1 verb arg2 arg3                 (a) If L already contains a matching frame F 0 , then increase
a
  Extracted from a representation of the child’s ontology.                     the frequency of both F 0 and its parent class;
                                                                          (b) Else add F to L, and find the best class for F (creating
  Figure 1: An input pair and its corresponding frame.                         a new class if needed).
knowledge in the probability formulas (not due to ex-                    Using the Acquired Knowledge
plicit negative evidence, nor any change in the learning                 As mentioned, both item-based and more general class-
parameters).                                                             based information are updated with the processing of
                                                                         each input. A key property of our model is how these
  Overview of the Computational Model                                    two sources of information interact in the use of language
                                                                         during the course of acquisition. We formalize a number
Learning Argument Structure Knowledge                                    of tasks in language use as different versions of a predic-
We assume that the input to the argument structure ac-                   tion problem. For example, sentence production is seen
quisition process consists of pairs of representations, one              as predicting the syntactic pattern of a frame based on
for the perceived utterance (what the child hears), and                  its semantic components; in comprehension, argument
one for the relevant aspect of the observed scene (the                   roles may be assigned, or partial meaning of a verb or
semantics described by the utterance).1 The first step                   noun induced, based on the participants in the scene
for our learning model is to extract the corresponding                   and the syntax of the utterance. Hand in hand with our
argument structure frame for the main verb of the ut-                    Bayesian classifier is a Bayesian prediction process for
terance.2 Each frame contains a set of features drawn                    use in such language tasks.
from the scene-utterance pair, as shown in Figure 1.                        The prediction process integrates item-based and
   Each observed frame is stored in the lexical entry of                 class-based information to make probabilistic predictions
the verb. If the current extracted frame has been pre-                   about argument structure information based on available
viously observed with the verb, then the frequency of                    frame features. If the child’s knowledge of a particular
the stored frame is increased, otherwise a new frame is                  use of a verb is sufficiently complete and frequent to have
inserted. Some of the frames of a verb may merge to                      become entrenched , then that information will be used
form a more general one (e.g., if two frames are identical               in both comprehension and production. On the other
except that the semantic types of the arguments of one                   hand, by generalizing over known frames, classes enable
frame are more general than the other).                                  predictions about missing or low-confidence item-based
   Any new frame (whether newly observed or the out-                     information. For example, in production, such predic-
put of a merging process) is input to the incremental                    tions allow the child to use a verb in a novel (for that
Bayesian classifier, which groups the new frame together                 verb) syntactic pattern, as long as semantically similar
with an existing class of frames that probabilistically has              verbs have been observed in that usage.
the most similar properties to it. If none of the existing
classes has sufficiently high probability, then a new class                 Bayesian Classification and Prediction
is created. The probability of each class is determined
by both syntactic and semantic features. Currently, a
                                                                         The Bayesian Classifier
class with a different syntactic pattern from that of the                The model we use for classification is an adaptation of
frame, or a different set of argument roles, would have a                a model of human categorization proposed by Ander-
very low probability. The probability of semantic primi-                 son (1991).3 This model has desirable properties for our
tives is determined by how frequently those of the frame                 domain. Its use of probabilities over observed informa-
occur across the frames of the candidate class. The prob-                tion captures the emphasis on item frequencies in child
ability of semantic categories of arguments is calculated                language acquisition. Also, the classification model is
similarly, taking into account the relationship of cate-                 incremental, enabling us to classify frames as they are
gories in the ontology.                                                  observed.
   The overall learning process of the model is summa-                      Classification of a frame F is formalized as a process
rized in Algorithm 1.                                                    of maximizing the probability of class k given the frame:
    1                                                                                  BestClass(F ) = argmax P (k|F )               (1)
      Picking out the specific semantic descriptor that matches                                              k
an utterance from the full scene representation is itself a chal-
lenge for the child. We assume that this “word-meaning map-
ping” has been performed, e.g., as in Siskind (1996).                        3
                                                                               Our classes are not pre-defined, as is often intended by
    2
      Here, we focus on verb argument structure, but our                 the term in machine learning. Our classes/classification pro-
model applies to other predicates as well.                               cess could as well be termed clusters/clustering.
                                                                      98

where k ranges over the indices of all classes, including            One option is to limit generalization to only the frames
an index of 0 to represent creation of a new class. Using         associated with the verb:
Bayes rule, this probability can be rewritten as:                                              X
                                                                             Pi (j|F ) =                 P (kv |F )Pi (j|kv )     (6)
                  P (k)P (F |k)        P (k)P (F |k)                                      kv ∈classes(v)
      P (k|F ) =                 =P                        (2)
                      P (F )          k0 P (k )P (F |k )
                                              0       0
                                                                  where classes(v) is the set of indices of the classes of
                                                                  the frames learned for verb v, such that kv is a class
   The prior probability of a class k is given by:4               containing a frame of verb v. Thus we use the classes
                                    nk                            associated with a verb to probabilistically predict miss-
                         P (k) =                           (3)    ing values (Pi (j|kv )), and weight those predictions by the
                                  n+1
                                                                  likelihood of the class given the partial frame (P (kv |F )).
where n is the total number of observed frames; nk is                However, this formulation ignores the information em-
the number of frames in class k, for k > 0; and n0 = 1.           bedded in the class structure more generally, unnecessar-
Thus, the estimation of the prior probability of an ex-           ily restricting the child when a partial frame for a verb
isting class is proportional to the frequency of frames in        does not match well with any frames seen previously
that class, and the probability of a new class is inversely       for that verb. We require a model in which prediction
proportional to the number of observed frames overall.            of missing features takes into account both the knowl-
   The probability of a frame F is expressed in terms of          edge of likely values across the frames associated with
the individual probabilities of its features (shown above         the given verb, as well as the knowledge of likely values
in Figure 1). Although these features in reality interact         associated with any class compatible with the partial
(e.g., the role of an argument and the syntactic position         frame. That is, we must balance item-based knowledge
it occurs in are interrelated), we adopt an independence          with class-based knowledge.
assumption to make the calculation feasible. Thus, the               Interestingly, we can achieve the item- and class-based
conditonal probability of a frame F is the product of the         integration by incorporating the classification process
conditional probabilities of its features:                        into our probability model for prediction. Essentially,
                                 Y                                we classify a partial frame F using equation (1) as if it
               P (F |k) =                  Pi (j|k)        (4)
                                                                  were a new frame, and treat it during prediction as if it
                           i∈FrameFeatures
                                                                  were inserted into the lexical entry for v with a frequency
where j is the value of the ith feature of F , and Pi (j|k) is    of 1. The best class kF (across all classes) for F may or
the probability of displaying value j on feature i within         may not be a class already linked to by a frame of v, so
class k. This probability is estimated using a smoothed           we must modify equation (6) by extending (if necessary)
maximum likelihood formulation.                                   classes(v), over which kv takes its values, to include kF .
                                                                  This ensures that both the overall best class, as well as
The Corresponding Prediction Model                                the classes associated with the verb, are taken into con-
Once the child learns the information above—i.e., the             sideration in predicting values for a partial frame.
individual frames stored with each verb, along with the              The probability P (kv |F ) in equation (6) is rewritten
class structure over them—we must consider how this               using Bayes rule (cf. equation (2)):
knowledge is used. An important aspect of our model                                P (kv )P (F |kv )        P (kv )P (F |kv )
is that essentially the same Bayesian framework can be               P (kv |F ) =                     =P                          (7)
                                                                                        P (F )              kv0 P (kv )P (F |kv )
                                                                                                                    0         0
employed in using the knowledge as that which acquires
                                                                  The factor P (F |kv ) is calculated as in equation (4), us-
it. We formulate a language task as a prediction prob-
                                                                  ing a uniform probability distribution over the possible
lem, in which missing feature values in a frame are filled
                                                                  values of the missing feature. The prior probability of
in based on the most probable values given the available
                                                                  the class, P (kv ), is calculated taking into account only
features. Following Anderson (1991):
                                                                  the classes in classes(v) (including kF ), not the full class
              BestValuei (F ) = argmax Pi (j|F )           (5)    structure. In calculating P (kv ), the frequency of each
                                      j
                                                                  class (its total number of frames) is weighted by the fre-
where F is the partial frame, i is the missing feature,           quency of the frame for v which points to it, balancing
and j ranges over the possible values of i.                       the overall likelihood of the class with the likelihood that
   In Anderson’s model, the classes are used in the calcu-        it is a class for v.
lation of Pi (j|F ) to determine the most probable value             If kF was not previously a class for v, then the weight
for the missing feature. However, the structure of our            from the frame frequency is only 1. Thus, a “new” class
acquired knowledge is more complex than that of his cat-          (new to v) for F has more influence the less often the
egory structures. In addition to the groupings of frames          verb has been seen overall; if the verb has been seen
into classes, we also have the groups of frames associated        frequently, then the weight from its observed frames to
with each particular verb. Thus there are two potential           their classes will outweigh the influence of the single par-
levels of generalization, rather than one—over the frames         tial frame to its “new” class. (Cf. the influence of a new
associated with a single verb (item-based), and over all          class achieved by equation (3).) Thus, class information
frames, through the full class structure (class-based).           outside the verb is always a factor in prediction, but
    4
      This formula is the same as that used by Anderson (1991)    will have decreased influence with increased item-based
with his “coupling probability” set to the mid value of 0.5.      frequency.
                                                               99

                                                                                 100                                                                                 100                                                                                 100                                                                                 100
              Experimental Set-Up                                                 90
                                                                                  80
                                                                                  70
                                                                                                                                                                      90
                                                                                                                                                                      80
                                                                                                                                                                      70
                                                                                                                                                                                                                                                          90
                                                                                                                                                                                                                                                          80
                                                                                                                                                                                                                                                          70
                                                                                                                                                                                                                                                                                                                                              90
                                                                                                                                                                                                                                                                                                                                              80
                                                                                                                                                                                                                                                                                                                                              70
                                                                                  60                                                                                  60                                                                                  60                                                                                  60
                                                                      accuracy                                                                            accuracy                                                                            accuracy                                                                            accuracy
                                                                                  50                                                                                  50                                                                                  50                                                                                  50
Basic Properties of the Input
                                                                                  40                                                                                  40                                                                                  40                                                                                  40
                                                                                  30                                                                                  30                                                                                  30                                                                                  30
                                                                                  20                                                                                  20                                                                                  20                                                                                  20
                                                                                  10                                                                                  10                                                                                  10                                                                                  10
                                                                                   0                                                                                   0                                                                                   0                                                                                   0
                                                                                       0   100   200   300          400           500   600   700   800                    0   100   200   300          400           500   600   700   800                    0   100   200   300          400           500   600   700   800                    0   100   200   300          400           500   600   700   800
                                                                                                             number of input pairs                                                               number of input pairs                                                               number of input pairs                                                               number of input pairs
We focus on learning the argument structure of a small
                                                                                                       go                                                                            come                                                                                      eat                                                                                 fall
group of verbs (and prepositions) whose semantic prim-
itives are largely detectable by the child from the scene.
We assume that a small number of nouns have been pre-                Figure 2: Sample learning curves for different verbs. X-
viously learned by the child, forming a simple ontology              axis is time (# of inputs); y-axis is cumulative accuracy.
that indicates their semantic category. As illustrated in
Figure 1, we use a simple logical form for representing an
observed scene. We assume that at the stage of learning              The Learning Curve
being modelled, the child has reliable hypotheses about              As an item-based model that incorporates a generaliza-
the assignment of roles to arguments.                                tion mechanism, we expect an overall U-shaped learning
   In the current implementation, a syntactic pattern is             curve from our system, but also expect variation among
limited to the order of the arguments with respect to the            individual verbs. For each verb, we ran eight separate
predicate term. Also, for now we do not address learning             simulations of our model over 800 input pairs: 200 se-
of morphology; all words appear only in their root forms.            quences of 4 complete input pairs followed by a 5th test
                                                                     input (using the target verb) in which the utterance was
The Input Corpora                                                    removed. Our prediction model was used to find the
We create input lists of scene-utterance pairs that con-             syntactic pattern with the highest probability for the re-
form to the distributional characteristics of the data               sulting partial frame. After each test input, we measured
children receive from their parents. We extracted from               the cumulative accuracy of the model by counting the
CHILDES the 20 most frequent verbs in mother’s speech                total number of times the predicted pattern was exactly
to each of Adam (2;3–4;10), Eve (1;6–2;3), and Sarah                 the same as that used in the removed utterance.
(2;3–5;1). The 13 verbs in common across these three                    Figure 2 shows a sample learning curve for each of the
lists were added to an input-generation lexicon, along               verbs go, come, eat and fall . Since the input corpora
with their frequency and a unique semantic symbol. We                are randomly generated, the performance of the model
also assign each verb a set of possible argument structure           varies across simulations in the early stages. For frequent
frames and associated frequencies, which are manually                verbs with a variety of argument structures, such as go
compiled by examination of all uses of a verb in all con-            and come, a U-shaped curve is often observed. A verb
versations of the same three children. Prepositions used             with fewer types of frames, such as eat, is less often
in these conversations were also added to the lexicon.               overgeneralized. The learning curve for fall , which is
   For each simulation in our experiments, an input cor-             less frequent, shows a delay compared to more frequent
pus of scene-utterance pairs is automatically created                verbs.5
from this input-generation lexicon, using the frequencies            Imitation and Generalization
to determine the probabilities of selecting a particular
verb and argument structure for each input. Arguments                To study generalization in our model, we examine its
of verbs are also probabilistically selected, constrained to         behaviour when presented with a novel verb for which
conform to the indicated semantic category of the argu-              it must produce a sentence. After training the model,
ment. Arguments which are predicates (such as prepo-                 we present it with only a scene representation, whose
sitional phrases) are constructed recursively.                       main predicate corresponds to a verb, gorp, which has
   To simulate noise, every third input pair in every gen-           not been seen in an utterance:
erated corpus has one of its features randomly removed.                                           GORP[cause,act] (KITTYhagent i , DOGGYhthemei )
During a simulation, each missing feature is replaced                Since this semantics resembles the typical transitive con-
with the most probable value predicted at that point in              struction, we expect the model to predict the transitive
learning, corresponding to a child learning from her own             pattern, “arg1 verb arg2”, despite its lack of knowledge
inferred knowledge. The resulting input data is noisy,               of this verb. To observe the pattern of responses over
especially in the initial stages of learning.                        the course of acquisition, we test the model after vary-
                                                                     ing amounts of training data. Averaging over 100 sim-
                                                                     ulations on different input corpora, the model predicts
              Experimental Results                                   this pattern with 68% probability after 5 input pairs, but
We focus on results of our model on the sentence pro-                with 99% probability after processing 50 input pairs.
duction task, for comparison to actual child data on verb               More interesting is the varying influence of item-based
use. The simulations we report all use the same param-               and class-based knowledge over the course of acquisition,
eter settings; only the randomly generated input corpus              which is a key feature of our model. To explore this,
differs. We first describe the learning curves displayed             we mimic the conditions of an experiment by Akhtar
by our model for some example verbs. We then exam-                      5
                                                                          In addition to classical overgeneralization errors, other
ine some interesting stages in the generally observed U-             errors reflected in the learning curves include incorrect learn-
shaped curve in more detail: imitation, generalization,              ing from noisy input, or from the model simply not having
overgeneralization and recovery.                                     learned the required usage.
                                                               100

                                         # Input Pairs           tive construction was between 0.14–0.21. In 7 out of 10
                                        50    100 150            simulations, the model showed a pattern of overgeneral-
   Generated     observed (SOV)        0.98 0.53 0.11            ization and recovery—using the transitive syntax for fall
    Pattern      “corrected” (SVO)     0.02 0.47 0.89            at some point, and eventually producing only correct in-
                                                                 transitive forms as it saw more examples of fall .
                                                                    The first 8 uses of fall for Adam in CHILDES (at 27
 Table 1: Average probability of the predicted patterns.         months) are given below, with the first 8 sentences gener-
                                                                 ated by our model in one of the simulations, illustrating
                                                                 the mix of the two patterns at this stage:
(1999), in which English-speaking children aged 2 to 4
were taught novel verbs used in non-standard (SOV or                           Adam             Our model
VSO) orders. In productions of these verbs, 2- and 3-                          go fall!         kitty fall
                                                                               no no fall no!   Mary fall
year-olds matched the observed patterns roughly half the                       no fall!         Mary fall toy
time and “corrected” the order to SVO roughly half the                         oh Adam fall.    doggy fall spoon
time. The 4-year-olds rarely matched the observed order,                       Adam fall toy.   apple fall
almost always correcting to SVO order.                                         Adam fall toy.   book fall
                                                                               oh fall.         John fall ball
   We trained our model using different numbers of in-                         I not fall.      ball fall
put pairs to simulate differing amounts of exposure to
the language. We then provided to the model a train-             By the age of 31 months, the causative (transitive) uses
ing input pair with a novel verb used in a non-standard          of fall gradually disappear from Adam’s conversations.
(SOV) order (kitty doggy gorp for the scene representa-          Over the 7 simulations in our model showing the pattern
tion above). Next, we presented the same scene represen-         of overgeneralization and recovery, causative sentences
tation to the model, without the utterance, and recorded         were no longer output after processing an average of 136
the syntactic pattern predicted by the model. We per-            training inputs.
formed 100 simulations for each number of training input
pairs, and averaged the probability of predicting each of        Summary of Results
the SOV (observed) and SVO (“corrected”) patterns. As            Taken together, these experiments show an impressive
can be seen in Table 1, the model, like children, shows          match between the general behaviour of the model and
a shift from imitation, where it repeats even an unusual         that of children concerning the interplay between item-
form, to generalization, where it relies on the ubiquitous       based and class-based knowledge in acquisition of ar-
patterns, the more exposure it has to the language.              gument structure. Imitation of observed forms occurs
                                                                 early in acquisition, but as evidence of general patterns
Overgeneralization and Recovery                                  increases, so does the tendency to generalize. This ten-
We noted that use of general knowledge sometimes leads           dency can even overwhelm infrequent verbs used in less
children to overgeneralize, but they eventually recover          common constructions, such that a period of overgen-
with only additional positive evidence. A typical over-          eralization may set in. However, simply receiving ad-
generalization is when a non-causative verb is used as           ditional examples of the verb in its correct usage can
causative, e.g., Don’t you fall me down (Bowerman,               guide the model to recovery from overgeneralization.
1982). We tracked the usage of fall by our model to              The model achieves this range of behaviour across the
see if we can detect a similar pattern to that in children.      course of acquisition with no explicit negative evidence,
   The entry for fall in the input-generation lexicon al-        nor even changes in the learning parameters, which are
lows only an intransitive syntactic pattern (as in The           held constant. The results are simply the consequence
blocks fell ). However, the scene representation for a use       of the Bayesian classification model and the unique in-
of fall may include the agent who caused the falling (e.g.,      teraction of class-based and item-based knowledge in the
if Adam pushed the blocks over). Each use of fall in             corresponding Bayesian prediction process.
these simulations includes a causal agent in the scene
description with a 0.5 probability. We therefore expect                               Related Work
the semantic similarity of the scene to that of a transitive     A number of recent computational models take an item-
construction to sometimes lead to overgeneralization—            based approach to language acquisition. Niyogi (2002)
i.e., the prediction of a transitive pattern with fall in a      proposes a Bayesian model that shows how syntactic
scene with a causal agent.                                       and semantic features of verbs interact to support learn-
   In these simulations, we test the behaviour of the            ing. However, in contrast to our model, the structure
model in producing a syntactic pattern for fall over the         of the verb classes and their probabilities, as well as the
course of acquisition. Every 5 training input pairs was          probabilities of verbs showing particular features, are all
followed by a semantic representation with fall as its           fixed. Chang’s (2004) model successfully learns multi-
main predicate, and the prediction model chose the best          word constructions (form-meaning pairs) from child data
syntactic pattern for it. (Any place-holders for argu-           annotated with scene representations, but relies on noise-
ments that were not present in the scene were left blank.)       free input and extensive prior knowledge, and construc-
Over 10 such simulations, the probability of receiving           tions are not generalized across verbs. The connectionist
a training pair containing fall varied from 0–0.02, and          model of Allen (1997) is able to make interesting general-
the probability of receiving an instance of the transi-          izations over argument structure syntax and semantics.
                                                             101

However, learning of general constructions is implicit,                                References
and the acquired knowledge cannot be used in any lan-             Akhtar, N. (1999). Acquiring basic word order: evidence
guage task other than limited comprehension.                        for data-driven learning of syntactic structure. Journal
   Existing models also deal with other aspects of the              of Child Language, 26:339–356.
problem we discuss here. The connectionist model of               Allen, J. (1997). Probabilistic constraints in acquisition.
Desai (2002) learns a miniature language from a set of              In Sorace, A., Heycock, C., and Shillcock, R., editors,
scene-sentence pairs, but prediction is limited to a com-           Proc. of GALA97, pages 300–305.
ponent of the meaning of a novel word based on its                Anderson, J. R. (1991). The adaptive nature of human
syntactic context. Buttery (2003) is focused on learn-              categorization. Psychological Review, 98(3):409–429.
ing syntax and addresses the learning of meaning only             Bowerman, M. (1982). Evaluating competing linguistic
at the word level. Onnis et al. (2002) present evidence             models with language acquisition data: implications of
that child-directed input has statistical properties that           developmental errors with causative verbs. Quaderni
enable the learner to recover from overgeneralization of            di semantica, 3:5–66.
argument structure patterns, given the selection of the
“simplest” grammar, but they do not develop an actual             Buttery, P. (2003). A computational model for first lan-
model of grammar learning.                                          guage acquisition. In Proc. of CLUK6, pages 1–8.
                                                                  Chang, N. (2004). Putting meaning into grammar learn-
                                                                    ing. In Proc. of the 1st Workshop on Psychocomputa-
          Conclusions and Future Work                               tional Models of Human Language Acquisition.
                                                                  Demuth, K., Machobane, M., Moloi, F., and Odato, C.
Our computational model demonstrates the feasibility                (2002). Rule learning and lexical frequency effects
of learning argument structure regularities from exam-              in learning verb-argument structure. In Proceedings
ples of verb usage, and suggests acquisition mechanisms             of the 26th Annual Boston University Conference on
underlying this process in children. The model exploits             Language Development, pages 142–153.
item frequencies within a Bayesian classification process;        Desai, R. (2002). Bootstrapping in miniature language
the explicit use of classes allows the model to capture             acquisition. In Proceedings of the 4th International
relevant generalizations without having to consider all of          Conference on Cognitive Modelling.
the lexical frames learned to that point. A novel formula-        Goldberg, A. E. (1999). The emergence of the semantics
tion views language use as a Bayesian prediction process;           of argument structure constructions. In MacWhinney,
a single probability formula smoothly integrates item-              B., editor, Emergence of Language. Lawrence Erlbaum
based and class-based information in predicting needed              Associates, Hillsdale, NJ.
argument structure properties. This probabilistic ap-             MacWhinney, B. (1995). The CHILDES project: tools
proach makes the model robust against noisy input and               for analyzing talk. Hillsdale, NJ: Lawrence Erlbaum.
low-confidence information. Furthermore, the model can
apply its acquired knowledge across a variety of language         Marcus, G. F. (1993). Negative evidence in language
tasks. Here we have focused on the task of sentence pro-            acquisition. Cognition, 46:53–85.
duction. Our simulations of the model over the course             Marcus, G. F., Pinker, S., Ullman, M., Hollander, M.,
of acquisition show a promising match with child data in            Rosen, T. J., and Xu, F. (1992). Overregularization
terms of the observed stages of imitation, generalization,          in language acquisition. Monographs of the Society for
possible overgeneralization, and eventual convergence on            Research in Child Development, 57(4, Serial No. 228).
correct argument structure usage.                                 Niyogi, S. (2002). Bayesian learning at the syntax-
   The model in its current form makes simplifying as-              semantics interface. In Proceedings of the 24th Annual
sumptions that must be addressed in future work. For                Conference of the Cognitive Science Society.
example, the input includes the semantic primitives for           Onnis, L., Roberts, M., and Chater, N. (2002). Simplic-
the coarse-grained semantics of the verbs, as well as the           ity: a cure for overgeneralizations in language acqui-
assignment of roles to arguments. A full usage-based ac-            sition? In Proceedings of the 24th Annual Conference
count of argument structure acquisition must show how               of the Cognitive Science Society, pages 720–725.
these are learnable from the input—indeed, a more likely          Pinker, S. (1989). Learnability and cognition: the acqui-
scenario is that acquisition of such aspects is interleaved         sition of argument structure. MIT Press.
with the learning of the properties discussed here. We            Rappaport Hovav, M. and Levin, B. (1998). Building
believe that using a distributed representation of roles,           verb meanings, pages 97–134. CSLI Publications.
e.g., as in Allen (1997), will begin to address this issue by     Siskind, J. M. (1996). A computational study of cross-
enabling the model to assign roles probabilistically. We            situational techniques for learning word-to-meaning
also need to further develop our acquisition mechanism              mappings. Cognition, 61:39–91.
to account for learning of collocations, idiomatic phrases
                                                                  Theakston, A. L. (2004). The role of entrenchment in
and fine-grained selectional preferences. One approach
                                                                    children’s and adults’ performance on grammaticality
might begin by maintaining a probability distribution
                                                                    judgment tasks. Cognitive Development, 19:15–34.
over the words that participate in each argument posi-
tion of a frame, raising additional interesting issues in         Tomasello, M. (2000). Do young children have adult
generalization of knowledge.                                        syntactic competence? Cognition, 74:209–253.
                                                              102

