UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Intermediate Features and Informational-level Constraint on Analogical Retrieval
Permalink
https://escholarship.org/uc/item/2t581143
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Finlayson, Mark Alan
Winston, Patrick Henry
Publication Date
2005-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                        Intermediate Features and Informational-level
                                  Constraint on Analogical Retrieval
                                                  Mark Alan Finlayson
                                                 Patrick Henry Winston
                                  Computer Science and Artificial Intelligence Laboratory,
                                            Massachusetts Institute of Technology,
                                          32 Vassar St., Cambridge, MA 02139 USA
                                               {markaf, phw}@csail.mit.edu
                           Abstract                              ings of stories for inferential soundness are negatively
                                                                 correlated. Other problem solving and retrieval studies
   Two different sorts of retrieval in analogical tasks,         showed that subjects needed to be explicitly informed of
   novice-like and expert-like, have been demonstrated in
   psychological experiments. With recent computational          the relation between two problems before they were able
   work in object recognition as an inspiration, we pro-         to apply analogical inferences, and that recall is heavily
   pose the computation of intermediate features, and their      dependent on surface semantic or syntactic similarities
   use as triggers for retrieval, as the relevant constraint     between representations (Reed, Ernst, & Banerji, 1974;
   at the informational level of characterization of the re-     Reed, Dempster, & Ettinger, 1985; Ross, 1984, 1987).
   trieval process as seen in experts. We conduct computa-
   tional experiments which show that, in conjunction with       The pattern of retrieval shown by these experiments is
   a feature-matching retrieval mechanism, features of an        clear: in uncontrolled populations, analogically related
   intermediate size and complexity give the strongest ana-      items are not preferred in retrieval.
   logical retrieval.                                               In contrast, evidence drawn from the literature on ex-
                                                                 perts suggests that a high level of skill and training in
    An Intriguing Difference in Retrieval                        a particular domain can allow for the recall of analogi-
                                                                 cally related knowledge. For example, in a classic study,
The retrieval of relevant precedents is commonly con-            Chi and colleagues (1981) demonstrated that physics ex-
sidered a critical first step in the analogical reason-          perts (advanced graduate students in the area) catego-
ing process (French, 2002). Psychologists have stud-             rize on the basis of abstract physics principles, whereas
ied retrieval in detail, using a straightforward experi-         novices categorize on the basis of literal features. Shafto
mental paradigm; in this paradigm, referred to by some           and Coley (2003) have shown similar effects with college
as the “And-now-for-something-completely-different ap-           students versus commercial fisherman in the categoriza-
proach” (Brown, 1989), subjects are given some set of            tion of marine creatures. Novick (1988) showed that
source tasks (such as memorizing a set of stories, or solv-      experts compared to novices are more likely to demon-
ing a problem), and then after some delay are given a            strate spontaneous analogical transfer when problems
second set of target tasks that are related to the source        share structural features. These categorization exper-
tasks in an analogical fashion. The experiments then             iments are not exactly parallel to the retrieval experi-
test if the subjects are able to perform the target tasks        ments described above; the single study on expert re-
by analogy with the source tasks.                                trieval we were able to locate is by Shneiderman (1977)
   Studies of this sort have characteristically provided         which showed that expert computer programmers re-
strong evidence that most people do not retrieve analog-         called computer programs primarily based on the pur-
ically profitable items from memory, even when delays            pose of the code, but not on its specific form, while
between source and target tasks are short, or when ana-          novices were heavily influenced by trivial syntactic con-
logical relationships are especially strong. The seminal         structions. For the purposes of this paper, we will take
demonstration of this was by Gick and Holyoak (1980,             as a given the natural conjecture that experts are better
1983) who reported that nearly two-thirds of their sub-          than novices at retrieving useful analogical precedents
jects were unable to spontaneously retrieve analogous            within their domain of expertise, with the caveat that
source problems. Since then, a wide variety of stud-             this supposed difference may be upended by future sur-
ies have provided strong evidence for people’s inabil-           prising experimental results.
ity to spontaneously recall relevant analogs. Gentner
and Landers (1985) conducted a story recall experiment
from which they concluded that, rather than retrieving
                                                                   Explanation at the Informational Level
on the basis of analogical relatedness, people retrieve          This supposed difference in recall between experts and
on the basis of surface similarity—i.e., the characteris-        novices is intriguing. How might it be explained? A
tics or properties of actors and objects involved in the         simple hypothesis is that immediate, visceral recall from
description. Rattermann and Gentner (1987) went on               memory is a relatively automatic process, and the ac-
to show that object-descriptions and first-order relations       tual mechanism does not vary substantially from person
between objects promote retrieval, but higher-order re-          to person. Instead, the difference between expert-like
lations do not, and that preference of retrieval and rat-        and novice-like retrieval would be in the parameters of
                                                             666

the process; in other words, the algorithm is the same,           Intermediate Features in Symbolic
but the constraints on algorithm are different. This so-          Representations
called informational or computational level treatment of          By analogy with visual recognition, we can think of fea-
the problem, an approach outlined explicitly by Marr              tures in the symbolic domain as portions of a description.
(1982), has already been profitably applied in the study          Our proposal is thus that retrieval occurs by a feature-
of analogy, in particular to models of the construction           matching process (Holyoak & Koh, 1987) and that it
of an analogical mapping (Keane, Ledgeway, & Duff,                is the size of the features which vary between experts
1994; Palmer, 1989). With respect to retrieval, we ask            and novices. If we think, as is common in computa-
the question, ‘what are the information-level constraints         tional cognitive modelling, of the cognitive descriptions
that allows experts to efficiently retrieve analogs?’ That        as graph-like representational structures, a feature would
is, what is it that experts are doing (or computing) which        be a collection of nodes from that description, and an in-
allows them to effect analogical retrieval? Our proposal          termediate feature would be a collection of nodes which
is that retrieval occurs by means of a feature-matching           was not too small or too large relative to the whole. This
process, and that when a person shows expert-like re-             notion is directly related to Gentner’s notion of zero-,
trieval, what they are doing is calculating and using what        first- and higher-order nodes in a symbolic representa-
we will call intermediate features. What are intermedi-           tion (Gentner, 1983). ‘Small’ features would have nodes
ate features? They may be explained best, perhaps, by             of low order in them; ‘intermediate’ features might con-
an analogy with previous work.                                    tain first- and second- order nodes plus their descendants
                                                                  (i.e., they would be first- and second- order systematic
Intermediate Features in Visual Recognition                       representations, as Gentner might say), and large fea-
                                                                  tures would include high-order nodes plus descendants.
Our inspiration for pursuing intermediate features for            A few examples of fragments of various sizes are shown
analogical recall was recent work on visual image clas-           in Figure 1, where the description of the orbit of a planet
sification by Ullman, Vidal-Naquet, and Sali (Ullman,             around a sun is split into small fragments (such as sun
Vidal-Naquet, & Sali, 2002). Ullman and coworkers                 and planet), intermediate fragments (such as the greater-
noted that the human visual system assembles complex              than relation and its children nodes) and large fragments
representations of images from simpler features, but that         (such as the whole description).
it is still an open question how these complex represen-
tations are used in visual processing. With this in mind,                                     causes
they showed that, from an information-theoretic point                                  and               orbit
of view, features of an intermediate size and complexity
                                                                                                greater
are best for the basic visual task of classification; for ex-
ample, to identify a face in an image, looking for face                         attract    mass of   mass of
fragments of intermediate size (such as a pair of eyes) is
more useful than looking for small features (an eye) or                               sun        planet
large features (a complete face).                                                                                           causes
   In their experiment they searched a database of ap-                                                                and             orbit
proximately 180 faces and automobiles for approxi-                    sun                                                     greater
mately 50 selected face fragments, and measured the
mutual information delivered by each fragment. In this               planet                 greater            attract    mass of  mass of
context, a feature that yields a great deal of mutual in-                               mass of  mass of
formation was a feature which, if present, was a good                                                                sun       planet
indicator that the class was present as well.                                     sun        planet
   From this perspective, Ullman and coworkers found
that fragments of an intermediate size and complex-
                                                                  Figure 1: Small, intermediate, and large-sized features
ity maximized the mutual information. Leveraging
this knowledge, they designed a detection scheme that             in a symbolic representation. Two features of small size,
weighted matches of intermediate features more heavily            drawn from the overall description, appear on the left. A
than matches of either small or large features, and pro-          feature of intermediate size, shown in the middle, would
duced a 97% face detection rate in novel images, with             in this case consist not only of two objects but also a
only a 2.1% false positive rate.                                  relation between them. A feature of large size, shown on
   They intuitively account for their detection scheme’s          the right, might be the whole description. We argue that
impressive results by noting that small, blurred fea-             features of intermediate size are essential when modelling
tures produce many false positives (a blurred eye often           precedent retrieval at the level of human experts.
matches a random image feature by chance) and that
large, complex features produce many false negatives (a              Also by analogy with Ullman’s visual recognition
detailed image of a face rarely matches anything in a             scheme, we can conceive of a feature-matching retrieval
small collection of stored faces). It is rather the features      scheme as one which takes a feature of a description
that are only somewhat blurred and of a intermediate              (i.e., a collection of nodes) and “looks” for this feature
size that are most useful for identification.                     in other description graphs in memory. Those sources
                                                              667

in memory which best match the set of input features
would be retrieved most strongly. We hypothesize that                 Basic Feature-matching Retrieval Test Algorithm
to achieve either novice-like or expert-like retrieval, we
merely change the average size of the feature used for            1. Break the target p and source t into all possible
retrieval. Novice-like retrieval would use small features,             features {FP } and {Ft }.
and expert-like retrieval would use intermediate features.
In other words, novice-like and expert-like retrieval are         2. ∀fp ∈ FP , from largest to smallest
achieved with the same mechanism, but with different                (a) Measure the pairwise match score ∀(fp , ft ) : ft ∈
constraints.
                                                                          Ft .
               Experimental Results                                (b) Take the best feature match and, if it is above
                                                                          the threshold, add its score to the total for t, and
To lend support to our hypothesis we have con-
structed an implementation of a feature-matching re-                      remove the corresponding ft from FT .
trieval scheme in which the feature size can be varied,           3. Return the total score for t.
and ran computational experiments that have shown
that analogical retrieval occurs preferentially when in-
termediate features are used. Our expected result is            Figure 2: Description of the algorithm which is used
to show that, with a generalized feature-matching algo-         to demonstrate the utility of intermediate features for
rithm, if one retrieves with small features, one achieves a     retrieving analogical sources from memory. The algo-
novice-like pattern of retrieval, and if one retrieves with     rithm takes as input a target and source description and
intermediate-sized features, one achieves an expert-like        a threshold, and returns a retrieval score. To judge what
pattern of retrieval.                                           is retrieved from a source set given a target description,
                                                                the algorithm is run on each source in the set and the
Outline of a Test Algorithm
                                                                scores are compared.
The algorithm that was run is outlined in Figure 2. Each
feature is a subtree of the description graph, that is, a
node plus descendants. Match scores for feature pairs           memory (Vaina & Greenblatt, 1979), where to each
are produced by a simple tree alignment, which starts           node is attached a collection of ordered lists (threads)
by matching the head nodes, measuring their seman-              of hierarchy terms to which the object or concept repre-
tic similarity, and then proceeding recursively down the        sented by the node belongs. In other words, every node,
tree, stopping when two nodes are incompatible (e.g.,           whether it represents an object, a relation, or something
have different numbers of arguments) or the features are        else, maintains one or more sequences of class member-
exhausted. The threshold in the algorithm is used to            ship strings, and each such sequence is called a thread.
vary the average size of the features that are involved         For example, a person might be described with the
in matching. It does not control the size of the features       thread ambassador--diplomat--politician--human
directly but rather eliminates feature pairings based on        --primate--animal--thing which indicates that the
the pair’s match score. When two features match well,           most specific class of which they are member is the
their match score is proportional to their size, so the         ambassador class, and the most general class the thing
threshold loosely controls the size of the features which       class. Thus, comparing with another person, say a
participate in matching. This technique was used so as to       fireman: fireman--rescue-worker--human--primate
eliminate highly-uninformative features from the match          --animal--thing we find that they match on the
pool at low thresholds.                                         last four classes, but not the others. By counting the
   This algorithm is intended to be a generalized feature-      number of elements in common between two threads we
matching algorithm. We do not claim that this is the            can get a rough measure of their semantic similarity. 1
algorithm used for the process, but rather it is our aim        As can be seen, our representation is highly similar to
to demonstrate that by using a feature-matching algo-           others used in research on analogy that it encompasses
rithm and concentrating on intermediate features, one           both episodic memory implemented as a graph (here,
can move from novice-like to expert-like retrieval.             nodes and their graph structure), and a type of semantic
                                                                memory implemented as frames attached to nodes in
Producing a Test Dataset                                        that graph (here, thread memory) (Thagard, Holyoak,
Like previous work from our laboratory on analogical            Nelson, & Gochfeld, 1990).
reasoning, our implementation takes near-natural                   To guarantee that our source memory contains
English paraphrases of situations and automatically             matches to our targets of the proper character, that is,
produces a description graph on which all subsequent            those that resemble the sorts of sources used in psy-
algorithms are run (Winston, 1980, 1982). In these              chological experiments, we synthesized sources by sys-
graphs, nodes represent objects or concepts, and edges          tematic transformation of nodes and threads in the tar-
represent a simple “argument-of” relation. The stories              1
                                                                      In particular, our algorithm takes the number of thread
averaged 16 sentences (deviation of 5.3) and contained          elements in common over the number of distinct thread ele-
on average 65 nodes (deviation 20.2). Our representation        ments between two nodes as the semantic similarity between
incorporates a rough model of semantics called thread           two nodes, a number that runs between 0 and 1.
                                                            668

get. For each target, we made four sources from its de-                      Type    Example
scription: an analogically related match (AN), a less-
analogically related match (LAN), a mere-appearance                         Probe    The boy ran away to the rock and hid
match (MA), and a literally-similar match (LS).2 For                                 because the girl threatened him.
example, suppose we began with a target description                    LS source     The man ran to the boulder and hid
“The boy ran away to the rock and hid because the                                    because the woman threatened him.
girl threatened him.” To make a literally-similar match               MA source      The girl threatened the boy because he
to this target, we replace the objects in the situation
                                                                                     ran to the rock and hid.
with nearly similar or identical objects, while leaving
the structure unchanged. In our implementation we re-                 AN source      The army retreated to the castle and
placed each object with another object which matches                                 dug in because the enemy army ap-
on all but the last thread element. Thus we might                                    proached.
replace boy with man and girl with woman and rock                    LAN source      The army returned to their castle, but
with boulder to produce “The man ran to the boulder                                  the enemy only approached when they
and hid because the woman threatened him.” To ob-
                                                                                     dug in.
tain a merely-apparently similar source, we leave the
objects unchanged, but scramble the higher-order struc-
                                                                   Table 1: Examples of systematic derivations of literally
ture. This means that we take higher-order nodes of
the same order and randomly switch their subjects and              similar (LS), merely-apparently similar (MA), analogical
objects. This might produce “The girl threatened the               (AN), and less-analogical (LAN) source matches to a
boy because he ran to the rock and hid.” An analogical             target.
match involves different objects, but the same sorts of re-
lations. To effect this we replaced all the objects in the
target with objects which matched on only highest mem-             into our graph representations. These descriptions were
bership classes, while leaving the higher-order structure          used as the targets. Each description was used to gener-
unchanged. Thus a generated analogical source might                ate the four related descriptions (literally-similar, mere-
be “The army retreated to the castle and dug in because            appearance, analogical, and less-analogical), and these
the enemy army approached.” To make what we call                   56 descriptions were used as sources. The retrieval algo-
a less-analogically similar match, we transform as if to           rithm was run with each target, and the retrieval score
make an analogy, but we mix up the subjects and objects            was measured between the target and its related sources.
of some fraction of higher-order relations as is done for          The results of each source type was averaged across all
a mere-appearance match.3 This might produce “The                  the targets for each threshold and normalized, and the
army returned to their castle, but the enemy only ap-              order of retrieval was compared to the predicted order
proached when they dug in.” Table 1 summarizes the                 for novice-like (LS > MA > AN > LAN) and expert-
different sorts of source types and their examples.                like (LS > AN > LAN > MA) retrieval, resulting in two
   Following on Gentner’s retrieval results (Rattermann            confidence curves shown in Figure 3.
& Gentner, 1987), we expect that novice-like retrieval                The curves are calculated as follows: the novice or ex-
will result in LS probes being most preferred, followed by         pert retrieval patterns differ in the position of the mere-
MA, AN and LAN in that order. Thus the novices prefer              appearance source. Each source score which was in the
superficially-similar stories (MA) to analogically-related         correct order relative to its associated mere-appearance
stories (AN and LAN). Expert-like retrieval would also             source score was given a confidence of 1 (i.e., a correct
place LS probes first, but would prefer AN and LAN                 prediction). If in the incorrect order, it was given a con-
probes before MA probes.                                           fidence of 0. If the scores were equal, they were given
                                                                   a confidence of 0.5 (fifty percent chance of choosing the
Experiment 1                                                       correct order). These three confidence values were then
Experiment 1 demonstrates that both novice-like and                averaged to obtain the probability of making a correct
expert-like retrieval can be achieved with variation of            order prediction given the retrieval scores assigned by
the feature-size parameter of a feature-matching mech-             the algorithm.
anism. In this experiment we run our demonstration                    These curves show the probability of predicting the
algorithm and vary its single parameter, the threshold,            correct human retrieval order given the retrieval scores
which loosely controls the size of the features used in            provided by the algorithm. As can be seen, the novice
matching.                                                          order is well predicted by the scores produced at a low
   Our dataset consisted of fourteen story descriptions            threshold, that is, at a low feature size. The expert order
provided in near-natural simple English. The fourteen              is well predicted at intermediate feature sizes. We see the
story descriptions were first parsed from simple English           effect anticipated, namely a novice-like retrieval pattern
                                                                   at low feature sizes, and an expert-like retrieval pattern
   2
     Note that the AN, MA, and LS match types are not              at intermediate features sizes.
unique to our work, but follow on source types established in
the analogy and retrieval literature (Gentner, 1983; Gentner       Experiment 2
& Landers, 1985).
   3
     For the experiments presented in this paper, the fraction     Experiment 2 shows that higher-order features do not
was approximately one-third.                                       contribute significantly to novice-like retrieval, and fur-
                                                               669

              1
                                                                                                                     1
                                                                                 Novice
             0.9                                                                                                                Novice
                                                                                 Expert                            0.9
                                                                                                                                Expert
             0.8
                                                                                                                   0.8
             0.7
                                                                                                                   0 .7
             0.6
                                                                                                                   0 .6
Confidence
                                                                                                      Confidence
             0.5
                                                                                                                   0 .5
             0.4
                                                                                                                   0 .4
             0.3
                                                                                                                   0 .3
             0.2
                                                                                                                   0.2
             0.1
                                                                                                                   0 .1
              0
                                                                                                                     0
                   0   2   4   6   8   10   12   14   16     18   20   22   24     26     28
                                                                                                                          28   26   24   22   20    18   16   14    12    10   8   6   4   2   0
                                                 Threshold
                                                                                                                                                              Threshold
Figure 3: Probability of predicting the correct order-                                               Figure 4: Probability of predicting the correct ordering
ing (either Novice or Expert) averaged over the dataset,                                             averaged over the dataset, as the threshold is dropped
against threshold. The behavior of our test program be-                                              from above. Note that the abscissa is reversed so that
comes more like that of human experts when features of                                               the left side of the figure indicates the same condition
small drop out allowing features of intermediate size to                                             as the left side of Figure 3, i.e., no features excluded.
dominate matching, at a threshold of about eight, but                                                As expected, the behavior of our test program mirrors
then the behavior degrades when intermediate features                                                human novices until features of small size no longer par-
drop and only larger features participate, at a threshold                                            ticipate in matching, at which point the results fluctuate
of about 18. On the other hand, the behavior of our                                                  widely because relatively few features participate in the
test program is most like that of a human novice when                                                match.
features of only small size participate in matching. The
abscissa runs until the threshold is larger than the score
of any feature pair.                                                                                 cause intermediate-features and small-features together
                                                                                                     do not produce expert-like retrieval (Experiment 2), and
                                                                                                     neither large-features (Experiment 1) nor small features
thermore that intermediate-features are alone respon-                                                (Experiment 2) alone do not produce expert-like re-
sible for expert-like retrieval. In the previous experi-                                             trieval, we conclude that intermediate features alone are
ment, all feature pairs which have score above a certain                                             responsible for expert-like retrieval.
threshold are allowed to count toward a source’s total
retrieval score. Thus, as the threshold is raised, so too                                                                                          Discussion
is the average score, and the average feature size. How-                                             Our work speaks to a computational-level account of the
ever, when the threshold is low, higher scoring matches                                              retrieval phenomenon and does not commit us to a par-
also contribute to the retrieval scores. According to                                                ticular implementation at the algorithmic level. For ex-
our conception of the intermediate features constraint,                                              ample, we can readily imagine efficient processes that
novices are characterized by their inability to access, in-                                          select appropriate intermediate-level features and com-
dex, form, or use these higher-order feature pairs. Thus                                             press them into single nodes; this would bring our model
this experiment investigates whether higher-order fea-                                               into alignment with fast algorithms based on feature-
tures effect the novice-like retrieval pattern. This ex-                                             vector comparisons (Forbus, Gentner, & Law, 1994).
periment used the same dataset and procedure as the                                                  Such an approach would explain why novices cannot
previous two experiments, with the exception that the                                                simple tell themselves to retrieve on intermediate size
algorithm was changed slightly to reject matches with a                                              chunks; they lack the apparatus for selecting and com-
score higher than the threshold, so that the threshold                                               pressing intermediate features.
could be dropped from above and we could investigate                                                    Alternatively, in an implementation based on a
the retrieval pattern as participating feature matches                                               constraint-satisfaction network (Thagard et al., 1990),
were restricted to smaller and smaller features.                                                     intermediate-sized representation pieces could be imple-
   As can be seen, the retrieval pattern of the algorithm                                            mented by applying a feature-size filter to the construc-
is novice-like until features of small size no longer par-                                           tion of the nodes in the constraint network. Then, when
ticipate in matching, allowing intermediate features to                                              the network is run, intermediate features would domi-
dominate. As the threshold drops from above, the novice                                              nate the retrieval of sources, and the system would ac-
retrieval pattern is maintained until features with small                                            complish expert-like retrieval.
scores begin to be discarded, at which point the pattern                                                For a hybrid system such as Kokinov’s (Kokinov,
becomes degraded and extremely noisy. This confirms                                                  1994), our results might indicate the proper balance be-
that higher-order features do not significantly contribute                                           tween the amount of structure and amount of semantics
to the novice-like retrieval pattern. Furthermore, be-                                               used in the construction of a representation intended
                                                                                               670

for expert-like retrieval. For a system based on dy-           Keane, M. T., Ledgeway, T., & Duff, S. (1994). Con-
namic binding of representational structures (Hummel                straints on analogical mapping: A comparison of
& Holyoak, 1997), intermediate features might indicate              three models. Cognitive Science, 18, 387-438.
the level at which representational elements should first      Kokinov, B. N. (1994). A hybrid model of reasoning
be synchronized.                                                    by analogy. In J. A. Barnden & K. J. Holyoak
                                                                    (Eds.), Analogical connections (Vol. 2, p. 247-318).
                    Contributions                                   Norwood, NJ: Ablex Publishing.
                                                               Marr, D. (1982). Vision. W.H. Freeman and Company.
First, at the information-level, we supported the view
that both novice-like and expert-like retrieval are man-       Novick, L. R. (1988). Analogical transfer, problem
ifestations of a single, parameterized feature-matching             similarity, and expertise. Journal of Experimen-
                                                                    tal Psychology: Learning, Memory, and Cognition,
mechanism.                                                          14, 510-520.
   Second, we implemented a representative algorithm
that embodied, in a transparent fashion, the basic             Palmer, S. E.       (1989).    Levels of description in
                                                                    information-processing theories of analogy. In
informational-level principles at the root of the hypoth-           S. Vosniadou & A. Ortony (Eds.), Similarity and
esis.                                                               analogical reasoning. Cambridge: Cambridge Uni-
   Finally, we conducted experiments with that algo-                versity Press.
rithm that showed that it can achieve both novice-like         Rattermann, M. J., & Gentner, D. (1987). Analogy
retrieval via small features and expert-like retrieval via          and similarity: Determinants of accessibility and
intermediate-sized features.                                        inferential soundness. In Proceedings of the annual
                                                                    conference of the cognitive science society (Vol. 9,
                 Acknowledgments                                    p. 23-35). Lawrence Erlbaum Associates.
Thanks to all the members of the Genesis group for their       Reed, S. K., Dempster, A., & Ettinger, M. (1985). Use-
assistance and constructive critiques. This work is sup-            fulness of analogous solutions for solving algebra
ported in part by the National Science Foundation under             word problems. Journal of Experimental Psychol-
Grant Numbers 0218861 and IIS-0413206.                              ogy: Learning, Memory, and Cognition, 11, 106-
                                                                    125.
                      References                               Reed, S. K., Ernst, G. W., & Banerji, R. (1974). The
                                                                    role of analogy in transfer between similar problem
Brown, A. L. (1989). Analogical learning and trans-                 states. Cognitive Psychology, 6, 436-450.
      fer: What develops? In S. Vosniadou & A. Ortony
      (Eds.), Similarity and analogical reasoning. Cam-        Ross, B. H. (1984). Remindings and their effects in
      bridge: Cambridge University Press.                           learning a cognitive skill. Cognitive Psychology,
                                                                    16, 371-416.
Chi, M. T. H., Feltovich, P. J., & Glaser, R. (1981).          Ross, B. H. (1987). This is like that: The use of ear-
      Categorization and representation of physics prob-            lier problems and the separation of similarity ef-
      lems by experts and novices. Cognitive Science, 5,            fects. Journal of Experimental Psychology: Learn-
      121-152.                                                      ing, Memory, and Cognition, 13, 629-639.
Forbus, K. D., Gentner, D., & Law, K.              (1994).     Shafto, P., & Coley, J. D. (2003). Development of
      MAC/FAC: A model of similarity-based retrieval.               categorization and reasoning in the natural world:
      Cognitive Science, 19, 141-205.                               Novices to experts, naive similarity to ecological
French, R. M. (2002). The computational modeling of                 knowledge. Journal of Experimental Psychology:
      analogy-making. Trends in Cognitive Sciences, 6,              Learning, Memory, and Cognition, 29, 641-649.
      200-205.                                                 Shneiderman, B. (1977). Measuring computer program
Gentner, D. (1983). Structure-mapping: A theoretical                quality and comprehension. International Journal
      framework for analogy. Cognitive Science, 7, 155-             of Man-Machine Studies, 9, 465-478.
      170.                                                     Thagard, P., Holyoak, K. J., Nelson, G., & Gochfeld, D.
Gentner, D., & Landers, R. (1985). Analogical remind-               (1990). Analog retrieval by constraint satisfaction.
      ing: A good match is hard to find. In Proceedings of          Artificial Intelligence, 46, 259-310.
      the international conference on systems, man and         Ullman, S., Vidal-Naquet, M., & Sali, E. (2002). Visual
      cybernetics (p. 607-613). Tucson, AZ: IEEE.                   features of intermediate complexity and their use
Gick, M. L., & Holyoak, K. J. (1980). Analogical prob-              in classification. Nature Neuroscience, 5, 682-687.
      lem solving. Cognitive Psychology, 12, 306-355.          Vaina, L., & Greenblatt, R. (1979). The use of thread
Gick, M. L., & Holyoak, K. J. (1983). Schema induction              memory in amnesia aphasia and concept learning
      and analogical transfer. Cognitive Psychology, 15,            (Working Paper No. No. 195). MIT Artificial In-
      1-38.                                                         telligence Laboratory.
Holyoak, K. J., & Koh, K. (1987). Surface and struc-           Winston, P. H. (1980). Learning and reasoning by anal-
      tural similarity in analogical transfer. Memory and           ogy. Communications of the ACM, 23, 689-703.
      Cognition, 15, 332-340.                                  Winston, P. H. (1982). Learning new principles from
Hummel, J. E., & Holyoak, K. J. (1997). Distributed                 precedents and exercises. Artificial Intelligence, 19,
      representations of structure: A theory of analogical          321-350.
      access and mapping. Psychological Review, 104,
      427-466.
                                                           671

