UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Individual Differences with Dirichlet Processes
Permalink
https://escholarship.org/uc/item/8p6619hf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Griffiths, Thomas L.
Lee, Michael D.
Navarro, Daniel J.
et al.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

               Modeling Individual Differences with Dirichlet Processes
                                   Daniel J. Navarro (daniel.navarro@adelaide.edu.au)
                           Department of Psychology, University of Adelaide, SA 5005, Australia
                                    Thomas L. Griffiths (thomas griffiths@brown.edu)
            Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912, USA
                                              Mark Steyvers (msteyver@uci.edu)
                Department of Cognitive Sciences, University of California, Irvine, Irvine CA 92697, USA
                                       Michael D. Lee (michael.lee@adelaide.edu.au)
                            Department of Psychology, University of Adelaide, SA 5005, Australia
                            Abstract                                data, it also foregoes the potential benefits of averag-
                                                                    ing, and guarantees that models are fit to all of the
   We introduce a Bayesian framework for modeling indi-             noise in the data. Clearly, individual subject analysis
   vidual differences, in which subjects are assumed to be-         increases the risk of overfitting, and hence reduces the
   long to one of a potentially infinite number of groups. In
   this model, the groups observed in any particular data           ability to make accurate predictions or to generalize to
   set are not viewed as a fixed set that fully explain the         new contexts. As a result, a number of authors have
   variation between individuals, but rather as representa-         considered more economical ways of expressing individ-
   tives of a latent, arbitrarily rich structure. As more peo-      ual differences, which seek to describe the ways in which
   ple are seen, the number of observed groups is allowed           people are the same as well as the ways in which they
   to grow, as more details about the individual differences
   are revealed. We use the Dirichlet process – a distri-           are different (e.g., Peruggia, Van Zandt & Chen, 2002;
   bution widely used in nonparametric Bayesian statistics          Rouder, Sun, Speckman, Lu & Zhou, in press; Steyvers,
   – to define a prior for the model, allowing us to learn          Tenenbaum, Wagenmakers & Blum, 2003; Webb & Lee,
   flexible parameter distributions without overfitting the         2004).
   data, or requiring the complex computations typically
   required for determining the dimensionality of a model.             Two dominant approaches have emerged in the litera-
   As an initial demonstration of the approach, we present          ture on modeling individual differences. In the stochas-
   an application of the method to categorization data.             tic parameters model (e.g., Peruggia et al., 2002; Rouder
                                                                    et al., in press), every participant is assumed to have a
   Much of cognitive science involves the development               unique parameter value θ that is randomly sampled from
and evaluation of models. Models formalize theoret-                 a parametric distribution, as illustrated in Figure 1a. In
ical predictions and have been successfully applied to              contrast, the groups model assumes that people fall into
a range of phenomena. A recurrent problem, however,                 one of a number of different clusters. Within a group,
is that individual differences between people are often             people are assumed to behave in essentially the same
overlooked. This occurs because, most often, models                 way, but each group is qualitatively different. Under
are evaluated against data that have been averaged or               this approach to individual differences modeling (e.g.,
aggregated across subjects, and so assume that there                Lee & Webb, in press; Steyvers et al., 2003; Webb &
are no individual differences between them. In this pa-             Lee, 2004), the goal is to partition subjects into a num-
per we introduce a new framework for modeling individ-              ber of groups and associate each group with a parameter
ual differences. Informed by recent insights in statistics          set θ, as illustrated by the parameter distribution shown
and machine learning (e.g., Escobar & West, 1995; Neal,             in Figure 1b.
2000), our infinite groups model makes it possible to di-
vide subjects who behave similarly into groups, without                      Hierarchical Bayesian Models
assuming an upper bound on the number of groups. This               The assumptions underlying these two approaches to
model is sufficiently flexible to capture the heterogeneous         modeling individual differences can be understood by
structure produced by different subjects pursuing differ-           viewing both as hierarchical Bayesian models (e.g., Lind-
ent strategies, allows the number of groups to grow nat-            ley & Smith, 1972). If data arise from a parametric dis-
urally as we observe more data, and avoids the complex              tribution x ∼ F (· | θ) described by some cognitive model,
computations often required for determining the dimen-              then there is assumed to exist a higher-order process
sionality of an individual differences model.                       G(· | φ) that generates the values of θ. A two level hier-
                                                                    archical model with parameters φ is written,
        Modeling Individual Differences                                                  θ|φ ∼      G(· | φ)
In those cases of cognitive modeling that recognize indi-                                 x|θ ∼     F (· | θ).
vidual differences, it is usually assumed that each subject
behaves in accordance with a different parameterization,            To apply Bayesian inference to such a model, we also
θ, of a single model, and that model is evaluated against           need to define a prior on φ. We will assume that φ ∼ π(·)
the data from each subject separately (e.g., Ashby, Mad-            for an appropriate distribution π(·).
dox & Lee, 1994; Nosofsky, 1986). Although this avoids                 In the stochastic parameters model G(· | φ) is usually
the problem of corrupting the underlying pattern of the             a tractable distribution such as a Gaussian, with φ corre-
                                                               1594

                                                                  assumed by the stochastic parameters model, but still
                                                                  allows efficient inference. We assume that subjects are
                                                                  drawn from an infinite number of groups, taking G(· | φ)
                                                                  to be a weighted combination of an infinite number of
  pdf                pdf                     pdf                  point masses, as in Figure 1c. That is,
                                                                                                  X
                                                                                                  ∞
                                                                                  G(· | w, ξ) =         wj δ(ξj ).       (2)
          θ                  θ                      θ                                             j=1
         (a)                 (b)                   (c)            While we assume that the number of groups is un-
                                                                  bounded, any finite set of subjects will contain represen-
Figure 1: Parameter distributions associated with stochastic      tatives from a finite subset of these groups. By avoiding
parameters approach to individual differences (panel a), the      setting an upper bound on the number of groups, we no
original groups approach (panel b), and the infinite groups       longer need to perform explicit model selection to iden-
approach (panel c).                                               tify the number of groups. This model has an inherent
                                                                  psychological plausibility: people can vary in any num-
                                                                  ber of ways, only some of which will be observed in a fi-
sponding to the parameters of that distribution. In the           nite sample. With infinitely many groups, there is always
groups model G(· | φ) is a weighted collection of k point         the possibility that a new subject can display a pattern of
masses, as depicted in Figure 1b. That is,                        behavior that has never been seen before. Moreover, the
                                                                  approach requires us to make our assumptions explicit,
                                k
                                X                                 in the form of a well-defined prior distribution over the
                G(· | w, ξ) =         wj δ(ξj ),         (1)      number of observed groups. In contrast, in most finite-
                                j=1
                                                                  order model selection scenarios these assumptions are
where δ(ξj ) denotes a point mass located at θ = ξj and           usually swept up in an implicit and often inappropriate
        P                                                         uniform prior over model order (for a notable exception,
where kj=1 wj = 1. In the groups model, φ corresponds             see Courville, Daw, Gordon & Touretzky, 2004).
to the parameters (w, ξ).                                            In order to apply Bayesian inference in the hierarchical
   This perspective reveals some of the strengths and             model defined by Equation 2, we need to define a prior
weaknesses of these two models. Assuming that θ fol-              π(·) on the parameters of G, in this case w and ξ. In a
lows a parametric distribution, as in the stochastic pa-          finite groups model with k groups (i.e., Equation 1), a
rameters model, simplifies the problem of fitting indi-           standard prior is
vidual differences models to data, but places strong con-
straints on the kind of variation that can manifest across                    ξ     ∼           G0(·)
                                                                                                                         (3)
subjects. A particularly severe problem arises when we                     w | α, k ∼ Dirichlet(α/k, . . ., α/k).
specify a unimodal distribution to capture individual dif-        In this prior, the locations of the k point masses ξj are
ferences that are inherently multimodal. In this case the         sampled from the base distribution denoted G0(·). The
model cannot capture the most important aspect of the             Dirichlet distribution over w gives us a prior over the
variation between people.                                         different ways in which k groups could be weighted, in
   Unlike the stochastic parameters approach, the pa-                                  Qk      (α/k)−1
rameter distributions postulated by group models natu-            which p(w | α, k) ∝ j=1 wj           . Specifying the base
rally account for multimodality in individual differences.        distribution, G0 and the dispersion parameter of the
By postulating two groups, for instance, we arrive at a           Dirichlet distribution, α, defines a prior over distribu-
bimodal distribution. However, there is a lack of flexi-          tions G(·) for any finite groups model.
bility in parameter distributions consisting only of a few           If we take the limit of the prior π(·) defined by Equa-
point masses. Moreover, a computational problem faced             tion 3 as k → ∞, we obtain the distribution known as
by the groups model is the difficulty in choosing the num-        the Dirichlet process (Ferguson, 1973). This distribution
ber of groups. This is typically treated as a model selec-        takes its name from the fact that is very much like an in-
tion problem, addressed by evaluating a series of models          finite dimensional Dirichlet distribution (see Schervish,
which assume different numbers of groups. Such a strat-           1995, pp. 52-60; Ghosh & Ramamoorthi, 2003). The
egy is time-consuming, and makes the false assumption             Dirichlet process provides a prior for infinite models,
that there really is a fixed number of groups, with fu-           and inference in models using this prior is generally
ture subjects belonging to the same set of groups. In             straightforward. If the distribution G(·) is sampled from
the remainder of the paper we will explore a model that           a Dirichlet process, we write G | G0, α ∼ DP (· | G0, α),
combines the strengths of these two approaches, having            so the infinite groups model can be written
the flexibility of the groups model, but a simple inference                     G | G0, α ∼       DP (· | G0, α)
algorithm.                                                                           θ|G ∼        G(·)
          The Infinite Groups Model                                                  x|θ ∼        F (· | θ),
In the infinite groups model, we adopt a distribution on          where α is the dispersion parameter (setting the prior
θ that is more flexible than the parametric distribution          on w) and G0(·) is the base distribution on ξ. In this
                                                           1595

                                                                              .3                                                        .3
model, the base distribution G0(·) represents our prior                                                   a=5, b=1                                                  a=5, b=1
                                                                                                          a=15, b=1                                                 a=5, b=0.4
beliefs about the kinds of parameter values θ that are                                                    a=25, b=1                                                 a=5, b=0.2
                                                                                                          a=35, b=1                                                 a=5, b=0.1
likely to capture human performance in a particular task,                     .2                                                        .2
                                                                 p(α | k,n)                                                p(α | k,n)
while the dispersion parameter α represents the amount
of variation that we expect to see in a finite sample. If
                                                                              .1                                                        .1
α is low, then most people will be expected to behave
similarly to one another, and the distribution G(·) will
concentrate most of its mass on a few points. However,                        0                                                         0
                                                                                   0   10   20       30    40         50                     0   10   20       30     40         50
if α is large, then people will be expected to be very                                           α                                                         α
different to one another, and G(·) will spread its mass                       .3                                                        .3
over a large number of points.                                                                            a=5, b=1
                                                                                                          a=15, b=1
                                                                                                                                                                    a=5, b=1
                                                                                                                                                                    a=5, b=0.4
   The Dirichlet process defines a distribution over the                                                  a=25, b=1
                                                                                                          a=35, b=1
                                                                                                                                                                    a=5, b=0.2
                                                                                                                                                                    a=5, b=0.1
assignment of subjects to groups. Since wj gives the                          .2                                                        .2
                                                                 p(α | k,n)                                                p(α | k,n)
probability that the ith observation belongs to the jth
group, it is convenient to introduce the group member-                        .1                                                        .1
ship variable gi , such that p(gi = j) = wj . Given the
group assignments of i − 1 subjects, it is straightfor-
ward to compute the probability distribution over gi                          0
                                                                                   0   10   20       30    40         50
                                                                                                                                        0
                                                                                                                                             0   10   20       30     40         50
given g−i = {g1, . . . , gi−1}. In a finite groups model                                         α                                                         α
with the prior given in Equation 3, we can integrate over
w1, . . . , wk , and obtain                                      Figure 2: Posterior distributions over α given k and n. In
                                                                 the top row k = 8 and n = 10, while in the bottom row
     p(gi = j | g−i, α, k) = (nj + α/k)/(i − 1 + α),             k = 79 and n = 1000. In both cases, the expected value of α
                                                                 is approximately 20. On the left hand side, the Gamma priors
where nj denotes the number of elements in g−i that are          over α have the same scale parameter but vary in shape. On
equal to j. If we let k → ∞, the limiting probabilities          the right hand side,the priors have the same shape parameter
are                                                              but vary in scale.
                           nj
                             i−1+α   if j ≤ k−i
     p(gi = j | g−i, α) ∝       α
                             i−1+α
                                     otherwise,                  In particular, Escobar and West (1995) note that if we
                                                                 let a → 0 and b → 0 we obtain a so-called scale-invariant
where k−i denotes the number of distinct groups present          prior in which p(α) ∝ 1/α (see Jeffreys, 1961). Posterior
in g−i. This gives the appropriate conditional probabil-         distributions for α are shown in Figure 2. The influence
ity under the Dirichlet process (Neal, 2000).                    of the prior is shown by varying both the shape (left
   Through the distribution over group assignments, the          hand side) and the scale (right hand side) parameters.
Dirichlet process induces a prior p(k | α, n) over the num-
ber of unique groups k that will manifest among n sub-                                      Modeling Discrete Data
jects. Antoniak (1974) shows that p(k | α, n) ∝ znk αk ,
where znk is an unsigned Stirling number of the first            We now turn to the derivation and application of the
kind (see Abramowitz & Stegun, 1972). He also observes           infinite groups model to situations in which participants
that the expected number of components sampled from              provide discrete data. Suppose that n people perform
a Dirichlet process is approximately given by,                   some task in which m possible responses can be made
                                                               on each trial, and each person experiences s trials. We
                                    n+α                          can describe the ith participant’s responses with the vec-
               E[k | α, n] ≈ α ln           .                    tor xi = (xi1, . . . , xim), where xih counts the number
                                     α
                                                                 of times that they made the hth response. Using the
Thus, although k → ∞ with probability 1 as n → ∞,                Dirichlet process, we assume that each person belongs
the number of components increases approximately log-            to one of an infinite number of groups, and that the pa-
arithmically with the number of observations.                    rameters for the jth group describe a multinomial rate
   In most contexts the dispersion α is unknown, so we           θj = (θj1, . . . , θjm ) such that θjh denotes the probability
specify a prior distribution p(α), allowing us to learn α        with which a member of group j makes response h on
from data. The posterior distribution over α is given by         any given trial. Since this likelihood function is multino-
                                                                 mial, it is convenient to assume that the base distribution
               p(α | k, n) ∝ B(α, n)αkp(α),                      G0(·) is a Dirichlet distribution with symmetric param-
                                                                 eter β. We will assume that the dispersion parameter
where B(α, n) is a standard Beta function. A com-                α is unknown, and follows a Gamma distribution with
mon choice for p(α) is the Gamma distribution α | a, b ∼         parameters a and b. The model is illustrated in Figure 3.
Gamma(· | a, b) in which p(α) ∝ αa−1e−bα (Escobar &                 This model lends itself to inference by Gibbs sampling,
West, 1995). If so,                                              a Monte Carlo method for sampling from the posterior
                                                                 distribution over the variables in a Bayesian model (see
            p(α | k, n) ∝ αa+k−1e−bαB(α, n).           (4)       Neal, 2000; Gilks, Richardson & Spiegelhalter, 1995). In
                                                          1596

                                                                                 Qm
                                                          
                                                           Q
                                                               Γ(mβ + q−i,j )      h=1 Γ(β + q·,j,h )   n−i,j
                                                          
                                                             m                                                                                         if j ≤ k−i
                                                              h=1 Γ(β + q−i,j,h )   Γ(mβ  + q·,j )    n − 1+α
                  p(gi = j | g−i, α, x) ∝                                         Qm                                                                                                       (5)
                                                          
                                                                     Γ(mβ)        h=1 Γ(β + q·,j,h )     α
                                                          
                                                                   Qm                                                                                  otherwise
                                                                      h=1 Γ(β)      Γ(mβ + q·,j ) n − 1 + α
                                                                                                    0.7                                               0.7
          Base Distribution   E       Shape       a        b     Scale
                                                                                                    0.6                                               0.6
                                                                                                    0.5                                               0.5
                              T                   D                                                 0.4                                               0.4
                                                                                         p(α | x)                                          p(k | x)
      Group Parameters
                                                      Dispersion
                                  f                                                                 0.3                                               0.3
                                                                                                    0.2                                               0.2
                                                  g        Group Membership                         0.1                                               0.1
                                                                                                     0                                                 0
                                                                                                          0       2        4     6   8                      4        6            8   10
                                                                                                                           α                                             k
                                                                                                    0.16                                              0.14
                                                  x         Observed Data
                                                                                                    0.14                                              0.12
                                                      n
                                                                                                    0.12
                                                                                                                                                       0.1
                                                                                                     0.1
Figure 3: Dependencies in the infinite groups model for dis-                                                                                          0.08
                                                                                         p(α | x)   0.08                                   p(k | x)
crete data as it is used here. Shaded circles denote observed                                                                                         0.06
                                                                                                    0.06
variables, white circles are latent variables, rounded squares
                                                                                                                                                      0.04
denote known parameter values, and plates indicate a set of                                         0.04
independent replications of the processes shown inside them.                                        0.02                                              0.02
                                                                                                          0                                                 0
                                                                                                              0       10       20     30                        40               50   60
                                                                                                                           α                                                 k
Gibbs sampling, we fix all assignments except one and
sample that assignment from the conditional posterior                                Figure 4: Simulations in which people provide s = 50 obser-
p(gi | g−i, x), a procedure which eventually converges to                            vations each, and m = 20 response options are possible on
samples from the complete posterior p(g | x). Since the                              every trial. In the upper panels there are n = 20 people and
Dirichlet is conjugate to the multinomial, it is straight-                           k = 5 groups. In the lower panels there are n = 200 people
forward to show that the conditional posterior distribu-                             and k = 50 groups.
tion over the ith group assignment is given by Equation
5. In this expression, q−i,j,h denotes the number of times
that a participant (not including the ith) currently as-                             p(η | α, k, n). These distributions are simply,
signed to group j made response h, and q−i,j denotes
the total number of responses made by these partici-                                     α | η, k, n ∼                         Gamma(· | a + k − 1, b − ln η)
                                                                                                                                                                                           (6)
pants. The terms q·,j,h and q·,j are defined similarly,                                  η | α, k, n ∼                         Beta(· | α, n).
except that the ith participant’s data are not excluded.
   For the dispersion parameter, we treat the prior over                             Equations 5 and 6 define the Gibbs sampler.
α as a Gamma(· | a, b) distribution. Using Antoniak’s                                   As a simple illustration, we created random data sets
(1974) results, the conditional posterior over α depends                             with n people and s discrete observations per person,
only on the number of observed groups k, not the spe-                                where each observation denotes a choice of one of m re-
cific assignments. Thus, by expanding the Beta function                              sponse options. The sample was divided into k groups,
B(α, n) in Equation 4 we observe that                                                and each group associated with a multinomial rate θ
                                                                                     sampled from a uniform distribution. People were al-
                                Z 1                                                  located randomly to groups using a uniform distribu-
                      a+k−1 −bα
      p(α | g, x) ∝ α      e        ηα−1 (1 − η)n−1 dη.                              tion, subject to the constraint that each group contained
                                              0                                      at least one member. Note that this allocation scheme
Since this conditional distribution is difficult to directly                         means that the Dirichlet process model is misspecified
sample from, it is convenient to employ a “data augmen-                              (which is as it should be in any worthwhile simulation).
tation”, in which we view p(α | g, x) as the marginaliza-                            Results are shown in Figure 4. The posterior distribu-
tion over η of the joint distribution,                                               tions over α are shown on the left and the posterior dis-
                                                                                     tributions over k are shown on the right. On the whole,
       p(α, η | k, n) ∝ αa+k−1e−bα ηα−1(1 − η)n−1.                                   the distributions converge on sensible answers, though in
                                                                                     both cases they reflect a fair degree of uncertainty about
Using this joint distribution, we can find p(α | η, k, n) and                        the number of groups present in the sample.
                                                                              1597

                                                                                                          0.9                                             0.45
                                                                                                          0.8                                              0.4
                                                                                                          0.7                                             0.35
                                                                                                          0.6                                              0.3
                                                                                               p(α | x)                                        p(k | x)
                                                                                                          0.5                                             0.25
                                                                                                          0.4                                              0.2
                                                                                                          0.3                                             0.15
                                                                                                          0.2                                              0.1
                                                                                                          0.1                                             0.05
                                                                                                           0                                                0
                                                                                                                0        5    10    15    20                      2        4            6        8
                                                                                                                              α                                                k
                                                                                          Figure 7: Posterior distributions over α and k when the
                                                                                          infinite groups model is applied to McKinley and Nosofsky’s
Figure 5: The category densities used in McKinley and
                                                                                          (1995) experiment 2.
Nosofsky’s (1995) experiment 2. Category A (dark grey) is a
mixture of four Gaussians, while category B (light grey) is a
mixture of two Gaussians.
                                                                                          Table 1: Percentage of occasions on which participants in
                                                                                          McKinley and Nosofsky’s (1995) experiment 2 appear in the
                             7                                                            same group.
                             6                                                                                      2        3     4      5      6                7   8            9        10
                                                                                                          1         37       0     73     0     58               68   36           43       55
          number of groups
                             5                                                                            2                  22    34     1     68               56   3            5        67
                                                                                                          3                        1     57      1                0   0            0        2
                             4                                                                            4                               0     44               52   53           60       43
                                                                                                          5                                      0                0   0            0        0
                             3
                                                                                                          6                                                      86   4            7        93
                                                                                                          7                                                           11           16       86
                                                                                                          8                                                                        91       4
                             2
                                                                                                          9                                                                                 7
                             1
                                 1   2   3   4    5       6      7    8   9   10
                                             number of participants
                                                                                          from this experiment. Since each trial is labeled by the
                                                                                          Gaussian distribution (i.e., A1, A2, A3, A4, B1, or B2)
Figure 6: Posterior over k as a function of n. The area of                                from which it was sampled, a natural way of viewing each
the squares is proportional to the posterior probability of k
given n.                                                                                  participant’s data is in terms of the probability of mak-
                                                                                          ing the correct response to stimuli sampled from each of
                                                                                          the six components. For each of the 10 participants we
                                                                                          used only the last 300 trials of the experiment, in order
 Individual Differences in Categorization
                                                                                          to look for differences in the learned category structure,
We now present an application of the infinite groups                                      rather than differences in the learning process itself. In
model. An elegant category learning experiment by                                         order to conduct a Bayesian analysis, we set principled
McKinley and Nosofsky (1995) investigated 10 people’s1                                    a priori parameter values rather than fitting the model
ability to discriminate between the two probabilistic cat-                                to the data. Since we know that both responses (i.e.,
egories shown in Figure 5. The stimuli were circles with a                                “A” and “B”) are possible but are otherwise “ignorant”,
radial line running through them, and so the two dimen-                                   the natural choice for the base distribution is the uni-
sions depicted in Figure 5 correspond to the radius of the                                form distribution β = 1 (see Jaynes, 2003, pp. 382–386),
circle, and the angle of the line. Category A (dark grey)                                 and since we have no strong beliefs about α we set the
is a mixture of four Gaussian distributions, while cate-                                  scale-invariant prior (see Jeffreys, 1961) in which a → 0,
gory B is a mixture of two Gaussians. On any given trial                                  b → 0.
in the experiment, a stimulus was sampled from one of                                        To illustrate the manner in which the model grows
the six Gaussian distributions. Participants were asked                                   with the data, imagine that the 10 participants entered
whether it came from category A or category B, and                                        the lab in order of participant ID. Figure 6 shows how
provided feedback as to the accuracy of their response.                                   the posterior distribution over k changes as more partic-
Because the categories are inherently probabilistic and                                   ipants are observed. Initially there is evidence for only a
the category densities are quite complicated, this task is                                single group, but once the 10th participant is observed,
very difficult, and shows evidence of differences not only                                there is strong evidence for about 3 or 4 groups, as il-
during the course of category learning, but in the final                                  lustrated in Figure 7. A more detailed description of
structures learned.                                                                       the relationships between participants is presented in Ta-
   In order to learn about the variation between partici-                                 ble 1, which shows the (marginal) probability that any
pants, we applied the infinite groups model to the data                                   two participants belong to the same group, and reveals
   1
     McKinley and Nosofsky (1995) actually report data for                                a rich pattern of similarities and differences. A subset
11 participants. However, the data currently available to us                              of this interaction is illustrated in Figure 8, which plots
include only 10 of these.                                                                 the last 300 stimuli observed by participants 5, 7, 8 and
                                                                                   1598

                                                                                 Acknowledgements
                                                                Research supported by Australian Research Council grant
                                                                DP-0451793. We thank Yves Rosseel for providing a copy of
                                                                the categorization data.
                                                                                        References
                                                                Abramowitz, M. & Stegun, I. A. (1972). Handbook of Mathe-
                                                                  matical Functions with Formulas, Graphs, and Mathemat-
                                                                  ical Tables. New York: Dover.
             participant 5            participant 7             Antoniak, C. E. (1974). Mixtures of Dirichlet processes with
                                                                  applications to Bayesian nonparametric problems. Annals
                                                                  of Statistics, 2, 1152-1174.
                                                                Ashby, F. G., Maddox, W. T. & Lee, W. W. (1994). On the
                                                                  dangers of averaging across subjects when using multidi-
                                                                  mensional scaling or the similarity-choice model. Psycho-
                                                                  logical Science 5, 144-151.
                                                                Courville, A. C., Daw, N. D., Gordon, G. J. & Touretzky,
                                                                  D. S. (2004). Model uncertainty in classical conditioning.
                                                                  In S. Thrun, L. Saul & B. Schölkopf (Eds) Advances in
                                                                  Neural Information Processing Systems, 16 (pp. 977–984).
                                                                  Cambridge, MA: MIT Press.
             participant 8            participant 9             Escobar, M. D. & West, M. (1995). Bayesian density estima-
                                                                  tion and inference using mixtures. Journal of the American
Figure 8: Last 300 trials for participants 5, 7, 8 and 9 in       Statistical Association, 90, 577-588.
McKinley and Nosofsky’s (1995) experiment 2. Black dots         Ferguson, T. S. (1973). A Bayesian analysis of some non-
                                                                  parametric problems. Annals of Statistics, 1, 209-230.
denote “A” responses, and grey dots denote “B” responses.
                                                                Ghosh, J. K. & Ramamoorthi, R. V. (2003). Bayesian Non-
                                                                  parametrics. New York: Springer.
                                                                Gilks, W. R. , Richardson, S., & Spiegelhalter, D. J. (1995).
9, and the decisions that they made. Broadly speaking,            Markov Chain Monte Carlo in Practice. London: Chap-
participant 5 is sensitive only to variation along the x-         man and Hall.
axis, participant 7 is sensitive only to variation on the       Jaynes, E. T. (2003). Probability Theory: The Logic of Sci-
y-axis, while participants 8 and 9 do a good job of learn-        ence. Cambridge, UK: Cambridge University Press.
                                                                Jeffreys, H. (1961). Theory of Probability (3rd ed.). London:
ing the category structures on both dimensions. As a              Oxford University Press.
result, participants 5 and 7 rarely appear in the same          Lee, M. D. & Webb, M. R. (in press). Modeling individual
group as one another or with participants 8 or 9 (with            differences in cognition. Psychonomic Bulletin & Review.
probabilities ranging from 0% to 7%), while participants        Lindley, D. V. & Smith, A. F. M. (1972). Bayes estimates for
8 and 9 almost always (91%) co-occur.                             the linear model. Journal of the Royal Statistical Society
                                                                  (Series B), 34, 1-41.
                   General Discussion                           McKinley, S. C. & Nosofsky, R. M. (1995). Investigations of
                                                                  exemplar and decision bound models in large, ill-defined
The individual differences framework outlined in this pa-         category structures. Journal of Experimental Psychology:
per provides a natural method of representing the sim-            Human Perception & Performance, 21, 128–148.
ilarities and differences between people. Moreover the          Neal, R. M. (2000). Markov chain sampling methods for
                                                                  Dirichlet process mixture models. Journal of Computa-
groups that are seen in any particular sample are not             tional and Graphical Statistics, 9, 249-265.
viewed as a fixed structure that fully explains the vari-       Nosofsky, R. M. (1986). Attention, similarity, and the
ation between individuals, but rather as representatives          identification-categorization relationship. Journal of Ex-
of a latent, arbitrarily rich structure. This means that,         perimental Psychology: General, 115, 39-57.
had we subsequently observed another 100 participants           Peruggia, M., Van Zandt, T., & Chen, M. (2002). Was it a
in the McKinley and Nososky (1995) experiment, the                car or a cat I saw? An analysis of response times for word
                                                                  recognition. In C. Gatsonis, A. Kass, R. E. Carriquiry, A.
number and variety of observed groups would grow as               Gelman, D. Higdon, D. K. Pauler, & I. Verdinelli (Eds.),
more detail about individual differences are revealed.            Case Studies in Bayesian Statistics 6 (pp. 319334). New
   The approach can be extended in a number of ways.              York: Springer-Verlag.
Firstly, in many situations we may want continuous mul-         Rouder, J. N., Sun, D., Speckman, P. L., Lu, J. & Zhou,
timodal parameter distributions. An “infinite stoch-              D. (in press). A parametric hierarchical framework for
                                                                  inference with response time distributions. Psychometrika.
astic groups” model would convolve each of the point            Schervish, M. J. (1995). Theory of Statistics. New York:
masses in the Dirichlet process with a continous distri-          Springer.
bution, giving an infinite model that subsumes both the         Steyvers, M., Tenenbaum, J. B., Wagenmakers, E. J. & Blum,
groups model and the stochastic parameters model. A               B. (2003). Inferring causal networks from observations and
second direction in which the framework could be ex-              intervations. Cognitive Science, 27, 453-487.
tended would be to allow structured relationships be-           Webb, M. R., & Lee, M. D. (2004). Modeling individual dif-
                                                                  ferences in category learning. In K. Forbus, D. Gentner
tween groups. Finally, we may wish to consider an “id-            & T. Regier, (Eds.), Proceedings of the 26th Annual Con-
iosyncratic strategies model”, in which it is assumed that        ference of the Cognitive Science Society, pp. 1440-1445.
all participants draw on a common set of strategies but           Mahwah, NJ: Erlbaum.
combine them in an unique way.
                                                           1599

